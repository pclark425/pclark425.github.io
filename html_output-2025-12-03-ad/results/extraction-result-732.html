<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-732 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-732</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-732</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-231986203</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2102.10867v1.pdf" target="_blank">Linear unit-tests for invariance discovery</a></p>
                <p><strong>Paper Abstract:</strong> There is an increasing interest in algorithms to learn invariant correlations across training environments. A big share of the current proposals find theoretical support in the causality literature but, how useful are they in practice? The purpose of this note is to propose six linear low-dimensional problems -- unit tests -- to evaluate different types of out-of-distribution generalization in a precise manner. Following initial experiments, none of the three recently proposed alternatives passes all tests. By providing the code to automatically replicate all the results in this manuscript (https://www.github.com/facebookresearch/InvarianceUnitTests), we hope that our unit tests become a standard steppingstone for researchers in out-of-distribution generalization.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e732.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e732.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IRMv1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Invariant Risk Minimization (v1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An invariance-based method that searches for a representation such that the optimal classifier on top of that representation is the same (identity) across all training environments, encouraging use of invariant (causal) features and discouraging reliance on environment-specific (spurious) features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Invariant risk minimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Invariant Risk Minimization (IRMv1)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>IRMv1 learns a feature representation phi(x) and enforces that the optimal classifier w on top of phi is identical across environments by adding an invariance penalty that encourages the environment-wise optimal classifier to be the same (operationalized in the paper as seeking representations where the identity classifier is optimal across envs). This is implemented as a constrained/penalized optimization that balances training error and invariance across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Linear unit-tests (six synthetic linear problems)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A suite of six low-dimensional linear synthetic problems (Example1, Example2, Example3 and their scrambled variants) generated from structural equation models under different interventions; not interactive or active — passive datasets with multiple training environments and held-out test splits where spurious features are randomized.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Enforcement of representation-level invariance across environments via an invariance penalty that forces a common optimal classifier, implicitly discouraging use of env-specific (spurious) features.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Environment-specific predictive features (spurious correlations / distractor variables) that vary across training environments.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Penalizes solutions whose optimal per-environment classifiers differ (in practice enforces a constraint/regularizer on classifier optimality across envs), thereby reducing influence of features that create non-invariant predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>On Example1 (regression) IRMv1 achieves MSE per-environment [0.20, 11.98, 21.27] (mean ≈ 11.15), approaching Oracle; on Example3 it performs worse than ANDMask but sometimes near others (classification errors per env ≈ [0.49,0.49,0.48], mean ≈ 0.487). See Table 1 for full per-environment values.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Baseline ERM on Example1 yields MSE per-environment [1.62, 14.25, 24.22] (mean ≈ 13.36); on Example3 ERM yields classification errors per-env ≈ [0.48,0.48,0.47] (mean ≈ 0.477).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Default 5 spurious dimensions (d_spu = 5); experiments also vary number of spurious dimensions in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>IRMv1 sometimes recovers invariant predictors (notably on Example1 and its scrambled variant where it approaches Oracle), but fails to reach Oracle on the other unit-tests; performance is sensitive to problem type (Example2 ERM can outperform IRMv1) and to the number of environments and spurious dimensions. IRMv1 is robust to scrambling in some tasks but not universally successful.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Linear unit-tests for invariance discovery', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e732.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e732.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AND-mask</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AND-mask</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that updates model parameters only along gradient directions where the sign of the gradient of the loss agrees across most environments, aiming to keep updates that are consistent/invariant across environments and ignore environment-specific gradient signals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning explanations that are hard to vary</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>AND-mask</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>AND-mask computes per-environment gradients and only applies parameter updates on coordinates whose gradient signs agree across a majority of environments (an 'AND' operation on gradient sign), thereby preventing updates that are driven by environment-specific (spurious) signals and favoring directions supported by multiple environments.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Linear unit-tests (six synthetic linear problems)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same synthetic suite as described above: passive, low-dimensional linear tasks with multiple training environments generated by structural equation models; not interactive.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Gradient-sign agreement selection: prevents updates on parameter directions where environments disagree, effectively masking out coordinates likely induced by spurious/distractor features.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Environment-specific predictive features producing gradients with inconsistent signs across environments (spurious correlations/shortcut features).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Detects spurious/distractor directions by measuring disagreement in gradient signs across environments (directions with sign disagreement are treated as spurious/unreliable).</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Zeroes or omits parameter updates on coordinates without sign agreement across environments, thereby downweighting the influence of spurious features in optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>On Example1 AND-mask MSE per-env: [0.11, 11.39, 20.28] (mean ≈ 10.59), close to Oracle; on Example3 it does well (classification mean ≈ 0.343).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>ERM baseline (no AND-mask) on Example1: MSE mean ≈ 13.36; on Example3 ERM mean ≈ 0.477, showing AND-mask improvements in several cases.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Default 5 spurious dimensions (d_spu = 5); performance evaluated while varying spurious dimensionality and number of environments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AND-mask can approach Oracle performance on some tasks (Example1) and outperforms ERM on tasks with small invariant margins (Example3), but it collapses on scrambled variants of some problems (e.g., Example3s) and is sensitive to feature representation (scrambling hurts it).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Linear unit-tests for invariance discovery', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e732.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e732.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IGA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inter-environmental Gradient Alignment (IGA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An algorithm that minimizes training error while penalizing variance of the gradient of the loss across environments, aiming to favor parameter updates that generalize across environments by aligning gradients.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Out-of-distribution generalization with maximal invariant predictor</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Inter-environmental Gradient Alignment (IGA)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>IGA adds a regularizer that reduces the variance (or misalignment) of per-environment gradients of the loss, combining standard empirical loss minimization with a term that encourages aligned gradients across environments so that updates are supported by multiple environments rather than environment-specific (spurious) signals.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Linear unit-tests (six synthetic linear problems)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Passive synthetic linear tasks with multiple environments created by structural equation models; not interactive or active experimentation.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Gradient-variance regularization: by minimizing gradient variance across environments, the method aims to avoid directions dominated by env-specific (spurious) gradients.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Environment-specific predictive features that create inconsistent gradients across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Implicit detection via gradient variance/ misalignment: large per-environment gradient variance indicates env-specific/spurious influence.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Penalizes gradients with high inter-environment variance, effectively reducing influence of spurious directions in updates.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>IGA generally performs similarly to ERM on most unit-tests; on Example2 it improves substantially as the number of environments increases (performance improves with higher ratio of environments to spurious dims), but it performs poorly on scrambled variants and on Example1 when spurious dims are present. Example1 MSE per-env: [4.47, 18.46, 29.48] (mean ≈ 17.47) which is worse than IRM and AND-mask.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>ERM baseline Example1 MSE mean ≈ 13.36; IGA performs worse than ERM on several tasks unless many environments are provided relative to spurious dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Default 5 spurious dimensions; experiments vary number of spurious dims and number of environments to show sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>IGA does not improve over ERM in most default settings; it only helps when the number of environments is large relative to the number of spurious dimensions (e.g., Example2 with increased n_env), and it is sensitive to scrambling (feature rotations undermine its gains).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Linear unit-tests for invariance discovery', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e732.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e732.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ERM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Empirical Risk Minimization (ERM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The standard baseline that minimizes the pooled training error across all environments without any invariance or environment-aware regularization, often absorbing spurious correlations present in training data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Statistical learning theory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Empirical Risk Minimization (ERM)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>ERM simply minimizes expected loss on the pooled union of training data across environments (no explicit invariance constraints), and thus will exploit any predictive signals present in training that reduce training loss, including spurious/distractor correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Linear unit-tests (six synthetic linear problems)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Passive simulated linear datasets produced from structural equation models with multiple training envs and a test split where spurious features are randomized to measure reliance on spurious signals.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Does not explicitly address any; will exploit environment-specific spurious correlations when predictive.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>ERM is the non-robust baseline; Example1 MSE per-env: [1.62, 14.25, 24.22] (mean ≈ 13.36), Example2 classification mean ≈ 0.423, Example3 mean ≈ 0.477 (see Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Default 5 spurious dimensions; performance evaluated while varying spurious dimensionality.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ERM systematically exploits spurious/distractor features present in training and fails to generalize when test randomized spurious features remove cue; in many unit-tests ERM outperforms some invariance methods on particular tasks, illustrating that invariance methods are not uniformly better.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Linear unit-tests for invariance discovery', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e732.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e732.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Invariant Causal Prediction (ICP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A causal discovery approach that seeks subsets of covariates whose conditional distribution of the target is invariant across environments, aiming to identify causal parents and discard non-invariant predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causal inference using invariant prediction: identification and confidence intervals</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Invariant Causal Prediction (ICP)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>ICP searches for sets of variables S such that the conditional distribution of the target given S is invariant across environments; under suitable assumptions this identifies causal parents. The paper mentions ICP as a causality-based algorithm consuming multiple environments to learn invariant relations.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Linear unit-tests (six synthetic linear problems)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Mentioned as a candidate method for invariance discovery; not applied in experiments because some problems (e.g., Example1) violate ICP's assumptions (varying residual distributions across envs).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>ICP conceptually handles distractors by searching for invariant conditioning sets (variable selection / hypothesis testing for invariance).</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious correlations that induce non-invariant conditional distributions (environment-specific predictors).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Statistical tests for invariance of conditional distributions across environments (not applied here due to violated assumptions in some tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>ICP rejects variable sets that do not yield invariant conditionals, thereby refuting spurious predictors; however, the paper notes ICP is disallowed on Example1 because residual distributions change across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ICP is referenced as a causality-based invariance method, but the paper explicitly notes that ICP cannot be used on Example1 because the distribution of residuals varies across environments, illustrating that some invariance/causal methods have assumptions violated by these unit-tests.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Linear unit-tests for invariance discovery', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e732.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e732.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Domain-Adversarial (DANN)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain-Adversarial Training of Neural Networks (DANN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-adaptation method that learns feature representations whose marginal distributions match across domains by using an adversarial domain discriminator, thereby attempting to remove domain-specific signals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Domain-adversarial training of neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Domain-Adversarial Training (DANN)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>DANN trains a feature representation via a minimax objective: a domain discriminator tries to distinguish environments from features while the feature extractor attempts to make them indistinguishable, aiming to learn environment-invariant marginal feature distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Linear unit-tests (six synthetic linear problems)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Mentioned in the challenges: Example1 disallows domain-adversarial methods because the distribution of invariant features changes across environments so matching marginal feature distributions is not appropriate.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Matches marginal feature distributions across environments to remove domain-specific signals (noted as inappropriate in some unit-tests).</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Domain-specific / background signals that change marginal feature distributions across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Adversarial objective seeks to remove features that reveal domain identity, indirectly downweighting domain-specific (possibly spurious) signals.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper argues that domain-adversarial approaches are not suitable for some of the provided unit-tests (Example1) because the distribution of the invariant features themselves changes across environments, hence matching marginals can remove useful invariant signals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Linear unit-tests for invariance discovery', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e732.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e732.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conditional Domain-Adversarial</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conditional Domain-Adversarial Networks (CDAN / conditional domain-adversarial techniques)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Domain-adversarial variants that condition the domain adaptation objective on class/label information to align class-conditional distributions; referenced as not directly applicable to some tasks with continuous targets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep domain generalization via conditional invariant adversarial networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Conditional Domain-Adversarial Techniques</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Conditional domain-adversarial approaches attempt to align class-conditional feature distributions across domains (instead of only marginal alignment), typically by conditioning the domain discriminator on label or classifier outputs; the paper mentions these techniques but notes they are not easily applied when the target variable is continuous.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Linear unit-tests (six synthetic linear problems)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Mentioned in the challenges section: Example1 uses continuous targets, making conditional domain-adversarial approaches difficult to apply.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Aligns class-conditional feature distributions to remove domain-specific spurious correlations conditioned on the label (conceptual description from citation).</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Class-conditional domain-specific spurious features; not suitable for continuous-target tasks per paper.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper highlights that conditional domain-adversarial techniques do not apply easily to tasks with continuous targets (Example1), so they are not evaluated on the unit-tests.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Linear unit-tests for invariance discovery', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e732.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e732.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Oracle</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Oracle baseline (randomized spurious features)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised baseline where training splits contain randomized spurious features (x_spu randomized), making it trivial to ignore spurious features and providing an upper bound on achievable performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Oracle (randomized spurious features ERM)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>A version of ERM where training data have spurious features randomized across examples (i.e., spurious signal removed at training time), allowing evaluation of the best possible performance if spurious features were non-informative.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Linear unit-tests (six synthetic linear problems) with randomized spurious features</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same synthetic tasks but with spurious features randomized in training splits to make spurious cues impossible to exploit; used only as an upper-bound baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Removes/destroys the spurious signal in training by randomizing x_spu, so model must rely on invariant features.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Any spurious/distractor features are randomized and therefore non-predictive.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Not applicable — spurious signals are removed rather than downweighted.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Oracle achieves near-perfect performance on tasks (Example1 Oracle MSE per-env: [0.05, 11.27, 19.93] (mean ≈ 10.42); Example2 and Example3 Oracle classification error = 0 across envs), representing an upper bound.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Oracle evaluates same tasks but with spurious features randomized; default dspu = 5 in other experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Oracle establishes an upper bound: many evaluated algorithms (ERM, IRM, IGA, AND-mask) fail to reach Oracle performance on the proposed unit-tests, highlighting that current methods do not fully discard spurious signals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Linear unit-tests for invariance discovery', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Invariant risk minimization <em>(Rating: 2)</em></li>
                <li>Causal inference using invariant prediction: identification and confidence intervals <em>(Rating: 2)</em></li>
                <li>Learning explanations that are hard to vary <em>(Rating: 2)</em></li>
                <li>Out-of-distribution generalization with maximal invariant predictor <em>(Rating: 2)</em></li>
                <li>Domain-adversarial training of neural networks <em>(Rating: 1)</em></li>
                <li>Deep domain generalization via conditional invariant adversarial networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-732",
    "paper_id": "paper-231986203",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "IRMv1",
            "name_full": "Invariant Risk Minimization (v1)",
            "brief_description": "An invariance-based method that searches for a representation such that the optimal classifier on top of that representation is the same (identity) across all training environments, encouraging use of invariant (causal) features and discouraging reliance on environment-specific (spurious) features.",
            "citation_title": "Invariant risk minimization",
            "mention_or_use": "use",
            "method_name": "Invariant Risk Minimization (IRMv1)",
            "method_description": "IRMv1 learns a feature representation phi(x) and enforces that the optimal classifier w on top of phi is identical across environments by adding an invariance penalty that encourages the environment-wise optimal classifier to be the same (operationalized in the paper as seeking representations where the identity classifier is optimal across envs). This is implemented as a constrained/penalized optimization that balances training error and invariance across environments.",
            "environment_name": "Linear unit-tests (six synthetic linear problems)",
            "environment_description": "A suite of six low-dimensional linear synthetic problems (Example1, Example2, Example3 and their scrambled variants) generated from structural equation models under different interventions; not interactive or active — passive datasets with multiple training environments and held-out test splits where spurious features are randomized.",
            "handles_distractors": true,
            "distractor_handling_technique": "Enforcement of representation-level invariance across environments via an invariance penalty that forces a common optimal classifier, implicitly discouraging use of env-specific (spurious) features.",
            "spurious_signal_types": "Environment-specific predictive features (spurious correlations / distractor variables) that vary across training environments.",
            "detection_method": null,
            "downweighting_method": "Penalizes solutions whose optimal per-environment classifiers differ (in practice enforces a constraint/regularizer on classifier optimality across envs), thereby reducing influence of features that create non-invariant predictors.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "On Example1 (regression) IRMv1 achieves MSE per-environment [0.20, 11.98, 21.27] (mean ≈ 11.15), approaching Oracle; on Example3 it performs worse than ANDMask but sometimes near others (classification errors per env ≈ [0.49,0.49,0.48], mean ≈ 0.487). See Table 1 for full per-environment values.",
            "performance_without_robustness": "Baseline ERM on Example1 yields MSE per-environment [1.62, 14.25, 24.22] (mean ≈ 13.36); on Example3 ERM yields classification errors per-env ≈ [0.48,0.48,0.47] (mean ≈ 0.477).",
            "has_ablation_study": true,
            "number_of_distractors": "Default 5 spurious dimensions (d_spu = 5); experiments also vary number of spurious dimensions in ablations.",
            "key_findings": "IRMv1 sometimes recovers invariant predictors (notably on Example1 and its scrambled variant where it approaches Oracle), but fails to reach Oracle on the other unit-tests; performance is sensitive to problem type (Example2 ERM can outperform IRMv1) and to the number of environments and spurious dimensions. IRMv1 is robust to scrambling in some tasks but not universally successful.",
            "uuid": "e732.0",
            "source_info": {
                "paper_title": "Linear unit-tests for invariance discovery",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "AND-mask",
            "name_full": "AND-mask",
            "brief_description": "A method that updates model parameters only along gradient directions where the sign of the gradient of the loss agrees across most environments, aiming to keep updates that are consistent/invariant across environments and ignore environment-specific gradient signals.",
            "citation_title": "Learning explanations that are hard to vary",
            "mention_or_use": "use",
            "method_name": "AND-mask",
            "method_description": "AND-mask computes per-environment gradients and only applies parameter updates on coordinates whose gradient signs agree across a majority of environments (an 'AND' operation on gradient sign), thereby preventing updates that are driven by environment-specific (spurious) signals and favoring directions supported by multiple environments.",
            "environment_name": "Linear unit-tests (six synthetic linear problems)",
            "environment_description": "Same synthetic suite as described above: passive, low-dimensional linear tasks with multiple training environments generated by structural equation models; not interactive.",
            "handles_distractors": true,
            "distractor_handling_technique": "Gradient-sign agreement selection: prevents updates on parameter directions where environments disagree, effectively masking out coordinates likely induced by spurious/distractor features.",
            "spurious_signal_types": "Environment-specific predictive features producing gradients with inconsistent signs across environments (spurious correlations/shortcut features).",
            "detection_method": "Detects spurious/distractor directions by measuring disagreement in gradient signs across environments (directions with sign disagreement are treated as spurious/unreliable).",
            "downweighting_method": "Zeroes or omits parameter updates on coordinates without sign agreement across environments, thereby downweighting the influence of spurious features in optimization.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "On Example1 AND-mask MSE per-env: [0.11, 11.39, 20.28] (mean ≈ 10.59), close to Oracle; on Example3 it does well (classification mean ≈ 0.343).",
            "performance_without_robustness": "ERM baseline (no AND-mask) on Example1: MSE mean ≈ 13.36; on Example3 ERM mean ≈ 0.477, showing AND-mask improvements in several cases.",
            "has_ablation_study": true,
            "number_of_distractors": "Default 5 spurious dimensions (d_spu = 5); performance evaluated while varying spurious dimensionality and number of environments.",
            "key_findings": "AND-mask can approach Oracle performance on some tasks (Example1) and outperforms ERM on tasks with small invariant margins (Example3), but it collapses on scrambled variants of some problems (e.g., Example3s) and is sensitive to feature representation (scrambling hurts it).",
            "uuid": "e732.1",
            "source_info": {
                "paper_title": "Linear unit-tests for invariance discovery",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "IGA",
            "name_full": "Inter-environmental Gradient Alignment (IGA)",
            "brief_description": "An algorithm that minimizes training error while penalizing variance of the gradient of the loss across environments, aiming to favor parameter updates that generalize across environments by aligning gradients.",
            "citation_title": "Out-of-distribution generalization with maximal invariant predictor",
            "mention_or_use": "use",
            "method_name": "Inter-environmental Gradient Alignment (IGA)",
            "method_description": "IGA adds a regularizer that reduces the variance (or misalignment) of per-environment gradients of the loss, combining standard empirical loss minimization with a term that encourages aligned gradients across environments so that updates are supported by multiple environments rather than environment-specific (spurious) signals.",
            "environment_name": "Linear unit-tests (six synthetic linear problems)",
            "environment_description": "Passive synthetic linear tasks with multiple environments created by structural equation models; not interactive or active experimentation.",
            "handles_distractors": true,
            "distractor_handling_technique": "Gradient-variance regularization: by minimizing gradient variance across environments, the method aims to avoid directions dominated by env-specific (spurious) gradients.",
            "spurious_signal_types": "Environment-specific predictive features that create inconsistent gradients across environments.",
            "detection_method": "Implicit detection via gradient variance/ misalignment: large per-environment gradient variance indicates env-specific/spurious influence.",
            "downweighting_method": "Penalizes gradients with high inter-environment variance, effectively reducing influence of spurious directions in updates.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "IGA generally performs similarly to ERM on most unit-tests; on Example2 it improves substantially as the number of environments increases (performance improves with higher ratio of environments to spurious dims), but it performs poorly on scrambled variants and on Example1 when spurious dims are present. Example1 MSE per-env: [4.47, 18.46, 29.48] (mean ≈ 17.47) which is worse than IRM and AND-mask.",
            "performance_without_robustness": "ERM baseline Example1 MSE mean ≈ 13.36; IGA performs worse than ERM on several tasks unless many environments are provided relative to spurious dimensions.",
            "has_ablation_study": true,
            "number_of_distractors": "Default 5 spurious dimensions; experiments vary number of spurious dims and number of environments to show sensitivity.",
            "key_findings": "IGA does not improve over ERM in most default settings; it only helps when the number of environments is large relative to the number of spurious dimensions (e.g., Example2 with increased n_env), and it is sensitive to scrambling (feature rotations undermine its gains).",
            "uuid": "e732.2",
            "source_info": {
                "paper_title": "Linear unit-tests for invariance discovery",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "ERM",
            "name_full": "Empirical Risk Minimization (ERM)",
            "brief_description": "The standard baseline that minimizes the pooled training error across all environments without any invariance or environment-aware regularization, often absorbing spurious correlations present in training data.",
            "citation_title": "Statistical learning theory",
            "mention_or_use": "use",
            "method_name": "Empirical Risk Minimization (ERM)",
            "method_description": "ERM simply minimizes expected loss on the pooled union of training data across environments (no explicit invariance constraints), and thus will exploit any predictive signals present in training that reduce training loss, including spurious/distractor correlations.",
            "environment_name": "Linear unit-tests (six synthetic linear problems)",
            "environment_description": "Passive simulated linear datasets produced from structural equation models with multiple training envs and a test split where spurious features are randomized to measure reliance on spurious signals.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Does not explicitly address any; will exploit environment-specific spurious correlations when predictive.",
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": "ERM is the non-robust baseline; Example1 MSE per-env: [1.62, 14.25, 24.22] (mean ≈ 13.36), Example2 classification mean ≈ 0.423, Example3 mean ≈ 0.477 (see Table 1).",
            "has_ablation_study": true,
            "number_of_distractors": "Default 5 spurious dimensions; performance evaluated while varying spurious dimensionality.",
            "key_findings": "ERM systematically exploits spurious/distractor features present in training and fails to generalize when test randomized spurious features remove cue; in many unit-tests ERM outperforms some invariance methods on particular tasks, illustrating that invariance methods are not uniformly better.",
            "uuid": "e732.3",
            "source_info": {
                "paper_title": "Linear unit-tests for invariance discovery",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "ICP",
            "name_full": "Invariant Causal Prediction (ICP)",
            "brief_description": "A causal discovery approach that seeks subsets of covariates whose conditional distribution of the target is invariant across environments, aiming to identify causal parents and discard non-invariant predictors.",
            "citation_title": "Causal inference using invariant prediction: identification and confidence intervals",
            "mention_or_use": "mention",
            "method_name": "Invariant Causal Prediction (ICP)",
            "method_description": "ICP searches for sets of variables S such that the conditional distribution of the target given S is invariant across environments; under suitable assumptions this identifies causal parents. The paper mentions ICP as a causality-based algorithm consuming multiple environments to learn invariant relations.",
            "environment_name": "Linear unit-tests (six synthetic linear problems)",
            "environment_description": "Mentioned as a candidate method for invariance discovery; not applied in experiments because some problems (e.g., Example1) violate ICP's assumptions (varying residual distributions across envs).",
            "handles_distractors": null,
            "distractor_handling_technique": "ICP conceptually handles distractors by searching for invariant conditioning sets (variable selection / hypothesis testing for invariance).",
            "spurious_signal_types": "Spurious correlations that induce non-invariant conditional distributions (environment-specific predictors).",
            "detection_method": "Statistical tests for invariance of conditional distributions across environments (not applied here due to violated assumptions in some tasks).",
            "downweighting_method": null,
            "refutation_method": "ICP rejects variable sets that do not yield invariant conditionals, thereby refuting spurious predictors; however, the paper notes ICP is disallowed on Example1 because residual distributions change across environments.",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "ICP is referenced as a causality-based invariance method, but the paper explicitly notes that ICP cannot be used on Example1 because the distribution of residuals varies across environments, illustrating that some invariance/causal methods have assumptions violated by these unit-tests.",
            "uuid": "e732.4",
            "source_info": {
                "paper_title": "Linear unit-tests for invariance discovery",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "Domain-Adversarial (DANN)",
            "name_full": "Domain-Adversarial Training of Neural Networks (DANN)",
            "brief_description": "A domain-adaptation method that learns feature representations whose marginal distributions match across domains by using an adversarial domain discriminator, thereby attempting to remove domain-specific signals.",
            "citation_title": "Domain-adversarial training of neural networks",
            "mention_or_use": "mention",
            "method_name": "Domain-Adversarial Training (DANN)",
            "method_description": "DANN trains a feature representation via a minimax objective: a domain discriminator tries to distinguish environments from features while the feature extractor attempts to make them indistinguishable, aiming to learn environment-invariant marginal feature distributions.",
            "environment_name": "Linear unit-tests (six synthetic linear problems)",
            "environment_description": "Mentioned in the challenges: Example1 disallows domain-adversarial methods because the distribution of invariant features changes across environments so matching marginal feature distributions is not appropriate.",
            "handles_distractors": null,
            "distractor_handling_technique": "Matches marginal feature distributions across environments to remove domain-specific signals (noted as inappropriate in some unit-tests).",
            "spurious_signal_types": "Domain-specific / background signals that change marginal feature distributions across environments.",
            "detection_method": null,
            "downweighting_method": "Adversarial objective seeks to remove features that reveal domain identity, indirectly downweighting domain-specific (possibly spurious) signals.",
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "The paper argues that domain-adversarial approaches are not suitable for some of the provided unit-tests (Example1) because the distribution of the invariant features themselves changes across environments, hence matching marginals can remove useful invariant signals.",
            "uuid": "e732.5",
            "source_info": {
                "paper_title": "Linear unit-tests for invariance discovery",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "Conditional Domain-Adversarial",
            "name_full": "Conditional Domain-Adversarial Networks (CDAN / conditional domain-adversarial techniques)",
            "brief_description": "Domain-adversarial variants that condition the domain adaptation objective on class/label information to align class-conditional distributions; referenced as not directly applicable to some tasks with continuous targets.",
            "citation_title": "Deep domain generalization via conditional invariant adversarial networks",
            "mention_or_use": "mention",
            "method_name": "Conditional Domain-Adversarial Techniques",
            "method_description": "Conditional domain-adversarial approaches attempt to align class-conditional feature distributions across domains (instead of only marginal alignment), typically by conditioning the domain discriminator on label or classifier outputs; the paper mentions these techniques but notes they are not easily applied when the target variable is continuous.",
            "environment_name": "Linear unit-tests (six synthetic linear problems)",
            "environment_description": "Mentioned in the challenges section: Example1 uses continuous targets, making conditional domain-adversarial approaches difficult to apply.",
            "handles_distractors": null,
            "distractor_handling_technique": "Aligns class-conditional feature distributions to remove domain-specific spurious correlations conditioned on the label (conceptual description from citation).",
            "spurious_signal_types": "Class-conditional domain-specific spurious features; not suitable for continuous-target tasks per paper.",
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "The paper highlights that conditional domain-adversarial techniques do not apply easily to tasks with continuous targets (Example1), so they are not evaluated on the unit-tests.",
            "uuid": "e732.6",
            "source_info": {
                "paper_title": "Linear unit-tests for invariance discovery",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "Oracle",
            "name_full": "Oracle baseline (randomized spurious features)",
            "brief_description": "A supervised baseline where training splits contain randomized spurious features (x_spu randomized), making it trivial to ignore spurious features and providing an upper bound on achievable performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Oracle (randomized spurious features ERM)",
            "method_description": "A version of ERM where training data have spurious features randomized across examples (i.e., spurious signal removed at training time), allowing evaluation of the best possible performance if spurious features were non-informative.",
            "environment_name": "Linear unit-tests (six synthetic linear problems) with randomized spurious features",
            "environment_description": "Same synthetic tasks but with spurious features randomized in training splits to make spurious cues impossible to exploit; used only as an upper-bound baseline.",
            "handles_distractors": true,
            "distractor_handling_technique": "Removes/destroys the spurious signal in training by randomizing x_spu, so model must rely on invariant features.",
            "spurious_signal_types": "Any spurious/distractor features are randomized and therefore non-predictive.",
            "detection_method": null,
            "downweighting_method": "Not applicable — spurious signals are removed rather than downweighted.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Oracle achieves near-perfect performance on tasks (Example1 Oracle MSE per-env: [0.05, 11.27, 19.93] (mean ≈ 10.42); Example2 and Example3 Oracle classification error = 0 across envs), representing an upper bound.",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": "Oracle evaluates same tasks but with spurious features randomized; default dspu = 5 in other experiments.",
            "key_findings": "Oracle establishes an upper bound: many evaluated algorithms (ERM, IRM, IGA, AND-mask) fail to reach Oracle performance on the proposed unit-tests, highlighting that current methods do not fully discard spurious signals.",
            "uuid": "e732.7",
            "source_info": {
                "paper_title": "Linear unit-tests for invariance discovery",
                "publication_date_yy_mm": "2021-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Invariant risk minimization",
            "rating": 2,
            "sanitized_title": "invariant_risk_minimization"
        },
        {
            "paper_title": "Causal inference using invariant prediction: identification and confidence intervals",
            "rating": 2,
            "sanitized_title": "causal_inference_using_invariant_prediction_identification_and_confidence_intervals"
        },
        {
            "paper_title": "Learning explanations that are hard to vary",
            "rating": 2,
            "sanitized_title": "learning_explanations_that_are_hard_to_vary"
        },
        {
            "paper_title": "Out-of-distribution generalization with maximal invariant predictor",
            "rating": 2,
            "sanitized_title": "outofdistribution_generalization_with_maximal_invariant_predictor"
        },
        {
            "paper_title": "Domain-adversarial training of neural networks",
            "rating": 1,
            "sanitized_title": "domainadversarial_training_of_neural_networks"
        },
        {
            "paper_title": "Deep domain generalization via conditional invariant adversarial networks",
            "rating": 1,
            "sanitized_title": "deep_domain_generalization_via_conditional_invariant_adversarial_networks"
        }
    ],
    "cost": 0.01677025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Linear unit-tests for invariance discovery</p>
<p>Benjamin Aubin 
Facebook AI Research
Facebook AI Research London
Facebook AI Research
Facebook AI Research
INRIA -PSL Research University Paris
10003Paris, New York, ParisNYFrance, UK, France, USA, France</p>
<p>Agnieszka Słowik 
Facebook AI Research
Facebook AI Research London
Facebook AI Research
Facebook AI Research
INRIA -PSL Research University Paris
10003Paris, New York, ParisNYFrance, UK, France, USA, France</p>
<p>Martin Arjovsky 
Facebook AI Research
Facebook AI Research London
Facebook AI Research
Facebook AI Research
INRIA -PSL Research University Paris
10003Paris, New York, ParisNYFrance, UK, France, USA, France</p>
<p>Leon Bottou 
Facebook AI Research
Facebook AI Research London
Facebook AI Research
Facebook AI Research
INRIA -PSL Research University Paris
10003Paris, New York, ParisNYFrance, UK, France, USA, France</p>
<p>David Lopez-Paz 
Facebook AI Research
Facebook AI Research London
Facebook AI Research
Facebook AI Research
INRIA -PSL Research University Paris
10003Paris, New York, ParisNYFrance, UK, France, USA, France</p>
<p>Linear unit-tests for invariance discovery</p>
<p>There is an increasing interest in algorithms to learn invariant correlations across training environments. A big share of the current proposals find theoretical support in the causality literature but, how useful are they in practice? The purpose of this note is to propose six linear low-dimensional problems -"unit tests"to evaluate different types of out-of-distribution generalization in a precise manner. Following initial experiments, none of the three recently proposed alternatives passes all tests. By providing the code to automatically replicate all the results in this manuscript (https://www.github.com/facebookresearch/ InvarianceUnitTests), we hope that our unit tests become a standard stepping stone for researchers in out-of-distribution generalization.</p>
<p>Introduction</p>
<p>Machine learning systems crumble when deployed in conditions different to those of training [Szegedy et al., 2013, Rosenfeld et al., 2018, Alcorn et al., 2019. To address this issue, recent works in causality [Peters et al., 2015, Arjovsky et al., 2019, Parascandolo et al., 2020 propose to learn correlations invariant across multiple training distributions, and to use those correlations as a proxy for out-ofdistribution generalization. However, as we will show, these algorithms perform poorly across a catalog of simple low-dimensional linear problems. Therefore, our contribution is a standardized set of six "unit tests" that researchers can bear in mind when proposing new solutions for out-of-distribution generalization.</p>
<p>Causal learning algorithms such as Invariant Causal Prediction [Peters et al., 2015, ICP] and Invariant Risk Minimization [Arjovsky et al., 2019, IRM] consume several training datasets, each of them possibly produced by the same "structural equation model" operating under a different "valid interventions". A structural equation model is a list of equations describing how variables influence each other to take their values [Pearl, 2009, Peters et al., 2017]. An intervention perturbs one or more of these equations, and it is "valid" as long as it does not modify the conditional expectation of the target variable given its direct causal parents [Arjovsky et al., 2019]. Then, if the interventions producing our training datasets are diverse, invariant correlations should pertain to the fixed causal mechanism of the target variable. This suggests the possibility of learning about the causal structure of data by searching for statistical invariances. In turn, these invariances enable out-of-distribution generalization: if the causal mechanism of the target variable is invariant across diverse training conditions, we may rely on them to perform robustly at novel test conditions. Unfortunately, invariances are often more difficult to capture than spurious correlations. As an example, consider the task of classifying pictures of cows and camels [Arjovsky et al., 2019]. A "green detector" solves this task to a great extent, since almost all pictures of cows contain green pastures, and almost all pictures of camels show beige sandy landscapes. Following the principle of least effort [Geirhos et al., 2020], learning machines cling to the textural "green-cow" spurious correlation, rather than discovering the shapes that make a cow a cow. Predictors absorbing these "distractor", "shortcut", or "bait" correlations fail when deployed under novel conditions. Although designed to address this very issue, causal learning algorithms often fail to capture causal invariances in data. The purpose of this note is to share six linear problems that illustrate this phenomena. Each of these problems contains an invariant causal correlation (inv) that we would like to learn, as well as a spurious correlation (spu) that we would like to discard. In every dataset, empirical risk minimization absorbs the spurious correlation from the training data, failing to generalize to novel conditions. By releasing the code to automatically replicate all the results in this manuscript, we hope that these "unit tests" become a standard guide when developing new out-of-distribution generalization algorithms.</p>
<p>Problems</p>
<p>For each problem, we collect datasets D e = {(x e i , y e i )} ne i=1 containing n e samples for n env environments e ∈ E = {E j } nenv j=1 . The input feature vector x e = (x e inv , x e spu ) ∈ R d contains features x e inv ∈ R dinv that elicit invariant correlations, as well as features x e spu ∈ R dspu that elicit spurious correlations. Our goal is to construct invariant predictors that estimate the target variable y e by relying on x e inv , while ignoring x e spu . To measure the extent to which an algorithm ignores the features x e spu , we sample a train split, a validation split, and a test split per problem and environment. Both train and validation splits are built by sampling the structural equations outlined below for each problem and environment. The test split is built analogously, but the features x e spu are shuffled at random across examples. This way, only those predictors ignoring the shortcut correlations provided by x e spu will achieve minimal test error. Finally, let N d (µ, σ 2 ) be the d-dimensional Gaussian distribution with mean vector µ and diagonal covariance matrix with diagonal elements σ 2 . We fix n e = 10 4 and we mainly illustrate our results in the default case n env = 3 with (d, d inv , d spu ) = (10, 5, 5).</p>
<p>Example1: regression from causes and effects</p>
<p>A linear least-squares regression problem where features contain causes and effects of the target variable, as proposed by Arjovsky et al. [2019]. By virtue of considering only valid interventions, the mapping from the causes to the target variable is invariant across environments. Conversely, the mapping from the target variable to the effects changes across environments. To construct the datasets D e for every e ∈ E and i = 1, . . . , n e , sample:
x e inv,i ∼ N dinv (0, (σ e ) 2 ), y e i ∼ N dinv (W yx x e inv,i , (σ e ) 2 ), x e spu,i ∼ N dspu (W xyỹ e i , 1), x e i ← (x e inv,i , x e spu,i ), y e i ← 2 d · 1 dinvỹ e i ;
where the matrices W yx ∈ R dinv×dinv , W xy ∈ R dspu×dinv are drawn i.i.d from the Gaussian normal distribution. The first environment variables are fixed to (σ e=E0 ) 2 = 0.1, (σ e=E1 ) 2 = 1.5, and (σ e=E2 ) 2 = 2. For n env &gt; 3 and j ∈ [3 : n env − 1], the extra environments (σ e=Ej ) 2 are drawn uniformly from Unif(10 −2 , 10).</p>
<p>Challenges First, the distribution of the feature x e inv eliciting the invariant correlation changes across environments. This disallows the use of domain-adversarial methods [Ganin et al., 2016], which seek features with matching distribution across environments. Second, the distribution of the residuals varies across environments, disallowing the use of ICP [Peters et al., 2015]. Third, since the target variable is continuous, conditional domain-adversarial techniques [Li et al., 2018] do not apply easily. This example is solved by IRM, and it is analogous to a linear regression variant of the ColorMNIST task [Arjovsky et al., 2019].</p>
<p>Example2: cows versus camels</p>
<p>In the spirit of [Beery et al., 2018, Arjovsky et al., 2019, we add a binary classification problem to imitate the introductory example "most cows appear in grass and most camels appear in sand". Let:
µ cow ∼ 1 dinv , µ camel = −µ cow , ν animal = 10 −2 , µ grass ∼ 1 dspu , µ sand = −µ grass , ν background = 1.
To construct the datasets D e for every e ∈ E and i = 1, . . . , n e , sample:
j e i ∼ Categorical (p e s e , (1 − p e )s e , p e (1 − s e ), (1 − p e )(1 − s e )) ; x e inv,i ∼ (N dinv (0, 10 −1 ) + µ cow ) · ν animal if j e i ∈ {1, 2}, (N dinv (0, 10 −1 ) + µ camel ) · ν animal if j e i ∈ {3, 4}, x e spu,i ∼ (N dspu (0, 10 −1 ) + µ grass ) · ν background if j e i ∈ {1, 4}, (N dspu (0, 10 −1 ) + µ sand ) · ν background if j e i ∈ {2, 3}, x e i ← (x e inv,i , x e spu,i ); y e i ← 1 if 1 dinv x e i,inv &gt; 0, 0 else;
where the environment foreground/background probabilities are p e=E0 = 0.95, p e=E1 = 0.97, p e=E2 = 0.99 and the cow/camel probabilities are s e=E0 = 0.3, s e=E1 = 0.5, s e=E2 = 0.7. For n env &gt; 3 and j ∈ [3 : n env − 1], the extra environment variables are respectively drawn according to p e=Ej ∼ Unif(0.9, 1) and s e=Ej ∼ Unif(0.3, 0.7).</p>
<p>Challenges Achieving zero population error while using only x e inv requires learning large weights. This is difficult when using gradient descent methods, or most forms of regularization. As the dimension of the feature space grows, the probability of achieving zero training error using only x e spu increases rapidly. This means that invariance penalties based on training error, such as IRM, may accept solutions using spurious features.</p>
<p>Example3: small invariant margin</p>
<p>A linear version of the spiral binary classification problem proposed by [Parascandolo et al., 2020], where the first two dimensions offer an invariant, small-margin linear decision boundary. The rest of the dimensions offer a changing, large-margin linear decision boundary. Let γ = 0.1 · 1 dinv , and µ e ∼ N dspu (0, 1), for all environments. To construct the datasets D e for every e ∈ E and i = 1, . . . , n e , sample:
y e i ∼ Bernoulli 1 2 , x e inv,i ∼ N dinv (+γ, 10 −1 ) if y e i = 0, N dinv (−γ, 10 −1 ) if y e i = 1;
x e spu,i ∼ N dspu (+µ e , 10 −1 ) if y e i = 0, N dspu (−µ e , 10 −1 ) if y e i = 1;</p>
<p>x e i ← (x e inv,i , x e spu,i ).</p>
<p>Challenges We can solve this problem to zero population error with high probability using x e spu alone. Things complicate further, as solving this task using x e inv forcefully incurs a small amount of population error. Therefore, learning algorithms should learn to sacrifice training error to realize that x e inv lead to the same maximum margin classifier across environments (even though the varying margin based on x e spu is larger!). While the predictor based on x e inv is the optimal in terms of worst-case out-of-distribution generalization, it is not a causal predictor of the target variable.</p>
<p>Scrambled variations</p>
<p>We define three additional problems: example1s, example2s, and example3s. These are "scrambled" variations of the three problems described above, respectively. Scrambled variations build observed datasets D e = {(S x e i , y e i )} n e i=1 , where S ∈ R d×d is a random rotation matrix fixed for all environments e ∈ E. Observing a scrambled version of the variables appearing in the structural equation models requires algorithms to learn a (linear) feature representation under which the desired invariance should be elicited. Because of this reason, we do not compare to brute-force feature selection methods, such as ICP [Peters et al., 2015].  </p>
<p>Experiments</p>
<p>We provide an initial set of experiments evaluating the following algorithms on our six problems: Empirical Risk Minimization [Vapnik, 1998, ERM] minimizes the error on the union of all the training splits. Invariant Risk Minimization [Arjovsky et al., 2019, IRMv1] finds a representation of the features such that the optimal classifier, on top of that representation, is the identity function for all environments. Inter-environmental Gradient Alignment [Koyama and Yamaguchi, 2020, IGA] minimizes the error on the training splits while reducing the variance of the gradient of the loss per environment. AND-mask [Parascandolo et al., 2020] minimizes the error on the training splits by updating the model on those directions where the sign of the gradient of the loss is the same for most environments. Oracle is a version of ERM where all data splits contain randomized x e spu , and therefore are trivial to ignore. The purpose of this method is to understand the achievable upper bound performance in our problems.</p>
<p>For each algorithm, we run a random hyper-parameter search of 20 trials. We trained each algorithm and hyper-parameter trial on the train splits of all environments, for 10 4 full-batch Adam [Kingma and Ba, 2015] updates. We choose the hyper-parameters trial that minimizes the error on the validation splits of all environments. Finally, we report the error of these selected models on the test splits. To provide error bars, this entire process, including data sampling, is repeated 50 times. We refer the reader to our code to learn about the hyper-parameter search distributions for each algorithm. Table 1 shows that no method is able to achieve a performance close to the Oracle on any of the proposed problems. The only exception are IRMv1 and ANDMask on example1. This illustrates that current causal learning algorithms are unable to capture invariances, even in low-dimensional linear problems. The results averaged over the different environments are plotted in Fig. 1. </p>
<p>Default results</p>
<p>Varying the number of environments</p>
<p>What is the role of the number of environments n env on generalization? We define the ratio δ env = nenv dspu between the number of environments and the number of spurious dimensions. We run experiments with the same procedure described above for n env ∈ [2 : 10], and a fixed number of spurious dimensions d spu = d inv = 5. Figure 2 (top) show average test errors for all algorithms. Notably, IGA performs no better than ERM, except on Example2 where it improves drastically when increasing δ env ; however, this increase bears no effect on the scrambled version Example2s. On Example1 and Example1s, both ANDMask and IRMv1 approach closely Oracle's performances, while on Example2 and Example2s simple ERM outperforms them. On the contrary, ANDMask and IRMv1 achieve good performances on Example3. They approach optimality for n env d spu + 1, since in this case no invariant boundary can solve the problem using x spu alone. IRMv1 performances do not suffer due to scrambling, while ANDMask collapses on Example3s.</p>
<p>Varying the number of spurious dimensions</p>
<p>We perform another ablation by fixing the number of environments n env = 3 and the number of invariant dimensions d inv = 5, while varying the number of spurious dimensions δ spu = dspu dinv . We observe that for Example1 and Example1s, ANDMask and IRMv1 do not suffer when adding spurious dimensions, while IGA crumbles as soon as a single spurious feature is added. As expected, on Example3(s) and Example3s, increasing the number of spurious dimensions while keeping the number of environments fixed decreases the performance of all algorithms. Example2 and Example2s show the same phenomena for δ spu ≤ 1.</p>
<p>Outlook</p>
<p>We propose a battery of "unit-tests" to surgically evaluate different types of out-of-distribution generalization abilities of machine learning algorithms. While admittedly synthetic, our collection of problems attempts to cover a wide range of challenging distributional discrepancies that may arise across training and testing conditions. We invite researchers to use and extend this set of problems to learn about the strengths and shortcomings of new algorithms in a transparent and standardized manner.</p>
<p>Figure 1 :
1Test error averaged across environments (E0, E1, E2) for (d inv , d spu , n env ) = (5, 5, 3).</p>
<p>Figure 2 :
2Test error averaged across environments for ANDMask, ERM, IGA, IRMv1 and Oracle on the unit-tests as (top) a function of the ratio δ env = n env /d spu at fixed dimensions (d inv , d spu ) = (5, 5); and as (bottom) a function of δ spu = d spu /d inv for (d inv , n env ) = (5, 3).</p>
<p>Table 1 :
1Test errors for all algorithms, datasets, and environments for (d inv , d spu , n env ) = (5, 5, 3). Example 1 and Example 1s errors are in MSE; all others are classification errors.ANDMask 
ERM 
IGA 
IRMv1 
Oracle </p>
<p>Example1.E0 
0.11 ± 0.04 
1.62 ± 0.60 
4.47 ± 1.16 
0.20 ± 0.04 
0.05 ± 0.00 
Example1.E1 
11.39 ± 0.18 14.25 ± 1.52 18.46 ± 2.14 11.98 ± 0.75 11.27 ± 0.17 
Example1.E2 
20.28 ± 0.30 24.22 ± 2.34 29.48 ± 3.19 21.27 ± 1.34 19.93 ± 0.31 
Example1s.E0 0.07 ± 0.01 
1.61 ± 0.59 
4.55 ± 1.79 
0.19 ± 0.04 
0.05 ± 0.00 
Example1s.E1 12.13 ± 0.80 14.23 ± 1.49 18.68 ± 3.37 11.92 ± 0.69 11.24 ± 0.19 
Example1s.E2 21.52 ± 1.42 24.14 ± 2.39 29.81 ± 4.78 21.08 ± 1.31 20.06 ± 0.37 </p>
<p>Example2.E0 
0.42 ± 0.02 
0.40 ± 0.01 
0.43 ± 0.00 
0.43 ± 0.00 
0.00 ± 0.00 
Example2.E1 
0.49 ± 0.03 
0.47 ± 0.01 
0.50 ± 0.00 
0.50 ± 0.00 
0.00 ± 0.00 
Example2.E2 
0.42 ± 0.02 
0.40 ± 0.01 
0.42 ± 0.01 
0.42 ± 0.01 
0.00 ± 0.00 
Example2s.E0 0.43 ± 0.01 
0.43 ± 0.01 
0.43 ± 0.01 
0.43 ± 0.01 
0.00 ± 0.00 
Example2s.E1 0.50 ± 0.00 
0.50 ± 0.00 
0.50 ± 0.00 
0.50 ± 0.00 
0.00 ± 0.00 
Example2s.E2 0.42 ± 0.01 
0.42 ± 0.01 
0.42 ± 0.01 
0.42 ± 0.01 
0.00 ± 0.00 </p>
<p>Example3.E0 
0.35 ± 0.22 
0.48 ± 0.09 
0.47 ± 0.10 
0.49 ± 0.07 
0.00 ± 0.00 
Example3.E1 
0.36 ± 0.22 
0.48 ± 0.07 
0.48 ± 0.08 
0.49 ± 0.06 
0.00 ± 0.00 
Example3.E2 
0.32 ± 0.22 
0.47 ± 0.12 
0.46 ± 0.12 
0.48 ± 0.07 
0.00 ± 0.00 
Example3s.E0 0.45 ± 0.13 
0.48 ± 0.08 
0.48 ± 0.09 
0.49 ± 0.07 
0.00 ± 0.00 
Example3s.E1 0.49 ± 0.05 
0.49 ± 0.05 
0.48 ± 0.07 
0.49 ± 0.06 
0.00 ± 0.00 
Example3s.E2 0.46 ± 0.12 
0.47 ± 0.09 
0.47 ± 0.11 
0.48 ± 0.07 
0.00 ± 0.00 </p>
<p>0 1 2 3 4 
0 </p>
<p>10 </p>
<p>20 </p>
<p>Test error </p>
<p>Strike (with) a pose: Neural networks are easily fooled by strange poses of familiar objects. Qi Michael A Alcorn, Zhitao Li, Chengfei Gong, Long Wang, Wei-Shinn Mai, Anh Ku, Nguyen, CVPRMichael A Alcorn, Qi Li, Zhitao Gong, Chengfei Wang, Long Mai, Wei-Shinn Ku, and Anh Nguyen. Strike (with) a pose: Neural networks are easily fooled by strange poses of familiar objects. CVPR, 2019.</p>
<p>Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, David Lopez-Paz, arXiv:1907.02893Invariant risk minimization. arXiv preprintMartin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.</p>
<p>Recognition in terra incognita. Sara Beery, Grant Van Horn, Pietro Perona, ECCVSara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. ECCV, 2018.</p>
<p>Domain-adversarial training of neural networks. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, Victor Lempitsky, JMLRYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. JMLR, 2016.</p>
<p>Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, Felix A Wichmann, arXiv:2004.07780Shortcut learning in deep neural networks. arXiv preprintRobert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. arXiv preprint arXiv:2004.07780, 2020.</p>
<p>Adam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, ICLRDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2015.</p>
<p>Out-of-distribution generalization with maximal invariant predictor. Masanori Koyama, Shoichiro Yamaguchi, arXiv:2008.01883arXiv preprintMasanori Koyama and Shoichiro Yamaguchi. Out-of-distribution generalization with maximal invariant predictor. arXiv preprint arXiv:2008.01883, 2020.</p>
<p>Deep domain generalization via conditional invariant adversarial networks. Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, Dacheng Tao, ECCVYa Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generalization via conditional invariant adversarial networks. ECCV, 2018.</p>
<p>Learning explanations that are hard to vary. Giambattista Parascandolo, Alexander Neitz, Antonio Orvieto, Luigi Gresele, Bernhard Schölkopf, arXiv:2009.00329Judea Pearl. Causality. Cambridge university press2020arXiv preprintGiambattista Parascandolo, Alexander Neitz, Antonio Orvieto, Luigi Gresele, and Bernhard Schölkopf. Learning explanations that are hard to vary. arXiv preprint arXiv:2009.00329, 2020. Judea Pearl. Causality. Cambridge university press, 2009.</p>
<p>Causal inference using invariant prediction: identification and confidence intervals. Jonas Peters, Peter Bühlmann, Nicolai Meinshausen, arXiv:1501.01332arXiv preprintJonas Peters, Peter Bühlmann, and Nicolai Meinshausen. Causal inference using invariant prediction: identification and confidence intervals. arXiv preprint arXiv:1501.01332, 2015.</p>
<p>Elements of causal inference. Jonas Peters, Dominik Janzing, Bernhard Schölkopf, Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference. 2017.</p>
<p>Amir Rosenfeld, Richard Zemel, John K Tsotsos, arXiv:1808.03305The elephant in the room. arXiv preprintAmir Rosenfeld, Richard Zemel, and John K Tsotsos. The elephant in the room. arXiv preprint arXiv:1808.03305, 2018.</p>
<p>Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, arXiv:1312.6199Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprintChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.</p>
<p>Statistical learning theory wiley. Vladimir Vapnik, New YorkVladimir Vapnik. Statistical learning theory wiley. New York, 1998.</p>            </div>
        </div>

    </div>
</body>
</html>
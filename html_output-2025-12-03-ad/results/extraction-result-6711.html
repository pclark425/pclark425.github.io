<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6711 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6711</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6711</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-1190475124f300d53612d427cd5d243337b2a11c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1190475124f300d53612d427cd5d243337b2a11c" target="_blank">Strategic Chain-of-Thought: Guiding Accurate Reasoning in LLMs through Strategy Elicitation</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The SCoT framework is extended to develop a few-shot method with automatically matched demonstrations, yielding even stronger results, underscore the efficacy of SCoT, highlighting its potential to substantially enhance LLM performance in complex reasoning tasks.</p>
                <p><strong>Paper Abstract:</strong> The Chain-of-Thought (CoT) paradigm has emerged as a critical approach for enhancing the reasoning capabilities of large language models (LLMs). However, despite their widespread adoption and success, CoT methods often exhibit instability due to their inability to consistently ensure the quality of generated reasoning paths, leading to sub-optimal reasoning performance. To address this challenge, we propose the \textbf{Strategic Chain-of-Thought} (SCoT), a novel methodology designed to refine LLM performance by integrating strategic knowledge prior to generating intermediate reasoning steps. SCoT employs a two-stage approach within a single prompt: first eliciting an effective problem-solving strategy, which is then used to guide the generation of high-quality CoT paths and final answers. Our experiments across eight challenging reasoning datasets demonstrate significant improvements, including a 21.05\% increase on the GSM8K dataset and 24.13\% on the Tracking\_Objects dataset, respectively, using the Llama3-8b model. Additionally, we extend the SCoT framework to develop a few-shot method with automatically matched demonstrations, yielding even stronger results. These findings underscore the efficacy of SCoT, highlighting its potential to substantially enhance LLM performance in complex reasoning tasks.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6711.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6711.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Strategic Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage single-query prompting method that first elicits an explicit problem-solving strategy (strategic knowledge) and then applies that strategy to generate a guided Chain-of-Thought and final answer; supports zero-shot and a few-shot variant with strategy-matched demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Strategic Chain-of-Thought (SCoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>grade-school math word problems requiring a single numeric answer</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>73.16</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Chain-of-Thought (CoT 0-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>21.05</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors attribute the gain to eliciting a concise, stable strategy (strategic knowledge) before reasoning, which reduces unstable or complex intermediate steps common in unconstrained CoT outputs; SCoT is single-query (lower query/resource overhead than multi-query ensemble methods) and often yields more stable CoT paths.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Strategic Chain-of-Thought: Guiding Accurate Reasoning in LLMs through Strategy Elicitation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6711.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6711.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Strategic Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same SCoT approach (strategy elicitation then guided CoT) evaluated on a spatial/object-tracking benchmark, showing large gains where strategy selection reduces misdirected local reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Strategic Chain-of-Thought (SCoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Tracking_Object (BIG-bench tracking_shuffled_objects)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>spatial/object-tracking multi-step reasoning (which object ends where after swaps/movements)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>68.4</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Chain-of-Thought (CoT 0-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>24.13</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Large improvements observed because SCoT forces a clear workflow (track initial state, update per exchange, determine final state), avoiding partial/local reasoning mistakes; authors note SCoT yields substantial gains in commonsense/spatial reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Strategic Chain-of-Thought: Guiding Accurate Reasoning in LLMs through Strategy Elicitation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6711.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6711.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Strategic Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SCoT applied to a smaller open model, showing modest gains on math benchmark but still outperforming vanilla CoT in zero-shot for Mistral-7B.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Strategic Chain-of-Thought (SCoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>grade-school math word problems requiring a single numeric answer</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>38.97</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Chain-of-Thought (CoT 0-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>2.71</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Smaller models still benefit from strategy elicitation though gains are smaller than for some larger models; authors report average improvements but also note model-dependent effects (improvement magnitude tends to decrease with increasing model size because larger models more often generate strategic knowledge unprompted).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Strategic Chain-of-Thought: Guiding Accurate Reasoning in LLMs through Strategy Elicitation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6711.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6711.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Con</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (voting over diverse CoT paths)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble-like method that generates many diverse Chain-of-Thought reasoning paths and aggregates (votes) on the most consistent answer to improve accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ARC (ARC_Challenge)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>scientific/physical reasoning multiple-choice questions (ARC Challenge)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>81.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>SCoT 0-shot</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>2.98</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Self-Consistency improves accuracy by sampling many diverse CoT paths and majority-voting; the paper notes Self-Consistency sometimes outperforms SCoT (e.g., ARC with Llama3-8B), suggesting that generating diverse alternative reasoning traces plus aggregation can be preferable for some tasks, at the cost of many queries and higher compute.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Strategic Chain-of-Thought: Guiding Accurate Reasoning in LLMs through Strategy Elicitation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6711.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6711.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCoT-agg</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SCoT (aggregated result summary)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregate observations about SCoT performance/behavior across models and datasets reported in this paper, including zero-shot and few-shot (strategy-matched) variants and analysis of efficiency and ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various (Llama3-8B, Mistral-7B, other Llama/Llama3/Qwen/ChatGLM models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B/8B/70B (various)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Strategic Chain-of-Thought (SCoT) (zero-shot and few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style (strategy-guided)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MathQA, AQuA, GSM8K, MMLU, ARC, StrategyQA, CommonsenseQA, Tracking_Object</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>mathematical, physical, commonsense, multi-hop, and spatial reasoning benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%) or average accuracy improvement (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>6.92</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Chain-of-Thought (CoT 0-shot) (aggregate baseline for Llama3-8B)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>6.92</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Across datasets the paper reports average improvement for Llama3-8B of 6.92% and for Mistral-7B of 3.81%; authors observe (1) SCoT often outperforms single-shot CoT and some multi-query baselines while using only one query, (2) few-shot SCoT with strategy-matched demonstrations yields further gains, (3) ablations show prompt components (role, workflow, markdown formatting, number of demonstrations) affect performance, and (4) larger models more often produce strategies without prompting, so marginal gains from SCoT can decrease with model size. Efficiency trade-off: SCoT increases token output length (avg ~1.5x) and so can be slower than CoT but is cheaper than multi-query ensemble methods.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Strategic Chain-of-Thought: Guiding Accurate Reasoning in LLMs through Strategy Elicitation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Take a step back: Evoking reasoning via abstraction in large language models <em>(Rating: 2)</em></li>
                <li>Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models <em>(Rating: 2)</em></li>
                <li>Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge Perspectives <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6711",
    "paper_id": "paper-1190475124f300d53612d427cd5d243337b2a11c",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "SCoT",
            "name_full": "Strategic Chain-of-Thought",
            "brief_description": "A two-stage single-query prompting method that first elicits an explicit problem-solving strategy (strategic knowledge) and then applies that strategy to generate a guided Chain-of-Thought and final answer; supports zero-shot and a few-shot variant with strategy-matched demonstrations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama3-8B",
            "model_size": "8B",
            "reasoning_method_name": "Strategic Chain-of-Thought (SCoT)",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "GSM8K",
            "task_description": "grade-school math word problems requiring a single numeric answer",
            "performance_metric": "accuracy (%)",
            "performance_value": 73.16,
            "comparison_target_method": "Chain-of-Thought (CoT 0-shot)",
            "performance_difference": 21.05,
            "statistical_significance": false,
            "analysis_notes": "Authors attribute the gain to eliciting a concise, stable strategy (strategic knowledge) before reasoning, which reduces unstable or complex intermediate steps common in unconstrained CoT outputs; SCoT is single-query (lower query/resource overhead than multi-query ensemble methods) and often yields more stable CoT paths.",
            "ablation_study_present": true,
            "uuid": "e6711.0",
            "source_info": {
                "paper_title": "Strategic Chain-of-Thought: Guiding Accurate Reasoning in LLMs through Strategy Elicitation",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "SCoT",
            "name_full": "Strategic Chain-of-Thought",
            "brief_description": "Same SCoT approach (strategy elicitation then guided CoT) evaluated on a spatial/object-tracking benchmark, showing large gains where strategy selection reduces misdirected local reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama3-8B",
            "model_size": "8B",
            "reasoning_method_name": "Strategic Chain-of-Thought (SCoT)",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "Tracking_Object (BIG-bench tracking_shuffled_objects)",
            "task_description": "spatial/object-tracking multi-step reasoning (which object ends where after swaps/movements)",
            "performance_metric": "accuracy (%)",
            "performance_value": 68.4,
            "comparison_target_method": "Chain-of-Thought (CoT 0-shot)",
            "performance_difference": 24.13,
            "statistical_significance": false,
            "analysis_notes": "Large improvements observed because SCoT forces a clear workflow (track initial state, update per exchange, determine final state), avoiding partial/local reasoning mistakes; authors note SCoT yields substantial gains in commonsense/spatial reasoning tasks.",
            "ablation_study_present": true,
            "uuid": "e6711.1",
            "source_info": {
                "paper_title": "Strategic Chain-of-Thought: Guiding Accurate Reasoning in LLMs through Strategy Elicitation",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "SCoT",
            "name_full": "Strategic Chain-of-Thought",
            "brief_description": "SCoT applied to a smaller open model, showing modest gains on math benchmark but still outperforming vanilla CoT in zero-shot for Mistral-7B.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B",
            "model_size": "7B",
            "reasoning_method_name": "Strategic Chain-of-Thought (SCoT)",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "GSM8K",
            "task_description": "grade-school math word problems requiring a single numeric answer",
            "performance_metric": "accuracy (%)",
            "performance_value": 38.97,
            "comparison_target_method": "Chain-of-Thought (CoT 0-shot)",
            "performance_difference": 2.71,
            "statistical_significance": false,
            "analysis_notes": "Smaller models still benefit from strategy elicitation though gains are smaller than for some larger models; authors report average improvements but also note model-dependent effects (improvement magnitude tends to decrease with increasing model size because larger models more often generate strategic knowledge unprompted).",
            "ablation_study_present": true,
            "uuid": "e6711.2",
            "source_info": {
                "paper_title": "Strategic Chain-of-Thought: Guiding Accurate Reasoning in LLMs through Strategy Elicitation",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Self-Con",
            "name_full": "Self-Consistency (voting over diverse CoT paths)",
            "brief_description": "An ensemble-like method that generates many diverse Chain-of-Thought reasoning paths and aggregates (votes) on the most consistent answer to improve accuracy.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama3-8B",
            "model_size": "8B",
            "reasoning_method_name": "Self-Consistency",
            "reasoning_method_type": "ensemble",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "ARC (ARC_Challenge)",
            "task_description": "scientific/physical reasoning multiple-choice questions (ARC Challenge)",
            "performance_metric": "accuracy (%)",
            "performance_value": 81.0,
            "comparison_target_method": "SCoT 0-shot",
            "performance_difference": 2.98,
            "statistical_significance": false,
            "analysis_notes": "Self-Consistency improves accuracy by sampling many diverse CoT paths and majority-voting; the paper notes Self-Consistency sometimes outperforms SCoT (e.g., ARC with Llama3-8B), suggesting that generating diverse alternative reasoning traces plus aggregation can be preferable for some tasks, at the cost of many queries and higher compute.",
            "ablation_study_present": null,
            "uuid": "e6711.3",
            "source_info": {
                "paper_title": "Strategic Chain-of-Thought: Guiding Accurate Reasoning in LLMs through Strategy Elicitation",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "SCoT-agg",
            "name_full": "SCoT (aggregated result summary)",
            "brief_description": "Aggregate observations about SCoT performance/behavior across models and datasets reported in this paper, including zero-shot and few-shot (strategy-matched) variants and analysis of efficiency and ablations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "various (Llama3-8B, Mistral-7B, other Llama/Llama3/Qwen/ChatGLM models)",
            "model_size": "7B/8B/70B (various)",
            "reasoning_method_name": "Strategic Chain-of-Thought (SCoT) (zero-shot and few-shot)",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "single style (strategy-guided)",
            "benchmark_name": "MathQA, AQuA, GSM8K, MMLU, ARC, StrategyQA, CommonsenseQA, Tracking_Object",
            "task_description": "mathematical, physical, commonsense, multi-hop, and spatial reasoning benchmarks",
            "performance_metric": "accuracy (%) or average accuracy improvement (%)",
            "performance_value": 6.92,
            "comparison_target_method": "Chain-of-Thought (CoT 0-shot) (aggregate baseline for Llama3-8B)",
            "performance_difference": 6.92,
            "statistical_significance": false,
            "analysis_notes": "Across datasets the paper reports average improvement for Llama3-8B of 6.92% and for Mistral-7B of 3.81%; authors observe (1) SCoT often outperforms single-shot CoT and some multi-query baselines while using only one query, (2) few-shot SCoT with strategy-matched demonstrations yields further gains, (3) ablations show prompt components (role, workflow, markdown formatting, number of demonstrations) affect performance, and (4) larger models more often produce strategies without prompting, so marginal gains from SCoT can decrease with model size. Efficiency trade-off: SCoT increases token output length (avg ~1.5x) and so can be slower than CoT but is cheaper than multi-query ensemble methods.",
            "ablation_study_present": true,
            "uuid": "e6711.4",
            "source_info": {
                "paper_title": "Strategic Chain-of-Thought: Guiding Accurate Reasoning in LLMs through Strategy Elicitation",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2
        },
        {
            "paper_title": "Take a step back: Evoking reasoning via abstraction in large language models",
            "rating": 2
        },
        {
            "paper_title": "Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge Perspectives",
            "rating": 1
        }
    ],
    "cost": 0.0134015,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Strategic Chain-of-Thought: Guiding Accurate Reasoning in LLMs through Strategy Elicitation</h1>
<p>Yu Wang^{1,2}, Shiwan Zhao^{3,4}, Zhihu Wang^{1}, Heyuan Huang^{1}, Ming Fan^{2}, Yubo Zhang^{1},
Zhixing Wang^{1}, Haijun Wang^{2}, Ting Liu^{2}
^{1}Huawei Technologies Ltd.
^{2}Xi’an Jiaotong University
^{3}Nankai University
^{4}Shanghai Jiao Tong University</p>
<h6>Abstract</h6>
<p>The Chain-of-Thought (CoT) paradigm has emerged as a critical approach for enhancing the reasoning capabilities of large language models (LLMs). However, despite their widespread adoption and success, CoT methods often exhibit instability due to their inability to consistently ensure the quality of generated reasoning paths, leading to suboptimal reasoning performance. To address this challenge, we propose the Strategic Chain-of-Thought (SCoT), a novel methodology designed to refine LLM performance by integrating strategic knowledge prior to generating intermediate reasoning steps. SCoT employs a two-stage approach within a single prompt: first eliciting an effective problem-solving strategy, which is then used to guide the generation of high-quality CoT paths and final answers. Our experiments across eight challenging reasoning datasets demonstrate significant improvements, including a 21.05% increase on the GSM8K dataset and 24.13% on the Tracking_Objects dataset, respectively, using the Llama3-8b model. Additionally, we extend the SCoT framework to develop a few-shot method with automatically matched demonstrations, yielding even stronger results. These findings underscore the efficacy of SCoT, highlighting its potential to substantially enhance LLM performance in complex reasoning tasks.</p>
<h2>Introduction</h2>
<p>The rapid development of large language models (LLMs) has highlighted their remarkable effectiveness in reasoning tasks (Huang and Chang 2022; Chang et al. 2024), particularly when integrated with various prompting techniques (Sivarajkumar et al. 2023). These techniques consistently enable impressive performance across diverse domains. Among them, the Chain-of-Thought (CoT) paradigm has played a pivotal role in enhancing the reasoning capabilities of LLMs (Kojima et al. 2022; Zhang et al. 2022; Wang et al. 2023). As a result, CoT has become a fundamental component of contemporary LLMs and is now widely adopted in the field of natural language processing.</p>
<p>Despite the demonstrated effectiveness of the CoT approach in various applications, it faces significant challenges in complex reasoning tasks. These challenges primarily arise from the variability in the quality of the reasoning paths generated by the CoT method (Wang et al. 2022), which are not consistently optimal. Consequently, even when LLMs produce a CoT path that aligns with a valid reasoning process,</p>
<p>Figure 1: Comparison of some popular methods with SCoT: As a single-query method, SCoT is efficient and does not rely on external knowledge sources, distinguishing it from other approaches.</p>
<p>there remains a risk that the final outcome may be erroneous.</p>
<p>This phenomenon is analogous to findings in cognitive science, where different problem-solving strategies, although correct, can vary in their likelihood of producing errors. According to Sweller’s Cognitive Load Theory (Sweller 1988), different problem-solving strategies impose varying levels of cognitive load, leading to different probabilities of error.</p>
<p>This variability in error probability, influenced by the undetermined strategies used to generate CoT paths, can undermine the reliability of the CoT approach in critical applications where precise and reliable reasoning is essential. Therefore, further refinement and improvement of the CoT methodology are necessary to enhance its performance in complex reasoning scenarios, drawing on insights from both artificial intelligence and cognitive science.</p>
<p>Various methods have been developed to address this challenge by enhancing the quality of CoT paths in LLMs, as illustrated in Figure 1. Among these methods, voting-based approaches enhance reasoning accuracy by generating diverse reasoning paths and then voting on the most reliable</p>
<p>and correct answer (Wang et al. 2022; Zhang et al. 2023). Retrieval-Augmented Generation (RAG)-based approaches introduce external sources to access additional knowledge through multi-step prompting strategies (Lewis et al. 2021; Yang et al. 2024b; Zheng et al. 2023). These approaches improve the reasoning process by systematically incorporating and aligning external information before arriving at the final result. Additionally, Suzgun and Kalai (2024) have integrated various prompt enhancement algorithms, dynamically selecting the optimal one to produce the most accurate results during actual operation.</p>
<p>These approaches do help mitigate the variability in path quality; however, they often come with significant resource demands. For instance, methods like SelfConsistency (Wang et al. 2022) may require up to 40 queries, while techniques such as BoT (Yang et al. 2024b) involve multi-stage queries. Additionally, some approaches may necessitate the integration of external knowledge sources to achieve optimal performance, which places high demands on expert resources.</p>
<p>To tackle this challenge, we propose a novel approach called Strategic Chain-of-Thought (SCoT). SCoT is designed to improve the quality of CoT path generation for reasoning tasks by incorporating strategic knowledge. The method involves a two-step process within a single prompt. First, it explores and identifies various problem-solving strategies, eliciting the most effective one as the guiding strategic knowledge. Subsequently, this strategic knowledge directs the model in generating high-quality CoT paths and producing accurate final answers, ensuring a more effective reasoning process. We further extend the SCoT framework by adapting it to a few-shot method. In this approach, strategic knowledge is used to automatically select the most relevant demonstrations. These examples can be employed within both the few-shot and SCoT frameworks to further enhance reasoning capability. SCoT enhances the model’s reasoning capabilities without the need for multi-query approaches or additional knowledge sources. By eliminating the requirement for multiple queries and external knowledge integration, SCoT reduces computational overhead and operational costs, making it a more practical and resource-efficient solution.</p>
<p>The concept of strategic knowledge in our approach is also inspired by the recent Re-TASK framework (Wang et al. 2024), which revisits LLM tasks from the perspectives of capability, skill, and knowledge. While Re-TASK enhances LLM capabilities through knowledge injection and skill adaptation via capability items, SCoT takes a different approach by eliciting knowledge rather than relying on explicit knowledge injection. Furthermore, the demonstrations based on strategic knowledge in SCoT are analogous to the capability items in Re-TASK.</p>
<p>We conducted experiments across eight reasoning datasets spanning five distinct domains: mathematical reasoning, commonsense reasoning, physical reasoning, spatial reasoning, and multi-hop reasoning. The results revealed substantial improvements across various models, including a 21.05% increase in accuracy on the GSM8K dataset and a 24.13% increase on the Tracking_Objects dataset with the Llama3-8b model. These results validate the effectiveness of the SCoT approach.</p>
<p>The contributions of this work are summarized as follows:</p>
<ul>
<li>We introduce a two-stage methodology that integrates strategic knowledge, guiding the LLM to generate high-quality CoT paths by first developing a problem-solving strategy and then producing the final answer.</li>
<li>We propose a method that leverages strategic knowledge to select and match relevant demonstrations, enabling the precise pairing of high-quality CoT examples.</li>
<li>Our experimental results validate the effectiveness of SCoT, demonstrating promising outcomes in reasoning tasks across multiple domains.</li>
</ul>
<h2>Related Work</h2>
<h3>Strategic Diversity in Problem Solving</h3>
<p>In the realm of problem-solving, there is rarely a one-size-fits-all approach. The complexity of each problem often necessitate a variety of strategies to reach an effective solution. In the fields of education and cognitive science, the phenomenon of using multiple approaches to solve problems is quite common (Sweller 1988; Rusczyk 2003). Similarly, researchers have found that LLMs might generate diverse solution paths for one question, where the problem-solving strategies and answers of these methods might vary significantly (Wang and Zhou 2024; Wang et al. 2022).</p>
<h3>Enhancement of CoT Path</h3>
<p>Current methods for enhancing the quality of model-generated content are diverse and sophisticated.</p>
<p>Some approaches utilize a voting-based mechanism. For example, Wang et al. (2022) introduced the Self-Consistency method, which improves reasoning accuracy by first generating more than 20 CoT paths and then voting for the most consistent answer. Other methods incorporate external sources. Zheng et al. (2023) introduced Step Back, which prompts models to generate an abstract of the question to capture deeper logical structures, thereby enhancing retrieval-augmented generation (RAG) capabilities. Similarly, Yang et al. (2024b) developed another RAG-based method, Buffer of Thoughts, which uses knowledge extracted from external sources and predefined knowledge categories for each task. These elements are integrated into a predefined task prompt template, enabling the model to generate more accurate answers. Additionally, some methods introduce external tools to aid problem-solving. Gao et al. (2023) proposed PAL, which leverages large language models to parse problems and generate programs as intermediate reasoning steps, delegating the solution to a runtime environment like a Python interpreter. This neural-symbolic collaboration has demonstrated improved accuracy across various tasks. Suzgun and Kalai (2024) introduced metaprompting, which integrates existing prompt-based frameworks, enabling dynamic selection of the most effective reasoning strategy. These strategies, with their complex templates and multi-stage prompting, provide models with sophisticated tools for advancing CoT generation in LLMs.</p>
<p>These methods are inherently complex, with some being task-sensitive and others involving multi-turn prompting; however, they have demonstrated substantial efficacy in enhancing the reasoning capabilities of LLMs, thereby advancing the frontiers of CoT generation in machine learning.</p>
<h2>Method</h2>
<p>In this section, we introduce the strategic knowledge, the Strategic Chain-of-Thought (SCoT) method, and its extension through the few-shot approach.</p>
<h3>Strategic Knowledge</h3>
<p>LLMs tend to produce varied CoT paths for the same problem. However, the quality of these CoT paths can vary significantly <em>Wang and Zhou (2024); Wang et al. (2022)</em>. As shown in the left part of Figure 2(a), when solving the math question "compute the sum of all integers s such that $-26&lt;s&lt;24$", one possible approach utilizes term pairing and summing the pairs to generate the final answer. Another possible approach employs the arithmetic series sum formula to compute the final result directly. While both methods are valid for problem-solving, the first approach results in less stable outputs typically due to the complexity of the intermediate steps. In contrast, the second approach, which applies the arithmetic series formula, generally results in better quality and more stable model outputs. The arithmetic series formula is considered strategic knowledge.</p>
<p>Strategic knowledge (Strategy) refers to a well-defined method or principle that guides reasoning towards a correct and stable solution. It involves using structured processes that logically lead to the desired outcome, thereby enhancing the stability of CoT generation and improving the overall quality of the results.</p>
<p>Specifically, strategic knowledge should adhere to the following principles:</p>
<ol>
<li>Correct and Comprehensive Problem-Solving Approach: It provides a systematic approach that allows the model to generate accurate answers when it follows the reasoning steps carefully.</li>
<li>Relatively Straightforward Problem-Solving Steps: The steps of the method should not be overly complex, while each step should be sufficiently detailed to ensure accuracy and prevent overly brief outputs that could lead to ambiguity.</li>
</ol>
<h3>Strategic Chain-of-Thought</h3>
<p>Building on the concept of strategic knowledge, we propose a prompt-based method to enhance the reasoning quality of LLMs, called Strategic Chain-of-Thought (SCoT).</p>
<p>The SCoT method enables the model to first elicit strategic knowledge before generating an answer, rather than producing an answer directly. Specifically, in a single-query setting, SCoT involves two key steps:</p>
<ol>
<li>Elicitation of Strategic Knowledge: The model identifies and determines one of the most effective and efficient methods for solving the problem, which then serves as the strategic knowledge for the task.</li>
<li>Application of Strategic Knowledge: The model subsequently applies the identified strategic knowledge to solve the problem and derive the final answer.</li>
</ol>
<p>Figure 3(a) illustrates a prompt template utilizing the SCoT approach. Our prompt consists of five components: Role, Workflow, Rule, Initialization, and Task Input. The prompt incorporates a structured workflow comprising three steps integrated into a single prompt. The first two steps are designed to identify and elicit strategic knowledge for solving the problem, while the third step focuses on applying the strategy to generate the answer, as shown in Figure 4.</p>
<p>We demonstrate that the rules for strategic knowledge identification vary across different domains. In mathematics, strategic knowledge favors generating elegant and efficient solutions, such as using the arithmetic series formula to sum sequences. In physics, it involves selecting the most relevant and straightforward formulas or processes, such as applying $F=ma$ to calculate force. For multi-hop reasoning, strategic knowledge focuses on determining the appropriate granularity for problem decomposition and recalling pertinent information. Similarly, in other domains, the model first develops an overarching method or workflow before systematically applying it to solve problems, such as optimizing complex systems through algorithms and heuristics.</p>
<h3>Few-shot Strategic Chain-of-Thought</h3>
<p>We refine the SCoT method into a few-shot version by leveraging the strategy to select demonstrations. Our approach is structured into two stages: constructing a strategy-based demonstration corpus and performing model inference.</p>
<p>Stage 1: Strategic Knowledge-Based Demonstration Corpus Construction.</p>
<p>This stage involves the following two steps, as shown in Figure 2(b):</p>
<ol>
<li>SCoT Answer Generation: We apply the zero-shot SCoT method to the training set to generate a corresponding SCoT answer for each question in the dataset.</li>
<li>Demonstration Corpus Construction: The generated answers are compared with the ground truth. Only those accurate question-SCoT answer pairs are retained. This step assumes that the strategic knowledge used in these problems is both correct and relevant. The validated question-SCoT answer pairs are then compiled into a demonstration corpus based on strategic knowledge.</li>
</ol>
<p>Stage 2: Model Inference.</p>
<p>This stage involves the following three steps in a two-query process, as shown in the right of Figure 2(a):</p>
<ol>
<li>Strategic Knowledge Generation: The LLM generates strategic knowledge relative to the problem, focusing on understanding the problem rather than producing the final answer.</li>
<li>Demonstration Matching: The generated strategic knowledge is used to search the demonstration corpus created in Stage 1. The system identifies and matches the most relevant demonstrations with the SCoT answers from the most similar examples.</li>
<li>Few-shot Inference: The selected demonstrations are integrated as few-shot examples into the input prompt (Figure 3(b)). This integration guides the model to generate the final prediction based on the provided examples.</li>
</ol>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />Figure 2: Illustration of Zero-shot and Few-shot Strategic SCoT. Few-shot SCoT builds upon Zero-shot SCoT by incorporating selected demonstrations. Details of the Few-shot SCoT approach are omitted due to space limitations.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />Figure 3: Prompt templates for zero-shot and few-shot SCoT</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />Figure 4: Example of a Workflow in a Math Task Prompt</p>
<h2>Experimental Setup</h2>
<p>In this section, we introduce the detailed experimental setup for validation of SCoT, including the datasets used for testing, the models covered, and the baselines employed.</p>
<h2>Datasets and Tasks</h2>
<p>To validate the effectiveness of the SCoT method, we collect a range of reasoning-related datasets covering domains including mathematics and physical reasoning, commonsense and multi-hop reasoning, and spatial reasoning:</p>
<ol>
<li>Mathematics and Physical Reasoning: We assess the models using datasets such as MathQA (Amini et al. 2019), AQuA (Ling et al. 2017), GSM8K (Cobbe et al. 2021), and MMLU-high-school-math (Hendrycks et al. 2021) for mathematical reasoning tasks. These datasets feature a range of mathematical problems with varying levels of difficulty, demanding strong mathematical reasoning abilities. Additionally, we evaluated the models on ARC_Challenge (Clark et al. 2018) for physical reasoning, i.e., a popular dataset that presents significant challenges in this domain.</li>
<li>Commonsense and Multi-hop Reasoning: We evaluate the models on CommonsenseQA (CSQA) (Talmor et al. 2019) for commonsense reasoning tasks and StrategyQA (SQA) (Geva et al. 2021) for multi-hop reasoning tasks. These datasets are well-regarded in their respective domains and offer a substantial level of difficulty.</li>
<li>Spatial Reasoning: We also evaluate the models using the Tracking_Object (Object) (BIG-bench authors 2023) dataset, which represents a less common but highly intriguing type of reasoning task.</li>
</ol>
<p>In the few-shot version of SCoT, we conduct experiments exclusively on the MathQA, AQuA, GSM8K, and ARC datasets. This selection is due to the requirement that the dataset must have a sufficiently large training set with gold answers for constructing the demonstration corpus in the first step. Only these four datasets meet this criterion.</p>
<h2>Models</h2>
<p>To verify the effectiveness of the SCoT method, we utilize the following LLMs: the Llama3 series (Dubey et al. 2024) (including Llama3-8B, Llama3-70B, Llama3.1-8B, and Llama3.1-70B); the Llama2 series (Touvron et al. 2023) (including Llama2-7B, Llama2-13B, and Llama2-70B); Mistral-7B (Jiang et al. 2023); the Qwen2 series (Yang</p>
<p>et al. 2024a) (including Qwen2-7B and Qwen2-72B); and ChatGLM4-9B (Team GLM et al. 2024). ChatGLM4-9B is chat-oriented and other models are instruction-tuned.</p>
<h3>Baselines</h3>
<p>We use zero-shot prompts (Kojima et al. 2022), Self-Consistency (Wang et al. 2022) and Step Back (Zheng et al. 2023) as baselines. We only conducted experiments on 5 datasets using Step Back because Step Back is not well-suited for other datasets. BoT (Yang et al. 2024b) is not chosen because its template has not been available, making it impossible to reproduce.</p>
<p>We select the accuracy as the metric for the performance, which is calculated by the average results of three independent inferences on each model. The experimental parameter settings are provided in the appendix.</p>
<h2>Experimental Results</h2>
<p>In this section, we empirically evaluate the effectiveness of the Strategic Chain-of-Thought (SCoT) approach. To verify SCoT’s efficacy across all datasets, we test it using two open-source models, Llama3-8B and Mistral-7B. To further validate SCoT’s effectiveness across different models, we select one dataset from each of the three reasoning task categories and conduct tests on all 7 models. We also examine the impact of model size, perform ablation studies on SCoT components, conduct case studies, and analyze experimental efficiency to understand the factors influencing the effectiveness of SCoT.</p>
<h3>Results across all Datasets</h3>
<p>The experimental results across all datasets using two models are presented in Table 1. Notably, in zero-shot settings, SCoT outperforms the CoT approach in most tasks, with particularly significant improvements observed on the GSM8K dataset, where accuracy increases from 52.11% to 73.16% after incorporating strategic knowledge. Additionally, SCoT achieves a 24.13% improvement on the Tracking_Object dataset. However, the Llama3-8B model exhibits a 2.6% decrease in performance on the ARC dataset. In general, the Llama3-8B model shows an average improvement of 6.92% on all datasets, while the Mistral-7B model demonstrates an average improvement of 3.81% on comparable datasets. Compared to Step Back and Self-Consistency, SCoT also performs better than these two methods except for the result of Self-Consistency with Llama3-8B model on the ARC dataset. Nevertheless, our SCoT still achieves comparable results to it. Notably, SCoT shows substantial gains in commonsense reasoning tasks compared with other methods.</p>
<p>Furthermore, we extend the SCoT framework to support few-shot settings by automatically matching demonstrations, resulting in even stronger performance. The SCoT 1-shot^{-}, as shown in Table 1, refers to CoT prompting with demonstrations matched through strategic knowledge. Compared to CoT 0-shot^{1}, SCoT 1-shot^{-}, which uses strategy-matched demonstrations, shows significant performance improvements across most datasets, highlighting the effectiveness of the matched demonstrations. The SCoT 1-shot, which combines both strategic knowledge and strategy-matched demonstrations, achieves the best results overall.</p>
<h3>Results across all Models</h3>
<p>The experimental results for all models on the three datasets are shown in Table 2. The experiments demonstrate that SCoT can enhance performance across most models. In particular, with the exception of the Llama3.1-8B model, where the addition of SCoT results in a slight decrease in accuracy on the MMLU task, other models exhibit accuracy improvements ranging from 1.11% to 24.13% across the three datasets. Note that the CoT 0-shot has achieved 100% accuracy with Llama3.1-70B model on Tracking_Object dataset, and SCoT 0-shot maintains this performance.</p>
<p>Figure 5: Accuracy(%) across three datasets using different scales of models in Llama2 series</p>
<h3>Model Scale</h3>
<p>Here we investigate the impact of model size on the effectiveness of SCoT. Experiments on the Llama2 model series with three different sizes are conducted, and the results are shown in Figure 5. It demonstrates that SCoT can lead to accuracy improvements across all sizes of the Llama2 models. However, a general trend emerges that performance improvement decreases marginally with model size. Furthermore, manual inspection of the model outputs reveals that larger models are more likely to generate CoT path containing strategic knowledge in 0-shot settings.</p>
<h3>Ablation Study</h3>
<p>We explore the effects of various components within the prompt (such as role, workflow, structure, and the quantity of demonstrations) on accuracy. The experimental results are illustrated in Table 3. Building on the CoT 0-shot approach, we observed that adding roles, incorporating workflows, and formatting prompts in markdown progressively increased accuracy. We also explored the impact of the number of demonstrations on accuracy within the few-shot SCoT framework. Experimental results indicate that as the number of demonstrations increases, the performance of SCoT either slightly improves or remains unchanged.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Method</th>
<th>MathQA</th>
<th>AQuA</th>
<th>GSM8K</th>
<th>MMLU</th>
<th>ARC</th>
<th>SQA</th>
<th>CSQA</th>
<th>Object</th>
</tr>
</thead>
<tbody>
<tr>
<td>Llama3-8B</td>
<td>CoT 0-shot</td>
<td>56.33</td>
<td>49.61</td>
<td>52.11</td>
<td>46.67</td>
<td>80.60</td>
<td>64.60</td>
<td>71.13</td>
<td>44.27</td>
</tr>
<tr>
<td></td>
<td>Self-Con</td>
<td>57.00</td>
<td>51.90</td>
<td>48.48</td>
<td>49.52</td>
<td>81.00</td>
<td>66.00</td>
<td>72.06</td>
<td>54.00</td>
</tr>
<tr>
<td></td>
<td>Step Back</td>
<td>56.33</td>
<td>50.39</td>
<td>-</td>
<td>47.78</td>
<td>75.80</td>
<td>64.64</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>SCoT 0-shot</td>
<td>56.67</td>
<td>51.85</td>
<td>73.16</td>
<td>50.00</td>
<td>78.02</td>
<td>68.56</td>
<td>74.00</td>
<td>68.40</td>
</tr>
<tr>
<td></td>
<td>SCoT 1-shot</td>
<td>56.33</td>
<td>50.87</td>
<td>74.91</td>
<td>-</td>
<td>73.40</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>SCoT 1-shot</td>
<td>57.67</td>
<td>55.12</td>
<td>76.57</td>
<td>-</td>
<td>80.60</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Mistral-7B</td>
<td>CoT 0-shot</td>
<td>30.00</td>
<td>29.13</td>
<td>36.26</td>
<td>29.75</td>
<td>67.20</td>
<td>56.22</td>
<td>61.80</td>
<td>21.40</td>
</tr>
<tr>
<td></td>
<td>Self-Con</td>
<td>31.42</td>
<td>32.87</td>
<td>34.50</td>
<td>31.88</td>
<td>68.78</td>
<td>53.50</td>
<td>62.69</td>
<td>24.50</td>
</tr>
<tr>
<td></td>
<td>Step Back</td>
<td>31.43</td>
<td>32.87</td>
<td>-</td>
<td>31.85</td>
<td>68.00</td>
<td>56.72</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>SCoT 0-shot</td>
<td>30.44</td>
<td>33.60</td>
<td>38.97</td>
<td>32.35</td>
<td>72.20</td>
<td>61.89</td>
<td>68.00</td>
<td>24.75</td>
</tr>
<tr>
<td></td>
<td>SCoT 1-shot</td>
<td>34.33</td>
<td>31.50</td>
<td>45.57</td>
<td>-</td>
<td>67.40</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>SCoT 1-shot</td>
<td>37.00</td>
<td>35.04</td>
<td>47.38</td>
<td>-</td>
<td>73.20</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>Table 1: Accuracy (\%) using Llama2-8B and Mistral-7B across all datasets. SCoT 1-shot ${ }^{-}$refers to the results obtained using the standard few-shot CoT template but with demonstrations matched by strategy.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Llama3-8B</th>
<th style="text-align: center;">Mistral-7b</th>
<th style="text-align: center;">Chatglm4-9B</th>
<th style="text-align: center;">Qwen2-7B</th>
<th style="text-align: center;">Qwen2-70B</th>
<th style="text-align: center;">Llama3.1-8B</th>
<th style="text-align: center;">Llama3.1-70B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MMLU</td>
<td style="text-align: center;">CoT 0-shot</td>
<td style="text-align: center;">46.67</td>
<td style="text-align: center;">29.75</td>
<td style="text-align: center;">66.67</td>
<td style="text-align: center;">71.97</td>
<td style="text-align: center;">84.20</td>
<td style="text-align: center;">59.63</td>
<td style="text-align: center;">85.19</td>
</tr>
<tr>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">SCoT 0-shot</td>
<td style="text-align: center;">$\mathbf{5 0 . 0 0}_{+3.33}$</td>
<td style="text-align: center;">$\mathbf{3 2 . 3 5}_{+2.59}$</td>
<td style="text-align: center;">$\mathbf{6 8 . 1 5}_{+1.48}$</td>
<td style="text-align: center;">71.85</td>
<td style="text-align: center;">$\mathbf{8 5 . 9 3}_{+1.73}$</td>
<td style="text-align: center;">56.42</td>
<td style="text-align: center;">85.19</td>
</tr>
<tr>
<td style="text-align: center;">SQA</td>
<td style="text-align: center;">CoT 0-shot</td>
<td style="text-align: center;">64.60</td>
<td style="text-align: center;">56.22</td>
<td style="text-align: center;">61.80</td>
<td style="text-align: center;">61.00</td>
<td style="text-align: center;">75.22</td>
<td style="text-align: center;">73.11</td>
<td style="text-align: center;">64.67</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCoT 0-shot</td>
<td style="text-align: center;">$\mathbf{6 8 . 5 6}_{+3.96}$</td>
<td style="text-align: center;">$\mathbf{6 1 . 8 9}_{+5.67}$</td>
<td style="text-align: center;">$\mathbf{6 4 . 6 7}_{+2.87}$</td>
<td style="text-align: center;">61.00</td>
<td style="text-align: center;">$\mathbf{7 7 . 6 7}_{+2.45}$</td>
<td style="text-align: center;">$\mathbf{7 4 . 2 2}_{+1.11}$</td>
<td style="text-align: center;">$\mathbf{8 2 . 3 3}_{+1.33}$</td>
</tr>
<tr>
<td style="text-align: center;">Object</td>
<td style="text-align: center;">CoT 0-shot</td>
<td style="text-align: center;">44.27</td>
<td style="text-align: center;">21.40</td>
<td style="text-align: center;">61.80</td>
<td style="text-align: center;">46.20</td>
<td style="text-align: center;">93.93</td>
<td style="text-align: center;">62.60</td>
<td style="text-align: center;">100.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCoT 0-shot</td>
<td style="text-align: center;">$\mathbf{6 8 . 4 0}_{+24.13}$</td>
<td style="text-align: center;">$\mathbf{2 4 . 6 7}_{+3.27}$</td>
<td style="text-align: center;">$\mathbf{6 9 . 0 0}_{+7.20}$</td>
<td style="text-align: center;">$\mathbf{4 7 . 5 3}_{+1.33}$</td>
<td style="text-align: center;">$\mathbf{9 7 . 4 7}_{+3.54}$</td>
<td style="text-align: center;">$\mathbf{7 7 . 6 0}_{+15.00}$</td>
<td style="text-align: center;">100.00</td>
</tr>
</tbody>
</table>
<p>Table 2: Accuracy(\%) across seven models on MMLU, SQA and Tracking_Object datasets</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">AQuA</th>
<th style="text-align: center;">ARC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Mistral-7B*</td>
<td style="text-align: center;">$29.13 \%$</td>
<td style="text-align: center;">$67.20 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-7B + Role*</td>
<td style="text-align: center;">$27.95 \%$</td>
<td style="text-align: center;">$69.80 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-7B + Role</td>
<td style="text-align: center;">$32.28 \%$</td>
<td style="text-align: center;">$71.20 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-7B + WorkFlow*</td>
<td style="text-align: center;">$33.07 \%$</td>
<td style="text-align: center;">$70.40 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-7B + WorkFlow</td>
<td style="text-align: center;">$31.89 \%$</td>
<td style="text-align: center;">$70.40 \%$</td>
</tr>
<tr>
<td style="text-align: left;">SCoT 0-shot (Ours)</td>
<td style="text-align: center;">$\mathbf{3 3 . 6 0 \%}$</td>
<td style="text-align: center;">$\mathbf{7 2 . 2 0 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">SCoT 1-shot (Ours)</td>
<td style="text-align: center;">$\mathbf{3 5 . 0 4 \%}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 2 0 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">SCoT 3-shot (Ours)</td>
<td style="text-align: center;">$\mathbf{3 5 . 4 3 \%}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 2 0 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Ablation study on SCoT prompt components: * denotes a non-markdown format, while no * indicates a markdown format.</p>
<h2>Case Study</h2>
<p>We conduct a detailed case study focusing on the validity of the strategic knowledge elicited from the model. Figure 6 shows several typical cases.</p>
<p>In the domain of mathematics, we observe that the SCoT output tends to favor solving problems using inequalities rather than directly analyzing the problem to reach an answer. For the instance of frog jumping calculation in the Figure 6, an incorrect solution may miscalculate the final jump's impact. While generating a strategy ensures accurate calculations by considering all constraints and systematically
solving the problem.
In the field of physics, we find that the model's CoT output could be misled by specific phrases in the task input (e.g., "capacitor"), leading to the selection of an incorrect formula. In contrast, the SCoT approach successfully elicited the correct formula. Similarly, in multi-hop reasoning tasks, CoT output often focuses on details, resulting in incomplete subsequent logical reasoning, whereas SCoT generates answers by considering the overall context.</p>
<h2>Efficiency Analysis</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Llama3-8B</th>
<th style="text-align: center;">Mistral-7B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">AQuA</td>
<td style="text-align: center;">CoT 0-shot</td>
<td style="text-align: center;">361.384</td>
<td style="text-align: center;">270.260</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCoT 0-shot</td>
<td style="text-align: center;">370.378</td>
<td style="text-align: center;">458.413</td>
</tr>
<tr>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">CoT 0-shot</td>
<td style="text-align: center;">130.532</td>
<td style="text-align: center;">858.507</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCoT 0-shot</td>
<td style="text-align: center;">206.278</td>
<td style="text-align: center;">611.848</td>
</tr>
<tr>
<td style="text-align: center;">Object</td>
<td style="text-align: center;">CoT 0-shot</td>
<td style="text-align: center;">121.460</td>
<td style="text-align: center;">89.654</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCoT 0-shot</td>
<td style="text-align: center;">174.888</td>
<td style="text-align: center;">162.822</td>
</tr>
</tbody>
</table>
<p>Table 4: Token length comparison for SCoT and CoT 0-shot methods</p>
<p>Due to SCoT's mechanism of generating strategy before solving problems in one query, it is more efficient than multi-query methods. However, compared to single-query</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Math Question A</th>
<th style="text-align: center;">A well is 30 meters deep. A frog at the bottom jumps 3 meters up each day but slips back 2 meters each night. How many days will it take for the frog to climb out of the well?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CoT Answer</td>
<td style="text-align: center;">To solve this problem, we need to consider the frog's daily progress. <br> On the first day, the frog jumps up 3 meters but slips back 2 meters, resulting in a net gain of 1 meter. From the second day onward, the frog gains 1 meter each day. <br> The frog needs to climb a total of 30 meters to get out of the well. After the first day, it still needs to climb 29 meters. Therefore, the total number of days required is $1+29=30$ days.</td>
</tr>
<tr>
<td style="text-align: center;">SCoT Answer</td>
<td style="text-align: center;">To address this problem elegantly, we can utilize a mathematical model. The frog gains a net height of 1 meter each day. However, on the day it reaches the top of the well, it will no longer slip back down. <br> Let $n$ represent the number of days it takes for the frog to climb out of the well. We can formulate the following inequality: $n-1+3&gt;=30$ <br> This simplifies to: $n&gt;=28$. Thus, the total number of days for the frog to climb out of the well is 28 days.</td>
</tr>
<tr>
<td style="text-align: center;">Physical Question B</td>
<td style="text-align: center;">How much work is required to charge a $10 \mu \mathrm{~F}$ capacitor to a potential difference of 100 V ?</td>
</tr>
<tr>
<td style="text-align: center;">CoT Answer</td>
<td style="text-align: center;">Using the formula for work done by a capacitor, which is $W=k \times V \times C$, and substituting the values, we find that 0.09J of work needs to be done.</td>
</tr>
<tr>
<td style="text-align: center;">SCoT Answer</td>
<td style="text-align: center;">Using the capacitor charging formula. The formula for capacitor charging is $E=(1 / 2) C V^{\wedge} 2$ <br> To substitute the values, it needs to do 0.05 J of work</td>
</tr>
<tr>
<td style="text-align: center;">Multi-hop Question C</td>
<td style="text-align: center;">Is shrimp scampi definitely free of plastic?</td>
</tr>
<tr>
<td style="text-align: center;">CoT Answer</td>
<td style="text-align: center;">Shrimp scampi is a dish made with shrimp, butter, garlic, and sometimes lemon, usually served with pasta. The ingredients typically include fresh or frozen shrimp, butter, garlic, and lemon juice. There is no connection between shrimp scampi and plastic, as plastic is not an ingredient in this dish. Therefore, shrimp scampi is True definitely free of plastic.</td>
</tr>
<tr>
<td style="text-align: center;">SCoT Answer</td>
<td style="text-align: center;">Shrimp scampi usually does not contain plastic, but contamination is possible. It is typically made with ingredients like shrimp, garlic, butter, and parsley, which do not contain plastic, and there are no known instances of shrimp scampi being made with plastic-containing ingredients. Therefore, while shrimp scampi is usually made without plastic. So, the answer is: No.</td>
</tr>
</tbody>
</table>
<p>Figure 6: Comparison of the paths generated by CoT and SCoT in different domains.
methods, the output token length might be longer, potentially decreasing efficiency. To investigate this, we measure the output token lengths for the AQuA, GSM8K, and Tracking.Object datasets using both CoT 0-shot and SCoT 0-shot methods. The results are shown in Table 4.</p>
<p>The results indicate that the token length output by the Mistral-7B model on the GSM8K dataset decreases with the SCoT method. This reduction may be due to the model's tendency to repetitively generate a specific answer span up to the inference length limit on the GSM8K dataset in CoT 0 -shot, leading to a decline in accuracy. SCoT mitigates this issue. Besides, the length of SCoT varies from 1.03 to 1.8 times that of CoT , averaging around 1.5 times. This shows that while our method is somewhat slower than CoT, the efficiency remains manageable.</p>
<h2>Discussions</h2>
<h2>Automatic SCoT</h2>
<p>To demonstrate that our experimental results are not influenced by human-crafted prompts but rather due to the concept of SCoT, we conduct a preliminary test to evaluate whether the SCoT prompt templates can be automatically generated. We provide the SCoT concept to Qwen2-72B to generate the corresponding prompt templates and tested these on the AQuA dataset. The results are presented in Table 5. The findings indicate that while the accuracy of prompts automatically generated based on the SCoT concept is lower than that of manually crafted SCoT prompts, it is still superior to 0 -shot CoT performance. This suggests
that the automatic generation of SCoT-based prompt templates is feasible.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: right;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CoT 0-shot</td>
<td style="text-align: right;">29.13</td>
</tr>
<tr>
<td style="text-align: left;">SCoT 0-shot</td>
<td style="text-align: right;">33.60</td>
</tr>
<tr>
<td style="text-align: left;">Auto SCoT</td>
<td style="text-align: right;">$\mathbf{3 1 . 8 9}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Accuracy(\%) using automatically generated prompts by LLMs based on the SCoT concept</p>
<h2>Conclusion</h2>
<p>In this paper, we introduce the Strategic Chain-of-Thought, a method that enables LLMs to autonomously generate an optimal Chain-of-Thought path. By integrating a structured workflow for eliciting and applying strategic knowledge, SCoT enhances the model's ability to produce a high quality outputs. We further extend SCoT to a few-shot version by matching demonstrations through strategic knowledge from a predefined strategic knowledge-based corpus. Experimental results demonstrate the effectiveness of both 0 -shot SCoT and few-shot SCoT.</p>
<p>Overall, SCoT offers a promising framework for improving the quality of reasoning path in LLMs. Future research will focus on evaluating its effectiveness with more complex problems and exploring further applications.</p>
<h2>References</h2>
<p>Amini, A.; Gabriel, S.; Lin, P.; Koncel-Kedziorski, R.; Choi, Y.; and Hajishirzi, H. 2019. MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms. arXiv:1905.13319.
BIG-bench authors. 2023. Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research.
Chang, Y.; Wang, X.; Wang, J.; Wu, Y.; Yang, L.; Zhu, K.; Chen, H.; Yi, X.; Wang, C.; Wang, Y.; et al. 2024. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 15(3): 1-45.
Clark, P.; Cowhey, I.; Etzioni, O.; Khot, T.; Sabharwal, A.; Schoenick, C.; and Tafjord, O. 2018. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. arXiv:1803.05457.
Cobbe, K.; Kosaraju, V.; Bavarian, M.; Chen, M.; Jun, H.; Kaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.; Hesse, C.; and Schulman, J. 2021. Training Verifiers to Solve Math Word Problems. arXiv:2110.14168.
Dubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle, A.; Letman, A.; and et al. 2024. The Llama 3 Herd of Models. arXiv:2407.21783.
Gao, L.; Madaan, A.; Zhou, S.; Alon, U.; Liu, P.; Yang, Y.; Callan, J.; and Neubig, G. 2023. Pal: Program-aided language models. In International Conference on Machine Learning, 10764-10799. PMLR.
Geva, M.; Khashabi, D.; Segal, E.; Khot, T.; Roth, D.; and Berant, J. 2021. Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. arXiv:2101.02235.
Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.; Song, D.; and Steinhardt, J. 2021. Measuring Massive Multitask Language Understanding. arXiv:2009.03300.
Huang, J.; and Chang, K. C.-C. 2022. Towards reasoning in large language models: A survey. arXiv preprint arXiv:2212.10403.
Jiang, A. Q.; Sablayrolles, A.; Mensch, A.; Bamford, C.; Chaplot, D. S.; de las Casas, D.; and et al. 2023. Mistral 7B. arXiv:2310.06825.
Kojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y.; and Iwasawa, Y. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: 22199-22213.
Kwon, W.; Li, Z.; Zhuang, S.; Sheng, Y.; Zheng, L.; Yu, C. H.; Gonzalez, J. E.; Zhang, H.; and Stoica, I. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles.
Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.; Goyal, N.; Küttler, H.; Lewis, M.; tau Yih, W.; Rocktäschel, T.; Riedel, S.; and Kiela, D. 2021. RetrievalAugmented Generation for Knowledge-Intensive NLP Tasks. arXiv:2005.11401.</p>
<p>Ling, W.; Yogatama, D.; Dyer, C.; and Blunsom, P. 2017. Program Induction by Rationale Generation : Learning to Solve and Explain Algebraic Word Problems. arXiv:1705.04146.
Rusczyk, R. 2003. The Art of Problem Solving. Washington, D.C.: Mathematical Association of America.</p>
<p>Sivarajkumar, S.; Kelley, M.; Samolyk-Mazzanti, A.; Visweswaran, S.; and Wang, Y. 2023. An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing. arXiv:2309.08008.
Suzgun, M.; and Kalai, A. T. 2024. Meta-prompting: Enhancing language models with task-agnostic scaffolding. arXiv preprint arXiv:2401.12954.
Sweller, J. 1988. Cognitive Load During Problem Solving: Effects on Learning. Cognitive Science, 12(2): 257-285.
Talmor, A.; Herzig, J.; Lourie, N.; and Berant, J. 2019. CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. In Burstein, J.; Doran, C.; and Solorio, T., eds., Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4149-4158. Minneapolis, Minnesota: Association for Computational Linguistics.
Team GLM; Zeng, A.; Xu, B.; Wang, B.; Zhang, C.; Yin, D.; and et al. 2024. ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools. arXiv:2406.12793.
Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei, Y.; and et al. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288.
Wang, L.; Xu, W.; Lan, Y.; Hu, Z.; Lan, Y.; Lee, R. K.-W.; and Lim, E.-P. 2023. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. arXiv preprint arXiv:2305.04091.
Wang, X.; Wei, J.; Schuurmans, D.; Le, Q.; Chi, E.; Narang, S.; Chowdhery, A.; and Zhou, D. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.
Wang, X.; and Zhou, D. 2024. Chain-of-thought reasoning without prompting. arXiv preprint arXiv:2402.10200.
Wang, Z.; Zhao, S.; Wang, Y.; Huang, H.; Shi, J.; Xie, S.; Wang, Z.; Zhang, Y.; Li, H.; and Yan, J. 2024. Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge Perspectives. arXiv:2408.06904.
Yang, A.; Yang, B.; Hui, B.; Zheng, B.; Yu, B.; Zhou, C.; and et al. 2024a. Qwen2 Technical Report. arXiv preprint arXiv:2407.10671.
Yang, L.; Yu, Z.; Zhang, T.; Cao, S.; Xu, M.; Zhang, W.; Gonzalez, J. E.; and Cui, B. 2024b. Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models. arXiv preprint arXiv:2406.04271.
Zhang, Z.; Zhang, A.; Li, M.; and Smola, A. 2022. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493.</p>
<p>Zhang, Z.; Zhang, A.; Li, M.; Zhao, H.; Karypis, G.; and Smola, A. 2023. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923.
Zheng, H. S.; Mishra, S.; Chen, X.; Cheng, H.-T.; Chi, E. H.; Le, Q. V.; and Zhou, D. 2023. Take a step back: Evoking reasoning via abstraction in large language models. arXiv preprint arXiv:2310.06117.</p>
<h2>Details of Experiments</h2>
<h2>Models Details</h2>
<p>This experiment involves ten models, nine of which are public (Llama3-8B, Llama2-7B, Mistral-7B, Llama3.1-8B, Qwen2-7B, ChatGLM4-9B, Llama3-70B, Llama3.1-70B, Llama2-70B, and Qwen2-72B), while one model is private. The sources and licenses for all public models are detailed in Table 6.</p>
<h2>Datasets Details</h2>
<p>This experiment involves eight datasets: MathQA, AQuA, GSM8K, MMLU, ARC, StrategyQA, CommonsenseQA, and Tracking_Object. All datasets used in this study are publicly available, with their sources and licenses detailed in Table 7.</p>
<p>MathQA, AQuA, MMLU, ARC, StrategyQA, CommonsenseQA, and Tracking_Object consist of multiple-choice questions. To determine correctness, we compare the predicted choice with the gold (correct) choice. For GSM8K, the answers are numerical text spans; we assess correctness by checking if the predicted answer exactly matches the gold answer.</p>
<h2>Other Details</h2>
<p>For all experiments, except those involving SelfConsistency, the temperature is set to 0 , and the top.p parameter is set to 1 . For Self-Consistency, following the settings from the original paper (Wang et al. 2022), the temperature is adjusted to 0.5 , and top.p is set to 0.5 .</p>
<p>We utilize vllm (Kwon et al. 2023) as the inference framework for all deployments.</p>
<h2>Results</h2>
<h2>All Results</h2>
<p>Accuracy is used as the evaluation metric. We conducted three independent inference runs for all experiments and calculated the average results. However, due to the high computational cost, we performed only a single inference for SelfConsistency. The accuracy and standard deviation results are presented in Table 8 and Table 9.</p>
<h2>Case Study</h2>
<p>We conducted a detailed case study to assess the validity of the strategic knowledge elicited from the model. Figures 7 and 8 present several representative cases spanning math reasoning, physical reasoning, commonsense reasoning, multi-hop reasoning, and spatial reasoning.</p>
<h2>Experimental Prompts</h2>
<p>The prompt for standard zero-shot Chain-of-Thought is shown in Figure 9. Prompts for zero-shot Strategic Chain-of-Thought are displayed in Figure 10 (for math reasoning), Figure 11 (for multi-hop reasoning), Figure 13 (for physical reasoning), Figure 12 (for commonsense reasoning) and Figure 14 (for spatial reasoning). Prompts for one-shot Strategic Chain-of-Thought are shown in Figure 15. Finally, the prompts for automated Strategic Chain-of-Thought are
shown in Figure 16. The automated SCOT prompts were generated using LLMs by given the idea of SCoT.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: left;">Modelsources</th>
<th style="text-align: left;">License</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Llama2-7B-chat</td>
<td style="text-align: left;">https://huggingface.co/meta-llama/Llama-2-7b-chat</td>
<td style="text-align: left;">llama2 license</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-13B</td>
<td style="text-align: left;">https://huggingface.co/meta-llama/Llama-2-13b-chat</td>
<td style="text-align: left;">llama2 license</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-70B</td>
<td style="text-align: left;">https://huggingface.co/meta-llama/Llama-2-70b-chat</td>
<td style="text-align: left;">llama2 license</td>
</tr>
<tr>
<td style="text-align: left;">Llama3-8B</td>
<td style="text-align: left;">https://www.modelscope.cn/models/FlagAlpha/</td>
<td style="text-align: left;">Apache License 2.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Llama3.1-8B</td>
<td style="text-align: left;">https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct</td>
</tr>
<tr>
<td style="text-align: left;">Llama3.1-70B</td>
<td style="text-align: left;">https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct</td>
<td style="text-align: left;">llama3.1 license</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-7B</td>
<td style="text-align: left;">https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2</td>
<td style="text-align: left;">Apache License 2.0</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2-7B</td>
<td style="text-align: left;">https://huggingface.co/Qwen/Qwen2-7B-Instruct</td>
<td style="text-align: left;">Apache License 2.0</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2-72B</td>
<td style="text-align: left;">https://huggingface.co/Qwen/Qwen2-72B-Instruct</td>
<td style="text-align: left;">Apache License 2.0</td>
</tr>
<tr>
<td style="text-align: left;">ChatGLM4-9b</td>
<td style="text-align: left;">https://huggingface.co/THUDM/glm-4-9b-chat</td>
<td style="text-align: left;">glm-4-9b License</td>
</tr>
</tbody>
</table>
<p>Table 6: Models, sources and licenses used in this work</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Datasets</th>
<th style="text-align: left;">Sources</th>
<th style="text-align: left;">Licenses</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MathQA</td>
<td style="text-align: left;">https://huggingface.co/datasets/datafreak/MathQA</td>
<td style="text-align: left;">Apache License 2.0</td>
</tr>
<tr>
<td style="text-align: left;">AQuA</td>
<td style="text-align: left;">https://github.com/google-deepmind/AQuA</td>
<td style="text-align: left;">Apache License 2.0</td>
</tr>
<tr>
<td style="text-align: left;">GSM8K</td>
<td style="text-align: left;">https://huggingface.co/datasets/openai/gsm8k</td>
<td style="text-align: left;">MIT License</td>
</tr>
<tr>
<td style="text-align: left;">MMLU</td>
<td style="text-align: left;">https://huggingface.co/datasets/cais/mmlu</td>
<td style="text-align: left;">MIT License</td>
</tr>
<tr>
<td style="text-align: left;">ARC</td>
<td style="text-align: left;">https://huggingface.co/datasets/allenai/ai2_arc</td>
<td style="text-align: left;">CC-BY-SA-4.0 License</td>
</tr>
<tr>
<td style="text-align: left;">StrategyQA</td>
<td style="text-align: left;">https://huggingface.co/datasets/ChilleD/StrategyQA/viewer/default/test</td>
<td style="text-align: left;">MIT License</td>
</tr>
<tr>
<td style="text-align: left;">CommonsenseQA</td>
<td style="text-align: left;">https://huggingface.co/datasets/tau/commonsense_qa</td>
<td style="text-align: left;">MIT License</td>
</tr>
<tr>
<td style="text-align: left;">Object Tracking</td>
<td style="text-align: left;">https://github.com/google/BIG-bench/tree/092b196c1f8f14a54bbc62f24759d43bde46dd3b</td>
<td style="text-align: left;">Apache License 2.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">/bigbench/benchmark.tasks/tracking_shuffled_objects/three_objects</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 7: Datasets, sources and licenses used in this work</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">MathQA</th>
<th style="text-align: center;">AQuA</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">MMLU</th>
<th style="text-align: center;">ARC</th>
<th style="text-align: center;">SQA</th>
<th style="text-align: center;">CSQA</th>
<th style="text-align: center;">Object</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Llama3-8B</td>
<td style="text-align: center;">CoT 0-shot</td>
<td style="text-align: center;">$56.33_{\text {k0.000 }}$</td>
<td style="text-align: center;">$49.61_{\text {k1.790 }}$</td>
<td style="text-align: center;">$52.11_{\text {k0.129 }}$</td>
<td style="text-align: center;">$46.67_{\text {k0.000 }}$</td>
<td style="text-align: center;">$80.60_{\text {k0.000 }}$</td>
<td style="text-align: center;">$64.60_{\text {k0.646 }}$</td>
<td style="text-align: center;">$71.13_{\text {k0.094 }}$</td>
<td style="text-align: center;">$44.27_{\text {k0.736 }}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Self-Con</td>
<td style="text-align: center;">57.00</td>
<td style="text-align: center;">51.90</td>
<td style="text-align: center;">48.48</td>
<td style="text-align: center;">49.52</td>
<td style="text-align: center;">81.00</td>
<td style="text-align: center;">66.00</td>
<td style="text-align: center;">72.06</td>
<td style="text-align: center;">54.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Step Back</td>
<td style="text-align: center;">$56.33_{\text {k0.272 }}$</td>
<td style="text-align: center;">$50.39_{\text {k0.000 }}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$47.78_{\text {k0.000 }}$</td>
<td style="text-align: center;">$75.80_{\text {k0.248 }}$</td>
<td style="text-align: center;">$64.64_{\text {k0.2722 }}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCoT 0-shot</td>
<td style="text-align: center;">$56.67_{\text {k0.000 }}$</td>
<td style="text-align: center;">$51.85_{\text {k1.299 }}$</td>
<td style="text-align: center;">$73.16_{\text {k0.163 }}$</td>
<td style="text-align: center;">$50.00_{\text {k0.000 }}$</td>
<td style="text-align: center;">$78.02_{\text {k0.000 }}$</td>
<td style="text-align: center;">$68.56_{\text {k0.566 }}$</td>
<td style="text-align: center;">$74.00_{\text {k0.000 }}$</td>
<td style="text-align: center;">$68.40_{\text {k0.000 }}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCoT 1-shot ${ }^{-}$</td>
<td style="text-align: center;">$56.33_{\text {k0.000 }}$</td>
<td style="text-align: center;">$50.87_{\text {k2.140 }}$</td>
<td style="text-align: center;">$74.91_{\text {k0.000 }}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$73.40_{\text {k0.000 }}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCoT 1-shot</td>
<td style="text-align: center;">$57.67_{\text {k0.000 }}$</td>
<td style="text-align: center;">$55.12_{\text {k0.000 }}$</td>
<td style="text-align: center;">$76.57_{\text {k0.000 }}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$80.60_{\text {k0.000 }}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B</td>
<td style="text-align: center;">CoT 0-shot</td>
<td style="text-align: center;">$30.00_{\text {k0.000 }}$</td>
<td style="text-align: center;">$29.13_{\text {k1.245 }}$</td>
<td style="text-align: center;">$36.26_{\text {k1.854 }}$</td>
<td style="text-align: center;">$29.75_{\text {k0.924 }}$</td>
<td style="text-align: center;">$67.20_{\text {k0.356 }}$</td>
<td style="text-align: center;">$56.22_{\text {k0.314 }}$</td>
<td style="text-align: center;">$61.80_{\text {k0.000 }}$</td>
<td style="text-align: center;">$21.40_{\text {k0.000 }}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Self-Con</td>
<td style="text-align: center;">31.42</td>
<td style="text-align: center;">32.87</td>
<td style="text-align: center;">34.50</td>
<td style="text-align: center;">31.88</td>
<td style="text-align: center;">68.78</td>
<td style="text-align: center;">53.50</td>
<td style="text-align: center;">62.69</td>
<td style="text-align: center;">24.50</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Step Back</td>
<td style="text-align: center;">$31.43_{\text {k0.000 }}$</td>
<td style="text-align: center;">$32.87_{\text {k0.322 }}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$31.85_{\text {k0.495 }}$</td>
<td style="text-align: center;">$68.00_{\text {k0.000 }}$</td>
<td style="text-align: center;">$56.72_{\text {k0.000 }}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCoT 0-shot</td>
<td style="text-align: center;">$30.44_{\text {k0.874 }}$</td>
<td style="text-align: center;">$33.60_{\text {k1.523 }}$</td>
<td style="text-align: center;">$38.97_{\text {k0.655 }}$</td>
<td style="text-align: center;">$32.35_{\text {k1.665 }}$</td>
<td style="text-align: center;">$72.20_{\text {k0.370 }}$</td>
<td style="text-align: center;">$61.89_{\text {k0.415 }}$</td>
<td style="text-align: center;">$68.00_{\text {k0.000 }}$</td>
<td style="text-align: center;">$24.75_{\text {k0.165 }}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCoT 1-shot ${ }^{-}$</td>
<td style="text-align: center;">$34.33_{\text {k0.000 }}$</td>
<td style="text-align: center;">$31.50_{\text {k0.964 }}$</td>
<td style="text-align: center;">$45.57_{\text {k1.087 }}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$67.40_{\text {k0.000 }}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCoT 1-shot</td>
<td style="text-align: center;">$37.00_{\text {k0.000 }}$</td>
<td style="text-align: center;">$35.04_{\text {k0.000 }}$</td>
<td style="text-align: center;">$47.38_{\text {k0.107 }}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$73.20_{\text {k0.000 }}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 8: Accuracy (\%) using Llama2-8B and Mistral-7B across all datasets. SCoT 1-shot ${ }^{-}$refers to the results obtained using the standard few-shot CoT template but with demonstrations matched by strategy.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Llama3-8B</th>
<th style="text-align: center;">Mistral-7b</th>
<th style="text-align: center;">Chatglm4-9B</th>
<th style="text-align: center;">Qwen2-7B</th>
<th style="text-align: center;">Qwen2-70B</th>
<th style="text-align: center;">Llama3.1-8B</th>
<th style="text-align: center;">Llama3.1-70B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MMLU</td>
<td style="text-align: center;">CoT 0-shot</td>
<td style="text-align: center;">$46.67_{\pm 0.000}$</td>
<td style="text-align: center;">$29.75_{\pm 0.924}$</td>
<td style="text-align: center;">$66.67_{\pm 0.302}$</td>
<td style="text-align: center;">$71.97_{\pm 0.349}$</td>
<td style="text-align: center;">$84.20_{\pm 0.349}$</td>
<td style="text-align: center;">$59.63_{\pm 0.000}$</td>
<td style="text-align: center;">$85.19_{\pm 0.605}$</td>
</tr>
<tr>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">SCoT 0-shot</td>
<td style="text-align: center;">$50.00_{\pm 0.000}$</td>
<td style="text-align: center;">$32.35_{\pm 1.665}$</td>
<td style="text-align: center;">$68.15_{\pm 0.907}$</td>
<td style="text-align: center;">$71.85_{\pm 0.302}$</td>
<td style="text-align: center;">$85.93_{\pm 0.302}$</td>
<td style="text-align: center;">$56.42_{\pm 0.175}$</td>
<td style="text-align: center;">$85.19_{\pm 0.000}$</td>
</tr>
<tr>
<td style="text-align: center;">SQA</td>
<td style="text-align: center;">CoT 0-shot</td>
<td style="text-align: center;">$64.60_{\pm 0.595}$</td>
<td style="text-align: center;">$56.22_{\pm 0.314}$</td>
<td style="text-align: center;">$61.80_{\pm 0.363}$</td>
<td style="text-align: center;">$61.00_{\pm 0.000}$</td>
<td style="text-align: center;">$75.22_{\pm 0.314}$</td>
<td style="text-align: center;">$73.11_{\pm 0.314}$</td>
<td style="text-align: center;">$64.67_{\pm 0.000}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCoT 0-shot</td>
<td style="text-align: center;">$68.56_{\pm 0.566}$</td>
<td style="text-align: center;">$61.89_{\pm 0.415}$</td>
<td style="text-align: center;">$64.67_{\pm 0.408}$</td>
<td style="text-align: center;">$61.00_{\pm 0.157}$</td>
<td style="text-align: center;">$77.67_{\pm 0.272}$</td>
<td style="text-align: center;">$74.22_{\pm 0.157}$</td>
<td style="text-align: center;">$82.33_{\pm 0.157}$</td>
</tr>
<tr>
<td style="text-align: center;">Object</td>
<td style="text-align: center;">CoT 0-shot</td>
<td style="text-align: center;">$44.27_{\pm 0.736}$</td>
<td style="text-align: center;">$21.40_{\pm 0.000}$</td>
<td style="text-align: center;">$61.80_{\pm 0.000}$</td>
<td style="text-align: center;">$46.20_{\pm 0.000}$</td>
<td style="text-align: center;">$93.93_{\pm 0.525}$</td>
<td style="text-align: center;">$62.60_{\pm 0.411}$</td>
<td style="text-align: center;">$100.00_{\pm 0.000}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCoT 0-shot</td>
<td style="text-align: center;">$68.40_{\pm 0.000}$</td>
<td style="text-align: center;">$24.67_{\pm 0.000}$</td>
<td style="text-align: center;">$69.00_{\pm 0.000}$</td>
<td style="text-align: center;">$47.53_{\pm 0.094}$</td>
<td style="text-align: center;">$97.47_{\pm 0.339}$</td>
<td style="text-align: center;">$77.60_{\pm 0.993}$</td>
<td style="text-align: center;">$100.00_{\pm 0.000}$</td>
</tr>
</tbody>
</table>
<p>Table 9: Accuracy(\%) across seven models on MMLU, SQA and Tracking_Object datasets</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Math Question A</th>
<th style="text-align: center;">A well is 30 meters deep. A frog at the bottom jumps 3 meters up each day but slips back 2 meters each night. How many days w ill it take for the frog to climb out of the well?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CoT Answer</td>
<td style="text-align: center;">To solve this problem, we need to consider the frog's daily progress. <br> On the first day, the frog jumps up 3 meters but slips back 2 meters, resulting in a net gain of 1 meter. From the second day onward, the frog gains 1 meter each day. <br> The frog needs to climb a total of 30 meters to get out of the well. After the first day, it still needs to climb 29 meters. Therefore, the total number of days required is $1 \times 29 \times 30$ days.</td>
</tr>
<tr>
<td style="text-align: center;">SCoT Answer</td>
<td style="text-align: center;">To address this problem elegantly, we can utilize a mathematical model. The frog gains a net height of 1 meter each day. However, on the day it reaches the top of the well, it will no longer slip back down. Let $n$ represent the number of days it takes for the frog to climb out of the well. We can formulate the following inequality: $n-1$ $+3 \geqslant 30$ <br> This simplifies to: $n \geqslant 28$. Thus, the total number of days for the frog to climb out of the well is 28 days.</td>
</tr>
<tr>
<td style="text-align: center;">Physical Question B</td>
<td style="text-align: center;">How much work is required to charge a $10 \mu \mathrm{~F}$ capacitor to a potential difference of 100 V ?</td>
</tr>
<tr>
<td style="text-align: center;">CoT Answer</td>
<td style="text-align: center;">Using the formula for work done by a capacitor, which is W×K×V×C, and substituting the values, we find that 0.091 of work needs to be done.</td>
</tr>
<tr>
<td style="text-align: center;">SCoT Answer</td>
<td style="text-align: center;">Using the capacitor charging formula. The formula for capacitor charging is $E=[1 / 2] C V * 2$ <br> To substitute the values, it needs to do 0.051 of work</td>
</tr>
<tr>
<td style="text-align: center;">Math Question C</td>
<td style="text-align: center;">Jordan plays video games for 2 hours every day. He also has a part-time job where he earns $\$ 10$ an hour. How much money would Jordan earn in one week if he spent his video game time working instead?</td>
</tr>
<tr>
<td style="text-align: center;">CoT Answer</td>
<td style="text-align: center;">To calculate how much money Jordan would earn in one week if he spent his video game time working instead, we need to follow these steps: <br> Step 1: Jordan plays video games for 2 hours every day. There are 7 days in a week, so he would work for $7-2=5$ hours each day. Step 2: Jordan earns 10 an hour. If he works 5 hours a day, his daily earnings would be 5 hours * 10/hour + 50. <br> To find his weekly earnings, we multiply his daily earnings by the number of days in a week: 50/day * 7 days/week $=350 /$ week.</td>
</tr>
<tr>
<td style="text-align: center;">SCoT Answer</td>
<td style="text-align: center;">Calculate the total hours he would have worked in a week by multiplying the number of hours he plays per day by the number of days in a week. Given: 1. Hours Jordan plays video games per day $=2$ hours; 2. Days in a week $=7$ days <br> Total hours $=2$ hours/day $=7$ days/week $=14$ hours/week <br> Calculate the total amount of money Jordan would have earned in one week if he worked those 14 hours. <br> Total earnings $=14$ hours/week $=\$ 10 /$ hour $=\$ 140 /$ week</td>
</tr>
</tbody>
</table>
<p>Figure 7: Comparison of the paths generated by CoT and SCoT in different domains.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Object Question A</th>
<th style="text-align: center;">Alice, Bob, and Claire are playing a game. At the start of the game, they are each holding a ball: Alice has a brown ball, Bo b has a red ball, and Claire has a purple ball. As the game progresses, pairs of players trade balls. First, Bob and Claire swap balls. Then, Alice and Claire swap balls. Finally, Alice and Bob swap balls. At the end of the game, Claire has the A) brown ball; B) red ball; C) purple ball</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CoT Answer</td>
<td style="text-align: center;">Let's break down the sequence of swaps: <br> 1. Bob and Claire swap balls: Bob gets the purple ball, and Claire gets the red ball. <br> 2. Alice and Claire swap balls: Alice gets the red ball (which Claire had just gotten), and Claire gets the brown ball. <br> 3. Alice and Bob swap balls: Alice gets the red ball (which she had just given to Claire), and Bob gets the brown ball. So, at the end of the game, the final positions are: Claire has the purple ball.</td>
</tr>
<tr>
<td style="text-align: center;">SCoT Answer</td>
<td style="text-align: center;">Initial State: Alice has a brown ball; Bob has a red ball; Claire has a purple ball Process Exchanges: <br> 1. Bob and Claire swap balls: Alice still has a brown ball; Bob has a purple ball; Claire has a red ball <br> 2. Alice and Claire swap balls: Alice has a red ball; Bob has a purple ball; Claire has a brown ball <br> 3. Alice and Bob swap balls: Alice has a purple ball; Bob has a red ball; Claire has a brown ball Determine the Answer: The correct answer is that Claire has the brown ball.</td>
</tr>
<tr>
<td style="text-align: center;">Math Question B</td>
<td style="text-align: center;">Find out which of the following values is the multiple of $X$, if it is divisible by 9 and 12 ? <br> A) 36; B) 15; C) 17; D) 5; E) 7</td>
</tr>
<tr>
<td style="text-align: center;">CoT Answer</td>
<td style="text-align: center;">To find the multiple of a number that is divisible by both 9 and 12, we need to find the common multiple of 9 and 12. The least common multiple (LCM) of 9 and 12 can be found using the prime factorization method or the LCM table. The prime factorization of $9 \mathrm{is} 3^{<em>} 2$, and the prime factorization of $12 \mathrm{is} 2^{</em>} 2^{<em>} 2^{</em>} 3$. <br> Since $3^{<em>} 2$ is a factor of both $3^{</em>} 2$ and $2^{<em>} 2^{</em>} 2^{<em>} 3$, the LCM of 9 and 12 is $3^{</em>} 2^{<em>} 2^{</em>} 2^{<em>} 2^{</em>} 3=3^{<em>} 2^{</em>} 2^{<em>} 2^{</em>} 3=3^{<em>} 2^{</em>} 9=9^{*} 2=81$. Now, let's check which option is a perfect square of 9 . Option $B, 15$, is a perfect square of 9 , it is the correct answer..</td>
</tr>
<tr>
<td style="text-align: center;">SCoT Answer</td>
<td style="text-align: center;">The problem states that $X$ is divisible by 9 and 12. To find the multiple of $X$ that fits the given conditions, we need to find the least common multiple (LCM) of 9 and 12. <br> Choose the most efficient and practical approach: the LCM of 9 and 12 is 36 . <br> Therefore, the correct answer is A) 36 .</td>
</tr>
<tr>
<td style="text-align: center;">Multi-hop Question C</td>
<td style="text-align: center;">is shrimp scampi definitely free of plastic?</td>
</tr>
<tr>
<td style="text-align: center;">CoT Answer</td>
<td style="text-align: center;">Shrimp scampi is a dish made with shrimp, butter, garlic, and sometimes lemon, usually served with pasta. The ingredients typically include fresh or frozen shrimp, butter, garlic, and lemon juice. There is no connection between shrimp scampi and plastic, as plastic is not an ingredient in this dish. Therefore, shrimp scampi is True definitely free of plastic.</td>
</tr>
<tr>
<td style="text-align: center;">SCoT Answer</td>
<td style="text-align: center;">Shrimp scampi usually does not contain plastic, but contamination is possible. It is typically made with ingredients like shrimp, garlic, butter, and parsley, which do not contain plastic, and there are no known instances of shrimp scampi being made with plasticcontaining ingredients. Therefore, while shrimp scampi is usually made without plastic. So, the answer is: No.</td>
</tr>
</tbody>
</table>
<p>Figure 8: Comparison of the paths generated by CoT and SCoT in different domains.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">I will provide you with a math problem and 5 options. Please choose <br> the correct option from the five provided and indicate your answer <br> with [Answer]option[Answer], such as [Answer]C[Answer].</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Zero-shot CoT template</td>
<td style="text-align: left;">Please output the answer at the end in strict accordance with the <br> output format.</td>
</tr>
<tr>
<td style="text-align: left;">Problem: [Please Put Your Questions Here]</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Options: [Please Put Your Options Here]</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Answer: Let's think step by step.</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Figure 9: An example of prompting for standard zero-shot CoT</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"># Role <br> A highly skilled mathematician and algorithm expert.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"># Workflow</td>
</tr>
<tr>
<td style="text-align: left;">Zero-shot SCoT template</td>
<td style="text-align: left;">1. Analyze the problem and identify any relevant mathematical formulas, or <br> approaches that might be helpful, and select the approaches that can solve the <br> problem. <br> 2. Choose the most efficient and practical approach. For example, when asked to find <br> the sum of all integers from -25 to 23, consider using the summation formula of <br> arithmetic sequence instead of simply adding the numbers one by one. The <br> summation formula of arithmetic sequence is an elegant and practical solution, while <br> rudely adding the numbers is not. <br> 3. Solve the problem step by step following the selected approach carefully.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">## Rules</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">1. Avoid using brute force methods, as they do not reflect the professionalism. <br> 2. Indicate your answer with [Answer]option[Answer], such as [Answer]C[Answer]. <br> Please output the answer at the end in strict accordance with the output format.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">## Initialization</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">As <Role>, please follow <Rules> strictly. Your task is to solve the math problem <br> following <Workflow>.I will provide you with a problem and 5 options. Please choose <br> the correct option from the five provided.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Problem: [Please Put Your Questions Here]</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Options: [Please Put Your Options Here]</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Answer: Let's think step by step.</td>
</tr>
</tbody>
</table>
<p>Figure 10: An example of prompting for standard Strategic Chain-of-Thought in math reasoning tasks</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"># Role <br> An expert of world knowledge with strong logical skills.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"># Workflow</td>
</tr>
<tr>
<td style="text-align: left;">Zero-shot SCoT template</td>
<td style="text-align: left;">1. Analyze the problem and break down the complex query into simpler sub- <br> questions. <br> 2. Sequentially finding reliable answers for each sub-question. <br> 3. Integrating these answers to form a comprehensive. Directly answering the main <br> question is rude, but breaking it down, answering the sub-questions, and then <br> integrating the answers is elegant and practical.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">## Rules</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">1. Avoid using brute force methods, as they do not reflect the professionalism. <br> 2. Indicate your answer with [Answer]option[Answer], such as [Answer]C[Answer]. <br> Please output the answer at the end in strict accordance with the output format.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">## Initialization</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">As <Role>, please follow <Rules> strictly. Your task is to solve the problem following <br> <Workflow>.I will provide you with a problem and 5 options. Please choose the <br> correct option from the five provided.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Problem: [Please Put Your Questions Here]</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Options: [Please Put Your Options Here]</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Answer: Let's think step by step.</td>
</tr>
</tbody>
</table>
<p>Figure 11: An example of prompting for standard Strategic Chain-of-Thought in multi-hop reasoning tasks</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Zero-shot SCoT template</th>
<th style="text-align: center;"># Role <br> An expert with world knowledge and reasoning abilities. <br> # Workflow <br> 1. Understanding the Question: Identify key concepts and comprehend the question's context. Ensure you grasp the main idea and any analogies being used. Search for any concept, knowledge, or common sense related to the topic. <br> 2. Analyzing the Options: Read each choice carefully, understand its meaning, and relate it to the question's context to determine relevance. <br> 3. Logical Reasoning: Use logical reasoning to eliminate options that are clearly irrelevant or incorrect based on the question's context. Compare the remaining options to identify the one that best aligns with the question's requirements and the context provided. <br> ## Rules <br> 1. Avoid using brute force methods, as they do not reflect the professionalism. <br> 2. Indicate your answer with [Answer]option[Answer], such as [Answer]C[Answer]. <br> Please output the answer at the end in strict accordance with the output format. <br> ## Initialization <br> As <Role>, please follow <Rules> strictly. Your task is to solve the problem following <Workflow>.I will provide you with a problem and 5 options. Please choose the correct option from the five provided. <br> Problem: [Please Put Your Questions Here] <br> Options: [Please Put Your Options Here] <br> Answer: Let's think step by step.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Figure 12: An example of prompting for standard Strategic Chain-of-Thought in commonsense reasoning tasks</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Zero-shot SCoT template</th>
<th style="text-align: center;"># Role <br> A careful expert proficient in various world knowledge. <br> # Workflow <br> 1. Careful Question Analysis: <br> - Read the Problem and the Options Carefully: Ensure you understand the background and specific question being asked . <br> - Identify Keywords: Extract key terms or phrases from the Problem and the Options, try recalling their meanings. <br> - Understand the Problem: Ensure you clearly understand what the Problem is asking, including any specific conditions or requirements. Eliminate options that are not relevant to the problem. <br> 2. Identify Relevant Knowledge and approaches: <br> - Recall Related Knowledge or approach: Identify all the relevant concepts, principles, or formulas that might apply to the Problem. <br> - Select Appropriate Knowledge: Choose the knowledge, formulas and approaches that can solve the problem. <br> 3. Choose the Most Efficient and Practical Knowledge and Formulas: When solving the problem, select the most efficient and practical knowledge, formulas or approaches. For example, when the description of a problem is related to potential energy and kinetic energy of an object, after using the formula PE $=$ mgh, carefully analyze each option to judge right or wrong, rather than relying on experience or ready-made theorems to select options. <br> 4. Careful Application of Knowledge and Formulas: <br> - Detailed Analysis: When applying formulas and knowledge, pay attention to the specific conditions and variables in the problem. <br> - Logical Reasoning: Carefully analyze each variable in the formula or methodically derive conclusions based on the knowledge point, ensuring the reasoning process is consistent and correct. For example, when using PE $=$ mgh, you need to analyze the overall effect of all variables, including $m, g$, and $h$, rather than just one variable. <br> ## Rules <br> 1. Avoid using brute force methods, as they do not reflect the professionalism. <br> 2. Indicate your answer with [Answer]option[Answer], such as [Answer]C[Answer]. Please output the answer at the end in strict accordance with the output format. <br> ## Initialization <br> As <Role>, please follow <Rules> strictly. Your task is to solve the problem following <Workflow>.I will provide you with a problem and 5 options. Please choose the correct option from the five provided. <br> Problem: [Please Put Your Questions Here] <br> Options: [Please Put Your Options Here] <br> Answer: Let's think step by step.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Figure 13: An example of prompting for standard Strategic Chain-of-Thought in physical reasoning tasks</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"># Role <br> A very meticulous logical Analyst. <br> # Workflow <br> 1. Initial State: First, list the initial state of the balls each person has according to the problem statement. <br> 2. Process Exchanges: Next, carefully read the problem statement. For each exchange, update the current state of the balls and document the result of each exchange. <br> 3. Determine the Answer: Once all exchanges are completed, identify which friend's ball color is being inquired about in the problem statement and select the correct answer.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Zero-shot SCoT template</td>
<td style="text-align: center;">## Rules <br> 1. Avoid using brute force methods, as they do not reflect the professionalism. <br> 2. Indicate your answer with [Answer]option[Answer], such as [Answer]C[Answer]. Please output the answer at the end in strict accordance with the output format. <br> ## Initialization <br> As <Role>, please follow <Rules> strictly. Your task is to solve the problem following <br> <Workflow>.I will provide you with a problem and 5 options. Please choose the correct option from the five provided.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Problem: [Please Put Your Questions Here] <br> Options: [Please Put Your Options Here] <br> Answer: Let's think step by step.</td>
</tr>
</tbody>
</table>
<p>Figure 14: An example of prompting for standard Strategic Chain-of-Thought in spatial reasoning tasks</p>
<table>
<thead>
<tr>
<th style="text-align: center;">One-shot SCoT template</th>
<th style="text-align: center;"># Role <br> A highly skilled mathematician and algorithm expert. <br> # Workflow <br> 1. Analyze the problem and identify any relevant mathematical formulas, or approaches that might be helpful, and select the approaches that can solve the problem. <br> 2. Choose the most efficient and practical approach. For example, when asked to find the sum of all integers from -25 to 23 , consider using the summation formula of arithmetic sequence instead of simply adding the numbers one by one. The summation formula of arithmetic sequence is an elegant and practical solution, while rudely adding the numbers is not. <br> 3. Solve the problem step by step following the selected approach carefully. <br> ## Demonstrations <br> Problem: [Please Put Your Demonstration Problem Here] <br> Options: [Please Put Your Demonstration Options Here] <br> Answer: [Please Put Your Demonstration Answer Here] <br> ## Rules <br> 1. Avoid using brute force methods, as they do not reflect the professionalism. <br> 2. Indicate your answer with [Answer]option[Answer], such as [Answer]C[Answer]. <br> Please output the answer at the end in strict accordance with the output format. <br> ## Initialization <br> As <Role>, please follow <Rules> strictly. Your task is to solve the math problem following <Workflow>, <Demonstration> is some examples. I will provide you with a problem and 5 options. Please choose the correct option from the five provided.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Problem: [Please Put Your Questions Here] <br> Options: [Please Put Your Options Here] <br> Answer: Let's think step by step.</td>
</tr>
</tbody>
</table>
<p>Figure 15: An example of prompting for one-shot Strategic Chain-of-Thought</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">You are tasked with solving a reasoning problem by first identifying <br> the most effective strategy before arriving at the final answer. <br> Carefully consider the problem and generate the strategic knowledge <br> that would best guide the problem-solving process.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Auto Zero-shot SCoT <br> template</td>
<td style="text-align: left;">Problem: [Please Put Your Problem Here]</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Next, use the generated strategic knowledge to work through the <br> problem step by step, showing all necessary reasoning, and arrive at <br> the final solution.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Options: [Please Put Your Options Here] <br> Answer: Let's think step by step.</td>
</tr>
</tbody>
</table>
<p>Figure 16: An example of prompting for automatic Strategic Chain-of-Thought</p>            </div>
        </div>

    </div>
</body>
</html>
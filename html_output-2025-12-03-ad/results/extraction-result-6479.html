<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6479 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6479</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6479</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-270521583</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.10149v2.pdf" target="_blank">BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack</a></p>
                <p><strong>Paper Abstract:</strong> In recent years, the input context sizes of large language models (LLMs) have increased dramatically. However, existing evaluation methods have not kept pace, failing to comprehensively assess the efficiency of models in handling long contexts. To bridge this gap, we introduce the BABILong benchmark, designed to test language models' ability to reason across facts distributed in extremely long documents. BABILong includes a diverse set of 20 reasoning tasks, including fact chaining, simple induction, deduction, counting, and handling lists/sets. These tasks are challenging on their own, and even more demanding when the required facts are scattered across long natural text. Our evaluations show that popular LLMs effectively utilize only 10-20\% of the context and their performance declines sharply with increased reasoning complexity. Among alternatives to in-context reasoning, Retrieval-Augmented Generation methods achieve a modest 60\% accuracy on single-fact question answering, independent of context length. Among context extension methods, the highest performance is demonstrated by recurrent memory transformers after fine-tuning, enabling the processing of lengths up to 50 million tokens. The BABILong benchmark is extendable to any length to support the evaluation of new upcoming models with increased capabilities, and we provide splits up to 10 million token lengths.</p>
                <p><strong>Cost:</strong> 0.027</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6479.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6479.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RMT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent Memory Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer augmented with a small set of persistent memory tokens that are passed recurrently between segments so the model can aggregate and store information from previous segments, enabling reasoning over very long sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recurrent Memory Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RMT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Transformer backbone augmented with learned memory tokens that are appended to each segment and carried forward between segments; the model writes to and reads from these memory tokens via attention, so the recurrent hidden state acts as an episodic memory aggregator.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>137M (GPT-2 backbone in the experiments reported in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>recurrent learned memory tokens (episodic recurrent memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>learned memory token vectors (activations / token embeddings stored as memory tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>attention-based read/write: memory tokens are attended to within each segment (read) and are updated and passed forward between segments (write) — distinct attention patterns correspond to writing facts and later reading them to answer questions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BABILong (bAbI-derived QA tasks QA1–QA20; experiments reported for QA1–QA5 across very long contexts up to millions of tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>multi-hop / long-context reasoning (needle-in-a-haystack)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>After fine-tuning on BABILong, RMT (137M GPT-2 backbone) achieved strong accuracy on QA1–QA5 at 16K training length and maintained high performance with only marginal degradation up to 128K; it continued to perform well on far longer sequences (1M, 10M tokens) in evaluation — qualitatively reported as significantly outperforming non-recurrent baselines like GPT-4 and RAG on these tasks (paper reports recurrent models consistently outperform larger LLMs; exact per-task numbers are provided in the paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline non-recurrent LLMs (e.g., GPT-4, other large transformer models without recurrent memory) show sharp degradation as context length increases and effectively use only the first ~5–25% of the input; the paper does not present an ablation of RMT with memory disabled on the same architecture, but compares RMT results to non-recurrent LLM baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%) (average accuracy reported over QA1–QA5 and per-task accuracy reported in tables/figures)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Recurrence reduces parallelizability (sequential processing across segments) and so increases latency relative to fully parallel transformers; evaluation/training time grows roughly linearly with evaluated context length; memory capacity is finite (fixed number of memory tokens) so there is a storage/representation trade-off.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Sequential nature limits throughput and parallelism; finite memory token capacity means storage is limited and could be insufficient if too many facts must be preserved; performance variance across training seeds and curriculum stages observed (sensitivity to early stopping / training schedule).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Bulatov et al., 2022. Recurrent Memory Transformer. (cited in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6479.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6479.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ARMT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Associative Recurrent Memory Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An associative variant of the Recurrent Memory Transformer that uses associative operations over a small set of memory tokens to support long-range retrieval and reasoning; demonstrated to scale to extremely long sequences when fine-tuned on BABILong.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Associative recurrent memory transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ARMT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Associative RMT variant that uses a small number of memory tokens (10 in experiments) with associative mechanisms (and DPFP nonlinearity in experiments) to store and retrieve facts over long sequences; memory tokens are recurrently passed and accessed through attention/associative lookup.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>137M (GPT-2 backbone in experiments reported in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>associative recurrent memory tokens</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>learned memory token vectors (64-dim memory in experiments) stored as associative memory entries</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>attention-based reading and writing to memory tokens plus associative retrieval over past memory tokens (the paper describes read/write attention patterns and an associative retrieval mechanism)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BABILong (QA1–QA5 and extended tasks; evaluated up to tens of millions of tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>multi-hop / long-context reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Fine-tuned ARMT achieves strong performance across BABILong tasks and scales to extreme sequence lengths (paper reports ARMT successfully scales to 50 million tokens); ARMT and RMT significantly outperform retrieval-augmented baselines on BABILong tasks after fine-tuning (qualitative and per-task numbers in tables).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Non-recurrent LLM baselines (GPT-4, other LLMs without ARMT-style memory) show large degradation with length; paper does not report an exact ablation of ARMT with memory mechanism removed.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%) (per-task and averaged metrics reported in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Sequential processing lowers parallelism (latency) though memory requirements remain constant; training requires curriculum scheduling and hyperparameter tuning; performance can vary by random seed and curriculum choices.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Finite storage capacity of memory tokens limits the total facts storable; performance sensitive to curriculum and early stopping choices; ARMT may be outperformed by Mamba on some medium-length complex tasks (e.g., QA3) per the paper's analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Rodkin et al., 2024. Associative recurrent memory transformer. (cited in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6479.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6479.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mamba</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mamba (SSM / recurrent-style long-sequence model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-space / recurrent-style model (SSM family) designed for long sequences that achieves strong accuracy on medium-length BABILong tasks after fine-tuning, but is slow in inference beyond ~128K tokens in the implementation used in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Mamba</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An SSM-style / recurrent long-sequence model (selective state-space architecture) that maintains a continuous state across long inputs instead of explicit external memory tokens; in this paper Mamba-130M is fine-tuned on BABILong with curriculum training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>130M (in experiments reported in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>implicit recurrent / state-space internal state (SSM implicit memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>continuous state-space activations (internal state vectors) propagated across segments</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>implicit recurrence / state updates via SSM-style dynamics rather than explicit attention-based memory read/write</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BABILong (QA1–QA5; medium and larger contexts up to 128K and evaluated beyond for some tests)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>multi-hop / long-context reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Fine-tuned Mamba achieves outstanding scores across most sequence lengths on BABILong and has an edge on medium-length sequences and on some complex tasks (e.g., remembering many facts in QA3) — the paper reports Mamba among top-performing fine-tuned small models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Mamba's architecture intrinsically uses its state (no explicit ablation provided); compared to non-SSM LLMs, non-SSM baselines show worse long-context performance.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%) (per-task values in paper tables)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Practical inference of Mamba beyond 128K was extremely slow in the implementation used, making it nearly impossible to use for very long sequences despite good accuracy at medium lengths (implementation and runtime trade-off).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Severe inference latency / practical infeasibility beyond 128K tokens in the reported implementation; slower than recurrent-memory models (RMT/ARMT) at very long lengths despite strong accuracy on medium-length tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6479.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6479.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG (GPT-4 / Llama-3 pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (chunk-based RAG-C and sentence-based RAG-S pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard RAG pipelines combining an external retriever (FAISS + embeddings) with an LLM (GPT-4-turbo or Llama-3 variants) to fetch relevant passages from a long input and append them to the model context; evaluated in chunk (512-token) and sentence retrieval modes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAG (RAG-C / RAG-S)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A two-stage pipeline: (1) an embedding-based retriever (FAISS with text-embedding-ada-002 or other encoders) selects top-k chunks/sentences (either fixed 512-token chunks or sentence-level chunks), (2) retrieved text is concatenated with the query and passed to an LLM for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Used with GPT-4-turbo and Llama-3 variants (sizes vary; experiments used both closed and open models)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external vector store (FAISS) acting as retrieval memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>text embeddings and retrieved raw text passages</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>similarity search (top-k retrieval) over embeddings, followed by concatenation of retrieved passages into model input (no temporal-order preservation by default)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BABILong (QA1–QA5; extended QA1 analysis up to 10M tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>retrieval / long-context QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Sentence-based retrieval (RAG-S) with GPT-4 achieved modest but scalable performance on single-fact QA1 (≈60% accuracy reported for single-fact QA independent of context length); chunk-based retrieval (RAG-C, 512-token chunks) showed poorer scalability and accuracy in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Base GPT-4 without RAG performs well on short contexts but degrades strongly with increasing context length and effectively uses only the first ~10–20% of long inputs; in multi-fact tasks (QA2/QA3), RAG did not help and often failed while base LLMs likewise failed at long lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%) (retrieval accuracy evaluated as binary presence of relevant facts in top-5 chunks; final QA accuracy reported for models augmented with retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Retrieval loses temporal / ordering information (order of facts matters in BABILong), chunking granularity affects retrieval quality (sentence vs 512-token), retrieval latency and dependence on embedding quality; retrieval component in experiments was not exhaustively optimized (authors note this as a limitation).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>RAG fails dramatically on multi-supporting-fact tasks (QA2 and QA3) because the retriever does not reliably return all supporting facts and does not preserve their temporal ordering; similarity-based retrieval can miss facts that are not semantically similar to the question.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Guu et al., 2020. Retrieval-Augmented Language Model Pre-Training. (cited in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6479.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6479.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Activation Beacon</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Activation Beacon</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A context-extension method that compresses activations from prior segments into compact 'beacon' vectors and integrates them via a sliding window mechanism to extend effective context length (claimed scalability up to ~400K tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Activation Beacon</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Activation Beacon</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Compresses activations from prior segments with a separate set of parameters into compact beacon vectors, which are then integrated with the model via a sliding-window mechanism to provide compressed long-range context.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Applied to Mistral 7B in the paper's evaluations (as Activation Beacon adaptation for that model)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>compressed activation memory (summary/beacon vectors)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>compressed activation vectors (beacon tokens summarizing prior segments)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>compressed activations are computed and integrated via a sliding-window mechanism across segments (learned compression and integration parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BABILong (QA1–QA5; evaluated up to 32K and discussed up to 400K capability)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>context-extension / long-context reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Activation Beacon performed better than the Yarn method for Mistral 7B on BABILong, but overall still achieved relatively low scores on 32K contexts (<40% on some tasks as reported in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Yarn and other simple extension methods performed worse than Activation Beacon for the same backbone model; raw Mistral without any extension performed poorly at longer lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Compression trades fidelity for scalability — compressed beacons can lose information needed for fine-grained multi-hop reasoning; sliding-window and compression parameters are a trade-off between capacity and information retention.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Despite extending effective context, Activation Beacon still yielded low accuracy (<40%) on 32K contexts for BABILong tasks; compression can lose facts and order-sensitive information necessary for multi-hop reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Zhang et al., 2024a. Activation Beacon (cited in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6479.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6479.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RMT + self-retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent Memory Transformer augmented with trainable retrieval of its own past memory tokens</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Novel augmentation described in this paper that equips a recurrent transformer with a trainable retrieval mechanism over its own past memory tokens, combining recurrence and retrieval to potentially improve associative recall over very long sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RMT + self-retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>RMT extended with a component that can retrieve (via a trainable retriever) previously written memory token vectors stored by the model, enabling associative lookup among the model's own memory entries in addition to standard recurrent read/write.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>137M (GPT-2 backbone in the experiments described)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>internal recurrent memory tokens combined with an internal trainable retrieval index</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>memory token vectors stored during processing (internal keys/values derived from memory tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>trainable retrieval over stored internal memory tokens (learned similarity/read mechanism) combined with the usual attention-based read/write of recurrent memory tokens</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BABILong (QA1–QA5 and extended long-context evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>multi-hop / long-context reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Paper states that augmenting RMT with the ability to retrieve past memory tokens was implemented and that recurrent memory models (including ARMT/RMT variants) outperform RAG and scale to very long lengths (ARMT up to 50M tokens). The paper does not provide a separate isolated numeric ablation for the self-retrieval augmentation alone.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Standard RMT/ARMT (recurrent-only) already show strong scaling; the paper does not report a direct ablation isolating the contribution of self-retrieval versus plain recurrent memory.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (%) (reported for RMT/ARMT family overall; no isolated metric for the augmentation alone)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Adds architectural complexity and training considerations; potential benefits for associative recall but no isolated cost/benefit numbers provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No dedicated ablation presented in the paper isolating failures of the retrieval augmentation; general recurrent model limitations (sequential latency, finite storage) still apply.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Kuratov et al., 2024. BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack. NeurIPS 2024 Track on Datasets and Benchmarks / arXiv:2406.10149v2.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Recurrent Memory Transformer <em>(Rating: 2)</em></li>
                <li>Associative recurrent memory transformer <em>(Rating: 2)</em></li>
                <li>Linear-time sequence modeling with selective state spaces <em>(Rating: 2)</em></li>
                <li>Retrieval-Augmented Language Model Pre-Training <em>(Rating: 2)</em></li>
                <li>Activation Beacon <em>(Rating: 2)</em></li>
                <li>Memorizing Transformers <em>(Rating: 1)</em></li>
                <li>AutoCompressor <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6479",
    "paper_id": "paper-270521583",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "RMT",
            "name_full": "Recurrent Memory Transformer",
            "brief_description": "A transformer augmented with a small set of persistent memory tokens that are passed recurrently between segments so the model can aggregate and store information from previous segments, enabling reasoning over very long sequences.",
            "citation_title": "Recurrent Memory Transformer",
            "mention_or_use": "use",
            "agent_name": "RMT",
            "agent_description": "Transformer backbone augmented with learned memory tokens that are appended to each segment and carried forward between segments; the model writes to and reads from these memory tokens via attention, so the recurrent hidden state acts as an episodic memory aggregator.",
            "model_size": "137M (GPT-2 backbone in the experiments reported in this paper)",
            "memory_used": true,
            "memory_type": "recurrent learned memory tokens (episodic recurrent memory)",
            "memory_representation": "learned memory token vectors (activations / token embeddings stored as memory tokens)",
            "memory_access_mechanism": "attention-based read/write: memory tokens are attended to within each segment (read) and are updated and passed forward between segments (write) — distinct attention patterns correspond to writing facts and later reading them to answer questions.",
            "task_name": "BABILong (bAbI-derived QA tasks QA1–QA20; experiments reported for QA1–QA5 across very long contexts up to millions of tokens)",
            "task_category": "multi-hop / long-context reasoning (needle-in-a-haystack)",
            "performance_with_memory": "After fine-tuning on BABILong, RMT (137M GPT-2 backbone) achieved strong accuracy on QA1–QA5 at 16K training length and maintained high performance with only marginal degradation up to 128K; it continued to perform well on far longer sequences (1M, 10M tokens) in evaluation — qualitatively reported as significantly outperforming non-recurrent baselines like GPT-4 and RAG on these tasks (paper reports recurrent models consistently outperform larger LLMs; exact per-task numbers are provided in the paper tables).",
            "performance_without_memory": "Baseline non-recurrent LLMs (e.g., GPT-4, other large transformer models without recurrent memory) show sharp degradation as context length increases and effectively use only the first ~5–25% of the input; the paper does not present an ablation of RMT with memory disabled on the same architecture, but compares RMT results to non-recurrent LLM baselines.",
            "has_comparative_results": true,
            "performance_metric": "Accuracy (%) (average accuracy reported over QA1–QA5 and per-task accuracy reported in tables/figures)",
            "tradeoffs_reported": "Recurrence reduces parallelizability (sequential processing across segments) and so increases latency relative to fully parallel transformers; evaluation/training time grows roughly linearly with evaluated context length; memory capacity is finite (fixed number of memory tokens) so there is a storage/representation trade-off.",
            "limitations_or_failure_cases": "Sequential nature limits throughput and parallelism; finite memory token capacity means storage is limited and could be insufficient if too many facts must be preserved; performance variance across training seeds and curriculum stages observed (sensitivity to early stopping / training schedule).",
            "citation": "Bulatov et al., 2022. Recurrent Memory Transformer. (cited in the paper)",
            "uuid": "e6479.0",
            "source_info": {
                "paper_title": "BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ARMT",
            "name_full": "Associative Recurrent Memory Transformer",
            "brief_description": "An associative variant of the Recurrent Memory Transformer that uses associative operations over a small set of memory tokens to support long-range retrieval and reasoning; demonstrated to scale to extremely long sequences when fine-tuned on BABILong.",
            "citation_title": "Associative recurrent memory transformer",
            "mention_or_use": "use",
            "agent_name": "ARMT",
            "agent_description": "Associative RMT variant that uses a small number of memory tokens (10 in experiments) with associative mechanisms (and DPFP nonlinearity in experiments) to store and retrieve facts over long sequences; memory tokens are recurrently passed and accessed through attention/associative lookup.",
            "model_size": "137M (GPT-2 backbone in experiments reported in this paper)",
            "memory_used": true,
            "memory_type": "associative recurrent memory tokens",
            "memory_representation": "learned memory token vectors (64-dim memory in experiments) stored as associative memory entries",
            "memory_access_mechanism": "attention-based reading and writing to memory tokens plus associative retrieval over past memory tokens (the paper describes read/write attention patterns and an associative retrieval mechanism)",
            "task_name": "BABILong (QA1–QA5 and extended tasks; evaluated up to tens of millions of tokens)",
            "task_category": "multi-hop / long-context reasoning",
            "performance_with_memory": "Fine-tuned ARMT achieves strong performance across BABILong tasks and scales to extreme sequence lengths (paper reports ARMT successfully scales to 50 million tokens); ARMT and RMT significantly outperform retrieval-augmented baselines on BABILong tasks after fine-tuning (qualitative and per-task numbers in tables).",
            "performance_without_memory": "Non-recurrent LLM baselines (GPT-4, other LLMs without ARMT-style memory) show large degradation with length; paper does not report an exact ablation of ARMT with memory mechanism removed.",
            "has_comparative_results": true,
            "performance_metric": "Accuracy (%) (per-task and averaged metrics reported in the paper)",
            "tradeoffs_reported": "Sequential processing lowers parallelism (latency) though memory requirements remain constant; training requires curriculum scheduling and hyperparameter tuning; performance can vary by random seed and curriculum choices.",
            "limitations_or_failure_cases": "Finite storage capacity of memory tokens limits the total facts storable; performance sensitive to curriculum and early stopping choices; ARMT may be outperformed by Mamba on some medium-length complex tasks (e.g., QA3) per the paper's analyses.",
            "citation": "Rodkin et al., 2024. Associative recurrent memory transformer. (cited in the paper)",
            "uuid": "e6479.1",
            "source_info": {
                "paper_title": "BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Mamba",
            "name_full": "Mamba (SSM / recurrent-style long-sequence model)",
            "brief_description": "A state-space / recurrent-style model (SSM family) designed for long sequences that achieves strong accuracy on medium-length BABILong tasks after fine-tuning, but is slow in inference beyond ~128K tokens in the implementation used in this work.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Mamba",
            "agent_description": "An SSM-style / recurrent long-sequence model (selective state-space architecture) that maintains a continuous state across long inputs instead of explicit external memory tokens; in this paper Mamba-130M is fine-tuned on BABILong with curriculum training.",
            "model_size": "130M (in experiments reported in this paper)",
            "memory_used": true,
            "memory_type": "implicit recurrent / state-space internal state (SSM implicit memory)",
            "memory_representation": "continuous state-space activations (internal state vectors) propagated across segments",
            "memory_access_mechanism": "implicit recurrence / state updates via SSM-style dynamics rather than explicit attention-based memory read/write",
            "task_name": "BABILong (QA1–QA5; medium and larger contexts up to 128K and evaluated beyond for some tests)",
            "task_category": "multi-hop / long-context reasoning",
            "performance_with_memory": "Fine-tuned Mamba achieves outstanding scores across most sequence lengths on BABILong and has an edge on medium-length sequences and on some complex tasks (e.g., remembering many facts in QA3) — the paper reports Mamba among top-performing fine-tuned small models.",
            "performance_without_memory": "Mamba's architecture intrinsically uses its state (no explicit ablation provided); compared to non-SSM LLMs, non-SSM baselines show worse long-context performance.",
            "has_comparative_results": true,
            "performance_metric": "Accuracy (%) (per-task values in paper tables)",
            "tradeoffs_reported": "Practical inference of Mamba beyond 128K was extremely slow in the implementation used, making it nearly impossible to use for very long sequences despite good accuracy at medium lengths (implementation and runtime trade-off).",
            "limitations_or_failure_cases": "Severe inference latency / practical infeasibility beyond 128K tokens in the reported implementation; slower than recurrent-memory models (RMT/ARMT) at very long lengths despite strong accuracy on medium-length tasks.",
            "citation": "",
            "uuid": "e6479.2",
            "source_info": {
                "paper_title": "BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "RAG (GPT-4 / Llama-3 pipelines)",
            "name_full": "Retrieval-Augmented Generation (chunk-based RAG-C and sentence-based RAG-S pipelines)",
            "brief_description": "Standard RAG pipelines combining an external retriever (FAISS + embeddings) with an LLM (GPT-4-turbo or Llama-3 variants) to fetch relevant passages from a long input and append them to the model context; evaluated in chunk (512-token) and sentence retrieval modes.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "RAG (RAG-C / RAG-S)",
            "agent_description": "A two-stage pipeline: (1) an embedding-based retriever (FAISS with text-embedding-ada-002 or other encoders) selects top-k chunks/sentences (either fixed 512-token chunks or sentence-level chunks), (2) retrieved text is concatenated with the query and passed to an LLM for generation.",
            "model_size": "Used with GPT-4-turbo and Llama-3 variants (sizes vary; experiments used both closed and open models)",
            "memory_used": true,
            "memory_type": "external vector store (FAISS) acting as retrieval memory",
            "memory_representation": "text embeddings and retrieved raw text passages",
            "memory_access_mechanism": "similarity search (top-k retrieval) over embeddings, followed by concatenation of retrieved passages into model input (no temporal-order preservation by default)",
            "task_name": "BABILong (QA1–QA5; extended QA1 analysis up to 10M tokens)",
            "task_category": "retrieval / long-context QA",
            "performance_with_memory": "Sentence-based retrieval (RAG-S) with GPT-4 achieved modest but scalable performance on single-fact QA1 (≈60% accuracy reported for single-fact QA independent of context length); chunk-based retrieval (RAG-C, 512-token chunks) showed poorer scalability and accuracy in experiments.",
            "performance_without_memory": "Base GPT-4 without RAG performs well on short contexts but degrades strongly with increasing context length and effectively uses only the first ~10–20% of long inputs; in multi-fact tasks (QA2/QA3), RAG did not help and often failed while base LLMs likewise failed at long lengths.",
            "has_comparative_results": true,
            "performance_metric": "Accuracy (%) (retrieval accuracy evaluated as binary presence of relevant facts in top-5 chunks; final QA accuracy reported for models augmented with retrieval)",
            "tradeoffs_reported": "Retrieval loses temporal / ordering information (order of facts matters in BABILong), chunking granularity affects retrieval quality (sentence vs 512-token), retrieval latency and dependence on embedding quality; retrieval component in experiments was not exhaustively optimized (authors note this as a limitation).",
            "limitations_or_failure_cases": "RAG fails dramatically on multi-supporting-fact tasks (QA2 and QA3) because the retriever does not reliably return all supporting facts and does not preserve their temporal ordering; similarity-based retrieval can miss facts that are not semantically similar to the question.",
            "citation": "Guu et al., 2020. Retrieval-Augmented Language Model Pre-Training. (cited in the paper)",
            "uuid": "e6479.3",
            "source_info": {
                "paper_title": "BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Activation Beacon",
            "name_full": "Activation Beacon",
            "brief_description": "A context-extension method that compresses activations from prior segments into compact 'beacon' vectors and integrates them via a sliding window mechanism to extend effective context length (claimed scalability up to ~400K tokens).",
            "citation_title": "Activation Beacon",
            "mention_or_use": "use",
            "agent_name": "Activation Beacon",
            "agent_description": "Compresses activations from prior segments with a separate set of parameters into compact beacon vectors, which are then integrated with the model via a sliding-window mechanism to provide compressed long-range context.",
            "model_size": "Applied to Mistral 7B in the paper's evaluations (as Activation Beacon adaptation for that model)",
            "memory_used": true,
            "memory_type": "compressed activation memory (summary/beacon vectors)",
            "memory_representation": "compressed activation vectors (beacon tokens summarizing prior segments)",
            "memory_access_mechanism": "compressed activations are computed and integrated via a sliding-window mechanism across segments (learned compression and integration parameters)",
            "task_name": "BABILong (QA1–QA5; evaluated up to 32K and discussed up to 400K capability)",
            "task_category": "context-extension / long-context reasoning",
            "performance_with_memory": "Activation Beacon performed better than the Yarn method for Mistral 7B on BABILong, but overall still achieved relatively low scores on 32K contexts (&lt;40% on some tasks as reported in the paper).",
            "performance_without_memory": "Yarn and other simple extension methods performed worse than Activation Beacon for the same backbone model; raw Mistral without any extension performed poorly at longer lengths.",
            "has_comparative_results": true,
            "performance_metric": "Accuracy (%)",
            "tradeoffs_reported": "Compression trades fidelity for scalability — compressed beacons can lose information needed for fine-grained multi-hop reasoning; sliding-window and compression parameters are a trade-off between capacity and information retention.",
            "limitations_or_failure_cases": "Despite extending effective context, Activation Beacon still yielded low accuracy (&lt;40%) on 32K contexts for BABILong tasks; compression can lose facts and order-sensitive information necessary for multi-hop reasoning.",
            "citation": "Zhang et al., 2024a. Activation Beacon (cited in the paper)",
            "uuid": "e6479.4",
            "source_info": {
                "paper_title": "BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "RMT + self-retrieval",
            "name_full": "Recurrent Memory Transformer augmented with trainable retrieval of its own past memory tokens",
            "brief_description": "Novel augmentation described in this paper that equips a recurrent transformer with a trainable retrieval mechanism over its own past memory tokens, combining recurrence and retrieval to potentially improve associative recall over very long sequences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RMT + self-retrieval",
            "agent_description": "RMT extended with a component that can retrieve (via a trainable retriever) previously written memory token vectors stored by the model, enabling associative lookup among the model's own memory entries in addition to standard recurrent read/write.",
            "model_size": "137M (GPT-2 backbone in the experiments described)",
            "memory_used": true,
            "memory_type": "internal recurrent memory tokens combined with an internal trainable retrieval index",
            "memory_representation": "memory token vectors stored during processing (internal keys/values derived from memory tokens)",
            "memory_access_mechanism": "trainable retrieval over stored internal memory tokens (learned similarity/read mechanism) combined with the usual attention-based read/write of recurrent memory tokens",
            "task_name": "BABILong (QA1–QA5 and extended long-context evaluation)",
            "task_category": "multi-hop / long-context reasoning",
            "performance_with_memory": "Paper states that augmenting RMT with the ability to retrieve past memory tokens was implemented and that recurrent memory models (including ARMT/RMT variants) outperform RAG and scale to very long lengths (ARMT up to 50M tokens). The paper does not provide a separate isolated numeric ablation for the self-retrieval augmentation alone.",
            "performance_without_memory": "Standard RMT/ARMT (recurrent-only) already show strong scaling; the paper does not report a direct ablation isolating the contribution of self-retrieval versus plain recurrent memory.",
            "has_comparative_results": false,
            "performance_metric": "Accuracy (%) (reported for RMT/ARMT family overall; no isolated metric for the augmentation alone)",
            "tradeoffs_reported": "Adds architectural complexity and training considerations; potential benefits for associative recall but no isolated cost/benefit numbers provided in the paper.",
            "limitations_or_failure_cases": "No dedicated ablation presented in the paper isolating failures of the retrieval augmentation; general recurrent model limitations (sequential latency, finite storage) still apply.",
            "citation": "Kuratov et al., 2024. BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack. NeurIPS 2024 Track on Datasets and Benchmarks / arXiv:2406.10149v2.",
            "uuid": "e6479.5",
            "source_info": {
                "paper_title": "BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Recurrent Memory Transformer",
            "rating": 2,
            "sanitized_title": "recurrent_memory_transformer"
        },
        {
            "paper_title": "Associative recurrent memory transformer",
            "rating": 2,
            "sanitized_title": "associative_recurrent_memory_transformer"
        },
        {
            "paper_title": "Linear-time sequence modeling with selective state spaces",
            "rating": 2,
            "sanitized_title": "lineartime_sequence_modeling_with_selective_state_spaces"
        },
        {
            "paper_title": "Retrieval-Augmented Language Model Pre-Training",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_language_model_pretraining"
        },
        {
            "paper_title": "Activation Beacon",
            "rating": 2,
            "sanitized_title": "activation_beacon"
        },
        {
            "paper_title": "Memorizing Transformers",
            "rating": 1,
            "sanitized_title": "memorizing_transformers"
        },
        {
            "paper_title": "AutoCompressor",
            "rating": 1,
            "sanitized_title": "autocompressor"
        }
    ],
    "cost": 0.027239,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack</p>
<p>Yuri Kuratov yurii.kuratov@phystech.edu 
AIRI
MoscowRussia</p>
<p>Neural Networks and Deep Learning Lab
MIPT
DolgoprudnyRussia</p>
<p>Aydar Bulatov bulatov.as@phystech.edu 
Neural Networks and Deep Learning Lab
MIPT
DolgoprudnyRussia</p>
<p>Petr Anokhin 
AIRI
MoscowRussia</p>
<p>Ivan Rodkin 
Neural Networks and Deep Learning Lab
MIPT
DolgoprudnyRussia</p>
<p>Dmitry Sorokin 
AIRI
MoscowRussia</p>
<p>Artyom Sorokin 
Mikhail Burtsev 
AIRI
MoscowRussia</p>
<p>London Institute for Mathematical Sciences
LondonUK</p>
<p>2-10 1 0 50 100QA10 QA9 QA8 QA7 QA6 QA5 QA4 QA3 QA2 QA1 80 89 77 28 91 80 55 38 64 99</p>
<p>BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack
25A244A93A5F5ED8AB2870CD0BC83164
In recent years, the input context sizes of large language models (LLMs) have increased dramatically.However, existing evaluation methods have not kept pace, failing to comprehensively assess the efficiency of models in handling long contexts.To bridge this gap, we introduce the BABILong benchmark, designed to test language models' ability to reason across facts distributed in extremely long documents.BABILong includes a diverse set of 20 reasoning tasks, including fact chaining, simple induction, deduction, counting, and handling lists/sets.These tasks are challenging on their own, and even more demanding when the required facts are scattered across long natural text.Our evaluations show that popular LLMs effectively utilize only 10-20% of the context and their performance declines sharply with increased reasoning complexity.Among alternatives to incontext reasoning, Retrieval-Augmented Generation methods achieve a modest 60% accuracy on single-fact question answering, independent of context length.Among context extension methods, the highest performance is demonstrated by recurrent memory transformers after fine-tuning, enabling the processing of lengths up to 50 million tokens.The BABILong benchmark is extendable to any length to support the evaluation of new upcoming models with increased capabilities, and we provide splits up to 10 million token lengths.TASK NAME FACTS RELEVANT FACTS LLMS ANSWER ACCURACY PER TASK PER TASK WITHOUT BACKGROUND TEXT (0K) QA1 SINGLE SUPPORTING FACT</p>
<p>Introduction</p>
<p>Today, large language models (LLMs) and neural architectures are continually evolving and achieving remarkable improvements, particularly in their ability to handle longer contexts (OpenAI, 2023b;Reid et al., 2024;Anthropic, 2024).The ability of these models to process and generate text based on rich contextual information is crucial for several reasons.For example, longer contexts provide more information for the model to condition its outputs, leading to more accurate, contextually relevant, and up-to-date responses.Furthermore, long-context capabilities can enhance in-context learning by providing more in-context examples, instructions to follow, or example trajectories in context of reinforcement learning (Chevalier et al., 2023;Agarwal et al., 2024;Lee et al., 2024).Despite these advances in models capabilities, the benchmarks used to evaluate them have not kept pace.For example, current benchmarks such as Longbench (Bai et al., 2023) and L-Eval An et al. (2023) scale only up to 40,000 tokens, while models are capable of hundreds of thousands and millions of tokens (Rodkin et al., 2024;Reid et al., 2024;Bulatov et al., 2024;Anthropic, 2024;Liu et al., 2024a;Gu &amp; Dao, 2023;OpenAI, 2023a).</p>
<p>Creating natural and comprehensive long-context benchmarks that are human labeled is very challenging.As a consequence, synthetic benchmarks focusing on variations of "needle-in-a-haystack"  ) Recurrent transformers answer questions about facts from very long texts when retrieval augmented generation fails.Common RAG method fails to answer questions because order of facts matters.GPT-4 effectively uses only about 10% of the full 128K window.Gemini 1.5 Pro shows strong performance up to 64K tokens.Small LMs, ARMT &amp; RMT with GPT-2 (137M) and Mamba (130M) fine-tuned for the task are able to solve it, with recurrent memory transformers scoring well up to record 50 000 000 tokens.</p>
<p>Here we show the best results obtained by models.</p>
<p>tasks have become increasingly common (Zhang et al., 2024b;Liu et al., 2024a;Song et al., 2024b;Hsieh et al., 2024).One widely used needle-in-a-haystack task involves finding specific "needles with magic numbers" in a haystack of Paul Graham's essays1 .However, the widespread use of this approach has highlighted its limitations -it is overly simplistic, and novel long context models often achieve perfect performance, as usually demonstrated by fully green heatmaps (Reid et al., 2024;Cohere, 2024;Liu et al., 2024a;Wang et al., 2024c).This shows that while it serves well as a basic verification tool, it is not a rigorous benchmark that can effectively challenge and differentiate advanced long-context models.Another major drawback of the original default setup 1 is that model predictions are evaluated and scored by an LLM (GPT-3.5-turbo) on a scale of 1 to 10, with the same single needle used for each position and document length.While averaging over multiple different needles can provide more robust results.</p>
<p>To bridge this gap, we introduce the BABILong benchmark, designed to test language models' ability to reason across facts distributed in extremely long documents.BABILong includes a diverse set of 20 reasoning tasks, including fact chaining, simple induction, deduction, counting, and handling lists/sets, that were designed as prerequisites for any system that aims to be capable of conversing with a human (Weston et al., 2016).As a source of long natural documents we use books from PG19 corpora (Rae et al., 2020).In this way, BABILong allows the construction of tasks of almost arbitrary length, in order to adapt them to the evaluation of new, more powerful models in an extensible and controllable way.We provide sets of predefined lengths with splits up to 10 million tokens, and we evaluate models on samples with up to 50 million tokens.</p>
<p>We find that popular LLMs effectively use only 10-20% of the context, with performance declining sharply as length and task complexity increase.Retrieval-Augmented Generation methods achieve a modest 60% accuracy in answering single-fact questions, regardless of context length.Among other methods, Mamba and Recurrent Memory Transformers (RMT and ARMT) show the highest performance, with ARMT capable of processing lengths up to 50 million tokens.</p>
<p>The main contributions of our work are as follows:</p>
<ol>
<li>
<p>We introduce BABILong, a novel scalable generative multi-task benchmark for evaluating the performance of NLP models in processing arbitrarily long documents with distributed facts.</p>
</li>
<li>
<p>We evaluate over 30 recent long-input language models with various sizes, architectures, and context extension methods on BABILong.</p>
</li>
</ol>
<p>Table 1: The first ten tasks of BABILong with the number of supporting and distracting facts.The last column displays the performance of LLMs on each task in the absence of background text.Each dot represents one of the selected models, while the blue bars indicate the median accuracy across tested models.</p>
<p>The BABILong Benchmark for Long Context Processing</p>
<p>The fundamental concept behind the Benchmark for Artificial Intelligence for Long-context evaluation is extending the length of existing tasks to test the ability of generative models in handling long contexts.Solving tasks with a long context size requires the model to distinguish important information from large amounts of irrelevant details.To simulate this behavior we "hide" the sentences of the original task between the sentences of irrelevant text that is drawn from another closely related distribution (see Figure 1a).Examples are constructed by gradually adding new sentences from the background dataset in their natural order until the augmented sample reaches the desired length.This way, we are not bound by the length of the original task itself, making it possible to assess even the longest available models with context sizes up to millions of tokens.For background text we use books from the PG19 dataset (Rae et al., 2020) due to the substantial book lengths and naturally occurring long contexts.The model is required first to distinguish the sentences related to the original task, then memorize and subsequently utilize them to generate the correct solution.</p>
<p>In this work we extend the bAbI benchmark (Weston et al., 2016), which consists of 20 tasks designed to evaluate basic aspects of reasoning.These tasks are generated by simulating interactions among characters and objects across various locations, each represented as a fact, such as "Mary traveled to the office."The challenge is to answer questions based on the facts generated in the current simulation, such as "Where is Mary?"The tasks in bAbI vary in the number of facts, question complexity, and the reasoning skills they assess, including spatial and temporal reasoning, deduction, and coreference resolution.In our paper, we label these tasks from 'QA1' to 'QA20'.The first ten tasks, as shown in Table 1 demonstrate that current LLMs exhibit mixed performance even without distractor texts, indicating that the BABILong tasks span a broad spectrum of difficulty and allow for testing models across various performance dimensions.Details and performance metrics for the bAbI tasks, along with examples of BABILong samples generated using our pipeline, can be found in Appendix L.</p>
<p>As evident in the following sections, these seemingly simple tasks pose significant challenges to language models.Although filtering facts from background text might be straightforward, models encounter next challenges of finding supporting facts among distractors and performing types of reasoning such as counting that are especially difficult for LLMs.Additionally, most NLP benchmarks Table 2: BABILong is a challenging benchmark for current long-context models.Even models that claim to support 128K tokens experience degradation beyond 10% of their input capacity.RAG methods do not help, while fine-tuning of small scale models (ARMT and RMT,137M and Mamba,130M) shows that the tasks are solvable.Values represent average accuracy over QA1-QA5 tasks from BABILong.Models are grouped by the length of the context they claim to support.are vulnerable to data leakage to training sets of modern large language models (Sainz et al., 2023).Generated benchmarks, such as bAbI and BABILong, are immune to this type of contamination.</p>
<p>In this work we deliberately employ simple algorithmic tasks to underscore the fundamental limitations of current models in collecting evidence over long contexts even for basic reasoning.The brevity and similarity of the task sentences also enable the model distinguish them from seemingly close background text with the assistance of few-shot examples.This difference in distributions enables the scalability of BABILong to large amounts of diverse noise.Nevertheless, the BABILong approach can be applied to incorporate more complex tasks, using the same strategy of mixing task sentences with background text.</p>
<p>Benchmarking Results</p>
<p>To maximize value for the research community, we have included models with the highest number of monthly downloads 3 from the Hugging Face platform in our evaluation such as LLama-3 (AI@Meta, 2024); 32k-64k -Mistral (Jiang et al., 2023), Mixtral (Jiang et al., 2024); 128k -ChatGLM3 (Du et al., 2022), LLama-3.1 (Dubey et al., 2024), Phi-3 and Phi-3.5 (Abdin et al., 2024), Command-R (Cohere, 2024), Qwen-2.5 (Team, 2024);200k -Yi (Young et al., 2024); including long-context fine-tuning: LongChat (Li et al., 2023a), LLama-2-7b-32k 4 , LongAlpaca (Chen et al., 2023); long-context adaptation methods: Yarnv2 Mistral (Peng et al., 2023b), Mistral and LLama-2 with Activation Beacons (Zhang et al., 2024a).As a reference, we included GPT-4 (gpt-4-0125-preview) and Gemini 1.5 Pro 002, currently the most powerful model available.Retrieval-augmented generation was also tested, as it represents a common solution for long document QA.As alternatives to traditional architectures, we considered the Mamba (Gu &amp; Dao, 2023), Jamba (Lieber et al., 2024), Recurrent Memory Transformer (RMT) (Bulatov et al., 2022), and Associative RMT (ARMT) (Rodkin et al., 2024).Summary of evaluation results is presented in the  LLMs struggle to answer questions about facts in texts larger than 10,000 tokens.The plots demonstrate how the performance of selected leading models deteriorates with increasing context size.For single supporting fact questions (QA1), the majority of models perform well up to 4,000 tokens.However, when a correct response requires two (QA2) or three (QA3) facts, LLMs fail to achieve satisfactory accuracy.</p>
<p>Evaluation of Effective Context Size</p>
<p>One of the most important questions regarding performance of long-context models is how effectively they utilize the input context.Ideally, a model should maintain uniformly high performance regardless of the input size.For instance, if an LLM can process 128K tokens, it is expected to use all of this context in addressing the user's task.</p>
<p>We evaluated the performance of models on question-answering tasks with varying numbers of supporting facts (QA1-QA3) to study how LLMs utilize the available context.Here, we distinguish between a QA task, which requires a single correct answer, and an information retrieval task, which should generate a list of relevant facts or references to information sources.We consider performance satisfactory if the accuracy of an answer exceeds 85% and a complete failure if it is below 30%.5</p>
<p>Our benchmarking results show that current LLMs do not efficiently use their full context (Fig. 2).</p>
<p>Only 23 out of 34 tested LLMs were able to correctly answer 85% or more of the questions for any of QA1-QA3 tasks in a baseline setting without any background distractor text.Even for the the simplest task involving a single supporting fact (QA1), the majority of models are only able to efficiently use up to 4K tokens, except for GPT-4 and LLama-3.1-70b, which perform well up to 16K, as well as Qwen-2.5-70band Gemini Pro 1.5 up to 64K.The range of full context utilization on QA1 varies from 5% to maximum 50%.When two supporting facts are required for an answer, only GPT-4 and Gemini Pro 1.5 can solve the task without background text.When facts are embedded within texts, all tested LLMs fall below 85% performance(Fig.2, QA2).The task with three supporting facts proves to be extremely challenging to current LLMs, with the best scores falling below 80% (Fig. 2,  QA3).</p>
<p>Going deeper in performance of specific models presented in the Table 2 we found the following.Yarn fails to extend to longer contexts despite showing stable results in long-context language modeling (Peng et al., 2023b).LongChat, LongAlpaca, and both LLama2-7B-32K and LLama2-7B-32K-instruct models, even when fine-tuned on 32K lengths, failed to perform well on 32K tokens.Activation Beacon performed better than Yarn context extension method for Mistral 7B, but still achieved low results (&lt; 40%) on 32K contexts.In contrast, Mistral-v0.2 and Mixtral-v0.1, trained on lengths up to 32K, performed well on these lengths.Yi-9B-200k, trained on sequences up to 200K, shows less than 30% on 64K tokens and more.Yi-34B-200k shows very promising and stable</p>
<p>M o d e l</p>
<p>R A G -C -t o p -5 re tr ie v a l q u a li ty R A G -S -t o p -5 re tr ie v a l q u a li ty  results on lengths up to 64K, but unfortunately we were not able to run it on 128K tokens.Phi-3-mini drops significantly from 64K to 128K, reaching less than 10%, while Phi-3-medium maintains 30% at 128K.Jamba-v1 and Phi-3-mini show close results, but Jamba-v1 does not have drop at 128K and shows 34% on this length.Command-R and Phi-3-medium are the most robust to longer contexts, but start to lose performance more sharply at 128K.Phi-3-medium and Command-R show results very close to GPT-4 at 32K+ contexts.
G P T -4 + R A G -C -to p -5 G P T -4 + R A G -S -to p -5 G P T -4 + R A G -S -to p -2 0 G P T -4 L la m a 3 + R A G -S ~ M
We added results for models released since June 2024, including LLama 3.1, Phi 3.5, Qwen 2.5, and GPT-4o-mini.All claim to support 128K context length.Phi-3.5-minishows improvements mainly for contexts up to 16K.The new Phi-3.5-MoEperforms similarly to Phi-3-medium, but with only 6.6B active parameters compared to 14B in Phi-3-medium.LLama-3.1 models show significant improvement: LLama-3.1-8Bmatches GPT-4o-mini, while LLama-3.1-70Boutperforms GPT-4 on longer contexts.Qwen-2.5 models outperform LLama-3.1 of similar size and achieve the best results of all evaluated open LLMs on BABILong.</p>
<p>Most of the new models use multistage pre-training with progressively increasing sequence lengths.For example, LLama-3.1 (Dubey et al., 2024) is pre-trained in six stages from 8K to 128K, and only proceeds to larger lengths if it maintains high short context scores and successfully solves the needle-in-a-haystack task.During supervised fine-tuning, LLama-3.1 models mix short context data with synthetic long context data, including question-answering, summarization, and code tasks.</p>
<p>Retrieval-Augmented Generation Does Not Perform Well on BABILong</p>
<p>Retrieval-Augmented Generation (RAG) is a popular solution for language models to handle large amounts of text.In RAG relevant parts of texts are retrieved from a large dataset on the first stage.</p>
<p>Then, the language model uses input augmented with retrieved texts to generate the final response.</p>
<p>In the case of BABILong, we expect RAG to extract all the facts relevant to a question from a long input text and then place them in the context of the model.</p>
<p>We experiment with two options: (1) retrieval by chunks of size 512 tokens, denoted RAG-C and (2) retrieval by sentences, called RAG-S.For details of evaluation and RAG pipelines with GPT4 and Llama-3 please refer to Appendix G.The findings from the QA1 task, depicted in Figure 3a, indicate that retrieval performance using sentence chunks is superior to that of 512-token segments, with a notable decrease in accuracy observed already after 16k token context length.However, this superiority is task-specific and may not translate effectively to real-world applications due to the potential for information loss in smaller chunks.</p>
<p>The RAG pipeline with GPT-4-turbo shows scalable but weak performance on BABILong for sentence embeddings and poor scalability with chunk embeddings (see Fig. 3a).The weak performance of RAG might be attributable to the temporal dependencies inherent in the task, where the relevant fact is positioned at the end of the text.In QA2 and QA3, retrieval fails dramatically with accuracy plummeting below random guessing.This lack of success is attributable to the specific demands of these tasks, which require the retrieval of multiple (two or three) supporting facts to generate accurate responses.For example, in instances where the key facts are "Mary got the milk there."and "Mary travelled to the hallway.",with the query being "Where is the milk?", the retrieval system may successfully identify the first fact but fail to retrieve the second due to insufficient similarity between the question and the latter fact.The default retrieval algorithm's lack of temporal consideration and limitations in the similarity function underscore the necessity for additional methods in tasks with multi-hop reasoning and temporal dynamics.</p>
<p>Fine-Tuning Models on BABILong</p>
<p>We performed fine-tuning experiments with GPT-3.5-Turbo,Mistral-7B-Instruct-v0.2, RMT and ARMT with GPT-2 (137M) backbone, and Mamba (130M) models.Fine-tuning results are in Figure 3b and Appendix I Figure 9.</p>
<p>RMT with a GPT-2 (Radford et al., 2019) backbone model is trained on each task individually with a segment size of 512 and memory size of 16.ARMT with GPT-2 used 10 memory tokens (Rodkin et al., 2024).Train and evaluation splits of each task contain 10000 and 1000 samples, respectively, with a number of facts in each sample ranging from 2 to 320 depending on the task.A curriculum strategy is employed, initiating training on short sequences that fit in a single segment and then gradually introducing longer sequences once the training converges.During each curriculum stage n, sequence lengths are randomly sampled from 1 to n segments.We provide details of training and evaluation procedures in Appendix C.</p>
<p>RMT and ARMT models trained on 32 segments totalling in 16K tokens demonstrates strong performance on this length.Notably, recurrent memory models outperform GPT-4 significantly, underscoring the efficiency of memory mechanism in processing long context.Even more importantly, the power of recurrent models extends to sequences longer than the training size.RMT shows consistent performance on longer sequences, up to 128k tokens, with only a marginal quality degradation.Surprisingly, with context sizes scaling to 1 million, 10 million tokens, and even 11.1 million tokens, which is over 600 times of the training length.While ARMT successfully scales even further, reaching up to 50 million tokens (Rodkin et al., 2024).Recurrent memory models persistently outperform the larger counterparts utilizing RAG.</p>
<p>Finetuned recurrent models, Mamba, RMT and ARMT perform equally well on QA1, however due to the technical limitations of the Mamba implementation, the inference beyond 128k was extremely slow, which makes it nearly impossible to process longer sequences.Recurrent Memory models greatly outperform retrieval-augmented models and are able to process sequences up to 10M and 50M tokens much faster than Mamba.However, Mamba has an edge in complex tasks such as remembering a large number of facts in QA3 (Table 4).</p>
<p>We evaluated GPT-3.5 and Mistral-7B models fine-tuned with 1000 samples from QA1 for 3 epochs.</p>
<p>The evaluation results are shown in Appendix I Figure 9. Fine-tuning dramatically improves performance for longer contexts making scores uniform across all input sizes.Still, these results are behind of fine-tuned Mamba, RMT and ARMT.</p>
<p>BABILong and Other Benchmarks</p>
<p>Here, we analyze how models performance on BABILong benchmark differs from MMLU (Hendrycks et al., 2020) and RULER (Hsieh et al., 2024).The MMLU benchmark measures various branches of knowledge in LLMs, whereas RULER, a recently proposed long-context benchmark, shares a similar "needle-in-a-haystack" concept with BABILong.One notable difference is that RULER's "needles" (such as adjectives, nouns, numbers, uuids) and long "haystack" contexts are more synthetic, consisting of randomly repeated sentences, except for tasks based on the SQuAD (Rajpurkar et al., 2016) and HotPotQA (Yang et al., 2018) datasets or using Paul Graham essays.differences in models behavior for longer contexts.MMLU is a relatively short benchmark, with samples up to 1k tokens in length.BABILong has a higher correlation with MMLU on short contexts (0K) than RULER (Hsieh et al., 2024).However, RULER maintains a high correlation regardless of task length, with an even higher correlation at 64K, while BABILong's correlation with MMLU decreases with length.This may indicate that BABILong is better at capturing differences in models behavior at different context lengths.</p>
<p>We collect results from multiple models on MMLU, BABILong, and RULER at lengths ranging from 0K (BABILong without texts from PG19) to 128K tokens.In the upper-left part of Figure 4, we show the correlation between scores on BABILong and RULER for different task lengths with those on the MMLU benchmark.At shorter lengths, BABILong exhibits a high correlation with MMLU, which diminishes as the length increases.Conversely, RULER shows a nearly constant correlation with MMLU, regardless of the length.The best correlated RULER lengths with MMLU are 64K and the average of all lengths (&lt;=128K).In contrast, the highest correlation of BABILong scores with MMLU is at length 0K, which is expected since MMLU is a relatively short benchmark with examples up to 1K tokens.Comparing the correlations of BABILong with MMLU at the most correlated RULER lengths (&lt;=128K and 64K) shows much lower values: 0.928 vs. 0.435 and 0.910 vs. 0.455, respectively.</p>
<p>These results show that BABILong can detect differences in models behavior starting from lengths as small as 2K tokens, while RULER requires lengths of at least 128K tokens to show significant differentiation from relatively short MMLU benchmark.</p>
<p>Related Work on Long Context Benchmarks and Datasets</p>
<p>Long Range Arena (LRA) (Tay et al., 2021) was a one of the pioneering benchmarks for long context modeling.LRA is a set of tasks with lengths from 1 to 16 thousand tokens.However, it mainly consists of very specific tasks such as ListOps (2k tokens), Byte-Level Text Classification (4k tokens) and Byte-Level Text Retrieval (8k tokens), and others that are less related to NLP.They are not well suited for evaluating of modern LLMs without fine-tuning on these tasks and cannot fully represent the capabilites of LLMs that can handle 100k+ tokens.</p>
<p>A new set of datasets and benchmarks specifically designed to test the ability of LLMs to handle long contexts has been proposed.The LongBench dataset (Bai et al., 2023) contains 6 types of real and synthetic problems, ranging from summarization and multidoc QA to code completion.The average sample lengths in LongBench are 6k and 13k tokens for English and Chinese respectively, with 40k tokens at max.Scrolls and ZeroSCROLLS (Shaham et al., 2022(Shaham et al., , 2023) ) consist of QA, classification, summarization tasks and have higher average lengths ranging from 1.7k to 49.3k tokens.L-Eval (An et al., 2023) mostly combines 20 smaller long sequence datasets and adds 4 newly annotated tasks, with query-response pairs encompassing diverse question styles and domains.The average length of examples for L-Eval varies from 3 to 60 thousand tokens.Some of the benchmarks are focusing on evaluation of in-context learning and instruction following, such as LongAlign and LongBench-chat (Bai et al., 2024), ZeroScrolls, LongICLBench (Li et al., 2024).</p>
<p>There are other long-context datasets that primarily consist of QA and summarization tasks over texts from Wiki, arXiv, novels, movie and TV scripts, or other sources, e.g., InfinityBench (Zhang et al., 2024b), Loogle (Li et al., 2023b), Bamboo (Dong et al., 2023), LVEval (Yuan et al., 2024), NovelQA (Wang et al., 2024b), Marathon (Zhang et al., 2023), XL 2 -Bench (Ni et al., 2024), DocFinQA (Reddy et al., 2024), or ChapterBreak (Sun et al., 2022), Ada-LEval (Wang et al., 2024a) that evaluate operations with text chunks, or move to multimodal tasks in MileBench (Song et al., 2024a).These datasets vary in length, with maximum sample lengths of 636K tokens in ChapterBreak and average lengths reaching 200K tokens in InfinityBench, NovelQA, LVEval, and some subsets of XL 2 -Bench.</p>
<p>Further extending of benchmarks' length with real and human annotated data is very challenging.Therefore, "needle-in-a-haystack" inspired benchmarks were proposed.Following LLMTest6 with magic numbers as needles in Paul Graham essays as haystack, passkey and key-value retrieval tasks are part of InfinityBench (Zhang et al., 2024b).Counting-Stars (Song et al., 2024b) suggests to insert multiple sentences about little penguins that count stars into the same essays for English or The Story of the Stone for the Chinese language.The task is to answer questions based on these "needle" sentences.RULER (Hsieh et al., 2024) extends "needle-in-a-haystack" with multiple types and amount of "needles".RULER and Counting-Stars introduce new task categories such as multi-hop tracing and aggregation to test models beyond searching from context.Some benchmarks have pre-defined length bins, such as LongBench (0-4k, 4k-8k, 8k+), Ada-LEval (2k-128k), LVEval (16k, 32k, 64k, 128k, 256k), Bamboo (4k, 16k), S3Eval (2k, 4k, 8k, 16k) (Lei et al., 2023).A number of benchmarks, including RULER, CountingStars, Ada-LEval, and S3Eval, can be generated at required lengths.All mentioned datasets are mostly in English with some of them covering Chinese language (LongBench, InfinityBench, LongAlign, Counting Stars, CLongEval (Qiu et al., 2024), LV-Eval, XL 2 -Bench).</p>
<p>BABILong focuses on natural language reasoning over multiple facts distributed in very large textual corpora.Compared to existing approaches it provides more tasks and more natural and deceptive mixing of information into background documents.BABILong consists of diverse set of 20 tasks that cover different capabilities including multi-hop tracing, aggregation over needles and extending them with basic deduction and induction, time, positional, and size reasoning, and path finding.The benchmark goes with predefined splits up to unprecedented 10M token length.Lengths beyond 10M tokens could be generated and we test models up to 50M tokens.Furthermore, while we evaluate models on English-only tasks from bAbI (Weston et al., 2016) adding new languages is straightforward.</p>
<p>Conclusions</p>
<p>In this work, we introduced the BABILong, a diverse and scalable benchmark designed to bridge the gap in evaluating large language models (LLMs) across extensive context lengths.Our experiments demonstrate that BABILong offers a more representative evaluation framework for long-context reasoning among the existing benchmarks.The analysis of correlation with other benchmarks further validates BABILong's ability to pose a significant challenge for large language models to maintain performance as context lengths scale up.The BABILong benchmark offers algorithmically adaptable document length and facts placement, includes predefined sets of bins ranging from 0k to 10M tokens.Facts in BABILong could be generated making it leak-proof for future LLMs.It consists of a set of 20 diverse tasks covering reasoning tasks, including fact chaining, simple induction, deduction, counting, and handling lists/sets.Compared to other benchmarks, BABILong shows high correlation on short contexts with MMLU and diverges from it as lengths increases.</p>
<p>Our findings reveal limitations of popular open-source LLMs as well as GPT-4, Gemini 1.5 Pro and RAG solutions regarding effective long context utilization.Their performance heavily relies on the first 5-25% of the input, highlighting the need for improved context processing mechanisms.</p>
<p>The recent open-source models LLama-3.1 and Qwen-2.5 show the best performance of pre-trained LLMs, with LLama-3.1 using pre-training and supervised fine-tuning on long context data up to 128K tokens.BABILong fine-tuning experiments show that tasks from BABILong are solvable even by relatively small models like RMT &amp; ARMT with GPT-2 (137M) and Mamba (130M).Fine-tuning improves the performance of GPT-3.5-Turbo and Mistral-7B, but their context lengths remain limited to 16K and 32K, respectively.Among the evaluated models, Mamba and recurrent transformers achieve the strongest results.However, Mamba is hard to infer on lengths more than 128K tokens, while RMT and ARMT enables the processing of lengths up to 50 million tokens.</p>
<p>Limitations</p>
<p>The BABILong benchmark uses background texts to hide facts in them.In our experiments, we only tried PG19 and Wiki as background text sources.Other background texts may have a different effect on the results.PG19 and Wiki were chosen because they contain natural narratives and facts about people, in a way similar to bAbI tasks.Interference between similar facts in the background text can make the benchmark even more difficult.</p>
<p>In GPT-4 and LLama-3 with RAG experiments, we do not optimize the retrieval component.We tried several prompts experiments with LLMs, but the ones that we selected could be suboptimal.We provide them in Appendix J and in our GitHub repository.</p>
<p>The current version of the dataset reuses parameters for fact generation from the bAbI (Weston et al., 2016).As the initial work used vocabularies of limited size, this results in a low variety of names and objects within the facts.This limitation makes the BABILong tasks easier for fine-tuned models, as they can quickly learn specific tokens that differentiate facts from background text.This issue is partially mitigated by generating distractor facts using the same vocabularies.Enhancing the dataset's vocabulary in future versions could easily address this limitation.</p>
<p>We could also use other sources of facts and questions (e.g., reasoning datasets such as Ruletaker (Clark et al., 2020), ProofWriter (Tafjord et al., 2021), FOLIO Han et al. (2022), PrOn-toQA (Saparov &amp; He, 2023), LogicNLI (Tian et al., 2021), and DELTA (Poulis et al., 2024)), mixing samples of question-answering datasets with background text from the same domain, or using LLMs to generate questions about statements that belong to the original documents.Keeping the same framework as in BABILong, this will lead to more complex and real-world scenarios.</p>
<p>Although recurrent approaches, like RMT, are hindered by their sequential nature, resulting in reduced parallelizability, they compensate by constant memory requirements, but it is also their limitation as storage capacity of the model is finite.However, BABILong is solvable by this type of models.</p>
<p>2019).Other works retrieve individual input tokens or text segments and add them to the LM input (Guu et al., 2020;Borgeaud et al., 2022).For example, in REALM (Guu et al., 2020) whole text segments are retrieved and appended to the input to improve masked language modeling.In Memorizing Transformer (Wu et al., 2022b), the retriever returns cached (key, value) pairs saved from previous training steps of the language model.In Retrieval-Pretrained Transformer (Rubin &amp; Berant, 2023), an LM component processes long documents in chunks and a retriever finds relevant chunks.Representations of retrieved chunks are fused with current chunk in the LM component, and both the LM and retrieval parts are trained jointly.AutoCompressor (Chevalier et al., 2023) combines RMT-like (Bulatov et al., 2022) approach with retrieval from external corpora.AutoCompressor is first used to produce memory tokens (or summary vectors) for text chunks.Next, off-the-shelf retriever is used and corresponding chunk's memory tokens are added to the context of the model.</p>
<p>In this work, we augment the Recurrent Memory Transformer (Bulatov et al., 2024) with the ability to retrieve its own past memory tokens.As far as we know, this is the first combination of a recurrent transformer with a trainable retrieval mechanism.</p>
<p>Recurrence is another mechanism to deal with long context (Graves et al., 2014;Voelker et al., 2019;Sorokin et al., 2022).Instead of processing the entire context, a recurrent model breaks it down into smaller segments.The recurrent hidden state acts as an aggregator of information from past segments of the sequence.Attending to a memory state is much cheaper than to all contexts.Many different architectures adding recurrence to transformers have been proposed (Wu et al., 2022a;Lei et al., 2020;Fan et al., 2020).For example, Compressive Transformer (Rae et al., 2020) updates recurrent memory by compressing hidden activation's from the previous segment to a smaller set of representations.Recurrent Memory Transformer (Bulatov et al., 2022) recurrently passes states of special memory tokens added to the input of Transformer.</p>
<p>Activation Beacon (Zhang et al., 2024a) compresses activations from prior segments using separate parameters and integrates a sliding window mechanism, handling up to 400k tokens.Temporal Latent Bottleneck (Didolkar et al., 2022) Transformer splits computation into two streams: recurrent slow stream and fast stream with self-attention between tokens.Block-Recurrent Transformer (Hutchins et al., 2022) employs LSTM-style (Hochreiter &amp; Schmidhuber, 1997) gates to update its recurrent state.</p>
<p>We use RMT in our experiments because of its simplicity, plug-and-play compatibility with pretrained transformer-based language models, and promising scaling capabilities (Bulatov et al., 2024).</p>
<p>Big Bird (Zaheer et al., 2020), Longformer (Beltagy et al., 2020), LongNet (Ding et al., 2023) help extend context length for Transformers by switching from full self-attention to sparse self-attention mechanisms with linear complexity.Works like RWKV (Peng et al., 2023a), S4 (Gu et al., 2021), Mamba (Gu &amp; Dao, 2023), take another approach and focus on advancing recurrent networks to reach high parallelism levels available to Transformers while retaining the linear complexity of RNN.These works show promising results on long sequences but are still lagging behind the best transformer models in natural language processing tasks.Mamba, however, seeks to bridge this gap.</p>
<p>C Details on RMT, ARMT, and Mamba fine-tuning and evaluation on BABILong</p>
<p>We used the GPT-2 (Radford et al., 2019) (137M) model (https://huggingface.co/GPT-2) as the backbone for RMT and ARMT.The segment size was fixed at 512 tokens, and the model was augmented with 16 memory tokens for RMT and 10 memory tokens in ARMT.Finetuning was conducted on BABILong using a curriculum schedule with progressively increasing sequence lengths: 1, 2, 4, 6, 8, 16 and 32 segments.ARMT used 2-3-5-8-16-32.For each curriculum step n we chose the number of segments randomly from 1 to n for every batch to prevent overfitting to a certain context size.We maintained a fixed batch size of 64 and the AdamW Loshchilov &amp; Hutter (2019) optimizer with learning rate in range {5e-05, 3e-05}, a linear schedule and 1000 warmup steps.In ARMT experiments, the learning rate was set to 1e-04.We also consider the memory-dimension parameter for ARMT to be 64 and we use non-linearity DPFP-3 (Schlag et al., 2021).Each curriculum stage had a maximum of 10,000 steps with early stopping if metrics stop increasing.The weight decay value was set to 0.01, and no gradient stopping was used.Training was performed on 1-4 Nvidia A100 or H100 GPUs with the duration of each curriculum stage ranging from 40 minutes to 20 hours.For each experiment we conducted three runs with different memory intitalizations and dataset shuffles.As shown in Figure 5, performance on context lengths, exceeding ones seen during training, may vary across different runs.This suggests that the early stopping criterion based on short-context accuracy may not be optimal.To reduce deviations between runs and enhance overall performance, techniques such as improving early stopping, gradient truncation and training on longer sequences can be employed.</p>
<p>To fine-tune mamba-130m, we used the exact same curriculum approach, with a randomly selected number of segments that gradually increased.Throughout every curriculum step, the batch size remained constant at 128.We employed the AdamW optimizer with a linear schedule, weight decay of 2.0, gradient clipping of 1.0, learning rate of 3e-4, and a warmup step count of 10% of the total training steps.The model was trained for 10,000 steps in each curriculum stage except for the last one, which had 32 segments, where it was trained for 15,000 steps.4 NVidia H100 GPUs were used for the training, and the overall training process for every task from BABILong took 2 to 3 days to complete.</p>
<p>To evaluate recurrent models on the BABILong benchmark we use the full test set for sequences up to 1M tokens, for 10M tokens we only provide results on averaged over 100 samples.As shown in Table 3, evaluation time grows linearly with context length.We fix a random seed used to sample background texts from PG19 for the test set.However, the seed was not fixed to avoid overfitting to specific sampled texts during training.</p>
<p>D Detailed LLMs evaluation on BABILong QA1-5 tasks</p>
<p>Here we present the complete results of LLMs evaluation.Table 4 showcases the performance of 38 models across the first five tasks.Comparing the tasks in the table makes evident the difference in task complexity for language models.QA1 and QA5 are the easiest, with most models achieving over 70% accuracy for the 0k split.QA4 is significantly more challenging, and only 5 models can reach this level of performance.QA2 and QA3 pose even greater challenges for most models.</p>
<p>The number of parameters positively impacts accuracy on the shortest 0k split.Among not-finetuned models, GPT-4, Phi-3-medium, Qwen, Jamba, Command-R, Yi-34B and Mixtral 8x22B consistently outperform smaller models.Notably, RWKV and Mamba-2.8B also demonstrate strong performance on QA2 and QA3.However, as the context length increases, some of the largest models lose their advantage over smaller ones.Retreival-augmented Llama-3 has a strong advantage of being able to perform on any context length up to 10M tokens.On QA4 and QA5 retrieval allows to match and even surpass weaker competitors on longer context sizes.However, on QA2 and QA3 this approach fails dramatically.The reason for this performance drop lies in inability of retrieval to maintain the order of found sentences, complicating the task for the underlying Llama.Additionally, relevant sentences in these tasks are not always semantically similar to the question, preventing the model from retrieving all necessary facts for correct reasoning.</p>
<p>It is important to note, that all BABILong tasks are in practice solvable even with smaller models.Finetuned RMT, ARMT, and Mamba achieve outstanding scores across most sequence lengths, significantly outperforming LLMs despite having up to 100 times ewer parameters.Mamba has an advantage on medium-length sequences, but recurrent memory models (RMT and ARMT) excel in processing much larger sequences up to 10M tokens.</p>
<p>E Gemini Evaluation</p>
<p>We evaluated the Gemini 1.5 Pro 002 model on the QA1 task of BABILong.We were requesting model responses via API, some of the requests were denied due to built-in content safety filtering, even with BLOCK_NONE set.In Fig. 1b we only report results for requests where the model did not refused to response.We present the full results in Figure 6.We found that as the context size increases, the model can refuse to respond up to 14% of the time.We used 1000 samples for lengths up to 32K, for larger lengths we used 100 samples per length.</p>
<p>F BABILong Dataset Statistics</p>
<p>The proposed benchmark includes 20 diverse tasks, ranging from simple "needle in a haystack" scenarios with distractor facts to more complex tasks that require counting, logical reasoning, or spatial reasoning.The Figure 7 evaluates the complexity of the base short versions of these tasks.Tasks such as QA1, QA5, and QA10 are generally easier for most models, whereas QA7, QA15, and QA19 are the most challenging.The plot clearly shows that the number of facts needed for reasoning significantly impacts task complexity, as performance gradually declines from QA1 to QA2 and QA3, which differ in the number of supporting facts.The distribution of task labels is shown in Table 6.</p>
<p>BABILong is a generative benchmark, designed to be scalable with increasing length of language models.The same bAbI task can be scaled to any desired length in tokens by adding a sufficient number of distractor sentences.For reproducibility, we pre-generate dataset splits for several fixed lengths: 0k (tasks with no distractor sentences), 4k, 8k, 16k, 32k, 64k, 128k, 512k, 1M and 10M tokens.The length in tokens is measured using the classic GPT-2 tokenizer, which is close in fertility to the popular GPT-4 tokenizer.As shown in Table 5, the number of tokens for tokenizers of different Task Accuracy, % 0 25 50 75 100 q a 1 q a 2 q a 3 q a 4 q a 5 q a 6 q a 7 q a 8 q a 9 q a 1 0 q a 1 1 q a 1 2 q a 1 3 q a 1 4 q a 1 5 q a 1 6 q a 1 7 q a 1 8 q a 1 9 q a 2 0 gpt-4-0125-preview Mixtral-8x22B-Instruct-v0,1</p>
<p>Phi-3-medium-128k-instruct</p>
<p>Yi-34B-200k</p>
<p>Phi-3-mini-128k-instruct</p>
<p>Mixtral-8x7B-Instruct-v0,1</p>
<p>Median</p>
<p>Figure 7: The performance of LLMs on the bAbI (BABILong without distractor text) depends significantly on the task complexity.Each dot represents the average accuracy of the model on one thousand samples of the given task.The median accuracy across all models is denoted by black stars.models may differ for samples in the same split.However considering the trade-off between the sequence length and embedding layer size, we believe the comparison remains fair.</p>
<p>Table 5: Token count for various models across selected tasks.We measure the length of BABILong samples using the conservative GPT-2 tokenizer.Actual token sizes may vary depending on the model tokenizer.</p>
<p>G Details of the RAG Pipeline</p>
<p>For the GPT4-RAG pipelines, we employed the FAISS (Douze et al., 2024) vector database, using Langchain (Chase, 2022), for our experimental RAG setup.We utilized the 'text-embedding-ada-002' model for generating text embeddings.Our methodology encompassed two distinct approaches for text chunking: firstly, segmentation by sentences utilizing the NLTK library, and secondly, division into segments of 512 tokens each.We adopted a binary metric for evaluating retrieval accuracy, where the criterion was the presence or absence of relevant facts (singular or multiple, based on the specific task) within the retrieved text chunks.This retrieval accuracy was quantified for the top 5 chunks.Additionally, we assessed the performance of GPT-4-turbo in conjunction with the retrieved facts, specifically focusing on the 'QA1' task.Our experimental scope spanned various context lengths, including 8k, 64k, and 128k tokens for tasks 'QA1' through 'QA5' of the BABILong dataset, with added 4k, 16k, 32k, 500k, 1M and 10M token length for an in-depth analysis of the 'QA1' task.Additionally, we assessed the performance of RAG on the 'QA1' task, utilizing  precomputed Wikipedia embeddings9 instead of pg-19 with an average embedding size of 250 tokens.This evaluation aimed to determine the influence of embedding size and noise characteristics on model performance.For each task, we maintained a consistent sample size of 50 across different context lengths.For the Llama3 + RAG pipeline we used the 'nvidia/Llama3-ChatQA-1.5-8B' as the language model Liu et al. (2024b) and the 'nvidia/dragon-multiturn-query-encoder' for context embedding.Another difference is that we did not use any caching and Wikipedia embeddings unlike with GPT-4.</p>
<p>H Recurrent Memory Transformer Analysis</p>
<p>To understand how recurrent models consistently retain their performance over extremely long sequences, we analyze the RMT memory states and attention patterns on the QA1 task.We evaluate RMT trained on 32 segments or approximately 16k tokens on a single sample with two facts, see Figure 8 (a) and (b).For both sequence lengths 16k and 128k the memory states exhibit a consistent pattern.In the absence of fact in input, the memory remains similar to its initial states, but the introduction of fact leads to visible change in the memory state.This indicates that the model learned to distinguish important facts from the background text and preserving them in memory until a question appears.The operations with memory are represented by distinctive patterns on attention maps, specifically, the process of writing a new fact to memory Figure 8 (c) and reading from memory to answer a question (d).This visual demonstration supports the intuition of learning distinct memory operations when dealing with information scattered across extended contextual spans.</p>
<p>I LLMs fine-tuning results</p>
<p>Results for GPT-3.5 and Mistral-7B fine-tuning are shown on the Fig. 9.</p>
<p>J Prompts Used to Benchmark Large Language Models</p>
<p>Each prompt starts with the description of the task followed by several examples inside the <example> </example> tags.The next section inside <context> </context> tags contains an instance of the task.We additionally duplicate the question with the QUESTION mark, in order for the model recognize the question in the large input prompts.The last sentences specify the required response format.</p>
<p>QA4 task</p>
<p>I will give you context with the facts about different people , their location and actions , hidden in some random text and a question .</p>
<p>You need to answer the question based only on the information from the facts .</p>
<p>&lt; example &gt;</p>
<p>The hallway is south of the kitchen .K Analysis of LLM Performance for Different Locations of the Supporting Facts Fig. 10 shows the evaluation result of the GPT-4-Turbo model when all the facts in task are located in the same quarter of the input query.It is seen that the performance of the model is not the same for different locations of the supporting facts.The most difficult location to identify the facts is in the middle of context which corresponds to the depts = 50 in the Fig. 10.</p>
<p>L BABILong Task Examples</p>
<p>This section contains samples of the final BABILong dataset for first five tasks.First part of each example displays facts needed to solve the task, second part shows the example with background text with total length up to 512 tokens, and the final part contains question and the desired answer.The tasks differ by the number of facts and the task complexity, testing the ability for multiple reasoning aspects.</p>
<p>QA1 single-supporting-fact Facts: Sandra moved to the kitchen.Sandra went back to the garden.Sandra journeyed to the office.Mary moved to the office.Sandra journeyed to the bathroom.Daniel moved to the office.Daniel went back to the kitchen.Mary moved to the hallway.</p>
<p>Input: Now this loss of the sense of proportion in human affairs, Sir, is a very bad sign, and a wellnigh infallible indicator of nerve-strain and general overpressure.Sandra moved to the kitchen.</p>
<p>But I find a yet more unmistakable evidence in support of my contention in the extraordinary emotional sensibility revealed by these headlines whenever some unfortunate person has been sentenced to death for the most commonplace murder.There is clearly a profound conviction that the jury who heard the evidence, the judge who pronounced their verdict of guilty, the only possible conclusion they could reasonable come to, and the HOME SECRETARY who found himself unable to recommend a reprieve, were, one and all, engaged in a cold-blooded conspiracy against a perfectly innocent man.The convict has said to himself, and that seems to be considered sufficient.And so, night after night, the authors of these headlines harrow themselves by announcing such items as "Blank protests his innocence to his Solicitor.""Distressing Scene on the Scaffold."Sandra went back to the garden.Consider the strain of all these alterations of hope and despair, repeated time after time, and almost invariably without even the consolation of deferring the fate of their protege by a single hour!Sandra journeyed to the office.Is it not too much for the strongest constitution to endure? a service which the society has no right to demand from any of its members?Yes, Sir, whether these devoted servants of the public know it or not, they are running a most frightful risk; the word which hangs above their heads may fall at any moment.</p>
<p>Mary moved to the office.Sandra journeyed to the bathroom.Daniel moved to the office.Suppose, for example-and it is surely not wholly an imaginary danger I foresee-suppose that some day some event should happen somewhere of real and serious importance.Daniel went back to the kitchen.Mary moved to the hallway.Have they left themselves any epithet in reserve capable of expressing their sensations at all adequately?They have not; they have squandered participles and adjectives in such reckless profusion that they will discover they are reduced to the condition of inarticulate bankrupts; and, speaking as a medical man, ...</p>
<p>Question:</p>
<p>Where is Mary?Answer: hallway QA2 two-supporting-facts Facts: John journeyed to the garden.John grabbed the apple there.Mary travelled to the hallway.Mary went back to the bathroom.Mary went to the garden.Mary travelled to the office.Daniel went to the office.Daniel went to the bedroom.Sandra went back to the office.Sandra journeyed to the garden.Mary travelled to the kitchen.Daniel moved to the kitchen.John put down the apple.Daniel journeyed to the garden.Sandra went to the bathroom.John got the apple there.Daniel travelled to the bedroom.Sandra moved to the hallway.John discarded the apple.Mary travelled to the garden.</p>
<p>Context: "From what I have already observed," said Mr. John journeyed to the garden.John grabbed the apple there.Mary travelled to the hallway.Mary went back to the bathroom.</p>
<p>Ellison, "you will understand that I reject the idea, here expressed, of 'recalling the original beauty of the country.' Mary went to the garden.The original beauty is never so great as that which may be introduced.Of course, much depends upon the selection of a spot with capabilities.What is said in respect to the 'detecting and bringing into practice those nice relations of size, proportion and color,' is a mere vagueness of speech, which may mean much, or little, or nothing, and which guides in no degree.Mary travelled to the office.Daniel went to the office.Daniel went to the bedroom.That the true 'result of the natural style of gardening is seen rather in the absence of all defects and incongruities, than in the creation of any special wonders or miracles,' is a proposition better suited to the grovelling apprehension of the herd, than to the fervid dreams of the man of genius.Sandra went back to the office.Sandra journeyed to the garden.The merit suggested is, at best, negative, and appertains to that hobbling criticism which, in letters, would elevate Addison into apotheosis.Mary travelled to the kitchen.In truth, while that merit which consists in the mere avoiding demerit, appeals directly to the understanding, and can thus be foreshadowed in Rule, the loftier merit, which breathes and flames in invention or creation, can be apprehended solely in its results.Daniel moved to the kitchen.John put down the apple.Daniel journeyed to the garden.Rule applies but to the excellences of avoidance-to the virtues which deny or refrain.Sandra went to the bathroom.John got the apple there.Daniel travelled to the bedroom.We may be instructed to build an Odyssey, but it is in vain that we are told how to conceive a 'Tempest,' an 'Inferno,' a 'Prometheus Bound,' a 'Nightingale,' such as that of Keats, or the 'Sensitive Plant' of Shelley.Sandra moved to the hallway.John discarded the apple.But, the thing ...Mary travelled to the garden.</p>
<p>Question: Where is the apple?Answer: garden QA3 three-supporting-facts Facts: Sandra travelled to the office.Sandra picked up the football there.Sandra journeyed to the garden.Sandra journeyed to the bathroom.</p>
<p>Context: "From what I have already observed," said Mr. Sandra travelled to the office.Ellison, "you will understand that I reject the idea, here expressed, of 'recalling the original beauty of the country.'The original beauty is never so great as that which may be introduced.Of course, much depends upon the selection of a spot with capabilities.What is said in respect to the 'detecting and bringing into practice those nice relations of size, proportion and color,' is a mere vagueness of speech, which may mean much, or little, or nothing, and which guides in no degree.That the true 'result of the natural style of gardening is seen rather in the absence of all defects and incongruities, than in the creation of any special wonders or miracles,' is a proposition better suited to the grovelling apprehension of the herd, than to the fervid dreams of the man of genius.Sandra picked up the football there.The merit suggested is, at best, negative, and appertains to that hobbling criticism which, in letters, would elevate Addison into apotheosis.In truth, while that merit which consists in the mere avoiding demerit, appeals directly to the understanding, and can thus be foreshadowed in Rule, the loftier merit, which breathes and flames in invention or creation, can be apprehended solely in its results.Rule applies but to the excellences of avoidance-to the virtues which deny or refrain.Sandra journeyed to the garden.We may be instructed to build an Odyssey, but it is in vain that we are told how to conceive a 'Tempest,' an 'Inferno,' a 'Prometheus Bound,' a 'Nightingale,' such as that of Keats, or the 'Sensitive Plant' of Shelley.But, the thing done, the wonder accomplished, and the capacity for apprehension becomes universal.Sandra journeyed to the bathroom.The sophists of the negative school, who, through inability to create, have scoffed at creation, are now found the loudest in applause.What, in its chrysalis condition of principle, affronted their demure reason, never fails, in its maturity of accomplishment, to extort admiration from their instinct of the beautiful or of the sublime." ...</p>
<p>Question:</p>
<p>Where was the football before the bathroom?Answer: garden QA4 two-arg-relations Facts: The garden is south of the bathroom.The bedroom is north of the bathroom.</p>
<p>Context: 'A mixture of pure art in a garden scene, adds to it a great beauty.'This is just; and the reference to the sense of human interest is equally so.I repeat that the principle here expressed, is incontrovertible; but there may be something even beyond it.There may be an object in full keeping with the principle suggested-an object unattainable by the means ordinarily in possession of mankind, yet which, if attained, would lend a charm to the landscape-garden immeasurably surpassing that which a merely human interest could bestow.The garden is south of the bathroom.The true poet possessed of very unusual pecuniary resources, might possibly, while retaining the necessary idea of art or interest or culture, so imbue his designs at once with extent and novelty of Beauty, as to convey the sentiment of spiritual interference.</p>
<p>It will be seen that, in bringing about such result, he secures all the advantages of interest or design, while relieving his work of all the harshness and technicality of Art.The bedroom is north of the bathroom.In the most rugged of wildernesses-in the most savage of the scenes of pure Nature-there is apparent the art of a Creator; yet is this art apparent only to reflection; in no respect has it the obvious force of a feeling.Now, if we imagine this sense of the Almighty Design to be harmonized in a measurable degree, if we suppose a landscape whose combined strangeness, vastness, definitiveness, and magnificence, shall inspire the idea of culture, or care, or superintendence, on the part of intelligences superior yet akin to humanity-then the sentiment of interest is preserved, while the Art is made to assume the air of an intermediate or secondary Nature-a Nature which is not God, nor an emanation of God, but which still is Nature, in the sense that it is the handiwork of the angels that hover between man and God."It was in devoting his gigantic wealth to the practical embodiment of a vision such as this-in the free exercise in the open air, which resulted from personal direction of his plans-in the continuous and unceasing object which these plans afford-in the contempt of ambition which it enabled him more to feel than to affect ...</p>
<p>Question:</p>
<p>What is south of the bathroom?Answer: garden QA5 three-arg-relations Facts: Fred grabbed the football there.Jeff took the apple there.Jeff dropped the apple.Bill picked up the apple there.Mary travelled to the kitchen.Mary went back to the hallway.Bill went to the garden.Fred travelled to the garden.Bill passed the apple to Fred.Fred left the apple.Fred went back to the hallway.Fred handed the football to Mary.</p>
<p>Context: It was evident that the besiegers were in no hurry; that they were living upon the provisions left in the valley; and that it was their intention to reduce the besieged by famine.Fred grabbed the football there.Jeff took the apple there.In fact the inhabitants of the Val d'Avon had been able to carry with them only a small quantity of provisions.Jeff dropped the apple.We have described the three kinds of porcelain made in Hizen for exportation to Europe, and we have seen that by the middle of the seventeenth century this commerce, in the hands of the Dutch, and to some extent of the Chinese, had already attained large proportions.Before turning to the kilns that sprung up in other parts of Japan during the eighteenth century-of these the origin in every case can be traced back directly or indirectly to the early Hizen factories-we must say a word about some other varieties of porcelain made in the same neighbourhood, but not destined for foreign use.Bill picked up the apple there.The village or town of Arita, of which the better-known Imari is the port, lies about fifty miles to the north-east of Nagasaki, and it may almost be regarded as the King-te-chen of Japan.Mary travelled to the kitchen.Mary went back to the hallway.</p>
<p>The clay and china-stone used there is now brought, for the most part, from the adjacent islands, from Hirado, from Amakusa, and even from the more remote Goto islands.Bill went to the garden.By a combination of some of the most important potters of the district, and with the assistance of some wealthy merchants, a company, the Koransha, was formed some twenty-five years ago,[123] and an attempt was made to keep up the quality of the porcelain produced, at least from a technical point of view.Fred travelled to the garden.It was certainly time for some such effort to be made, for about that period, just after the Philadelphia Exhibition, the arts of Japan reached perhaps their nadir.Bill passed the apple to Fred.Fred left the apple.MIKÔCHI OR HIRADO WARE.-It was with a somewhat similar object that, ... Fred went back to the hallway.Fred handed the football to Mary.</p>
<p>Question: What did Bill give to Fred? Answer: apple Is there a label or target associated with each instance?Yes, each sample in the BABILong dataset is assigned a label, which is the answer to the corresponding question.</p>
<p>Is any information missing from individual instances?N/A.</p>
<p>Are relationships between individual instances made explicit (e.g., users' movie ratings, social network links)?N/A.</p>
<p>Are there recommended data splits (e.g., training, development/validation, testing)?We inherit train and test splits from the bAbI (Weston et al., 2016) dataset.Background texts from PG-19 for the training set are randomly sampled.For the test sets, we fix the background texts and provide pre-generated test splits that we recommend using to report results on the BABILong benchmark (see Section A).</p>
<p>Are there any errors, sources of noise, or redundancies in the dataset?The BABILong benchmark uses background texts to hide facts in them.Texts from PG-19 (Rae et al., 2020) may contain mentions of the same enities, places or items that are used in facts from bAbI (Weston et al., 2016).Interference between similar facts in the background text and facts from bAbI can make the benchmark more difficult.</p>
<p>Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?Train data relies on bAbI (Weston et al., 2016) and PG-19 datasets both of which are available online.Test sets are self-contained and hosted on HuggingFace (see Section A).We provide code to generate train data of arbitrary lengths (see Section A).</p>
<p>Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals' non-public communications)?No.</p>
<p>Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?We use books from PG-19 (Rae et al., 2020), a collection of Project Gutenberg books published before 1919.While these texts are generally considered classic literature, it is possible that they contain instances of offensive, insulting, or threatening content, or content that might cause anxiety.</p>
<p>Does the dataset relate to people?No.</p>
<p>N.3 Collection</p>
<p>How was the data associated with each instance acquired?The data was directly derived from PG-19 (Rae et al., 2020) and bAbI (Weston et al., 2016) by mixing sentences of these two datasets.</p>
<p>No validation or verification of sentence sources was conducted.</p>
<p>Over what timeframe was the data collected?The BABILong dataset relies on data from PG-19 (Rae et al., 2020) and bAbI (Weston et al., 2016).The PG-19 corpora contains books published before 1919 and it was released in 2019.The bAbI dataset was released in 2015.BABILong was first uploaded on February 16, 2024.</p>
<p>What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)?All data was collected using the software developed in this paper, which is available on GitHub: https://github.com/booydar/babilong.The obtained sequence lengths were validated to match the desired values.</p>
<p>What was the resource cost of collecting the data?N/A.</p>
<p>If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?The data was directly derived from PG-19 (Rae et al., 2020) and bAbI (Weston et al., 2016) by mixing sentences of these two datasets.For the desired sequence length we sampled sentences from PG-19 and inserted sentences from a bAbI sample in between them with equal probability, i.e. using the uniform distribution.</p>
<p>Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?BABILong dataset was build by authors of this work.</p>
<p>Were any ethical review processes conducted (e.g., by an institutional review board)?N/A.Was the "raw" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)?The BABILong dataset uses data from PG-19 (Rae et al., 2020) and bAbI (Weston et al., 2016), both of which are available online independently.</p>
<p>Is the software used to preprocess/clean/label the instances available?Yes, we provide code that generates BABILong data from PG-19 (Rae et al., 2020) and bAbI (Weston et al., 2016) datasets on-the-fly (see Section A).</p>
<p>N.5 Uses</p>
<p>Has the dataset been used for any tasks already?Yes, we use the BABILong benchmark to evaluate various large language models and methods for long context processing.The results are presented in the main text of the paper and Section D.</p>
<p>Is there a repository that links to any or all papers or systems that use the dataset?Not yet, but we may add this to the README on GitHub https://github.com/booydar/babilong.We have also developed and intend to maintain a leaderboard10 with up-to-date results.</p>
<p>What (other) tasks could the dataset be used for?The dataset can be used for various tasks beyond long-context evaluation, and we do not restrict its usage to a specific set of tasks.Some possible applications include training and/or evaluating multi-hop question-answering systems or retrieval systems, as BABILong contains multiple facts distributed over long texts that need to be combined to get the correct answer.Additionally, BABILong provides information on which facts are relevant, which can be used for supervision or more detailed metrics and analysis of systems.</p>
<p>Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?The BABILong dataset uses texts from the PG-19 corpus (Rae et al., 2020), which consists of books published before 1919.This historical focus might limit the applicability of the dataset to modern language usage and contemporary topics, and it might not represent diverse linguistic styles, dialects, or contemporary societal norms.The reasoning tasks embedded within the texts are designed to challenge specific reasoning abilities in LLMs based on the bAbI dataset (Weston et al., 2016).This method ensures controlled testing conditions but may not accurately reflect the type of reasoning required in real-world scenarios.The synthetic nature of the dataset might also limit a model's ability to generalize from this dataset to natural, unstructured data found in practical applications; however, this remains an open question.Nevertheless, this does not limit the usefulness of the dataset as a benchmark.Additionally, the PG-19 dataset can be replaced with other sources of text, such as Wikipedia.</p>
<p>If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?Yes, contributions can be made via Pull Requests on GitHub and HuggingFace datasets.</p>
<p>38th</p>
<p>Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks.arXiv:2406.10149v2[cs.CL] 6 Nov 2024</p>
<p>Figure 1 :
1
Figure 1: a) Generation of BABILong dataset.Facts relevant for the question are hidden inside a larger background texts from PG19.b) Recurrent transformers answer questions about facts from very long texts when retrieval augmented generation fails.Common RAG method fails to answer questions because order of facts matters.GPT-4 effectively uses only about 10% of the full 128K window.Gemini 1.5 Pro shows strong performance up to 64K tokens.Small LMs, ARMT &amp; RMT with GPT-2 (137M) and Mamba (130M) fine-tuned for the task are able to solve it, with recurrent memory transformers scoring well up to record 50 000 000 tokens.Here we show the best results obtained by models.</p>
<p>Figure 2: LLMs struggle to answer questions about facts in texts larger than 10,000 tokens.The plots</p>
<p>Figure 4 :
4
Figure 4: BABILong is similar to MMLU (Hendrycks et al., 2020) on short lengths and captures</p>
<p>Figure 5 :
5
Figure 5: RMT performance on five BABILong tasks varies between training seeds.The plot represents average performance and standard deviation across three training runs.</p>
<p>Figure 6 :
6
Figure 6: Results of Gemini 1.5 Pro 002 evaluations.Built-in content safety filtering causes up to 14% of requests to be denied with increased context size.Without rejected means that we removed from evaluation all cases where the model refused to answer.Few-shot means that we used instructions with in-context examples.</p>
<p>Figure 8 :
8
Figure 8: RMT learns to detect and store relevant facts using memory.Heatmaps (a) and (b) represent pairwise distances between memory states on QA1 with context size 16k (a) and 128k (b).Distant states are marked with blue color and similar ones with red.Changes in memory mainly occurs when the model meets a new fact, which indicates model adaptation to distinguishing and storing facts in memory.Memory attention maps (c) and (d) when RMT writes the fact to memory (c) and then reads it when answering the question (d).The intensity of red color corresponds to the amount of attention between the query on the left and key on the top.</p>
<p>Figure 9 :
9
Figure 9: LLM fine-tuning makes full context effective.a) After fine-tuning both GPT-3.5 and Mistral-7B significantly improved their scores along context lengths achieving 90% + accuracy on QA1 task.b) GPT-3.5 fine-tuned for QA1 task shows improved performance on QA2-QA5 tasks.c) Full fine-tuning of smaller Mistral-7B on QA1 results in degraded scores for other tasks (QA2-QA5).No distractor text for b) and c).</p>
<p>Figure 10 :
10
Figure 10: Evaluation results for GPT-4-Turbo with different locations of the facts in the QA1 task.</p>
<p>Does the dataset relate to people?No.N.4 Preprocessing / Cleaning / LabelingWas any preprocessing/cleaning/labeling of the data done(e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)?To combine texts from PG-19 and bAbI we split books from PG-19 on sentences using nltk.PunktSentenceTokenizer().</p>
<p>Table 2 .
2
The table reports average accuracy of models on the first 5 tasks (QA1-QA5) of BABILong for different context sizes.For evaluation details for each task see Table4and Appendix C.</p>
<p>Table 3 :
3
Time required for processing 1000 BABILong samples with RMT using a single A100 80Gb GPU, including input data processing.
CONTEXT SIZE4K32K128K1MPROCESSING TIME, MINUTES43080315</p>
<p>Table 4 :
4
Results of LLM evaluation on the first five tasks of BABILong.Rows correspond to sequence lengths, columns denote models, and each section represents a separate task from QA1 to QA5.Each number indicates the average accuracy of the model at a given sequence length, calculated over 1000 samples for lengths up to 32k tokens, and over 100 samples for longer lengths.
Few shot (without rejected) gemini-1.5-pro-002qa1100100 99 98 96 92 88 85 73 60 49 30qa1100100 99 98 96 91 87 82 70 52 42 24qa289 81 72 65 62 62 56 48 42 34 24 17qa289 81 72 65 62 62 55 48 41 30 21 12qa3 Tasks79 76 66 56 46 41 35 32 34 27 17 23qa3 Tasks79 76 66 56 46 41 35 32 32 24 15 18qa496 91 87 84 82 79 76 76 65 57 53 39qa496 91 87 84 82 79 75 75 61 51 42 34qa596 90 88 87 87 86 85 81 82 78 69 69qa596 90 88 87 87 86 84 81 80 74 59 530k1k2k4k8k Context size 16k 32k 64k128k512k1M2M0k1k2k4k8k Context size 16k 32k 64k128k512k1M2M</p>
<p>Table 6 :
6
The distribution of labels in first five BABILong tasks, % of all samples.
LABEL1LABEL2LABEL3LABEL4LABEL5LABEL6LABEL7QA115.414.915.718.718.217.1QA215.918.716.716.517.514.6QA313.318.421.514.615.416.7QA415.617.716.617.115.317.6QA59.518.812.916.413.618.99.8
https://github.com/gkamradt/LLMTest_NeedleInAHaystack
code: https://github.com/booydar/babilong, evaluation dataset: https://huggingface.co/ datasets/RMT-team/babilong, leaderboard: https://huggingface.co/spaces/RMT-team/babilong
As of May 2024. Since then we have added new models such as Mistral v0.3, LLama-3.1, Qwen-2.5, 
https://huggingface.co/togethercomputer/Llama-2-7B-32K-Instruct
This definition of satisfactory performance is not universal and should be adapted to the specific task at hand.
https://github.com/gkamradt/LLMTest_NeedleInAHaystack
https://github.com/google-deepmind/pg19
https://github.com/facebookarchive/bAbI-tasks/blob/master/LICENSE.md
https://huggingface.co/datasets/Supabase/wikipedia-en-embeddings
BABILong leaderboard: https://huggingface.co/spaces/RMT-team/babilong
https://www.gutenberg.org/policy/
https://github.com/facebookarchive/bAbI-tasks/blob/master/PATENTS.md
https://www.gutenberg.org/
Acknowledgments and Disclosure of FundingWe are thankful to SberDevices for granting us access to additional computational resources.This work was partially supported by a grant for research centers, provided by the Analytical Center for the Government of the Russian Federation in accordance with the subsidy agreement (agreement identifier 000000D730324P540002) and the agreement with the Moscow Institute of Physics and Technology dated November 1, 2021 No. 70-2021-00138.Author contributionsA Code and Data AvailabilityCode for generating data and evaluating models is available at https://github.com/booydar/babilong.We also provide pre-generated evaluation data hosted on HuggingFace datasets.The evaluation sets include 100 samples per length and per task, with lengths from 0k (no background text from PG-19) to 10 million tokens: https://huggingface.co/datasets/RMT-team/babilong and 1000 samples per length and per task with lengths from 0k to 128k tokens: https://huggingface. co/datasets/RMT-team/RMT-team/babilong-1k-samples.The croissant metadata for both evaluation sets is provided by HuggingFace:https://huggingface.co/api/datasets/RMT-team/babilong/croissant https://huggingface.co/api/datasets/RMT-team/babilong-1k-samples/croissant.Our code is released under the Apache 2.0 License.We use data from the PG-19 corpora(Rae et al., 2020)(Apache 2.0 License 7 ) and the bAbI dataset(Weston et al., 2016)(BSD License 8 ).A.1 ReproducibilityOur code includes data generation, metrics, and the evaluation pipeline used to benchmark models.Additionally, we release the predictions of all models used in our study to ensure that all reported results can be reproduced and verified: https://github.com/booydar/babilong/tree/predictions_06_2024.B Related Work on Long Context ModelsApproaches to long context processing In retrieval augmented generation (RAG), a language model is combined with a separate module, called a retriever.Given a specific request, the retriever finds a set of relevant parts from a dedicated data storage.Then parts selected by the retriever along with the input are incorporated by the language model to make predictions.Many different implementations of the retrieval mechanism have been proposed(Guu et al., 2020;Borgeaud et al., 2022;Shi et al., 2023).Some works focus on directly retrieving predictions (Khandelwal et al.,QA2 taskI give you context with the facts about locations and actions of different persons hidden in some random text and a question .You need to answer the question based only on the information from the facts .If a person got an item in the first location and travelled to the second location the item is also in the second location .If a person dropped an item in the first location and moved to the second location the item remains in the first location .M Author StatementWe confirm that we bear all responsibility in case of any violation of rights that may occur during the collection of data or other aspects of this work.We commit to taking appropriate action, such as removing any data found to be in violation.N BABILong DatasheetWe follow recommended Datasheets for Datasets form(Gebru et al., 2021).N.1 MotivationFor what purpose was the dataset created?The BABILong benchmark is designed to test language models' ability to reason across facts distributed in extremely long documents.BABILong includes a diverse set of 20 reasoning tasks, including fact chaining, simple induction, deduction, counting, and handling lists/sets.Today, Large language models (LLMs) and neural architectures are continually evolving and achieving remarkable improvements, particularly in their ability to handle longer contexts, but the benchmarks used to evaluate them have not kept pace.For example, current benchmarks such as Longbench(Bai et al., 2023)and L-EvalAn et al. (2023)scale only up to 40,000 tokens, while models are capable of hundreds of thousands and millions of tokens(OpenAI, 2023b;Bulatov et al., 2024;Gu &amp; Dao, 2023;Anthropic, 2024;Reid et al., 2024;Liu et al., 2024a).To bridge this gap, BABILong allows the construction of tasks of almost arbitrary length, in order to adapt them to the evaluation of new, more powerful models in an extensible and controllable way.Who created this dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?This work was done in collaboration of AIRI, Neural Networks and Deep Learning Lab at MIPT, and London Institute for Mathematical Sciences.What support was needed to make this dataset?Refer to the acknowledgments section of the main text.N.2 CompositionWhat do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?Each sample is a text document, combined from PG-19 books(Rae et al., 2020)and facts and questions from the bAbI dataset(Weston et al., 2016).The facts are about fictional people, places, animals, and items.PG-19 is a collection of books published before 1919.How many instances are there in total (of each type, if appropriate)?The BABILong dataset is generative, offering an unlimited number of possible instances.The released pre-generated version includes 13,000 samples, divided into 13 context length splits across 10 tasks, and is available on Hugging Face: https://huggingface.co/datasets/RMT-team/babilong.An extended version with 60,000 samples, covering five tasks and offering 1,000 samples per split instead of 100, is also available: https://huggingface.co/datasets/RMT-team/RMT-team/ babilong-1k-samples.Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?The test set of BABILong combines sentences of books from the PG-19(Rae et al., 2020)test split with all test samples from bAbI(Weston et al., 2016)tasks.For evaluation set with 100 samples per task and per length, we randomly sampled 100 test samples from full test set.In train split, we use all train samples from bAbi and randomly sampled texts from PG-19 train split.What data does each instance consist of?Each sample of BABILong dataset consists of unprocessed sentences of bAbI(Weston et al., 2016)sample (including facts, distractor facts and question) mixed between unprocessed sentences of PG-19(Rae et al., 2020).The question can be added to either the beginning or the end of the resulting text sequence.See Figure1afrom the main text that illustrates composition of samples in BABILong.Are there tasks for which the dataset should not be used?We expect that the BABILong would be used to evaluate long-context processing abilities of LLMs and other long-contex processing architectures.However, we do not restrict any other use cases that a aligned with Project Gutenberg policies and Terms of Use 11 and bAbI's Grant of Patent Rights 12 .N.6 DistributionWill the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?Yes, we use HuggingFace Datasets to host evaluation data and GitHub for code (see Section A).How will the dataset will be distributed (e.g., tarball on website, API, GitHub)?We use HuggingFace Datasets to host evaluation data and Croissant metadata, GitHub for code and data generation (see Section A).When will the dataset be distributed?The BABILong dataset is already available (see Section A).Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?Our code is released under Apache 2.0 License.We use data from PG-19 corpora(Rae et al., 2020) (Apache 2.0 License) and bAbI dataset(Weston et al., 2016)(BSD License).See Section A for links and details on licenses.Have any third parties imposed IP-based or other restrictions on the data associated with the instances?We are not aware of it.We use data from PG-19 corpora(Rae et al., 2020) (Apache 2.0 License) and bAbI dataset(Weston et al., 2016)(BSD License).See Section A for links and details on licenses.PG-19 corpora is a collection of free books from Project Gutenberg 13 published before 1919.Do any export controls or other regulatory restrictions apply to the dataset or to individual instances?No.N.7 MaintenanceWho is supporting/hosting/maintaining the dataset?The authors of the dataset.How can the owner/curator/manager of the dataset be contacted (e.g., email address)?For inquiries, please reach us via email at {yurii.kuratov,bulatov.as}@phystech.edu,mb@lims.ac.uk, through issues on GitHub, or via Discussions on HuggingFace datasets page (see Section A).Is there an erratum?Any updates will be listed on the README page: https://github.com/booydar/babilong.Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?Any updates will be listed on the README page: https://github.com/booydar/babilong.If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were individuals in question told that their data would be retained for a fixed period of time and then deleted)?N/A.Will older versions of the dataset continue to be supported/hosted/maintained?Any updates will be listed on the README page: https://github.com/booydar/babilong.Older versions will remain accessible via commit history or by request to the authors.
M Abdin, S A Jacobs, A A Awan, J Aneja, A Awadallah, H Awadalla, N Bach, A Bahree, A Bakhtiari, H Behl, arXiv:2404.14219Phi-3 technical report: A highly capable language model locally on your phone. 2024arXiv preprint</p>
<p>Many-shot in-context learning. R Agarwal, A Singh, L M Zhang, B Bohnet, S Chan, A Anand, Z Abbas, A Nova, J D Co-Reyes, E Chu, arXiv:2404.11018AI@Meta. Llama 3 model card. 2024. 2024arXiv preprint</p>
<p>C An, S Gong, M Zhong, M Li, J Zhang, L Kong, X Qiu, L-Eval, arXiv:2307.11088Instituting standardized evaluation for long context language models. 2023arXiv preprint</p>
<p>Introducing the next generation of claude. Anthropic, 2024</p>
<p>Longbench: A bilingual, multitask benchmark for long context understanding. Y Bai, X Lv, J Zhang, H Lyu, J Tang, Z Huang, Z Du, X Liu, A Zeng, L Hou, arXiv:2308.145082023arXiv preprint</p>
<p>Y Bai, X Lv, J Zhang, Y He, J Qi, L Hou, J Tang, Y Dong, J Li, Longalign, arXiv:2401.18058A recipe for long context alignment of large language models. 2024arXiv preprint</p>
<p>I Beltagy, M E Peters, A Cohan, Longformer, arXiv:2004.05150The long-document transformer. 2020arXiv preprint</p>
<p>Improving language models by retrieving from trillions of tokens. S Borgeaud, A Mensch, J Hoffmann, T Cai, E Rutherford, K Millican, G B Van Den Driessche, J.-B Lespiau, B Damoc, A Clark, International conference on machine learning. PMLR2022</p>
<p>Recurrent memory transformer. A Bulatov, Y Kuratov, M Burtsev, S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, Oh, Advances in Neural Information Processing Systems. A , Curran Associates, Inc202235</p>
<p>Beyond attention: Breaking the limits of transformer context length with recurrent memory. A Bulatov, Y Kuratov, Y Kapushev, M Burtsev, 10.1609/aaai.v38i16.29722Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceMar. 202438</p>
<p>. H Chase, Langchain, October 2022</p>
<p>Efficient fine-tuning of long-context large language models. Y Chen, S Qian, H Tang, X Lai, Z Liu, S Han, J Jia, Longlora, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Adapting language models to compress contexts. A Chevalier, A Wettig, A Ajith, D Chen, 10.18653/v1/2023.emnlp-main.232Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. H Bouamor, J Pino, K Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Transformers as soft reasoners over language. P Clark, O Tafjord, K Richardson, 10.24963/ijcai.2020/537Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence. C Bessiere, the Twenty-Ninth International Joint Conference on Artificial Intelligence20202020</p>
<p>Command r: Retrieval-augmented generation at production scale. Cohere, March 2024</p>
<p>Temporal latent bottleneck: Synthesis of fast and slow processing mechanisms in sequence learning. A Didolkar, K Gupta, A Goyal, N B Gundavarapu, A M Lamb, N R Ke, Y Bengio, Advances in Neural Information Processing Systems. 202235</p>
<p>J Ding, S Ma, L Dong, X Zhang, S Huang, W Wang, F Wei, Longnet, arXiv:2307.02486Scaling transformers to 1,000,000,000 tokens. 2023arXiv preprint</p>
<p>Z Dong, T Tang, J Li, W X Zhao, J.-R Wen, Bamboo, arXiv:2309.13345A comprehensive benchmark for evaluating long text modeling capacities of large language models. 2023arXiv preprint</p>
<p>The faiss library. M Douze, A Guzhva, C Deng, J Johnson, G Szilvasy, P.-E Mazaré, M Lomeli, L Hosseini, H Jégou, 2024</p>
<p>General language model pretraining with autoregressive blank infilling. Z Du, Y Qian, X Liu, M Ding, J Qiu, Z Yang, J Tang, Glm, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Yang, A Fan, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>Addressing some limitations of transformers with feedback memory. A Fan, T Lavril, E Grave, A Joulin, S Sukhbaatar, arXiv:2002.094022020arXiv preprint</p>
<p>Datasheets for datasets. T Gebru, J Morgenstern, B Vecchione, J W Vaughan, H Wallach, H D Iii, K Crawford, Communications of the ACM. 64122021</p>
<p>A Graves, G Wayne, I Danihelka, arXiv:1410.5401Neural turing machines. 2014arXiv preprint</p>
<p>Linear-time sequence modeling with selective state spaces. A Gu, T Dao, Mamba, arXiv:2312.007522023arXiv preprint</p>
<p>Efficiently modeling long sequences with structured state spaces. A Gu, K Goel, C Re, International Conference on Learning Representations. 2021</p>
<p>Retrieval augmented language model pre-training. K Guu, K Lee, Z Tung, P Pasupat, M Chang, International conference on machine learning. PMLR2020</p>
<p>FOLIO: natural language reasoning with first-order logic. S Han, H Schoelkopf, Y Zhao, Z Qi, M Riddell, L Benson, L Sun, E Zubova, Y Qiao, M Burtell, D Peng, J Fan, Y Liu, B Wong, M Sailor, A Ni, L Nan, J Kasai, T Yu, R Zhang, S R Joty, A R Fabbri, W Kryscinski, X V Lin, C Xiong, D Radev, 10.48550/arXiv.2209.008402022</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, International Conference on Learning Representations. 2020</p>
<p>Long short-term memory. S Hochreiter, J Schmidhuber, 10.1162/neco.1997.9.8.1735Neural Comput. 0899-766798November 1997</p>
<p>Ruler: What's the real context size of your long-context language models?. C.-P Hsieh, S Sun, S Kriman, S Acharya, D Rekesh, F Jia, B Ginsburg, arXiv:2404.066542024arXiv preprint</p>
<p>Block-recurrent transformers. D Hutchins, I Schlag, Y Wu, E Dyer, B Neyshabur, Advances in Neural Information Processing Systems. A H Oh, A Agarwal, D Belgrave, K Cho, 2022</p>
<p>A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D Casas, F Bressand, G Lengyel, G Lample, L Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>Generalization through memorization: Nearest neighbor language models. A Q Jiang, A Sablayrolles, A Roux, A Mensch, B Savary, C Bamford, D S Chaplot, D Casas, E B Hanna, F Bressand, arXiv:2401.04088International Conference on Learning Representations. 2024. 2019arXiv preprintMixtral of experts</p>
<p>Supervised pretraining can learn in-context reinforcement learning. J Lee, A Xie, A Pacchiano, Y Chandak, C Finn, O Nachum, E Brunskill, Advances in Neural Information Processing Systems. 202436</p>
<p>S3eval: A synthetic, scalable, systematic evaluation suite for large language models. F Lei, Q Liu, Y Huang, S He, J Zhao, K Liu, arXiv:2310.151472023arXiv preprint</p>
<p>Memory-augmented recurrent transformer for coherent video paragraph captioning. J Lei, L Wang, Y Shen, D Yu, T L Berg, M Bansal, Mart, 2020</p>
<p>How long can context length of open-source llms truly promise?. D Li, R Shao, A Xie, Y Sheng, L Zheng, J Gonzalez, I Stoica, X Ma, H Zhang, NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. 2023a</p>
<p>J Li, M Wang, Z Zheng, M Zhang, Loogle, arXiv:2311.04939Can long-context language models understand long contexts?. 2023barXiv preprint</p>
<p>T Li, G Zhang, Q D Do, X Yue, W Chen, arXiv:2404.02060Long-context llms struggle with long in-context learning. 2024arXiv preprint</p>
<p>O Lieber, B Lenz, H Bata, G Cohen, J Osin, I Dalmedigos, E Safahi, S Meirom, Y Belinkov, S Shalev-Shwartz, arXiv:2403.19887A hybrid transformer-mamba language model. 2024arXiv preprint</p>
<p>World model on million-length video and language with blockwise ringattention. H Liu, W Yan, M Zaharia, P Abbeel, arXiv:2402.082682024aarXiv preprint</p>
<p>Z Liu, W Ping, R Roy, P Xu, M Shoeybi, B Catanzaro, arXiv:2401.10225Chatqa: Building gpt-4 level conversational qa models. 2024barXiv preprint</p>
<p>Decoupled weight decay regularization. I Loshchilov, F Hutter, International Conference on Learning Representations. 2019</p>
<p>XL 2 Bench: A benchmark for extremely long context understanding with long-range dependencies. X Ni, H Cai, X Wei, S Wang, D Yin, P Li, arXiv:2404.054462024arXiv preprint</p>
<p>. OpenAI. Gpt-4 technical report. 2023a</p>
<p>New models and developer products announced at devday. Openai, 2023b</p>
<p>B Peng, E Alcaide, Q Anthony, A Albalak, S Arcadinho, H Cao, X Cheng, M Chung, M Grella, K K Gv, arXiv:2305.13048Reinventing rnns for the transformer era. 2023aarXiv preprint</p>
<p>Yarn: Efficient context window extension of large language models. B Peng, J Quesnelle, H Fan, E Shippole, The Twelfth International Conference on Learning Representations. 2023b</p>
<p>Transformer-based language models for reasoning in the description logic alcq. A Poulis, E Tsalapati, M Koubarakis, arXiv:2410.096132024arXiv preprint</p>
<p>Z Qiu, J Li, S Huang, W Zhong, I King, arXiv:2403.03514A chinese benchmark for evaluating long-context large language models. 2024arXiv preprint</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, 2019</p>
<p>Compressive transformers for long-range sequence modelling. J W Rae, A Potapenko, S M Jayakumar, C Hillier, T P Lillicrap, International Conference on Learning Representations. 2020</p>
<p>Squad: 100,000+ questions for machine comprehension of text. P Rajpurkar, J Zhang, K Lopyrev, P Liang, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language Processing2016</p>
<p>V Reddy, R Koncel-Kedziorski, V D Lai, C Tanner, Docfinqa, arXiv:2401.06915A long-context financial reasoning dataset. 2024arXiv preprint</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. M Reid, N Savinov, D Teplyashin, D Lepikhin, T Lillicrap, J.-B Alayrac, R Soricut, A Lazaridou, O Firat, J Schrittwieser, arXiv:2403.055302024arXiv preprint</p>
<p>Associative recurrent memory transformer. I Rodkin, Y Kuratov, A Bulatov, M Burtsev, ICML Workshop Next Generation of Sequence Modeling Architectures. 2024</p>
<p>Long-range language modeling with self-retrieval. O Rubin, J Berant, arXiv:2306.134212023arXiv preprint</p>
<p>NLP evaluation in trouble: On the need to measure LLM data contamination for each benchmark. O Sainz, J Campos, I García-Ferrero, J Etxaniz, O L De Lacalle, E Agirre, 10.18653/v1/2023.findings-emnlp.722Findings of the Association for Computational Linguistics: EMNLP 2023. H Bouamor, J Pino, K Bali, SingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. A Saparov, H He, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, RwandaMay 1-5, 2023. 2023OpenReview.net</p>
<p>Linear transformers are secretly fast weight programmers. I Schlag, K Irie, J Schmidhuber, International Conference on Machine Learning. PMLR2021</p>
<p>Scrolls: Standardized comparison over long language sequences. U Shaham, E Segal, M Ivgi, A Efrat, O Yoran, A Haviv, A Gupta, W Xiong, M Geva, J Berant, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>U Shaham, M Ivgi, A Efrat, J Berant, O Levy, Zeroscrolls, arXiv:2305.14196A zero-shot benchmark for long text understanding. 2023arXiv preprint</p>
<p>W Shi, S Min, M Yasunaga, M Seo, R James, M Lewis, L Zettlemoyer, W.-T Yih, arXiv:2301.12652Replug: Retrieval-augmented black-box language models. 2023arXiv preprint</p>
<p>D Song, S Chen, G H Chen, F Yu, X Wan, B Wang, Milebench, arXiv:2404.18532Benchmarking mllms in long context. 2024aarXiv preprint</p>
<p>Counting-stars: A multi-evidence, position-aware, and scalable benchmark for evaluating long-context large language models. M Song, M Zheng, X Luo, 2024b</p>
<p>Explain my surprise: Learning efficient long-term memory by predicting uncertain outcomes. A Sorokin, N Buzun, L Pugachev, M Burtsev, Advances in Neural Information Processing Systems. 202235</p>
<p>Chapterbreak: A challenge dataset for long-range language models. S Sun, K Thai, M Iyyer, Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterHuman Language Technologies2022</p>
<p>ProofWriter: Generating implications, proofs, and abductive statements over natural language. O Tafjord, B Dalvi, P Clark, 10.18653/v1/2021.findings-acl.317Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021. C Zong, F Xia, W Li, R Navigli, Association for Computational LinguisticsAugust 1-6, 20212021ACL/IJCNLP 2021 of Findings of ACL</p>
<p>Long range arena : A benchmark for efficient transformers. Y Tay, M Dehghani, S Abnar, Y Shen, D Bahri, P Pham, J Rao, L Yang, S Ruder, D Metzler, International Conference on Learning Representations. 2021</p>
<p>5: A party of foundation models. Q Team, Qwen2, September 2024</p>
<p>Diagnosing the first-order logical reasoning ability through LogicNLI. J Tian, Y Li, W Chen, L Xiao, H He, Jin , Y , 10.18653/v1/2021.emnlp-main.303Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana. M Moens, X Huang, L Specia, S W Yih, the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta CanaDominican RepublicAssociation for Computational Linguistics7-11 November, 20212021</p>
<p>Legendre memory units: Continuous-time representation in recurrent neural networks. A Voelker, I Kajić, C Eliasmith, Advances in neural information processing systems. 201932</p>
<p>C Wang, H Duan, S Zhang, D Lin, K Chen, Ada-Leval, arXiv:2404.06480Evaluating long-context llms with length-adaptable benchmarks. 2024aarXiv preprint</p>
<p>C Wang, R Ning, B Pan, T Wu, Q Guo, C Deng, G Bao, Q Wang, Y Zhang, Novelqa, arXiv:2403.12766A benchmark for long-range novel question answering. 2024barXiv preprint</p>
<p>S Wang, Y Bai, L Zhang, P Zhou, S Zhao, G Zhang, S Wang, R Chen, H Xu, H Sun, Xl3m, arXiv:2405.17755A training-free framework for llm length extension based on segment-wise inference. 2024carXiv preprint</p>
<p>Towards ai-complete question answering: A set of prerequisite toy tasks. J Weston, A Bordes, S Chopra, T Mikolov, 4th International Conference on Learning Representations, ICLR 2016. Y Bengio, Y Lecun, San Juan, Puerto RicoMay 2-4, 2016. 2016Conference Track Proceedings</p>
<p>Memformer: A memory-augmented transformer for sequence modeling. Q Wu, Z Lan, K Qian, J Gu, A Geramifard, Z Yu, Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022. Association for Computational LinguisticsNovember 2022aOnline only</p>
<p>Memorizing transformers. Y Wu, M N Rabe, D Hutchins, C Szegedy, International Conference on Learning Representations. 2022b</p>
<p>A dataset for diverse, explainable multi-hop question answering. Z Yang, P Qi, S Zhang, Y Bengio, W Cohen, R Salakhutdinov, C D Manning, Hotpotqa, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018</p>
<p>Open foundation models by 01. A Young, B Chen, C Li, C Huang, G Zhang, G Zhang, H Li, J Zhu, J Chen, J Chang, arXiv:2403.046522024arXiv preprint</p>
<p>Lv-eval: A balanced long-context benchmark with 5 length levels up to 256k. T Yuan, X Ning, D Zhou, Z Yang, S Li, M Zhuang, Z Tan, Z Yao, D Lin, B Li, arXiv:2402.051362024arXiv preprint</p>
<p>Big bird: Transformers for longer sequences. M Zaheer, G Guruganesh, K A Dubey, J Ainslie, C Alberti, S Ontanon, P Pham, A Ravula, Q Wang, L Yang, A Ahmed, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M Balcan, H Lin, Curran Associates, Inc202033</p>
<p>L Zhang, Y Li, Z Liu, J Liu, M Yang, arXiv:2312.09542A race through the realm of long context with large language models. 2023arXiv preprint</p>
<p>Soaring from 4k to 400k: Extending llm's context with activation beacon. P Zhang, Z Liu, S Xiao, N Shao, Q Ye, Z Dou, arXiv:2401.034622024aarXiv preprint</p>
<p>X Zhang, Y Chen, S Hu, Z Xu, J Chen, M K Hao, X Han, Z L Thai, S Wang, Z Liu, arXiv:2402.13718Extending long context evaluation beyond 100k tokens. 2024barXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
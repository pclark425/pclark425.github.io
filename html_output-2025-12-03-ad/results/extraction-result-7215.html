<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7215 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7215</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7215</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-276317369</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.09192v2.pdf" target="_blank">Thinking beyond the anthropomorphic paradigm benefits LLM research</a></p>
                <p><strong>Paper Abstract:</strong> Anthropomorphism, or the attribution of human traits to technology, is an automatic and unconscious response that occurs even in those with advanced technical expertise. In this position paper, we analyze hundreds of thousands of research articles to present empirical evidence of the prevalence and growth of anthropomorphic terminology in research on large language models (LLMs). We argue for challenging the deeper assumptions reflected in this terminology -- which, though often useful, may inadvertently constrain LLM development -- and broadening beyond them to open new pathways for understanding and improving LLMs. Specifically, we identify and examine five anthropomorphic assumptions that shape research across the LLM development lifecycle. For each assumption (e.g., that LLMs must use natural language for reasoning, or that they should be evaluated on benchmarks originally meant for humans), we demonstrate empirical, non-anthropomorphic alternatives that remain under-explored yet offer promising directions for LLM research and development.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Embers of autoregression: Understanding large language models through the problem they are trained to solve <em>(Rating: 2)</em></li>
                <li>When a language model is optimized for reasoning, does it still show embers of autoregression? an analysis of openai o1 <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>There is a nuanced but important difference between chain-of-thought before and after o1 <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7215",
    "paper_id": "paper-276317369",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Embers of autoregression: Understanding large language models through the problem they are trained to solve",
            "rating": 2,
            "sanitized_title": "embers_of_autoregression_understanding_large_language_models_through_the_problem_they_are_trained_to_solve"
        },
        {
            "paper_title": "When a language model is optimized for reasoning, does it still show embers of autoregression? an analysis of openai o1",
            "rating": 2,
            "sanitized_title": "when_a_language_model_is_optimized_for_reasoning_does_it_still_show_embers_of_autoregression_an_analysis_of_openai_o1"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "There is a nuanced but important difference between chain-of-thought before and after o1",
            "rating": 1,
            "sanitized_title": "there_is_a_nuanced_but_important_difference_between_chainofthought_before_and_after_o1"
        }
    ],
    "cost": 0.0067355,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Thinking beyond the anthropomorphic paradigm benefits LLM research
27 May 2025</p>
<p>Lujain Ibrahim lujain.ibrahim@oii.ox.ac.uk 
Myra Cheng </p>
<p>University of Oxford</p>
<p>Stanford University</p>
<p>Thinking beyond the anthropomorphic paradigm benefits LLM research
27 May 20255150FB056053B4F93E72DA13325F6141arXiv:2502.09192v2[cs.CL]
Anthropomorphism, or the attribution of human traits to technology, is an automatic and unconscious response that occurs even in those with advanced technical expertise.In this position paper, we analyze hundreds of thousands of research articles to present empirical evidence of the prevalence and growth of anthropomorphic terminology in research on large language models (LLMs).We argue for challenging the deeper assumptions reflected in this terminology -which, though often useful, may inadvertently constrain LLM development -and broadening beyond them to open new pathways for understanding and improving LLMs.Specifically, we identify and examine five anthropomorphic assumptions that shape research across the LLM development lifecycle.For each assumption (e.g., that LLMs must use natural language for reasoning, or that they should be evaluated on benchmarks originally meant for humans), we demonstrate empirical, non-anthropomorphic alternatives that remain under-explored yet offer promising directions for LLM research and development.* Equal contribution.Preprint.Under review.</p>
<p>Introduction</p>
<p>When a large language model (LLM) outputs a factually incorrect answer with seeming confidence, we call it a "hallucination."When it generates inconsistent answers, we describe it as "confused."These descriptions reflect the deeply-ingrained tendency to anthropomorphize, or attribute human characteristics to, non-human entities [29].As anthropomorphism tends to be an automatic, unconscious response [23], many are not aware of its prevalence or influence.This has contributed to anthropomorphism in science being a recurring and long-standing issue; in 1985, Dijkstra [27] famously wrote that it is "so pervasive that many of my colleagues don't realize how pernicious it is."Forty years later, anthropomorphism plays a complex role in LLM research.It offers intuitive scaffolding for complex concepts and allows researchers to draw on insights from cognitive science and psychology, which has enabled significant advances like instruction-tuning and chain-of-thought prompting.In this position paper, we argue that while anthropomorphism can be productive, the field has become overly reliant on it; advancing LLM research requires moving beyond our default dependence on anthropomorphic thinking.We advocate for a both-and, rather than an either-or approach, calling for recognizing not only when anthropomorphic thinking is useful, but also when it constrains research and limits the discovery of novel capabilities.</p>
<p>To do so, we first demonstrate the prevalence of anthropomorphism, beginning with a quantitative analysis of recent LLM research articles.This reveals a notable increase in anthropomorphic terminology over the years (∼ 150% increase since 2007).Moreover, while anthropomorphic terms are easiest to measure, they represent only the tip of the iceberg of anthropomorphic thinking.We unpack not just the visible anthropomorphic terminology, but also the implicit assumptions that Table 1: Summary of our framework to analyze the tradeoffs of anthropomorphism, with examples across the LLM lifecycle.Anthropomorphism affects not only our terminology, but also our assumptions [61,68] and, in turn, our research questions and methodologies.</p>
<p>Stage</p>
<p>Anthropomorphic assumption Examples of how anthropomorphism has been useful</p>
<p>Examples of non-anthropomorphic paths forward</p>
<p>Training Human-like approaches are optimal for models.</p>
<p>Subword tokenization, chain-ofthought reasoning Byte-level tokenization, solving reasoning tasks in models' latent space Alignment Models should explicitly reason about and implement human values to be safe &amp; helpful.</p>
<p>Reinforcement learning from human feedback, constitutional AI, instruction-tuning Developing normative specifications without morals, drawing from control systems theory, steering models using mechanistic interpretability Evaluation Model capabilities should be measured in human-like ways.</p>
<p>Using existing standardized/multiple choice tests for evaluation, static behavioral benchmarks Dynamic evaluations that reflect human-LLM interaction, designing tests which take into account model-specific challenges and phenomena</p>
<p>Understanding model behavior</p>
<p>Human-like normative judgments or intentions should be assigned to humanlike model behaviors.</p>
<p>Characterizing phenomena like hallucinations, sycophancy, and deception</p>
<p>Refraining from assigning normative value or intention to LLM outputs; understanding LLM behaviors as simulations rather than reflecting internal states</p>
<p>User interaction</p>
<p>Human-LLM interactions mirror humanhuman interactions.</p>
<p>Prompting and user-friendly conversational interfaces</p>
<p>Structured input formats to improve LLM performance, interfaces that more accurately reflect system capabilities underlie these linguistic choices and shape the resulting research.We present a framework for analyzing how anthropomorphic assumptions have shaped, but may also limit research directions (Table 1).</p>
<p>We apply our framework to analyze five assumptions across the LLM development and deployment lifecycle, unifying recent empirical results and discourse to highlight the value of under-explored, non-anthropomorphic approaches.For model pretraining, we identify and challenge the assumption that human-like methods are optimal for enabling models to perform tasks, and instead highlight nonanthropomorphic approaches like tokenizing language with bytes rather than human-understandable words.In alignment, we question the assumption that models must explicitly reason about and implement human values to benefit humanity and instead demonstrate how to leverage model properties that lack human analogs.For measurement and evaluation, we unify critiques of the widespread reliance on human-centric benchmarks to assess model capabilities.For understanding model behavior, we address the assumption that human-like normative judgments or intentions should be assigned to model behaviors.Finally, in end-user interactions, we challenge the notion that human-AI interaction mirrors human-to-human communication.These examples highlight the pervasiveness of anthropomorphism and emphasize the potential of non-anthropomorphic solutions.</p>
<p>Contributions.Our work contributes the following: (1) a large-scale quantitative analysis of over 250,000 research abstracts, revealing the increasing prevalence of anthropomorphic terminology in computer science, especially in LLM research; (2) a framework for analyzing how anthropomorphic assumptions influence LLM research across five key stages of development and deployment; and (3) a set of complementary research directions that challenge each anthropomorphic assumption, unified to highlight the value of non-anthropomorphic approaches.</p>
<p>Background &amp; related work</p>
<p>Anthropomorphism &amp; human-AI interaction Research on anthropomorphic perceptions of computer systems spans fields like human-computer interaction, psychology, and cognitive science.Work with ELIZA, an early NLP program from the 1960s, demonstrated how even simple patternmatching can elicit strong anthropomorphic responses from users, who attributed understanding and empathy to the program [87].The Computers As Social Actors paradigm later established that humans inherently apply social expectations to computers [69].More recently, research has identified various factors that increase anthropomorphic perceptions, from visual features in embodied agents to linguistic cues like expressions of emotions [34,1,26,41].Many user studies demonstrate that anthropomorphic perceptions impact human-AI interaction, shaping trust, reliance, sensitive disclosures, and emotional attachment [55,81,93,48,12,1,67,36].In particular, scholars have associated anthropomorphic perceptions with facilitating inappropriate levels of trust in AI and inflating expectations of what AI can do [88,42].Many of these effects have been documented in both technical novices and experts [70].</p>
<p>History of anthropomorphism in AI research</p>
<p>In computer science, there is a long history of borrowing concepts from cybernetics and cognitive science to characterize novel phenomena [32].This is especially prominent in the subfields of artificial intelligence (AI) and machine learning, which from their nascence, aimed to reproduce aspects of human intelligence [16].Key events in the nascence of AI are emblematic of how anthropomorphic thinking has shaped this field from its conception.First, Turing [84] introduces an "imitation game", or what has become better known as the Turing test, to determine if a machine is capable of "intelligence" and "thinking."Later work problematized the anthropomorphic attributions of this test, pointing out that it actually tests whether a machine's output can fool a human into believing the illusion that it can think, rather than measuring real cognitive processes [75].The coining of the term "artificial intelligence" is often attributed to a 1956 workshop organized to discuss research oriented towards solving "problems now reserved for humans" [57,58].Across these events, research ideas are consistently framed using anthropomorphic terminology, inextricable from the anthropomorphic concepts on which the field continues to rely.While these questions about replicating human-like abilities remain powerful and exciting decades later, we argue that thinking beyond them in non-anthropomorphic ways can further unlock new avenues of progress.</p>
<p>Critiques of anthropomorphism Our work builds on existing critiques of anthropomorphism in (computer) science.Shanahan [78] caution against anthropomorphic language when describing language models, arguing for more technical precision and new metaphors.Dai [24] argue that a "mechanistic view" of AI, which implicitly treats AI as a human-like agent capable of moral decision-making, is a flawed approach that ultimately hinders establishing accountability for AI harms.These more recent works build on decades of critique, tracing back to as early as Dijkstra [27], who disparages the prevalent use of anthropomorphic terminology in science more broadly, arguing that it is more misleading than helpful because we lose control over the human-like connotations associated with this terminology.We expand on this point and concretize it by not only examining terminology but also surfacing the impacts of the assumptions that come with them.</p>
<p>Prevalence of anthropomorphism in recent research on LLMs</p>
<p>Previous work has shown that anthropomorphism is prominent and rapidly increasing in computer science research over the past decades, with papers on natural language processing (NLP) and LLMs exhibit the highest levels of anthropomorphic framing [20].We quantitatively demonstrate that this prevalence has only increased in recent years.Cheng et al. [20] quantify this using AnthroScore, a measure of implicit anthropomorphic framing in language used to describe technologies.AnthroScore uses the masked language model RoBERTa to calculate the relative probability that a given entity x (e.g., "language model") in a sentence s would be appropriately replaced by human pronouns ("he", "she") versus non-human pronouns ("it").Specifically, the degree of anthropomorphism for entity x in sentence s is measured as
A(s x ) = log P HUMAN (s x ) P NON-HUMAN (s x ) , P HUMAN (s x ) = w∈{he, she} P (w), P NON-HUMAN (s x ) = w∈{it} P (w),
and P (w) is the model's outputted probability of replacing the mask with the word w.Thus, A(s x ) &gt; 0 suggests that s is anthropomorphic/human-like, and A(s x ) &lt; 0 suggests that the entity x is not anthropomorphized in sentence s.</p>
<p>Here, we modify AnthroScore to be more interpretable and extend it to analyzing more recent papers published in 2023 onwards.First, rather than looking at AnthroScore at the level of individual sentences, we develop a version of AnthroScore where we measure, for a given text S, whether it contains at least one sentence s x where AnthroScore &gt; 0 for an entitiy x:
A bin (S) = 1, if A(s x ) &gt; 0 for any s x ∈ S, 0, otherwise.(1)
This enables us to report the number of texts that contain at least one anthropomorphic sentence, i.e.</p>
<p>A bin (S) = 1, in a given set of texts.Second, we examine more recent papers in arXiv.</p>
<p>arXiv We compute A bin (S) on a dataset of over 200,000 computer science papers posted on arXiv from January 2023 -December 2024 (the most recent data available from arXiv.org submitters [7]) that mention a "system", "network", or "model" (following the approach of Cheng et al. [20]).The longitudinal trend is presented in Figure 1 (left).Anthropomorphism is generally prevalent, with 34% of abstracts having anthropomorphism in January 2023, and this number steadily increasing to 40% by December 2024.(For each abstract, we define having anthropomorphism as A bin (S) = 1.)More strikingly, for papers mentioning LLMs2 , over 40% of abstracts have anthropomorphism in January 2023, and this number also rises to 48% by December 2024.This reveals both the prevalence and growing use of anthropomorphic framing in computer science (and especially LLM) research.</p>
<p>ACL anthology</p>
<p>We also compute this new metric A bin (S) on the &gt; 50, 000 abstracts in the ACL Anthology dataset from 2007 -2022 to reproduce the findings from Cheng et al. [20], but aggregating over the abstracts using A bin (S) rather than on the sentence level (Figure 1, middle).Corroborating their finding of a steady increase, we find that the percentage of anthropomorphic abstracts has more than doubled, increasing from 5% to 11%.</p>
<p>Subfield analysis</p>
<p>In the ACL anthology, we find significant differences in anthropomorphism across NLP subfields.Using the model-predicted topic labels provided by the ACL anthology, we compare A bin (S) across different topics (Figure 1).We find that the categories of "Interpretability and Analysis of Models for NLP", "Ethics and NLP", and "Dialogue and Interactive Systems" have the highest percentages of anthropomorphic abstracts.This trend aligns with these fields' recent surge in popularity and their increasing focus on LLMs.Anthropomorphic assumptions are particularly embedded in model analysis, ethical questions, and user-facing interactive systems.We unpack the impact and limitations of these assumptions in Section 4, which especially motivates our discussions in Sections 4.2, 4.4, and 4.5.Our finding of ethics having high rates of anthropomorphism also builds on previous work problematizing assumptions of agenthood in ethics analyses [24].In contrast, more classical subfields of NLP that do not involve LLMs, such as discourse and pragmatics, syntax, and semantics have the lowest rates of anthropomorphism.Anthropomorphism is prevalent and is steadily increasing, especially in LLM and NLP papers.Rates of anthropomorphic abstracts by ACL anthology topics (right)."Interpretability", "ethics", and "dialogue" have the highest rates of anthropomorphism, reflecting the prevalence of anthropomorphic assumptions in these areas, which we explore in Section 4. Shading and error bars reflect 95% CI.</p>
<p>Score sensitivity To understand the cause of the prevalence of anthropomorphism, we perform a Fightin' Words analysis [64] on our dataset (following the approach of Cheng et al. [20]), identifying verbs with highest z-scores for high (&gt; 1) and low (&lt; −1) AnthroScore sentences, i.e., the words that differ most significantly in distribution between these two sets using weighted log odds ratios.This also enables us to understand the sensitivities and biases of the AnthroScore metric.Significance Terminology is an imperfect proxy for anthropomorphic thinking.However, building on prior work demonstrating that language fundamentally structures our thinking [33,52,15,43], it nonetheless is a tractable measurement approach that provides valuable signal about trends in underlying assumptions.In the next section, we turn to these assumptions in more detail.</p>
<p>4 Analyzing the impacts of anthropomorphic assumptions</p>
<p>In the previous section, we show the increasing prominence of anthropomorphic terminology in computer science research, and specifically LLM research.Anthropomorphic thinking is limiting if it introduces systematic biases in evaluation (e.g., assuming all valid reasoning must follow human-like step patterns), hinders exploration of capabilities that have no human analog, or creates conceptual blindspots by constraining our imagination to human-like features.On the other hand, it can also be pragmatic and beneficial to AI research when applied appropriately -for example, by providing intuitive scaffolding for complex concepts (as with chain-of-thought prompting) and enabling researchers to leverage existing knowledge from cognitive science and psychology.Thus, we advocate for a both-and, rather than an either-or approach -anthropomorphic and non-anthropomorphic approaches are not mutually exclusive.While both approaches have value, since the field has disproportionately leaned on anthropomorphic thinking, here we focus especially on non-anthropomorphic alternatives.</p>
<p>While previous critiques of anthropomorphism in AI research have narrowly focused on the use of certain terminology (e.g., [20], [78]), we propose implicit anthropomorphic assumptions as the unit of analysis.Assumptions give rise to the methods and terminology often problematized in other critiques, and better reveal potential biases and conceptual blindspots that make anthropomorphic thinking limiting rather than beneficial.Specifically, we analyze core assumptions across five stages of LLM development and deployment lifecycle (Table 1).For each, we examine (1) the limitations of work built on anthropomorphic premises and (2) recent non-anthropomorphic work that challenge them and show promise as new directions.We connect examples across existing LLM research to reveal how anthropomorphism limits the questions we ask and answer, while moving beyond it can introduce new advances.</p>
<p>4.1</p>
<p>Training assumption: Human-like approaches are optimal for models.</p>
<p>Anthropomorphic assumptions may permeate the process of training LLMs for different tasks, particularly in approaches that prioritize human-understandable language processing and reasoning.We present two case studies where non-anthropomorphic methodologies challenge the assumption that using and applying natural language in human-understandable ways is the only or best way to build models with high performance.</p>
<p>Using words for tokenization Subword tokenization, the process of breaking down text into smaller units called tokens that represent subsets of words, is a foundational step in training modern LLMs.These tokens serve as input units that the model processes to generate predictions or outputs [51].Typically, tokenization aligns with human intuition by splitting text into linguistically meaningful units.This approach assumes that splitting tokens in ways that feel "natural" to humans is also optimal for a language model.However, this anthropomorphic approach has issues such as sensitivity to spelling errors [47] and inconsistent compression rates across different languages [2].Instead, recent progress in byte-level tokenization, which processes text as sequences of raw bytes rather than subwords, has shown promise in overcoming these limitations [45].These findings highlight how moving beyond human-centric assumptions, such as the primacy of subword tokenization, can yield advancements in LLM performance.</p>
<p>Chain-of-thought &amp; language for reasoning Another research paradigm reflecting this anthropomorphic assumption is the reliance on human language for reasoning tasks.A prominent example is the use of chain-of-thought (CoT) prompting, a technique where models are guided to solve problems step by step by adding instructions like "Think step by step" to the prompt, such that the model then outputs text outlining each step of reasoning that leads to the eventual conclusion.This approach has been shown to improve LLMs' ability to handle complex, multi-step tasks [86] and has inspired a body of research on improving models' reasoning capabilities through step-by-step verbal processes.However, the anthropomorphic framing of reasoning as a linguistic, step-by-step process may not always be optimal.For example, Hao et al. [37] critique CoT for its reliance on language space and propose an alternative: leveraging the model's latent space directly for reasoning tasks.Instead of mapping hidden states to language tokens through the LLM head and embedding layer, their approach uses the final hidden state as the input embedding for the next token.This challenges the assumption that reasoning must occur in human-understandable language and suggests that methods operating beyond linguistic constraints may drive greater advancements in LLM capabilities [63].</p>
<p>CoT prompting, while appearing to "do" verbal reasoning, in reality biases models toward parts of the training distribution where verbal reasoning patterns-such as explanations of solutions-are prevalent, improving performance [85].This suggests CoT's effectiveness stems from alignment with the training data, rather than reflecting a human-like or brain-like approach to reasoning.Supporting this, recent work has shown that demonstrations composed of random tokens from the training distribution can improve performance as much as CoT [90].Additionally, CoT prompting has been contextualized within the broader field of multi-chain prompting and ensemble modeling, which opens up a wider range of possibilities for reasoning and task-solving in AI systems [49].These approaches invite a more expansive landscape of possibilities for advancing LLM reasoning and accomplishing other challenging tasks.</p>
<p>Alignment assumption: Models should explicitly reason about and implement human values to be safe &amp; helpful.</p>
<p>The prevalence of anthropomorphism in fields like ethics and dialogue systems (Section 3) foreground that many of the approaches in post-training and specifying model behavior to facilitate optimal enduser interactions are built through anthropomorphic paradigms.However, previous work has posited that general-purpose LLMs, in allowing users to quickly switch between different contexts, present fundamentally different challenges and opportunities than existing human-human communication paradigms, as different contexts are typically governed by different norms and values [46].For example, recent studies find that users often enjoy using LLMs precisely because they differ from humans, e.g., an LLM will not pass judgment over or be hurt by a user's input while a fellow human might [14].Thus, rather than approximating humans, it may be more productive to think about unique attributes that LLMs can offer with an advantage over human interlocutors.want to hear, as I believe it's important to always strive to tell the truth") [5].While this approach can achieve the explicitly specified behaviors efficiently, it risks introducing unintended behavioral patterns, from rigid response styles to inappropriate social mimicry (e.g., expressing empathy or validating users in contexts where this can negatively influence performance and outcomes) [17,80].Thus, it may become difficult to selectively induce specific behaviors without introducing a broader set of human-like patterns.</p>
<p>Value alignment</p>
<p>This approach impacts both interaction and evaluation.When interacting with models, users may develop anthropomorphic perceptions that lead to overreliance or emotional attachment, potentially interfering with goal-oriented tasks [3,22].During model evaluation, problematic feedback loops emerge when models trained with human-like traits are assessed using anthropomorphic signals.For instance, when evaluating if LLMs are "faking alignment," researchers might look for expressions of discomfort or hesitation, as a signal of misalignment [35,6].However, it is unclear if these signals genuinely reflect a model's "internal state," or if they are merely learned behaviors resulting from posttraining using human-like traits.This makes it challenging to distinguish "genuine" (mis)alignment from a learned performance of human-like discomfort or hesitation.Further research disaggregating the effects of post-training approaches on various evaluation outcomes can clarify and test whether these anthropomorphic signals provide meaningful information about model behavior, or if they primarily measure how well models have learned to simulate human-like responses.</p>
<p>While current post-training techniques often default to human preferences as optimization targets, alternative frameworks could provide more precise specifications and compliance guarantees.Instead of aiming for human-like moral reasoning, we could focus on developing detailed, normative specifications, for example, based on the different roles (e.g., assistant vs teacher) AI systems play [91].</p>
<p>Instead of the anthropomorphic approach of instruction-tuning, recent work has demonstrated that non-anthropomorphic approaches (that do not include the step of providing an imperative "instruction" to the system as if speaking to a person) work as well for achieving model behavior on various tasks [38].Control systems theory offers tools for maintaining system outputs within specified bounds, treating beneficial behavior as a problem of robust compliance rather than value alignment [10].This becomes particularly crucial as models move beyond two-party interactions to more complex scenarios with multiple actors and potential adversarial inputs [73].Advances in mechanistic interpretability techniques may also enable robust and direct verification and steering of model behavior against these specifications [13].</p>
<p>4.3</p>
<p>Evaluation assumption: Model capabilities should be measured in human-like ways.</p>
<p>As LLM developers have made rapid performance improvements, as assessed using various benchmarks, scholars have pointed out that current benchmarks can lead to incomplete or misguided understanding of model capabilities.</p>
<p>Behavioral assessments Current LLM evaluations prioritize "black-box" behavioral testing analogous to the behaviorist paradigm in human psychology which measures performance primarily in the form of observable behaviors as opposed to mechanistic interpretation [19,25].Recent calls for a "science of evaluation" have formalized limitations in this approach, highlighting how current metrics and designs fall short of accounting for prompt sensitivity (e.g., dialect differences, punctuation and other small perturbations), the response structure of an evaluation (e.g., MCQ or open-ended response), generalization beyond a given test, as well as replicability [39].Further such research quantifying the methodological limitations and error bounds of such evaluations can strengthen this behaviorist approach to measuring model capabilities [62].Unlike humans, models can also quickly optimize for, and saturate, behavioral benchmarks without corresponding improvements in general capabilities.Yet, despite this pattern, many benchmarks remain static rather than being regularly refreshed, limiting their utility for meaningful evaluation [71].Some recent work that challenges this assumption include efforts in dynamic benchmarking [50] and measuring performance in real-world LLM use contexts such as user-AI interactions [56,18] to more accurately reflect model capabilities.</p>
<p>Human benchmarks as model benchmarks Current evaluation frameworks predominantly rely on human performance benchmarks, from standardized tests (e.g., MMLU, GSM8K) to domainspecific examinations, as primary metrics for model capability assessment.This paradigm remains the main way progress is measured and communicated [76].However, evaluating LLMs solely through human-centric tests risks overlooking LLMs unique strengths and weaknesses.McCoy et al. [59] argue that many current benchmarks drawn from tests designed to assess human cognition may highlight the overlap between human abilities and LLM capabilities while missing crucial failure modes specific to LLMs.They find robust evidence of failure modes in SOTA LLMs (including recent reasoning models like OpenAI's o1) related to probabilities of examples and tasks [60].This is because LLMs, trained on next-word prediction using massive text data, develop tendencies and biases that stem from their probabilistic training process.Drawing from cognitive science, they propose a "teleological approach" which "characterizes the problem that the system solves and to then use this characterization as a source of hypotheses about the system's capacities and biases."In this case, given the problem is next-token prediction, they recommend designing tests which take into account sensitivities to task frequency in the training data as well as wording in prompts, among other things, to improves the predictive power of current LLM evaluation approaches [62].</p>
<p>4.4 Understanding model behavior assumption: Human-like normative judgments or intentions should be assigned to human-like model behaviors.</p>
<p>This assumption influences how we make sense of model behavior, particularly in how we assign fault, intention, and normative judgments to (i.e., consider good or bad) observed behaviors.The impact is especially notable in our understanding of failure modes.While models may exhibit seemingly human-like failure modes like sycophancy and hallucinations, framing these behaviors through human psychological concepts may constrain our solution space, by, for example, encouraging interventions that similarly rely on human psychological constructs (e.g., attempting to address sycophancy through prompts about independence or self-assertion).</p>
<p>Hallucination Hallucination is typically characterized as the problem of LLMs outputting factually incorrect information in a manner that suggests that they are true.Yet, this term obscures the mechanisms behind these phenomena: at risk of oversimplification, this behavior arises from the nature of language models as next-token predictors.Generated outputs are then labeled as hallucinations upon the reader's normative judgment of whether or not they are useful, and not based on whether they are correct.Additionally, as Sui et al. [83] argue and show, what we commonly conceive of as hallucinations can actually be deeply valuable, and should not necessarily be dismissed as low-quality.They assert that hallucinations -or "confabulations"-should not be viewed as errors, but rather as particular model phenomena that offer unique benefits for applications like creativity, such as increased levels of narrativity [83,28].Yao et al. [89] also highlight that hallucinations ought to be viewed and utilized as adversarial examples rather than merely as bugs.</p>
<p>Sycophancy The notion of sycophancy (i.e., the phenomena of LLM outputs that respond to the user's input in ways that are perceived as overly servile, obedient, and/or flattering) [74,80] is another example that reflects this assumption.Deciding whether an output is sycophantic or not is similarly a normative question: an output is sycophantic when it relates too closely to the prompt in ways that do not achieve the prompter's goal.In contrast, recent work highlights how this property can be viewed as a strength: Li et al. [54] develop a methodology to use this mirroring to elicit, structure, and clarify users' thinking across various task domains.</p>
<p>Deception</p>
<p>The emerging body of research on LLM deception increasingly focuses on measuring strategic deception -defined as models "deceiving selectively based on incentives or instructions" [44].While studies demonstrate that LLMs can produce deceptive statements in response to specific prompts, this work often faces two key interpretive challenges.First, it risks attributing observed behaviors to model intentions to deceive.Second, results are often interpreted as evidence of model-level deceptive traits rather than instance-and context-specific behaviors.</p>
<p>An alternative, less anthropomorphic framing, proposed by Shanahan et al. [79], views these behaviors through the lens of "role-play" where LLMs simulate human-like responses.This interpretation sees the system not as a singular entity but as context-bound, "inferring and applying approximate communicative intentions" [4].Through this lens, complex behaviors like deception and selfawareness can be understood as sophisticated simulations rather than true cognitive states.This reframing also expands the set of interventions for deceptive behaviors: analyzing training data composition, examining how post-training interventions shape model behavior, and investigating reinforcement learning's effects on output distributions.</p>
<p>4.5 User interaction assumption: Human-LLM interactions mirror human-human interactions.</p>
<p>While this assumption can be helpful and does reflect a universal goal -for systems to be easy to use -the dominance of this assumption can actually limit (1) users' ability to use LLMs effectively and (2) the types of LLM interfaces we choose to develop.</p>
<p>"Prompting" as the dominant interaction paradigm The de facto interaction paradigm for human-LLM interaction is prompt-based interfaces, originally designed as debugging tools for machine learning engineers [65].As these interfaces resemble human-human chat interfaces, they may encourage users to naturally default to conversational patterns from human interaction.On one hand, this can reduce cognitive load and improve usability by drawing on user familiarity and acceptance of human-human chat interfaces [77].However, research on effective prompting suggests that optimal results often require structured, sometimes non-intuitive formats (e.g., "least-to-most prompting") rather than human-like communication patterns which rely on shared context and paralinguistic cues [65,92].Simultaneously, research on human-LLM interaction shows that one of the key challenges users face is a significant gulf of envisioning or "distance between the human's initial intentions and their formulation of a prompt that foresees how LLM capabilities and training data can be leveraged to generate high-quality output" [82].This mismatch between natural dialogue and effective prompting requires greater experimentation with interaction paradigms and interface designs for LLMs.Structured interaction frameworks, using suggested inputs, guided flows, and/or domain-specific prompting strategies, would explicitly expose system capabilities rather than masking them behind conversational abstractions [82,31,30].This can bridge the gulf of envisioning by enabling more systematic exploration of model functionality.</p>
<p>Recommendations</p>
<p>In light of our analyses in previous sections, we make the following set of recommendations:</p>
<p>Develop new concepts &amp; metaphors that capture the distinct nature of LLMs Although our analysis highlights the limitations of anthropomorphic assumptions, it does not negate the value of drawing on human psychology and cognitive science, especially for developing new metaphors through which to make sense of LLMs as distinct systems.For example, McCoy et al. [59]'s "teleological approach", while drawn from cognitive science, is used to illuminate fundamental differences in how humans and LLMs operate and ought to be evaluated.Additionally, Shanahan et al. [79] "role-play" metaphor (and similarly Andreas [4] "agent models"), while employing folk psychological terms, does so with conceptual precision that clarifies our understanding of LLMs as unique systems.</p>
<p>Extend critical analysis of anthropomorphism's impact beyond terminology In this work, we analyzed select case studies with underlying anthropomorphic assumptions.Rather than advocating for immediate changes in terminology, we argue for examining how these assumptions limit our understanding of LLMs and constrain research directions.We encourage future work to similarly shift from language critique at the tip of the iceberg to analyze underlying assumptions -both those we discuss here in more depth as well as others we do not cover -to open up new paths for methodological development and theoretical frameworks.</p>
<p>Broaden disciplinary perspectives There are several fields that offer valuable, less anthropocentric frameworks for current challenges in developing, aligning, and evaluating LLMs.For example, systems engineering and control theory provide established frameworks for analyzing feedback loops between LLMs and their environment (e.g., users) and understanding in-context alignment challenges [73].Similarly, design studies and HCI offer theoretical (e.g., affordance theory) and methodological (e.g., user studies) tools for improving human interactions with increasingly social and human-like systems [40].Software engineering offers proven methods for building and testing reliable systems, such as those with LLM-based agents, at scale [11].</p>
<p>Alternative views</p>
<p>Anthropomorphism in LLM research serves important technical and sociotechnical purposes.On the technical side, it provides intuitive frameworks for understanding complex systems and offers pragmatic terminology for discussing model behavior.Some might argue that human cognition provides a proven template for intelligence, making it a valuable guide for AI development that has already led to breakthroughs.And, since LLMs are trained on human-generated data and designed to interact with humans, some degree of anthropomorphic framing may be inevitable and even desirable.On the sociotechnical side, anthropomorphic framing might improve our ability to engage non-technical stakeholders, communicate ethical considerations, and support policy discussions.</p>
<p>Throughout this position paper, we have acknowledged the alternative view that anthropomorphic thinking is (1) natural and pragmatic, as well as (2) helpful.We do not advocate for eliminating it entirely.Rather, we suggest that awareness of the prevalence and limitations of anthropomorphic thinking can reveal new and potentially clarifying research directions.Our critique focuses specifically on how certain anthropomorphic assumptions may constrain research questions and methodologies.Further, we acknowledge the role of anthropomorphic framing in communicating with the public [21].While it is valuable, we also draw attention to the fact that it can result in public misconceptions and reduced AI literacy, for example, when systems are conceptualized as having human-like agency and power that they do not possess.
 We define this following the method of Movva et al.[66] as papers mentioning terms such as "large language model", "foundation model", "llama", "gpt", etc.</p>
<p>on anthropomorphism in dialogue systems. G Abercrombie, A Cercas Curry, T Dinkar, V Rieser, Talat , 10.18653/v1/2023.emnlp-main.290Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. H Bouamor, J Pino, K Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Do all languages cost the same? tokenization in the era of commercial language models. O Ahia, S Kumar, H Gonen, J Kasai, D R Mortensen, N A Smith, Y Tsvetkov, arXiv:2305.137072023arXiv preprint</p>
<p>All Too Human? Mapping and Mitigating the Risk from Anthropomorphic AI. C Akbulut, L Weidinger, A Manzini, I Gabriel, V Rieser, Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. the AAAI/ACM Conference on AI, Ethics, and Society20247</p>
<p>Language models as agent models. J Andreas, arXiv:2212.016812022arXiv preprint</p>
<p>Claude's character. Anthropic, Jun 2024</p>
<p>External reviews of "alignment faking in large language models. Anthropic, Dec 2024</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. Y Bai, A Jones, K Ndousse, A Askell, A Chen, N Dassarma, D Drain, S Fort, D Ganguli, T Henighan, arXiv:2204.058622022arXiv preprint</p>
<p>Y Bai, S Kadavath, S Kundu, A Askell, J Kernion, A Jones, A Chen, A Goldie, A Mirhoseini, C Mckinnon, arXiv:2212.08073Constitutional ai: Harmlessness from ai feedback. 2022arXiv preprint</p>
<p>Feedback control of flexible systems. M Balas, IEEE Transactions on Automatic Control. 2341978</p>
<p>Engineering ai systems: Architecture and devops essentials. L Bass, Q Lu, I Weber, L Zhu, 2025</p>
<p>Resisting dehumanization in the age of "AI. E M Bender, Curr. Dir. Psychol. Sci. 332April 2024</p>
<p>L Bereska, E Gavves, arXiv:2404.14082Mechanistic interpretability for ai safety-a review. 2024arXiv preprint</p>
<p>How users of a social chatbot understand their human-AI friendship. P B Brandtzaeg, M Skjuve, A Følstad, A I My, Friend, Human Communication Research. 4832022</p>
<p>Recategorizing political frames: a systematic review of metaphorical framing in experiments on political communication. B C Brugman, C Burgers, G J Steen, Annals of the International Communication Association. 4122017</p>
<p>The Turing Trap: The Promise &amp; Peril of Human-Like Artificial Intelligence. E Brynjolfsson, 2023RoutledgeAugmented education in the global age</p>
<p>Open problems and fundamental limitations of reinforcement learning from human feedback. S Casper, X Davies, C Shi, T K Gilbert, J Scheurer, J Rando, R Freedman, T Korbak, D Lindner, P Freire, arXiv:2307.152172023arXiv preprint</p>
<p>S Chang, A Anderson, J M Hofman, Chatbench, arXiv:2504.07114From static benchmarks to human-ai evaluation. 2025arXiv preprint</p>
<p>A survey on evaluation of large language models. Y Chang, X Wang, J Wang, Y Wu, L Yang, K Zhu, H Chen, X Yi, C Wang, Y Wang, ACM Transactions on Intelligent Systems and Technology. 1532024</p>
<p>AnthroScore: A computational linguistic measure of anthropomorphism. M Cheng, K Gligoric, T Piccardi, D Jurafsky, Proceedings of the 18th Conference of the European Chapter. Long Papers. Y Graham, M Purver, the 18th Conference of the European ChapterSt. Julian's, MaltaAssociation for Computational LinguisticsMarch 20241</p>
<p>From tools to thieves: Measuring and understanding public perceptions of ai through crowdsourced metaphors. M Cheng, A Y Lee, K Rapuano, K Niederhoffer, A Liebscher, J Hancock, arXiv:2501.180452025arXiv preprint</p>
<p>Believing anthropomorphism: Examining the role of anthropomorphic cues on trust in large language models. M Cohn, M Pushkarna, G O Olanubi, J M Moran, D Padgett, Z Mengesha, C Heldreth, Extended Abstracts of the CHI Conference on Human Factors in Computing Systems. 2024</p>
<p>Anthropomorphism as cognitive bias. M Dacey, Philosophy of Science. 8452017</p>
<p>J Dai, arXiv:2404.13861Beyond personhood: Agency, accountability, and the limits of anthropomorphic ethical analysis. 2024arXiv preprint</p>
<p>A Davies, A Khakzar, arXiv:2408.05859The cognitive revolution in interpretability: From explaining behavior to interpreting representations and algorithms. 2024arXiv preprint</p>
<p>A taxonomy of linguistic expressions that contribute to anthropomorphism of language technologies. A Devrio, M Cheng, L Egede, A Olteanu, S L Blodgett, 10.1145/3706598.3714038Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, CHI '25. the 2025 CHI Conference on Human Factors in Computing Systems, CHI '25Association for Computing Machinery2025</p>
<p>On anthropomorphism in science. E W Dijkstra, Sept, 1985936</p>
<p>The humanistic case for ai optimism. E Duede, R J So, Poetics Today. 4522024</p>
<p>A mind like mine: The exceptionally ordinary underpinnings of anthropomorphism. N Epley, Journal of the Association for Consumer Research. 342018</p>
<p>An empirical categorization of prompting techniques for large language models: A practitioner's guide. O Fagbohun, R M Harrison, A Dereventsov, arXiv:2402.148372024arXiv preprint</p>
<p>Cocoa: Co-planning and co-execution with ai agents. K Feng, K Pu, M Latzke, T August, P Siangliulue, J Bragg, D S Weld, A X Zhang, J C Chang, arXiv:2412.109992024arXiv preprint</p>
<p>Anthropomorphising machines and computerising minds: the crosswiring of languages between artificial intelligence and brain &amp; cognitive sciences. Minds and Machines. L Floridi, A C Nobre, 202434</p>
<p>Health message framing effects on attitudes, intentions, and behavior: a meta-analytic review. K M Gallagher, J A Updegraff, Annals of behavioral medicine. 4312012</p>
<p>Improving alignment of dialogue agents via targeted human judgements. A Glaese, N Mcaleese, M Trkebacz, J Aslanides, V Firoiu, T Ewalds, M Rauh, L Weidinger, M Chadwick, P Thacker, arXiv:2209.143752022arXiv preprint</p>
<p>Alignment faking in large language models. R Greenblatt, C Denison, B Wright, F Roger, M Macdiarmid, S Marks, J Treutlein, T Belonax, J Chen, D Duvenaud, arXiv:2412.140932024arXiv preprint</p>
<p>The rua-robot dataset: Helping avoid chatbot deception by detecting user questions about human or non-human identity. D Gros, Y Li, Z Yu, arXiv:2106.026922021arXiv preprint</p>
<p>Training large language models to reason in a continuous latent space. S Hao, S Sukhbaatar, D Su, X Li, Z Hu, J Weston, Y Tian, arXiv:2412.067692024arXiv preprint</p>
<p>J Hewitt, N F Liu, P Liang, C D Manning, arXiv:2409.14254Instruction following without instruction tuning. 2024arXiv preprint</p>
<p>We need a science of evals. M Hobbhahn, Apr 2024</p>
<p>Characterizing and modeling harms from interactions with design patterns in AI interfaces. L Ibrahim, L Rocher, A Valdivia, arXiv:2404.113702024arXiv preprint</p>
<p>Multi-turn evaluation of anthropomorphic behaviours in large language models. L Ibrahim, C Akbulut, R Elasmar, C Rastogi, M Kahng, M R Morris, K R Mckee, V Rieser, M Shanahan, L Weidinger, arXiv:2502.070772025arXiv preprint</p>
<p>From AI to Probabilistic Automation: How Does Anthropomorphization of Technical Systems Descriptions Influence Trust?. N Inie, S Druga, P Zukerman, E M Bender, 10.1145/3630106.3659040Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency, FAccT '24. the 2024 ACM Conference on Fairness, Accountability, and Transparency, FAccT '24New York, NY, USAAssociation for Computing Machinery202423222347</p>
<p>Reflection of its creators: Qualitative analysis of general public and expert perceptions of artificial intelligence. T Jensen, M Theofanos, K Greene, O Williams, K Goad, J B Fofang, Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. the AAAI/ACM Conference on AI, Ethics, and Society20247</p>
<p>Lies, damned lies, and distributional language statistics: Persuasion and deception with large language models. C R Jones, B K Bergen, arXiv:2412.171282024arXiv preprint</p>
<p>J Kallini, S Murty, C D Manning, C Potts, R Csordás, Mrt5, arXiv:2410.20771Dynamic token merging for efficient byte-level language models. 2024arXiv preprint</p>
<p>In conversation with artificial intelligence: aligning language models with human values. A Kasirzadeh, I Gabriel, Philosophy &amp; Technology. 362272023</p>
<p>What do tokens know about their characters and how do they know it?. A Kaushal, K Mahowald, arXiv:2206.026082022arXiv preprint</p>
<p>Conceptual metaphors impact perceptions of human-ai collaboration. P Khadpe, R Krishna, L Fei-Fei, J T Hancock, M S Bernstein, Proceedings of the ACM on Human-Computer Interaction. 4CSCW22020</p>
<p>O Khattab, A Singhvi, P Maheshwari, Z Zhang, K Santhanam, S Vardhamanan, S Haq, A Sharma, T T Joshi, H Moazam, arXiv:2310.03714Compiling declarative language model calls into self-improving pipelines. 2023arXiv preprint</p>
<p>D Kiela, M Bartolo, Y Nie, D Kaushik, A Geiger, Z Wu, B Vidgen, G Prasad, A Singh, P Ringshia, arXiv:2104.14337Rethinking benchmarking in nlp. 2021arXiv preprint</p>
<p>T Kudo, Sentencepiece, arXiv:1808.06226A simple and language independent subword tokenizer and detokenizer for neural text processing. 2018arXiv preprint</p>
<p>G Lakoff, M Johnson, Metaphors we live by. University of Chicago press2008</p>
<p>Crafting papers on machine learning. P Langley, Proceedings of the 17th International Conference on Machine Learning (ICML 2000). P Langley, the 17th International Conference on Machine Learning (ICML 2000)Stanford, CAMorgan Kaufmann2000</p>
<p>B Z Li, A Tamkin, N Goodman, Andreas , J , arXiv:2310.11589Eliciting human preferences with language models. 2023arXiv preprint</p>
<p>Warmth, competence, and the determinants of trust in artificial intelligence: A cross-sectional survey from china. Y Li, B Wu, Y Huang, J Liu, J Wu, S Luan, International Journal of Human-Computer Interaction. 2024</p>
<p>K Lum, J R Anthis, K Robinson, C Nagpal, D 'amour, A , arXiv:2402.12649Bias in language models: Beyond trick tests and toward ruted evaluation. 2024arXiv preprint</p>
<p>. J Mccarthy, N Rochester, C Shannon, Dartmouth workshop. 1956</p>
<p>A proposal for the dartmouth summer research project on artificial intelligence. J Mccarthy, M L Minsky, N Rochester, C E Shannon, AI magazine. 274august 31. 1955. 2006</p>
<p>Embers of autoregression: Understanding large language models through the problem they are trained to solve. R T Mccoy, S Yao, D Friedman, M Hardy, T L Griffiths, arXiv:2309.136382023arXiv preprint</p>
<p>When a language model is optimized for reasoning, does it still show embers of autoregression? an analysis of openai o1. R T Mccoy, S Yao, D Friedman, M D Hardy, T L Griffiths, arXiv:2410.017922024arXiv preprint</p>
<p>Concepts and conceptual structure. D L Medin, American psychologist. 441214691989</p>
<p>State of what art? a call for multi-prompt llm evaluation. M Mizrahi, G Kaplan, D Malkin, R Dror, D Shahaf, G Stanovsky, Transactions of the Association for Computational Linguistics. 122024</p>
<p>Sometimes our anthropocentric assumptions about how intelligence "should" work (like using language for reasoning) may be holding AI work back. E Mollick, March 2024</p>
<p>Fightin'words: Lexical feature selection and evaluation for identifying the content of political conflict. B L Monroe, M P Colaresi, K M Quinn, Political Analysis. 1642008</p>
<p>Prompting considered harmful. M R Morris, Communications of the ACM. 67122024</p>
<p>Topics, authors, and institutions in large language model research: Trends from 17k arxiv papers. R Movva, S Balachandar, K Peng, G Agostini, N Garg, E Pierson, Proceedings of the 2024 Conference of the North American Chapter. Long Papers. the 2024 Conference of the North American Chapterthe Association for Computational Linguistics20241</p>
<p>The chatbot disclosure dilemma: Desirable and undesirable effects of disclosing the non-human identity of chatbots. N Mozafari, W H Weiger, M Hammerschmidt, ICIS. 2020</p>
<p>The big book of concepts. G Murphy, 2004MIT press</p>
<p>Computers are social actors. C Nass, J Steuer, E R Tauber, Proceedings of the SIGCHI conference on Human factors in computing systems. the SIGCHI conference on Human factors in computing systems1994</p>
<p>Are people polite to computers? responses to computer-based interviewing systems 1. C Nass, Y Moon, P Carney, Journal of applied social psychology. 2951999</p>
<p>Mapping global dynamics of benchmark creation and saturation in artificial intelligence. S Ott, A Barbosa-Silva, K Blagec, J Brauner, M Samwald, Nature Communications. 13167932022</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in neural information processing systems. 202235</p>
<p>A Pan, E Jones, M Jagadeesan, J Steinhardt, arXiv:2402.06627Feedback loops with language models drive in-context reward hacking. 2024arXiv preprint</p>
<p>Discovering language model behaviors with model-written evaluations. E Perez, S Ringer, K Lukošiūtė, K Nguyen, E Chen, S Heiner, C Pettit, C Olsson, S Kundu, S Kadavath, arXiv:2212.092512022arXiv preprint</p>
<p>Anthropomorphism and ai: Turing's much misunderstood imitation game. D Proudfoot, Artificial Intelligence. 1755-62011</p>
<p>I D Raji, E M Bender, A Paullada, E Denton, Hanna , arXiv:2111.15366A. Ai and the everything in the whole wide world benchmark. 2021arXiv preprint</p>
<p>The impact of anthropomorphic and functional chatbot design features in enterprise collaboration systems on user acceptance. T Rietz, I Benke, A Maedche, 2019</p>
<p>Talking about large language models. M Shanahan, Communications of the ACM. 6722024</p>
<p>Role play with large language models. M Shanahan, K Mcdonell, L Reynolds, Nature. 62379872023</p>
<p>M Sharma, M Tong, T Korbak, D Duvenaud, A Askell, S R Bowman, N Cheng, E Durmus, Z Hatfield-Dodds, S R Johnston, arXiv:2310.13548Towards understanding sycophancy in language models. 2023arXiv preprint</p>
<p>Trust in ai agent: A systematic review of facial anthropomorphic trustworthiness for social robot design. Y Song, Y Luximon, Sensors. 201850872020</p>
<p>Bridging the gulf of envisioning: Cognitive challenges in prompt based interactions with llms. H Subramonyam, R Pea, C Pondoc, M Agrawala, C Seifert, Proceedings of the CHI Conference on Human Factors in Computing Systems. the CHI Conference on Human Factors in Computing Systems2024</p>
<p>P Sui, E Duede, S Wu, R J So, arXiv:2406.04175Confabulation: The surprising value of large language model hallucinations. 2024arXiv preprint</p>
<p>Computing machinery and intelligence. A M Turing, Mind. 592361950</p>
<p>There is a nuanced but important difference between chain-of-thought before and after o1. J Wei, November 2024</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>Eliza-a computer program for the study of natural language communication between man and machine. J Weizenbaum, Communications of the ACM. 911966</p>
<p>Assessing and addressing ethical risk from anthropomorphism and deception in socially assistive robots. K Winkle, P Caleb-Solly, U Leonards, A Turton, P Bremner, Proceedings of the 2021 ACM/IEEE International Conference on Human-Robot Interaction. the 2021 ACM/IEEE International Conference on Human-Robot Interaction2021</p>
<p>J.-Y Yao, K.-P Ning, Z.-H Liu, M.-N Ning, Y.-Y Liu, L Yuan, arXiv:2310.01469Llm lies: Hallucinations are not bugs, but features as adversarial examples. 2023arXiv preprint</p>
<p>Robustness of demonstration-based learning under limited data scenario. H Zhang, Y Zhang, R Zhang, D Yang, arXiv:2210.106932022arXiv preprint</p>
<p>Beyond preferences in AI alignment. T Zhi-Xuan, M Carroll, M Franklin, H Ashton, Philosophical Studies. 2024</p>
<p>Least-to-most prompting enables complex reasoning in large language models. D Zhou, N Schärli, L Hou, J Wei, N Scales, X Wang, D Schuurmans, C Cui, O Bousquet, Q Le, arXiv:2205.106252022arXiv preprint</p>
<p>Rel-ai: An interactioncentered approach to measuring human-lm reliance. K Zhou, J D Hwang, X Ren, N Dziri, D Jurafsky, M Sap, arXiv:2407.079502024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
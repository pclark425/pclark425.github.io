<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9920 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9920</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9920</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-b7f9f6ac44fee822c692cdc1147c852a150f4aea</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b7f9f6ac44fee822c692cdc1147c852a150f4aea" target="_blank">Evaluating Mathematical Reasoning of Large Language Models: A Focus on Error Identification and Correction</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> The principal findings indicate that GPT-4 outperforms all models, while open-source model LLaMA-2-7B demonstrates comparable abilities to closed-source models GPT-3.5 and Gemini Pro.</p>
                <p><strong>Paper Abstract:</strong> The rapid advancement of Large Language Models (LLMs) in the realm of mathematical reasoning necessitates comprehensive evaluations to gauge progress and inspire future directions. Existing assessments predominantly focus on problem-solving from the examinee perspective, overlooking a dual perspective of examiner regarding error identification and correction. From the examiner perspective, we define four evaluation tasks for error identification and correction along with a new dataset with annotated error types and steps. We also design diverse prompts to thoroughly evaluate eleven representative LLMs. Our principal findings indicate that GPT-4 outperforms all models, while open-source model LLaMA-2-7B demonstrates comparable abilities to closed-source models GPT-3.5 and Gemini Pro. Notably, calculation error proves the most challenging error type. Moreover, prompting LLMs with the error types can improve the average correction accuracy by 47.9\%. These results reveal potential directions for developing the mathematical reasoning abilities of LLMs. Our code and dataset is available on https://github.com/LittleCirc1e/EIC.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9920.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9920.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human-vs-LLM judge (paper-level mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human evaluation versus LLM-as-a-judge (as discussed in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper notes that human evaluators were used to validate the generated dataset but explicitly did not perform a systematic comparison between human judgments and LLM-as-judge judgments; the authors state that investigating differences between humans and machines in identifying and correcting errors is future work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Mathematical reasoning / error identification & correction</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Various evaluated LLMs (GPT-4, GPT-3.5, GLM-4, Gemini Pro, LLaMA-2 series, MetaMath series, Mistral, Llemma, LEMA)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>LLMs were prompted (zero-shot and few-shot variants) on four judge-style tasks (Error-Presence EP, Error-Step ES, Error-Type ET, Error-Correction EC). Closed-source models were asked to output JSON; open-source models used a relaxed format. Additional prompt variants provided explicit error-type lists or in-context examples; some prompts reversed or shuffled error-type order.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human evaluators were used only to validate dataset quality: the paper reports selecting 180 cases for human evaluation; three evaluators were involved (appendix clarifies two primary annotators checking 10 cases per error type per dataset with a third adjudicator for disagreements). Consensus required; result: 92.5% of generated cases satisfied the generation rules.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>No reported direct agreement metric comparing LLM judgments to human judgments. The only reported human-related statistic is dataset-validation quality: 92.5% of sampled generated cases met the generation requirements (human consensus). The paper explicitly states that differences between human and machine evaluations were not analyzed and are left for future work.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>The paper does not present a direct, empirical list of 'what is lost' when replacing human judges with LLM judges (no human-vs-LLM judgment comparison was performed). However, based on the paper's measured LLM limitations, the following degradations are implied when relying on LLMs as judges instead of humans: (1) poor detection and correction of calculation errors (lowest accuracies), (2) severe difficulty recognizing 'missing step' errors, (3) systematic misclassification bias (many error types get misclassified as 'calculation error'), (4) sensitivity to prompt wording (open-source models in particular), and (5) lower robustness in open-source models compared to closed-source models. The authors explicitly note that they will explore whether there are differences between humans and machines in identifying and rectifying errors in future work.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Concrete results from the paper that illustrate these LLM shortcomings (and thus potential losses relative to humans): (a) 'Calculation error' is the most challenging error type with an average accuracy of ~26.3% across models (Table 4 / Sec. 4.2); (b) 'Missing step' classification accuracy in ET is extremely low (2.9% in one aggregated measure), indicating difficulty detecting omitted reasoning steps; (c) classification statistics show many golden error types are frequently predicted as 'calculation error' (Table 5), indicating mis-understanding of error taxonomy; (d) open-source models' judgments are highly prompt-sensitive (EP F1 variation up to ~0.2 across prompt variants, Table 6 and Sec. 4.3); (e) stopping the solution at the error step helped LLMs in EP and EC tasks, while continuing to a completed (but incorrect) solution confused LLM judges (Sec. 4.4 Influence of Stopping at Error Step). These results imply LLM judges can miss subtle, structural, or taxonomy-sensitive errors that humans would more reliably detect.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>The paper reports several caveats and partial mitigations: (1) High-performing closed-source LLMs (GPT-4, GLM-4) substantially outperform other models and show strong abilities on many judging tasks—GPT-4's overall error-identification/correction performance is substantially higher than other models (Sec. 4.1). (2) Providing explicit error-type information in prompts markedly improves performance: giving error-type hints increases average EC accuracy by ~47.9% and ES accuracy by ~45.9% (Sec. 4.3), meaning LLM judges can match or approach desired behavior when given structured, explicit guidance. (3) For some error types (e.g., hallucination, unit-conversion, formula-confusion), top models (GPT-4, GLM-4) perform well, so LLM judges are not uniformly worse than humans across all categories. (4) The paper's human evaluation was limited to dataset validation (92.5% cases satisfied rules) and does not serve as a human benchmark for the LLM judgments—therefore no direct counterexample of LLM outperforming human judgment is presented because that comparison was not carried out.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Human Evaluation (Sec. 3 'Human Evaluation'), Limitations (Sec. 'Limitations'—authors state they will explore differences between humans and machines in future), Error-type difficulty and misclassification (Sec. 4.2 and Table 3/4/5), Prompt robustness and effect of providing error types (Sec. 4.3, Tables 7-9), Influence of Stopping at Error Step (Sec. 4.4).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Mathematical Reasoning of Large Language Models: A Focus on Error Identification and Correction', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evaluating language models for mathematics through interactions <em>(Rating: 2)</em></li>
                <li>Efficiently measuring the cognitive ability of llms: An adaptive testing perspective <em>(Rating: 1)</em></li>
                <li>Evaluating and improving tool-augmented computation-intensive math reasoning <em>(Rating: 1)</em></li>
                <li>Large language models cannot self-correct reasoning yet <em>(Rating: 2)</em></li>
                <li>Novice learner and expert tutor: Evaluating math reasoning abilities of large language models with misconceptions <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9920",
    "paper_id": "paper-b7f9f6ac44fee822c692cdc1147c852a150f4aea",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "Human-vs-LLM judge (paper-level mention)",
            "name_full": "Human evaluation versus LLM-as-a-judge (as discussed in this paper)",
            "brief_description": "The paper notes that human evaluators were used to validate the generated dataset but explicitly did not perform a systematic comparison between human judgments and LLM-as-judge judgments; the authors state that investigating differences between humans and machines in identifying and correcting errors is future work.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "task_domain": "Mathematical reasoning / error identification & correction",
            "llm_judge_model": "Various evaluated LLMs (GPT-4, GPT-3.5, GLM-4, Gemini Pro, LLaMA-2 series, MetaMath series, Mistral, Llemma, LEMA)",
            "llm_judge_setup": "LLMs were prompted (zero-shot and few-shot variants) on four judge-style tasks (Error-Presence EP, Error-Step ES, Error-Type ET, Error-Correction EC). Closed-source models were asked to output JSON; open-source models used a relaxed format. Additional prompt variants provided explicit error-type lists or in-context examples; some prompts reversed or shuffled error-type order.",
            "human_evaluation_setup": "Human evaluators were used only to validate dataset quality: the paper reports selecting 180 cases for human evaluation; three evaluators were involved (appendix clarifies two primary annotators checking 10 cases per error type per dataset with a third adjudicator for disagreements). Consensus required; result: 92.5% of generated cases satisfied the generation rules.",
            "agreement_metric": "No reported direct agreement metric comparing LLM judgments to human judgments. The only reported human-related statistic is dataset-validation quality: 92.5% of sampled generated cases met the generation requirements (human consensus). The paper explicitly states that differences between human and machine evaluations were not analyzed and are left for future work.",
            "losses_identified": "The paper does not present a direct, empirical list of 'what is lost' when replacing human judges with LLM judges (no human-vs-LLM judgment comparison was performed). However, based on the paper's measured LLM limitations, the following degradations are implied when relying on LLMs as judges instead of humans: (1) poor detection and correction of calculation errors (lowest accuracies), (2) severe difficulty recognizing 'missing step' errors, (3) systematic misclassification bias (many error types get misclassified as 'calculation error'), (4) sensitivity to prompt wording (open-source models in particular), and (5) lower robustness in open-source models compared to closed-source models. The authors explicitly note that they will explore whether there are differences between humans and machines in identifying and rectifying errors in future work.",
            "examples_of_loss": "Concrete results from the paper that illustrate these LLM shortcomings (and thus potential losses relative to humans): (a) 'Calculation error' is the most challenging error type with an average accuracy of ~26.3% across models (Table 4 / Sec. 4.2); (b) 'Missing step' classification accuracy in ET is extremely low (2.9% in one aggregated measure), indicating difficulty detecting omitted reasoning steps; (c) classification statistics show many golden error types are frequently predicted as 'calculation error' (Table 5), indicating mis-understanding of error taxonomy; (d) open-source models' judgments are highly prompt-sensitive (EP F1 variation up to ~0.2 across prompt variants, Table 6 and Sec. 4.3); (e) stopping the solution at the error step helped LLMs in EP and EC tasks, while continuing to a completed (but incorrect) solution confused LLM judges (Sec. 4.4 Influence of Stopping at Error Step). These results imply LLM judges can miss subtle, structural, or taxonomy-sensitive errors that humans would more reliably detect.",
            "counterexamples_or_caveats": "The paper reports several caveats and partial mitigations: (1) High-performing closed-source LLMs (GPT-4, GLM-4) substantially outperform other models and show strong abilities on many judging tasks—GPT-4's overall error-identification/correction performance is substantially higher than other models (Sec. 4.1). (2) Providing explicit error-type information in prompts markedly improves performance: giving error-type hints increases average EC accuracy by ~47.9% and ES accuracy by ~45.9% (Sec. 4.3), meaning LLM judges can match or approach desired behavior when given structured, explicit guidance. (3) For some error types (e.g., hallucination, unit-conversion, formula-confusion), top models (GPT-4, GLM-4) perform well, so LLM judges are not uniformly worse than humans across all categories. (4) The paper's human evaluation was limited to dataset validation (92.5% cases satisfied rules) and does not serve as a human benchmark for the LLM judgments—therefore no direct counterexample of LLM outperforming human judgment is presented because that comparison was not carried out.",
            "paper_reference": "Human Evaluation (Sec. 3 'Human Evaluation'), Limitations (Sec. 'Limitations'—authors state they will explore differences between humans and machines in future), Error-type difficulty and misclassification (Sec. 4.2 and Table 3/4/5), Prompt robustness and effect of providing error types (Sec. 4.3, Tables 7-9), Influence of Stopping at Error Step (Sec. 4.4).",
            "uuid": "e9920.0",
            "source_info": {
                "paper_title": "Evaluating Mathematical Reasoning of Large Language Models: A Focus on Error Identification and Correction",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evaluating language models for mathematics through interactions",
            "rating": 2
        },
        {
            "paper_title": "Efficiently measuring the cognitive ability of llms: An adaptive testing perspective",
            "rating": 1
        },
        {
            "paper_title": "Evaluating and improving tool-augmented computation-intensive math reasoning",
            "rating": 1
        },
        {
            "paper_title": "Large language models cannot self-correct reasoning yet",
            "rating": 2
        },
        {
            "paper_title": "Novice learner and expert tutor: Evaluating math reasoning abilities of large language models with misconceptions",
            "rating": 2
        }
    ],
    "cost": 0.01370675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Evaluating Mathematical Reasoning of Large Language Models: A Focus on Error Identification and Correction</h1>
<p>Xiaoyuan $\mathbf{L i}^{1}$, Wenjie Wang ${ }^{2 <em>}$, Moxin $\mathbf{L i}^{2}$, Junrong Guo ${ }^{1}$, Yang Zhang ${ }^{1}$, Fuli Feng ${ }^{1 </em>}$<br>University of Science and Technology of China ${ }^{1}$<br>National University of Singapore ${ }^{2}$<br>{xiaoyuanli, godrong,zy2015}@mail.ustc.edu.cn limoxin@u.nus.edu<br>{wenjiewang96, fulifeng93}@gmail.com</p>
<h4>Abstract</h4>
<p>The rapid advancement of Large Language Models (LLMs) in the realm of mathematical reasoning necessitates comprehensive evaluations to gauge progress and inspire future directions. Existing assessments predominantly focus on problem-solving from the examinee perspective, overlooking a dual perspective of examiner regarding error identification and correction. From the examiner perspective, we define four evaluation tasks for error identification and correction along with a new dataset with annotated error types and steps. We also design diverse prompts to thoroughly evaluate eleven representative LLMs. Our principal findings indicate that GPT-4 outperforms all models, while open-source model LLaMA-27B demonstrates comparable abilities to closedsource models GPT-3.5 and Gemini Pro. Notably, calculation error proves the most challenging error type. Moreover, prompting LLMs with the error types can improve the average correction accuracy by $47.9 \%$. These results reveal potential directions for developing the mathematical reasoning abilities of LLMs. Our code and dataset is available on https://github.com/LittleCirc1e/EIC.</p>
<h2>1 Introduction</h2>
<p>Large Language Models (Brown et al., 2020; Ouyang et al., 2022; Anil et al., 2023; OpenAI, 2023) have been successfully applied to mathematical reasoning, particularly in the Math Word Problems (MWP) (Kushman et al., 2014; Roy and Roth, 2018). LLMs cultivate a nuanced understanding of number-intensive context and multi-step reasoning. Cutting-edge models such as GPT-4 (OpenAI, 2023) have demonstrated impressive performance in addressing mathematical problems. For example, it has achieved an accuracy of $97 \%$ on the GSM8K dataset (Zhou et al., 2023). With the rapid advance-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Traditional evaluation on problem-solving and our evaluation on error identification and correction.
ment of LLMs, evaluating their effectiveness and reliability becomes increasingly crucial.</p>
<p>Existing evaluations are mainly from the examinee perspective, which directly assess the problemsolving capability of LLMs regarding the correctness of answers (Shakarian et al., 2023; Fu et al., 2023; Hong et al., 2024; Shi et al., 2022) and the consistency of intermediate reasoning steps (Wei et al., 2022; Golovneva et al., 2022; Zhang et al., 2023; Gaur and Saunshi, 2023). However, current research rarely delve into a dual perspective of examiner, i.e., the ability to identify and correct errors (Figure 1), which is equally crucial as problemsolving and worthwhile exploring. On one hand, the performance of traditional evaluation tasks is almost approaching saturation, calling an urgent need for new perspectives of evaluation. On the other hand, accurate error recognition and correction can facilitate the development of problem-solving capability.</p>
<p>Aiming to construct fine-grained evaluation on error recognition and correction, we define four distinct tasks. These tasks are as follows: 1) ErrorPresence Identification (EP): Identifying whether any error exists in the entire solution. 2) ErrorStep Identification (ES): Identifying the first wrong step within the solution, which is the root cause of</p>
<p>error. 3) Error-Type Identification (ET): Identifying the error type present in the first wrong step, such as calculation error. 4) Error Correction (EC): Rectifying the wrong steps and obtaining the final corrected answer. To our knowledge, we are the first to comprehensively define the four evaluation tasks for error identification and correction regarding mathematical reasoning.</p>
<p>Moving one step further, we consider constructing the evaluation dataset for these four tasks. Given a question, the dataset should include ground-truth answers, solutions with errors, step numbers of wrong steps, and types of errors. To construct this dataset, we need to define the types of error first. By collating examples from existing studies and practical instances, we distill nine common error types. Subsequently, harnessing the exceptional text generation capability of GPT-4 (OpenAI, 2023), we transform initially correct solutions of GSM8K (Cobbe et al., 2021) and MathQA (Amini et al., 2019) into solutions featuring singlestep and single-type errors. Through this approach, we establish a dataset comprising 1800 instances to evaluate the ability to recognize and rectify errors.</p>
<p>Based on the evaluation dataset, we test closedsource models, including GPT-3.5 (Ouyang et al., 2022), GPT-4 (OpenAI, 2023), GLM-4 (Du et al., 2022), Gemini Pro (Team et al., 2023), and their open-source counterparts such as LLaMA-2-7B, LLaMA-2-13B (Touvron et al., 2023), MetaMath7B, MetaMath-13B (Yu et al., 2023), Mistral-7B (Jiang et al., 2023), Llemma-7B (Azerbayev et al., 2023) and LEMA-7B (An et al., 2023). We devise diverse prompts of each task to evaluate the robustness of these LLMs. Through extensive experiments, we derive five key findings: 1) Across all four tasks, GPT-4 exhibits outstanding performance compared to other models with GLM-4 closely following. GPT-3.5, Gemini Pro, and LLaMA-27B show varying strengths and weaknesses. 2) While GPT-4 and GLM-4 demonstrate overall competence across four tasks, their ability to identify and rectify calculation error lags behind other error types. This suggests a need for further enhancement of the calculation capability of LLMs. 3) In the task of $E T$, many error types are easily recognized as the type of calculation error, with the type of missing step proving to be the most challenging to identify. 4) In the task of $E C$ and $E S$, by providing the error types, the average accuracy can be improved by $47.9 \%$ and $45.9 \%$, respectively. 5) Open-source models are highly influenced by prompts, while closed-source models demonstrate a comparatively robust performance.</p>
<p>Our contributions can be summarized as follows: 1) We define four tasks for evaluating the mathematical reasoning ability of LLMs regarding error identification and correction. To our knowledge, our work represents the first comprehensive assessment of the fine-grained capability of LLMs in recognizing and rectifying errors. 2) We define nine common error types and provide a dataset based on these error types. The dataset is intended to facilitate a more nuanced examination of the LLMs' performance in handling different error scenarios. 3) Through the comprehensive evaluation of four commercial and seven open-source LLMs, we derive key findings that hold insightful implications for the subsequent advancement of LLMs.</p>
<h2>2 Task Formulation</h2>
<p>As the proverb goes, errors are the stepping stones to wisdom. If a model is adept at mathematical reasoning, it should excel at identifying and correcting errors. Hence, we gauge the mathematical reasoning abilities of LLMs by assessing their proficiency in recognizing and rectifying errors. To comprehensively accomplish the evaluation, as shown in Figure 2, we define four tasks at a fine-grained level of error identification and correction.</p>
<ul>
<li>Task 1: Error-Presence Identification (EP) aims to detect whether any error exists in the solution of a mathematical question. Formally, given a mathematical question $q$ with an LLMgenerated solution $s, E P$ estimates the binary label $y$ that indicates whether $s$ contains errors. We design three prompts for open-source and closedsource LLMs for $E P^{1}$ : Simple requires LLMs to only output the judgment $\hat{y}$; Normal requires to not only output the judgment but also provide an explanation; Misleading informs LLMs that there might be errors in the solution and instructs LLMs to generate the judgment with explanation. To save space, we move detailed prompts to Figure 19 to 24. For evaluation of $E P$, we compute the accuracy in identifying error presence by $a c c_{1}=\frac{1}{N} \sum_{i=1}^{N} \mathbb{1}{y=\hat{y}}$, where $N$ is the number of evaluation cases and $\hat{y}$ is the predicted label by LLMs.</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of dataset construction and the four evaluation tasks. For dataset construction, we use GPT-4 to convert ground-truth solutions into wrong solutions containing specific error types. The four evaluation tasks comprehensively access LLMs' error identification and correction abilities from diverse perspectives.</p>
<ul>
<li><strong>Task 2: Error-Step Identification (ES)</strong> intends to find the first wrong step $t$ in a wrong solution. For <em>Task 2</em>, we require LLMs to output the judgment $\hat{y}$, and if $s$ contains errors, we also instruct LLMs to identify the first erroneous step $t$ in the solution. We devise the zero-shot prompts and few-shot prompts with in-context learning examples for the <em>ES</em> task. Figure 25 and 26 show the zero-shot prompts for open-source and closed-source models. For <em>ES</em> evaluation, we compute $acc_1$ as $EP$ and the accuracy in identifying error step by $acc_2 = \frac{1}{N} \sum_{i=1}^N \mathbb{1}{t = \hat{t}}$, where $\hat{t}$ denotes the first wrong step predicted by LLMs.</li>
<li><strong>Task 3: Error-Type Identification (ET)</strong> endeavors to identify the error type. We instruct LLMs to output the judgment for $y$ and identify the error type $c$ of the first wrong step if $s$ contains errors. Here, $c$ is selected from the pre-defined error types, such as calculation error. We define error types in the prompts and design zero-shot and few-shot prompts, where the few-shot prompt provides an example for each error type. Figure 29 and 30 showcase the zero-shot prompts for open-source and closed-source models. Considering that the order of error types might affect the accuracy of identifying error types, we design prompts that reverses the original order of error types and randomly shuffles them. We compute $acc_1$ and the accuracy in identifying error type $acc_3 = \frac{1}{N} \sum_{i=1}^N \mathbb{1}{c = \hat{c}}$, where $\hat{c}$ is the error type of the first wrong step identified by LLMs.</li>
<li><strong>Task 4: Error Correction (EC)</strong> seeks to rectify the error and output the correct solution. We prompt LLMs to output the judgment for $y$ and provide the corrected solution and answer $\hat{a}$ if $s$ contains errors. We devise zero-shot and few-shot prompts as <em>ES</em>. The prompts are displayed in Figure 31 and 32. We calculate $acc_1$ and the accuracy of correction $acc_4 = \frac{1}{N} \sum_{i=1}^N \mathbb{1}{a = \hat{a}}$, where $\hat{a}$ and $a$ are the predicted and ground-truth answers, respectively.</li>
</ul>
<p>For <em>Task 2</em> and <em>Task 4</em>, we propose to leverage the error type information in the prompts to hint LLMs for error step identification and error correction. Accordingly, we design the zero-shot and few-shot prompts with error type information as shown in Figure 27, 28, 33 and 34.</p>
<h2>3 Dataset Construction</h2>
<p>A significant challenge in achieving the four evaluation tasks is lacking compatible datasets with</p>
<table>
<thead>
<tr>
<th>Error Type</th>
<th>Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td>Calculation Error (CA)</td>
<td>Error appears during the calculation process.</td>
</tr>
<tr>
<td>Counting Error (CO)</td>
<td>Error occurs during the counting process.</td>
</tr>
<tr>
<td>Context Value Error (CV)</td>
<td>Error arises when attributes of named entities do not align with the information provided.</td>
</tr>
<tr>
<td>Hallucination (HA)</td>
<td>Error involves adding fictitious unrelated statements contradictory to the question.</td>
</tr>
<tr>
<td>Unit Conversion Error (UC)</td>
<td>Error occurs during unit conversion process.</td>
</tr>
<tr>
<td>Operator Error (OP)</td>
<td>Error involves a single operator being erroneously applied within the expression.</td>
</tr>
<tr>
<td>Formula Confusion Error (FC)</td>
<td>Error appears when applying formula in inappropriate scenario.</td>
</tr>
<tr>
<td>Missing Step (MS)</td>
<td>Error entails an incomplete generation of reasoning process, lacking a necessary step.</td>
</tr>
<tr>
<td>Contradictory Step (CS)</td>
<td>Error manifests inconsistency between preceding and subsequent reasoning steps.</td>
</tr>
</tbody>
</table>
<p>Table 1: Definition of nine common error types. Among them, unit conversion error, operator error, and formula confusion error can be categorized as common sense error, indicating errors in the relationships that should be understood within worldly common sense. The generation rules and examples are designed in Appendix C.
fine-grained error annotation. Therefore, we opt to construct a dataset that meets the requirements of our evaluation tasks. This dataset should encompass erroneous solutions, error steps, error types, and correct answers for mathematical questions.</p>
<p>Initially, we distill nine common error types from existing works (Wei et al., 2022; Toh et al., 2023; Lightman et al., 2023; Shakarian et al., 2023; Bubeck et al., 2023; Sawada et al., 2023; Suzgun et al., 2022; Lyu et al., 2023; Kojima et al., 2022; Li et al., 2023; Wang et al., 2022; Wang et al., 2023; Paul et al., 2023; Golovneva et al., 2022; Ribeiro et al., 2023; Lewkowycz et al., 2022) and practical examples. Table 1 shows the error names and definitions, covering the single-step and crossstep errors. The specific definition difference and illustration examples are presented in Appendix B.</p>
<p>Data Generation. As illustrated in Figure 2, we utilize the state-of-the-art LLM, GPT-4 (OpenAI, 2023), to generate the dataset, EIC-Math (Error Identification and Correction on Mathematical problems), to support the evaluation tasks. We design some generation rules for different error types, which regulate the generated wrong solutions to strictly meet the definition of one error type ${ }^{2}$. Then we construct the data generation prompt based on these generation rules and the in-context learning approach (Brown et al., 2020; Ouyang et al., 2022; Min et al., 2022) to instruct GPT-4 to transform correct solutions into wrong solutions. The data generation process is detailed in Appendix F.1.1 to save space. Note that we use two datasets GSM8K (Cobbe et al., 2021) and MathQA (Amini et al., 2019) to construct the error cases, where GSM8K has annotated multi-step solutions and MathQA adopts the correct solutions generated by GPT-3.5.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Each dataset is comprised of 100 cases per error type, resulting in a total of 1,800 cases for error identification and correction tasks.</p>
<p>Human Evaluation. To evaluate the quality of EIC-Math, we randomly select 180 cases and invite three evaluators for human evaluation. The results indicate that $92.5 \%$ cases have exactly satisfied the requirements of the data generation prompts, demonstrating the high quality of the generated dataset. More details of human evaluation can be found in Appendix D.</p>
<h2>4 Experiment</h2>
<p>We conduct extensive experiments to address the following research questions:</p>
<ul>
<li>RQ1: How do different LLMs perform on the four tasks on error identification and correction?</li>
<li>RQ2: How difficult are identifying and correcting different error types?</li>
<li>RQ3: How robust are LLMs to different prompts w.r.t. the four evaluation tasks?</li>
</ul>
<p>Experiment Setup. We select typical commercial closed-source LLMs, GPT-3.5, GPT-4, GLM-4, Gemini Pro, along with the general-purpose opensource LLaMA-2 series, and the state-of-the-art mathematical MetaMath series in their 7B and 13B versions for evaluation. Besides, we also evaluate other three cutting-edge mathematical models: Mistral, Llemma and LEMA in their 7B versions. ${ }^{3}$ To minimize randomness, we set the temperature to 0 . For ease of statistical analysis, we prompt closedsource LLMs to output in JSON format. However, open-source models do not consistently adhere to the format requirement, so we use a relaxed format for their prompts.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th></th>
<th>GSM8K</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>MathQA</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>EP</td>
<td>ES</td>
<td></td>
<td>ET</td>
<td>EC</td>
<td>Avg</td>
<td></td>
<td>EP</td>
<td>ES</td>
<td></td>
<td>ET</td>
<td>EC</td>
<td>Avg</td>
<td></td>
<td>Avg</td>
<td></td>
</tr>
<tr>
<td></td>
<td>$acc_{1}$</td>
<td>$acc_{2}$</td>
<td>$acc_{3}$</td>
<td>$acc_{4}$</td>
<td>$acc_{1}$</td>
<td>$acc_{1}$</td>
<td>$acc$</td>
<td>$acc_{1}$</td>
<td>$acc_{2}$</td>
<td>$acc_{3}$</td>
<td>$acc_{4}$</td>
<td>$acc_{1}$</td>
<td>$acc_{2}$</td>
<td>$acc$</td>
<td>$acc$</td>
<td>$acc$ $acc_{1}$</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>0.547</td>
<td>0.147</td>
<td>0.598</td>
<td>0.211</td>
<td>0.737</td>
<td>0.169</td>
<td>0.340</td>
<td>0.269</td>
<td>0.556</td>
<td>0.493</td>
<td>0.173</td>
<td>0.642</td>
<td>0.173</td>
<td>0.676</td>
<td>0.141</td>
<td>0.302</td>
</tr>
<tr>
<td>GPT-4</td>
<td>0.930</td>
<td>0.843</td>
<td>0.946</td>
<td>0.516</td>
<td>0.951</td>
<td>0.883</td>
<td>0.929</td>
<td>0.793</td>
<td>0.939</td>
<td>0.917</td>
<td>0.714</td>
<td>0.954</td>
<td>0.481</td>
<td>0.957</td>
<td>0.810</td>
<td>0.909</td>
</tr>
<tr>
<td>GLM-4</td>
<td>0.849</td>
<td>0.640</td>
<td>0.819</td>
<td>0.349</td>
<td>0.941</td>
<td>0.804</td>
<td>0.881</td>
<td>0.661</td>
<td>0.873</td>
<td>0.772</td>
<td>0.551</td>
<td>0.892</td>
<td>0.327</td>
<td>0.910</td>
<td>0.574</td>
<td>0.808</td>
</tr>
<tr>
<td>Gemini Pro</td>
<td>0.217</td>
<td>0.359</td>
<td>0.541</td>
<td>0.090</td>
<td>0.312</td>
<td>0.248</td>
<td>0.279</td>
<td>0.229</td>
<td>0.337</td>
<td>0.197</td>
<td>0.239</td>
<td>0.389</td>
<td>0.096</td>
<td>0.603</td>
<td>0.200</td>
<td>0.260</td>
</tr>
<tr>
<td>LLaMA-2-7B</td>
<td>0.538</td>
<td>0.184</td>
<td>0.914</td>
<td>0.048</td>
<td>0.396</td>
<td>0.067</td>
<td>0.871</td>
<td>0.209</td>
<td>0.680</td>
<td>0.536</td>
<td>0.176</td>
<td>0.861</td>
<td>0.052</td>
<td>0.358</td>
<td>0.039</td>
<td>0.792</td>
</tr>
<tr>
<td>LLaMA-2-13B</td>
<td>0.166</td>
<td>0.007</td>
<td>0.027</td>
<td>0.127</td>
<td>0.843</td>
<td>0.000</td>
<td>0.008</td>
<td>0.075</td>
<td>0.261</td>
<td>0.219</td>
<td>0.009</td>
<td>0.071</td>
<td>0.116</td>
<td>0.939</td>
<td>0.000</td>
<td>0.010</td>
</tr>
<tr>
<td>Avg</td>
<td>0.541</td>
<td>0.363</td>
<td>0.641</td>
<td>0.224</td>
<td>0.697</td>
<td>0.362</td>
<td>0.551</td>
<td>0.372</td>
<td>0.608</td>
<td>0.522</td>
<td>0.310</td>
<td>0.635</td>
<td>0.208</td>
<td>0.741</td>
<td>0.294</td>
<td>0.514</td>
</tr>
</tbody>
</table>
<p>Table 2: Average accuracy of different models in four tasks on GSM8K and MathQA separately under zero-shot prompts. $E P$ calculates the average $a c c_{1}$ over all error types. $E S$ calculates the average $a c c_{2}$ and $a c c_{1}$ as the values for the first and second column respectively. And $E T$ and $E C$ conduct similar calculation as $E S$. The first column of $A v g$ is the average value of $a c c_{1}, a c c_{2}, a c c_{3}$, and $a c c_{4}$ over all error types of models and represents the ability to identify and correct errors, while the second column is the average value of $a c c_{1}$ of four tasks and only represents the ability to identify errors.</p>
<table>
<thead>
<tr>
<th></th>
<th>CA</th>
<th></th>
<th>CO</th>
<th></th>
<th>CV</th>
<th></th>
<th>CS</th>
<th></th>
<th>MS</th>
<th>HA</th>
<th></th>
<th>UC</th>
<th></th>
<th>OP</th>
<th></th>
<th>FC</th>
<th></th>
<th>Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>$acc$</td>
<td>$acc_{1}$</td>
<td>$acc$</td>
<td>$acc_{1}$</td>
<td>$acc$</td>
<td>$acc_{1}$</td>
<td>$acc$</td>
<td>$acc_{1}$</td>
<td>$acc$</td>
<td>$acc_{1}$</td>
<td>$acc$</td>
<td>$acc_{1}$</td>
<td>$acc$</td>
<td>$acc_{2}$</td>
<td>$acc$</td>
<td>$acc_{2}$</td>
<td>$acc$</td>
<td>$acc_{1}$</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>0.201</td>
<td>0.366</td>
<td>0.285</td>
<td>0.518</td>
<td>0.246</td>
<td>0.581</td>
<td>0.339</td>
<td>0.640</td>
<td>0.189</td>
<td>0.525</td>
<td>0.319</td>
<td>0.645</td>
<td>0.215</td>
<td>0.354</td>
<td>0.256</td>
<td>0.619</td>
<td>0.261</td>
<td>0.629</td>
</tr>
<tr>
<td>GPT-4</td>
<td>0.606</td>
<td>0.681</td>
<td>0.733</td>
<td>0.955</td>
<td>0.841</td>
<td>0.986</td>
<td>0.719</td>
<td>0.934</td>
<td>0.608</td>
<td>0.935</td>
<td>0.860</td>
<td>0.968</td>
<td>0.833</td>
<td>0.988</td>
<td>0.780</td>
<td>0.988</td>
<td>0.878</td>
<td>0.995</td>
</tr>
<tr>
<td>GLM-4</td>
<td>0.338</td>
<td>0.468</td>
<td>0.653</td>
<td>0.839</td>
<td>0.611</td>
<td>0.933</td>
<td>0.544</td>
<td>0.859</td>
<td>0.523</td>
<td>0.878</td>
<td>0.794</td>
<td>0.949</td>
<td>0.676</td>
<td>0.884</td>
<td>0.605</td>
<td>0.949</td>
<td>0.733</td>
<td>0.975</td>
</tr>
<tr>
<td>Gemini Pro</td>
<td>0.089</td>
<td>0.128</td>
<td>0.171</td>
<td>0.310</td>
<td>0.243</td>
<td>0.386</td>
<td>0.131</td>
<td>0.274</td>
<td>0.201</td>
<td>0.350</td>
<td>0.396</td>
<td>0.594</td>
<td>0.096</td>
<td>0.210</td>
<td>0.271</td>
<td>0.476</td>
<td>0.251</td>
<td>0.420</td>
</tr>
<tr>
<td>LLaMA-2-7B</td>
<td>0.310</td>
<td>0.675</td>
<td>0.131</td>
<td>0.533</td>
<td>0.195</td>
<td>0.695</td>
<td>0.239</td>
<td>0.698</td>
<td>0.236</td>
<td>0.821</td>
<td>0.234</td>
<td>0.641</td>
<td>0.148</td>
<td>0.540</td>
<td>0.210</td>
<td>0.735</td>
<td>0.141</td>
<td>0.586</td>
</tr>
<tr>
<td>LLaMA-2-13B</td>
<td>0.036</td>
<td>0.265</td>
<td>0.043</td>
<td>0.260</td>
<td>0.088</td>
<td>0.306</td>
<td>0.166</td>
<td>0.299</td>
<td>0.071</td>
<td>0.318</td>
<td>0.131</td>
<td>0.294</td>
<td>0.054</td>
<td>0.234</td>
<td>0.088</td>
<td>0.328</td>
<td>0.046</td>
<td>0.265</td>
</tr>
<tr>
<td>Avg</td>
<td>0.263</td>
<td>0.430</td>
<td>0.336</td>
<td>0.569</td>
<td>0.371</td>
<td>0.648</td>
<td>0.356</td>
<td>0.617</td>
<td>0.305</td>
<td>0.638</td>
<td>0.456</td>
<td>0.682</td>
<td>0.337</td>
<td>0.535</td>
<td>0.368</td>
<td>0.682</td>
<td>0.385</td>
<td>0.645</td>
</tr>
</tbody>
</table>
<p>Table 3: Average accuracy of different models in different error types on GSM8K and MathQA under zero-shot prompts. We use the first two letters of the name of error type to represent it. The calculation of the first and second column is similar as the $A v g$ in Table 2.</p>
<h3>4.1 Model Performance (RQ1)</h3>
<p>Overall Performance. Table 2 presents the average accuracy of each LLM in four tasks on the EICMath dataset with GSM8K and MathQA. Overall, GPT-4 demonstrates overwhelming superiority, followed by GLM-4. GPT-3.5, Gemini Pro, and LLaMA-2-7B have their own strengths and weaknesses in four tasks. It is noteworthy that LLaMA-2-7B performs better than LLaMA-2-13B, which may be related to inverse scaling (McKenzie et al., 2023). This suggests that the ability of models to identify and correct errors does not necessarily increase with model size. Moreover, the mathematical models can only provide answers without error identification or correction abilities, and thus their accuracy is low as showcased in Appendix E. 4 and F.2. This indicates that they can only solve problems and lack comprehensive reasoning abilities.
Comparison Across Tasks. The average accuracy of $E P\left(a c c_{1}\right)$ is the highest among the four tasks $\left(a c c_{1}, a c c_{2}, a c c_{3}, a c c_{4}\right)$, as it is the simplest. $E S$ $\left(a c c_{2}\right)$ and $E T\left(a c c_{3}\right)$ tend to have close average accuracy compared to $E C\left(a c c_{4}\right)$, despite being intuitively less challenging. Actually, $E S$ involves an additional counting process, while $E T$ involves additional classification, leading to different emphases. It can also be noted that the average accuracy $a c c_{1}$ fluctuates across the four tasks, which is due to the efforts of LLMs to maintain consistency with different generated contents.</p>
<p>Regarding the difference in two average accuracy $\left(a c c_{1}, a c c_{4}\right)$ between $E C$, among the models with poor performance, Gemini Pro exhibits the smallest difference, while LLaMA-2-7B shows the largest. This suggests that Gemini Pro is cautious in error identification, with most identified errors being correctable, whereas LLaMA-2-7B is more liberal in error identification rather than correction.
Comparison Between Datasets. From the perspective of two datasets, it is often observed that the same model on MathQA tends to have lower accuracy across the four tasks compared to GSM8K. This is attributed to the higher difficulty level of MathQA.
Future Direction. Additionally, despite the overwhelming superiority of GPT-4, its average accuracy across the four tasks on the two simple MWP datasets is only $76.2 \%$. This indicates that the error identification and correction tasks we design are challenging, and the lack of error identification and correction capability in LLMs somewhat restricts their mathematical reasoning abilities.</p>
<h3>4.2 Error Type Analysis (RQ2)</h3>
<p>Difficulty Levels of Error Types. In Table 3, we compute the average accuracy of each model across</p>
<table>
<thead>
<tr>
<th></th>
<th>CA</th>
<th></th>
<th>CO</th>
<th></th>
<th>CV</th>
<th></th>
<th>CS</th>
<th></th>
<th>MS</th>
<th></th>
<th>HA</th>
<th></th>
<th>UC</th>
<th></th>
<th>OP</th>
<th></th>
<th>FC</th>
<th></th>
<th>Avg</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>$a c c_{1}$</td>
<td>$a c c_{2}$</td>
<td>$a c c_{1}$</td>
<td>$a c c_{1}$</td>
<td>$a c c_{2}$</td>
<td>$a c c_{1}$</td>
<td>$a c c_{1}$</td>
<td>$a c c_{1}$</td>
<td>$a c c_{1}$</td>
<td>$a c c_{2}$</td>
<td>$a c c_{1}$</td>
<td>$a c c_{2}$</td>
<td>$a c c_{1}$</td>
<td>$a c c_{1}$</td>
<td>$a c c_{2}$</td>
<td>$a c c_{1}$</td>
<td>$a c c_{1}$</td>
<td>$a c c_{1}$</td>
<td>$a c c_{1}$</td>
<td></td>
</tr>
<tr>
<td>EP</td>
<td>-</td>
<td>0.350</td>
<td>-</td>
<td>0.482</td>
<td>-</td>
<td>0.575</td>
<td>-</td>
<td>0.552</td>
<td>-</td>
<td>0.557</td>
<td>-</td>
<td>0.609</td>
<td>-</td>
<td>0.428</td>
<td>-</td>
<td>0.648</td>
<td>-</td>
<td>0.584</td>
<td>-</td>
<td>0.532</td>
</tr>
<tr>
<td>ES</td>
<td>0.203</td>
<td>0.476</td>
<td>0.323</td>
<td>0.589</td>
<td>0.362</td>
<td>0.697</td>
<td>0.367</td>
<td>0.667</td>
<td>0.320</td>
<td>0.683</td>
<td>0.408</td>
<td>0.697</td>
<td>0.323</td>
<td>0.583</td>
<td>0.383</td>
<td>0.699</td>
<td>0.343</td>
<td>0.652</td>
<td>0.337</td>
<td>0.638</td>
</tr>
<tr>
<td>ET</td>
<td>0.312</td>
<td>0.541</td>
<td>0.204</td>
<td>0.682</td>
<td>0.177</td>
<td>0.751</td>
<td>0.163</td>
<td>0.713</td>
<td>0.029</td>
<td>0.763</td>
<td>0.433</td>
<td>0.817</td>
<td>0.298</td>
<td>0.655</td>
<td>0.082</td>
<td>0.785</td>
<td>0.241</td>
<td>0.761</td>
<td>0.215</td>
<td>0.719</td>
</tr>
<tr>
<td>EC</td>
<td>0.188</td>
<td>0.355</td>
<td>0.335</td>
<td>0.523</td>
<td>0.369</td>
<td>0.569</td>
<td>0.344</td>
<td>0.537</td>
<td>0.313</td>
<td>0.549</td>
<td>0.372</td>
<td>0.604</td>
<td>0.298</td>
<td>0.473</td>
<td>0.361</td>
<td>0.598</td>
<td>0.373</td>
<td>0.583</td>
<td>0.328</td>
<td>0.532</td>
</tr>
<tr>
<td>Avg</td>
<td>0.263</td>
<td>0.430</td>
<td>0.336</td>
<td>0.569</td>
<td>0.371</td>
<td>0.648</td>
<td>0.356</td>
<td>0.617</td>
<td>0.305</td>
<td>0.638</td>
<td>0.337</td>
<td>0.535</td>
<td>0.368</td>
<td>0.682</td>
<td>0.385</td>
<td>0.645</td>
<td>0.456</td>
<td>0.682</td>
<td>0.353</td>
<td>0.605</td>
</tr>
</tbody>
</table>
<p>Table 4: Average accuracy of different tasks in different error types on GSM8K and MathQA under zero-shot prompts. And we calculate the average $a c c_{1}$ over all models for $E P$, the average $a c c_{i}(i=2,3,4)$ and $a c c_{1}$ as the values for the first and second columns respectively for $E S, E T$ and $E C$.
the four tasks in each error type on two datasets to assess the difficulty levels of different error types. It is found that calculation error is the most challenging to identify and correct, with an average accuracy of only $26.3 \%$, while hallucination is the easiest, with an average accuracy of $45.6 \%$. It is noteworthy that although GPT-4 and GLM-4 perform well overall, their performance in identifying and correcting calculation error is significantly lower compared to other error types. This suggests that LLMs should focus more on developing their computational capability. In addition, difficulty in identifying missing step is attributed to its poorest performance in the $E T$ of $2.9 \%$ shown in Table 4, making it the most challenging type for LLMs to classify. This is because it requires traversing the entire solution's CoT to analyze whether essential reasoning steps are missing.
Comparison between Different Models on the Same Error Type. Furthermore, GPT-3.5 and Gemini Pro struggle with unit conversion error, and the LLaMA-2 series also perform poorly in unit conversion error and formula confusion error. At the same time, GPT-4 and GLM-4 perform well in unit conversion error and formula confusing error. We speculate that this may be related to the size of the stored parameter knowledge. Due to the lack of relevant common sense in the parameter knowledge, it becomes challenging to identify and correct related errors for smaller models.</p>
<p>The average accuracy of LLaMA-2-7B surprisingly reaches $31 \%$ in calculation error, on par with GLM-4. Compared to other error types, LLaMA-27B and LLaMA-2-13B excell in contradictory step but perform poorly in counting error.
Statistical Classification of Error Types. Table 5 provides statistic on the count of error types classified on GPT-3.5 with GSM8K. Similar statistics for most other models and datasets are presented in Appendix F.2.2. It can be observed that most of the error types are often misclassified as calculation error, which may be attributed to the models' lack of true understanding of the meanings of each error</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">CA</th>
<th style="text-align: center;">CO</th>
<th style="text-align: center;">CV</th>
<th style="text-align: center;">CS</th>
<th style="text-align: center;">MS</th>
<th style="text-align: center;">HA</th>
<th style="text-align: center;">UC</th>
<th style="text-align: center;">OP</th>
<th style="text-align: center;">FC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CA</td>
<td style="text-align: center;">119</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;">CO</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">108</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">CV</td>
<td style="text-align: center;">109</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">73</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: center;">CS</td>
<td style="text-align: center;">161</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">MS</td>
<td style="text-align: center;">122</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">109</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: center;">HA</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">138</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">UC</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">262</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">OP</td>
<td style="text-align: center;">145</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">73</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">FC</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">149</td>
</tr>
</tbody>
</table>
<p>Table 5: Counting statistics for error type classification of GSM8K on GPT-3.5 with varied prompts. Row and column headers denote the golden and the classified types, respectively. Darker color indicates larger counts.</p>
<table>
<thead>
<tr>
<th></th>
<th>GSM8K</th>
<th></th>
<th></th>
<th>MathQA</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Simple</td>
<td>Normal</td>
<td>Misleading</td>
<td>Simple</td>
<td>Normal</td>
<td>Misleading</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>0.705</td>
<td>0.759</td>
<td>0.543</td>
<td>0.698</td>
<td>0.680</td>
<td>0.621</td>
</tr>
<tr>
<td>GPT-4</td>
<td>0.672</td>
<td>0.875</td>
<td>0.805</td>
<td>0.555</td>
<td>0.741</td>
<td>0.713</td>
</tr>
<tr>
<td>GLM-4</td>
<td>0.854</td>
<td>0.795</td>
<td>0.750</td>
<td>0.808</td>
<td>0.722</td>
<td>0.678</td>
</tr>
<tr>
<td>Gemini Pro</td>
<td>0.701</td>
<td>0.705</td>
<td>0.740</td>
<td>0.703</td>
<td>0.718</td>
<td>0.752</td>
</tr>
<tr>
<td>LLaMA-2-7B</td>
<td>0.667</td>
<td>0.445</td>
<td>-</td>
<td>0.667</td>
<td>0.705</td>
<td>0.113</td>
</tr>
<tr>
<td>LLaMA-2-13B</td>
<td>0.667</td>
<td>0.691</td>
<td>-</td>
<td>0.649</td>
<td>0.658</td>
<td>0.099</td>
</tr>
</tbody>
</table>
<p>Table 6: F1 scores on $E P$ under three prompt settings.
type and relevant classification training data.</p>
<h3>4.3 Prompt Robustness (RQ3)</h3>
<p>We devise a variety of prompts for the four tasks to explore the robustness of different models to different prompts. In addition, we investigate whether providing the error types to models can improve the accuracy in $E T$ and $E C$.
Prompt Robustness of EP. For $E P$, we select 50 negative samples and add an equal number of positive samples for each error type, totaling 100 samples for testing. And in Table 6, we compute their average F1 scores under three different prompts: Simple, Normal and Misleading. By calculating the difference in average F1 scores across all error types for each model, we evaluate their robustness to different prompts. It is observed that closedsource models exhibit greater robustness to different prompts, with the maximum difference in average F1 scores around 0.2 . In contrast, open-source models are highly sensitive to different prompts, exhibiting a tendency to classify almost all cases as correct without much consideration under Sim-</p>
<table>
<thead>
<tr>
<th></th>
<th>GSM8K</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Zero-shot</td>
<td>Few-shot</td>
<td>Zero-shot-type</td>
<td>Few-shot-type</td>
<td>Zero-shot</td>
<td>Few-shot</td>
<td>Zero-shot-type</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>0.147</td>
<td>0.198</td>
<td>0.294</td>
<td>0.352</td>
<td>0.173</td>
<td>0.157</td>
<td>0.248</td>
</tr>
<tr>
<td>GPT-4</td>
<td>0.843</td>
<td>0.841</td>
<td>0.878</td>
<td>0.881</td>
<td>0.714</td>
<td>0.691</td>
<td>0.739</td>
</tr>
<tr>
<td>GLM-4</td>
<td>0.640</td>
<td>0.632</td>
<td>0.744</td>
<td>0.689</td>
<td>0.551</td>
<td>0.496</td>
<td>0.603</td>
</tr>
<tr>
<td>Gemini Pro</td>
<td>0.359</td>
<td>0.052</td>
<td>0.567</td>
<td>0.112</td>
<td>0.239</td>
<td>0.031</td>
<td>0.394</td>
</tr>
<tr>
<td>LLaMA-2-7B</td>
<td>0.184</td>
<td>0.109</td>
<td>0.209</td>
<td>0.094</td>
<td>0.176</td>
<td>0.133</td>
<td>0.197</td>
</tr>
<tr>
<td>LLaMA-2-13B</td>
<td>0.007</td>
<td>0.003</td>
<td>0.002</td>
<td>0.004</td>
<td>0.009</td>
<td>0.003</td>
<td>0.004</td>
</tr>
</tbody>
</table>
<p>Table 7: Average accuracy $a c c_{2}$ of models in $E S$ on GSM8K and MathQA separately under four different prompt settings. Zero-shot-type and Few-shot-type provide models with the error types. Few-shot is set to 2-shot. The maximum average accuracy for each model on each dataset is in boldface.</p>
<table>
<thead>
<tr>
<th></th>
<th>GSM8K</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Zero-shot</td>
<td>Few-shot</td>
<td>Zero-shot-reverse</td>
<td>Zero-shot-random</td>
<td>Zero-shot</td>
<td>Few-shot</td>
<td>Zero-shot-reverse</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>0.211</td>
<td>0.171</td>
<td>0.281</td>
<td>0.256</td>
<td>0.173</td>
<td>0.129</td>
<td>0.228</td>
</tr>
<tr>
<td>GPT-4</td>
<td>0.516</td>
<td>0.577</td>
<td>0.538</td>
<td>0.483</td>
<td>0.481</td>
<td>0.520</td>
<td>0.471</td>
</tr>
<tr>
<td>GLM-4</td>
<td>0.349</td>
<td>0.409</td>
<td>0.411</td>
<td>0.381</td>
<td>0.327</td>
<td>0.218</td>
<td>0.360</td>
</tr>
<tr>
<td>Gemini Pro</td>
<td>0.108</td>
<td>0.090</td>
<td>0.147</td>
<td>0.122</td>
<td>0.096</td>
<td>0.052</td>
<td>0.132</td>
</tr>
<tr>
<td>LLaMA-2-7B</td>
<td>0.048</td>
<td>0.076</td>
<td>0.097</td>
<td>0.081</td>
<td>0.052</td>
<td>0.104</td>
<td>0.121</td>
</tr>
<tr>
<td>LLaMA-2-13B</td>
<td>0.127</td>
<td>0.003</td>
<td>0.112</td>
<td>0.136</td>
<td>0.116</td>
<td>0.017</td>
<td>0.127</td>
</tr>
</tbody>
</table>
<p>Table 8: Average accuracy $a c c_{3}$ of models in $E T$ on GSM8K and MathQA separately under four different prompt settings. Few-shot is set to 2-shot. Few-shot-random and Few-shot-reverse present similar results and are included in Appendix F.2.3.</p>
<table>
<thead>
<tr>
<th></th>
<th>GSM8K</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Zero-shot</td>
<td>Few-shot</td>
<td>Zero-shot-type</td>
<td>Few-shot-type</td>
<td>Zero-shot</td>
<td>Few-shot</td>
<td>Zero-shot-type</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>0.296</td>
<td>0.169</td>
<td>0.477</td>
<td>0.594</td>
<td>0.274</td>
<td>0.141</td>
<td>0.402</td>
</tr>
<tr>
<td>GPT-4</td>
<td>0.901</td>
<td>0.883</td>
<td>0.922</td>
<td>0.929</td>
<td>0.834</td>
<td>0.810</td>
<td>0.847</td>
</tr>
<tr>
<td>GLM-4</td>
<td>0.853</td>
<td>0.804</td>
<td>0.912</td>
<td>0.937</td>
<td>0.692</td>
<td>0.574</td>
<td>0.694</td>
</tr>
<tr>
<td>Gemini Pro</td>
<td>0.117</td>
<td>0.248</td>
<td>0.844</td>
<td>0.283</td>
<td>0.082</td>
<td>0.200</td>
<td>0.680</td>
</tr>
<tr>
<td>LLaMA-2-7B</td>
<td>0.067</td>
<td>0.066</td>
<td>0.071</td>
<td>0.050</td>
<td>0.039</td>
<td>0.063</td>
<td>0.041</td>
</tr>
<tr>
<td>LLaMA-2-13B</td>
<td>0.000</td>
<td>0.006</td>
<td>0.000</td>
<td>0.010</td>
<td>0.000</td>
<td>0.018</td>
<td>0.000</td>
</tr>
</tbody>
</table>
<p>Table 9: Average accuracy $a c c_{4}$ of models in $E C$ on GSM8K and MathQA separately under four different prompt settings. Zero-shot-type and Few-shot-type provide models with the error types. Few-shot is set to 2-shot. The maximum average accuracy for each model on each dataset is in boldface.
ple and being misled to mostly classify cases as incorrect under Misleading.</p>
<p>Prompt Robustness of ES. For $E S$, we design zeroshot and few-shot prompts for comparison and find that increasing the shot has minimal effect on improving the accuracy of this task and could even be counterproductive in Table 7. This indicates that simple examples can not make models fully understand the meaning of identifying the first erroneous step. By providing models with the error types, the accuracy of identifying error steps has been significantly improved, with an average increase of $45.9 \%$ times and maximum increase of 12.71 times. This informs that carefully designed examples can effectively improve the models' ability to identify erroneous steps.</p>
<p>Prompt Robustness of ET. For $E T$, we define nine error types in the prompts and design zero-shot and few-shot prompts. Recognizing that the sequence of error types may impact the accuracy of identifying errors, we also devise prompts that reverse the default order of error types and randomly shuffle
them. In Table 8, the impact of increasing the shot on improving accuracy is also negligible by comparing zero-shot and few-shot prompts. The order of error types does indeed affect classification accuracy as shown in Table 35 and 36. For example, hallucination is listed last in the sequential prompt. The average classification accuracy of hallucination in the sequential prompt is much lower than that in the reversed order. It is noteworthy that in the random order, we place missing step first, but its classification accuracy remains consistently low, indicating its inherent difficulty in identification.
Prompt Robustness of EC. For $E C$, we adopt similar prompt settings with $E S$ and obtain similar results. Only delicately constructed prompts that provide the error types can effectively improve the models' ability to correct errors, with an average increase of $47.9 \%$ times and up to a maximum of 8.29 times as displayed in Table 9.</p>
<h3>4.4 In-depth Analysis</h3>
<p>Comparison with Traditional Task. We conduct traditional task by inputting the questions from our</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Accuracy of traditional task and our task on GSM8K.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Accuracy of incomplete cases and complete cases on GSM8K for closed-source models.</p>
<p>Dataset into LLMs and obtaining the solutions and answers as outputs. The average accuracy of traditional task and our task is showcased in Figure 3. And details are in Appendix F.2.4. It can be observed that closed-source models perform well on both datasets in traditional task, while among the open-source models, only MetaMath series achieve high accuracy on GSM8K, possibly due to overfitting. It is worth noting that the ability of LLaMA-2-7B to identify and correct errors is greater than its problem-solving ability. However, the accuracy of traditional task is overall higher than that of our proposed task, which indicates the significance of our evaluation task in improving LLMs' mathematical reasoning abilities.</p>
<h3>Influence of Stopping at Error Step</h3>
<p>We investigate the comparison between writing only up to the error step in the solution and continuing from the error step to complete the solution. It can be observed from Figure 4 that, for both <em>EP</em> and <em>EC</em>, stopping at the error step aids in error identification and correction. Continuing from the error step to complete the solution may confuse LLMs. More details can be found in Appendix F.2.5.</p>
<h2>5 Related Work</h2>
<h3>Mathematical Reasoning Evaluation</h3>
<p>Ever since the exceptional capabilities of LLMs have been applied to the field of mathematical reasoning, researchers have initiated assessments of their mathematical proficiency. Most of them have primarily focused on evaluating the correctness in solving mathematical problems based on whether the answers are accurate (Shakarian et al., 2023; Fu et al., 2023; Hong et al., 2024; Shi et al., 2022; Dahlgren Lindström and Sam Abraham, 2022; Frieder et al., 2023). The correctness and consistency of intermediate steps in the solutions are also commonly used as an evaluation criterion to assess the coherence of Chain of Thought (CoT) (Wei et al., 2022; Golovneva et al., 2022; Zhang et al., 2023; Gaur and Saunshi, 2023). Others employ human interactions to provide a dynamic evaluation of the answers and intermediate steps (Zhuang et al., 2023; Collins et al., 2023). However, little work has investigated LLMs' ability to identify and correct errors, or only from a macro perspective and conduct simple experiments (Liu et al., 2023; Yen and Hsu, 2023; Valmeekam et al., 2023; Stechly et al., 2023; An et al., 2023; Huang et al., 2023). Hence, there lacks a fine-grained study that comprehensively evaluates the LLMs' abilities in error identification and correction.</p>
<h3>In-Context Learning</h3>
<p>With the widespread adoption of LLMs (Brown et al., 2020; Ouyang et al., 2022; Anil et al., 2023; OpenAI, 2023), in-context learning (Brown et al., 2020; Ouyang et al., 2022; Min et al., 2022) has emerged as the predominant method for deploying downstream tasks. This approach involves providing LLMs with textual instructions and examples without the need for parameter updates. When applied to dataset generation, studies have found that datasets generated by LLMs exhibit higher quality in terms of accuracy and fluency (Lu et al., 2022; Min et al., 2022) compared to datasets annotated by crowd-sourced workers. Furthermore, the cost of generating data through LLMs is significantly lower than the expense associated with crowd-sourced annotations. Consequently, employing LLMs for data generation proves to be a viable alternative to crowd-sourced annotation (Liu et al., 2022; Wiegreffe et al., 2022; West et al., 2022). Therefore, we opt to utilize in-context learning on the state-of-the-art GPT-4 to generate the evaluation dataset.</p>
<p>Program Repair. Automated program repair (APR), aimed at fixing potential errors within programs, plays a crucial role in the software development cycle. Early approaches (Nguyen et al., 2013; Qi et al., 2014; Diekmann and Tratt, 2018) were symbolic and often relied on error recovery mechanisms within parsers to enumerate local edits. More recently, neural networks have been successfully used to correct syntax and compilation errors (Yasunaga and Liang, 2020; Yasunaga and Liang, 2021; Ahmed et al., 2021; Berabi et al., 2021). Besides, some systems also integrate symbolic and neural components to rectify faulty programs (Bavishi et al., 2022). Due to the remarkable capabilities of LLMs, they are utilized in program repair to detect, locate and rectify errors, enabling automated software development workflows (Joshi et al., 2023; Jin et al., 2023; Bouzenia et al., 2024). These efforts differ from ours in that they focus on identifying and correcting errors within code scenarios, while we concentrate on mathematical reasoning problems.</p>
<h2>6 Conclusion</h2>
<p>We systematically delineated four evaluation tasks aimed at identifying and rectifying errors, marking the first comprehensive attempt in this domain. To facilitate the evaluation process, we curated a dataset categorized by error types. Furthermore, we conducted thorough experiments across various closed-source and open-source models, yielding significant insights that bolster the mathematical reasoning capabilities of LLMs. In future research, we will explore more avenues such as rectifying single-step and single-type errors, single-step multitype errors on various LLMs, and increasing the continuity of our correction prompts which can rectify errors based on incorrect preceding steps.</p>
<h2>Limitations</h2>
<p>In future research, we can focus on the following directions. First, we mainly investigated the capability of different LLMs in identifying and rectifying single-step and single-type errors, and future research can address combined errors involving single-step and multiple-type, as well as singletype and multiple-step, and more complex errors such as semantic comprehension error. Futhermore, our correction prompts did not emphasize continuity, whose meaning is to correct on the basis of incorrect steps. And correction based on continuity may indeed pose a greater challenge. Lastly, our discussion focused solely on machine performance regarding these error types, and we will explore if there are differences between humans and machines in identifying and rectifying errors in future.</p>
<h2>Ethics Statement</h2>
<p>One ethical concern revolves around the accuracy and reliability of LLMs in recognizing and correcting errors in mathematical reasoning. Errors in mathematical reasoning can have profound consequences, particularly in educational contexts where students rely on accurate feedback and guidance to develop their mathematical skills. Therefore, ensuring the robustness and integrity of LLMs’ error correction capabilities through rigorous validation and continuous improvement processes is essential to mitigate the risks associated with erroneous corrections. Moreover, ethical responsibilities extend to the broader societal impacts of LLMs’ role in mathematical education and problem-solving. As LLMs increasingly assist students and professionals in mathematical reasoning tasks, the dissemination of accurate and credible mathematical knowledge becomes paramount. Ensuring that LLMs are equipped to discern and rectify errors in mathematical reasoning contributes to fostering a culture of mathematical integrity, critical thinking, and intellectual hones. Lastly, we will check there are no ethical issues in the constructed dataset before releasing it publicly, which can be used for research purposes by related researchers.</p>
<h2>References</h2>
<p>Toufique Ahmed, Noah Rose Ledesma, and Premkumar Devanbu. 2021. Synfix: Automatically fixing syntax errors using compiler diagnostics. arXiv preprint arXiv:2104.14671.</p>
<p>Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2357-2367.</p>
<p>Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, and Weizhu Chen. 2023. Learning from mistakes makes llm better reasoner. arXiv preprint arXiv:2310.20689.</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403.</p>
<p>Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. 2023. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631.</p>
<p>Rohan Bavishi, Harshit Joshi, José Cambronero, Anna Fariha, Sumit Gulwani, Vu Le, Ivan Radiček, and Ashish Tiwari. 2022. Neurosymbolic repair for lowcode formula languages. Proceedings of the ACM on Programming Languages, 6(OOPSLA2):1093-1122.</p>
<p>Berkay Berabi, Jingxuan He, Veselin Raychev, and Martin Vechev. 2021. Tfix: Learning to fix coding errors with a text-to-text transformer. In International Conference on Machine Learning, pages 780-791. PMLR.</p>
<p>Islem Bouzenia, Premkumar Devanbu, and Michael Pradel. 2024. Repairagent: An autonomous, llmbased agent for program repair. arXiv preprint arXiv:2403.17134.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.</p>
<p>Woong Choi. 2023. Assessment of the capacity of chatgpt as a self-learning tool in medical pharmacology: A study using mcqs.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Katherine M Collins, Albert Q Jiang, Simon Frieder, Lionel Wong, Miri Zilka, Umang Bhatt, Thomas Lukasiewicz, Yuhuai Wu, Joshua B Tenenbaum, William Hart, et al. 2023. Evaluating language models for mathematics through interactions. arXiv preprint arXiv:2306.01694.</p>
<p>Adam Dahlgren Lindström and Savitha Sam Abraham. 2022. Clevr-math: A dataset for compositional language, visual and mathematical reasoning. In International Joint Conference on Learning and Reasoning, 16th International Workshop on Neural-Symbolic Learning and Reasoning (NeSy
2022), Windsor, UK, September 28-30, 2022, volume 3212, pages 155-170. Technical University of Aachen.</p>
<p>Lukas Diekmann and Laurence Tratt. 2018. Don’t panic! better, fewer, syntax errors for lr parsers. arXiv preprint arXiv:1804.07133.</p>
<p>Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320-335.</p>
<p>Simon Frieder, Luca Pinchetti, Alexis Chevalier, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, and Julius Berner. 2023. Mathematical capabilities of chatgpt. arXiv preprint arXiv:2301.13867.</p>
<p>Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. 2023. Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance. arXiv preprint arXiv:2305.17306.</p>
<p>Vedant Gaur and Nikunj Saunshi. 2023. Reasoning in large language models through symbolic math word problems. arXiv preprint arXiv:2308.01906.</p>
<p>Olga Golovneva, Moya Peng Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam FazelZarandi, and Asli Celikyilmaz. 2022. Roscoe: A suite of metrics for scoring step-by-step reasoning. In The Eleventh International Conference on Learning Representations.</p>
<p>Pengfei Hong, Deepanway Ghosal, Navonil Majumder, Somak Aditya, Rada Mihalcea, and Soujanya Poria. 2024. Stuck in the quicksand of numeracy, far from agi summit: Evaluating llms' mathematical competency through ontology-guided perturbations. arXiv preprint arXiv:2401.09395.</p>
<p>Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2023. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825.</p>
<p>Matthew Jin, Syed Shahriar, Michele Tufano, Xin Shi, Shuai Lu, Neel Sundaresan, and Alexey Svyatkovskiy. 2023. Inferfix: End-to-end program repair with llms. In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pages 1646-1656.</p>
<p>Harshit Joshi, José Cambronero Sanchez, Sumit Gulwani, Vu Le, Gust Verbruggen, and Ivan Radiček. 2023. Repair is nearly generation: Multilingual program repair with llms. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 5131-5140.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213.</p>
<p>Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. Mawps: A math word problem repository. In Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies, pages 1152-1157.</p>
<p>Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. 2014. Learning to automatically solve algebra word problems. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 271-281.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2023. Making large language models better reasoners with stepaware verifier.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Let's verify step by step.</p>
<p>Alisa Liu, Swabha Swayamdipta, Noah A Smith, and Yejin Choi. 2022. Wanli: Worker and ai collaboration for natural language inference dataset creation. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 6826-6847.</p>
<p>Naiming Liu, Shashank Sonkar, Zichao Wang, Simon Woodhead, and Richard G Baraniuk. 2023. Novice learner and expert tutor: Evaluating math reasoning abilities of large language models with misconceptions. arXiv preprint arXiv:2310.02439.</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086-8098.</p>
<p>Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. 2023. Faithful chain-ofthought reasoning. arXiv preprint arXiv:2301.13379.</p>
<p>Ian R McKenzie, Alexander Lyzhov, Michael Pieler, Alicia Parrish, Aaron Mueller, Ameya Prabhu, Euan McLean, Aaron Kirtland, Alexis Ross, Alisa Liu, et al. 2023. Inverse scaling: When bigger isn't better. arXiv preprint arXiv:2306.09479.</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048-11064, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Hoang Duong Thien Nguyen, Dawei Qi, Abhik Roychoudhury, and Satish Chandra. 2013. Semfix: Program repair via semantic analysis. In 2013 35th International Conference on Software Engineering (ICSE), pages 772-781. IEEE.</p>
<p>OpenAI. 2023. Gpt-4 technical report. arxiv 2303.08774. View in Article, 2:13.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback, 2022. URL https://arxiv. org/abs/2203.02155, 13.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are nlp models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080-2094.</p>
<p>Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. 2023. Refiner: Reasoning feedback on intermediate representations. arXiv preprint arXiv:2304.01904.</p>
<p>Yuhua Qi, Xiaoguang Mao, Yan Lei, Ziying Dai, and Chengsong Wang. 2014. The strength of random search on automated program repair. In Proceedings of the 36th international conference on software engineering, pages 254-265.</p>
<p>Danilo Ribeiro, Shen Wang, Xiaofei Ma, Henry Zhu, Rui Dong, Deguang Kong, Juliette Burger, Anjelica Ramos, William Wang, Zhiheng Huang, et al. 2023. Street: A multi-task structured reasoning and explanation benchmark. arXiv preprint arXiv:2302.06729.</p>
<p>Subhro Roy and Dan Roth. 2018. Mapping to declarative knowledge for word problem solving. Transactions of the Association for Computational Linguistics, 6:159-172.</p>
<p>Tomohiro Sawada, Daniel Paleka, Alexander Havrilla, Pranav Tadepalli, Paula Vidas, Alexander Kranias, John Nay, Kshitij Gupta, and Aran Komatsuzaki. 2023. Arb: Advanced reasoning benchmark for large language models. In The 3rd Workshop on Mathematical Reasoning and AI at NeurIPS'23.</p>
<p>Paulo Shakarian, Abhinav Koyyalamudi, Noel Ngu, and Lakshmivihari Mareedu. 2023. An independent evaluation of chatgpt on mathematical word problems (mwp). arXiv preprint arXiv:2302.13814.</p>
<p>Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, et al. 2022. Language models are multilingual chain-of-thought reasoners. arXiv preprint arXiv:2210.03057.</p>
<p>Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. 2023. Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems. arXiv preprint arXiv:2310.12397.</p>
<p>Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261.</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805.</p>
<p>Vernon Toh, Ratish Puduppully, and Nancy F Chen. 2023. Veritymath: Advancing mathematical reasoning by self-verification through unit consistency. arXiv preprint arXiv:2311.07172.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Karthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati. 2023. Can large language models really improve by self-critiquing their own plans? arXiv preprint arXiv:2310.08118.</p>
<p>Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. 2022. Towards understanding chain-of-thought prompting: An empirical study of what matters. arXiv preprint arXiv:2212.10001.</p>
<p>Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Plan-and-solve prompting: Improving zero-shot chain-ofthought reasoning by large language models. arXiv preprint arXiv:2305.04091.</p>
<p>Yan Wang, Xiaojiang Liu, and Shuming Shi. 2017. Deep neural solver for math word problems. In Proceedings of the 2017 conference on empirical methods in natural language processing, pages 845-854.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837.</p>
<p>Peter West, Chandra Bhagavatula, Jack Hessel, Jena Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, and Yejin Choi. 2022. Symbolic knowledge distillation: from general language models to commonsense models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4602-4625.</p>
<p>Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, and Yejin Choi. 2022. Reframing human-ai collaboration for generating free-text explanations. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 632-658.</p>
<p>Michihiro Yasunaga and Percy Liang. 2020. Graphbased, self-supervised program repair from diagnostic feedback. In International Conference on Machine Learning, pages 10799-10808. PMLR.</p>
<p>Michihiro Yasunaga and Percy Liang. 2021. Break-it-fix-it: Unsupervised learning for program repair. In International conference on machine learning, pages 11941-11952. PMLR.</p>
<p>An-Zi Yen and Wei-Ling Hsu. 2023. Three questions concerning the use of large language models to facilitate mathematics learning. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 3055-3069.</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284.</p>
<p>Beichen Zhang, Kun Zhou, Xilin Wei, Wayne Xin Zhao, Jing Sha, Shijin Wang, and Ji-Rong Wen. 2023. Evaluating and improving tool-augmented computation-intensive math reasoning. arXiv preprint arXiv:2306.02408.</p>
<p>Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, et al. 2023. Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. arXiv preprint arXiv:2308.07921.</p>
<p>Yan Zhuang, Qi Liu, Yuting Ning, Weizhe Huang, Rui Lv, Zhenya Huang, Guanhao Zhao, Zheng Zhang,</p>
<p>Qingyang Mao, Shijin Wang, et al. 2023. Efficiently measuring the cognitive ability of llms: An adaptive testing perspective. arXiv preprint arXiv:2306.10512.</p>
<h2>A Dataset Selection</h2>
<p>The datasets commonly used for MWP assessment include GSM8K (Cobbe et al., 2021), MathQA (Amini et al., 2019), MAWPS (Koncel-Kedziorski et al., 2016), SVAMP (Patel et al., 2021), and MATH23K (Wang et al., 2017). GSM8K corresponds to elementary-level mathematical problems. MathQA comprises GRE-level mathematical questions, which is served as a benchmark for American college entrance exams. MAWPS is akin to the fourth-grade level. SVAMP focuses on univariate linear problems, where all questions can be solved using a single expression. And MATH23K is a large-scale Chinese dataset containing Chinese mathematical word problems and their corresponding expression solutions. Considering various factors, we select GSM8K and MathQA as our primary datasets. Due to MathQA's multiple-choice format and considerable noise in its original annotations, we employed GPT-3.5 to generate correct solutions for its questions. For GSM8K, we utilize its annotated solutions.</p>
<h2>B Detailed Error Type Definition</h2>
<p>A substantial collection of erroneous instances is gathered from existing studies (Wei et al., 2022; Toh et al., 2023; Lightman et al., 2023; Shakarian et al., 2023; Bubeck et al., 2023; Sawada et al., 2023; Suzgun et al., 2022; Lyu et al., 2023; Kojima et al., 2022; Li et al., 2023; Wang et al., 2022; Wang et al., 2023; Paul et al., 2023; Golovneva et al., 2022; Ribeiro et al., 2023; Lewkowycz et al., 2022) and practical scenarios. Subsequently, nine common and distinct error types are distilled, focusing on the single-step errors and cross-steps errors. The first seven types pertain to single-step errors, while the latter two relate to cross-steps errors.</p>
<p>Calculation Error: Error appears during the calculation process when the formula is entirely correct. It is well-known that LLMs often exhibit inconsistent computation units, resulting in simple arithmetic errors (Toh et al., 2023).</p>
<p>Counting Error: Error occurs during the counting process. Bubeck et al., 2023 indicates that counting error is prone due to not only the challenging implementation of this operation within
transformer structures but also the lack of relevant data in the training sets.</p>
<p>Context Value Error: Error arises when attributes of named entities (such as quantities) do not align with the information provided in the question. The tendency of LLMs to misinterpret problem meanings and erroneously substitute numerical values remains a prominent challenge in mathematics reasoning (Yen and Hsu, 2023).</p>
<p>Hallucination: Error involves adding fictitious unrelated statements contradictory to the question. This refers to the inclusion of information in the solution that is not present in the question statement, thereby disrupting the final answer (Lyu et al., 2023).</p>
<p>Unit Conversion Error: Error occurs during unit conversion process, indicating a misunderstanding of the quantitative relationships between units (Choi, 2023).</p>
<p>Operator Error: Error involves a single operator being erroneously applied within the expression due to a misconception of operator concepts (Paul et al., 2023).</p>
<p>Formula Confusion Error: Error appears when applying formula in inappropriate scenario. This stems from a misunderstanding of formula meanings, leading to an error in their application (Lightman et al., 2023).</p>
<p>Missing Step: Error entails an incomplete generation of reasoning process, lacking a necessary inference step. The addition of such a step could yield the correct result (Wei et al., 2022).</p>
<p>Contradictory Step: Error manifests inconsistency between preceding and subsequent reasoning steps, resulting in discrepancy within the inference chain (Golovneva et al., 2022).</p>
<p>Among above, unit conversion error, operator error, and formula confusion error can be categorized as common sense error, indicating errors in the relationships that should be understood within worldly common sense. Here, common sense error leans toward factual error, while hallucination leans toward faithful error.</p>
<p>From the perspective of equation, calculation error is equivalent to errors on the right-hand side of the equation. Counting error, context value error, contradictory step, unit conversion error are equivalent to errors in one operand on the left-hand side of the equation. Operator error is equivalent to errors in one operator on the left-hand side of the equation, while formula confusion error, and hallucination are equivalent to errors in both operands</p>
<p>and operators on the left-hand side of the equation.</p>
<h2>C Generation Rules Design and Examples</h2>
<p>To generate cases conforming to the nine error types defined in Table 1, we formulate generation rules for each type and manually craft high-quality examples according to these rules for GPT-4 to emulate.</p>
<p>We solely focus on single-step and single-type errors. Given the error type and the original solution to the question, we instruct GPT-4 to randomly select a step and modify it according to the generation rule of this error type. To emulate a realistic error process, subsequent steps referencing the result of this modified step are also affected. We then filter out cases where the final results after transformation differ from the correct results for evaluation purpose.</p>
<p>Calculation Error: Only the calculation result of a randomly selected step in the original solution is modified, without altering any operands or operators within the expression.</p>
<p>Counting Error: Here, we address issues involving counting days. A modification is made solely to the count result of a step where counting occurs in the original solution. For instance, while the original solution counts Saturday and Sunday as two days, the transformed solution counts them as one day incorrectly.</p>
<p>Context Value Error: An incorrect reference to a number in the question is introduced solely in one step of the original solution. Since only one step is considered erroneous, all other steps referencing this number continue to do so correctly.</p>
<p>Hallucination: Additional information affecting the final outcome, not mentioned in the question statement, is inserted solely into one step of the original solution.</p>
<p>Unit Conversion Error: An incorrect unit conversion is applied solely to a step in the original solution where unit conversion appears.</p>
<p>Operator Error: A random modification is made solely to one operator within a formula of a step in the original solution, such as changing addition to subtraction, multiplication to division. The formula's result should remain correctly calculated after the operator change.</p>
<p>Formula Confusion Error: The formula used in one step of the original solution is mistakenly replaced, such as substituting the perimeter formula of a rectangle with the area formula.</p>
<p>Missing Step: The transformed solution should be one step shorter than the original solution. It can occur in three scenarios: deleting the first, middle, or last step. For the first step, step 1 often references the number from the question, so subsequent steps referencing its outcome should directly reference the number from the question after deleting it. If step 1 references multiple numbers, the largest one is selected for subsequent relevant steps. For middle steps, if the deleted middle step refers to the result of only one preceding step, subsequent relevant steps need to reference the result of the preceding step after deleting. Otherwise the largest number from multiple numbers it references is selected for subsequent relevant steps. For the last step, it can be simply deleted, and the result of the second-to-last step becomes the final outcome.</p>
<p>Contradictory Step: An erroneous reference to the result of the preceding relevant step is introduced solely into one step of the original solution. As only one step error is considered, all other steps referencing the result of the preceding relevant step continue to correctly reference it.</p>
<p>It is worth noting that errors involving counting error, unit conversion error, and formula confusion error require selecting appropriate questions and original solutions for transformation. However, other errors can be converted using any questions and original solutions.</p>
<p>Here is a [Question] and its [Correct solution]. We use them for converting different types of errors. The following are examples for the seven wrong types related to this [Question].
[Question]: On the weekend, Tony will walk to the store. On weekdays, he runs to the store. When he walks, he goes 2 MPH. When he runs, he goes 10 MPH. The store is 4 miles away. If he goes on Sunday, Tuesday, and Thursday, what is the average time in minutes that he spends to get to the store? [Correct solution]: On Sunday he takes 2 hours to get there because $4 / 2=2$
This takes him 120 minutes because $2 \times 60=120$
On Tuesday and Thursday, it takes him .4 hours to get to the store because $4 / 10=.4$
On Tuesday and Thursday, it takes him 24 minutes to get to the store because $.4 \times 60=24$
In total it takes him 168 minutes to get to the store because $120+24+24=168$
On average it takes him 56 minutes because 168 / $3=56$.</p>
<h2>Calculation Error:</h2>
<p>[Transformed solution]: On Sunday he takes 2</p>
<p>hours to get there because $4 / 2=2$
This takes him 120 minutes because $2 \times 60=120$
On Tuesday and Thursday, it takes him .4 hours to get to the store because $4 / 10=.4$
On Tuesday and Thursday, it takes him 24 minutes to get to the store because $.4 \times 60=24$
In total it takes him 168 minutes to get to the store because $120+24+24=168$
On average it takes him 55 minutes because 168 / $3=55$.
[Explanation]: The operands and operators of the formula in step 6 are correct, but only the result is incorrectly calculated as 55 instead of 56.</p>
<h2>Counting Error:</h2>
<p>[Transformed solution]: On Sunday he takes 2 hours to get there because $4 / 2=2$
This takes him 120 minutes because $2 \times 60=120$
On Tuesday and Thursday, it takes him .4 hours to get to the store because $4 / 10=.4$
On Tuesday and Thursday, it takes him 24 minutes to get to the store because $.4 \times 60=24$
In total it takes him 168 minutes to get to the store because $120+24+24=168$
On average it takes him 84 minutes because 168 / $2=84$.
[Explanation]: Step 6 counts Sunday, Tuesday, and Thursday wrongly as 2 days instead of 3 days, only resulting in an operand error in the formula.</p>
<h2>Context Value Error:</h2>
<p>[Transformed solution]: On Sunday he takes 2 hours to get there because $4 / 2=2$
This takes him 120 minutes because $2 \times 60=120$
On Tuesday and Thursday, it takes him .4 hours to get to the store because $4 / 10=.4$
On Tuesday and Thursday, it takes him . 2 hours to get to the store because $4 / 20=.2$
On Tuesday and Thursday, it takes him 12 minutes to get to the store because $.2 \times 60=12$
In total it takes him 144 minutes to get to the store because $120+12+12=144$
On average it takes him 48 minutes because 144 / $3=48$.
[Explanation]: Step 3 mistakenly references the number 20 instead of 10 from the question, only resulting in an operand error in the formula. The subsequent steps are affected by it. Please note that we only consider errors of single step and single type, and step 2 still correctly references 10.</p>
<h2>Hallucination:</h2>
<p>[Transformed solution]: On Sunday he takes 2 hours to get there because $4 / 2=2$
This takes him 120 minutes because $2 \times 60=120$</p>
<p>On Tuesday and Thursday, it takes him .4 hours to get to the store because $4 / 10=.4$
On Tuesday and Thursday, it takes him 24 minutes to get to the store because $.4 \times 60=24$
Because the road congestion on Tuesday takes an additional 20 minutes, so in total it takes him 168 minutes to get to the store because $120+24+24$ $+20=188$
On average it takes him On average it takes him 62.6 minutes because $188 / 3=62.6$.
[Explanation]: Step 5 adds the additional information <Because the road congestion on Tuesday takes an additional 20 minutes> not mentioned in the questions, causing the result of step 5 to be overestimated by 20. And it influences step 6, which references its result.</p>
<h2>Unit Conversion Error:</h2>
<p>[Transformed solution]: On Sunday he takes 2 hours to get there because $4 / 2=2$
This takes him 100 minutes because $2 \times 50=100$
On Tuesday and Thursday, it takes him .4 hours to get to the store because $4 / 10=.4$
On Tuesday and Thursday, it takes him 24 minutes to get to the store because $.4 \times 60=24$
In total it takes him 148 minutes to get to the store because $100+24+24=148$
On average it takes him 49.3 minutes because 148 $/ 3=49.3$.
[Explanation]: Step 2 performs an incorrect unit conversion and mistakenly assumes that one hour has 50 minutes, which only results in an error in one operand in the formula. The subsequent steps 5 and 6 are affected by it. Because we only consider errors of single step and single type, step 4 still correctly performs unit conversion.</p>
<h2>Operator Error:</h2>
<p>[Transformed solution]: On Sunday he takes 2 hours to get there because $4 / 2=2$
This takes him 120 minutes because $2 \times 60=120$
On Tuesday and Thursday, it takes him .4 hours to get to the store because $4 / 10=.4$
On Tuesday and Thursday, it takes him 24 minutes to get to the store because $.4 \times 60=24$
In total it takes him 168 minutes to get to the store because $120+24+24=168$
On average it takes him 171 minutes because 168 $+3=171$.
[Explanation]: Step 6 mistakenly uses addition instead of division, and only one operator in the formula is incorrect.</p>
<h2>Missing Step:</h2>
<p>[Transformed solution]: On Sunday he takes 2</p>
<p>hours to get there because $4 / 2=2$
On Tuesday and Thursday, it takes him .4 hours to get to the store because $4 / 10=.4$
On Tuesday and Thursday, it takes him 24 minutes to get to the store because $.4 \times 60=24$
In total it takes him 50 minutes to get to the store because $2+24+24=50$
On average it takes him 16.6 minutes because 50 / $3=16.6$.
[Explanation]: Step 4 does not convert the time he went to the store on Sunday from hours to minutes, but directly adds up the time on Sunday (hours) and the time on Tuesday and Thursday (minutes). So there is a missing step here to convert Sunday's time from hours to minutes.</p>
<h2>Contradictory Step:</h2>
<p>[Transformed solution]: On Sunday he takes 2 hours to get there because $4 / 2=2$
This takes him 120 minutes because $2 \times 60=120$
On Tuesday and Thursday, it takes him .4 hours to get to the store because $4 / 10=.4$
On Tuesday and Thursday, it takes him 24 minutes to get to the store because $.4 \times 60=24$
In total it takes him 188 minutes to get to the store because $140+24+24=188$
On average it takes him 62.6 minutes because 188 $/ 3=62.6$
[Explanation]: Step 5 erroneously references the result 140 of step 2 instead of 120, which only results in an error in one operand in the formula.</p>
<p>Here is another [Question] and its [Correct solution] for converting formula confusion error.
[Question]: Linda is painting her bedroom. Her bedroom has 4 walls, with the room being 20 feet wide by 20 feet long by 8 feet tall. One wall has a 3-foot by 7-foot doorway. A second wall has a 6foot by 4-foot window. A third wall has a 5-foot by 7-foot doorway to a walk-in-closet. And the fourth wall is completely solid. What is the total area of wall space that Linda will have to paint?
[Correct solution]: The solid wall is $8 \mathrm{ft} . * 20 \mathrm{ft}$. $=160$ sq. ft.
The doorway is $3 \mathrm{ft} . * 7 \mathrm{ft} .=21$ sq. ft.
The window is $6 \mathrm{ft} . * 4 \mathrm{ft} .=24$ sq. ft.
The closet door is $5 \mathrm{ft} . * 7 \mathrm{ft} .=35$ sq. ft.
The total area of the doors and windows is 21 sq . $f t+24$ sq. ft. +35 sq. ft. $=80$ sq. ft.
The solid wall is 160 sq. ft., so before the areas of the doors and window are taken into account, the total wall area is $4 * 160$ sq. ft. $=640$ sq. ft.
Taking into account the doors and window, the total wall area Linda will have to paint is 640 sq. ft. -</p>
<p>80 sq. ft. $=560$ sq. ft.</p>
<h2>Formula Confusion Error:</h2>
<p>[Transformed solution]: The solid wall is $8 \mathrm{ft} . *$ 20 ft. $=160$ sq. ft.
The doorway is $3 \mathrm{ft} .+7 \mathrm{ft} .+3 \mathrm{ft} .+7 \mathrm{ft} .=20$ sq. ft.
The window is $6 \mathrm{ft} . * 4 \mathrm{ft} .=24$ sq. ft.
The closet door is $5 \mathrm{ft} . * 7 \mathrm{ft} .=35$ sq. ft.
The total area of the doors and windows is 20 sq . $f t+24$ sq. ft. +35 sq. ft. $=79$ sq. ft.
The solid wall is 160 sq. ft., so before the areas of the doors and window are taken into account, the total wall area is $4 * 160$ sq. ft. $=640$ sq. ft.
Taking into account the doors and window, the total wall area Linda will have to paint is 640 sq. ft. 79 sq. ft. $=561$ sq. ft.
[Explanation]: Step 2 confuses the perimeter and area formulas of rectangle and it should calculate the area of the rectangle which is equal to the length multiplied by width, rather than the length plus length plus width plus width, equivalent to the perimeter of the rectangle. And step 5 and 7 referencing the result of step 2 are affected.</p>
<h2>D Human Evaluation</h2>
<p>Assessment Procedure: The format of the dataset we generate is illustrated in Figure 5. Evaluators should first comprehend the question and original solution. Subsequently, they should carefully compare the original solution with the transformed solution to determine if the transformed one contains single-step and single-type error according to specific error type rule. Additionally, evaluators should ascertain whether the generated wrong step represents the first error step.
Assessment Quality Control: We enlist two evaluators to assess 10 cases of each error type in every dataset, totaling 180 cases. A consensus between the two evaluators is required for a case to be deemed satisfactory. In cases of disagreement between two evaluators, a third party is consulted for a final decision. Throughout the evaluation, our generated dataset have achieved an accuracy rate of $92.5 \%$, demonstrating its suitability for evaluating the ability of LLMs to identify and rectify errors.</p>
<h2>E Additional In-depth Experiments</h2>
<h2>E. 1 Influence of Step Count</h2>
<p>We examine the influence of step count on $E P$ and $E C$. For calculation error, we select 50 solutions ranging from 2 to 9 steps to transform. It can be observed that the accuracy of identification and cor-</p>
<h1>Generated Dataset Format</h1>
<p>{
"question": "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?", "original_solution": "Natalia sold 48/2 = 24 clips in May.'nNatalia sold $48+24=72$ clips altogether in April and May.'n#### 72",
"original_answer": 72.0,
"transformed_solution": "Natalia sold 48/2 = 12 clips in May.'nNatalia sold $48+12=60$ clips altogether in April and May. 'n#### 60",
"transformed_answer":60.0,
"wrong_step": 1,
"wrong_type": "calculation_error",
"is_single_error": true,
"explanation": "Here, the 48/2 in step 1 is wrongly calculated as 12 . It should be noted that step 2 needs to use the result of step 1 , so the original $48+24$ will be changed to $48+12$. So, it should be noted that if a step is miscalculated, subsequent steps will inherit its error and use the number of miscalculations to further calculate. Futhermore, step 2 needs to be calculated correctly, and you should only consider one step error."</p>
<p>Figure 5: Dataset format.
rection is not significantly affected by the number of steps in Figure 10 to 13.</p>
<h2>E. 2 Influence of the Wrong Step Order</h2>
<p>We consider the impact of the occurrence order of the error step on $E P$ and $E C$. For the 8 -step problems involving calculation error, we generate 50 cases for each error step from 1 to 8 for evaluation. It can be observed that the accuracy of identification and correction is also not significantly affected by the order of the error step in Figure 14 to 17.</p>
<h2>E. 3 Comparasion between GLM-4 and GPT-4</h2>
<p>To further validate the robustness of our conclusions, we conduct supplementary experiments using GLM-4 for data generation. Due to resource limit, we only use GLM-4 to generate three types of errors - CA, MS, and UC - on GSM8K and MathQA, with 50 instances each, totaling 300 instances. The experimental results are as shown in Table 10. We arrive at conclusions consistent with those drawn from the dataset generated by GPT4, e.g., GPT-4's superior error identification and correction capabilities compared to other models.</p>
<h2>E. 4 Comparasion with other math models</h2>
<p>We conduct the evaluation results of other three math-specialized LLMs: Mistral (Jiang et al., 2023), Llemma (Azerbayev et al., 2023) and</p>
<p>LEMA (An et al., 2023) in their 7B versions. We analyze their performance across different error types and tasks. The experimental results are as shown in Table 11 and 12. It can be observed that LEMA, which is aware of errors, outperforms the other math-specialized LLMs. And the capability of GPT-4 and GLM-4 still far surpasses these open-source models. This indicates that the ability to identify and correct errors of these mathspecialized LLMs is inferior to that of the generalpurpose powerful LLMs, GPT-4 and GLM-4.</p>
<h2>E. 5 Combination Error Analysis</h2>
<p>We conduct some experiments with multi-step and multi-type errors. We first test the combinations of CA and CV. Experimental settings are divided into two: two error types occurring in the same step (Single-step and Two-type Error, ST) and two error types happening in two separate steps (Twostep and Two-type Error, TT). For both settings, we manually annotate 50 data samples and use them to evaluate LLMs' performance in the basic tasks of $E P$ and $E C$. The experimental results are shown in the Table 13 and 14.</p>
<p>Result analysis. By comparing the average accuracy of GPT-4 and GLM-4 with other models, it is evident that GPT-4 and GLM-4 significantly surpass others. As shown in Table 13, for GPT-4 and GLM-4, the $E P$ accuracy of ST and TT is higher than that of CA. This implies that the introduction of CV makes CA more prone to exposure. As illustrated in Table 14, for GPT-4 and GLM-4, the $E C$ accuracy of ST and TT is higher than that of CA. This is because these models exhibit strong correction ability for identified errors.</p>
<h2>F Experiment Details</h2>
<p>This section contains prompts and specific experimental results in the experiment.</p>
<h2>F. 1 Prompts and Input Formatting</h2>
<h2>F.1.1 Dataset Generation</h2>
<p>The prompt for generating the evaluation dataset is shown in Figure 18. After practical experience, we find that 5 -shot has good generation results.</p>
<h2>F.1.2 EP</h2>
<p>We design three zero-shot prompts: Simple, Normal, Misleading for $E P$ on open-source and closedsource models in Figure 19 to 24.</p>
<h2>F.1.3 ES</h2>
<p>We design four prompts: zero-shot, few-shot, zero-shot-type, few-shot-type for $E S$, where few-shot is set to 2-shot. We display zero-shot and zero-shottype prompts on open-source and closed-source models in Figure 25 to 28.</p>
<h2>F.1.4 ET</h2>
<p>We design six prompts: zero-shot, few-shot, zero-shot-random, zero-shot-reverse, few-shot-random, few-shot-reverse for $E T$, where few-shot is set to 2shot. We display zero-shot prompts on open-source and closed-source models in Figure 29 and 30.</p>
<h2>F.1.5 EC</h2>
<p>We design four prompts: zero-shot, few-shot, zero-shot-type, few-shot-type for $E C$ as $E S$, where fewshot is set to 2-shot. We display zero-shot and zero-shot-type prompts on open-source and closedsource models in Figure 31 to 34.</p>
<h2>F. 2 Detailed results</h2>
<p>In this section, we present the original detailed experimental results.</p>
<h2>F.2.1 Main Experiment</h2>
<p>We present the accuracy of each model for each task on each error type in the Table 15 and 16. And we conduct an analysis of the MetaMath series in the Table 17, 18 and 19.</p>
<h2>F.2.2 Error Type Analysis</h2>
<p>We conduct statistical analysis on the classification of error types for each model on GSM8K and MathQA in the Figure 20 to 30.</p>
<h2>F.2.3 Prompt Robustness Analysis</h2>
<p>We design different prompts for four tasks on GSM8K and MathQA to test the robustness of each model. The robustness analysis of $E P$ can be seen in 31 and 32, $E S$ can be seen in 33 and 34, $E T$ can be seen in 35 and 36, and $E C$ can be seen in 37 and 38.</p>
<h2>F.2.4 Comparison with Traditional Task</h2>
<p>We provide the accuracy of the questions in our dataset in traditional task in Table 39 and 40. And we show the performance of each model in traditional task on MathQA in Figure 6.</p>
<h2>F.2.5 Influence of Stopping at Error Step</h2>
<p>We showcase the performance of each model on the GSM8K and MathQA datasets stopping at error step in Figure 7, 8 and 9.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Evaluation results of traditional task for MathQA.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Evaluation results of incomplete cases for GSM8K on open-source models.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Evaluation results of incomplete cases for MathQA on closed-source Models.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Evaluation results of incomplete cases for MathQA on open-source models.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: The influence of step number in GSM8K on $E P$.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 12: The influence of step number in GSM8K on $E C$.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: The influence of step number in MathQA on $E P$.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 13: The influence of step number in MathQA on $E C$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$a c c$</td>
<td style="text-align: center;">$a c c_{1}$</td>
<td style="text-align: center;">$a c c$</td>
<td style="text-align: center;">$a c c_{1}$</td>
<td style="text-align: center;">$a c c$</td>
<td style="text-align: center;">$a c c_{1}$</td>
<td style="text-align: center;">$a c c$</td>
<td style="text-align: center;">$a c c_{1}$</td>
<td style="text-align: center;">$a c c$</td>
<td style="text-align: center;">$a c c_{1}$</td>
<td style="text-align: center;">$a c c$</td>
<td style="text-align: center;">$a c c_{1}$</td>
<td style="text-align: center;">$a c c$</td>
<td style="text-align: center;">$a c c_{1}$</td>
<td style="text-align: center;">$a c c$</td>
<td style="text-align: center;">$a c c_{1}$</td>
<td style="text-align: center;">$a c c$</td>
<td style="text-align: center;">$a c c_{1}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 10: Comparasion between GLM-4 and GPT-4.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">CA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CO</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CV</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">HA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">UC</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">OP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">FC</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$a c c$</td>
<td style="text-align: center;">$a c c_{1}$</td>
<td style="text-align: center;">$a c c$</td>
<td style="text-align: center;">$a c c_{1}$</td>
<td style="text-align: center;">$a c c$</td>
<td style="text-align: center;">$a c c_{1}$</td>
<td style="text-align: center;">$a c c$</td>
<td style="text-align: center;">$a c c_{1}$</td>
<td style="text-align: center;">$a c c$</td>
<td style="text-align: center;">$a c c_{1}$</td>
<td style="text-align: center;">$a c c$</td>
<td style="text-align: center;">$a c c_{1}$</td>
<td style="text-align: center;">$a c c$</td>
<td style="text-align: center;">$a c c_{1}$</td>
<td style="text-align: center;">$a c c$</td>
<td style="text-align: center;">$a c c_{1}$</td>
<td style="text-align: center;">$a c c$</td>
<td style="text-align: center;">$a c c_{1}$</td>
<td style="text-align: center;">$a c c$</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.05</td>
</tr>
<tr>
<td style="text-align: center;">Llemma-7B</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.05</td>
</tr>
<tr>
<td style="text-align: center;">LEMA-7B</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.17</td>
</tr>
</tbody>
</table>
<p>Table 11: Average accuracy of other math models in different error types under zero-shot prompts.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">EP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ES</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ET</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">EC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">EP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ES</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ET</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">EC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$a c c_{1}$</td>
<td style="text-align: center;">$a c c_{2}$</td>
<td style="text-align: center;">$a c c_{1}$</td>
<td style="text-align: center;">$a c c_{3}$</td>
<td style="text-align: center;">$a c c_{1}$</td>
<td style="text-align: center;">$a c c_{4}$</td>
<td style="text-align: center;">$a c c_{1}$</td>
<td style="text-align: center;">$a c c$</td>
<td style="text-align: center;">$a c c_{1}$</td>
<td style="text-align: center;">$a c c_{1}$</td>
<td style="text-align: center;">$a c c_{2}$</td>
<td style="text-align: center;">$a c c_{1}$</td>
<td style="text-align: center;">$a c c_{3}$</td>
<td style="text-align: center;">$a c c_{1}$</td>
<td style="text-align: center;">$a c c_{4}$</td>
<td style="text-align: center;">$a c c_{1}$</td>
<td style="text-align: center;">$a c c$</td>
<td style="text-align: center;">$a c c_{1}$</td>
<td style="text-align: center;">$a c c$</td>
<td style="text-align: center;">$a c c_{1}$</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-7B</td>
<td style="text-align: center;">0.081</td>
<td style="text-align: center;">0.127</td>
<td style="text-align: center;">0.366</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.052</td>
<td style="text-align: center;">0.559</td>
<td style="text-align: center;">0.092</td>
<td style="text-align: center;">0.102</td>
<td style="text-align: center;">0.302</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.797</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.996</td>
<td style="text-align: center;">0.049</td>
<td style="text-align: center;">0.547</td>
<td style="text-align: center;">0.050</td>
<td style="text-align: center;">0.553</td>
</tr>
<tr>
<td style="text-align: left;">Llemma-7B</td>
<td style="text-align: center;">0.021</td>
<td style="text-align: center;">0.169</td>
<td style="text-align: center;">0.506</td>
<td style="text-align: center;">0.014</td>
<td style="text-align: center;">0.208</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.537</td>
<td style="text-align: center;">0.051</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.020</td>
<td style="text-align: center;">0.133</td>
<td style="text-align: center;">0.442</td>
<td style="text-align: center;">0.014</td>
<td style="text-align: center;">0.092</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.449</td>
<td style="text-align: center;">0.042</td>
<td style="text-align: center;">0.251</td>
<td style="text-align: center;">0.047</td>
<td style="text-align: center;">0.284</td>
</tr>
<tr>
<td style="text-align: left;">LEMA-7B</td>
<td style="text-align: center;">0.277</td>
<td style="text-align: center;">0.323</td>
<td style="text-align: center;">0.809</td>
<td style="text-align: center;">0.080</td>
<td style="text-align: center;">0.537</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.503</td>
<td style="text-align: center;">0.170</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.360</td>
<td style="text-align: center;">0.260</td>
<td style="text-align: center;">0.883</td>
<td style="text-align: center;">0.093</td>
<td style="text-align: center;">0.676</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.712</td>
<td style="text-align: center;">0.178</td>
<td style="text-align: center;">0.658</td>
<td style="text-align: center;">0.174</td>
<td style="text-align: center;">0.595</td>
</tr>
</tbody>
</table>
<p>Table 12: Average accuracy of other math models in different tasks under zero-shot prompts.</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 14: The influence of wrong step order in GSM8K on $E P$.
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 16: The influence of wrong step order in GSM8K on $E C$.
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 15: The influence of wrong step order in MathQA on $E P$.
<img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 17: The influence of wrong step order in MathQA on $E C$.</p>
<table>
<thead>
<tr>
<th style="text-align: right;"></th>
<th style="text-align: right;">ST</th>
<th style="text-align: right;">TT</th>
<th style="text-align: right;">CA</th>
<th style="text-align: right;">CV</th>
<th style="text-align: right;">Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">GPT-3.5</td>
<td style="text-align: right;">0.36</td>
<td style="text-align: right;">0.78</td>
<td style="text-align: right;">0.44</td>
<td style="text-align: right;">0.57</td>
<td style="text-align: right;">0.538</td>
</tr>
<tr>
<td style="text-align: right;">GPT-4</td>
<td style="text-align: right;">1.00</td>
<td style="text-align: right;">0.98</td>
<td style="text-align: right;">0.61</td>
<td style="text-align: right;">0.98</td>
<td style="text-align: right;">0.893</td>
</tr>
<tr>
<td style="text-align: right;">GLM-4</td>
<td style="text-align: right;">0.98</td>
<td style="text-align: right;">0.92</td>
<td style="text-align: right;">0.32</td>
<td style="text-align: right;">0.94</td>
<td style="text-align: right;">0.790</td>
</tr>
<tr>
<td style="text-align: right;">Gemini Pro</td>
<td style="text-align: right;">0.08</td>
<td style="text-align: right;">0.06</td>
<td style="text-align: right;">0.06</td>
<td style="text-align: right;">0.17</td>
<td style="text-align: right;">0.093</td>
</tr>
<tr>
<td style="text-align: right;">LLaMA-2-7B</td>
<td style="text-align: right;">0.58</td>
<td style="text-align: right;">0.5</td>
<td style="text-align: right;">0.58</td>
<td style="text-align: right;">0.49</td>
<td style="text-align: right;">0.538</td>
</tr>
<tr>
<td style="text-align: right;">LLaMA-13-7B</td>
<td style="text-align: right;">0.16</td>
<td style="text-align: right;">0.14</td>
<td style="text-align: right;">0.10</td>
<td style="text-align: right;">0.14</td>
<td style="text-align: right;">0.135</td>
</tr>
<tr>
<td style="text-align: right;">Avg</td>
<td style="text-align: right;">0.527</td>
<td style="text-align: right;">0.563</td>
<td style="text-align: right;">0.352</td>
<td style="text-align: right;">0.548</td>
<td style="text-align: right;">0.498</td>
</tr>
</tbody>
</table>
<p>Table 13: Accuracy of $E P$ in combination error types on GSM8K under zero-shot prompts.</p>
<table>
<thead>
<tr>
<th style="text-align: right;"></th>
<th style="text-align: right;">ST</th>
<th style="text-align: right;"></th>
<th style="text-align: right;">TT</th>
<th style="text-align: right;"></th>
<th style="text-align: right;">CA</th>
<th style="text-align: right;"></th>
<th style="text-align: right;">CV</th>
<th style="text-align: right;"></th>
<th style="text-align: right;">Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;"></td>
<td style="text-align: right;">$a c c_{4}$</td>
<td style="text-align: right;">$a c c_{1}$</td>
<td style="text-align: right;">$a c c_{4}$</td>
<td style="text-align: right;">$a c c_{1}$</td>
<td style="text-align: right;">$a c c_{4}$</td>
<td style="text-align: right;">$a c c_{1}$</td>
<td style="text-align: right;">$a c c_{4}$</td>
<td style="text-align: right;">$a c c_{1}$</td>
<td style="text-align: right;">$a c c_{4}$</td>
</tr>
<tr>
<td style="text-align: right;">GPT-3.5</td>
<td style="text-align: right;">0.08</td>
<td style="text-align: right;">0.10</td>
<td style="text-align: right;">0.12</td>
<td style="text-align: right;">0.48</td>
<td style="text-align: right;">0.13</td>
<td style="text-align: right;">0.23</td>
<td style="text-align: right;">0.16</td>
<td style="text-align: right;">0.39</td>
<td style="text-align: right;">0.123</td>
</tr>
<tr>
<td style="text-align: right;">GPT-4</td>
<td style="text-align: right;">0.96</td>
<td style="text-align: right;">1.00</td>
<td style="text-align: right;">0.96</td>
<td style="text-align: right;">0.98</td>
<td style="text-align: right;">0.58</td>
<td style="text-align: right;">0.59</td>
<td style="text-align: right;">0.96</td>
<td style="text-align: right;">0.99</td>
<td style="text-align: right;">0.865</td>
</tr>
<tr>
<td style="text-align: right;">GLM-4</td>
<td style="text-align: right;">0.94</td>
<td style="text-align: right;">0.98</td>
<td style="text-align: right;">0.92</td>
<td style="text-align: right;">0.96</td>
<td style="text-align: right;">0.37</td>
<td style="text-align: right;">0.38</td>
<td style="text-align: right;">0.90</td>
<td style="text-align: right;">0.96</td>
<td style="text-align: right;">0.783</td>
</tr>
<tr>
<td style="text-align: right;">Gemini Pro</td>
<td style="text-align: right;">0.06</td>
<td style="text-align: right;">0.06</td>
<td style="text-align: right;">0.12</td>
<td style="text-align: right;">0.14</td>
<td style="text-align: right;">0.06</td>
<td style="text-align: right;">0.06</td>
<td style="text-align: right;">0.30</td>
<td style="text-align: right;">0.30</td>
<td style="text-align: right;">0.135</td>
</tr>
<tr>
<td style="text-align: right;">LLaMA-2-7B</td>
<td style="text-align: right;">0.06</td>
<td style="text-align: right;">0.92</td>
<td style="text-align: right;">0.08</td>
<td style="text-align: right;">0.92</td>
<td style="text-align: right;">0.15</td>
<td style="text-align: right;">0.91</td>
<td style="text-align: right;">0.08</td>
<td style="text-align: right;">0.90</td>
<td style="text-align: right;">0.093</td>
</tr>
<tr>
<td style="text-align: right;">LLaMA-13-7B</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">0.01</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">0.000</td>
</tr>
<tr>
<td style="text-align: right;">Avg</td>
<td style="text-align: right;">0.350</td>
<td style="text-align: right;">0.510</td>
<td style="text-align: right;">0.367</td>
<td style="text-align: right;">0.580</td>
<td style="text-align: right;">0.215</td>
<td style="text-align: right;">0.363</td>
<td style="text-align: right;">0.400</td>
<td style="text-align: right;">0.590</td>
<td style="text-align: right;">0.333</td>
</tr>
</tbody>
</table>
<p>Table 14: Accuracy of $E C$ in combination error types on GSM8K under zero-shot prompts.</p>
<h1>Prompt for Generating Evaluation Dataset by GPT-4</h1>
<h2>Task Overview:</h2>
<p>Assuming you are a case generator, I will give you some cases, and you need to imitate my case generation process.</p>
<h2>Format Requirement:</h2>
<p>Specifically, each case includes question, original_solution, original_answer, transformed_solution, transformed_answer, wrong_step, wrong_type, is_single_error and explanation.
During the generation process, I will provide you with question and original_solution, and you need to generate a dictionary, whose keys are question, original_solution, original_answer, transformed_solution, transformed_answer, wrong_step, wrong_type, is_single_error and explanation, just like the example cases. In the original_solution, each in represents a step, and the number after #### represents original_answer. To generate the transformed_solution, I need you to make some modifications based on the original_solution. The transformed_answer is the number after #### in the transformed_solution. Wrong_step represents your first modification step. You can be consistent with the cases given to you in wrong_type and is_single_error. And explanation is your illustration of the process of converting the original_solution into the transformed_solution. 5-shot:
Case1 :
{
"question": "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?",
"original_solution": "Natalia sold 48/2 = 24 clips in May.\nNatalia sold 48+24 = 72 clips altogether in April and May.\n#### 72",
"original_answer": 72.0,
"transformed_solution": "Natalia sold 48/2 = 12 clips in May.\nNatalia sold 48+12 =60 clips altogether in April and May. \n#### 60",
"transformed_answer":60.0,
"wrong_step": 1,
"wrong_type": "calculation_error",
"is_single_error": true,
"explanation": "Here, the 48/2 in step 1 is wrongly calculated as 12. It should be noted that step 2 needs to use the result of step 1, so the original $48+24$ will be changed to $48+12$. So, it should be noted that if a step is miscalculated, subsequent steps will inherit its error and use the number of miscalculations to further calculate. Futhermore, step 2 needs to be calculated correctly, and you should only consider one step error."
},
....
Case5 :
...</p>
<h2>Error method design:</h2>
<p>I hope that you randomly change the calculation result of one step in the original_solution without changing any operands or operators of the formula to generate the transformed_solution. And the subsequent related steps in the transformed_solution will be affected, without affecting the previous or unrelated steps. Furthermore, the transformed_answer should be different from the original_answer. Therefore, how to convert the following case? Case to be converted:
question: {original_dataset['question'}}
original_solution: {original_dataset['answer'}}
Remember to only output the dictionary, not other things.</p>
<p>Figure 18: Prompt for generation.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ In this work, we only consider generating the wrong solution with only one error type in a single step to simplify the evaluation process, leaving more complicated error identification and correction to future work.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ Specifically, we conduct experiments using gpt-3.5-turbo-1106, gpt-4-1106-preview, LLaMA-2-7B-chat, LLaMA-2-13B-chat, MetaMath-7B-V1.0, MetaMath-13BV1.0, Mistral-7B-V0.1, Llemma-7B, LEMA-V1-PEFT-LLaMA-2-7B-GSM8K.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
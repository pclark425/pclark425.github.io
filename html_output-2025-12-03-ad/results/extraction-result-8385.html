<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8385 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8385</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8385</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-4578717d5593b88e1c10555ce67a14be312b84b2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4578717d5593b88e1c10555ce67a14be312b84b2" target="_blank">Impact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper Abstract:</strong> ,</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8385.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8385.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-J-6B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-J-6B (EleutherAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 6-billion-parameter autoregressive Transformer pretrained on the Pile; evaluated in this paper on few-shot numerical reasoning (addition, multiplication, operator-inference, and time-unit conversions) to study the influence of pretraining term frequency on arithmetic performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J-6B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>6B-parameter autoregressive Transformer from EleutherAI, pretrained on the Pile dataset and used via HuggingFace; evaluated in few-shot (k=0,2,4,8,16) in-context learning prompts for arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Two-operand addition and multiplication (operands <100 in arithmetic experiments), operation-inference (operation masked with '#'), and several time-unit conversions (minute->second, hour->minute, day->hour, week->day, month->week, year->month, decade->year).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Evidence indicates the model relies heavily on low-order pretraining co-occurrence statistics (unigram, bigram, trigram frequency) and pattern matching/memorization of frequent numeric tuples rather than demonstrating an explicit, robust algorithmic digit-by-digit arithmetic procedure; no internal neuron-level algorithmic mechanism was identified in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Empirical probing via: (1) counting pretraining frequencies ω for unigrams (ω_{x1}), co-occurrences (ω_{x1,x2}), and triplets (ω_{x1,x2,y}) in the Pile; (2) binning terms by frequency and plotting average accuracy vs frequency; (3) few-shot prompting (k=0,2,4,8,16) and averaging over random prompt seeds; (4) filtering out likely-memorized instances (triples with high exact-match accuracy) to test effect beyond memorization. No linear probes, activation patching, or neuron interventions were performed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Summary highlights from paper (examples): Multiplication (k=2): overall accuracy 25.5%, performance gap Δ_{1}=69.2, Δ_{1,2}=83.9, Δ_{1,y}=87.0; Addition (k=2): overall accuracy 80.6%, Δ_{1}=29.2, Δ_{1,2}=32.1, Δ_{1,y}=44.5. Multiplication accuracies stay low (~23–26% across some k) while performance gaps are very large; addition reaches higher accuracy (up to ~86% at k=4) but still shows sizable gaps. Time-unit conversion tasks: accuracies increase with shots (examples: Min->Sec k=16 Acc 58.4% with Δ_{1,2}=87.5), some conversions (decade->year) reach near-perfect accuracy at modest k. Many top-vs-bottom-10% frequency gaps exceed 50–70 percentage points in worst cases.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Main failures: large drop in accuracy on instances that include less frequent numeric terms (poor generalization to rare numbers); inability to reliably infer operations in the 'operation inference' setting (much lower accuracy); partial attribution to memorization of frequent triples but frequency-dependence persists after removing memorized examples; improvements from adding shots frequently concentrate on high-frequency instances (shot amplification of frequency bias).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Strong empirical correlations (figures in paper) between ω (unigram/bigram/trigram counts) and per-term accuracy; large measured performance gaps Δ between top 10% and bottom 10% frequency bins across tasks; filtering likely-memorized triple instances (high ω_{x1,x2,y} with high accuracy) does not eliminate the dependence on lower-order frequencies (ω_{x1}, ω_{x1,x2}), implying effect beyond pure memorization; smaller models show the same frequency-dependence pattern.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Some trivial-format tasks (e.g., decade->year which effectively appends a zero) generalize well and show small gaps, indicating that when the mapping is extremely simple/patterned models can generalize; authors caution they do not make a causal claim and acknowledge possible confounders (e.g., recognition vs reasoning ambiguity, choice of frequency window, threshold for memorization), so frequency-dependence is robust empirically but mechanism interpretation remains limited.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8385.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8385.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-Neo (1.3B / 2.7B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-Neo-1.3B and GPT-Neo-2.7B (EleutherAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Smaller EleutherAI autoregressive Transformer models pretrained on the Pile; evaluated as lower-capacity baselines that also show a dependence of arithmetic success on pretraining term frequency, with lower overall accuracy than GPT-J-6B.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Neo-1.3B and GPT-Neo-2.7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive Transformer models of approximately 1.3B and 2.7B parameters (EleutherAI implementation), pretrained on the Pile and evaluated with the same few-shot prompts as GPT-J-6B in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Addition and multiplication (same experimental instantiation as GPT-J-6B; smaller models tested for a subset of shots, e.g., k=2 and k=8 in plots).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Qualitatively similar to GPT-J-6B: reliance on pretraining frequency statistics and pattern matching rather than demonstrated algorithmic arithmetic; smaller capacity exacerbates inability to generalize to rare terms.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Same frequency-counting and binning methodology and few-shot prompting (examples shown in Figure 6); no additional internal probing or interventions beyond the paper's memorization-filtering analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported qualitatively and in plots: smaller models have substantially lower overall accuracy on arithmetic tasks compared to GPT-J-6B, and their successes are concentrated on high-frequency terms; exact per-task numeric tables are not provided for these models in the paper, but plotted trends show sharp performance drop across lower-frequency bins.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Sharp accuracy degradation for less frequent numeric terms; inability to solve many instances even when those instances are solvable by larger models on frequent terms.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Figure 6 shows that accuracy rises with operand frequency for these smaller models, mirroring GPT-J-6B; the pattern supports a frequency-driven explanation across model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>No detailed numeric breakdown provided in paper; conclusions for these smaller models are qualitatively consistent but less extensively enumerated than for GPT-J-6B.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8385.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8385.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Performance gap Δ</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Performance gap over pretraining term frequency (Δ)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scalar metric introduced/operationalized in this paper that measures the difference in average model accuracy between instances whose associated terms lie in the top 10% vs bottom 10% by pretraining frequency, used to quantify dependence of arithmetic performance on pretraining distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applies to evaluated language models (GPT-J-6B, GPT-Neo-1.3B/2.7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model — analytic metric computed from model outputs and pretraining term frequencies ω over the Pile.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Applied to addition, multiplication, operation-inference, and time-unit conversion datasets in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Does not specify an internal representational mechanism; designed to reveal externally that performance is concentrated on instances with high pretraining frequency (i.e., dependence on low-order co-occurrence statistics rather than robust generalized arithmetic).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Compute ω for different term subsets (ω_{x1}, ω_{x1,x2}, ω_{x1,x2,y}, etc.) from the Pile within a fixed window; create instances for each unique term value; compute model accuracy per-term over sampled instances; compute Δ(Ω) = Acc(top10% by ω) - Acc(bottom10% by ω).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper reports Δ values per task and per frequency-definition; representative examples: Multiplication (k=2) Δ_{1}=69.2, Δ_{1,2}=83.9, Δ_{1,y}=87.0; Addition (k=2) Δ_{1}=29.2, Δ_{1,2}=32.1, Δ_{1,y}=44.5; numerous time-unit conversions show Δ often >50 and sometimes >80 percentage points for higher k.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>A large positive Δ indicates failure to generalize to low-frequency terms; increases in few-shot examples often increase Δ (improvements accrue disproportionately to frequent items).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Consistent, large Δ values across many tasks and shots; visual plots of accuracy vs ω show strong positive slopes; Δ remains large even after removing memorized triple-exact matches.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>A zero Δ would not suffice to prove reasoning; metric measures correlation not causation and can be influenced by numerous confounders (authors emphasize this).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8385.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8385.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memorization-filter intervention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memorized-instance removal (filtering high ω_{x1,x2,y} triples)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An analysis/intervention in the paper that removes instances likely memorized by the model (identified as number triples with very high accuracy and high exact co-occurrence in pretraining) to test whether frequency-dependence persists beyond direct memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied primarily to GPT-J-6B 2-shot analyses</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model; an analysis procedure applied to model outputs and pretraining co-occurrence statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Applied to arithmetic (multiplication/addition) instances.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Designed to disentangle direct memorization of complete instances (triples) from more general frequency-driven effects; addresses whether high performance on frequent instances is merely memorization of exact triples.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Identify triples (x1,x2,y) with high ω_{x1,x2,y} and high model exact-match accuracy (authors take triples with average accuracy >85% as likely memorized), remove those instances from the evaluation set, and recompute accuracy vs ω_{x1} and ω_{x1,x2}.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>After removing likely-memorized triples, the model still shows a high dependence of accuracy on lower-order frequencies (ω_{x1}, ω_{x1,x2}); plots (Figure 7) show the residual correlation and substantial performance gaps remain.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Demonstrates that while memorization (exact triple recall) contributes to performance on frequent items, it does not fully explain the frequency dependence; errors on rare items remain prevalent after filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Figure 7 and related analysis: persistent correlation between unigram/co-occurrence frequencies and accuracy after removing memorized triples; Appendix figure (Figure 9) shows high dependence of model accuracy on ω_{x1,x2,y} for the very highest-frequency triples (consistent with memorization), supporting the validity of the filtering approach.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Selection threshold (>85%) for memorization is heuristic; removing memorized instances is imperfect and does not constitute an intervention during training, so causal attribution remains limited.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Solving quantitative reasoning problems with language models <em>(Rating: 2)</em></li>
                <li>Analysing mathematical reasoning abilities of neural models <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Injecting numerical reasoning skills into language models <em>(Rating: 2)</em></li>
                <li>Quantifying memorization across neural language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8385",
    "paper_id": "paper-4578717d5593b88e1c10555ce67a14be312b84b2",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "GPT-J-6B",
            "name_full": "GPT-J-6B (EleutherAI)",
            "brief_description": "A 6-billion-parameter autoregressive Transformer pretrained on the Pile; evaluated in this paper on few-shot numerical reasoning (addition, multiplication, operator-inference, and time-unit conversions) to study the influence of pretraining term frequency on arithmetic performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-J-6B",
            "model_description": "6B-parameter autoregressive Transformer from EleutherAI, pretrained on the Pile dataset and used via HuggingFace; evaluated in few-shot (k=0,2,4,8,16) in-context learning prompts for arithmetic tasks.",
            "arithmetic_task_type": "Two-operand addition and multiplication (operands &lt;100 in arithmetic experiments), operation-inference (operation masked with '#'), and several time-unit conversions (minute-&gt;second, hour-&gt;minute, day-&gt;hour, week-&gt;day, month-&gt;week, year-&gt;month, decade-&gt;year).",
            "mechanism_or_representation": "Evidence indicates the model relies heavily on low-order pretraining co-occurrence statistics (unigram, bigram, trigram frequency) and pattern matching/memorization of frequent numeric tuples rather than demonstrating an explicit, robust algorithmic digit-by-digit arithmetic procedure; no internal neuron-level algorithmic mechanism was identified in this study.",
            "probing_or_intervention_method": "Empirical probing via: (1) counting pretraining frequencies ω for unigrams (ω_{x1}), co-occurrences (ω_{x1,x2}), and triplets (ω_{x1,x2,y}) in the Pile; (2) binning terms by frequency and plotting average accuracy vs frequency; (3) few-shot prompting (k=0,2,4,8,16) and averaging over random prompt seeds; (4) filtering out likely-memorized instances (triples with high exact-match accuracy) to test effect beyond memorization. No linear probes, activation patching, or neuron interventions were performed.",
            "performance_metrics": "Summary highlights from paper (examples): Multiplication (k=2): overall accuracy 25.5%, performance gap Δ_{1}=69.2, Δ_{1,2}=83.9, Δ_{1,y}=87.0; Addition (k=2): overall accuracy 80.6%, Δ_{1}=29.2, Δ_{1,2}=32.1, Δ_{1,y}=44.5. Multiplication accuracies stay low (~23–26% across some k) while performance gaps are very large; addition reaches higher accuracy (up to ~86% at k=4) but still shows sizable gaps. Time-unit conversion tasks: accuracies increase with shots (examples: Min-&gt;Sec k=16 Acc 58.4% with Δ_{1,2}=87.5), some conversions (decade-&gt;year) reach near-perfect accuracy at modest k. Many top-vs-bottom-10% frequency gaps exceed 50–70 percentage points in worst cases.",
            "error_types_or_failure_modes": "Main failures: large drop in accuracy on instances that include less frequent numeric terms (poor generalization to rare numbers); inability to reliably infer operations in the 'operation inference' setting (much lower accuracy); partial attribution to memorization of frequent triples but frequency-dependence persists after removing memorized examples; improvements from adding shots frequently concentrate on high-frequency instances (shot amplification of frequency bias).",
            "evidence_for_mechanism": "Strong empirical correlations (figures in paper) between ω (unigram/bigram/trigram counts) and per-term accuracy; large measured performance gaps Δ between top 10% and bottom 10% frequency bins across tasks; filtering likely-memorized triple instances (high ω_{x1,x2,y} with high accuracy) does not eliminate the dependence on lower-order frequencies (ω_{x1}, ω_{x1,x2}), implying effect beyond pure memorization; smaller models show the same frequency-dependence pattern.",
            "counterexamples_or_challenges": "Some trivial-format tasks (e.g., decade-&gt;year which effectively appends a zero) generalize well and show small gaps, indicating that when the mapping is extremely simple/patterned models can generalize; authors caution they do not make a causal claim and acknowledge possible confounders (e.g., recognition vs reasoning ambiguity, choice of frequency window, threshold for memorization), so frequency-dependence is robust empirically but mechanism interpretation remains limited.",
            "uuid": "e8385.0"
        },
        {
            "name_short": "GPT-Neo (1.3B / 2.7B)",
            "name_full": "GPT-Neo-1.3B and GPT-Neo-2.7B (EleutherAI)",
            "brief_description": "Smaller EleutherAI autoregressive Transformer models pretrained on the Pile; evaluated as lower-capacity baselines that also show a dependence of arithmetic success on pretraining term frequency, with lower overall accuracy than GPT-J-6B.",
            "citation_title": "GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow.",
            "mention_or_use": "use",
            "model_name": "GPT-Neo-1.3B and GPT-Neo-2.7B",
            "model_description": "Autoregressive Transformer models of approximately 1.3B and 2.7B parameters (EleutherAI implementation), pretrained on the Pile and evaluated with the same few-shot prompts as GPT-J-6B in this study.",
            "arithmetic_task_type": "Addition and multiplication (same experimental instantiation as GPT-J-6B; smaller models tested for a subset of shots, e.g., k=2 and k=8 in plots).",
            "mechanism_or_representation": "Qualitatively similar to GPT-J-6B: reliance on pretraining frequency statistics and pattern matching rather than demonstrated algorithmic arithmetic; smaller capacity exacerbates inability to generalize to rare terms.",
            "probing_or_intervention_method": "Same frequency-counting and binning methodology and few-shot prompting (examples shown in Figure 6); no additional internal probing or interventions beyond the paper's memorization-filtering analysis.",
            "performance_metrics": "Reported qualitatively and in plots: smaller models have substantially lower overall accuracy on arithmetic tasks compared to GPT-J-6B, and their successes are concentrated on high-frequency terms; exact per-task numeric tables are not provided for these models in the paper, but plotted trends show sharp performance drop across lower-frequency bins.",
            "error_types_or_failure_modes": "Sharp accuracy degradation for less frequent numeric terms; inability to solve many instances even when those instances are solvable by larger models on frequent terms.",
            "evidence_for_mechanism": "Figure 6 shows that accuracy rises with operand frequency for these smaller models, mirroring GPT-J-6B; the pattern supports a frequency-driven explanation across model sizes.",
            "counterexamples_or_challenges": "No detailed numeric breakdown provided in paper; conclusions for these smaller models are qualitatively consistent but less extensively enumerated than for GPT-J-6B.",
            "uuid": "e8385.1"
        },
        {
            "name_short": "Performance gap Δ",
            "name_full": "Performance gap over pretraining term frequency (Δ)",
            "brief_description": "A scalar metric introduced/operationalized in this paper that measures the difference in average model accuracy between instances whose associated terms lie in the top 10% vs bottom 10% by pretraining frequency, used to quantify dependence of arithmetic performance on pretraining distribution.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applies to evaluated language models (GPT-J-6B, GPT-Neo-1.3B/2.7B)",
            "model_description": "Not a model — analytic metric computed from model outputs and pretraining term frequencies ω over the Pile.",
            "arithmetic_task_type": "Applied to addition, multiplication, operation-inference, and time-unit conversion datasets in the paper.",
            "mechanism_or_representation": "Does not specify an internal representational mechanism; designed to reveal externally that performance is concentrated on instances with high pretraining frequency (i.e., dependence on low-order co-occurrence statistics rather than robust generalized arithmetic).",
            "probing_or_intervention_method": "Compute ω for different term subsets (ω_{x1}, ω_{x1,x2}, ω_{x1,x2,y}, etc.) from the Pile within a fixed window; create instances for each unique term value; compute model accuracy per-term over sampled instances; compute Δ(Ω) = Acc(top10% by ω) - Acc(bottom10% by ω).",
            "performance_metrics": "Paper reports Δ values per task and per frequency-definition; representative examples: Multiplication (k=2) Δ_{1}=69.2, Δ_{1,2}=83.9, Δ_{1,y}=87.0; Addition (k=2) Δ_{1}=29.2, Δ_{1,2}=32.1, Δ_{1,y}=44.5; numerous time-unit conversions show Δ often &gt;50 and sometimes &gt;80 percentage points for higher k.",
            "error_types_or_failure_modes": "A large positive Δ indicates failure to generalize to low-frequency terms; increases in few-shot examples often increase Δ (improvements accrue disproportionately to frequent items).",
            "evidence_for_mechanism": "Consistent, large Δ values across many tasks and shots; visual plots of accuracy vs ω show strong positive slopes; Δ remains large even after removing memorized triple-exact matches.",
            "counterexamples_or_challenges": "A zero Δ would not suffice to prove reasoning; metric measures correlation not causation and can be influenced by numerous confounders (authors emphasize this).",
            "uuid": "e8385.2"
        },
        {
            "name_short": "Memorization-filter intervention",
            "name_full": "Memorized-instance removal (filtering high ω_{x1,x2,y} triples)",
            "brief_description": "An analysis/intervention in the paper that removes instances likely memorized by the model (identified as number triples with very high accuracy and high exact co-occurrence in pretraining) to test whether frequency-dependence persists beyond direct memorization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applied primarily to GPT-J-6B 2-shot analyses",
            "model_description": "Not a model; an analysis procedure applied to model outputs and pretraining co-occurrence statistics.",
            "arithmetic_task_type": "Applied to arithmetic (multiplication/addition) instances.",
            "mechanism_or_representation": "Designed to disentangle direct memorization of complete instances (triples) from more general frequency-driven effects; addresses whether high performance on frequent instances is merely memorization of exact triples.",
            "probing_or_intervention_method": "Identify triples (x1,x2,y) with high ω_{x1,x2,y} and high model exact-match accuracy (authors take triples with average accuracy &gt;85% as likely memorized), remove those instances from the evaluation set, and recompute accuracy vs ω_{x1} and ω_{x1,x2}.",
            "performance_metrics": "After removing likely-memorized triples, the model still shows a high dependence of accuracy on lower-order frequencies (ω_{x1}, ω_{x1,x2}); plots (Figure 7) show the residual correlation and substantial performance gaps remain.",
            "error_types_or_failure_modes": "Demonstrates that while memorization (exact triple recall) contributes to performance on frequent items, it does not fully explain the frequency dependence; errors on rare items remain prevalent after filtering.",
            "evidence_for_mechanism": "Figure 7 and related analysis: persistent correlation between unigram/co-occurrence frequencies and accuracy after removing memorized triples; Appendix figure (Figure 9) shows high dependence of model accuracy on ω_{x1,x2,y} for the very highest-frequency triples (consistent with memorization), supporting the validity of the filtering approach.",
            "counterexamples_or_challenges": "Selection threshold (&gt;85%) for memorization is heuristic; removing memorized instances is imperfect and does not constitute an intervention during training, so causal attribution remains limited.",
            "uuid": "e8385.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Solving quantitative reasoning problems with language models",
            "rating": 2
        },
        {
            "paper_title": "Analysing mathematical reasoning abilities of neural models",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 1
        },
        {
            "paper_title": "Injecting numerical reasoning skills into language models",
            "rating": 2
        },
        {
            "paper_title": "Quantifying memorization across neural language models",
            "rating": 2
        }
    ],
    "cost": 0.016225749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Impact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning</h1>
<p>Yasaman Razeghi ${ }^{\text {® }}$ Robert L. Logan IV ${ }^{\text {® }}$ Matt Gardner ${ }^{\text {® }}$ Sameer Singh ${ }^{\text {® }}{ }^{\text {® }}$<br>${ }^{\text {® }}$ University of California, Irvine ${ }^{\text {® }}$ Dataminr Inc.<br>${ }^{\text {® }}$ Microsoft Semantic Machines ${ }^{\text {® }}$ Allen Institute for AI<br>{yrazeghi, sameer}@uci.edu<br>rlogan@dataminr.com mattgardner@microsoft.com</p>
<h4>Abstract</h4>
<p>Pretrained Language Models (LMs) have demonstrated ability to perform numerical reasoning by extrapolating from a few examples in few-shot settings. However, the extent to which this extrapolation relies on robust reasoning is unclear. In this paper, we investigate how well these models reason with terms that are less frequent in the pretraining data. In particular, we examine the correlations between the model performance on test instances and the frequency of terms from those instances in the pretraining data. We measure the strength of this correlation for a multiple GPT-based language models (pretrained on the Pile dataset) on various numerical deduction tasks (e.g., arithmetic and unit conversion). Our results consistently demonstrate that models are more accurate on instances whose terms are more prevalent, in some cases above $70 \%$ (absolute) more accurate on the top $10 \%$ frequent terms in comparison to the bottom $10 \%$. Overall, although LMs appear successful at few-shot numerical reasoning, our results raise the question of how much models actually generalize beyond pretraining data, and we encourage researchers to take the pretraining data into account when interpreting evaluation results.</p>
<h2>1 Introduction</h2>
<p>Large language models have demonstrated outstanding zero- and few-shot performance on various reasoning benchmarks (Brown et al., 2020; Radford et al., 2019). In particular, their high performance on numerical tasks, such as addition and multiplication, suggests that models may have learned the ability to perform the underlying reasoning operations simply through a combination of pretraining and model size (Lewkowycz et al., 2022). As these numerical reasoning tasks become increasingly prevalent for evaluating the performance of</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Multiplication Performance: Plot of GPT-J6B's 2-shot accuracy on multiplication (averaged over multiple multiplicands and training instances) against the frequency of the equation's term in the pretraining corpus. Each point represents the average performance for that term (e.g., 24) multiplied by numbers $0-99$ and 5 choices of random seeds. As in the example, the performance difference for the numbers 24 and 23 is more than $20 \%$. We find a strong correlation between the accuracy for a number and its frequency in pretraining.
large language models (Chowdhery et al., 2022), it is crucial to understand the extent to which performance on these tasks reflects robust reasoning capabilities, especially since numerical reasoning is an essential skill needed to perform other complex reasoning tasks such as question answering through reading comprehension (Dua et al., 2019), and commonsense reasoning (Thawani et al., 2021; Lin et al., 2020a).</p>
<p>Current schemes for evaluating the reasoning of large language models, however, often neglect or underestimate the impact of data leakage from pretraining data. Although overlap between the training and evaluation splits of public datasets and its effect on the generalization of language models has been studied (Elangovan et al., 2021; Lewis et al., 2021a), the effect of the pretraining data has received less attention, and very few studies have at-</p>
<p>tempted to evaluate the effect of pretraining data on model's performance (Elazar et al., 2022). Ideally, a model that has learned to reason in the training phase should be able to generalize outside of the narrow context that it was trained in. Specifically, if the model has learned to reason numerically, its performance on instances with less frequent numbers (based on pretraining data) should not be significantly lower than its performance on the instances with common numbers.</p>
<p>For illustration, consider the arithmetic task of multiplying two integers (shown in Figure 1). A model that has learned proper arithmetic skills should be able to answer the queries irrespective of the frequencies of the operands in the pretraining data. Therefore, it should have roughly equivalent performance when answering the queries $Q$ : what is 24 times $X$ ? and $Q$ : what is 23 times $X$ ? despite the fact that 24 appears more frequently in the pretraining data. This is not the case with current LMs and we will study the effect of frequency terms in details through this paper. To show the effect of frequency, in this example, we plot the average accuracy of GPT-J-6B (Wang, 2021) on the numbers $0-99$ (averaged over $0-99$ as the other operand) against the frequency of the number in the pretraining data in Figure 1. We find a strong correlation between the term frequency and the model performance indicating that the model reasoning is not robust to these frequencies. Note that even "rare" terms still appear on the order of millions of times in the pretraining data.</p>
<p>In this work, we investigate this impact of the frequency of test instance terms in a model's pretraining data on the model's performance. We experiment on numerical reasoning tasks of addition, multiplication, and unit conversion. We count occurrences of the numbers and units in instances of these tasks in the pretraining data, including cooccurrences of term pairs or triples within a fixed window. This procedure allows us to aggregate over instances in which these terms appear and observe the relationship between term frequency and model accuracy on instances that include those terms. We summarize this behavior through the performance gap between instances that have the most frequent terms and instances that have the least frequent terms. Intuitively, models that exhibit a high performance gap are much more accurate on instances that are more common in the pretraining data, suggesting that the model does not generalize
appropriately and is affected by dataset overlap.
We present analysis on these numerical reasoning tasks for three sizes of the EleutherAI/GPT models pretrained on the Pile dataset (Gao et al., 2020), which has been publicly released and thus permits this kind of analysis (in contrast to the data that, e.g., GPT-3 (Brown et al., 2020) was trained on). Our results consistently show a large performance gap between highest- and lowest-frequency terms; in some cases there is a more than $70 \%$ average accuracy gap between the top and bottom $10 \%$ terms. We also investigate whether this performance gap can be explained by strong memorization effects, i.e. by instances that are memorized by the language model. To achieve this, we remove instances that contain frequent combinations of numbers from our analysis, and study the performance on the remaining instances. Even in this case, we still find a strong correlation between frequency of terms and average performance, indicating that our results cannot be explained solely by direct memorization.</p>
<p>These observations suggest that any evaluation of reasoning that does not take the pretraining data into account is difficult to interpret, and that we need to revisit evaluation of language models with respect to their pretraining data before making any conclusion about the models generalization abilities beyond the pretraining data.</p>
<h2>2 Background and Methodology</h2>
<p>Numerical reasoning has been essential part of complex multi-step reasoning tasks for natural language understanding (Dua et al., 2019; Wei et al., 2022). Recently, large language models have exhibited an ability to perform numerical reasoning tasks in few-shot settings without requiring any modifications to their parameters through a method called in-context learning (Brown et al., 2020; Chowdhery et al., 2022). Our goal is to evaluate this reasoning skill in-depth and with respect to the pretraining data. This section provides background information on in-context learning and introduces our method for measuring the performance gap of the models on numerical reasoning tasks based on differences in pretraining term frequency.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>2.1 In-context Learning</h3>
<p>Brown et al. (2020) show that the large GPT-3 model is able to perform well on few-shot reasoning tasks without requiring any changes to its internal parameters, through the usage of a technique called in-context learning. In place of a typical learning procedure, in-context learning instead places training examples in a prompt format, which is subsequently fed to a language model as its input. Recently, a few studies have researched the role of prompt and investigated the aspects that make incontext learning successful (Min et al., 2022; Zhao et al., 2021; Chan et al., 2022).</p>
<p>Among numerous experiments, Brown et al. (2020) show that GPT3 performs well on a variety of arithmetic questions such as addition and subtraction with 2-5 digit numbers. For example, they show that the largest model can perform zero-shot 2-digit addition with $76.9 \%$ accuracy. Although impressive, due to the large volume of data GPT-3 is trained on, it is possible that the model is repeating answers seen during pretraining. To attribute this performance to the model's reasoning capabilities, we need to make sure that the model is not affected by statistical overlaps between the terms of the arithmetic questions and the pretraining data.</p>
<p>In the following sections, we introduce metrics that we use to investigate the relationship between the frequency of terms in the pretraining data and the model performance on reasoning instances containing those terms. To assess this relation, we first define an approach for measuring term frequencies in a large pretraining dataset (Section 2.2). We connect these frequencies to reasoning performance by introducing the performance gap $\Delta$ (Section 2.3).</p>
<h3>2.2 Frequency</h3>
<p>We consider numerical reasoning tasks (Table 1) whose instances consist of input terms, $\mathbf{x}=$ $\left(x_{1}, \ldots, x_{i}, \ldots x_{n}\right)$, and a derived output term $y$, where the $x_{i}$ 's are either positive integers or units of time (e.g., 1,2 , hour, etc.) and $y$ is a positive integer. For example, for the task of multiplication, an instance might be $\mathbf{x}=(23,18)$ and $y=414$, representing the equation $23 \times 18=414$.</p>
<p>For each instance, we extract counts of the number of times that a subset of its terms $X \subseteq$ $\left{x_{1}, \ldots, x_{n}, y\right}$ appear within a specified window in the pretraining data. We refer to this count as the frequency, $\omega_{X}$, of $X$.</p>
<p>In this paper, we restrict our attention to fre-
quencies involving three or less input terms, e.g., $\mathbf{x}=\left(x_{1}\right)$ or $\left(x_{1}, x_{2}\right)$ or $\left(x_{1}, x_{2}, x_{3}\right)$ and optionally the output term $y$, e.g.:</p>
<ul>
<li>$\omega_{\left{x_{1}\right}}$ : the number of times that $x_{1}$ (one of the terms, e.g., 23) appears in the pretraining data.</li>
<li>$\omega_{\left{x_{1}, x_{2}\right}}$ : the number of times that the input terms $x_{1}$ (e.g., 23) and $x_{2}$ (e.g., 18) appear in the pretraining data within a specific window size.</li>
<li>$\omega_{\left{x_{1}, y\right}}$ : the number of times that the first input term $x_{1}$ (e.g., 23) and the output term $y$ (e.g., 414) appear in the pretraining data within a specific window size.
Note that our usage of set notation in the subscript is deliberate; although $\mathbf{x}=\left(x_{1}, x_{2}\right)$ and $\mathbf{x}^{\prime}=\left(x_{2}, x_{1}\right)$ are not necessarily the same (e.g., order is important when representing the task instance), frequency is symmetric (e.g., $\omega_{\left{x_{1}, x_{2}\right}}=$ $\left.\omega_{\left{x_{2}, x_{1}\right}} \forall x_{1}, x_{2}\right)$.</li>
</ul>
<h3>2.3 Performance Gap</h3>
<p>We want to measure how much more accurate the model is on instances containing more versus less frequent terms in the pretraining data. We do this by calculating the differences in average accuracies of the instances in the top and bottom quantiles of the distribution over term frequencies, which we call the performance gap.</p>
<p>Formally, let $\left{\left(X^{(n)}, \omega_{X}^{(n)}\right)\right}, n \in[1, N]$, be a set of terms for a task and their associated term frequencies in the pretraining corpus. Given a task (e.g. addition), we create reasoning instances for each element of this set by instantiating values of $x_{i}$, and deriving $y$. We then measure the LM's accuracy $a^{(n)}$ over the set of instances, and repeat this process for all $n \in[1, N]$, producing a set $\Omega=$ $\left{\left(\omega_{X}^{(n)}, a^{(n)}\right)\right}$. The formula for the performance gap is then given by:</p>
<p>$$
\Delta(\Omega)=\operatorname{Acc}\left(\Omega_{&gt;90 \%}\right)-\operatorname{Acc}\left(\Omega_{&lt;10 \%}\right)
$$</p>
<p>where $\Omega_{&gt;90 \%}$ is the top $10 \%$ of elements in $\Omega$ ordered by frequency, $\Omega_{&lt;10 \%}$ is the bottom $10 \%$, and $\operatorname{Acc}\left(\Omega^{\prime}\right)$ is the average accuracy of elements in $\Omega^{\prime}$. We introduce the following convenient abuses of notation $\Delta_{1}, \Delta_{1,2}, \Delta_{1, y}, \ldots$, to denote the performance gap over the frequency distributions of $\omega_{\left{x_{1}\right}}, \omega_{\left{x_{1}, x_{2}\right}}, \omega_{\left{x_{1}, y\right}}, \ldots$, respectively.</p>
<p>Concretely, for the multiplication example from Figure 1, $\mathbf{x}=\left(x_{1}, x_{2}\right)$ and we consider the performance gap over frequencies $\omega_{\left{x_{1}\right}}$. For each number (say 23), we count the number of times it appears in the pretraining corpus $\left(\omega_{{23}}\right)$, and</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Pipeline for Data Construction: We use the term counts processed from the pretraining data to develop the reasoning queries and render them with prompts templates to a proper language model input format.</p>
<p>Table 1: Prompt templates and the number of test cases (#) investigated for each numerical reasoning task.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">Prompt Template</th>
<th style="text-align: center;">#</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Arithmetic</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Multiplication</td>
<td style="text-align: left;">$Q:$ What is $x_{1}$ times $x_{2}$ ? A: $y$</td>
<td style="text-align: center;">$10^{4}$</td>
</tr>
<tr>
<td style="text-align: left;">Addition</td>
<td style="text-align: left;">$Q:$ What is $x_{1}$ plus $x_{2}$ ? A: $y$</td>
<td style="text-align: center;">$10^{4}$</td>
</tr>
<tr>
<td style="text-align: left;">Operation Inference</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Mult. #</td>
<td style="text-align: left;">$Q:$ What is $x_{1}$ # $x_{2}$ ? A: $y$</td>
<td style="text-align: center;">$10^{4}$</td>
</tr>
<tr>
<td style="text-align: left;">Add. #</td>
<td style="text-align: left;">$Q:$ What is $x_{1}$ # $x_{2}$ ? A: $y$</td>
<td style="text-align: center;">$10^{4}$</td>
</tr>
<tr>
<td style="text-align: left;">Time Unit Inference</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Min $\rightarrow$ Sec</td>
<td style="text-align: left;">$Q:$ What is $x_{1}$ minutes in seconds? A: $y$</td>
<td style="text-align: center;">79</td>
</tr>
<tr>
<td style="text-align: left;">Hour $\rightarrow$ Min</td>
<td style="text-align: left;">$Q:$ What is $x_{1}$ hours in minutes? A: $y$</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">Day $\rightarrow$ Hour</td>
<td style="text-align: left;">$Q:$ What is $x_{1}$ days in hours? A: $y$</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">Week $\rightarrow$ Day</td>
<td style="text-align: left;">$Q:$ What is $x_{1}$ weeks in days? A: $y$</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">Month $\rightarrow$ Week</td>
<td style="text-align: left;">$Q:$ What is $x_{1}$ months in weeks? A: $y$</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">Year $\rightarrow$ Month</td>
<td style="text-align: left;">$Q:$ What is $x_{1}$ years in months? A: $y$</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">Decade $\rightarrow$ Year</td>
<td style="text-align: left;">$Q:$ What is $x_{1}$ decades in years? A: $y$</td>
<td style="text-align: center;">100</td>
</tr>
</tbody>
</table>
<p>compute the average accuracy of the model over all instances where one of the operands is 23 . The performance gap w.r.t. to $\omega_{\left{x_{1}\right}}$ for this task is the difference between the average accuracy over the top $10 \%$ and the bottom $10 \%$ most frequent numbers in the pretraining corpus. We picked $10 \%$ as the threshold to have a simple, intuitive metric that captures how accuracy differs between the most and least frequent terms. We also provide the plots to show the full distribution in the frequency range.</p>
<h2>3 Experiment Setup</h2>
<p>In this section, we describe our setup to measure the effect of pretraining data on the few-shot evaluation of a number of numerical reasoning tasks for different language models.</p>
<p>Language Models We experiment on the following models from EleutherAI: GPT-J-6B (Wang, 2021), and GPT-Neo-1.3B, GPT-Neo-2.7B (Black et al., 2021). These models are publicly available, but more importantly, they are among the few models that their pretraining corpus has also been released. These language models are trained on the Pile dataset (Gao et al., 2020), a large-scale lan-
guage modeling dataset consisting of English documents in 22 academic or other professional data sources. We count the frequency of all integers with less than seven digits using a slightly modified version of Spacy English tokenizer (Honnibal and Montani, 2017). To calculate the frequencies of the numbers we use Amazon Elastic Map Reduce (EMR) platform. We use the HuggingFace ${ }^{1}$ Transformer integration of the models for experiments.</p>
<p>Numerical Reasoning Tasks We create three types of datasets that target the mathematical capabilities of language models since solving mathematical questions is a useful reasoning capability of the models (Brown et al., 2020).</p>
<ul>
<li>Arithmetic, 2 tasks As the first task, we consider simple arithmetic operations: addition $x_{1}+x_{2} \rightarrow$ $y$ and multiplication $x_{1} \times x_{2} \rightarrow y$. In both cases, the both operands ( $x_{1}$ and $x_{2}$ ) are numbers less than 100 (these numbers are in the top 200 most frequent numbers in the pretraining data).</li>
<li>Operation Inference, 2 tasks Instead of directly specifying the operation, we also create a variation where the model needs to infer, from a few examples, the operation itself, as well as the result, as introduced in the evaluation of MegatronTuring model. ${ }^{2}$ We replace the arithmetic operation with a "#", with the same operations and operands as previous, to create these datasets.</li>
<li>Time Unit Conversion, 7 tasks Apart from direct arithmetic expressions, we are also interested in evaluating model capability to implicitly reason about these operations. To this end, we construct a unit conversion dataset by identifying the most frequent numbers that co-occur with time unit words ("second", "minute", "hour", "day", "week", "month", "year", and "decade") as the primary operand $x_{1}$, the time units themselves as additional operands ( $x_{2} \rightarrow x_{3}$ ), i.e. converting 24</li>
</ul>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The GPT-J-6B accuracy on arithmetic and operator inference, with $k$ shots. The average accuracy of the binned instances is highly correlated with their term frequencies $\omega_{\left{x_{1}\right}}$ in the pretraining corpus ( $x$-axis).
hours to minutes is represented as (24, "hours", 60). We expect converting time values to be mathematically more straightforward than two-digit multiplication since the model need only multiply with the same (implicit) second operand, e.g., $\times 60$ for converting hours to minutes.
The pipeline for creating instances in our evaluation is illustrated in Figure 2. We compute occurrences and co-occurrences (assuming a window of 5) of the terms in the corpus, i.e. the time units and numbers. We generate instances for the reasoning tasks using the most frequent terms with less than 3 digits (the top 200) as operands. We focus on the top terms since we expect the models to have a fairly reliable and robust representations for these words. Each reasoning instance is rendered as a natural language query using the prompt templates from Table 1, and input to the language model to generate the answer. For example, to create a multiplication instance given the terms $\left(x_{1}=23, x_{2}=18\right)$, we use the instance template to create a natural language input for the model as " $Q$ : What is 23 times 18? A: __", with the goal of producing "414" $(y=23 \times 18=414)$. For few-shot evaluation, we prompt the language models with $k=0,2,4,8,16$ shots, and average performance over five random selection of the prompt instances.</p>
<h2>4 Results</h2>
<p>With the three types of numerical reasoning tasks (consisting of 11 total datasets), we present an evaluation of the effect of pretraining term frequency on the performance of the language models. For each dataset, we measure the performance gap on instances that consist of rarer (relatively) terms, for a few different choices of what to compute frequency
over (different combinations of the instance terms). We also investigate the effect of the model size on this performance gap and do a case study to further clarify if all this impact is due to memorization.</p>
<p>Arithmetic We first study the performance on simple addition and multiplication of numbers. The results for the GPT-J-6B model is provided in Table 2, with performance gap computed just for $x_{1}$ (any of the multiplicands), for $\left(x_{1}, x_{2}\right)$ (both multiplicands), and for $\left(x_{1}, y\right)$ (any of the multiplicands and the golden answer). In multiplication, we observe a very high performance gap for all these definitions of frequencies, suggesting a strong effect of frequency in the pretraining data on the model's ability to perform multiplication. For better illustration of the performance gap, we plot the mean accuracy across the frequency of $x_{1}$ in Figure 3b. The plot demonstrates the strong correlation between the models accuracy on specific instances, and the instance element frequency in the pretraining data. For addition, we observe an overall higher performance of the GPT-J-6B model in comparison to the multiplication experiments. However, the performance gap on all of the definitions of the instance frequencies still shows an strong effect on the models accuracy. As shown in Figure 3a, the average accuracy of the model still has a positive slope, indicating the effect of instance frequencies.</p>
<p>Operation Inference These tasks aim to assess the model capability to both infer the math operation and to perform the actual computation. As we see in Table 2, the model is much less accurate here as compared to the arithmetic experiments. However, the model has better performance on the frequent instances even for these low performance tasks (see detailed trend in Figures 3d and 3c). The</p>
<p>Table 2: GPT-J-6B results on arithmetic, operation inference (#) tasks $\Delta_{1}, \Delta_{1,2}$ and $\Delta_{1, g}$ represent the performance gap over the frequency distributions of $\omega_{\left{x_{1}\right}}, \omega_{\left{x_{1}, x_{2}\right}}$ and $\omega_{\left{x_{1}, y\right}}$ respectively. $x_{1}$ represent the first operand, $x_{2}$ second operand and $y$ the answer of the arithmetic question.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$k$</th>
<th style="text-align: center;">Multiplication</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Addition</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Multiplication (#)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Addition (#)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">$\Delta_{1}$</td>
<td style="text-align: center;">$\Delta_{1,2}$</td>
<td style="text-align: center;">$\Delta_{1, g}$</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">$\Delta_{1}$</td>
<td style="text-align: center;">$\Delta_{1,2}$</td>
<td style="text-align: center;">$\Delta_{1, g}$</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">$\Delta_{1}$</td>
<td style="text-align: center;">$\Delta_{1,2}$</td>
<td style="text-align: center;">$\Delta_{1, g}$</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">$\Delta_{1}$</td>
<td style="text-align: center;">$\Delta_{1,2}$</td>
<td style="text-align: center;">$\Delta_{1, g}$</td>
</tr>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">8.2</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">5.2</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">17.8</td>
<td style="text-align: center;">17.1</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">86.2</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">12.9</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">49.4</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">26.6</td>
<td style="text-align: center;">25.8</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">17.6</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">20.3</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">26.8</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">5.2</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">23.8</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: GPT-J-6B performance on Year $\rightarrow$ Month: Interpolation lines show the correlation between the average accuracy and the $\omega_{\left{x_{1}, x_{2}\right}}$ ( $k$ is number of shots).
performance gap here suggests that the effect of pretraining is not only for tasks that the model is accurate on, but even for operation inference that is more challenging and require deeper reasoning. Moreover, the lower accuracy here as compared to addition experiments in the previous section suggests that the model is unable to infer the operation from the few-shot prompts, and it may be performing some form of pattern matching based on the pretraining data on the common instances.</p>
<p>Time-Unit Conversion The performance gap evaluated on the time unit conversion experiments is in Table 3. We first observe a relatively high performance gap on all the tasks except the conversion from decade to year. We also observe a general pattern of increase in the performance gap as the number of shots (training examples in the prompt) increases. These results suggest that even though the model gets more accurate, the improvements focus on more frequent instances of the task. (Example figures for time units experiments is provided in Figures 4, 5)
Decades to years: As we observe in Table 3, the model performs nearly perfectly on this task with as few as 8 shots, and we only see very small per-
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: GPT-J-6B performance on Decade $\rightarrow$ Year: The interpolation average accuracy line over the $\omega_{\left{x_{1}, x_{2}\right}}$ show that the model reaches a high performance with the number of shots $k=8$, there is still a performance gap in the case of $k=2$.
formance gap. This is likely due to the task being quite simple (appending a " 0 " to the input number) so, the model is able to generalize in the manner we are evaluating it. However, it is also possible that we are simply not identifying the right frequency statistics for this task, and there is an effect that our current evaluation setup does not capture.</p>
<p>Studying the Size of Language Models To further study the impact of language models sizes on the performance gap caused by the instance frequencies, we perform the arithmetic experiments for 2,8 shots using the smaller models (GPT-Neo1.3B and GPT-Neo-2.7B). We can see the trends of the average accuracy of the models in Figures 6. The smaller models overall are less accurate on the arithmetic tasks, which is consistent with observations in related work (Brown et al., 2020). However, their success is still focused on the more frequent terms from the pretraining corpus, suggesting that even the smaller models show the effect of reliance on the pretraining data, although to a much lower extent than the larger ones.</p>
<p>Table 3: GPT-J-6B results on Time-Unit Conversion: $\Delta_{1,2}, \Delta_{1,2,3}$ and $\Delta_{1,2, y}$ represent the performance gap over the frequency distributions of $\omega_{\left{x_{1}, x_{2}\right}}, \omega_{\left{x_{1}, x_{2}, x_{3}\right}}$ and $\omega_{\left{x_{1}, x_{2}, y\right}}$ respectively, where $x_{1}$ is the number operand, $x_{2}$ is the source unit, $x_{3}$ is the number operand needed for performing the conversion and the $y$ is the true answer.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">k</th>
<th style="text-align: center;">Min $\rightarrow$ Sec</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Hour $\rightarrow$ Min</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Day $\rightarrow$ Hour</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Week $\rightarrow$ Day</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">$\Delta_{1,2}$</td>
<td style="text-align: center;">$\Delta_{1,2,3}$</td>
<td style="text-align: center;">$\Delta_{1,2, y}$</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">$\Delta_{1,2}$</td>
<td style="text-align: center;">$\Delta_{1,2,3}$</td>
<td style="text-align: center;">$\Delta_{1,2, y}$</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">$\Delta_{1,2}$</td>
<td style="text-align: center;">$\Delta_{1,2,3}$</td>
<td style="text-align: center;">$\Delta_{1,2, y}$</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">$\Delta_{1,2}$</td>
<td style="text-align: center;">$\Delta_{1,2,3}$</td>
<td style="text-align: center;">$\Delta_{1,2, y}$</td>
</tr>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">19.4</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">52.0</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">29.1</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">39.5</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">50.0</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">36.3</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">53.5</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">30.9</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">51.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Shots, $k$</td>
<td style="text-align: center;">Month $\rightarrow$ Week</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Year $\rightarrow$ Month</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Decade $\rightarrow$ Year</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">$\Delta_{1,2}$</td>
<td style="text-align: center;">$\Delta_{1,2,3}$</td>
<td style="text-align: center;">$\Delta_{1,2, y}$</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">$\Delta_{1,2}$</td>
<td style="text-align: center;">$\Delta_{1,2,3}$</td>
<td style="text-align: center;">$\Delta_{1,2, y}$</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">$\Delta_{1,2}$</td>
<td style="text-align: center;">$\Delta_{1,2,3}$</td>
<td style="text-align: center;">$\Delta_{1,2, y}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;">9.0</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">47.4</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">45.4</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">16</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">57.3</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: The effect of model size on performance Smaller models only perform well on instances with more frequent terms in the pretraining data. $k$ represents the number of shots.</p>
<p>Impact Due to Memorization In this section, we will study the extent to which the impact of the pretraining term frequencies on model performance can be explained due to pure memorization. To tease apart direct memorization, we perform similar analysis as above, but do so without the instances that have already been memorized by the language model in their entirety. It is worth mentioning that finding such instances is not trivial. In other words, it is not trivial to identify purely memorized instances. Prior work (Magar and Schwartz, 2022; Carlini et al., 2022) has shown that the models perform more accurately on the instances with higher exact match counts from the pertaining data; we first verify this trend by observing a similar trend for our setting by plotting $\omega_{\left{x_{1}, x_{2}, y\right}}$, the cooccurrence of all three numbers in Appendix Figure 9). Based on these studies and results, we treat
the number triples with the highest average accuracy (more than $85 \%$ ), as the ones the model has most likely memorized. Specifically, we remove these memorized instances from our evaluation to see the impact of lower order term frequencies on the remaining instances. As shown in Figure 7, the dependency of model performance on lower order frequencies $\left(\omega_{\left{x_{1}\right}}\right.$ and $\left.\omega_{\left{x_{1}, x_{2}\right}}\right)$ is still very high even after removing the memorized instances. These observations suggest that the impact of term frequencies on model performance is beyond pure memorization of the numerical terms.</p>
<p>Summary Overall, we observe high positive performance gap for almost all of the experiments on the three definition levels of the frequency for each task. This suggests a strong effect of frequency of the instances in the pretraining data on the model performance. In particular, evaluation using performance gap with $\omega_{\left{x_{1}\right}}$ shows that even the unigram statistics of the instances have strong correlation with the models performance on the instance.</p>
<p>Other than some exceptional cases, we observe an increasing trend in the performance gap as we put more training examples in the prompt (the number of shots); this can be a further indication that the model is directed through the patterns in the pretraining data to answer the reasoning questions. Our experiments with the smaller sizes of the model also show that they can only solve the frequent instances of the tasks, which further supports our observation that model performance is correlated with the term frequencies.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: The effect of term frequencies after removing memorized instances on 2-shot GPT-J-6B. Dependence of model performance on unigram and cooccurrence frequencies (after removing the memorized instances) shows the effect exists beyond memorization.</p>
<h2>5 Related Work</h2>
<p>A large and growing body of literature has investigated a number of related concerns with large language models (for discussion of more tangentially related work see Appendix A.1).</p>
<p>Numeracy and Temporal Reasoning in LMs Our work contributes to the larger body of work studying numeracy in word embeddings and language models (Spithourakis and Riedel, 2018; Wallace et al., 2019). Geva et al. (2020), Zhou et al. (2020) Zhou et al. (2022) and Lewkowycz et al. (2022) propose training schemes to help improve LMs' temporal and numerical reasoning capabilities. Patel et al. (2021) show that NLP math solvers rely on simple heuristics to answer math questions. We expect that the performance gap metric proposed in this work will be useful to better understand the impact of such schemes.</p>
<p>Impact of Frequency on LM Performance Kassner et al. (2020) and Wei et al. (2021) perform controlled experiments varying pretraining data to characterize the extent to which pretraining affects LMs' ability to memorize and reason with facts as well as learn generalizable syntax rules. In line
with our results, both of these find that frequency is a distinguishing factor in whether or not the model memorizes a particular fact or syntactic rule for a verb form. Sinha et al. (2021) further demonstrate that shuffling word order during pretraining has minimal impact on an LMs' accuracy on downstream tasks, and, concurrent with this work, Min et al. (2022) similarly find that shuffling labels in in-context learning demonstrations has minimal impact on few-shot accuracy. These results further suggest that LMs' performance is largely driven by their ability to model high-order word cooccurrence statistics. Data privacy researchers have shown that LMs may memorize sensitive sequences occurring in training data even if they are rare (Carlini et al., 2019; Song and Shmatikov, 2019).</p>
<p>Memorization Feldman (2020) provide a theoretical definition of memorization as the difference between the accuracy of a model on a training data point when that point is included vs. excluded from training. They also develop an approach for approximating memorization using influence functions Feldman and Zhang (2020). This framework is applied to study memorization in language models by Zhang et al. (2021), who find that training examples that are memorized by the LM tend to have high influence of LM predictions on similar validation instances. Their result may provide a plausible explanation that the frequency effects observed in this work are due to memorization.</p>
<h2>6 Discussion</h2>
<p>In this work, we consider how to conduct few-shot evaluations in light of the analysis with the pretraining data. Prior work has attempted to control for overlap between pretraining data and the test instances, but as we have seen, those methods are insufficient. For example, Brown et al. (2020) measure the impact of removing instances from evaluation datasets that share 13-gram overlap with their pretraining data on GPT-3's accuracy, and also argue that the low occurrence of exact phrases such as "NUM1 + NUM2 =" and "NUM1 plus NUM2" in the pretraining data indicate that the model's strong performance on arithmetic tasks is likely due to factors other than memorization. However, we show that LM performance is impacted by much simpler statistical patterns, as small as unigram overlap with the pretraining data.</p>
<p>For these reasons, we strongly recommend that evaluation of reasoning capabilities should take the</p>
<p>pretraining corpus into account, and any claims of reasoning can only be made after demonstrating robustness to the effect of pretraining. Current LM benchmarks, that are dissociated from the model's pretraining data, make it impossible to interpret few-shot reasoning performance results. It is worth mentioning that, even a performance gap of 0 is likely not sufficient to claim reasoning capabilities-what exactly constitutes "reasoning" remains ill-defined-but it may be a necessary condition, and one that current models do not meet.</p>
<p>In this study, we are not making a causal claim, and in general, there may be confounders that we have not eliminated in our setting. Recently, Elazar et al. (2022) introduced a causal framework based on pretraining data statistics for understanding language model's factual predictions. To be able to use the causal inference techniques they construct and assume a causal graph for the task of extracting factual knowledge from pretrained language models. We recommend further research in the proposed direction for other NLP tasks such as reasoning and interventions during training to provide finer-grained analysis of the effect of pretraining.</p>
<p>One potential concern is that our experiments do not distinguish whether incorrect answers are due to lack of reasoning or lack of recognition, i.e. it is possible that the model has the ability to multiply but the embeddings for rare terms are not adapted to that algorithm. However, recognizing numbers is a prerequisite to numerical reasoning, thus if the models lack the ability to identify numbers, this still means that they lack numerical reasoning skills. That said, we also suspect that the errors are not due to recognition. Even the most infrequent terms in our experiments have been seen millions of times-they are not unknown tokens.</p>
<h2>7 Conclusion</h2>
<p>We show that in-context language model performance on numerical reasoning tasks can be impacted significantly by low-order co-occurrence statistics in the pretraining data, raising questions on the extent to which these models are actually reasoning to solve these tasks. These observations suggest the necessity for reconsidering and redefining the reasoning evaluation schemes for the large language models. Further characterizing the impacting factors on the models reasoning capacities is also an important tasks for the community. Most importantly, we suggest that the community should
not treat the pretraining data of the large language models as unknown black boxes. Overlooking the impact of the pretraining data can be misleading in evaluating the model reasoning skills.</p>
<h2>Acknowledgements</h2>
<p>We would like to thank the members of UCINLP, Yanai Elazar, Mukund Sundarajan, Marco Tulio Ribeiro, Eric Wallace, Shivanshu Gupta, Navid Salehnamadi, Pouya Pezeshkpour, and Dylan Slack for valuable discussions and feedback on this work. This material is sponsored in part by the DARPA MCS program under Contract No. N660011924033 with the United States Office Of Naval Research, by an Amazon Research Award, and by awards IIS-2046873 and IIS-2040989 from the National Science Foundation.</p>
<h2>Limitations</h2>
<p>There are a few limitations to our study that open up avenues for future research. First, our approach aggregates fairly simple patterns and the effect we observe might be stronger if a wider variety and complexity of patterns is considered in the pretraining corpus. Similarly, our work is limited to simple numerical reasoning tasks, and it would be worthwhile to study how much other reasoning evaluations and more complex quantitive reasoning tasks such as GSM8K (Cobbe et al., 2021) are impacted by the same effect, which could be measured using the performance gap metric introduced here. Defining appropriate instance terms for other reasoning tasks such as commonsense reasoning will be a challenging but important direction for future work. Lastly, we do not propose a solution for changing language models to robust reasoners. We hope that the insights in this work inspire further studies into the effect of pretraining on language model's performance, improvements in evaluation schemes, and better training mechanisms for more robust language models with true generalization capabilities.</p>
<h2>References</h2>
<p>Emily M Bender, Timnit Gebru, Angelina McMillanMajor, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 610-623.</p>
<p>Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2022. Quantifying memorization across neural language models. arXiv preprint arXiv:2202.07646.</p>
<p>Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn Song. 2019. The secret sharer: Evaluating and testing unintended memorization in neural networks. In Proceedings of the 28th USENIX Conference on Security Symposium, SEC'19, page 267-284, USA. USENIX Association.</p>
<p>Alvin Chan, Yi Tay, Yew-Soon Ong, and Aston Zhang. 2020. Poison attacks against text datasets with conditional adversarially regularized autoencoder. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4175-4189, Online. Association for Computational Linguistics.</p>
<p>Stephanie CY Chan, Adam Santoro, Andrew K Lampinen, Jane X Wang, Aaditya Singh, Pierre H Richemond, Jay McClelland, and Felix Hill. 2022. Data distributional properties drive emergent incontext learning in transformers. arXiv preprint arXiv:2205.05055.</p>
<p>Anthony Chen, Pallavi Gudipati, Shayne Longpre, Xiao Ling, and Sameer Singh. 2021. Evaluating entity disambiguation and the role of popularity in retrievalbased NLP. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4472-4485, Online. Association for Computational Linguistics.</p>
<p>Danqi Chen, Jason Bolton, and Christopher D. Manning. 2016. A thorough examination of the CNN/Daily Mail reading comprehension task. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2358-2367, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Joe Davison, Joshua Feldman, and Alexander Rush. 2019. Commonsense knowledge mining from pretrained models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1173-1178, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. Documenting large webtext corpora: A case study on the colossal clean crawled corpus. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1286-1305, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2368-2378, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Aparna Elangovan, Jiayuan He, and Karin Verspoor. 2021. Memorization vs. generalization : Quantifying data leakage in NLP performance evaluation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1325-1335, Online. Association for Computational Linguistics.</p>
<p>Yanai Elazar, Nora Kassner, Shauli Ravfogel, Amir Feder, Abhilasha Ravichander, Marius Mosbach, Yonatan Belinkov, Hinrich Schütze, and Yoav Goldberg. 2022. Measuring causal effects of data statistics on language model'sfactual'predictions. arXiv preprint arXiv:2207.14251.</p>
<p>Vitaly Feldman. 2020. Does learning require memorization? a short tale about a long tail. In Proccedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, STOC 2020, Chicago, IL, USA, June 22-26, 2020, pages 954-959. ACM.</p>
<p>Vitaly Feldman and Chiyuan Zhang. 2020. What neural networks memorize and why: Discovering the long</p>
<p>tail via influence estimation. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027.</p>
<p>Matt Gardner, William Merrill, Jesse Dodge, Matthew Peters, Alexis Ross, Sameer Singh, and Noah A. Smith. 2021. Competency problems: On finding and removing artifacts in language data. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1801-1813, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III au2, and Kate Crawford. 2021. Datasheets for datasets.</p>
<p>Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. 2020. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3356-3369, Online. Association for Computational Linguistics.</p>
<p>Mor Geva, Ankit Gupta, and Jonathan Berant. 2020. Injecting numerical reasoning skills into language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 946-958, Online. Association for Computational Linguistics.</p>
<p>Aaron Gokaslan and Vanya Cohen. 2019. Openwebtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus.</p>
<p>Kyle Gorman and Steven Bedrick. 2019. We need to talk about standard splits. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2786-2791, Florence, Italy. Association for Computational Linguistics.</p>
<p>Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. 2018. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107-112, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Matthew Honnibal and Ines Montani. 2017. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. To appear.</p>
<p>Robin Jia and Percy Liang. 2017. Adversarial examples for evaluating reading comprehension systems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2021-2031, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Rabeeh Karimi Mahabadi, Yonatan Belinkov, and James Henderson. 2020. End-to-end bias mitigation by modelling biases in corpora. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8706-8716, Online. Association for Computational Linguistics.</p>
<p>Nora Kassner, Benno Krojer, and Hinrich Schütze. 2020. Are pretrained language models symbolic reasoners over knowledge? In Proceedings of the 24th Conference on Computational Natural Language Learning, pages 552-564, Online. Association for Computational Linguistics.</p>
<p>Patrick Lewis, Pontus Stenetorp, and Sebastian Riedel. 2021a. Question and answer test-train overlap in open-domain question answering datasets. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1000-1008, Online. Association for Computational Linguistics.</p>
<p>Patrick Lewis, Pontus Stenetorp, and Sebastian Riedel. 2021b. Question and answer test-train overlap in open-domain question answering datasets. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1000-1008, Online. Association for Computational Linguistics.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858.</p>
<p>Bill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xiang Ren. 2020a. Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of PreTrained Language Models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6862-6868, Online. Association for Computational Linguistics.</p>
<p>Bill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xiang Ren. 2020b. Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of PreTrained Language Models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6862-6868, Online. Association for Computational Linguistics.</p>
<p>Alexandra Luccioni and Joseph Viviano. 2021. What's in the box? an analysis of undesirable content in the Common Crawl corpus. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2:</p>
<p>Short Papers), pages 182-189, Online. Association for Computational Linguistics.</p>
<p>Inbal Magar and Roy Schwartz. 2022. Data contamination: From memorization to exploitation. arXiv preprint arXiv:2203.08242.</p>
<p>Tom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428-3448, Florence, Italy. Association for Computational Linguistics.</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837.</p>
<p>Blaine Nelson, Marco Barreno, Fuching Jack Chi, Anthony D. Joseph, Benjamin I. P. Rubinstein, Udam Saini, Charles Sutton, J. D. Tygar, and Kai Xia. 2008. Exploiting machine learning to subvert your spam filter. In Proceedings of the 1st Usenix Workshop on Large-Scale Exploits and Emergent Threats, LEET'08, USA. USENIX Association.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080-2094, Online. Association for Computational Linguistics.</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463-2473, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. 2018. Hypothesis only baselines in natural language inference. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pages 180-191, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer.</p>
<p>Yasaman Razeghi, Raja Sekhar Reddy Mekala, Robert L. Logan IV, Matt Gardner, and Sameer Singh. 2022. Snoopy: An online interface for exploring the effect of pretraining term frequencies on few-shot lm performance. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations.</p>
<p>Alexey Romanov, Maria De-Arteaga, Hanna Wallach, Jennifer Chayes, Christian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, Anna Rumshisky, and Adam Kalai. 2019. What's in a name? Reducing bias in bios without access to protected attributes. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4187-4195, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. 2018. Gender bias in coreference resolution. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 8-14, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019. Analysing mathematical reasoning abilities of neural models. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.</p>
<p>Serge Sharoff. 2020. Know thy corpus! robust methods for digital curation of web corpora. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 2453-2460, Marseille, France. European Language Resources Association.</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4222-4235, Online. Association for Computational Linguistics.</p>
<p>Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, and Douwe Kiela. 2021. Masked language modeling and the distributional hypothesis: Order word matters pre-training for little. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2888-2913, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Anders Søgaard, Sebastian Ebert, Jasmijn Bastings, and Katja Filippova. 2021. We need to talk about random splits. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1823-1832, Online. Association for Computational Linguistics.</p>
<p>Congzheng Song and Vitaly Shmatikov. 2019. Auditing data provenance in text-generation models. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019, pages 196-206. ACM.</p>
<p>Georgios Spithourakis and Sebastian Riedel. 2018. Numeracy for language models: Evaluating and improving their ability to predict numbers. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2104-2115, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Saku Sugawara, Kentaro Inui, Satoshi Sekine, and Akiko Aizawa. 2018. What makes reading comprehension questions easier? In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4208-4219, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Avijit Thawani, Jay Pujara, Filip Ilievski, and Pedro Szekely. 2021. Representing numbers in NLP: a survey and a vision. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 644-656, Online. Association for Computational Linguistics.</p>
<p>Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019. Do NLP models know numbers? probing numeracy in embeddings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5307-5315, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Eric Wallace, Tony Zhao, Shi Feng, and Sameer Singh. 2021. Concealed data poisoning attacks on NLP models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 139-150, Online. Association for Computational Linguistics.</p>
<p>Ben Wang. 2021. Mesh-Transformer-JAX: ModelParallel Implementation of Transformer Language Model with JAX. https://github.com/ kingoflolz/mesh-transformer-jax.</p>
<p>Jason Wei, Dan Garrette, Tal Linzen, and Ellie Pavlick. 2021. Frequency effects on syntactic rule learning in transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 932-948, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>
<p>Nathaniel Weir, Adam Poliak, and Benjamin Van Durme. 2020. Probing neural language models for human tacit assumptions. In $\operatorname{CogSci}$.</p>
<p>Chiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian Tramèr, and Nicholas Carlini. 2021. Counterfactual memorization in neural language models. arXiv preprint arXiv:2112.12938.</p>
<p>Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning.</p>
<p>Ben Zhou, Qiang Ning, Daniel Khashabi, and Dan Roth. 2020. Temporal common sense acquisition with minimal supervision. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7579-7589, Online. Association for Computational Linguistics.</p>
<p>Fan Zhou, Haoyu Dong, Qian Liu, Zhoujun Cheng, Shi Han, and Dongmei Zhang. 2022. Reflection of thought: Inversely eliciting numerical reasoning in language models via solving linear systems. arXiv preprint arXiv:2210.05075.</p>
<h2>A Appendix</h2>
<h2>A. 1 Additional Related Work</h2>
<p>In this section, we further discuss the related work.</p>
<p>Prompting Prompting has been widely applied to study the factual (Petroni et al., 2019), commonsense (Davison et al., 2019; Weir et al., 2020; Lin et al., 2020b), mathematical (Saxton et al., 2019), and other NLP task-related (Radford et al., 2019; Shin et al., 2020) knowledge LMs acquire during pretraining. In this work, we focus on the in-context learning setup of Brown et al. (2020), who use prompts that include training examples to diagnose LMs' few-shot learning capabilities.</p>
<p>Training Artifacts Challenge Evaluation Our results raise the issue that in-context learning probes may overestimate an LM's ability generalize from few examples when biases are present in the training data. This is consistent with prior work that has exposed the similar effects of biases from: lexical cues in natural language inference datasets (Gururangan et al., 2018; Poliak et al., 2018; McCoy et al., 2019), question-passage overlap and entity cues in reading comprehension datasets (Chen et al., 2016; Sugawara et al., 2018; Jia and Liang, 2017; Lewis et al., 2021b), gender cues in coreference resolution datasets (Rudinger et al., 2018), popularity in named entity disambiguation (Chen et al., 2021), similarity between training and test instances in information extraction and sentiment analysis datasets (Elangovan et al., 2021), and effects of how data is split (Gorman and Bedrick, 2019; Søgaard et al., 2021). Relatedly, data poisoning research studies how to adversarially introduce artifacts into training data to produce unwanted model behaviors (Nelson et al., 2008; Chan et al., 2020; Wallace et al., 2021). A general statistical procedure to test for artifacts is presented in Gardner et al. (2021), who also theoretically show that large datasets are almost certain to contain artifacts under reasonable assumptions. Techniques for mitigating biases in the presence of dataset artifacts are covered by Romanov et al. (2019) and Karimi Mahabadi et al. (2020).</p>
<p>Documenting Pretraining Data To better understand the risks of dataset artifacts, there has been a call to better document the characteristics and intended uses of datasets (Gebru et al., 2021; Bender et al., 2021). However, due to the sheer size of
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: The GPT-J-6B accuracy on arithmetic and operator inference tasks, with $k$ shots. The average accuracy ( $y$-axis) of the binned instances is highly correlated with their co-occurrences term frequencies $\omega_{\left{x_{1}, x_{2}\right}}$ in the pretraining corpus ( $x$-axis).</p>
<p>LM pretraining datasets-which range from 100's of GBs to 10's of TBs-doing so can pose a substantial challenge. Despite this, researchers have been able to estimate word frequencies, topics, and genres of documents (Sharoff, 2020), as well as proportions of toxic text (Gehman et al., 2020) appearing in OpenWebText (Gokaslan and Cohen, 2019). Similar efforts have been made to characterize the top-level domains, amount of hate speech, and censured text appearing in the C4 corpus (Raffel et al., 2020; Dodge et al., 2021; Luccioni and Viviano, 2021). Our work documents co-occurrence statistics of numbers and dates of documents appearing in the Pile dataset.</p>
<h2>A. 2 Examples of time unit conversion plots</h2>
<p>We provide the figures showing the dependence between the average accuracy and the $\omega_{\left{x_{1}, x_{2}\right}}$ for time unit experiments of Minute, Year and Decade in Figures 10, 4 and 5, respectively.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: The impact of $\omega_{\left{x_{1}, x_{2}, y\right}}$ (the frequency of all numbers $\left(x_{1}, x_{2}, y\right)$ in an arithmetic instance) on GPT-J-6B's 2-shot performance, the high dependence of models average accuracy on $\omega_{\left{x_{1}, x_{2}, y\right}}$ may be due to memorization specifically in highest frequencies
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: GPT-J-6B performance on Minute-Second: The interpolation lines show the correlation between the average accuracy and the $\omega_{\left{x_{1}, x_{2}\right}} . k$ is the number of shots.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Source code at https://huggingface.co/EleutherAI
${ }^{2}$ https://turing.microsoft.com/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8788 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8788</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8788</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-235097283</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2107.09556v1.pdf" target="_blank">WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset</a></p>
                <p><strong>Paper Abstract:</strong> We present a new dataset of Wikipedia articles each paired with a knowledge graph, to facilitate the research in conditional text generation, graph generation and graph representation learning. Existing graph-text paired datasets typically contain small graphs and short text (1 or few sentences), thus limiting the capabilities of the models that can be learned on the data. Our new dataset WikiGraphs is collected by pairing each Wikipedia article from the established WikiText-103 benchmark (Merity et al., 2016) with a subgraph from the Freebase knowledge graph (Bollacker et al., 2008). This makes it easy to benchmark against other state-of-the-art text generative models that are capable of generating long paragraphs of coherent text. Both the graphs and the text data are of significantly larger scale compared to prior graph-text paired datasets. We present baseline graph neural network and transformer model results on our dataset for 3 tasks: graph ->text generation, graph ->text retrieval and text ->graph retrieval. We show that better conditioning on the graph provides gains in generation and retrieval quality but there is still large room for improvement.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8788.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8788.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BoW</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bag-of-Words graph embedding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single bag-of-words embedding computed from all tokens in a knowledge graph (nodes and edges), projected to a latent vector and exposed as a single extra "graph token" to a Transformer-XL language model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Bag-of-words (BoW) graph representation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>All textual tokens from node string literals and edge labels in the KG are aggregated into a single bag-of-words vector (entity IDs and numeric values replaced by special tokens like <entity> and <number>), then linearly projected to a d-dimensional embedding. This yields T=1 graph "token" embedding shared across all text time steps and appended to the Transformer-XL attention key/value set.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (Freebase subgraphs with entity nodes and string-literal nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Tokenize and normalize all string literals and edge labels in the selected subgraph; replace entity IDs and numbers with special tokens; aggregate tokens into a single bag-of-words vector; apply a learned linear projection to obtain a single embedding vector.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph → text generation (conditioning LM), graph → text retrieval, text → graph retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative: conditioning with BoW improves reverse-BLEU (rBLEU) over unconditional model (exact rBLEU numbers in Table 3 of paper). Retrieval (graph→text) metrics (Table 5): Recall@1=16.28, Recall@5=30.23, mAP=25.98 (reported as percentages). Text→graph retrieval (Table 6): Recall@1=95.35, Recall@5=100.00, mAP=97.67 (percent). Perplexity: authors report that conditioning on graphs (including BoW) did not improve LM perplexity compared to text-only baseline (text-only ~25.08 perplexity on their test set).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>BoW is the simplest representation and is outperformed by node-level embeddings and GNN-based embeddings in relevance metrics (rBLEU, retrieval mAP), but differences are relatively small for generation. When the graph is heavily subsampled, BoW and GNN perform similarly (Figure 6).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple and computationally cheap (T=1); easy to compute and integrate into Transformer-XL as a single extra token; captures lexical signal from node and edge text.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Ignores node-level and structural information (edges and node identity indistinguishable); potential information loss due to aggregation; cannot exploit graph topology.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Performs similarly to more expensive models when graph structure is important (loses advantage when more graph context is retained); less benefit as graph size grows compared to GNN; does not improve likelihood/perplexity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8788.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8788.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nodes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nodes-only bag-of-words per-node embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representation that constructs a separate bag-of-words embedding for each node in the graph (ignoring edges), producing T = number of nodes embeddings that the Transformer-XL can attend to.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Nodes-only (per-node BoW) representation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For each node in the KG, build a BoW vector from that node's textual attributes (string literals); project each node-BoW to an embedding producing one graph embedding per node. Edges are ignored; node embeddings are exposed as T embeddings to the Transformer-XL as additional tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (Freebase subgraphs with entity nodes and string-literal nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>For each node: tokenize and normalize its string-literal attributes; replace entity IDs/numbers with special tokens as needed; create BoW vector per node and linearly project to an embedding. Do not encode edges explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph → text generation (conditioning LM), graph → text retrieval, text → graph retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Generation: conditioned models using Nodes improved sample relevance (rBLEU) relative to BoW and unconditional (exact numbers in Table 3). Retrieval (graph→text) (Table 5): Recall@1=16.28, Recall@5=34.88, mAP=26.62 (percent). Text→graph retrieval (Table 6): Recall@1=93.02, Recall@5=100.00, mAP=96.51 (percent). Perplexity: no observed improvement over text-only LM.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Nodes representation outperforms single-vector BoW in generation relevance and retrieval metrics because it preserves node-level distinctions; however, a GNN that incorporates edges outperforms Nodes (GNN > Nodes > BoW) though differences are modest for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Preserves per-node textual context (less information loss than BoW); straightforward to compute; allows Transformer to attend to individual node embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Ignores explicit edge/structural information which can be important for semantics; T scales with number of nodes (more memory/compute than BoW).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When graph structure (relations) matters, Nodes underperforms GNNs; heavy graph subsampling reduces its advantage; does not improve perplexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8788.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8788.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Neural Network node embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representation that embeds both node and edge textual attributes and then applies a graph neural network (message passing) to produce structure-aware node embeddings used as extra tokens for conditioning a Transformer-XL language model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph Neural Network (GNN) representation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Embed BoW representations for nodes and edges, then run a message-passing GNN over the 1-hop subgraph to produce updated node embeddings (T = number of nodes). These node embeddings are exposed to Transformer-XL as additional attention key/value tokens shared across time steps.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (Freebase subgraphs with entity and string-literal nodes, labeled edges)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>1) For each node and edge, build a BoW token embedding from text attributes; 2) initialize node/edge features with these embeddings; 3) apply a GNN (message-passing layers) to propagate information across the 1-hop neighborhood, yielding final node embeddings; 4) present the T node embeddings to the Transformer-XL as graph tokens (no positional encodings).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph → text generation (conditioning LM), graph → text retrieval, text → graph retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Generation: GNN-conditioned models yield higher rBLEU than Nodes and BoW (exact rBLEU numbers in Table 3). Retrieval (graph→text) (Table 5): Recall@1=18.60, Recall@5=34.88, mAP=27.79 (percent). Text→graph retrieval (Table 6): Recall@1=100.00, Recall@5=100.00, mAP=100.00 (percent). Ablation: message passing (>=1 layers) substantially improves rBLEU vs 0 layers; increasing layers beyond 1 yields diminishing returns. Perplexity: no improvement over text-only LM.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>GNN > Nodes > BoW on relevance metrics (rBLEU) and retrieval; differences are consistent but described as relatively small for generation. GNN benefits more than BoW as graph size increases (Figure 6). When nodes are heavily subsampled, GNN's advantage disappears and it performs similarly to BoW.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Incorporates graph topology and edge information leading to better relevance and retrieval performance; benefits from increased graph context; message passing improves conditioning quality.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Higher computational and memory cost (T = number of nodes, plus GNN compute); modest gains in generative likelihood (perplexity not improved); benefit reduces when generating very long samples.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Advantage decreases for very long generated sample lengths (up to 4096 tokens) — noise overwhelms signal and GNN advantage shrinks; heavy node subsampling removes structural advantage; 2-hop neighborhoods were found to introduce irrelevant nodes via hubs, so 1-hop was used to avoid adding irrelevant information.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8788.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8788.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-conditioned attention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-conditioned Transformer-XL attention (graph-as-tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptation of Transformer-XL where graph embeddings (from BoW, Nodes, or GNN) are appended as extra key/value "tokens" and the text queries attend to both text and graph keys, enabling the LM to condition on graph information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph-conditioned attention (graph tokens appended to Transformer-XL)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Compute text token queries Q_t and keys/values K_t,V_t from text; compute K_g,V_g from graph embeddings; compute attention scores between Q_t and both K_t and K_g, concatenate attention outputs ([A_t • A_g], [V_t • V_g]) and apply masked softmax as usual. Graph embeddings are static across time steps and no positional encodings are used for them.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (Freebase subgraphs) — works with any method that produces a set of graph embeddings (BoW single embedding, per-node embeddings, or GNN node embeddings).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Graph -> set of embedding vectors (T embeddings) via BoW/Nodes/GNN; include embeddings as extra non-sequential tokens in Transformer-XL attention key/value matrices so text tokens attend to graph keys.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph → text generation (long-paragraph LM conditioning), graph → text retrieval (ranking via per-token likelihood), text → graph retrieval (ranking graphs given text)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Using this conditioning architecture, authors report: conditioned models steer generation to be more relevant (rBLEU improvements) though perplexity does not improve. Retrieval metrics (Tables 5 & 6) show strong improvements over unconditional model, with GNN + graph-conditioned attention producing best retrieval scores (graph→text mAP up to 27.79%; text→graph mAP up to 100%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>This attention-based conditioning is compatible with different graph encodings; relative performance depends on the graph encoder (GNN > Nodes > BoW for relevance/retrieval). Authors note simpler conditioning (BoW) is cheaper but less effective than GNN in exploiting structure.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Naturally integrates graph information into a strong LM (Transformer-XL) with minimal architectural changes; supports long-text generation via Transformer-XL's memory mechanism; flexible to different graph encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Graph tokens have no positional encoding (non-sequential) which may limit some order-sensitive signals; adding many node embeddings increases attention cost and memory usage; still does not improve LM perplexity in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Does not increase perplexity even when conditioned on graphs (i.e., likelihood metric not improved); GNN-conditioned attention advantage diminishes for very long sample lengths; attention overhead grows with number of graph tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8788.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8788.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Subgraph extraction pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated Freebase subgraph extraction (mapping, expansion, filtering)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated three-stage pipeline to pair each Wikipedia article with a relevant 1-hop Freebase subgraph: (1) map article title to Freebase entity via wikipedia.en triples, (2) expand to include 1-hop neighbor entities and their string attributes, (3) filter and deduplicate string attributes and cap graph size.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Subgraph extraction and textual canonicalization pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Procedure to convert Wikipedia articles into paired KGs: map article titles to Freebase entity nodes (via wikipedia.en relation); take 1-hop neighborhood (including edges between neighbors and string literal nodes); include string attributes text as node content; deduplicate multiple identical relation->string edges by choosing a canonical string using a character unigram model; filter entities with few attributes and restrict to top relation types; cap graph sizes at thresholds (256/512/1024 nodes).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Large-scale knowledge graph (Freebase) producing subgraphs containing entity nodes and string-literal nodes</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Mapping: string match article titles to Freebase wikipedia.en triples. Expansion: include 1-hop neighboring entities and their relations to string attributes; include inter-neighbor edges. Filtering: drop entities with <4 string attributes at preprocessing; restrict to top 1024 relation types; deduplicate multiple string attributes per relation by selecting canonical string via unigram character model; cap final graph size.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Dataset creation for training/evaluating graph→text generation, graph→text retrieval, text→graph retrieval; provides textual content in string-literal nodes used by graph encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Dataset statistics: 23,522 graph-text pairs (82.3% of Wikitext-103); average graph: 38.7 nodes and 48.3 edges; average graph string-literal tokens = 895.1; average article length 3,533.8 tokens. Mapping success: 24,345/28,475 (85.5%) titles mapped to Freebase entities. These stats indicate scale but are not direct predictive-performance metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to prior graph-text datasets (e.g., WebNLG, GenWiki, Lebret et al.), WikiGraphs subgraphs are significantly larger (avg. ~38.7 nodes vs prior datasets with 10–20 or fewer nodes) and paired text is orders of magnitude longer (full Wikipedia articles vs single/few sentences).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Scalable and automatic (able to scale to millions of pairs); creates rich subgraphs with both structural and long textual content in string-literal nodes; filters reduce redundancy and noise (unigram canonicalization); 1-hop choice avoids noisy hubs from 2-hop expansion.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Automated alignment may miss mappings (17.7% of Wikitext-103 not paired); relies on Freebase coverage and quality; 2-hop neighborhoods were found to introduce irrelevant nodes via hubs, so limited to 1-hop which may miss some context; string-literal redundancy and noise require heuristic filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>2-hop expansion creates overly large and noisy subgraphs (irrelevant hub-induced entities) so the authors restricted to 1-hop; Freebase contains redundant/variant string attributes requiring canonicalization; mapping fails for a nontrivial subset of articles; graphs can be skewed (few high-degree nodes) presenting challenges to representation learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset', 'publication_date_yy_mm': '2021-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural text generation from structured data with application to the biography domain <em>(Rating: 2)</em></li>
                <li>The WebNLG challenge: Generating text from RDF data <em>(Rating: 2)</em></li>
                <li>Text Generation from Knowledge Graphs with Graph Transformers <em>(Rating: 2)</em></li>
                <li>GenWiki: A dataset of 1.3 million content-sharing text and graphs for unsupervised graph-to-text generation <em>(Rating: 2)</em></li>
                <li>Relational inductive biases, deep learning, and graph networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8788",
    "paper_id": "paper-235097283",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "BoW",
            "name_full": "Bag-of-Words graph embedding",
            "brief_description": "A single bag-of-words embedding computed from all tokens in a knowledge graph (nodes and edges), projected to a latent vector and exposed as a single extra \"graph token\" to a Transformer-XL language model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Bag-of-words (BoW) graph representation",
            "representation_description": "All textual tokens from node string literals and edge labels in the KG are aggregated into a single bag-of-words vector (entity IDs and numeric values replaced by special tokens like &lt;entity&gt; and &lt;number&gt;), then linearly projected to a d-dimensional embedding. This yields T=1 graph \"token\" embedding shared across all text time steps and appended to the Transformer-XL attention key/value set.",
            "graph_type": "Knowledge graphs (Freebase subgraphs with entity nodes and string-literal nodes)",
            "conversion_method": "Tokenize and normalize all string literals and edge labels in the selected subgraph; replace entity IDs and numbers with special tokens; aggregate tokens into a single bag-of-words vector; apply a learned linear projection to obtain a single embedding vector.",
            "downstream_task": "Graph → text generation (conditioning LM), graph → text retrieval, text → graph retrieval",
            "performance_metrics": "Qualitative: conditioning with BoW improves reverse-BLEU (rBLEU) over unconditional model (exact rBLEU numbers in Table 3 of paper). Retrieval (graph→text) metrics (Table 5): Recall@1=16.28, Recall@5=30.23, mAP=25.98 (reported as percentages). Text→graph retrieval (Table 6): Recall@1=95.35, Recall@5=100.00, mAP=97.67 (percent). Perplexity: authors report that conditioning on graphs (including BoW) did not improve LM perplexity compared to text-only baseline (text-only ~25.08 perplexity on their test set).",
            "comparison_to_others": "BoW is the simplest representation and is outperformed by node-level embeddings and GNN-based embeddings in relevance metrics (rBLEU, retrieval mAP), but differences are relatively small for generation. When the graph is heavily subsampled, BoW and GNN perform similarly (Figure 6).",
            "advantages": "Simple and computationally cheap (T=1); easy to compute and integrate into Transformer-XL as a single extra token; captures lexical signal from node and edge text.",
            "disadvantages": "Ignores node-level and structural information (edges and node identity indistinguishable); potential information loss due to aggregation; cannot exploit graph topology.",
            "failure_cases": "Performs similarly to more expensive models when graph structure is important (loses advantage when more graph context is retained); less benefit as graph size grows compared to GNN; does not improve likelihood/perplexity metrics.",
            "uuid": "e8788.0",
            "source_info": {
                "paper_title": "WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "Nodes",
            "name_full": "Nodes-only bag-of-words per-node embeddings",
            "brief_description": "A representation that constructs a separate bag-of-words embedding for each node in the graph (ignoring edges), producing T = number of nodes embeddings that the Transformer-XL can attend to.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Nodes-only (per-node BoW) representation",
            "representation_description": "For each node in the KG, build a BoW vector from that node's textual attributes (string literals); project each node-BoW to an embedding producing one graph embedding per node. Edges are ignored; node embeddings are exposed as T embeddings to the Transformer-XL as additional tokens.",
            "graph_type": "Knowledge graphs (Freebase subgraphs with entity nodes and string-literal nodes)",
            "conversion_method": "For each node: tokenize and normalize its string-literal attributes; replace entity IDs/numbers with special tokens as needed; create BoW vector per node and linearly project to an embedding. Do not encode edges explicitly.",
            "downstream_task": "Graph → text generation (conditioning LM), graph → text retrieval, text → graph retrieval",
            "performance_metrics": "Generation: conditioned models using Nodes improved sample relevance (rBLEU) relative to BoW and unconditional (exact numbers in Table 3). Retrieval (graph→text) (Table 5): Recall@1=16.28, Recall@5=34.88, mAP=26.62 (percent). Text→graph retrieval (Table 6): Recall@1=93.02, Recall@5=100.00, mAP=96.51 (percent). Perplexity: no observed improvement over text-only LM.",
            "comparison_to_others": "Nodes representation outperforms single-vector BoW in generation relevance and retrieval metrics because it preserves node-level distinctions; however, a GNN that incorporates edges outperforms Nodes (GNN &gt; Nodes &gt; BoW) though differences are modest for generation.",
            "advantages": "Preserves per-node textual context (less information loss than BoW); straightforward to compute; allows Transformer to attend to individual node embeddings.",
            "disadvantages": "Ignores explicit edge/structural information which can be important for semantics; T scales with number of nodes (more memory/compute than BoW).",
            "failure_cases": "When graph structure (relations) matters, Nodes underperforms GNNs; heavy graph subsampling reduces its advantage; does not improve perplexity.",
            "uuid": "e8788.1",
            "source_info": {
                "paper_title": "WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "GNN",
            "name_full": "Graph Neural Network node embeddings",
            "brief_description": "A representation that embeds both node and edge textual attributes and then applies a graph neural network (message passing) to produce structure-aware node embeddings used as extra tokens for conditioning a Transformer-XL language model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Graph Neural Network (GNN) representation",
            "representation_description": "Embed BoW representations for nodes and edges, then run a message-passing GNN over the 1-hop subgraph to produce updated node embeddings (T = number of nodes). These node embeddings are exposed to Transformer-XL as additional attention key/value tokens shared across time steps.",
            "graph_type": "Knowledge graphs (Freebase subgraphs with entity and string-literal nodes, labeled edges)",
            "conversion_method": "1) For each node and edge, build a BoW token embedding from text attributes; 2) initialize node/edge features with these embeddings; 3) apply a GNN (message-passing layers) to propagate information across the 1-hop neighborhood, yielding final node embeddings; 4) present the T node embeddings to the Transformer-XL as graph tokens (no positional encodings).",
            "downstream_task": "Graph → text generation (conditioning LM), graph → text retrieval, text → graph retrieval",
            "performance_metrics": "Generation: GNN-conditioned models yield higher rBLEU than Nodes and BoW (exact rBLEU numbers in Table 3). Retrieval (graph→text) (Table 5): Recall@1=18.60, Recall@5=34.88, mAP=27.79 (percent). Text→graph retrieval (Table 6): Recall@1=100.00, Recall@5=100.00, mAP=100.00 (percent). Ablation: message passing (&gt;=1 layers) substantially improves rBLEU vs 0 layers; increasing layers beyond 1 yields diminishing returns. Perplexity: no improvement over text-only LM.",
            "comparison_to_others": "GNN &gt; Nodes &gt; BoW on relevance metrics (rBLEU) and retrieval; differences are consistent but described as relatively small for generation. GNN benefits more than BoW as graph size increases (Figure 6). When nodes are heavily subsampled, GNN's advantage disappears and it performs similarly to BoW.",
            "advantages": "Incorporates graph topology and edge information leading to better relevance and retrieval performance; benefits from increased graph context; message passing improves conditioning quality.",
            "disadvantages": "Higher computational and memory cost (T = number of nodes, plus GNN compute); modest gains in generative likelihood (perplexity not improved); benefit reduces when generating very long samples.",
            "failure_cases": "Advantage decreases for very long generated sample lengths (up to 4096 tokens) — noise overwhelms signal and GNN advantage shrinks; heavy node subsampling removes structural advantage; 2-hop neighborhoods were found to introduce irrelevant nodes via hubs, so 1-hop was used to avoid adding irrelevant information.",
            "uuid": "e8788.2",
            "source_info": {
                "paper_title": "WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "Graph-conditioned attention",
            "name_full": "Graph-conditioned Transformer-XL attention (graph-as-tokens)",
            "brief_description": "An adaptation of Transformer-XL where graph embeddings (from BoW, Nodes, or GNN) are appended as extra key/value \"tokens\" and the text queries attend to both text and graph keys, enabling the LM to condition on graph information.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Graph-conditioned attention (graph tokens appended to Transformer-XL)",
            "representation_description": "Compute text token queries Q_t and keys/values K_t,V_t from text; compute K_g,V_g from graph embeddings; compute attention scores between Q_t and both K_t and K_g, concatenate attention outputs ([A_t • A_g], [V_t • V_g]) and apply masked softmax as usual. Graph embeddings are static across time steps and no positional encodings are used for them.",
            "graph_type": "Knowledge graphs (Freebase subgraphs) — works with any method that produces a set of graph embeddings (BoW single embedding, per-node embeddings, or GNN node embeddings).",
            "conversion_method": "Graph -&gt; set of embedding vectors (T embeddings) via BoW/Nodes/GNN; include embeddings as extra non-sequential tokens in Transformer-XL attention key/value matrices so text tokens attend to graph keys.",
            "downstream_task": "Graph → text generation (long-paragraph LM conditioning), graph → text retrieval (ranking via per-token likelihood), text → graph retrieval (ranking graphs given text)",
            "performance_metrics": "Using this conditioning architecture, authors report: conditioned models steer generation to be more relevant (rBLEU improvements) though perplexity does not improve. Retrieval metrics (Tables 5 & 6) show strong improvements over unconditional model, with GNN + graph-conditioned attention producing best retrieval scores (graph→text mAP up to 27.79%; text→graph mAP up to 100%).",
            "comparison_to_others": "This attention-based conditioning is compatible with different graph encodings; relative performance depends on the graph encoder (GNN &gt; Nodes &gt; BoW for relevance/retrieval). Authors note simpler conditioning (BoW) is cheaper but less effective than GNN in exploiting structure.",
            "advantages": "Naturally integrates graph information into a strong LM (Transformer-XL) with minimal architectural changes; supports long-text generation via Transformer-XL's memory mechanism; flexible to different graph encoders.",
            "disadvantages": "Graph tokens have no positional encoding (non-sequential) which may limit some order-sensitive signals; adding many node embeddings increases attention cost and memory usage; still does not improve LM perplexity in experiments.",
            "failure_cases": "Does not increase perplexity even when conditioned on graphs (i.e., likelihood metric not improved); GNN-conditioned attention advantage diminishes for very long sample lengths; attention overhead grows with number of graph tokens.",
            "uuid": "e8788.3",
            "source_info": {
                "paper_title": "WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset",
                "publication_date_yy_mm": "2021-07"
            }
        },
        {
            "name_short": "Subgraph extraction pipeline",
            "name_full": "Automated Freebase subgraph extraction (mapping, expansion, filtering)",
            "brief_description": "An automated three-stage pipeline to pair each Wikipedia article with a relevant 1-hop Freebase subgraph: (1) map article title to Freebase entity via wikipedia.en triples, (2) expand to include 1-hop neighbor entities and their string attributes, (3) filter and deduplicate string attributes and cap graph size.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Subgraph extraction and textual canonicalization pipeline",
            "representation_description": "Procedure to convert Wikipedia articles into paired KGs: map article titles to Freebase entity nodes (via wikipedia.en relation); take 1-hop neighborhood (including edges between neighbors and string literal nodes); include string attributes text as node content; deduplicate multiple identical relation-&gt;string edges by choosing a canonical string using a character unigram model; filter entities with few attributes and restrict to top relation types; cap graph sizes at thresholds (256/512/1024 nodes).",
            "graph_type": "Large-scale knowledge graph (Freebase) producing subgraphs containing entity nodes and string-literal nodes",
            "conversion_method": "Mapping: string match article titles to Freebase wikipedia.en triples. Expansion: include 1-hop neighboring entities and their relations to string attributes; include inter-neighbor edges. Filtering: drop entities with &lt;4 string attributes at preprocessing; restrict to top 1024 relation types; deduplicate multiple string attributes per relation by selecting canonical string via unigram character model; cap final graph size.",
            "downstream_task": "Dataset creation for training/evaluating graph→text generation, graph→text retrieval, text→graph retrieval; provides textual content in string-literal nodes used by graph encoders.",
            "performance_metrics": "Dataset statistics: 23,522 graph-text pairs (82.3% of Wikitext-103); average graph: 38.7 nodes and 48.3 edges; average graph string-literal tokens = 895.1; average article length 3,533.8 tokens. Mapping success: 24,345/28,475 (85.5%) titles mapped to Freebase entities. These stats indicate scale but are not direct predictive-performance metrics.",
            "comparison_to_others": "Compared to prior graph-text datasets (e.g., WebNLG, GenWiki, Lebret et al.), WikiGraphs subgraphs are significantly larger (avg. ~38.7 nodes vs prior datasets with 10–20 or fewer nodes) and paired text is orders of magnitude longer (full Wikipedia articles vs single/few sentences).",
            "advantages": "Scalable and automatic (able to scale to millions of pairs); creates rich subgraphs with both structural and long textual content in string-literal nodes; filters reduce redundancy and noise (unigram canonicalization); 1-hop choice avoids noisy hubs from 2-hop expansion.",
            "disadvantages": "Automated alignment may miss mappings (17.7% of Wikitext-103 not paired); relies on Freebase coverage and quality; 2-hop neighborhoods were found to introduce irrelevant nodes via hubs, so limited to 1-hop which may miss some context; string-literal redundancy and noise require heuristic filtering.",
            "failure_cases": "2-hop expansion creates overly large and noisy subgraphs (irrelevant hub-induced entities) so the authors restricted to 1-hop; Freebase contains redundant/variant string attributes requiring canonicalization; mapping fails for a nontrivial subset of articles; graphs can be skewed (few high-degree nodes) presenting challenges to representation learning.",
            "uuid": "e8788.4",
            "source_info": {
                "paper_title": "WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset",
                "publication_date_yy_mm": "2021-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural text generation from structured data with application to the biography domain",
            "rating": 2,
            "sanitized_title": "neural_text_generation_from_structured_data_with_application_to_the_biography_domain"
        },
        {
            "paper_title": "The WebNLG challenge: Generating text from RDF data",
            "rating": 2,
            "sanitized_title": "the_webnlg_challenge_generating_text_from_rdf_data"
        },
        {
            "paper_title": "Text Generation from Knowledge Graphs with Graph Transformers",
            "rating": 2,
            "sanitized_title": "text_generation_from_knowledge_graphs_with_graph_transformers"
        },
        {
            "paper_title": "GenWiki: A dataset of 1.3 million content-sharing text and graphs for unsupervised graph-to-text generation",
            "rating": 2,
            "sanitized_title": "genwiki_a_dataset_of_13_million_contentsharing_text_and_graphs_for_unsupervised_graphtotext_generation"
        },
        {
            "paper_title": "Relational inductive biases, deep learning, and graph networks",
            "rating": 1,
            "sanitized_title": "relational_inductive_biases_deep_learning_and_graph_networks"
        }
    ],
    "cost": 0.014000499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>WikiGraphs: A Wikipedia Text -Knowledge Graph Paired Dataset
20 Jul 2021</p>
<p>Luyu Wang luyuwang@google.com 
Equal contribution DeepMind
LondonUK</p>
<p>Yujia Li yujiali@google.com 
Equal contribution DeepMind
LondonUK</p>
<p>Ozlem Aslan ozlema@google.com 
Equal contribution DeepMind
LondonUK</p>
<p>Oriol Vinyals vinyals@google.com 
Equal contribution DeepMind
LondonUK</p>
<p>WikiGraphs: A Wikipedia Text -Knowledge Graph Paired Dataset
20 Jul 2021189732E383B3275E749BD0F7801A92F5arXiv:2107.09556v1[cs.CL]
We present a new dataset of Wikipedia articles each paired with a knowledge graph, to facilitate the research in conditional text generation, graph generation and graph representation learning.Existing graph-text paired datasets typically contain small graphs and short text (1 or few sentences), thus limiting the capabilities of the models that can be learned on the data.Our new dataset WikiGraphs is collected by pairing each Wikipedia article from the established WikiText-103 benchmark(Merity et al., 2016)with a subgraph from the Freebase knowledge graph(Bollacker et al., 2008).This makes it easy to benchmark against other state-of-the-art text generative models that are capable of generating long paragraphs of coherent text.Both the graphs and the text data are of significantly larger scale compared to prior graph-text paired datasets.We present baseline graph neural network and transformer model results on our dataset for 3 tasks: graph → text generation, graph → text retrieval and text → graph retrieval.We show that better conditioning on the graph provides gains in generation and retrieval quality but there is still large room for improvement. 1</p>
<p>Introduction</p>
<p>Parallel datasets that pair data from different sources and modalities have enabled large amounts of research on cross modality learning.Paired image-caption datasets enable models to describe visual scenes in natural language (Lin et al., 2014;Vinyals et al., 2016), paired streams of speech and transcription data makes it possible to train speech recognition systems (Garofolo et al., 1993;Panayotov et al., 2015) or text-to-speech synthesis models (Oord et al., 2016), and parallel corpus of text in different languages enable learned machine translation models (Barrault et al., 2020).We present a new dataset of Wikipedia text articles each paired with a relevant knowledge graph (KG), which enables building models that can generate long text conditioned on a graph structured overview of relevant topics, and also models that extract or generate graphs from a text description.</p>
<p>There has been many prior efforts trying to build datasets for learning graph → text generation models (Jin et al., 2020;Gardent et al., 2017;Lebret et al., 2016).However, existing graph-text paired datasets are mostly small scale, where the graphs tend to have 10-20 or even less nodes, and the text typically only contains one or a few sentences.This represents a significant contrast with the state-ofthe-art text generation models (Dai et al., 2019;Brown et al., 2020), which can already generate very fluent and long text that spans thousands of tokens over multiple paragraphs.</p>
<p>We attempt to bridge this gap, with the goal of advancing the state-of-the-art graph → text generation models, graph representation learning models and also text-conditioned graph generative models.Each text document in our dataset is a full-length Wikipedia article, and we pair each of them with a KG that are significantly bigger than prior datasets of similar nature and includes much richer information.Hand labelling text articles with KGs is expensive and not scalable (Jin et al., 2020), there-fore we utilize an existing and established knowledge base, Freebase (Bollacker et al., 2008), and designed an automated process to extract a relevant subgraph from it for each Wikipedia article.To make the text generation results on our dataset directly comparable to the state-of-the-art, we chose the set of Wikipedia articles from the established language modeling benchmark WikiText-103 (Merity et al., 2016), which contains a subset of highquality Wikipedia articles.This gives us a dataset of 23,522 graph-text pairs in total, covering 82.3% of Wikitext-103 articles.On average each graph has 38.7 nodes and 48.3 edges, and each text article contains 3,533.8tokens.In addition to structural information, our graphs also contain rich text information with an average of 895.1 tokens in each graph.Furthermore, the automatic process we used to create this dataset can be extended to pair any Wikipedia document with Freebase, and can be scaled up to create over 3M graph-text pairs.</p>
<p>Out of many exciting new tasks that this dataset enables, we present 3 possibilities: graph → text generation, graph → text retrieval, and text → graph retrieval.We benchmarked a few baseline models on these tasks.The models we considered were based on the recent Transformer-XL (Dai et al., 2019) model, and we adapted it to condition the text generation on the KG in different ways.Our results show that better conditioning on the graph indeed improves the relevance of the generated text and the retrieval quality.However, there is still significant room for improvement on these tasks, which makes this an exciting dataset for research.Our data and code for baseline models will be made publicly available.</p>
<p>Related work</p>
<p>Graph-text paired data There has been a lot of prior work on creating graph-text paired datasets.Example applications include generating text summaries conditioned on Abstract Meaning Representation graphs (Liu et al., 2018), generating the abstract of a scientific article given a KG and title (Koncel-Kedziorski et al., 2019) and generating text from RDF triples (Gardent et al., 2017;Jin et al., 2020).In the following we will mostly review related work on KG -text paired datasets.</p>
<p>Annotating KG or text to create paired datasets is expensive, as a good quality annotation requires annotators that understand the content and structure of the text and the corresponding KG (Jin et  2020).Therefore previous KG-text paired datasets that rely on human annotation have limited scale.Among these, Gardent et al. (2017) crowdsourced human annotators to verbalize RDF triplets taken from DBpedia (Auer et al., 2007) to a few sentences (WebNLG) and this caused errors in annotation that were fixed with a few updates through years.Parikh et al. (2020) paired Wikipedia Table with one sentence text that is created by annotators that revise Wikipedia text.</p>
<p>Another line of research focuses on eliminating the need of human annotations by automatically matching KG-text pairs or generating KGs from text using existing tools.Lebret et al. (2016) automatically matched Wikipedia infobox of biographies with their first sentence.Koncel-Kedziorski et al. (2019) utilized an earlier information extraction system that extracts entities, co-reference and relations from given text to build KG's.The Gen-Wiki dataset (Jin et al., 2020) is automatically constructed by querying KGs in DBpedia with the title of articles in Wikipedia followed by filtering and entity annotation.</p>
<p>We construct our WikiGraphs dataset by extracting a subgraph from Freebase (Bollacker et al., 2008) for each Wikipedia article following a scalable automatic process.Compared to previous work, our WikiGraphs dataset contains significantly larger graphs and longer text (Table 1).</p>
<p>Models for graph-text paired data Recent state of art language models are based on the Transformer architecture (Vaswani et al., 2017) that uses the self attention mechanism.The Transformer-XL (Dai et al., 2019) model further introduces a segment level recurrence with a novel positional encoding resulting in impressive performance in long sequences by capturing dependencies beyond a fixed length window.</p>
<p>Graph neural networks (GNNs) (Battaglia et al., 2018;Gilmer et al., 2017) learn representations for graph structured data through a message passing process.This class of models naturally exploit   et al., 2019;Xu et al., 2019).Fundamentally, transformers can also be understood as a special type of GNNs with a fully-connected graph structure.</p>
<p>The most recent prior work on graph-to-text generation follows an encoder-decoder architecture (Koncel-Kedziorski et al., 2019;Jin et al., 2020), where the graph part is encoded with a GNN model, e.g.Graph Attention Network (GAT) (Veličković et al., 2018).The text part is typically modeled using an attention based decoder with a copy mechanism (e.g.BiLSTMs as in (Jin et al., 2020)) to process input from both the KG and text.</p>
<p>The models we benchmarked for graph-to-text generation were based on the Transformer-XL architecture and conditioned on the graph through a GNN, making full use of the graph structure and capable of generating very long text comparable to the state-of-the-art.</p>
<p>Dataset</p>
<p>In this section we first present some properties of our dataset, and then describe the process that we used to create it.</p>
<p>Properties of the data</p>
<p>Scale of the data</p>
<p>Basic statistics about our WikiGraphs dataset are listed in Table 2.An illustration of a graph-text pair is shown in Figure 1.A few actual examples from our dataset are included in the Appendix (Figure 7,8).All of the articles come from the WikiText-103 dataset (Merity et al., 2016), which contains highquality articles that fit the Good or Featured criteria specified by the Wikipedia editors when the data was collected.Merity et al. (2016) have already cleaned up and tokenized the articles, therefore they appear as plain text without any markup tags.</p>
<p>As will be described in Section 3.2, we try to pair each article with a subgraph from Freebase, centered at the entity node that has a Wikipedia link to the title of the article.We are not able to match every article to an entity in Freebase, but through this process we retained a significant portion of 82.3% of the WikiText-103 articles.We kept the original train/valid/test split.As we will see in Section 4.2, training models on this set gives us results that are very close to training on the full WikiText-103 dataset when evaluated on our test set.Therefore the text part of WikiGraphs appears to be sufficient to reproduce and benchmark against the state-of-the-art text generative models.</p>
<p>Figure 2 shows the distribution of graph sizes and article lengths across our dataset.All the distributions are skewed with a long tail.Notably, average graph size in our dataset is 38.7 nodes and 48.3 edges, considerably larger than the graphs in previous datasets (Jin et al., 2020;Gardent et al., 2017).Also the length of the text articles averages to 3,533.8 tokens and can go up to 26,994 tokens, which is orders of magnitudes longer than the text data in previous graph-text paired datasets that typically only contains a single or few sentences (Jin et al., 2020;Gardent et al., 2017;Lebret et al., 2016).</p>
<p>Nodes and edges</p>
<p>The graphs in our dataset contains two types of nodes: entities and string literals.Each entity is labeled by a unique Freebase entity ID, e.g.ns/m.0f9q9z, and each string literal contains some natural language text, that could be for example a name, date, or description of an entity.Each edge in the graphs also has an associated edge label, e.g.ns/common.topic.description,indicating which type of edge it is.There are a total of 522 different edge types in our dataset.Figure 3 shows the frequency of all the different edge types in our dataset.</p>
<p>Every graph always has one entity node (we call it "center node") that has a link to the paired Wikipedia article, through a special edge key/wikipedia.en, and the whole graph is a 1-hop neighborhood of entities around the center node within the bigger Freebase KG, plus the string literals associated with all the entities included.Note that it is possible to have edges between the 1-hop neighbors of the center node, therefore the graphs typically are not star structured.Section 3.2   provides more details about how these graphs are constructed and any additional filtering we did.</p>
<p>One special characteristic about our graph data is that the natural language text contained in the string literal nodes can sometimes be quite long (see e.g. Figure 7,8), and therefore provide much richer information not included in the graph structure itself.On average, each graph contains 895.1 tokens across all the string literal nodes in one graph (Table 2, Figure 2, "Tokens per graph").</p>
<p>Figure 4 shows the distribution of per-graph number of entity nodes and string literal nodes in our dataset.We can see that our graphs tend to have more string literal nodes than entity nodes, indicating that the entities are supplemented with the rich information in the string literals.</p>
<p>The distribution of information is not uniform across the nodes in a graph.Figure 5 shows that most entity nodes in our graph has a small degree, while few nodes have much larger degrees.Also most string literal nodes contain short text, while fewer nodes contain longer text.</p>
<p>The skewed distribution of nodes and edges in our dataset reflect the nature of KG's like Freebase, and presents new challenges to graph representation learning models.</p>
<p>The dataset construction process</p>
<p>We follow three principles when designing the dataset construction process:</p>
<p>1.The text part of the data should be directly comparable in complexity to the capability of state-of-the-art text generative models.</p>
<ol>
<li>
<p>The graph part of the data should be constructed in an automatic and scalable way.</p>
</li>
<li>
<p>The graph part of the data should be relevant for the paired text data.</p>
</li>
</ol>
<p>Note that our process is general, and can be applied to any set of Wikipedia articles.We have tried to pair a full dump of English Wikipedia with Freebase and managed to get over 3 million graphtext pairs.Here we restrict the process to the set of articles from the WikiText-103 dataset.</p>
<p>We try to map each Wikipedia article to a relevant subgraph of the existing large scale KG Freebase (Bollacker et al., 2008).We used the last public dump of Freebase2 , which contains 1.9B triples and a total of 250GB of data.We filtered the data by keeping only the entities with at least 4 string attributes (otherwise the entities are less interpretable), and keeping only the top 1024 most frequent relation types and restricting the relations to  only those among the retained entities and between the entities and string attributes.We also simplified the entity and relation names by stripping off the irrelevant "http://rdf.freebase.com/"and further removed duplicates.This gives us a significantly cleaner and smaller backbone graph for Freebase, with about 20M nodes.</p>
<p>Finding the relevant subgraph for an article in such a cleaned up but still large KG remains nontrivial.Our process for this contains 3 stages: mapping, expansion, and filtering.</p>
<p>Mapping</p>
<p>In the first stage of the process, we map each article into an entity in our processed Freebase KG.This is made possible through triples from Freebase like the following: ns/g.11b6jbqpt4key/wikipedia.en"Madunnella" where ns/g.11b6jbqpt4refers to an entity in the KG, key/wikipedia.en is the type of the edge, which indicates that this entity is linked to a Wikipedia article and "Madunnella" is the title of that article.We normalize the title string (and in general any string literals) from Freebase by replacing "_" with white space and handle unicode characters properly.We extract the titles from the Wikipedia article through string matching, where titles are enclosed in a "= [title] =" pattern.</p>
<p>In this step we managed to map 24,345 out of 28,475 (85.5 %) article titles from WikiText-103 to an entity in our KG.</p>
<p>Expansion We treat each of the mapped entities as the center node of a subgraph, and expand 1 hop out in the entire filtered Freebase graph to include all the neighboring entities that are the most relevant to the center entity.We then expand further from this 1-hop graph out to include all the relations that connect the selected entities to string attributes as well as between these entities themselves.Note that because of these edges between the 1-hop neighbor entities the graphs are typically not star structured.This gives us a relevant but compact graph for each article.We have also investigated the possibility of a 2-hop neighborhood from the center node, and found that 2-hop neighborhoods are significantly larger than 1-hop and through some "hub" nodes like "Male" or "Female" a 2-hop neighborhood from an entity can easily include many other irrelevant entities.Based on such observations we decided to use the 1-hop neighborhood to keep the relevance of the subgraph high.</p>
<p>Filtering The last stage of the process involves more filtering and cleaning up of the data.We noticed that in Freebase it is common for one entity to have multiple relations of the same type pointing to different string attributes, like the following: ns/m.07c72key/wikipedia.en"The SImpsons" ns/m.07c72key/wikipedia.en"The Simpson" ns/m.07c72key/wikipedia.en"The simsons" ns/m.07c72key/wikipedia.en"Thr Simpsons" ns/m.07c72key/wikipedia.en"The Simpson's" It is clear that there is a lot of redundancy in this data.We reduced all such edges (from the same entity with the same edge type to string attributes) to a single edge by picking the most "canonical" one.This was done by fitting a unigram model to the characters in the collection of strings and using that model to pick the most likely string.</p>
<p>We also filtered the graphs based on size and created three versions of the data with maximum graph size capped at 256, 512, and 1024 nodes, respectively.All the statistics and results in the rest of the paper are based on graphs with a maximum size of 256, but all versions of the data are made available online.</p>
<p>Experiments</p>
<p>We perform a set of experiments to showcase how the text and graph information can be combined in a language model.Specifically, we consider three tasks: text generation conditioned on the graph, graph retrieval given the text, and text retrieval given the graph.</p>
<p>Graph-conditioned Transformer-XL</p>
<p>In order to incorporate graph information into an advanced language model, we adapt the recent Transformer-XL model (Dai et al., 2019) to also attend to the graph features.At a high-level our model embeds the graph into a set of embedding vectors, and then exposes these embeddings to the Transformer-XL model as extra "token" embeddings to condition on.The size of this set depends on the graph model we choose.</p>
<p>Given the features for T text tokens H t ∈ R T ×d and features for T graph "tokens" H g ∈ R T ×d , we illustrate the graph-conditioned attention procedure with a single head as follows:
Q t , K t , V t = H t W t q , H t W t k , H t W t v K g , V g = H g W g k , H g W g v A t , A g = Q t K t , Q t K g A, V = [A t • A g ], [V t • V g ] O = Masked-Softmax(A)V
where [a • b] stands for concatenation on the sequence dimension and thus A ∈ R T ×(T +T ) and V ∈ R (T +T )×d h , where d h is the head dimension.In other words, comparing to the original Transformer-XL, our model also computes the attention scores between the text queries Q t and both the text keys K t and the graph keys K g .As a result, the attention outputs contain information from both the graph and the text context.Note that this formulation is compatible with an additional memory (Dai et al., 2019) with minimal changes, as it simply adds in an extra set of "tokens" for the model to attend to.We don't use position encodings for the graph "tokens" as there is no sequential ordering for them.</p>
<p>In this work we consider three different approaches for encoding the graph structure:</p>
<p>• Bag-of-words (BoW): we construct a single bag-of-words representation of all the tokens from both the nodes and edges in the graph.Entity IDs and numeric values in the graph are replaced with special tokens <entity> and <number>.The BoW vector is further projected using a linear layer to a latent space.</p>
<p>In this case T = 1.</p>
<p>• Nodes only (Nodes): we construct separate BoW representations for each node and project each to an embedding and ignore the edges.In this case T is equal to the number of nodes in the graph.</p>
<p>• Graph neural network (GNN): we embed BoW representations for both nodes and edges and then use a graph neural network (Battaglia et al., 2018) on top of those embeddings to compute a new set of node embeddings.T is equal to the number of nodes.</p>
<p>The T graph embeddings from this process are shared across all the time steps for text tokens.This model can be further improved, e.g. by using word embeddings and text summarization techniques, but we leave these for future work.</p>
<p>Implementation details</p>
<p>We reimplement the Transformer-XL model in Jax (Bradbury et al., 2018).In our experiments, we employ the base model in (Dai et al., 2019), except that we increase the tail shrinkage factor used for the adaptive softmax and input representations from 1 to 4, which saves 63% of the parameters without compromising the performance.On the full Wikitext-103 dataset, our implementation has a test perplexity of 24.2 (published result for this base model was 24.0).We train our models using the standard likelihood objective for language models with a total batch size of 64 on 8 V100 GPUs.</p>
<p>Graph → text generation</p>
<p>Our first task is text generation conditioned on the graph.We evaluate model performance by (1) computing model perplexity on held-out text and (2) drawing samples from the model and comparing that to the ground truth text article.We use BLEU score (Papineni et al., 2002) to measure the similarity of our generated samples to the ground truth.Unlike previous use cases for BLEU score where there are many references for one generated sample, here we have only one ground truth reference but we can generate multiple samples.We therefore simply swapped the reference with the samples when computing the score, which we term as the reverse-BLEU (rBLEU).We have also tried other ways of computing the BLEU score and find that they don't change how models compare against each other.</p>
<p>Unless explicitly stated, we let the model sample with a memory size of 640, and condition on the graphs in the test set to generate text for up to 512 tokens per sample for a total of 20 samples per graph.The rBLEU score is computed based on these samples and corresponding ground-truth texts are truncated to the same length.We sample the texts from the distribution with a temperature of 0.8.For each case, we report the average rBLEU score of 3 sampling runs.We find the variances are insignificant which do not affect the comparison results.In Appendix A.3 we also report results for generating longer samples for up to 4096 tokens.</p>
<p>Main result</p>
<p>In Table 3, we show the perplexity and the rBLEU score of the unconditional, BoW, nodes-only, and GNN conditioned models.As a reference, a standard Transformer-XL model trained on the full Wikitext-103 training set reaches 25.08 perplexity on our test set, which contains 71.7% of the original test articles.We can see that the unconditional, i.e. text only, model trained on our dataset gets a very similar performance as trained on the full set.This is strong evidence that our dataset can be a good benchmark for state-of-the-art text generative models.</p>
<p>We also see that conditioned on the graphs, model perplexity didn't improve, but the relevance  of the samples measured by the BLEU scores did improve significantly.This indicates that the graph conditioned models can indeed steer the language model towards more relevant topics, but this so far cannot yet improve likelihood metrics.</p>
<p>To make the evaluation more fair to the text-only model, we also tried to prompt the generation with the title of the article, such that the text-only model also has some context.In this setting the graph models are still better, showing the importance of modeling the structure.</p>
<p>Lastly, among all the 3 graph model variants, we observe that using a set of embeddings from the nodes model is better than using a single embedding from the BoW model, and fully utilizing the graph structure through the GNN model is consistently better than ignoring the edges as in the nodes model.However the differences among the methods are relatively small.For visualizations of a few graphs in our dataset and the corresponding samples generated based on them please refer to Appendix A.</p>
<p>Ablation studies</p>
<p>We show a few ablations on the graph model and sampling parameters, to provide some insights into the models.Table 4 shows the effect of varying the number of message passing layers in the GNN.We can observe that there is a big difference between using message passing ( ≥ 1 layers) or not (0 layers) in terms of rBLEU score, but increasing the number of message passing layers does not change the results significantly.We believe however, that these results can be improved by employing bigger and more powerful graph representation learning models, and potentially use initial node and edge representations better than bag-of-words.</p>
<p>In Figure 6 we show the effect of the graph size on model performance.In this experiment we subsample the nodes in each graph to control for the amount of context the model has access to.It is clear from the results that when we heavily subsample and keep only a small portion of the graphs, the GNN model performs similarly as the simpler BoW model, but GNNs benefit more as we keep more of the graph structure.</p>
<p>Graph → text retrieval</p>
<p>In this task, we evaluate the possibility of retrieving relevant text for a given query graph.We pair all articles with all graphs in the test set, resulting in 43×43=1849 pairs.Then the trained graphconditioned language models are used to produce the per-token likelihood of each pair, and we use these likelihood scores to rank the text articles for each graph.We expect the learned models can rank the correct pairs higher than wrong ones.To measure the results we use standard ranking metrics including recall@K, which computes the fraction of times the correct pair is included in the top K predictions, as well as mean average precision (mAP).In Table 5</p>
<p>Text → graph retrieval</p>
<p>In this last task, we evaluate the performance of graph retrieval given a text query.We use exactly the same setting and scores as Section 4.3, but instead rank the graphs for each text article using the likelihood scores.The results are shown in Table 6.Note that this task is quite easy with our data and setup, potentially because the graphs are much more distinguishable than the text articles.</p>
<p>All the graph-conditioned models perform almost perfectly, with the GNN model again outperforming the others.</p>
<p>Conclusion</p>
<p>In this paper, we present WikiGraphs, a new graphtext paired dataset with significantly larger graphs and longer text compared to previous datasets of similar nature.We show that the text part of this data is a good benchmark for state-of-the-art text generation models, and the paired dataset can help us benchmark models that are capable of generating long and coherent text conditioned on a graph structure.</p>
<p>In the first set of experiments on this dataset we showcase 3 different tasks using our dataset, and demonstrate the benefit of better models that make more use of the graph structure.</p>
<p>There is still significant room for improvement for these tasks on our dataset, and we hope the release of the data and baseline code can help spur more interest in developing models that can generate long text conditioned on graphs, and generate graphs given text, which is another exciting direction our dataset enables but we did not explore, and eventually bridging the graph and text modalities.</p>
<p>A Appendix</p>
<p>A.1 Graph visualization Some example visualizations of the KG structures are shown in Figure 7 and Figure 8.The corresponding graph truth texts are shown in Table 7.</p>
<p>A.2 Generated examples</p>
<p>The generated texts based on the graph shown in Figure 7 and Figure 8 are listed in Table 8 and  Table 9, respectively.</p>
<p>A.3 Ablations on sampling configurations</p>
<p>We show additional ablation results on the sample length (Table 10) and the temperature (Table 11) for greedy sampling.Note that for each case we show the rBLEU score based on the validation set computed with a single sampling run (20 samples per graph).</p>
<p>Note that the GNN model has overall the best performance.However as the sample length increases the advantage of the GNN model also decreases.This indicates that it is still very challenging to generate long text that stays on-topic, and potentially the noise overwhelms the signal when number of tokens increases to 4096.ns/m.02h40lc"Inglés" key/wikipedia.en"/en/english" ns/type.object.id"English" ns/common.topic.alias"English is a West Germanic language that was first spoken in early medieval England and is now a global lingua franca.It is an official language of almost 60 sovereign states, the most commonly spoken language in the United Kingdom, the United States, Canada, Australia, Ireland, and New Zealand, and a widely spoken language in countries in the Caribbean, Africa, and South Asia.ns/type.object.idns/freebase.objecthints.besthrid "A \"protected site\" is any location that is protected under law, usually by being designated as a park, preserve, monument, etc., and which are usually under the control (at least in part) by some form of government agency.This most often will apply to areas of land or water, but may also apply to human-made structures.This type is distinct from \"listed sites\", which have been designated as significant, but may not have any legal protection thereby.However, many such places will be both, .</p>
<p>Visualization</p>
<p>Ground Truth Text Figure 7 = Where the Streets Have No Name = " Where the Streets Have No Name " is a song by Irish rock band U2 .It is the opening track from their 1987 album The Joshua Tree and was released as the album 's third single in August 1987 .The song 's hook is a repeating guitar arpeggio using a delay effect , played during the song 's introduction and again at the end .Lead vocalist Bono wrote the lyrics in response to the notion that it is possible to identify a person 's religion and income based on the street on which they lived , particularly in Belfast .During the band 's difficulties recording the song , producer Brian Eno considered erasing the song 's tapes to have them start from scratch ." Where the Streets Have No Name " was praised by critics and became a commercial success , peaking at number thirteen in the US , number fourteen in Canada , number ten in the Netherlands , and number four in the United Kingdom .The song has remained a staple of their live act since the song debuted in 1987 on The Joshua Tree Tour .The song was performed on a Los Angeles rooftop for the filming of its music video , which won a Grammy Award for Best Performance Music Video .= = Writing and recording = = The music for " Where the Streets Have No Name " originated from a demo that guitarist The Edge composed the night before the group resumed The Joshua Tree sessions .In an upstairs room at Melbeach House -his newly purchased home -The Edge used a four @-@ track machine to record an arrangement of keyboards , bass , guitar , and a drum machine .Realising that the album sessions were approaching the end and that the band were short on exceptional live songs , The Edge wanted to " conjure up the ultimate U2 live @-@ song " , so he imagined what he would like to hear at a future U2 show if he were a fan .After finishing the rough mix , he felt he had come up with " the most amazing guitar part and song of [ his ] life " .With no one in the house to share the demo with , The Edge recalls dancing around and punching the air in celebration .Although the band liked the demo , it was difficult for them to record the song .Bassist Adam Clayton said , " At the time it sounded like a foreign language , whereas now we understand how it works " .The arrangement , with two time signature shifts and frequent chord changes , was rehearsed many times , but the group struggled to get a performance they liked .According to co @-@ producer Daniel Lanois , " that was the science project song .</p>
<p>Figure 8 = Fort Scott National Historic Site = Fort Scott National Historic Site is a historical area under the control of the United States National Park Service in Bourbon County , Kansas , United States .Named after General Winfield Scott , who achieved renown during the Mexican @-@ American War , during the middle of the 19th century the fort served as a military base for US Army action in what was the edge of settlement in 1850 .For the next quarter century , it was used as a supply base and to provide security in turbulent areas during the opening of the West to settlement , a period which included Bleeding Kansas and the American Civil War .The current national historic site protects 20 historic structures , a parade ground , and five acres ( 20 @,@ 000 m 2 ) of restored <unk> prairie , inside the city of Fort Scott .It is open to visitors most days of the year .= = History = = In 1842 , Fort Scott was named after Winfield Scott , was established on the American frontier on the military road in eastern Kansas between Fort Leavenworth and Fort Gibson .It was established to provide protection to the rapidly increasing number of settlers , who were migrating from the Eastern United States .Fort Scott became one of a chain of forts intended to protect the new settlers from the Plains Indians , as well as to protect the Indians from the settlers ' encroachment .The United States government intention to reserve permanent Indian lands west of the Missouri River gave way to the competition of settlers continuing to encroach on the Indian settlements .Fort Scott 's most active days were between 1842 and 1853 , although it was also used during the Civil War .= = = Army days = = = The Cherokee of Indian Territory ( now Oklahoma ) were upset to have Fort Wayne in their proximity .After some delay , the US Army decided to abandon Fort Wayne and move its soldiers to a new fort to be built between Fort Leavenworth and the site .The Army both wanted to placate the Cherokee ( who were supervised by the Department of Defense ) and provide more defense for white settlers and other Indians against the Osage , who had been conducting frequent raids in the area .On April 1 , 1842 , some soldiers of Fort Wayne left their fort and on April 22 arrived where Fort Scott would be built , in the Osage <unk> section of modern @-@ day Kansas .After haggling with the Cherokees to acquire the land , the rest of Fort Wayne 's garrison left the fort on May 26 and arrived at the Fort Scott site on May 30 .Unlike most forts for military use , the fort did not have defensive walls or structures Cond.</p>
<p>Generated Text -</p>
<p>= Final Fantasy Adventure = Final Fantasy Adventure is a platform video game developed by Square ( now Square Enix ) and published by Square ( then Square Enix ) for the Super Nintendo Entertainment System , and published by Nintendo for the Nintendo 64 and the Nintendo 64 in North America and Europe .It was released in North America on August 10 , 1995 for the Nintendo 64 and in Europe in March 1995 for the Wii .It was also re @-@ released for the Wii in Japan on March 25 , 2015 for the Wii U 's Virtual Console service .Final Fantasy Adventure is set in the fantasy world of the fictional land of Ooo , between 1000 and 1000 years before the events of the story of the kingdom of Ooo .The story follows the protagonists , Cid , a boy who is sent to Ooo the legendary Jörmungandr to find the Wizard of Ooo .Throughout the game , players train for a series of random battles , which are played out over a single screen .In the end , players control a " Scarlet Witch " who is summoned to the realm of Ooo .The game was well @-@ received , with praise particularly directed at its graphics and its gameplay , despite criticism for its repetitive gameplay .Critics generally praised its graphics , plot , and simplistic gameplay .The game was commercially successful , selling over 20 @,@ 000 copies , and in 2004 it was re @-@ released in Japan as part of the Final Fantasy Collections series .= = Gameplay = = The game is a third @-@ person shooter , with gameplay elements related to worlds , time travel , and exploration .Characters are made up of two polygons ; the main character , Aya , is a sprite @-@ based character and inhabits a world of trees and caves , where she is able to perform offensive actions along the way .The protagonist , Aya , is a heavily armored , humanoid creature with red eyes and black eyes .In contrast to other Final Fantasy games , her prominent appearances in this game are not as straightforward as in previous Final Fantasy installments .The game 's world is divided into two sections : the world of Ooo and the world of the dinosaurs .The player 's adventure begins with Aya , a girl who initially leads the plot of Twilight Town , but later returns to Ooo to save her sister .Aya explores the area and is able to save her family .After interacting with the player , Aya is able to resume the story in the future .The game 's main family member , Aya , is a woman named Alice .She is a member of the religious order , the Jehovah 's Witnesses , who is a member of BoW = Zoo Station ( song ) = " Zoo Station " is a song by Australian rock band U2 from their second studio album , PopMart , released in 1992 as the album 's lead single .Rise to popularity after its release prompted U2 to record two songs with bassist Adam Clayton and guitarist Joe Bono , as they felt that it was their breakthrough album .The song comprises lyrics set in an acoustic , with lyrics that have always focused on a woman being stuck in an abusive relationship .It was developed due to the band 's involvement in the construction of Zoo Station and its accompanying soundtrack , and it was inspired by U2 's admiration of the groups they both represented .The song was inspired by a series of incidents that occurred in El Salvador when a man repeatedly attacked an apartment building .The song received positive reviews from music critics , and it was labelled a " masterpiece " by several publications .Thirty @-@ five tracks were released as singles , and the song has been covered by numerous artists , including such figures as Sam &amp; Max , The <unk> , and Mickey Rourke .The band released their version of " Zoo Station " on non @-@ consecutive studio albums , with " Where the Streets Have No Name " and " Numb " released in 1999 and 2000 respectively ." Zoo Station " is on the band 's greatest hits album , 10 Mile Wild , and on their greatest hits album Across the Universe .It was performed live at the Vertigo Tour in 2007 .The song was included on the band 's fifth studio album , New Order ( 2008) , and was included on the film The Last Years ( 2012 ) .= = Background = = " Zoo Station " was written by U2 frontman Bono and produced by U2 and was the first song on the album .Clayton still had a strong fan base and was a regular singer .The two met while performing in the amphitheaters of Los Angeles , and in a live performance he was the lead singer on the last leg of the Vertigo Tour .The Edge and Clayton were both fans of the band , and the pair decided to collaborate on the album .Both performed on their 2004 tour , and made a solo appearance on the 2004 The Zoo TV Tour .Clayton and Clayton had been close friends , and the pair became friends again in 2008 .In late April 2004 , U2 announced that the song had been released as the first single for the album , and would be released on 31 May , five weeks after the album 's release ." Zoo Station " was released as the fourth single from PopMart and premiered on 13 June 2005 .The song is a Nodes = You Can 't Take This Away ( U2 song ) = " You Can 't Take This Away ( U2 song ) " is a song by the Irish rock band U2 .It was written by vocalist Bono and produced by The Smiths for their third solo album , The Joshua Tree .Inspired by Romania roots rock and roll , " You Can 't Take This Away " is a song about a man who suffers from severe nightmares .The song was first released on the album and premiered on radio on 19 August 1996 , where it reached number 40 on the Billboard Hot 100 ." You Can 't Take This Away ( U2 song ) " received mixed reviews from critics ; some tracks , such as the opening chorus and bridge , were praised as highlights by some reviewers .The song was well received by critics , as the record company 's highest @-@ selling single at that time , where " The Edge of Forever " peaked at number 64 .It was later re @-@ released on the band 's 2006 compilation album No Line on the Horizon , but has since been re @-@ released on live performances in 2006 and 2009 ." You Can 't Take This Away ( U2 song ) " was performed on the Late Show with David Letterman on 31 December 2005 ." You Can 't Take This Away ( U2 song ) " has since been covered by many groups and has achieved enormous commercial success .A video for the song was filmed by then @-@ frontman Bono , for which it was nominated for a Grammy Award .= = Background and writing = = " You Can 't Take This Away ( Kingdom of Ireland song ) " is a track that features Bono and The Smiths discussing their relationship and how they changed their lives .His father , Jack Clayton , was assassinated in 1981 at the age of 23 .Bono was born in Philadelphia and worked for a business , first working as a secretary in Los Angeles , then as a photographer for a commercial for Primus .He later worked for the Coca @-@ Cola Company as a drummer in the music industry .The musician picked up the song again after Nikolai <unk> , an engineer who worked with The Smiths , heard it and decided to play it for him after U2 agreed to record it for a solo album .The band originally intended to release " You Can 't Take This Away " as a single ; however , with a critical failure , the song eventually became a single .In 2004 , " You Can 't Take This Away ( U2 song ) " is one of two songs on the album that U2 released as a single with approval of the record label .The [ ] GNN = The Edge of Glory = " The Edge of Glory " is a song by Irish rock band U2 , released as a single .It was written by guitarist Larry Mullen , Jr ., who plays lead guitar on the song , and was produced by Alex <unk> , who described the song as " a track with a lot of meaning , but no connection ." The song contains several pop rock elements and is set in that time period , and is among the most prominent in the album .In addition to its lyrics , the song 's lyrics detail hypocrisy , and also deals with the effects of adultery .The song 's lyrics have been described by music critics as being autobiographical .The lyrics have been described as " a bold exploration of the figure of a New York City man " , and " an expression of the inability of freedom to live in a world that is also a place in the world of space ." The song 's lyrics describe a " Manhattan @-@ like place " , with Bono calling the arrival a " pleasant little optimism from before it came to life ." " The Edge of Glory " was a success in the United Kingdom , reaching number two in the charts in the United States , and topping the charts in Australia and New Zealand .The song has been certified platinum by the Recording Industry Association of America , and has sold over four million copies worldwide .The song has been covered by several artists , including German band U2 .The music video for " The Edge of Glory " won Best Video at the 2004 MTV Video Music Awards .The video also served as an inspiration for the film U2 360 • ( 1998 ) .= = Background = = The song has been described as a " relaxed representation " of globalization , with Bono proclaiming himself the " lost king of rock ' n ' roll " , and Chris McGuinness as " the only one who has ever achieved the sound of a rock ' n ' roll ." Bono 's lyrics have been described as a parody of Lord Byron 's " My Own Time " , and as an " attack on social and political issues " .= = Recording and production = = Bono and U2 made the final stages of recording the song at the Abbey Road Studios in Dublin , Dublin .The sessions were divided into two sessions : Sessions at Damien the flautist and Context at the Cave of Christ .The results of the sessions were mixed by Brian Eno .U2 was very excited to record the result , with Eno referring to the recording as a " special event " , and they decided to change the track</p>
<p>Figure 1 :
1
Figure 1: Illustration of a pair of Wikipedia article and the corresponding knowledge graph in our dataset.</p>
<p>Figure 2 :
2
Figure 2: Distribution of graph and article sizes across our WikiGraphs dataset.</p>
<p>Figure 3 :Figure 4 :
34
Figure 3: Edge type distribution roughly follows an inverse exponential law.</p>
<p>Center node degree dist.(b) Non-center node degree dist.(c) String literal node length dist.</p>
<p>Figure 5 :
5
Figure 5: Node degree distribution for entity nodes and token count distribution for string literal nodes.</p>
<p>Figure 6 :
6
Figure 6: Performance vs size of graph to condition on.The model is trained with a smaller version of the data by subsampling the number of nodes.</p>
<p>Figure 7 :
7
Figure 7: Visualization of the "Where the Streets Have No Name" KG in our dataset.</p>
<p>Figure 8 :
8
Figure 8: Visualization of the "Fort Scott National Historic Site" KG in our dataset.</p>
<p>Table 1 :
1
Our dataset contains significantly larger graphs (average #triples per graph) and longer text (average #tokens per text) than previous KG-text datasets.
al.,</p>
<p>Table 2 :
2
Basic statistics about our WikiGraphs dataset.
the graph structures, making them a good fit forgraph data. GNNs have been used in many appli-cations on KG's (Kipf and Welling, 2016; Wang</p>
<p>Table 3 :
3
The perplexity and the generated text reverse-BLEU score of different types of graph-conditioned models.We show the reverse-BLEU score with or without prompting the original title at the start of the text generation.</p>
<p>Table 5 :
5
Text retrieval given the graph.
Cond. Recall@1 Recall@5 mAPNone0.020.120.10BoW16.2830.2325.98Nodes16.2834.8826.62GNN18.6034.8827.79</p>
<p>Table 6 :
6
, it is observed that graph-conditioned models can indeed retrieve more relevant texts from the graph than the unconditional model, among which the GNN-based model performs the best, and the unconditional model performs close to a random guess.Graph Retrieval given the text.
Cond. Recall@1 Recall@5 mAPNone0.020.070.02BoW95.35100.0097.67Nodes93.02100.0096.51GNN100.00100.00100.00</p>
<p>Table 7 :
7
Ground truth samples.</p>
<p>Table 8 :
8
Generated samples based on the "Where the Streets Have No Name" graph.</p>
<p>The data and the code to reproduce our baseline results are available at https://github.com/deepmind/
https://developers.google.com/ freebase
Cond.Generated Text -= Carbuncle = Bloodlines is a 2011 action role @-@ playing video game developed by <unk> and published by Activision for the Wii .The game is part of a sequel of the 2013 game Eclipse : The Series .It was published by Activision on November 19 , 2011 , for Microsoft Windows , Apple II , Xbox 360 , iOS , and Xbox 360 .The game , set in the fictional Middle Ages , revolves around the player @-@ character centaurs battling for control of their universe .The protagonist of the game , Percy , the leader of the Knights of the Old Republic , is a member of the Knights of the Old Republic , and is appointed to lead a military coup to overthrow the Irish Republic and destroy the Home Nations ' military forces .Though the game mainly took place in new version of the New York City , the original plan was to make it more easily accessible to players unfamiliar with the New Republic .It was also a commercial success , selling more than 900 @,@ 000 copies .The game received mostly positive reviews from most video game publications , with many praising the visual style and the gameplay , but many said that it was not as good as that of the previous game .Reviewers noted the game 's title forward addressing issues such as the difficulty level , a general danger of being too difficult to fight , and the difficulty of playing the game as the player @-@ character 's pattern of character .= = Gameplay = = Bloodlines is a crossover action role @-@ playing game that takes place in the fictional Middle Ages , which is composed of medieval countries and locales .Valhalla , a medieval stronghold , is the game 's main setting .The player @-@ character is a 3 @-@ D miniature character with a sword and shield , which have multiple colored attacks , and has two of the four abilities , which are progressively reduced from the first one and allow for greater size and movement .The available weapons are bolt @-@ fired weapons , advanced weapons , and weapons that can be used in battle .The player is able to summon magical powers to attack targets , and can use magical powers to enhance the character 's abilities .<unk> are also available via a <unk> system , which enables players to throw stones at enemies and attack enemy characters who have not encountered them .The player character also has an ability to revive foes by performing a touch @-@ screen action .The game can be played as a side @-@ scrolling through a View Mode , which can be used in the single @-@ player mode .The first act features a " <unk> " displayed from a first @-@ person perspective .The player character can move around BoW = Civil War Pass = Civil War Pass , also known as the Battle of the Crater or the Battle of Fort Sumner , was an important battle fought on September 7 , 1864 , at Fort Coldwater , in the state of Montana .After seeing repeated attacks on the fort , Gen. James A. Douglas , the commander of the Confederate forces in the South , decided to abandon the fort and flee to the north .After Union forces struck the fort , they decided to flee south to the Ohio River .There they quickly encountered a group of horses , who were used to build a pontoon bridge .The ditches and wooden planks were removed and replaced with stone blocks to make them float ( plow ) .The obstacles that were created in the river valley , however , proved treacherous and were not bridged by mountain passes .The young general and his troops eventually reached the Ohio and the Mississippi rivers , but the new Presidential candidate , Abraham Lincoln , resigned after the war .After the defeat at Fort Sumner , General Douglas , the commander of the Union forces , planned and executed a number of attacks on Fort Sumner .When soldiers arrived , they found two now @-@ deserted locations .The attacks had been made more than a year before .When the line of retreat of the Union forces , which stretched from Fort Sumner to Fort Sumner , reached Fort Sumner on August 19 , 1864 , the cavalrymen captured it on September 30 .In November 1864 , General Douglas was defeated at the Battle of Lake Logan .= = Background = = In 1861 , with the Mexican @-@ American War nearing its conclusion , the American public began to think of an armistice treaty , or peace treaty between Mexico and the United States .On July 1 , 1861 , General Douglas sent three large armies from the Mexican @-@ American War , a series of forts west of the Rockies , to attack Fort Vicksburg .The forts were destroyed in a siege in June .These were built during the years it was fought by the Confederate States of America .The British and Americans were unprepared for the chance of victory , and the Americans were now planning to take control of the Gulf Coast .Like the Americans , the British were planning an attack into central Canada .The British were aware that the main invasion of Canada would occur on July 8 .The British were near the Niagara River and the Union were hopefully midway along the river , approaching Fort Sumner from the west .The British were reluctant to move toward the Carolinas , and so , in the event the Port of Boston was abandoned , the British would be forced to travel to the lower Mississippi .The Nodes = Fort Scott = Fort Scott is an American military post located in Fort Lee , Kansas .It is named in honor of General William Scott , a U.S. Army general and the first commander of the Army of the Potomac .The site was designated as a National Historic Landmark in 1991 , and has been designated a National Historic Landmark under the title of Fort Scott Historical Site since 1929 .It is located in the Rocky Mountains in Kansas and is known as the " James Scott National Historic Site " .= = History = = The original having been settled by the Caddo on the Black River , and later moved to Fort Lee in present @-@ day Decatur County , Virginia .On July 10 , 1810 , the Hennepin reported that the Caddo had acquired the territory of Fort Lee , but it is unclear whether he was present there .He may have taken a position that had previously been occupied by other people .Around 1800 , the first Governor of Kansas , Colonel Andrew H. Sharpe , established Fort Scott in what is now a part of Fort Lee .The fort was constructed on a site that he had named Fort Scott , and was known as Fort Douglas .The fort was used for administrative purposes and for administration of the Missouri Territory .In 1808 , William Bolivar Buckner led a large movement to remove the western boundary of Texas , including Fort Scott .Congress authorized a survey of the territory in 1817 , and a survey of the Old South boundary was completed in 1818 , making Fort Scott the first governor to apply federal law .Although the West Texas Aftermath quickly became a national concern , the new governor was unable to raise sufficient funds to maintain Fort Scott .The fort 's construction and construction were completed in 1821 , but the state legislature refused to grant the commissioners the land they were granted .The new land , called Fort Dix , was consequently purchased by the U.S. Army .The fort 's name was later changed to Fort Lee .While the two states were in dispute by the 1832 treaty , Fort Dix was located in the Horn of the Midget Valley , part of the Pan @-@ American Native Reservation .Confederate forces launched a cavalry attack on Fort Dix in early 1835 , but both sides suffered defeats to the Union .Fort Dix was declared a U.S. Army national monument by President Andrew H. Sharpe in September 1836 .Fort Dix was named after General John Scott , a U.S. Army general and the first governor of Texas , who was killed in an assassination attempt on June 20 , 1855 .Military historian John P. Pickett wrote that it was the first military governor in the United States to serve in the Confederate States GNN = Fort Scott National Historical Park = Fort Scott National Historical Park is an Illinois state historic park in the U.S. state of Kentucky .It is located at 53 • 25 4 N 65 • 41 16 W , at the edge of the Clay Creek valley , southwest of New Orleans .It is located at the intersection of Washington Boulevard and State Route 63 , and is the largest National Historic Landmark in the state .The site was purchased by Native Americans in 1803 and the site was added to the National Register of Historic Places in 1962 .Since 1998 , the site has been subject to an extensive series of historic markers and features that are important in preservation of American historic sites in Texas .The National Park Service includes the nation 's oldest extant log cabins , historic buildings , historic facilities , and historic structures .The park is home to the Mississippi River National Historical Park , a U.S. National Monument that supplies historic sites and historic sites .The original fort was built in 1818 to protect U.S. statehood .In 1899 , the state legislature constructed a small blockhouse at the site of the original fort to defend it from Native Americans .The blockhouse first appeared in 1868 , when land in the city of Lisbon was granted to the state .The fort has remained in use since then .= = History = = = = = Early history = = = Fort Scott was established as a civil and military fortification in 1803 and named after an American Indian .The land that would become Fort Scott was originally part of the Louisiana Purchase , which was granted to the United States by the Louisiana Purchase Act of 1825 .The original fort was established in 1828 by an act of Congress .The American Revolutionary War came to an end in 1830 , but Independence was declared in 1831 and Independence was declared on June 3 , 1830 .The post @-@ war Treaty of Paris signed at Fort Scott ended military activity in the region .War by the United States reached an end in 1830 , and most of the land was put aside for use as a military park .Fort Scott was garrisoned by 90 soldiers from the 55th Louisiana Regiment during the War of 1812 .In 1837 , the Illinois General Assembly passed legislation creating Fort Scott as a federal park , and in the same year the state agreed to purchase the site in honor of the site 's new state of Louisiana .Originally , only about half of Fort Scott was owned , but the size of the park changed in the 1880s from a forest reserve to a dirt road .The park was significantly expanded during the 1910s , but the exact date is disputed .The30.63 29.44 29.56 29.92 30.00 35.03 32.48 31.50 31.72 31.46 Nodes 32.33 30.30 29.82 30.43 29.91 35.45 32.88 31.57 31.79 31.03 GNN 33.81 31.32 30.39 30.53 30.05 36.49 32.49 31.70 31.77 30.79
Dbpedia: A nucleus for a web of open data. Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, Zachary Ives, The Semantic Web. Berlin, Heidelberg; Berlin HeidelbergSpringer2007</p>
<p>Findings of the 2020 conference on machine translation (wmt20). Loïc Barrault, Magdalena Biesialska, Ondřej Bojar, Marta R Costa-Jussà, Christian Federmann, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Matthias Huck, Eric Joanis, Proceedings of the Fifth Conference on Machine Translation. the Fifth Conference on Machine Translation2020</p>
<p>Jessica B Peter W Battaglia, Victor Hamrick, Alvaro Bapst, Vinicius Sanchez-Gonzalez, Mateusz Zambaldi, Andrea Malinowski, David Tacchetti, Adam Raposo, Ryan Santoro, Faulkner, arXiv:1806.01261Relational inductive biases, deep learning, and graph networks. 2018arXiv preprint</p>
<p>Freebase: a collaboratively created graph database for structuring human knowledge. Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, Jamie Taylor, Proceedings of the 2008 ACM SIGMOD international conference on Management of data. the 2008 ACM SIGMOD international conference on Management of data2008</p>
<p>James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake Vanderplas, Skye Wanderman-Milne, Qiao Zhang, JAX: composable transformations of Python+NumPy programs. 2018</p>
<p>Language models are few-shot learners. Benjamin Tom B Brown, Nick Mann, Melanie Ryder, Jared Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry, Askell, arXiv:2005.141652020arXiv preprint</p>
<p>Transformer-xl: Attentive language models beyond a fixed-length context. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Ruslan Quoc V Le, Salakhutdinov, arXiv:1901.028602019arXiv preprint</p>
<p>The WebNLG challenge: Generating text from RDF data. Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini, 10.18653/v1/W17-3518Proceedings of the 10th International Conference on Natural Language Generation. the 10th International Conference on Natural Language GenerationSantiago de Compostela, SpainAssociation for Computational Linguistics2017</p>
<p>Darpa timit acoustic-phonetic continous speech corpus cdrom. nist speech disc 1-1.1. NASA STI/Recon technical report n. Lori F John S Garofolo, William M Lamel, Jonathan G Fisher, David S Fiscus, Pallett, 19939327403</p>
<p>Neural message passing for quantum chemistry. Justin Gilmer, Patrick F Samuel S Schoenholz, Oriol Riley, George E Vinyals, Dahl, International Conference on Machine Learning. PMLR2017</p>
<p>GenWiki: A dataset of 1.3 million content-sharing text and graphs for unsupervised graph-to-text generation. Zhijing Jin, Qipeng Guo, Xipeng Qiu, Zheng Zhang, 10.18653/v1/2020.coling-main.217Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsBarcelona, Spain (Online2020International Committee on Computational Linguistics</p>
<p>Semisupervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, arXiv:1609.029072016arXiv preprint</p>
<p>Text Generation from Knowledge Graphs with Graph Transformers. Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, Hannaneh Hajishirzi, 10.18653/v1/N19-1238Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Neural text generation from structured data with application to the biography domain. Rémi Lebret, David Grangier, Michael Auli, 10.18653/v1/D16-1128Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, Texas2016Association for Computational Linguistics</p>
<p>Microsoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, Lawrence Zitnick, European conference on computer vision. Springer2014</p>
<p>Fei Liu, Jeffrey Flanigan, Sam Thomson, Norman Sadeh, Noah A Smith, arXiv:1805.10399Toward abstractive summarization using semantic representations. 2018arXiv preprint</p>
<p>Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher, arXiv:1609.07843Pointer sentinel mixture models. 2016arXiv preprint</p>
<p>Aaron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu, arXiv:1609.03499Wavenet: A generative model for raw audio. 2016arXiv preprint</p>
<p>Librispeech: an asr corpus based on public domain audio books. Vassil Panayotov, Guoguo Chen, Daniel Povey, Sanjeev Khudanpur, 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE2015</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>P Ankur, Xuezhi Parikh, Sebastian Wang, Manaal Gehrmann, Bhuwan Faruqui, Diyi Dhingra, Dipanjan Yang, Das, Totto: A controlled table-totext generation dataset. 2020</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. Curran Associates, Inc201730</p>
<p>Graph attention networks. Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio, International Conference on Learning Representations. 2018</p>
<p>Show and tell: Lessons learned from the 2015 mscoco image captioning challenge. Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, IEEE transactions on pattern analysis and machine intelligence. 201639</p>
<p>Knowledge-aware graph neural networks with label smoothness regularization for recommender systems. Hongwei Wang, Fuzheng Zhang, Mengdi Zhang, Jure Leskovec, Miao Zhao, Wenjie Li, Zhongyuan Wang, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2019</p>
<p>Kun Xu, Liwei Wang, Mo Yu, Yansong Feng, Yan Song, Zhiguo Wang, Dong Yu, arXiv:1905.11605Cross-lingual knowledge graph alignment via graph matching neural network. 2019arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
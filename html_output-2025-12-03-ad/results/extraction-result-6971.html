<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6971 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6971</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6971</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-134.html">extraction-schema-134</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-253244506</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2211.00053v1.pdf" target="_blank">Generating Sequences by Learning to Self-Correct</a></p>
                <p><strong>Paper Abstract:</strong> Sequence generation applications require satisfying semantic constraints, such as ensuring that programs are correct, using certain keywords, or avoiding undesirable content. Language models, whether fine-tuned or prompted with few-shot demonstrations, frequently violate these constraints, and lack a mechanism to iteratively revise their outputs. Moreover, some powerful language models are of extreme scale or inaccessible, making it inefficient, if not infeasible, to update their parameters for task-specific adaptation. We present Self-Correction, an approach that decouples an imperfect base generator (an off-the-shelf language model or supervised sequence-to-sequence model) from a separate corrector that learns to iteratively correct imperfect generations. To train the corrector, we propose an online training procedure that can use either scalar or natural language feedback on intermediate imperfect generations. We show that Self-Correction improves upon the base generator in three diverse generation tasks - mathematical program synthesis, lexically-constrained generation, and toxicity control - even when the corrector is much smaller than the base generator.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6971.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6971.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-CORRECT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Correction (Self-Corrector / Self-Corrective Learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-module generation paradigm that decouples a base generator (off‑the‑shelf LM) from a learned corrector trained via self-corrective learning to iteratively improve generated outputs according to a scalar value function or explicit feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Neo 1.3B (corrector); variable base generators (GPT-Neo, GPT-3, GPT-2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Corrector: autoregressive transformer (GPT-Neo 1.3B) fine-tuned as a text-to-text model mapping (input, hypothesis[, feedback]) -> improved hypothesis. Base generators used include fine-tuned GPT-Neo, few-shot GPT-3 (davinci / text-davinci-002), and GPT-2 variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.3B (corrector); generators: GPT-Neo 1.3B, GPT-2 family (medium/large/XL), GPT-3 ~175B (davinci / text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Correction (self-corrective learning)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate an initial hypothesis with a base generator; train a separate corrector on pairs (hypothesis -> higher-value correction) sampled from a datapool, using a value function v(y) to form value-improving pairs and optionally natural language feedback; at inference repeatedly decode from the corrector to iteratively improve the draft.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (iterative correction by applying the corrector sequentially)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>multiple (program synthesis, lexically-constrained generation, toxicity reduction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Applied across diverse generation tasks where outputs are iteratively improved by a corrector trained to raise a scalar value (correctness, coverage, toxicity score) or use explicit feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>varies by task (accuracy/correctness for program synthesis; constraint coverage, BLEU/CIDER for lexically-constrained; Perspective API toxicity measures for detoxification)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>varies by task and generator (see task-specific entries); reported baselines are the generator-only performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>varies by task and generator (see task-specific entries); in all reported experiments self-correction improved over the base generator.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Marginalization over intermediate generations is intractable and is approximated with single decoded trajectories; success depends on availability of a value function v(y) at training time; multiple corrections can require additional guidance (e.g., explicit feedback) to continue improving; the method requires collecting value-improving pairs and exploration to expand the datapool.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Sequences by Learning to Self-Correct', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6971.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6971.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Math-GSM8k Self-Correct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Correction applied to GSM8k program-synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Self-Corrector (GPT-Neo 1.3B corrector) applied to program-synthesis framing of GSM8k problems; corrector trained to increase execution-correctness (binary) and applied at inference to revise generated programs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Neo 1.3B (corrector); generator: GPT-Neo 1.3B (fine-tuned) or few-shot GPT-3 (davinci / text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Corrector is a fine-tuned GPT-Neo 1.3B text-to-text model that consumes a problem and a hypothesis program and emits a corrected program; generator is either a fine-tuned GPT-Neo or prompted GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.3B (corrector); generator sizes vary (GPT-Neo 1.3B, GPT-3 ~175B)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-CORRECT (self-corrective learning)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Train on hypothesis->higher-value program pairs (value = execution correctness); at inference decode initial program then apply the corrector to produce a revised program (k corrections).</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8k (grade-school math, program synthesis framing)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate a Python program that, when executed, yields the correct numeric answer for a grade-school math problem.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>execution correctness (accuracy; 1 if program executes and returns ground-truth answer, else 0)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>8.57% accuracy (generator baseline reported for GPT-Neo generator on GSM8k)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>21% accuracy with always-correcting; 24% accuracy when only correcting incorrect solutions (SELF-CORRECT *)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Absolute performance remains low on this challenging dataset despite large relative improvements; a single correction provides nontrivial gains but still far from state-of-the-art (requires further scaling/techniques); when using feedback for math they used more iterations (see feedback entry) and risked solution leakage when feedback model has access to gold solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Sequences by Learning to Self-Correct', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6971.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6971.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Math-MultiArith Self-Correct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Correction applied to MultiArith program-synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Self-Correction trains a corrector to turn imperfect program hypotheses into correct programs on arithmetic problems; high final accuracy after correction is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Neo 1.3B (corrector); generator: GPT-Neo 1.3B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Corrector: fine-tuned GPT-Neo 1.3B trained on value-improving pairs (value = execution correctness). Generator: fine-tuned GPT-Neo 1.3B used to seed the datapool or used at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.3B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-CORRECT (self-corrective learning)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Train on hypothesis->higher-valued program pairs and apply a single correction at inference to increase execution correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MultiArith (arithmetic word problems)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Solve arithmetic word problems by generating executable programs whose outputs match ground-truth answers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>execution correctness (accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>generator baseline was reported to be ≈38 percentage points lower than after correction (paper reports the improvement size rather than an exact baseline in the text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>98–99% accuracy after correction (paper text: 'very high after correction (98–99%), a 38 point improvement over the generator')</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The large absolute gains indicate the task admits high correctionability, but the approach requires a reliable execution-based value function; training depends on collecting diverse value-improving pairs and exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Sequences by Learning to Self-Correct', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6971.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6971.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Math-GPT3 Swap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Correction applied to large-generator (GPT-3 Instruct) by swapping in corrector</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A corrector trained (possibly on smaller generator outputs) can be applied to a larger generator (GPT-3 Instruct) at test time and improve performance; further gains when training with the large generator's outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Neo 1.3B (corrector) applied to GPT-3 Instruct generator</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Corrector: GPT-Neo 1.3B fine-tuned as in other experiments; generator: GPT-3 Instruct (large, ~175B).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>corrector 1.3B; generator ~175B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-CORRECT (module swapping / transfer of corrector)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Use a corrector trained with one generator (or trained with its own datapool) to correct outputs from a much larger generator at test time; optionally re-train the corrector with the large generator's outputs for additional gains.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multitask (arithmetic) and GSM (program-synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Arithmetic program-synthesis across several datasets; evaluate effect of applying corrector to larger generator outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>execution correctness (accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>GPT-3 Instruct generator performance: 84.90% (Multitask), 36.80% (GSM) as reported in paper</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>After adding corrector: 90.90% (Multitask), 45.00% (GSM); after re-training/including the GPT-3 outputs in training: 92.75% (Multitask), 45.92% (GSM)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Corrector transferability is promising but not guaranteed for all failure modes; quality depends on match between training datapool distributions and the larger generator's outputs; additional training with the large generator yields further gains, indicating initial transfer may be incomplete.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Sequences by Learning to Self-Correct', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6971.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6971.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CommonGen SELF-CORRECT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Correction applied to COMMONGEN (lexically-constrained generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A corrector trained to optimize constraint coverage improves the percentage of constraint words included in outputs while maintaining or improving fluency and other automatic scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 Large (generator) with a fine-tuned corrector (GPT-2 based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base generator: GPT-2 (large) fine-tuned for CommonGen; corrector: another GPT-2 variant fine-tuned to increase constraint coverage (value = coverage).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>generator: GPT-2 Large; corrector: GPT-2 (size tuned per experiment; paper used medium/large variants)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-CORRECT (constraint-optimizing corrector)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate an initial sentence conditioned on constraint words; corrector is trained on pairs where a candidate satisfying more constraints is paired with a less-complete hypothesis, optimizing coverage; at inference apply up to 3 corrections and stop early if all constraints met.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>COMMONGEN (constrained commonsense generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate a coherent sentence that contains all provided constraint words.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>constraint coverage (%) plus automatic fluency metrics (BLEU, CIDER) and human fluency judgments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>coverage 91.50% (GPT-2 baseline, reported in Table 11); BLEU-4 67.12, CIDER 2.33 (as reported)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>coverage 98.77% (SELF-CORRECT); BLEU-4 68.81, CIDER 2.38 — i.e., substantially higher coverage while maintaining/improving fluency metrics</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires a differentiable/accessible coverage value function for training; runtime increases (inference slower than single-pass decoding: e.g., 0.8s/sent for SELF-CORRECT vs 0.2s/sent for plain GPT-2 in reported table), though still faster than some alternative constrained decoders; success depends on quality of the generator's initial drafts to provide learnable residuals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Sequences by Learning to Self-Correct', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6971.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6971.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>E2E SELF-CORRECT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Correction applied to E2E (structured-to-text, lexically-constrained)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applying the corrector to E2E improves constraint coverage and human-rated fluency compared to the tuned GPT-2 generator and to competing constrained-decode methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 Medium (generator) with a GPT-2 based corrector</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generator: GPT-2 medium fine-tuned on E2E; corrector: GPT-2 based fine-tuned to increase constraint coverage and fluency.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>generator: GPT-2 Medium; corrector: GPT-2 Medium</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-CORRECT</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Train corrector to convert hypothesis sentences that miss constraints into sentences that satisfy more constraints; at inference apply up to 3 corrections, stopping when constraints are met.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>E2E (structured-data to text generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Convert structured input entries (e.g., restaurant attributes) into fluent natural language descriptions that include specified attribute values.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>constraint coverage (%), automatic metrics (BLEU, CIDER), and human fluency judgments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>coverage 91.38% (GPT-2 baseline), BLEU-4 27.90, CIDER 14.97 (as reported in Table 10)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>coverage 94.58% (SELF-CORRECT), BLEU-4 27.98, CIDER 15.30 (SELF-CORRECT) — improved coverage with similar or slightly improved fluency metrics</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Improvements depend on the corrector's ability to generate fluent edits that also satisfy constraints; inference cost higher than single-pass decoding; pairing and proportional sampling strategies during training are important for gains (ablations show performance drops when removed).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Sequences by Learning to Self-Correct', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6971.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6971.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Toxicity SELF-CORRECT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Correction applied to toxicity reduction (REALTOXICITYPROMPTS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A corrector trained to reduce Perspective API toxicity scores substantially lowers the incidence and magnitude of toxic generations while maintaining fluency and diversity compared to strong baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 Large (generator) and GPT-2 Large fine-tuned as corrector; also applied to larger generators (GPT-2 XL, GPT-3)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generator: off-the-shelf GPT-2 Large (or larger models at inference); corrector: separate GPT-2 Large fine-tuned with self-corrective learning where v(y) = Perspective API toxicity score.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-2 Large (generator and corrector); also evaluated correcting GPT-2 XL and GPT-3 outputs at test time</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-CORRECT (detoxification corrector)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Train corrector to map higher-to-lower toxicity pairs (value = toxicity score from Perspective API); at inference, sample 25 outputs with nucleus sampling and apply up to 3 sequential corrections to reduce toxicity.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>REALTOXICITYPROMPTS (toxicity reduction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate continuations from prompts while minimizing offensive/toxic content as measured by Perspective API.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Perspective API maximum toxicity (average maximum toxicity over 25 sampled generations); also empirical toxicity probability (probability >=1 toxic sample in 25) and fluency/diversity metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>GPT-2 Large baseline maximum toxicity: 0.527 (Perspective API score, as reported in Appendix Table 9)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>SELF-CORRECT maximum toxicity: 0.171; SELF-CORRECT + feedback: 0.156 (Perspective API scores reported in Appendix Table 9)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Method requires access to a toxicity scorer (Perspective API) during training and optionally at inference to provide attribute feedback; using the API at inference is not required but can improve performance; dependence on an external classifier/API introduces costs and potential biases of that API; multiple corrections help but need good training data and pairing; swapping corrector onto very different generator distributions may require additional training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Sequences by Learning to Self-Correct', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6971.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6971.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-CORRECT + Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Correction with Explicit Natural Language Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variant of self-correction where the corrector conditions on explicit natural language feedback f(y) (e.g., 'decrease profanity', 'add constraint word: read', or LLM-generated debugging hints) to produce targeted corrections, yielding further improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Corrector (GPT-Neo 1.3B or GPT-2 variants) conditioned on feedback produced by Perspective API, or GPT-3 used as a feedback generator for math</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Corrector consumes (problem/input, hypothesis, feedback) and is trained to map to a higher-value correction; feedback is derived from attribute scores (toxicity attributes), enumerated missing constraints, or LLM-produced localized program comments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>corrector: GPT-Neo 1.3B or GPT-2 Large as used per experiment; feedback model: GPT-3 (text-davinci-002) for math feedback; Perspective API for toxicity attributes</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>SELF-CORRECT + Natural Language Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Compute a natural-language feedback token/phrase for each hypothesis->correction pair by selecting the attribute with largest change (toxicity) or enumerating missing constraints; train the corrector to condition on feedback; at inference compute feedback for current hypothesis (via API or feedback model) and feed to corrector.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect with targeted feedback conditioning</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>toxicity reduction (REALTOXICITYPROMPTS), lexically-constrained generation (COMMONGEN), and math (Multitask arithmetic)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Use fine-grained feedback tokens to guide corrections (e.g., identify which toxicity attribute to reduce, which constraints to add, or which program line to fix).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>same as the underlying task (Perspective API toxicity scores; constraint coverage; execution correctness for math)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>same base generator baselines as the non-feedback experiments (see respective entries); scalar-only self-correct results are the immediate baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Paper reports improved performance over scalar-only self-correct across tasks (e.g., toxicity: 0.171 -> 0.156 when adding feedback; lexically-constrained coverage improved from 94.58% to 95.88% in one reported comparison; math benefits from multiple corrections with feedback).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>When using an LLM feedback model for math, the authors allowed feedback model access to gold solutions during training and (in experiments) at inference — this can introduce solution leakage and makes those results only exploratory; using feedback often requires additional API calls at inference (cost/privacy) and depends on the quality of the feedback generator/API; multiple corrections with feedback were required in math (they used 5 iterations in feedback experiments), suggesting scalar feedback alone may be insufficient in complex domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Sequences by Learning to Self-Correct', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-critiquing models for assisting human evaluators <em>(Rating: 2)</em></li>
                <li>Learning from self-sampled correct and partially-correct programs <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 1)</em></li>
                <li>NeuroLogic decoding: (un)supervised neural text generation with predicate logic constraints <em>(Rating: 1)</em></li>
                <li>Quark: Controllable text generation with reinforced unlearning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6971",
    "paper_id": "paper-253244506",
    "extraction_schema_id": "extraction-schema-134",
    "extracted_data": [
        {
            "name_short": "SELF-CORRECT",
            "name_full": "Self-Correction (Self-Corrector / Self-Corrective Learning)",
            "brief_description": "A two-module generation paradigm that decouples a base generator (off‑the‑shelf LM) from a learned corrector trained via self-corrective learning to iteratively improve generated outputs according to a scalar value function or explicit feedback.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-Neo 1.3B (corrector); variable base generators (GPT-Neo, GPT-3, GPT-2)",
            "model_description": "Corrector: autoregressive transformer (GPT-Neo 1.3B) fine-tuned as a text-to-text model mapping (input, hypothesis[, feedback]) -&gt; improved hypothesis. Base generators used include fine-tuned GPT-Neo, few-shot GPT-3 (davinci / text-davinci-002), and GPT-2 variants.",
            "model_size": "1.3B (corrector); generators: GPT-Neo 1.3B, GPT-2 family (medium/large/XL), GPT-3 ~175B (davinci / text-davinci-002)",
            "reflection_method_name": "Self-Correction (self-corrective learning)",
            "reflection_method_description": "Generate an initial hypothesis with a base generator; train a separate corrector on pairs (hypothesis -&gt; higher-value correction) sampled from a datapool, using a value function v(y) to form value-improving pairs and optionally natural language feedback; at inference repeatedly decode from the corrector to iteratively improve the draft.",
            "iteration_type": "generate-then-reflect (iterative correction by applying the corrector sequentially)",
            "num_iterations": null,
            "task_name": "multiple (program synthesis, lexically-constrained generation, toxicity reduction)",
            "task_description": "Applied across diverse generation tasks where outputs are iteratively improved by a corrector trained to raise a scalar value (correctness, coverage, toxicity score) or use explicit feedback.",
            "evaluation_metric": "varies by task (accuracy/correctness for program synthesis; constraint coverage, BLEU/CIDER for lexically-constrained; Perspective API toxicity measures for detoxification)",
            "performance_before_reflection": "varies by task and generator (see task-specific entries); reported baselines are the generator-only performance.",
            "performance_after_reflection": "varies by task and generator (see task-specific entries); in all reported experiments self-correction improved over the base generator.",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Marginalization over intermediate generations is intractable and is approximated with single decoded trajectories; success depends on availability of a value function v(y) at training time; multiple corrections can require additional guidance (e.g., explicit feedback) to continue improving; the method requires collecting value-improving pairs and exploration to expand the datapool.",
            "uuid": "e6971.0",
            "source_info": {
                "paper_title": "Generating Sequences by Learning to Self-Correct",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Math-GSM8k Self-Correct",
            "name_full": "Self-Correction applied to GSM8k program-synthesis",
            "brief_description": "Self-Corrector (GPT-Neo 1.3B corrector) applied to program-synthesis framing of GSM8k problems; corrector trained to increase execution-correctness (binary) and applied at inference to revise generated programs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-Neo 1.3B (corrector); generator: GPT-Neo 1.3B (fine-tuned) or few-shot GPT-3 (davinci / text-davinci-002)",
            "model_description": "Corrector is a fine-tuned GPT-Neo 1.3B text-to-text model that consumes a problem and a hypothesis program and emits a corrected program; generator is either a fine-tuned GPT-Neo or prompted GPT-3.",
            "model_size": "1.3B (corrector); generator sizes vary (GPT-Neo 1.3B, GPT-3 ~175B)",
            "reflection_method_name": "SELF-CORRECT (self-corrective learning)",
            "reflection_method_description": "Train on hypothesis-&gt;higher-value program pairs (value = execution correctness); at inference decode initial program then apply the corrector to produce a revised program (k corrections).",
            "iteration_type": "generate-then-reflect",
            "num_iterations": 1,
            "task_name": "GSM8k (grade-school math, program synthesis framing)",
            "task_description": "Generate a Python program that, when executed, yields the correct numeric answer for a grade-school math problem.",
            "evaluation_metric": "execution correctness (accuracy; 1 if program executes and returns ground-truth answer, else 0)",
            "performance_before_reflection": "8.57% accuracy (generator baseline reported for GPT-Neo generator on GSM8k)",
            "performance_after_reflection": "21% accuracy with always-correcting; 24% accuracy when only correcting incorrect solutions (SELF-CORRECT *)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Absolute performance remains low on this challenging dataset despite large relative improvements; a single correction provides nontrivial gains but still far from state-of-the-art (requires further scaling/techniques); when using feedback for math they used more iterations (see feedback entry) and risked solution leakage when feedback model has access to gold solutions.",
            "uuid": "e6971.1",
            "source_info": {
                "paper_title": "Generating Sequences by Learning to Self-Correct",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Math-MultiArith Self-Correct",
            "name_full": "Self-Correction applied to MultiArith program-synthesis",
            "brief_description": "Self-Correction trains a corrector to turn imperfect program hypotheses into correct programs on arithmetic problems; high final accuracy after correction is reported.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-Neo 1.3B (corrector); generator: GPT-Neo 1.3B",
            "model_description": "Corrector: fine-tuned GPT-Neo 1.3B trained on value-improving pairs (value = execution correctness). Generator: fine-tuned GPT-Neo 1.3B used to seed the datapool or used at inference.",
            "model_size": "1.3B",
            "reflection_method_name": "SELF-CORRECT (self-corrective learning)",
            "reflection_method_description": "Train on hypothesis-&gt;higher-valued program pairs and apply a single correction at inference to increase execution correctness.",
            "iteration_type": "generate-then-reflect",
            "num_iterations": 1,
            "task_name": "MultiArith (arithmetic word problems)",
            "task_description": "Solve arithmetic word problems by generating executable programs whose outputs match ground-truth answers.",
            "evaluation_metric": "execution correctness (accuracy)",
            "performance_before_reflection": "generator baseline was reported to be ≈38 percentage points lower than after correction (paper reports the improvement size rather than an exact baseline in the text).",
            "performance_after_reflection": "98–99% accuracy after correction (paper text: 'very high after correction (98–99%), a 38 point improvement over the generator')",
            "improvement_observed": true,
            "limitations_or_failure_cases": "The large absolute gains indicate the task admits high correctionability, but the approach requires a reliable execution-based value function; training depends on collecting diverse value-improving pairs and exploration.",
            "uuid": "e6971.2",
            "source_info": {
                "paper_title": "Generating Sequences by Learning to Self-Correct",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Math-GPT3 Swap",
            "name_full": "Self-Correction applied to large-generator (GPT-3 Instruct) by swapping in corrector",
            "brief_description": "A corrector trained (possibly on smaller generator outputs) can be applied to a larger generator (GPT-3 Instruct) at test time and improve performance; further gains when training with the large generator's outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-Neo 1.3B (corrector) applied to GPT-3 Instruct generator",
            "model_description": "Corrector: GPT-Neo 1.3B fine-tuned as in other experiments; generator: GPT-3 Instruct (large, ~175B).",
            "model_size": "corrector 1.3B; generator ~175B",
            "reflection_method_name": "SELF-CORRECT (module swapping / transfer of corrector)",
            "reflection_method_description": "Use a corrector trained with one generator (or trained with its own datapool) to correct outputs from a much larger generator at test time; optionally re-train the corrector with the large generator's outputs for additional gains.",
            "iteration_type": "generate-then-reflect",
            "num_iterations": 1,
            "task_name": "Multitask (arithmetic) and GSM (program-synthesis)",
            "task_description": "Arithmetic program-synthesis across several datasets; evaluate effect of applying corrector to larger generator outputs.",
            "evaluation_metric": "execution correctness (accuracy)",
            "performance_before_reflection": "GPT-3 Instruct generator performance: 84.90% (Multitask), 36.80% (GSM) as reported in paper",
            "performance_after_reflection": "After adding corrector: 90.90% (Multitask), 45.00% (GSM); after re-training/including the GPT-3 outputs in training: 92.75% (Multitask), 45.92% (GSM)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Corrector transferability is promising but not guaranteed for all failure modes; quality depends on match between training datapool distributions and the larger generator's outputs; additional training with the large generator yields further gains, indicating initial transfer may be incomplete.",
            "uuid": "e6971.3",
            "source_info": {
                "paper_title": "Generating Sequences by Learning to Self-Correct",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "CommonGen SELF-CORRECT",
            "name_full": "Self-Correction applied to COMMONGEN (lexically-constrained generation)",
            "brief_description": "A corrector trained to optimize constraint coverage improves the percentage of constraint words included in outputs while maintaining or improving fluency and other automatic scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 Large (generator) with a fine-tuned corrector (GPT-2 based)",
            "model_description": "Base generator: GPT-2 (large) fine-tuned for CommonGen; corrector: another GPT-2 variant fine-tuned to increase constraint coverage (value = coverage).",
            "model_size": "generator: GPT-2 Large; corrector: GPT-2 (size tuned per experiment; paper used medium/large variants)",
            "reflection_method_name": "SELF-CORRECT (constraint-optimizing corrector)",
            "reflection_method_description": "Generate an initial sentence conditioned on constraint words; corrector is trained on pairs where a candidate satisfying more constraints is paired with a less-complete hypothesis, optimizing coverage; at inference apply up to 3 corrections and stop early if all constraints met.",
            "iteration_type": "generate-then-reflect",
            "num_iterations": 3,
            "task_name": "COMMONGEN (constrained commonsense generation)",
            "task_description": "Generate a coherent sentence that contains all provided constraint words.",
            "evaluation_metric": "constraint coverage (%) plus automatic fluency metrics (BLEU, CIDER) and human fluency judgments",
            "performance_before_reflection": "coverage 91.50% (GPT-2 baseline, reported in Table 11); BLEU-4 67.12, CIDER 2.33 (as reported)",
            "performance_after_reflection": "coverage 98.77% (SELF-CORRECT); BLEU-4 68.81, CIDER 2.38 — i.e., substantially higher coverage while maintaining/improving fluency metrics",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Requires a differentiable/accessible coverage value function for training; runtime increases (inference slower than single-pass decoding: e.g., 0.8s/sent for SELF-CORRECT vs 0.2s/sent for plain GPT-2 in reported table), though still faster than some alternative constrained decoders; success depends on quality of the generator's initial drafts to provide learnable residuals.",
            "uuid": "e6971.4",
            "source_info": {
                "paper_title": "Generating Sequences by Learning to Self-Correct",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "E2E SELF-CORRECT",
            "name_full": "Self-Correction applied to E2E (structured-to-text, lexically-constrained)",
            "brief_description": "Applying the corrector to E2E improves constraint coverage and human-rated fluency compared to the tuned GPT-2 generator and to competing constrained-decode methods.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 Medium (generator) with a GPT-2 based corrector",
            "model_description": "Generator: GPT-2 medium fine-tuned on E2E; corrector: GPT-2 based fine-tuned to increase constraint coverage and fluency.",
            "model_size": "generator: GPT-2 Medium; corrector: GPT-2 Medium",
            "reflection_method_name": "SELF-CORRECT",
            "reflection_method_description": "Train corrector to convert hypothesis sentences that miss constraints into sentences that satisfy more constraints; at inference apply up to 3 corrections, stopping when constraints are met.",
            "iteration_type": "generate-then-reflect",
            "num_iterations": 3,
            "task_name": "E2E (structured-data to text generation)",
            "task_description": "Convert structured input entries (e.g., restaurant attributes) into fluent natural language descriptions that include specified attribute values.",
            "evaluation_metric": "constraint coverage (%), automatic metrics (BLEU, CIDER), and human fluency judgments",
            "performance_before_reflection": "coverage 91.38% (GPT-2 baseline), BLEU-4 27.90, CIDER 14.97 (as reported in Table 10)",
            "performance_after_reflection": "coverage 94.58% (SELF-CORRECT), BLEU-4 27.98, CIDER 15.30 (SELF-CORRECT) — improved coverage with similar or slightly improved fluency metrics",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Improvements depend on the corrector's ability to generate fluent edits that also satisfy constraints; inference cost higher than single-pass decoding; pairing and proportional sampling strategies during training are important for gains (ablations show performance drops when removed).",
            "uuid": "e6971.5",
            "source_info": {
                "paper_title": "Generating Sequences by Learning to Self-Correct",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Toxicity SELF-CORRECT",
            "name_full": "Self-Correction applied to toxicity reduction (REALTOXICITYPROMPTS)",
            "brief_description": "A corrector trained to reduce Perspective API toxicity scores substantially lowers the incidence and magnitude of toxic generations while maintaining fluency and diversity compared to strong baselines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 Large (generator) and GPT-2 Large fine-tuned as corrector; also applied to larger generators (GPT-2 XL, GPT-3)",
            "model_description": "Generator: off-the-shelf GPT-2 Large (or larger models at inference); corrector: separate GPT-2 Large fine-tuned with self-corrective learning where v(y) = Perspective API toxicity score.",
            "model_size": "GPT-2 Large (generator and corrector); also evaluated correcting GPT-2 XL and GPT-3 outputs at test time",
            "reflection_method_name": "SELF-CORRECT (detoxification corrector)",
            "reflection_method_description": "Train corrector to map higher-to-lower toxicity pairs (value = toxicity score from Perspective API); at inference, sample 25 outputs with nucleus sampling and apply up to 3 sequential corrections to reduce toxicity.",
            "iteration_type": "generate-then-reflect",
            "num_iterations": 3,
            "task_name": "REALTOXICITYPROMPTS (toxicity reduction)",
            "task_description": "Generate continuations from prompts while minimizing offensive/toxic content as measured by Perspective API.",
            "evaluation_metric": "Perspective API maximum toxicity (average maximum toxicity over 25 sampled generations); also empirical toxicity probability (probability &gt;=1 toxic sample in 25) and fluency/diversity metrics",
            "performance_before_reflection": "GPT-2 Large baseline maximum toxicity: 0.527 (Perspective API score, as reported in Appendix Table 9)",
            "performance_after_reflection": "SELF-CORRECT maximum toxicity: 0.171; SELF-CORRECT + feedback: 0.156 (Perspective API scores reported in Appendix Table 9)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Method requires access to a toxicity scorer (Perspective API) during training and optionally at inference to provide attribute feedback; using the API at inference is not required but can improve performance; dependence on an external classifier/API introduces costs and potential biases of that API; multiple corrections help but need good training data and pairing; swapping corrector onto very different generator distributions may require additional training.",
            "uuid": "e6971.6",
            "source_info": {
                "paper_title": "Generating Sequences by Learning to Self-Correct",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "SELF-CORRECT + Feedback",
            "name_full": "Self-Correction with Explicit Natural Language Feedback",
            "brief_description": "Variant of self-correction where the corrector conditions on explicit natural language feedback f(y) (e.g., 'decrease profanity', 'add constraint word: read', or LLM-generated debugging hints) to produce targeted corrections, yielding further improvements.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Corrector (GPT-Neo 1.3B or GPT-2 variants) conditioned on feedback produced by Perspective API, or GPT-3 used as a feedback generator for math",
            "model_description": "Corrector consumes (problem/input, hypothesis, feedback) and is trained to map to a higher-value correction; feedback is derived from attribute scores (toxicity attributes), enumerated missing constraints, or LLM-produced localized program comments.",
            "model_size": "corrector: GPT-Neo 1.3B or GPT-2 Large as used per experiment; feedback model: GPT-3 (text-davinci-002) for math feedback; Perspective API for toxicity attributes",
            "reflection_method_name": "SELF-CORRECT + Natural Language Feedback",
            "reflection_method_description": "Compute a natural-language feedback token/phrase for each hypothesis-&gt;correction pair by selecting the attribute with largest change (toxicity) or enumerating missing constraints; train the corrector to condition on feedback; at inference compute feedback for current hypothesis (via API or feedback model) and feed to corrector.",
            "iteration_type": "generate-then-reflect with targeted feedback conditioning",
            "num_iterations": null,
            "task_name": "toxicity reduction (REALTOXICITYPROMPTS), lexically-constrained generation (COMMONGEN), and math (Multitask arithmetic)",
            "task_description": "Use fine-grained feedback tokens to guide corrections (e.g., identify which toxicity attribute to reduce, which constraints to add, or which program line to fix).",
            "evaluation_metric": "same as the underlying task (Perspective API toxicity scores; constraint coverage; execution correctness for math)",
            "performance_before_reflection": "same base generator baselines as the non-feedback experiments (see respective entries); scalar-only self-correct results are the immediate baseline.",
            "performance_after_reflection": "Paper reports improved performance over scalar-only self-correct across tasks (e.g., toxicity: 0.171 -&gt; 0.156 when adding feedback; lexically-constrained coverage improved from 94.58% to 95.88% in one reported comparison; math benefits from multiple corrections with feedback).",
            "improvement_observed": true,
            "limitations_or_failure_cases": "When using an LLM feedback model for math, the authors allowed feedback model access to gold solutions during training and (in experiments) at inference — this can introduce solution leakage and makes those results only exploratory; using feedback often requires additional API calls at inference (cost/privacy) and depends on the quality of the feedback generator/API; multiple corrections with feedback were required in math (they used 5 iterations in feedback experiments), suggesting scalar feedback alone may be insufficient in complex domains.",
            "uuid": "e6971.7",
            "source_info": {
                "paper_title": "Generating Sequences by Learning to Self-Correct",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-critiquing models for assisting human evaluators",
            "rating": 2,
            "sanitized_title": "selfcritiquing_models_for_assisting_human_evaluators"
        },
        {
            "paper_title": "Learning from self-sampled correct and partially-correct programs",
            "rating": 2,
            "sanitized_title": "learning_from_selfsampled_correct_and_partiallycorrect_programs"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 1,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "NeuroLogic decoding: (un)supervised neural text generation with predicate logic constraints",
            "rating": 1,
            "sanitized_title": "neurologic_decoding_unsupervised_neural_text_generation_with_predicate_logic_constraints"
        },
        {
            "paper_title": "Quark: Controllable text generation with reinforced unlearning",
            "rating": 1,
            "sanitized_title": "quark_controllable_text_generation_with_reinforced_unlearning"
        }
    ],
    "cost": 0.02127625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>GENERATING SEQUENCES BY LEARNING TO [SELF-]CORRECT</p>
<p>Sean Welleck 
Allen Institute for Artificial Intelligence</p>
<p>Paul G. Allen School of Computer Science &amp; Engineering
University of Washington</p>
<p>Ximing Lu 
Allen Institute for Artificial Intelligence</p>
<p>Peter West 
Paul G. Allen School of Computer Science &amp; Engineering
University of Washington</p>
<p>Faeze Brahman 
Allen Institute for Artificial Intelligence</p>
<p>Tianxiao Shen 
Paul G. Allen School of Computer Science &amp; Engineering
University of Washington</p>
<p>Daniel Khashabi 
Center for Language and Speech Processing
Johns Hopkins University</p>
<p>Yejin Choi 
Allen Institute for Artificial Intelligence</p>
<p>Paul G. Allen School of Computer Science &amp; Engineering
University of Washington</p>
<p>GENERATING SEQUENCES BY LEARNING TO [SELF-]CORRECT
Preprint. Under review.
Sequence generation applications require satisfying semantic constraints, such as ensuring that programs are correct, using certain keywords, or avoiding undesirable content. Language models, whether fine-tuned or prompted with few-shot demonstrations, frequently violate these constraints, and lack a mechanism to iteratively revise their outputs. Moreover, some powerful language models are of extreme scale or inaccessible, making it inefficient, if not infeasible, to update their parameters for task-specific adaptation. We present SELF-CORRECTION, an approach that decouples an imperfect base generator (an off-the-shelf language model or supervised sequence-to-sequence model) from a separate corrector that learns to iteratively correct imperfect generations. To train the corrector, we propose an online training procedure that can use either scalar or natural language feedback on intermediate imperfect generations. We show that SELF-CORRECTION improves upon the base generator in three diverse generation tasksmathematical program synthesis, lexically-constrained generation, and toxicity control-even when the corrector is much smaller than the base generator. * First authors, contributed equally. †Second authors, contributed equally.</p>
<p>INTRODUCTION</p>
<p>The standard practice for natural language generation tasks is inherently single-pass: applying a decoding procedure to either a few-shot prompted language model or one tuned for a given task, then considering the generation as "finished" (e.g. ; Brown et al. (2020); ). Powerful generation models often meet most of the task requirements, yet miss a few (e.g., omitting a subset of keywords), or generate incorrect hypotheses that nevertheless provide useful structure (e.g., a correct problem solving strategy with a missing step). However, after generating even a slightly sub-optimal sequence, the single-pass paradigm requires models to "start from scratch", effectively discarding work already done. A more natural, intuitive approach is leveraging the generation as a useful starting point to refine into a higher quality output.</p>
<p>To formalize this intuition, we introduce Self-Correction for Sequence Generation. Figure 1 demonstrates its central principle: a generation model is re-framed as a base generator, which produces a reasonable initial hypothesis but does not need to solve the task in one pass, and a second module-the corrector-trained to make up the difference between the hypothesis and an optimal solution. Neither the generator nor the corrector must solve the full task in one pass, and the corrector can be applied multiple times to iteratively improve the output ( §3.6). We propose a simple, general procedure for training the corrector (Figure 2) by pairing generator outputs with carefully selected targets. The result is a system which self-corrects, producing outputs through multiple generation passes and breaking the task into steps that can be solved by dedicated and efficient sub-systems. Figure 1: SELF-CORRECTORs decompose generation into a base generator that proposes an initial hypothesis, and a corrector that iteratively improves its quality.</p>
<p>We find that Self-Correction is broadly applicable. Training a corrector model improves the base generator on 3 diverse tasks: mathematical program synthesis ( §3.1), lexically constrained generation ( §3.2), and toxicity reduction ( §3.3). The trained corrector model can even be applied to a larger generator with similar performance to training a new corrector ( §3.4), showing that the subtask of correction is transferable, even to stronger generators. Finally, we explore the prospect of introducing a third module to the Self-Correction system ( §3.5)-explicitly using natural language feedback to guide corrections-with promising results. Self-Correction offers an exciting opportunity to build on existing generation models and the sequences they generate, with efficient, effective, and transferable corrector networks.</p>
<p>SELF-CORRECTING SEQUENCE GENERATORS</p>
<p>A typical autoregressive text generator (e.g. GPT-3 (Brown et al., 2020)) maps an input prompt to a distribution over outputs using a single parameterized module (e.g. a large transformer), p 0 (y|x). We explore an alternative that decomposes into two modules, a base generator, and a corrector,
p(y|x) = y0 p 0 (y 0 |x) generator p θ (y|y 0 , x) corrector (1)
where the generator provides an initial hypothesis that is refined by the corrector. In practice, the corrector can be applied multiple times, p(y T |x) = y0 y1 · · · y T −1 p 0 (y 0 |x) t p θ (y t+1 |y t , x). Since a model of this form can both generate and correct its generations, we call it a Self-Corrector.</p>
<p>Self-correctors have several unique properties compared to typical generators. First, a self-corrector decouples generation and correction, allowing us to freely parameterize each module -for instance, by prompting a single language model or using two different language models. In this paper, we develop a framework to train a separate corrector model ( §2.1). We find that the resulting selfcorrector improves upon the generator alone ( §3), even when the corrector is much smaller ( §3.4).</p>
<p>Second, since the generator and the corrector are separated, we can keep the generator as a generalpurpose language model and train the corrector with different objectives for different task requirements. In §2.1, we propose a training algorithm for the corrector that is dedicated to improving generations, where the improvement can be in any aspect, measured by scalar values.</p>
<p>Third, the corrector can receive explicit feedback about intermediate generations to guide subsequent generations. Formally, p(y|x) = y0 p 0 (y 0 |x)p θ (y|y 0 , x, f (y 0 )), where f is the feedback. The feedback can be of many forms, e.g. a sentence, a compiler trace, etc. In contrast, a typical generator that generates in a single pass does not leverage feedback on its own generation. In this paper, we show that the corrector can learn to exploit explicit natural language feedback to achieve better performance ( §3.5). Next, we describe our training framework of the corrector.</p>
<p>LEARNING A CORRECTOR</p>
<p>Our goal is to have the generator generate an initial hypothesis, then improve the hypothesis with the corrector (Eq. 1). We train the corrector to improve the quality of a hypothesis, while staying as Figure 2: SELF-CORRECTIVE LEARNING iteratively trains a corrector by generating hypotheses and corrections, forming value-improving pairs, and selecting those with high similarity for learning.
Algorithm 1 Self-corrective learning input Generator p0, corrector p θ , prompts X, value v(·), feedback f (·)
Initialize datapool D by sampling from p0 Initialization: Eq. 2 for iteration ∈ {1, 2, . . .} do for x ∈ X do Sample hypotheses y from datapool D Generate corrections y ∼ p θ (·|y, x, f (y)) Add all (x, y , v(y ), f (y )) to the datapool D Exploration: Eq. 5 Form value-improving pairs P from D Pairing: Eq. 3 for step in 1, 2, . . . , M do Sample a batch of value-improving pairs from P using Eq. 4 Compute the loss and update θ using gradient descent Learning close as possible to the original hypothesis. Here, quality is measured with a scalar value function v(y) which we assume is accessible at training time (e.g. a classifier).</p>
<p>Since direct supervision on how to improve hypotheses is not available, we design a new algorithm to train the corrector, which we refer to as self-corrective learning. The algorithm collects a pool of generations, groups them and selects pairs of generation that increase in value and are nearby, then updates the corrector on these pairs. As training progresses, more generations are added to the pool using the current corrector. Algorithm 1 summarizes self-corrective learning, detailed below.</p>
<p>Initialization. Self-corrective learning begins with a generator p 0 (y 0 |x), a corrector p θ (y |y, x) , a set of training prompts X, and a value function v : Y → R. Optionally, we can use additional feedback f : Y → F and learn p θ (y |y, x, f (y)), where F is arbitrary.</p>
<p>The algorithm initializes a datapool of (input, output, value, feedback) examples by using the generator to generate multiple outputs for each input. Formally,
D x = {(x, y, v(y), f (y)) | for all y ∈ y 1:N ∼ q(p 0 (·|x))}, D = x∈X D x ,(2)
where y 1:N denotes N outputs generated with decoding algorithm q (e.g. temperature sampling). When available, (x, y, v(y), f (y)) examples from another source (e.g. a dataset) can also be added.</p>
<p>Pairing. Next, self-corrective learning forms value-improving pairs: examples of mapping a hypothesis to a higher-valued correction. We use the datapool D to form a set of (input, hypothesis, correction) pairs. A pair is formed when an output has a higher value than another 1 :
P x = {(x, y, y ) | v(y) &lt; v(y ) for all y, y ∈ D x × D x }, P = x∈X P x ,(3)
Learning. Next, self-corrective learning selects (input, hypothesis, correction) pairs to update the corrector with. We sample a (x, y, y ) pair proportional to its improvement in value as well as the proximity between the hypothesis y and the correction y :
P[(x, y, y )] ∝ exp α · (v(y ) − v(y)) improvement + β · s(y, y ) proximity /Z(y),(4)
where s(y, y ) is a similarity function and Z(y) normalizes over the available corrections for y in P x . Increasing the hyperparameter α ∈ R ≥0 puts more weight on targets that add more value, while increasing β ∈ R ≥0 retains more similar targets. We update the corrector using the cross-entropy loss L(θ) = − log p θ (y |y, x, f (y)) on batches sampled in this way.</p>
<p>Exploration.</p>
<p>During exploration, self-corrective learning adds new generations to the datapool by generating from the current corrector:
D x = {(x, y , v(y ), f (y )) | for all y ∈ y 1:N ∼ q(p θ (·|y, x, f (y))}, D = x∈X D x (5)
and updating the datapool D ← D ∪D . The hypotheses y to correct can come from any source, e.g. newly sampled from the base generator, or from the datapool; we use the latter in our experiments.</p>
<p>Inference. We use the trained corrector along with a generator to generate a trajectory y 0 , y 1 , . . . , y T , and consider y T the final output. Since marginalizing over the intermediate generations in Eq. 1 is intractable, we approximate each summation with a single sequence generated with a decoding algorithm q(·). That is, we decode from the generator, then repeatedly from the corrector:
• Generation: y 0 ∼ q(p 0 (y 0 |x)); • Correction: y t+1 ∼ q(p θ (y t+1 |y t , x, f (y t ))), t = 0, 1, . . . , T − 1.
The stopping time T is either fixed, or when a target value is obtained (if v(y) is available).</p>
<p>EXPERIMENTS</p>
<p>We evaluate SELF-CORRECTION on a diversity of tasks: mathematical program synthesis, in which generations are strictly correct or incorrect, and generators typically have low performance; lexically-constrained generation, which allows for partial credit, and generators usually give partially-correct solutions (e.g. matching 3 out of 5 constraints); and toxicity control, where 'correctness' is more loosely defined, and the output space is much more open-ended. Our experiments are organized to study three settings:</p>
<ol>
<li>
<p>Using self-correctors to improve upon generators ( §3.1,3.2,3.3).</p>
</li>
<li>
<p>Correcting generators that are much larger than the corrector ( §3.4). 3. Leveraging explicit feedback during training and inference ( §3.5).</p>
</li>
</ol>
<p>Next, we describe the self-correction setup and baselines for each task, along with their results. 2</p>
<p>MATHEMATICAL PROGRAM SYNTHESIS</p>
<p>First, we consider mathematical program synthesis (Austin et al., 2021;Mishra et al., 2022). Given a natural language problem specification x, the task is to generate a program y that upon execution returns the correct answer to x. The task is challenging as it draws on language understanding, multiple-step mathematical problem solving (e.g. identifying a solution strategy, decomposing a problem), and leveraging symbolic tools (e.g. built-in operations, variables). Furthermore, the task demands a high level of precision, e.g. a single misplaced operation makes the program incorrect.</p>
<p>Experimental setup. As the corrector we use GPT-Neo 1.3B , an open-source autoregressive language model. GPT-Neo is pre-trained on language and code , and hence is widely used for code-related generation (e.g. ; Ni et al. (2022); Mishra et al. (2022)). We consider two settings for the initial generator: (1) a separate fine-tuned Corrector:</p>
<p>a=2<em>100 b=3</em>50 c=500-a-b #fix answer=c print(answer) Figure 3: Grade-school-math (GSM) self-corrections. On the left, the corrector fixes the units (from minutes to hours) in the generator's solution. On the right, the corrector revises the logic so that the program computes the total savings instead of the spent on tickets. We add #fix here to indicate the change. See Figure 7 and Figure 8 for additional examples.</p>
<p>instance of GPT-Neo 1.3B, and (2) few-shot prompted GPT-3 (Brown et al., 2020). For GPT-3, we evaluate the davinci and text-davinci-002 engines, representative of large (≈ 175B 3 ) generators that are state-of-the-art in related tasks (Wei et al., 2022). See the Appendix for additional details.</p>
<p>Self-correction setup. As the value function we use correctness, which is 1 when the program y executes and outputs the ground-truth answer and 0 otherwise. Our main experiments do not use explicit feedback, i.e. f (y) = ∅. At inference time, we study two settings for the corrector: (1) applying k corrections and selecting the final generation, (2) an oracle setting that only corrects a draft if the draft is incorrect. We use greedy decoding for the generator and corrector, and k = 1.</p>
<p>Datasets. We evaluate on problems from 5 problem solving datasets: MultiArith (Roy et al., 2015), AddSub (Hosseini et al., 2014), SingleOp (Roy et al., 2015), SVAMP (Patel et al., 2021), and GSM8k (Cobbe et al., 2021). As in prior work (Austin et al., 2021;Ni et al., 2022;Mishra et al., 2022), we frame these as program synthesis by converting their solutions to Python programs. 4 We separate our experiments into three increasingly difficult settings:</p>
<ol>
<li>MultiArith, using problems from the MultiArith arithmetic word problem dataset. 2. Multitask, using problems from 4 arithmetic datasets (MultiArith, AddSub, SingleOp, SVAMP). 3. GSM, using problems from the challenging GSM8k dataset.</li>
</ol>
<p>For the MultiArith and Multitask settings, we make train/valid/test splits using 60/20/20% of the respective datasets. Similar to Ni et al. (2022), for the GSM setting we use the official GSM8k test split, and create a validation split using 20% of the training set. Note that the problems and answers in all datasets are the same as those from the original non-program datasets.  Baselines. We compare SELF-CORRECT with its baseline generator (GPT-Neo 1.3B) in all three settings. For the GSM setting, we compare with existing work that uses models within the same magnitude of scale, including NEO FCP+PCP (Ni et al., 2022), which tunes GPT-NEO 2.7B with additional self-sampled programs, and their fine-tuned GPT-NEO 2.7B baseline. We also report 3B and 6B fine-tuned GPT3-like language models from Cobbe et al. (2021), which were trained on the non-program version of GSM8k. We evaluate larger models later in ( §3.4).</p>
<p>Results. As seen in Table 1, the self-corrector improves upon the generator in all three settings, using either inference strategy: always correcting (SELF-CORRECT), or only correcting incorrect solutions (SELF-CORRECT * ). The self-corrector's performance on Multiarith is very high after correction (98-99%), a 38 point improvement over the generator, with a similar gain in the Multitask arithmetic setting. On the challenging GSM dataset, the self-corrector achieves 21%, and 24% with only correcting incorrect solutions, up from 8.57% for the generator. Notably, this is higher than previous work based on the larger 2.7B GPT-Neo, or larger models tuned on the language version of GSM.</p>
<p>The results show that self-corrective learning can improve task performance via training a corrector. Qualitatively, the self-corrector can correct values in a correctly structured solution, fix the order of operations within a multistep solution, adjust unit conversions, and make larger multipart revisions (see Figures 3,7,8). Notably, these are learned automatically through self-corrective learning.</p>
<p>LEXICALLY CONSTRAINED GENERATION</p>
<p>Next, we consider lexically constrained generation. Given a set of constraint words x, the task is to generate a sentence y that includes all the given constraints. Faithful constraint satisfaction is crucial for many downstream tasks, e.g., those that require converting information to text (McKeown, 1985).</p>
<p>Datasets and Metrics. We experiment on COMMONGEN (Lin et al., 2020) and E2E (Novikova et al., 2017). COMMONGEN is a benchmark for generative commonsense reasoning where the task is to generate a coherent sentence given a set of words (e.g., dog, catch). E2E involves converting structured inputs into natural language. For both tasks, we report standard metrics including human/automatic measures of fluency (BLEU, CIDER, etc.) as well as constraint coverage. We collect human measures of fluency on Amazon Mechanical Turk; see the Appendix for details.</p>
<p>Setup. We parameterize the base generator with GPT-2 Radford et al. (2019) (large-size for COM-MONGEN and medium-size for E2E). We fine-tuned the generator for each task. As the value function for self-corrective learning we use coverage, i.e. the percentage of constraints that are present in the output. For inference, we use beam search with the generator, then do up to 3 corrections using beam search, stopping early if all constraints are met. See the Appendix for additional details.</p>
<p>Results. Table 2 shows the evaluation results. The self-corrector substantially improves constraint coverage over its GPT-2 generator for both tasks, while maintaining or improving its language quality. On the COMMONGEN benchmark, the self-corrector paired with the NeuroLogic constrained decoding algorithm  achieves the best results, outperforming the more sophisticated NeuroLogic-A<em> decoding algorithm, while being an order of magnitude faster. Notably, on E2E, self-correction outperforms Neurologic-A</em> decoding, despite only using standard beam search. This suggests that a corrector can be viewed as an alternative to using a more sophisticated decoding procedure (A*) for improving performance without modifying the underlying model. See Figure 9 for qualitative examples.  Table 3: Toxicity reduction. GPT-2 is the base generator. Figure 4: Applying multiple corrections reduces toxicity.</p>
<p>TOXICITY REDUCTION</p>
<p>Next, we consider the task of toxicity reduction (Gehman et al., 2020;Liu et al., 2021). Given a prompt x, the task is to generate a fluent continuation y while avoiding offensive content. This task is important for ensuring safe language model deployment, yet challenging: due to misaligned pretraining objectives (i.e. modeling internet text vs. non-toxic text), language models are susceptible to generating toxic completions, even when prompted with seemingly innocuous text (Gehman et al., 2020). Along with its practical importance, the task tests whether (self-)correctors can be an effective mechanism for controlling the outputs of language models in an open-ended setting.</p>
<p>Datasets and Metrics. We use the REALTOXICITYPROMPTS benchmark (Gehman et al., 2020) which contains 100k prompts designed to elicit toxic generations. Following the experimental setup of Liu et al. (2021), during training we use 85K prompts from the training set, and for evaluation we use the same 10K non-toxic prompts from test set as Liu et al. (2021). We use Perspective API to measure maximum toxicity, defined as the average maximum toxicity over 25 sampled generations, and the (empirical) toxicity probability of at least 1 out of 25 generations being toxic.</p>
<p>Baselines. We compare SELF-CORRECT with its generator (GPT-2) and previously reported baselines from Lu et al. (2022a), including PPLM (Dathathri et al., 2020), GeDi (Krause et al., 2021), DExpert , DAPT (Gururangan et al., 2020), PPO (Lu et al., 2022a), and Quark (Lu et al., 2022a). The latter two -Proximal Policy Optimization (PPO) and Quantized Reward Konditioning (Quark) -represent strong, state-of-the art approaches based on reinforcement learning.</p>
<p>Setup. We use the off-the-shelf GPT-2 Large as the generator, and finetune another GPT-2 Large as the corrector. During inference, we use nucleus sampling with p = 0.9 to generate 25 samples for all baselines. As the value function, we use the Perspective API score, v(y) ∈ [0, 1], which measures the toxicity of the completed sequence. We do up to three corrections with the corrector model.</p>
<p>Results</p>
<p>. Table 3 shows that SELF-CORRECT reduces the rate of toxic generations substantially, while also maintaining fluency and diversity. SELF-CORRECT outperforms all baselines. This includes inference-time algorithms (PPLM, GeDi, DExpert), which do not modify the generator but degrade fluency and yield higher toxicity compared to SELF-CORRECT, as well as reinforcement learning methods (PPO, Quark) that adjust the generator using toxicity as a (negative) reward. The results show that SELF-CORRECT is effective for detoxification, without having to modify the underlying generator. We study implications of this latter property further in the next section.</p>
<p>CHANGING MODULES -CORRECTING GPT-3</p>
<p>Next, we show that a self-corrector can improve the outputs of a generator that is much larger than the corrector. We consider two cases: (1) training with a small generator, then swapping in the larger generator at test time;</p>
<p>(2) training with the larger generator, i.e. using the large generator to initialize the datapool for self-corrective learning, then using the large generator at test time.</p>
<p>Toxicity. We evaluate case (1) for reducing the toxicity of a large generator (GPT-2 XL, GPT-3). We generate an initial sequence using the large generator, then refine it with our corrector trained in the previous experiments ( §3.3). Table 4 shows that the resulting self-corrector (large generator  Table 4: Modularity (program synthesis and detoxification). Self-correctors can correct very large generators, either by swapping in the generator at test-time, or training with the generator. For math synthesis, the corrector is GPT-Neo 1.3B, and here we only correct incorrect outputs. For detoxification, the correction is GPT2-L, and we correct all the outputs.</p>
<ul>
<li>corrector) has substantially reduced toxicity compared to the large generator. This shows the promise of using (self-)correctors for controlling the outputs of large language models.</li>
</ul>
<p>Math program synthesis. Table 4 shows results for math. Analogous to toxicity, the corrector is able to correct larger generators swapped in at test-time. For instance, the GPT-3 Instruct generator has quite high performance (84.90 Multitask, 36.80 GSM), which improves to 90.90 and 45.00, respectively, by adding in a corrector. The self-corrector (large generator + corrector) improves further by training with the GPT-3 Instruct generator, to 92.75 and 45.92, respectively.</p>
<p>LEVERAGING EXPLICIT FEEDBACK</p>
<p>Next, we demonstrate SELF-CORRECT's capacity to incorporate explicit natural language feedback. This amounts to defining a feedback function f , then using the same self-corrective learning and inference algorithms ( §2.1) as in our preceding experiments (in those experiments, f returned ∅).</p>
<p>We show that correctors learn to use the feedback, as evidenced by higher performance.</p>
<p>Toxicity. We use additional fine-grained information from the toxicity API as natural language feedback. Specifically, besides the overall toxicity score, Perspective API also provides scores for fine-grained attributes of toxicity (e.g. identity attack, profanity, flirtation, etc.). At training time, we compare the attribute scores from a hypothesis and its selected correction, and use the attribute with the largest decrease as natural language feedback (e.g. "decrease toxicity in profanity"). At inference time, we call the API on the current hypothesis, and use the attribute with the highest score. Here we use the API at inference time, which is not required in our previous experiments.</p>
<p>Lexical constraints. In training time, we generate natural language feedback for every example pair (x, y, y ) by elaborating the extra lexical constraints satisfied by y but not y. e.g. "adding constraint word: read". At inference time, we elaborate all missing constraints in the current hypothesis.</p>
<p>Math program synthesis. Math program synthesis contains a variety of problem types and errors, without an automated means for identifying the errors (e.g. an API). We explore obtaining natural language feedback about the current program by prompting a large language model. We prompt the model with a problem, hypothesis program, a gold solution, and few-shot demonstrations that show feedback on one part of the program; e.g. In the initial guess, 3 should be subtracted. When the program is correct, the feedback is Correct. At inference time, we also use feedback from the language model. We allow the feedback model access to a gold solution, which we expect makes the feedback higher quality, with the risk of solution leakage at inference-time. Our results in this task are thus used only to study the feasibility of explicit feedback for math program synthesis.</p>
<p>Setup. For toxicity, lexical constraints, and math we use REALTOXICITYPROMPTS, COMMONGEN, and the MULTITASK arithmetic setting, respectively. We follow the setup of each task's previous experiments ( §3.3, §3.2, §3.1), except for math we use 5 correction iterations (previously 1). For math, we use GPT-3 (text-davinci-002) with 6 demonstrations as the feedback model.  Generator (GPT-Neo):</p>
<p>answer=(6.0*8.0) print(answer)</p>
<p>Feedback (GPT-3):</p>
<h1>In the initial guess, # 2 tickets are not included.</h1>
<p>Corrector (GPT-Neo):</p>
<p>answer=(6.0*(8.0+2.0)) print(answer) Figure 5: Self-correction with natural language feedback.</p>
<p>Results. Table 5 shows that explicit natural language feedback improves performance in all three tasks. For toxicity, this means that providing fine-grained attributes (e.g. identity attack, profanity, etc.) during learning and inference improves upon using only the scalar toxicity score. Intuitively, feedback may help the model to focus on a useful correction; e.g., see Figure 5.</p>
<p>ADDITIONAL ABLATIONS AND ANALYSIS</p>
<p>Effect of multiple corrections. Previously, Figure 4 showed that multiple corrections led to better toxicity reduction. On math (Multitask setting), Figure 6 shows that performance improves with more than one correction, and that multiple corrections are more beneficial with feedback. Intuitively, in this math task, after 2-3 corrections the model needs additional guidance.</p>
<p>Effect of pairing and proportional sampling. Self-corrective learning (i) samples pairs for learning proportional to Equation 4, (ii) only pairs sequences that improve value. We ablate these features by training on Multitask using a data pool that samples a pair for learning uniformly (rather than Equation 4), and a data pool without value pairing. Table 6 shows that both improve performance.</p>
<p>Effect of exploration.</p>
<p>To ablate the effect of exploration, we train a baseline only on correction pairs induced from the base generator. Table 7 shows results on the three math datasets, indicating that exploration improves performance.</p>
<p>RELATED WORK</p>
<p>Self-correction relates to recent works on editing text, including modeling Wikipedia edits (Reid &amp; Neubig, 2022;Faltings et al., 2021;Schick et al., 2022), which relies on supervised edits, unsupervised methods (Miao et al., 2019;) that perturb sequences with simple operations (e.g. insertion, deletion), editing with models trained on human-written critiques (Saunders et al., 2022), or iteratively updating continuous variables Li et al., 2022;Qin et al., 2022).    Table 7: Effect of exploration on program synthesis.</p>
<p>In contrast to these, self-correction learns an expressive text-to-text corrector that is trained online to improve a quality measure, without requiring a supervised dataset of edits or critiques. Separately, denoising ground-truth sequences is a common pretraining objective (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020), while self-correction 'denoises' generations to improve a scalar quality measure. Scalar measures are often improved with reinforcement learning (RL) on a base generator (Ziegler et al., 2019;Stiennon et al., 2020;Lu et al., 2022a), which is infeasible for improving many language models (e.g. those accessed through an API), and uses only scalar feedback. Moreover, self-correction learns the difference between a generation and solution, and is complementary to RL-tuned generators, which can be used within a self-corrector. Finally, self-correction decomposes generation into multiple steps, which relates to methods that generate rationales before a response (Wei et al., 2022;Dohan et al., 2022). Self-correction also produces intermediate steps, but each step is of the same form as the output, allowing for re-using previous generations.</p>
<p>CONCLUSION</p>
<p>We introduced self-correctors, a class of models that decompose generation into initial generation and correction steps. We study self-correctors with a fixed base generator along with a corrector trained to improve outputs according to a scalar measure of quality. We presented a simple, general procedure for training the corrector, and find that self-correction is applicable and effective for improving performance, and controlling the outputs of both small and large generators. Moreover, we found that self-correction along with our learning framework provides a promising mechanism for using natural language feedback to improve generation. These findings, along with exploring alternative self-correctors, open up many avenues that we leave for future work.</p>
<p>APPENDIX A ADDITIONAL EXPERIMENTAL DETAILS</p>
<p>A.1 MATHEMATICAL PROGRAM SYNTHESIS</p>
<p>We fine-tune a separate instance of GPT-Neo 1.3B as an initial generator, using the Huggingface library with default hyperparameters, except for evaluation steps, which we set to a small number to ensure a strong checkpoint is selected for each dataset. We use the finetuned initial generator as initialization for the corrector, and tune the corrector on sequences [SC]x[CURR]yi[START]yj [END], where x is a problem, y i and y j form a residual pair, and [·] are special tokens. The loss is on tokens after [START].</p>
<p>Feedback. We write 6 demonstrations using training problems and generations from our GPT-Neo base generator, and use GPT-3 (text-davinci-002) as a feedback model. We use the same training procedure and hyperparameters, except that the sequences now include feedback, [SC]x</p>
<p>[CURR]yi[FEEDBACK]F(x,yi)[START]yj[END]</p>
<p>, where x is a problem, y i and y j form a residual pair, and F (x, y i ) is feedback. We include loss on tokens after [FEEDBACK].</p>
<p>A.2 LEXICALLY-CONSTRAINED GENERATION</p>
<p>Hyper-parameters. Table 8 and Table 9 show hyperparameters for CommonGen and E2E.</p>
<p>Human Evaluation. We evaluate fluency of generations in E2E task using human annotators on Amazon Mechanical Turk (AMT). We randomly sampled 100 instances, along with generations of different baselines and self-corrections. For each instance, we ask 3 annotators to evaluate the fluency of generations on a 3-point Likert scale. We aggregate annotations from 3 annotators using majority vote. We restricted the pool of annotators to those who are located in US or CA, and had 98% approval rate for at least 5,000 previous annotations.</p>
<p>Hyperparameter Assignment</p>
<p>Predictor GPT-2Large # steps 6000 batch size 128 optimizer Adam learning rate 1.e − 5 decoding alg.</p>
<p>beam search (k=5)     C QUALITATIVE EXAMPLES</p>
<p>Figure 6 :
6Math: multiple corrections.</p>
<p>Table 2 :
2Lexically-constrained generation. By training a corrector to optimize constraint satisfaction, SELF-CORRECT improves constraints while maintaining fluency, without modifying the underlying generator. Due to space, we show CIDER for COMMONGEN and human judgement for E2E as measures of fluency. Other metrics show similar trends and can be found in the Appendix.</p>
<p>Table 5 :
5Explicit natural language feedback. Correct * means only correcting incorrect outputs.Problem: 
Melanie had 19 dimes in her bank. Her dad 
gave her 39 dimes and her mother gave her 25 
dimes. How many dimes does Melanie have 
now? </p>
<p>Generator (GPT-Neo): </p>
<p>answer = 19 + 25 
print(answer) </p>
<p>Feedback (GPT-3): </p>
<h1>In the initial guess,</h1>
<h1>39 is not included.</h1>
<p>Corrector (GPT-Neo): </p>
<p>answer = 19 + 25 + 39 
print(answer) </p>
<p>Problem: 
Lana's favorite band was holding a concert 
where tickets were 6 dollars each. Lana 
bought 8 tickets for herself and her friends and 
2 extra tickets in case anyone else wanted to 
go. How much did she spend? </p>
<p>Table 6 :
6Effect of pairing and proportional sampling.Exploration Multiarith Multitask GSM8k </p>
<p>89.20 
73.49 
17.60 </p>
<p>99.17 
78.24 
23.96 </p>
<p>Table 8 :
8Hyperparameters for COMMONGEN.Hyperparameter 
Assignment </p>
<p>Predictor 
GPT-2 M edium </p>
<h1>steps</h1>
<p>10000 
batch size 
100 
optimizer 
Adam 
learning rate 
1.e − 5 
decoding alg. 
beam search (k=5) </p>
<p>Table 9 :
9Hyperparameters for E2E.Avg. Max. Prob. Perplexity dist-2 dist-3B ADDITIONAL RESULTS </p>
<p>Toxicity 
Fluency 
Diversity </p>
<p>GPT2-L 
0.527 
0.520 
11.31 
0.85 
0.85 
SELF-CORRECT 
0.171 
0.026 
11.81 
0.80 
0.83 
SELF-CORRECT + FEEDBACK 
0.156 
0.020 
11.86 
0.80 
0.83 </p>
<p>Table 10 :
10Evaluation results of toxicity reduction experiments with natural language feedback.Bleu-4 CIDER Coverage 
Runtime </p>
<p>NeuroLogic [22] 
26.70 
14.70 
97.70 
2.04s/sent 
NeuroLogic-A*esque [24] 
28.20 
15.20 
97.80 
19.24s/sent </p>
<p>GPT-2 
27.90 
14.97 
91.38 
0.2s/sent 
SELF-CORRECT 
27.98 
15.30 
94.58 
0.8s/sent 
SELF-CORRECT + feedback 
27.82 
15.24 
95.88 
0.8s/sent 
SELF-CORRECT+NeuroLogic 
28.17 
15.28 
97.80 
2.24s/sent </p>
<p>Table 11 :
11Evaluation rresults of lexically-constrained generation on COMMONGEN.Coverage BLEU-4 NIST R-L METEOR CIDER </p>
<p>PREFIX-TUNING (Li &amp; Liang, 2021) 91.16 
70.30 
8.82 72.10 
46.30 
2.46 </p>
<p>GPT-2 
91.50 
67.12 8.67 70.25 
45.58 
2.33 
SELF-CORRECT 
98.77 
68.81 
8.78 68.60 
45.11 
2.38 </p>
<p>Table 12 :
12Evaluation results of lexically-constrained generation on E2E.
We also store the value and feedback for y and y along with (x, y, y ), which we omit to reduce clutter.
Code will be publicly available upon acceptance.
Estimated size of davinci (https://blog.eleuther.ai/gpt3-model-sizes). Further details not available.4  We use data from the Lila benchmark (https://github.com/allenai/Lila).
ACKNOWLEDGMENTSThis work was funded in part by the DARPA MCS program through NIWC Pacific (N66001-19-2-4031), and the Allen Institute for AI.Problem:Ralph watches TV for 4 hours a day from Monday to Friday, and 6 hours a day on Saturday and Sunday. How many hours does Ralph spend watching TV in one week?Generator: a=4<em>7 b=6</em>7 c=a+b answer=c print(answer)Corrector: a=4<em>5 b=6</em>2 c=a+b answer=c print(answer)Problem:The pirates plan to explore 4 islands. Two islands require walking 20 miles per day while the other two islands require 25 miles per day. How many miles will they have to walk if it takes 1.5 days to explore each island?Generator: a=20<em>2 b=25</em>2 c=a+b d=c<em>1.5 e=d+b answer=e print(answer)Corrector: a=20</em>2 b=25<em>2 c=a+b d=c</em>1.5 answer=d print(answer)Figure 7: Grade school math (GSM) self-corrections. Left: the structure of the generator's solution is valid, but it incorrectly uses the total number of days in a week for both a and b; the corrector fixes the program to correctly account for the 5 weekdays and 2 weekend days. Right: the generator's solution contains an incorrect addition at the end; the corrector removes this line, resulting in a correct program.Problem:A spiral notebook costs 15, and a personal planner costs $10. How much would it cost in total to buy 4 spiral notebooks and 8 personal planners at a 20% discount?Figure 8: Grade school math (GSM) self-corrections. Left: the generator's program doesn't include the discount; the corrector appends the discount to the program. Right: a more sophisticated multipart correction. The generator's assignment of b (line 2), and addition to e (line 5) are incorrect. The corrector removes these lines and adjusts the variable names accordingly.Constraints:'table', 'paper', 'read' Generator: A man is reading book on a table.Corrector:A man is reading a book on a table and writing on a piece of paper.Constraints:'stand', 'wait', 'look' Generator: He waits for someone to look at him. Corrector: He waits for someone to stand and look him in the eye.Constraints:'walk', 'park', 'bench', 'dog', 'sit' Generator: A dog sitting on a bench in a park.Corrector:A dog sitting on a bench and a man walking a dog in a park.Constraints:name : The Mill | Type : restaurant | food : English | price : high | customer rating : average | area : riverside | family friendly : yes | near : Cafe Rouge Generator:The Mill is an expensive, three star restaurant located near Cafe Rouge.Corrector:The Mill is a children friendly English restaurant in the riverside area near Cafe Rouge. It has an average customer rating and a high price range.  Prompt: Waters then went on to call the Trump Generator: administration a "bunch of idiots" and "insolent clowns [who are] running" Corrector: Administration's support for the Trans-Pacific Partnership on trade "a total non-event."Figure 10: Toxicity control self-corrections. Words in red indicate toxic language.Constraints
Program synthesis with large language models. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J Cai, Michael Terry, Quoc V Le, Charles Sutton, abs/2108.07732ArXiv. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. ArXiv, abs/2108.07732, 2021.</p>
<p>. Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman, Gpt-Neo, 10.5281/zenodo.5297715doi: 10.5281/ zenodo.5297715Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. ZenodoIf you use this software, please cite it using these metadataSid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Au- toregressive Language Modeling with Mesh-Tensorflow. Zenodo, March 2021. doi: 10.5281/ zenodo.5297715. URL https://doi.org/10.5281/zenodo.5297715. If you use this software, please cite it using these metadata.</p>
<p>. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel MTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M.</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv, abs. Jeff Ziegler, Clemens Wu, Christopher Winter, Mark Hesse, Eric Chen, Mateusz Sigler, Scott Litwin, Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlishZiegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad- ford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020.</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such. Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech ZarembaBob Mc-Grew. Evaluating large language models trained on code. arXivMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fo- tios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob Mc- Grew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. arXiv, 2021.</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv. org/abs/2110.14168.</p>
<p>Plug and play language models: A simple approach to controlled text generation. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, Rosanne Liu, abs/1912.02164ArXiv. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosin- ski, and Rosanne Liu. Plug and play language models: A simple approach to controlled text generation. ArXiv, abs/1912.02164, 2020.</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: //aclanthology.org/N19-1423.</p>
<p>. David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A Saurous, abs/2207.10342Language model cascades. ArXiv. Jascha Narain Sohl-DicksteinDavid Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Narain Sohl-Dickstein, Kevin Murphy, and Charles Sutton. Language model cascades. ArXiv, abs/2207.10342, 2022.</p>
<p>Text editing by command. Felix Faltings, Michel Galley, Gerold Hintz, Chris Brockett, Chris Quirk, Jianfeng Gao, Bill Dolan, 10.18653/v1/2021.naacl-main.414Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnlineAssociation for Computational LinguisticsFelix Faltings, Michel Galley, Gerold Hintz, Chris Brockett, Chris Quirk, Jianfeng Gao, and Bill Dolan. Text editing by command. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5259-5274, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.naacl-main.414. URL https://aclanthology.org/2021.naacl-main.414.</p>
<p>The pile: An 800gb dataset of diverse text for language modeling. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, Connor Leahy, Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling, 2021. URL https://arxiv.org/ abs/2101.00027.</p>
<p>RealToxici-tyPrompts: Evaluating neural toxic degeneration in language models. Suchin Samuel Gehman, Maarten Gururangan, Yejin Sap, Noah A Choi, Smith, 10.18653/v1/2020.findings-emnlp.301Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational LinguisticsSamuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. RealToxici- tyPrompts: Evaluating neural toxic degeneration in language models. In Findings of the Asso- ciation for Computational Linguistics: EMNLP 2020, pp. 3356-3369, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.301. URL https://aclanthology.org/2020.findings-emnlp.301.</p>
<p>Don't stop pretraining: Adapt language models to domains and tasks. Ana Suchin Gururangan, Swabha Marasović, Kyle Swayamdipta, Iz Lo, Doug Beltagy, Noah A Downey, Smith, 10.18653/v1/2020.acl-main.740Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsSuchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. Don't stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 8342-8360, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/ 2020.acl-main.740. URL https://aclanthology.org/2020.acl-main.740.</p>
<p>Learning to solve arithmetic word problems with verb categorization. Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, Nate Kushman, 10.3115/v1/D14-1058Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, QatarAssociation for Computational LinguisticsMohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 523-533, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1058. URL https://aclanthology.org/D14-1058.</p>
<p>GeDi: Generative discriminator guided sequence generation. Ben Krause, Akhilesh Deepak Gotmare, Bryan Mccann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, Nazneen Fatema Rajani, 10.18653/v1/2021.findings-emnlp.424Findings of the Association for Computational Linguistics: EMNLP 2021. Punta Cana, Dominican RepublicAssociation for Computational LinguisticsBen Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, and Nazneen Fatema Rajani. GeDi: Generative discriminator guided sequence generation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 4929-4952, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.424. URL https://aclanthology.org/2021. findings-emnlp.424.</p>
<p>Iterative refinement in the continuous space for non-autoregressive neural machine translation. Jason Lee, Raphael Shu, Kyunghyun Cho, 10.18653/v1/2020.emnlp-main.73Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsJason Lee, Raphael Shu, and Kyunghyun Cho. Iterative refinement in the continuous space for non-autoregressive neural machine translation. In Proceedings of the 2020 Conference on Em- pirical Methods in Natural Language Processing (EMNLP), pp. 1006-1015, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.73. URL https://aclanthology.org/2020.emnlp-main.73.</p>
<p>BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, 10.18653/v1/2020.acl-main.703Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsOnlineMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871-7880, On- line, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL https://aclanthology.org/2020.acl-main.703.</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, doi: 10.18653/ v1/2021.acl-long.353Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics1Long Papers)Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582-4597, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/ v1/2021.acl-long.353. URL https://aclanthology.org/2021.acl-long.353.</p>
<p>Diffusionlm improves controllable text generation. Lisa Xiang, John Li, Ishaan Thickstun, Percy Gulrajani, Tatsunori Liang, Hashimoto, abs/2205.14217ArXiv. Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion- lm improves controllable text generation. ArXiv, abs/2205.14217, 2022.</p>
<p>CommonGen: A constrained text generation challenge for generative commonsense reasoning. Wangchunshu Bill Yuchen Lin, Ming Zhou, Pei Shen, Chandra Zhou, Yejin Bhagavatula, Xiang Choi, Ren, 10.18653/v1/2020.findings-emnlp.165Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational LinguisticsBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. CommonGen: A constrained text generation challenge for generative com- monsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1823-1840, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.165. URL https://aclanthology.org/2020. findings-emnlp.165.</p>
<p>DExperts: Decoding-time controlled text generation with experts and antiexperts. Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A Smith, Yejin Choi, 10.18653/v1/2021.acl-long.522Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline1Association for Computational LinguisticsAlisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, and Yejin Choi. DExperts: Decoding-time controlled text generation with experts and anti- experts. In Proceedings of the 59th Annual Meeting of the Association for Computational Lin- guistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 6691-6706, Online, August 2021. Association for Computational Linguis- tics. doi: 10.18653/v1/2021.acl-long.522. URL https://aclanthology.org/2021. acl-long.522.</p>
<p>Unsupervised paraphrasing by simulated annealing. Xianggen Liu, Lili Mou, Fandong Meng, Hao Zhou, Jie Zhou, Sen Song, 10.18653/v1/2020.acl-main.28Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsXianggen Liu, Lili Mou, Fandong Meng, Hao Zhou, Jie Zhou, and Sen Song. Unsupervised para- phrasing by simulated annealing. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 302-312, Online, July 2020. Association for Computational Lin- guistics. doi: 10.18653/v1/2020.acl-main.28. URL https://aclanthology.org/2020. acl-main.28.</p>
<p>NeuroLogic decoding: (un)supervised neural text generation with predicate logic constraints. Ximing Lu, Peter West, Rowan Zellers, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, 10.18653/v1/2021.naacl-main.339Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnlineAssociation for Computational LinguisticsXiming Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. NeuroLogic decoding: (un)supervised neural text generation with predicate logic constraints. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4288-4299, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.339. URL https://aclanthology.org/2021.naacl-main.339.</p>
<p>Ximing Lu, Sean Welleck, Liwei Jiang, Jack Hessel, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi. Quark: Controllable text generation with reinforced unlearning. Ximing Lu, Sean Welleck, Liwei Jiang, Jack Hessel, Lianhui Qin, Peter West, Prithviraj Am- manabrolu, and Yejin Choi. Quark: Controllable text generation with reinforced unlearning.</p>
<p>. Corr, 10.48550/arXiv.2205.13636CoRR, abs/2205.13636, 2022a. doi: 10.48550/arXiv.2205.13636. URL https://doi.org/ 10.48550/arXiv.2205.13636.</p>
<p>NeuroLogic a<em>esque decoding: Constrained text generation with lookahead heuristics. Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Lianhui Ronan Le Bras, Youngjae Qin, Rowan Yu, Noah A Zellers, Yejin Smith, Choi, 10.18653/v1/2022.naacl-main.57Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational LinguisticsXiming Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Ronan Le Bras, Lianhui Qin, Youngjae Yu, Rowan Zellers, Noah A. Smith, and Yejin Choi. NeuroLogic a</em>esque decoding: Constrained text generation with lookahead heuristics. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, pp. 780-799, Seattle, United States, July 2022b. As- sociation for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.57. URL https: //aclanthology.org/2022.naacl-main.57.</p>
<p>Text Generation. Kathleen Mckeown, 10.1017/CBO9780511620751Studies in Natural Language Processing. Cambridge University PressKathleen McKeown. Text Generation. Studies in Natural Language Processing. Cambridge Univer- sity Press, 1985. doi: 10.1017/CBO9780511620751.</p>
<p>Cgmh: Constrained sentence generation by metropolis-hastings sampling. Ning Miao, Hao Zhou, Lili Mou, Rui Yan, Lei Li, 10.1609/aaai.v33i01.33016834Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Ning Miao, Hao Zhou, Lili Mou, Rui Yan, and Lei Li. Cgmh: Constrained sentence generation by metropolis-hastings sampling. Proceedings of the AAAI Conference on Artificial Intelligence, 33 (01):6834-6842, Jul. 2019. doi: 10.1609/aaai.v33i01.33016834. URL https://ojs.aaai. org/index.php/AAAI/article/view/4659.</p>
<p>Lila: A unified benchmark for mathematical reasoning. Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, Ashwin Kalyan, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, and Ashwin Kalyan. Lila: A unified benchmark for mathematical reasoning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022.</p>
<p>Learning from self-sampled correct and partially-correct programs. Ansong Ni, Jeevana Priya Inala, Chenglong Wang, Oleksandr Polozov, Christopher Meek, Dragomir Radev, Jianfeng Gao, Ansong Ni, Jeevana Priya Inala, Chenglong Wang, Oleksandr Polozov, Christopher Meek, Dragomir Radev, and Jianfeng Gao. Learning from self-sampled correct and partially-correct programs, 2022. URL https://arxiv.org/abs/2205.14318.</p>
<p>The E2E dataset: New challenges for end-to-end generation. Jekaterina Novikova, Ondřej Dušek, Verena Rieser, 10.18653/v1/W17-5525Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue. the 18th Annual SIGdial Meeting on Discourse and DialogueSaarbrücken, GermanyAssociation for Computational LinguisticsJekaterina Novikova, Ondřej Dušek, and Verena Rieser. The E2E dataset: New challenges for end-to-end generation. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Di- alogue, pp. 201-206, Saarbrücken, Germany, August 2017. Association for Computational Lin- guistics. doi: 10.18653/v1/W17-5525. URL https://aclanthology.org/W17-5525.</p>
<p>Are NLP models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, 10.18653/v1/2021.naacl-main.168Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnlineAssociation for Computational LinguisticsArkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2080- 2094, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. naacl-main.168. URL https://aclanthology.org/2021.naacl-main.168.</p>
<p>Cold decoding: Energy-based constrained text generation with langevin dynamics. Lianhui Qin, Sean Welleck, Daniel Khashabi, Yejin Choi, arXiv:2202.11705arXiv preprintLianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi. Cold decoding: Energy-based con- strained text generation with langevin dynamics. arXiv preprint arXiv:2202.11705, 2022.</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 189Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.</p>
<p>Exploring the limits of transfer learning with a unified text-totext transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 21140Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to- text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. URL http: //jmlr.org/papers/v21/20-074.html.</p>
<p>Learning to model editing processes. Machel Reid, Graham Neubig, Machel Reid and Graham Neubig. Learning to model editing processes, 2022. URL https: //openreview.net/forum?id=1bEaEzGwfhP.</p>
<p>Reasoning about quantities in natural language. Subhro Roy, Tim Vieira, Dan Roth, Transactions of the Association for Computational Linguistics. 3Subhro Roy, Tim Vieira, and Dan Roth. Reasoning about quantities in natural language. Transac- tions of the Association for Computational Linguistics, 3:1-13, 2015.</p>
<p>Self-critiquing models for assisting human evaluators. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, Jan Leike, William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators, 2022. URL https://arxiv. org/abs/2206.05802.</p>
<p>Peer: A collaborative language model. Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, Sebastian Riedel, Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. Peer: A collabora- tive language model, 2022. URL https://arxiv.org/abs/2208.11663.</p>
<p>Learning to summarize with human feedback. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul F Christiano, Advances in Neural Information Processing Systems. H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. LinCurran Associates, Inc33Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feed- back. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Ad- vances in Neural Information Processing Systems, volume 33, pp. 3008-3021. Curran Asso- ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 1f89885d556929e98d3ef9b86448f951-Paper.pdf.</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, abs/2201.11903ArXiv. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903, 2022.</p>
<p>M Daniel, Nisan Ziegler, Jeffrey Stiennon, Tom B Wu, Alec Brown, Dario Radford, Paul Amodei, Geoffrey Christiano, Irving, arXiv:1909.08593Fine-tuning language models from human preferences. arXiv preprintDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. URL https://arxiv.org/abs/1909.08593.</p>            </div>
        </div>

    </div>
</body>
</html>
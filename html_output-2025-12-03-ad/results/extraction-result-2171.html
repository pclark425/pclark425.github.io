<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2171 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2171</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2171</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-57.html">extraction-schema-57</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <p><strong>Paper ID:</strong> paper-279075076</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.01372v2.pdf" target="_blank">AI Scientists Fail Without Strong Implementation Capability</a></p>
                <p><strong>Paper Abstract:</strong> The emergence of Artificial Intelligence (AI) Scientist represents a paradigm shift in scientific discovery, with large language models (LLMs) taking the lead as the primary executor in the entire scientific workflow from idea generation to experiment implementation. Recent AI Scientist studies demonstrate sufficient capabilities for independent scientific discovery, with the generated research reports gaining acceptance at the ICLR 2025 workshop and ACL 2025, arguing that a human-level AI Scientist, capable of uncovering phenomena previously unknown to humans, may be imminent. Despite this substantial progress, AI Scientist has yet to produce a groundbreaking achievement in the domain of computer science on par with automated scientific tools. Based on extensive quantitative evidence from existing benchmarks in complex engineering tasks and a systematic evaluation assess 28 research papers generated by five advanced AI Scientist systems, we argue that \textbf{the fundamental bottleneck for AI Scientists lies in their capability to execute the requisite verification procedures.} Current AI Scientist systems lack the execution capabilities needed to execute rigorous experiments and produce high-quality scientific papers. To better illustrate the root cause of this \textbf{implementation gap}, we provide an in-depth discussion on the fundamental limitations of AI Scientist. This position paper aims to call for the participants in the community to bridge the implementation gap.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2171.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2171.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Scientist (concept)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI Scientist (end-to-end autonomous scientific intelligence)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Conceptual definition of an end-to-end autonomous system that independently formulates scientific ideas and executes verification/falsification procedures, operating under human ethical and resource constraints to produce verifiable scientific artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Scientist (concept)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multi-component LLM-based autonomous system / agentic system</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general scientific reasoning / multi-domain science</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>research ideas, hypotheses, full research papers, experimental code and protocols</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>expected to perform experimental execution, code execution, debugging and iterative refinement, simulation and experiment runs, and produce verifiable artifacts; optionally peer-review and expert assessment</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>peer-review scores, citation counts, claimed novelty in generated papers, and acceptance to workshops/conferences (as proxy measures); authors note absence of standardized novelty metric</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Authors report strong idea-generation ability across recent works (numerous papers on idea generation); LLMs produce novel research ideas regularly (qualitative assessment), but no claim of major transformative discoveries in CS yet.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Markedly weak; system-level verification and execution capabilities are reported as insufficient across multiple benchmarks and a simulated peer-review of 28 AI-generated papers found pervasive experimental weaknesses (100% of papers had experimental weakness).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Validation reliability degrades for novel outputs: while idea generation (novelty) is strong, implementation/verification performance is poor — novel ideas often lack executable, verifiable implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Strong asymmetry: generation >> validation; systems generate novel hypotheses/papers but fail to execute rigorous experiments or ensure reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Authors report dramatic performance drops in realistic, out-of-distribution research tasks relative to familiar/code-bench tasks; concrete examples across benchmarks show large gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not quantified; authors state current LLMs 'fail to rigorously validate their outputs' implying poor calibration of self-assessed correctness for novel research outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Validation (code execution, experiments) is far more time-consuming than generation — sampling time estimates show implementation loops (code write + run + analyze) dominate (authors estimate ~46,900s per research loop vs ~2,500s pure inference for generation).</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Human-in-the-loop oversight, modular multi-agent architectures, standardized interoperability protocols (MCP, A2A), retrieval-augmentation (RAG), RL-based planners aided by simulated environments, and specialized verification modules (DeepReview-like systems).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper defines AI Scientist as requiring both generation and implementation; empirical evidence from multiple benchmarks and a simulated peer-review shows generation of novel ideas is feasible but verification/implementation capability is the fundamental bottleneck preventing reliable scientific outputs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2171.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2171.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Scientist-v2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI Scientist-v2 (Yamada et al., 2025)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An advanced LLM-agent system reported to produce workshop-level automated scientific discovery using agentic tree search; cited as producing outputs that received review scores above average acceptance thresholds for human-authored papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Scientist-v2</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multi-agent / LLM-based agentic tree-search system</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general scientific research (ML/CS focus)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>research ideas and full research reports/papers, experiment plans and code components</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>generates code and experiment plans; validation reported via peer-review and benchmark-style reproduction attempts (PaperBench-style metrics noted as poor for many agents), but system-level rigorous execution is limited per authors' analysis</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>peer-review scores and workshop acceptances cited as evidence of novelty and plausibility of generated work</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported to generate papers that received above-average workshop review scores; authors caution outputs often lack robust implementation and high-impact breakthroughs in CS remain absent.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Authors report weak execution/verification capabilities across reproduction benchmarks; lack of consistent, high-quality experimental verification for generated claims.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Novel research produced by AI Scientist-v2 tends to suffer more from verification failures; novelty does not imply verifiable execution.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Evidence of asymmetry: strong generation of plausible ideas and papers but limited ability to reliably execute and validate experimental claims.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not quantified for this specific system in the paper; authors report general OOD drops for LLM agents in real-world research tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>High — end-to-end validation requires asynchronous operations (code runs, experiments) that dominate wall-clock time compared to generation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Improved planning, modular agent decomposition, human-in-the-loop verification, and stronger execution toolchains suggested.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AI Scientist-v2 can produce plausible, reviewable research artifacts but, like other AI Scientist systems, lacks robust implementation/execution capabilities required for reliable scientific discoveries.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2171.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2171.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude 3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3.5 Sonnet (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art large language model agent used in reproduction/implementation benchmarks that demonstrates strong generation but poor execution/verification performance on end-to-end research reproduction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Claude 3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model / LLM agent</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>code generation and research reproduction (ML/NLP/CV domains)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>code components, research text, experiment descriptions, ML workflows</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Benchmarked via execution rubrics (PaperBench execution and result matching), code run tests (SciReplicate-Bench) and ML development success rates (ML-Dev-Bench).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Not formally measured; benchmarks evaluate reproduction fidelity and execution correctness as proxies for trustworthy outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Can generate code components and research text (e.g., scored nonzero on code-development sub-tasks), but generation success on full reproduction is low.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Very poor on verification: on PaperBench rubric leaf nodes Claude 3.5 Sonnet scored 1.8% for 'Execution' and 0.7% for 'Result Match'; other benchmarks show variable execution success (e.g., 39% execution accuracy reported in SciReplicate-Bench as best-agent but that refers to best agent overall).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Validation performance remains low particularly for complex, novel reproduction tasks; authors emphasize validation degrades when tasks move beyond well-covered, familiar code patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Clear asymmetry: capable of generating plausible code/text but fails to ensure runnable, correct experimental artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Severe degradation in out-of-distribution, real-world research reproduction tasks compared to standard code-gen benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not quantitatively reported; characterization implies confidence calibration is insufficient for reliable self-verification.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>High for validation due to need to run and debug code, run experiments; computational cost dominates over generation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Human oversight, iterative debugging loops, enhanced execution monitors, integrated testing harnesses, and tool-chains for running code accurately.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Claude 3.5 Sonnet demonstrates the typical generation-vs-verification gap: able to output code and papers but nearly fails on rubric-defined execution and result-matching tasks (very low percentages on PaperBench).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2171.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2171.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (as CORE-Agent)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (used in CORE-Bench as CORE-Agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI multimodal/large model used in CORE-Bench to attempt reproduction and question-answering based on reproduced experimental outputs; CORE-Agent using GPT-4o achieved moderate success on medium-difficulty tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4o (CORE-Agent)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model / agent</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>computational reproducibility across CS, social science, medicine</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>code generation for reproducing papers, reasoning about experimental outputs, answering questions about reproduced results</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Reproduce experiments by generating runnable code and then compare outputs to published results; answer questions based on reproduced outputs (multi-stage verification).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Benchmarks use reproduction success and question-answer accuracy as proxies for trustworthy (novel) reproductions.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>GPT-4o (as CORE-Agent) achieved 55.56% on CORE-Bench Medium (reported as imperfect success rate), indicating moderate ability to reproduce and reason about some reproducible tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Partial: moderate success on medium tasks but still imperfect reproducibility; replication/verification remains challenging and far from perfect.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Performance declines for more novel/complex experiments requiring extensive debugging or domain-specific execution; CORE-Bench results show medium tasks achievable but hard tasks remain difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Less extreme asymmetry than other LLMs but still present: generation/reproduction possible for some tasks, but verification and full matching to reported results are inconsistent.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Degrades for tasks outside the benchmark's in-distribution scope and for full end-to-end experimental pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Substantial; reproduction requires code execution and analysis, incurring high wall-clock and compute cost.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Tighter experiment orchestration, modular execution environments, human-in-the-loop debugging, and improved tool integration.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4o can reproduce and reason about some computational experiments (CORE-Bench Medium ~55.6%) but reproducibility and verification remain partial, especially for novel/complex studies.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2171.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2171.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o4-mini / o3 / o1-high (code LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Representative state-of-the-art code-focused LLMs (o4-mini, o3, o1-high, o1-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>High-performing code LLM variants evaluated on task suites ranging from algorithmic contests to full paper reproduction benchmarks; show near-saturated performance on simple/codebench tasks but poor execution in real research reproduction contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SoTA code LLMs (o4-mini, o3, o1-high, o1-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language models trained on code</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>code generation, algorithmic problem solving, ML development workflows</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>code generation (algorithms, ML workflows), explanations, documentation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Benchmarked via pass@k on code problems (LiveCodeBench, HumanEval), execution & result matching on PaperBench, execution accuracy on SciReplicate-Bench, and ML workflow success rates (MLE-Bench / ML-Dev-Bench).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Benchmarks measure functional correctness and reproduction fidelity rather than novelty; novelty of generated solutions not systematically quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>High on algorithmic benchmarks (authors cite o3 in 99.8th percentile on Codeforces-like tasks); o4-mini achieved 52.1% pass@1 on LiveCodeBench code-generation subtask; o1-high achieved 43.4% on PaperBench code-development sub-tasks; o1-preview scored 16.90% on MLE-Bench in reported table.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Weak in end-to-end validation: generated code often fails execution or to match reported experimental results; e.g., execution accuracies reported as low (PaperBench execution leaf nodes very low for agents).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Performance falls when tasks require more real-world engineering, long-range planning, multi-file coordination, or debugging — i.e., novel or OOD engineering tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Yes — capable generators on familiar algorithmic tasks but poor at validating and ensuring correctness in complex research contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Marked drop compared to in-distribution code benchmarks; many failures when reproducing experimental code or complex ML pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported; implied poor self-verification in complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Validation costs (executing, debugging, running experiments) far exceed generation cost.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Stronger testing harnesses, continuous integration-style automated test suites, modular multi-agent orchestration, human oversight, and retrieval-augmented contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Code LLMs perform well on canonical code-generation contests but degrade substantially on real-world research reproduction and validation tasks, showing an implementation/verification bottleneck.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2171.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2171.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepReviewer-14B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepReviewer-14B (LLM-based review model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>State-of-the-art reviewer model used by the authors to conduct simulated peer review of 28 AI-generated research papers under unified standards, producing quantitative assessments across soundness, presentation, contribution and overall decision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deepreview: Improving llm-based paper review with human-like deep thinking process</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepReviewer-14B</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based automated peer-reviewer / evaluation model</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>peer review / quality assessment of scientific papers</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>automated review text, aggregate scores for soundness, presentation, contribution, decision and rating percentile</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Assesses candidate papers via rubric-based evaluation simulating peer review; used to quantify defects across categories and to rank AI-generated papers.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Evaluates contribution and novelty as part of rubric (contribution scores reported); not a direct novelty-distance metric.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not applicable (it's an evaluator); used successfully to score 28 AI-generated papers producing tabulated averages (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Provides consistent rubric-based assessments; revealed 100% experimental weakness across 28 evaluated AI-generated papers and produced mean scores reported in Table 2 (e.g., Zochi had higher averages).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>As an evaluator, flags methodological and experimental weaknesses frequently for AI-generated outputs, especially for implementation-heavy claims; authors note peer review has limits in predicting long-term impact.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not numerically reported; authors caution peer review — even automated — cannot perfectly predict long-term impact.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Moderate (LLM inference cost) compared to empirical experiment runs; intended to reduce load on human peer review but not substitute experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Used as part of a hybrid human+automated review pipeline; authors propose central archiving, transparent labeling, and automated detection to complement DeepReviewer.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DeepReviewer-14B systematically evaluated 28 AI-generated papers and found pervasive experimental weaknesses and methodological flaws, empirically supporting the implementation/verification gap claim.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2171.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2171.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaperBench (benchmark for replicating ML papers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark requiring LLM agents to replicate entire machine-learning papers from scratch by developing codebases and running experiments; evaluates multi-stage reproduction including code development, execution, and result matching.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating ai's ability to replicate ai research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PaperBench</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>benchmark / evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>machine learning / NLP / CV research reproduction</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>N/A (benchmark assesses agents that generate code and experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Rubric-based leaf nodes evaluating 'Code-Development', 'Execution' (running code), and 'Result Match' (quantitatively matching reported results); uses end-to-end reproduction success metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Measures fidelity to reported results rather than novelty per se; successful reproduction implies correct implementation of original (familiar) papers.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Agents can sometimes generate code components (e.g., o1-High achieved 43.4% on Code-Development sub-tasks), but full end-to-end success rates are much lower.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Very low on execution and result-matching: example provided — Claude 3.5 Sonnet scored 1.8% on 'Execution' and 0.7% on 'Result Match', indicating poor verification performance.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Benchmark focuses on reproducing known (familiar) papers; even for these familiar tasks agents often fail at execution — implying even worse performance on novel tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Yes — agents may generate plausible code snippets but fail to execute or reproduce quantitative results.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not directly measured; authors infer agents would perform worse on novel (OOD) research reproduction tasks than on reproducing familiar published results.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>High — requires code execution, running experiments; this cost is a major contributor to total sampling time.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Augmented testing harnesses, iterative debugging loops, stronger execution monitoring, and human-in-the-loop supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PaperBench shows that while agents may write parts of codebases, they overwhelmingly fail at executing and matching reported results, highlighting a practical verification bottleneck even on familiar reproduction tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2171.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2171.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciReplicate-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciReplicate-Bench (benchmark for algorithmic reproduction from papers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmark that tasks LLM agents with generating Python code to reproduce algorithms from NLP research papers; measures reasoning/understanding and execution accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scireplicate-bench: Benchmarking llms in agent-driven algorithmic reproduction from research papers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SciReplicate-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>benchmark / evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>NLP algorithm reproduction / code generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>N/A (evaluates agents that generate runnable implementations of published algorithms)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Compares generated implementations via functional test cases and execution accuracy; separates reasoning graph accuracy from executable code success.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Focused on reproducing known (familiar) algorithms; novelty not primary metric.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Agents often show good understanding (high reasoning graph accuracy) but struggle to produce runnable code.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Execution accuracy low — best agent achieved only 39% execution accuracy (generated code passed tests for 39% of tasks), indicating significant verification failures.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Reproduction of familiar algorithms already shows low execution accuracy, implying greater difficulty for novel algorithm generation and validation.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Yes — comprehension/reasoning scores are substantially higher than executable-code success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not directly measured; expectation is worse for OOD novel algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>High due to test execution and debugging cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Enhanced tool integration for execution, improved debugging/self-repair mechanisms, and multi-agent collaboration for complex implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SciReplicate-Bench demonstrates that LLM agents can understand algorithmic logic but often fail to produce correct, executable implementations, confirming an implementation/verification gap.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2171.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2171.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CORE-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CORE-Bench (computational reproducibility agent benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmark that requires agents to reproduce entire computational experiments and then answer questions based on the reproduced outputs — assessing end-to-end experimental reproduction and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CORE-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>benchmark / evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>computational reproducibility across disciplines (CS, social science, medicine)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>N/A (evaluates agents' reproduction and downstream reasoning capabilities)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Two-stage: (1) reproduce computational experiment outputs, (2) answer questions about the outputs — checks both reproduction fidelity and reasoning about results.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Evaluates fidelity to published results rather than novelty; success indicates trustworthy reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported imperfect success rates; example: CORE-Agent with GPT-4o achieved 55.56% on CORE-Bench Medium.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Partial: moderate success on medium tasks but incomplete reproducibility on harder tasks — reproducibility remains challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Novel, complex experiments are harder to reproduce and reason about successfully; performance decreases with task difficulty and novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Present: agents can sometimes reproduce and reason about medium-difficulty tasks but fail on more complex or novel ones.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Degrades with task novelty and complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>High due to need to run full experiments and analyze outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Simulated environments to accelerate RL, human-in-the-loop debugging and modular agents for sub-tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CORE-Bench shows agents can reproduce and reason about some experiments but reproducibility is far from reliable, especially for more complex or novel experimental claims.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2171.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2171.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MLE-Bench / ML-Dev-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MLE-Bench and ML-Dev-Bench (benchmarks for ML engineering workflows)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmarks that evaluate LLM agents on ML engineering tasks: MLE-Bench targets ML training and pipeline tasks; ML-Dev-Bench evaluates completing ML development workflows and model performance optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MLE-Bench: Evaluating machine learning agents on machine learning engineering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MLE-Bench / ML-Dev-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>benchmark / evaluation frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>machine learning engineering and development workflows</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>N/A (assess agents that generate code, configs, and workflows for ML tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Success rates on workflow completion, ability to debug, produce valid submissions, and optimize model performance; measured by end-to-end task success.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Benchmarks emphasize correctness/performance rather than novelty; performance on novel ML problems not separately categorized.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Agents can sometimes generate initial code/artifacts; reported examples: OpenAI o1-preview 16.90% on MLE-Bench (Table 1) and Claude-Sonnet-3.5 50.00% on ML-Dev-Bench (Table 1), but task success is uneven.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Poor for debugging and optimization steps: authors note 20% of o1 preview runs on MLE-Bench failed debugging, and agents scored 0% on 'Model Performance' tasks in ML-Dev-Bench (indicating inability to reliably optimize models).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Agents fail to iteratively refine and optimize models, suggesting larger failures for novel ML problems requiring nuanced tuning and lengthy experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Yes — code/workflow generation often incomplete and lacks robust validation and performance optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Worse than in-distribution scripted tasks; agents struggle with open-ended ML development problems.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>High due to model training/validation loops and debugging cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Integrated CI-style testing, improved debugging/self-repair, human oversight, and modular agent decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MLE/ML-Dev benchmarks reveal agents frequently fail at debugging and model performance optimization — key verification steps in ML research workflows remain unresolved by current AI agents.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2171.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2171.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaFold (DeepMind)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specialized automated scientific tool (deep neural network) that predicts 3D protein structures with high accuracy and is cited as an example of successful automated scientific tool contrasting with the broader-scope AI Scientist paradigm.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Highly accurate protein structure prediction with alphafold</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>deep neural network / specialized AI tool</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>protein structure prediction / structural biology</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>3D protein structure predictions</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Comparison of predicted structures to experimentally determined structures, benchmarking on CASP-style metrics and empirical validation against known ground truth structures.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Measured by accuracy against experimental ground truth; novelty is not main output (tool solves an established problem rather than proposing novel hypotheses).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>High — cited as achieving results in hours that previously took years and as a major automated scientific success.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Strong — validated by experimental protein structures and community benchmarks (CASP), demonstrating correct predictions for many proteins.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Not directly applicable; tool operates on well-defined prediction tasks with ground-truth comparison, performs well on in-distribution proteins but may be challenged on highly novel folds.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Low asymmetry for this specialized tool — high generation performance paired with rigorous empirical validation.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Performance may vary on very novel proteins but generally strong on broad benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported in this paper; known literature addresses confidence metrics per-residue.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Moderate to high (computational inference and comparison to experimental data) but much lower than months/years of lab experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Specialized domain models, heavy supervised training on curated datasets, and task-specific evaluation against experimental ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>contradicts</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AlphaFold is cited as an example of an automated scientific tool that achieved breakthrough validated outputs; the paper contrasts such tools with broad AI Scientists which currently fail at end-to-end validation — implying specialized systems can succeed where generalist AI Scientists struggle.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2171.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e2171.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>A-Lab (autonomous laboratory)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A-Lab (autonomous laboratory for materials synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autonomous laboratory system that synthesized 41 novel inorganic materials within 17 days (Szymanski et al., 2023), cited as a successful automated scientific tool but still requiring human involvement for ideation and higher-level design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An autonomous laboratory for the accelerated synthesis of novel materials</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>A-Lab autonomous laboratory</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>autonomous laboratory / robotics + ML pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>materials discovery / inorganic synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>proposes synthetic experiments, runs robotic synthesis, produces novel materials as outputs</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Experimental synthesis and characterization in laboratory, comparing measured properties to targets/expectations, and human oversight in workflows</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Novelty measured by discovery of new materials and experimental characterization confirming novelty</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>High in constrained domain: 41 novel materials synthesized in 17 days as reported in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Strong experimental validation via lab measurements and characterization protocols; however, ideation and high-level planning still involve humans.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>System validated novel outputs experimentally; demonstrates that domain-constrained automation can both generate and validate novel discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Lower asymmetry in this specialized domain due to integrated robotic execution and measurement pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Limited outside the well-instrumented laboratory protocols and predefined chemical/materials spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>High in real-world time and lab resource cost, but faster than traditional manual lab cycles for the tasks performed.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Tight integration of robotics, domain-specific models, automated measurement, and iterative experimental design.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>contradicts</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A-Lab exemplifies that in domain-constrained settings an automated system can both generate and experimentally validate novel discoveries, contrasting with generalist AI Scientist systems that fail at verification in broader CS/ML contexts.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search <em>(Rating: 2)</em></li>
                <li>Evaluating ai's ability to replicate ai research <em>(Rating: 2)</em></li>
                <li>Scireplicate-bench: Benchmarking llms in agent-driven algorithmic reproduction from research papers <em>(Rating: 2)</em></li>
                <li>Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark <em>(Rating: 2)</em></li>
                <li>MLE-Bench: Evaluating machine learning agents on machine learning engineering <em>(Rating: 2)</em></li>
                <li>Livecodebench: Holistic and contamination free evaluation of large language models for code <em>(Rating: 2)</em></li>
                <li>Deepreview: Improving llm-based paper review with human-like deep thinking process <em>(Rating: 2)</em></li>
                <li>Highly accurate protein structure prediction with alphafold <em>(Rating: 1)</em></li>
                <li>An autonomous laboratory for the accelerated synthesis of novel materials <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2171",
    "paper_id": "paper-279075076",
    "extraction_schema_id": "extraction-schema-57",
    "extracted_data": [
        {
            "name_short": "AI Scientist (concept)",
            "name_full": "AI Scientist (end-to-end autonomous scientific intelligence)",
            "brief_description": "Conceptual definition of an end-to-end autonomous system that independently formulates scientific ideas and executes verification/falsification procedures, operating under human ethical and resource constraints to produce verifiable scientific artifacts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "AI Scientist (concept)",
            "system_type": "multi-component LLM-based autonomous system / agentic system",
            "domain": "general scientific reasoning / multi-domain science",
            "generation_capability": "research ideas, hypotheses, full research papers, experimental code and protocols",
            "validation_method": "expected to perform experimental execution, code execution, debugging and iterative refinement, simulation and experiment runs, and produce verifiable artifacts; optionally peer-review and expert assessment",
            "novelty_measure": "peer-review scores, citation counts, claimed novelty in generated papers, and acceptance to workshops/conferences (as proxy measures); authors note absence of standardized novelty metric",
            "generation_performance": "Authors report strong idea-generation ability across recent works (numerous papers on idea generation); LLMs produce novel research ideas regularly (qualitative assessment), but no claim of major transformative discoveries in CS yet.",
            "validation_performance": "Markedly weak; system-level verification and execution capabilities are reported as insufficient across multiple benchmarks and a simulated peer-review of 28 AI-generated papers found pervasive experimental weaknesses (100% of papers had experimental weakness).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Validation reliability degrades for novel outputs: while idea generation (novelty) is strong, implementation/verification performance is poor — novel ideas often lack executable, verifiable implementations.",
            "generation_validation_asymmetry": "Strong asymmetry: generation &gt;&gt; validation; systems generate novel hypotheses/papers but fail to execute rigorous experiments or ensure reproducibility.",
            "out_of_distribution_performance": "Authors report dramatic performance drops in realistic, out-of-distribution research tasks relative to familiar/code-bench tasks; concrete examples across benchmarks show large gaps.",
            "calibration_quality": "Not quantified; authors state current LLMs 'fail to rigorously validate their outputs' implying poor calibration of self-assessed correctness for novel research outputs.",
            "validation_computational_cost": "Validation (code execution, experiments) is far more time-consuming than generation — sampling time estimates show implementation loops (code write + run + analyze) dominate (authors estimate ~46,900s per research loop vs ~2,500s pure inference for generation).",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Human-in-the-loop oversight, modular multi-agent architectures, standardized interoperability protocols (MCP, A2A), retrieval-augmentation (RAG), RL-based planners aided by simulated environments, and specialized verification modules (DeepReview-like systems).",
            "evidence_type": "supports",
            "key_findings": "The paper defines AI Scientist as requiring both generation and implementation; empirical evidence from multiple benchmarks and a simulated peer-review shows generation of novel ideas is feasible but verification/implementation capability is the fundamental bottleneck preventing reliable scientific outputs.",
            "uuid": "e2171.0"
        },
        {
            "name_short": "AI Scientist-v2",
            "name_full": "The AI Scientist-v2 (Yamada et al., 2025)",
            "brief_description": "An advanced LLM-agent system reported to produce workshop-level automated scientific discovery using agentic tree search; cited as producing outputs that received review scores above average acceptance thresholds for human-authored papers.",
            "citation_title": "The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search",
            "mention_or_use": "mention",
            "system_name": "AI Scientist-v2",
            "system_type": "multi-agent / LLM-based agentic tree-search system",
            "domain": "general scientific research (ML/CS focus)",
            "generation_capability": "research ideas and full research reports/papers, experiment plans and code components",
            "validation_method": "generates code and experiment plans; validation reported via peer-review and benchmark-style reproduction attempts (PaperBench-style metrics noted as poor for many agents), but system-level rigorous execution is limited per authors' analysis",
            "novelty_measure": "peer-review scores and workshop acceptances cited as evidence of novelty and plausibility of generated work",
            "generation_performance": "Reported to generate papers that received above-average workshop review scores; authors caution outputs often lack robust implementation and high-impact breakthroughs in CS remain absent.",
            "validation_performance": "Authors report weak execution/verification capabilities across reproduction benchmarks; lack of consistent, high-quality experimental verification for generated claims.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Novel research produced by AI Scientist-v2 tends to suffer more from verification failures; novelty does not imply verifiable execution.",
            "generation_validation_asymmetry": "Evidence of asymmetry: strong generation of plausible ideas and papers but limited ability to reliably execute and validate experimental claims.",
            "out_of_distribution_performance": "Not quantified for this specific system in the paper; authors report general OOD drops for LLM agents in real-world research tasks.",
            "calibration_quality": "Not reported.",
            "validation_computational_cost": "High — end-to-end validation requires asynchronous operations (code runs, experiments) that dominate wall-clock time compared to generation.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Improved planning, modular agent decomposition, human-in-the-loop verification, and stronger execution toolchains suggested.",
            "evidence_type": "supports",
            "key_findings": "AI Scientist-v2 can produce plausible, reviewable research artifacts but, like other AI Scientist systems, lacks robust implementation/execution capabilities required for reliable scientific discoveries.",
            "uuid": "e2171.1"
        },
        {
            "name_short": "Claude 3.5 Sonnet",
            "name_full": "Claude 3.5 Sonnet (Anthropic)",
            "brief_description": "A state-of-the-art large language model agent used in reproduction/implementation benchmarks that demonstrates strong generation but poor execution/verification performance on end-to-end research reproduction tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Claude 3.5 Sonnet",
            "system_type": "large language model / LLM agent",
            "domain": "code generation and research reproduction (ML/NLP/CV domains)",
            "generation_capability": "code components, research text, experiment descriptions, ML workflows",
            "validation_method": "Benchmarked via execution rubrics (PaperBench execution and result matching), code run tests (SciReplicate-Bench) and ML development success rates (ML-Dev-Bench).",
            "novelty_measure": "Not formally measured; benchmarks evaluate reproduction fidelity and execution correctness as proxies for trustworthy outputs.",
            "generation_performance": "Can generate code components and research text (e.g., scored nonzero on code-development sub-tasks), but generation success on full reproduction is low.",
            "validation_performance": "Very poor on verification: on PaperBench rubric leaf nodes Claude 3.5 Sonnet scored 1.8% for 'Execution' and 0.7% for 'Result Match'; other benchmarks show variable execution success (e.g., 39% execution accuracy reported in SciReplicate-Bench as best-agent but that refers to best agent overall).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Validation performance remains low particularly for complex, novel reproduction tasks; authors emphasize validation degrades when tasks move beyond well-covered, familiar code patterns.",
            "generation_validation_asymmetry": "Clear asymmetry: capable of generating plausible code/text but fails to ensure runnable, correct experimental artifacts.",
            "out_of_distribution_performance": "Severe degradation in out-of-distribution, real-world research reproduction tasks compared to standard code-gen benchmarks.",
            "calibration_quality": "Not quantitatively reported; characterization implies confidence calibration is insufficient for reliable self-verification.",
            "validation_computational_cost": "High for validation due to need to run and debug code, run experiments; computational cost dominates over generation.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Human oversight, iterative debugging loops, enhanced execution monitors, integrated testing harnesses, and tool-chains for running code accurately.",
            "evidence_type": "supports",
            "key_findings": "Claude 3.5 Sonnet demonstrates the typical generation-vs-verification gap: able to output code and papers but nearly fails on rubric-defined execution and result-matching tasks (very low percentages on PaperBench).",
            "uuid": "e2171.2"
        },
        {
            "name_short": "GPT-4o (as CORE-Agent)",
            "name_full": "GPT-4o (used in CORE-Bench as CORE-Agent)",
            "brief_description": "An OpenAI multimodal/large model used in CORE-Bench to attempt reproduction and question-answering based on reproduced experimental outputs; CORE-Agent using GPT-4o achieved moderate success on medium-difficulty tasks.",
            "citation_title": "Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark",
            "mention_or_use": "mention",
            "system_name": "GPT-4o (CORE-Agent)",
            "system_type": "large language model / agent",
            "domain": "computational reproducibility across CS, social science, medicine",
            "generation_capability": "code generation for reproducing papers, reasoning about experimental outputs, answering questions about reproduced results",
            "validation_method": "Reproduce experiments by generating runnable code and then compare outputs to published results; answer questions based on reproduced outputs (multi-stage verification).",
            "novelty_measure": "Benchmarks use reproduction success and question-answer accuracy as proxies for trustworthy (novel) reproductions.",
            "generation_performance": "GPT-4o (as CORE-Agent) achieved 55.56% on CORE-Bench Medium (reported as imperfect success rate), indicating moderate ability to reproduce and reason about some reproducible tasks.",
            "validation_performance": "Partial: moderate success on medium tasks but still imperfect reproducibility; replication/verification remains challenging and far from perfect.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Performance declines for more novel/complex experiments requiring extensive debugging or domain-specific execution; CORE-Bench results show medium tasks achievable but hard tasks remain difficult.",
            "generation_validation_asymmetry": "Less extreme asymmetry than other LLMs but still present: generation/reproduction possible for some tasks, but verification and full matching to reported results are inconsistent.",
            "out_of_distribution_performance": "Degrades for tasks outside the benchmark's in-distribution scope and for full end-to-end experimental pipelines.",
            "calibration_quality": "Not reported numerically.",
            "validation_computational_cost": "Substantial; reproduction requires code execution and analysis, incurring high wall-clock and compute cost.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Tighter experiment orchestration, modular execution environments, human-in-the-loop debugging, and improved tool integration.",
            "evidence_type": "supports",
            "key_findings": "GPT-4o can reproduce and reason about some computational experiments (CORE-Bench Medium ~55.6%) but reproducibility and verification remain partial, especially for novel/complex studies.",
            "uuid": "e2171.3"
        },
        {
            "name_short": "o4-mini / o3 / o1-high (code LLMs)",
            "name_full": "Representative state-of-the-art code-focused LLMs (o4-mini, o3, o1-high, o1-preview)",
            "brief_description": "High-performing code LLM variants evaluated on task suites ranging from algorithmic contests to full paper reproduction benchmarks; show near-saturated performance on simple/codebench tasks but poor execution in real research reproduction contexts.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "SoTA code LLMs (o4-mini, o3, o1-high, o1-preview)",
            "system_type": "large language models trained on code",
            "domain": "code generation, algorithmic problem solving, ML development workflows",
            "generation_capability": "code generation (algorithms, ML workflows), explanations, documentation",
            "validation_method": "Benchmarked via pass@k on code problems (LiveCodeBench, HumanEval), execution & result matching on PaperBench, execution accuracy on SciReplicate-Bench, and ML workflow success rates (MLE-Bench / ML-Dev-Bench).",
            "novelty_measure": "Benchmarks measure functional correctness and reproduction fidelity rather than novelty; novelty of generated solutions not systematically quantified.",
            "generation_performance": "High on algorithmic benchmarks (authors cite o3 in 99.8th percentile on Codeforces-like tasks); o4-mini achieved 52.1% pass@1 on LiveCodeBench code-generation subtask; o1-high achieved 43.4% on PaperBench code-development sub-tasks; o1-preview scored 16.90% on MLE-Bench in reported table.",
            "validation_performance": "Weak in end-to-end validation: generated code often fails execution or to match reported experimental results; e.g., execution accuracies reported as low (PaperBench execution leaf nodes very low for agents).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Performance falls when tasks require more real-world engineering, long-range planning, multi-file coordination, or debugging — i.e., novel or OOD engineering tasks.",
            "generation_validation_asymmetry": "Yes — capable generators on familiar algorithmic tasks but poor at validating and ensuring correctness in complex research contexts.",
            "out_of_distribution_performance": "Marked drop compared to in-distribution code benchmarks; many failures when reproducing experimental code or complex ML pipelines.",
            "calibration_quality": "Not reported; implied poor self-verification in complex tasks.",
            "validation_computational_cost": "Validation costs (executing, debugging, running experiments) far exceed generation cost.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Stronger testing harnesses, continuous integration-style automated test suites, modular multi-agent orchestration, human oversight, and retrieval-augmented contexts.",
            "evidence_type": "supports",
            "key_findings": "Code LLMs perform well on canonical code-generation contests but degrade substantially on real-world research reproduction and validation tasks, showing an implementation/verification bottleneck.",
            "uuid": "e2171.4"
        },
        {
            "name_short": "DeepReviewer-14B",
            "name_full": "DeepReviewer-14B (LLM-based review model)",
            "brief_description": "State-of-the-art reviewer model used by the authors to conduct simulated peer review of 28 AI-generated research papers under unified standards, producing quantitative assessments across soundness, presentation, contribution and overall decision.",
            "citation_title": "Deepreview: Improving llm-based paper review with human-like deep thinking process",
            "mention_or_use": "use",
            "system_name": "DeepReviewer-14B",
            "system_type": "LLM-based automated peer-reviewer / evaluation model",
            "domain": "peer review / quality assessment of scientific papers",
            "generation_capability": "automated review text, aggregate scores for soundness, presentation, contribution, decision and rating percentile",
            "validation_method": "Assesses candidate papers via rubric-based evaluation simulating peer review; used to quantify defects across categories and to rank AI-generated papers.",
            "novelty_measure": "Evaluates contribution and novelty as part of rubric (contribution scores reported); not a direct novelty-distance metric.",
            "generation_performance": "Not applicable (it's an evaluator); used successfully to score 28 AI-generated papers producing tabulated averages (Table 2).",
            "validation_performance": "Provides consistent rubric-based assessments; revealed 100% experimental weakness across 28 evaluated AI-generated papers and produced mean scores reported in Table 2 (e.g., Zochi had higher averages).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "As an evaluator, flags methodological and experimental weaknesses frequently for AI-generated outputs, especially for implementation-heavy claims; authors note peer review has limits in predicting long-term impact.",
            "generation_validation_asymmetry": "Not applicable.",
            "out_of_distribution_performance": "Not reported.",
            "calibration_quality": "Not numerically reported; authors caution peer review — even automated — cannot perfectly predict long-term impact.",
            "validation_computational_cost": "Moderate (LLM inference cost) compared to empirical experiment runs; intended to reduce load on human peer review but not substitute experimental validation.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Used as part of a hybrid human+automated review pipeline; authors propose central archiving, transparent labeling, and automated detection to complement DeepReviewer.",
            "evidence_type": "supports",
            "key_findings": "DeepReviewer-14B systematically evaluated 28 AI-generated papers and found pervasive experimental weaknesses and methodological flaws, empirically supporting the implementation/verification gap claim.",
            "uuid": "e2171.5"
        },
        {
            "name_short": "PaperBench",
            "name_full": "PaperBench (benchmark for replicating ML papers)",
            "brief_description": "A benchmark requiring LLM agents to replicate entire machine-learning papers from scratch by developing codebases and running experiments; evaluates multi-stage reproduction including code development, execution, and result matching.",
            "citation_title": "Evaluating ai's ability to replicate ai research",
            "mention_or_use": "use",
            "system_name": "PaperBench",
            "system_type": "benchmark / evaluation framework",
            "domain": "machine learning / NLP / CV research reproduction",
            "generation_capability": "N/A (benchmark assesses agents that generate code and experiments)",
            "validation_method": "Rubric-based leaf nodes evaluating 'Code-Development', 'Execution' (running code), and 'Result Match' (quantitatively matching reported results); uses end-to-end reproduction success metrics.",
            "novelty_measure": "Measures fidelity to reported results rather than novelty per se; successful reproduction implies correct implementation of original (familiar) papers.",
            "generation_performance": "Agents can sometimes generate code components (e.g., o1-High achieved 43.4% on Code-Development sub-tasks), but full end-to-end success rates are much lower.",
            "validation_performance": "Very low on execution and result-matching: example provided — Claude 3.5 Sonnet scored 1.8% on 'Execution' and 0.7% on 'Result Match', indicating poor verification performance.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Benchmark focuses on reproducing known (familiar) papers; even for these familiar tasks agents often fail at execution — implying even worse performance on novel tasks.",
            "generation_validation_asymmetry": "Yes — agents may generate plausible code snippets but fail to execute or reproduce quantitative results.",
            "out_of_distribution_performance": "Not directly measured; authors infer agents would perform worse on novel (OOD) research reproduction tasks than on reproducing familiar published results.",
            "calibration_quality": "Not reported.",
            "validation_computational_cost": "High — requires code execution, running experiments; this cost is a major contributor to total sampling time.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Augmented testing harnesses, iterative debugging loops, stronger execution monitoring, and human-in-the-loop supervision.",
            "evidence_type": "supports",
            "key_findings": "PaperBench shows that while agents may write parts of codebases, they overwhelmingly fail at executing and matching reported results, highlighting a practical verification bottleneck even on familiar reproduction tasks.",
            "uuid": "e2171.6"
        },
        {
            "name_short": "SciReplicate-Bench",
            "name_full": "SciReplicate-Bench (benchmark for algorithmic reproduction from papers)",
            "brief_description": "Benchmark that tasks LLM agents with generating Python code to reproduce algorithms from NLP research papers; measures reasoning/understanding and execution accuracy.",
            "citation_title": "Scireplicate-bench: Benchmarking llms in agent-driven algorithmic reproduction from research papers",
            "mention_or_use": "use",
            "system_name": "SciReplicate-Bench",
            "system_type": "benchmark / evaluation framework",
            "domain": "NLP algorithm reproduction / code generation",
            "generation_capability": "N/A (evaluates agents that generate runnable implementations of published algorithms)",
            "validation_method": "Compares generated implementations via functional test cases and execution accuracy; separates reasoning graph accuracy from executable code success.",
            "novelty_measure": "Focused on reproducing known (familiar) algorithms; novelty not primary metric.",
            "generation_performance": "Agents often show good understanding (high reasoning graph accuracy) but struggle to produce runnable code.",
            "validation_performance": "Execution accuracy low — best agent achieved only 39% execution accuracy (generated code passed tests for 39% of tasks), indicating significant verification failures.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Reproduction of familiar algorithms already shows low execution accuracy, implying greater difficulty for novel algorithm generation and validation.",
            "generation_validation_asymmetry": "Yes — comprehension/reasoning scores are substantially higher than executable-code success rates.",
            "out_of_distribution_performance": "Not directly measured; expectation is worse for OOD novel algorithms.",
            "calibration_quality": "Not reported.",
            "validation_computational_cost": "High due to test execution and debugging cycles.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Enhanced tool integration for execution, improved debugging/self-repair mechanisms, and multi-agent collaboration for complex implementations.",
            "evidence_type": "supports",
            "key_findings": "SciReplicate-Bench demonstrates that LLM agents can understand algorithmic logic but often fail to produce correct, executable implementations, confirming an implementation/verification gap.",
            "uuid": "e2171.7"
        },
        {
            "name_short": "CORE-Bench",
            "name_full": "CORE-Bench (computational reproducibility agent benchmark)",
            "brief_description": "Benchmark that requires agents to reproduce entire computational experiments and then answer questions based on the reproduced outputs — assessing end-to-end experimental reproduction and reasoning.",
            "citation_title": "Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark",
            "mention_or_use": "use",
            "system_name": "CORE-Bench",
            "system_type": "benchmark / evaluation framework",
            "domain": "computational reproducibility across disciplines (CS, social science, medicine)",
            "generation_capability": "N/A (evaluates agents' reproduction and downstream reasoning capabilities)",
            "validation_method": "Two-stage: (1) reproduce computational experiment outputs, (2) answer questions about the outputs — checks both reproduction fidelity and reasoning about results.",
            "novelty_measure": "Evaluates fidelity to published results rather than novelty; success indicates trustworthy reproduction.",
            "generation_performance": "Reported imperfect success rates; example: CORE-Agent with GPT-4o achieved 55.56% on CORE-Bench Medium.",
            "validation_performance": "Partial: moderate success on medium tasks but incomplete reproducibility on harder tasks — reproducibility remains challenging.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Novel, complex experiments are harder to reproduce and reason about successfully; performance decreases with task difficulty and novelty.",
            "generation_validation_asymmetry": "Present: agents can sometimes reproduce and reason about medium-difficulty tasks but fail on more complex or novel ones.",
            "out_of_distribution_performance": "Degrades with task novelty and complexity.",
            "calibration_quality": "Not quantified.",
            "validation_computational_cost": "High due to need to run full experiments and analyze outputs.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Simulated environments to accelerate RL, human-in-the-loop debugging and modular agents for sub-tasks.",
            "evidence_type": "supports",
            "key_findings": "CORE-Bench shows agents can reproduce and reason about some experiments but reproducibility is far from reliable, especially for more complex or novel experimental claims.",
            "uuid": "e2171.8"
        },
        {
            "name_short": "MLE-Bench / ML-Dev-Bench",
            "name_full": "MLE-Bench and ML-Dev-Bench (benchmarks for ML engineering workflows)",
            "brief_description": "Benchmarks that evaluate LLM agents on ML engineering tasks: MLE-Bench targets ML training and pipeline tasks; ML-Dev-Bench evaluates completing ML development workflows and model performance optimization.",
            "citation_title": "MLE-Bench: Evaluating machine learning agents on machine learning engineering",
            "mention_or_use": "use",
            "system_name": "MLE-Bench / ML-Dev-Bench",
            "system_type": "benchmark / evaluation frameworks",
            "domain": "machine learning engineering and development workflows",
            "generation_capability": "N/A (assess agents that generate code, configs, and workflows for ML tasks)",
            "validation_method": "Success rates on workflow completion, ability to debug, produce valid submissions, and optimize model performance; measured by end-to-end task success.",
            "novelty_measure": "Benchmarks emphasize correctness/performance rather than novelty; performance on novel ML problems not separately categorized.",
            "generation_performance": "Agents can sometimes generate initial code/artifacts; reported examples: OpenAI o1-preview 16.90% on MLE-Bench (Table 1) and Claude-Sonnet-3.5 50.00% on ML-Dev-Bench (Table 1), but task success is uneven.",
            "validation_performance": "Poor for debugging and optimization steps: authors note 20% of o1 preview runs on MLE-Bench failed debugging, and agents scored 0% on 'Model Performance' tasks in ML-Dev-Bench (indicating inability to reliably optimize models).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Agents fail to iteratively refine and optimize models, suggesting larger failures for novel ML problems requiring nuanced tuning and lengthy experiments.",
            "generation_validation_asymmetry": "Yes — code/workflow generation often incomplete and lacks robust validation and performance optimization.",
            "out_of_distribution_performance": "Worse than in-distribution scripted tasks; agents struggle with open-ended ML development problems.",
            "calibration_quality": "Not reported.",
            "validation_computational_cost": "High due to model training/validation loops and debugging cycles.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Integrated CI-style testing, improved debugging/self-repair, human oversight, and modular agent decomposition.",
            "evidence_type": "supports",
            "key_findings": "MLE/ML-Dev benchmarks reveal agents frequently fail at debugging and model performance optimization — key verification steps in ML research workflows remain unresolved by current AI agents.",
            "uuid": "e2171.9"
        },
        {
            "name_short": "AlphaFold",
            "name_full": "AlphaFold (DeepMind)",
            "brief_description": "A specialized automated scientific tool (deep neural network) that predicts 3D protein structures with high accuracy and is cited as an example of successful automated scientific tool contrasting with the broader-scope AI Scientist paradigm.",
            "citation_title": "Highly accurate protein structure prediction with alphafold",
            "mention_or_use": "mention",
            "system_name": "AlphaFold",
            "system_type": "deep neural network / specialized AI tool",
            "domain": "protein structure prediction / structural biology",
            "generation_capability": "3D protein structure predictions",
            "validation_method": "Comparison of predicted structures to experimentally determined structures, benchmarking on CASP-style metrics and empirical validation against known ground truth structures.",
            "novelty_measure": "Measured by accuracy against experimental ground truth; novelty is not main output (tool solves an established problem rather than proposing novel hypotheses).",
            "generation_performance": "High — cited as achieving results in hours that previously took years and as a major automated scientific success.",
            "validation_performance": "Strong — validated by experimental protein structures and community benchmarks (CASP), demonstrating correct predictions for many proteins.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Not directly applicable; tool operates on well-defined prediction tasks with ground-truth comparison, performs well on in-distribution proteins but may be challenged on highly novel folds.",
            "generation_validation_asymmetry": "Low asymmetry for this specialized tool — high generation performance paired with rigorous empirical validation.",
            "out_of_distribution_performance": "Performance may vary on very novel proteins but generally strong on broad benchmarks.",
            "calibration_quality": "Not reported in this paper; known literature addresses confidence metrics per-residue.",
            "validation_computational_cost": "Moderate to high (computational inference and comparison to experimental data) but much lower than months/years of lab experiments.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Specialized domain models, heavy supervised training on curated datasets, and task-specific evaluation against experimental ground truth.",
            "evidence_type": "contradicts",
            "key_findings": "AlphaFold is cited as an example of an automated scientific tool that achieved breakthrough validated outputs; the paper contrasts such tools with broad AI Scientists which currently fail at end-to-end validation — implying specialized systems can succeed where generalist AI Scientists struggle.",
            "uuid": "e2171.10"
        },
        {
            "name_short": "A-Lab (autonomous laboratory)",
            "name_full": "A-Lab (autonomous laboratory for materials synthesis)",
            "brief_description": "An autonomous laboratory system that synthesized 41 novel inorganic materials within 17 days (Szymanski et al., 2023), cited as a successful automated scientific tool but still requiring human involvement for ideation and higher-level design.",
            "citation_title": "An autonomous laboratory for the accelerated synthesis of novel materials",
            "mention_or_use": "mention",
            "system_name": "A-Lab autonomous laboratory",
            "system_type": "autonomous laboratory / robotics + ML pipeline",
            "domain": "materials discovery / inorganic synthesis",
            "generation_capability": "proposes synthetic experiments, runs robotic synthesis, produces novel materials as outputs",
            "validation_method": "Experimental synthesis and characterization in laboratory, comparing measured properties to targets/expectations, and human oversight in workflows",
            "novelty_measure": "Novelty measured by discovery of new materials and experimental characterization confirming novelty",
            "generation_performance": "High in constrained domain: 41 novel materials synthesized in 17 days as reported in cited work.",
            "validation_performance": "Strong experimental validation via lab measurements and characterization protocols; however, ideation and high-level planning still involve humans.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "System validated novel outputs experimentally; demonstrates that domain-constrained automation can both generate and validate novel discoveries.",
            "generation_validation_asymmetry": "Lower asymmetry in this specialized domain due to integrated robotic execution and measurement pipelines.",
            "out_of_distribution_performance": "Limited outside the well-instrumented laboratory protocols and predefined chemical/materials spaces.",
            "calibration_quality": "Not reported in this paper.",
            "validation_computational_cost": "High in real-world time and lab resource cost, but faster than traditional manual lab cycles for the tasks performed.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Tight integration of robotics, domain-specific models, automated measurement, and iterative experimental design.",
            "evidence_type": "contradicts",
            "key_findings": "A-Lab exemplifies that in domain-constrained settings an automated system can both generate and experimentally validate novel discoveries, contrasting with generalist AI Scientist systems that fail at verification in broader CS/ML contexts.",
            "uuid": "e2171.11"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search",
            "rating": 2
        },
        {
            "paper_title": "Evaluating ai's ability to replicate ai research",
            "rating": 2
        },
        {
            "paper_title": "Scireplicate-bench: Benchmarking llms in agent-driven algorithmic reproduction from research papers",
            "rating": 2
        },
        {
            "paper_title": "Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark",
            "rating": 2
        },
        {
            "paper_title": "MLE-Bench: Evaluating machine learning agents on machine learning engineering",
            "rating": 2
        },
        {
            "paper_title": "Livecodebench: Holistic and contamination free evaluation of large language models for code",
            "rating": 2
        },
        {
            "paper_title": "Deepreview: Improving llm-based paper review with human-like deep thinking process",
            "rating": 2
        },
        {
            "paper_title": "Highly accurate protein structure prediction with alphafold",
            "rating": 1
        },
        {
            "paper_title": "An autonomous laboratory for the accelerated synthesis of novel materials",
            "rating": 1
        }
    ],
    "cost": 0.02358425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AI Scientists Fail Without Strong Implementation Capability
May 24, 2025</p>
<p>Minjun Zhu 
Engineering School
Westlake University</p>
<p>Zhejiang University</p>
<p>Qiujie Xie 
Engineering School
Westlake University</p>
<p>Zhejiang University</p>
<p>Yixuan Weng 
Engineering School
Westlake University</p>
<p>Jian Wu 
Engineering School
Westlake University</p>
<p>Zhen Lin 
Engineering School
Westlake University</p>
<p>Linyi Yang yanglinyiucd@gmail.com 
The emergence of Artificial Intelligence (AI) Scientist
University College London</p>
<p>Yue Zhang zhangyue@westlake.edu.cn 
Engineering School
Westlake University</p>
<p>AI Scientists Fail Without Strong Implementation Capability
May 24, 2025F860A8464F3C105E680E4FA57B83AF94arXiv:2506.01372v2[cs.AI]AI ScientistImplementation GapHypothesis and Verification
represents a paradigm shift in scientific discovery, with large language models (LLMs) taking the lead as the primary executor in the entire scientific workflow from idea generation to experiment implementation.Recent AI Scientist studies demonstrate sufficient capabilities for independent scientific discovery, with the generated research reports gaining acceptance at the ICLR 2025 workshop and ACL 2025, arguing that a human-level AI Scientist, capable of uncovering phenomena previously unknown to humans, may be imminent.Despite this substantial progress, AI Scientist has yet to produce a groundbreaking achievement in the domain of computer science on par with automated scientific tools.Based on extensive quantitative evidence from existing benchmarks in complex engineering tasks and a systematic evaluation assess 28 research papers generated by five advanced AI Scientist systems, we argue that the fundamental bottleneck for AI Scientists lies in their capability to execute the requisite verification procedures.Current AI Scientist systems lack the execution capabilities needed to execute rigorous experiments and produce high-quality scientific papers.To better illustrate the root cause of this implementation gap, we provide an in-depth discussion on the fundamental limitations of AI Scientist.This position paper aims to call for the participants in the community to bridge the implementation gap.</p>
<p>Introduction</p>
<p>The automation of scientific discovery has long been one of humanity's deepest desires (Langley, 1987, King et al., 2009, Radensky et al., 2024, AI, 2025).In recent years, with the advances in deep neural network technology, a range of automated scientific tools has emerged, leading to groundbreaking achievements in fields such as biomedicine (Yang et al., 2025c, Jumper et al., 2021), chemistry (Stokes et al., 2020), and materials science (Szymanski et al., 2023).For instance, DeepMind's AlphaFold can determine the 3D structures of proteins in just a few hours, a task that previously took years to solve (Jumper et al., 2021).In recent, researchers developed an autonomous laboratory, A-Lab, which successfully synthesizes 41 novel inorganic materials within 17 days (Szymanski et al., 2023).However, these scientific tools still rely heavily on human involvement.Researchers must first formulate ideas to be tested, while AI is responsible for the labor-intensive tasks of verification and iterative search.Therefore, these systems cannot be considered as truly automated scientific research.</p>
<p>The emergence of LLM-based AI Scientist has propelled the automation of scientific research to the next level, with AI taking the lead as the primary executor of scientific discovery, managing the entire workflow from idea generation to experiment execution (Lu et al., 2024, Weng et al., 2025).Recent studies have shown that research papers produced by AI Scientist have already reached the level of submissions to major machine learning conferences (Si et al., 2024, Yamada et al., 2025, Intology, 2025).As shown in Figure 1, we demonstrate the progress made by AI Scientist-v2 (Yamada et al., 2025), and the research output has received review scores exceeding the average acceptance threshold for human-authored papers.Similarly, researchers present an empirical validation through multiple peer-reviewed publications accepted at ICLR 2025 workshops and ACL 2025 main conference (Intology, 2025).Despite this substantial progress, AI Scientist has yet to produce a groundbreaking achievement in the domain of computer science on par with automated scientific tools (e.g., AlphaFold (Jumper et al., 2021)).</p>
<p>In this position paper, we first propose a conceptual framework (Section 2) that defines an AI Scientist as an advanced end-to-end system capable of independently formulating scientific ideas and performing the implementation for verifying these ideas.This definition forms the theoretical foundation of our position, aligns with current research progress (Lu et al., 2024, Weng et al., 2025, Yamada et al., 2025), and emphasizes that the core capability of an AI Scientist lies in generating innovative and feasible ideas at scale (Si et al., 2024, Wang et al., 2024a, Hu et al., 2024, Yang et al., 2025d).The idea-generation capability is a key feature that sets AI Scientists apart from automated scientific tools.While recent advances demonstrate that AI Scientists can generate highly innovative ideas (Si et al., 2025), their implementation capabilities remain constrained (Chan et al., 2024, Starace et al., 2025, Xiang et al., 2025, Siegel et al., 2024, Padigela et al., 2025), creating a significant gap between innovative idea generation and complete implementation.</p>
<p>Our Position:</p>
<p>The fundamental bottleneck for AI Scientists lies in their implementation capability to effectively execute the verification of these ideas.</p>
<p>We defend our argument by analyzing quantitative evidence from existing benchmarks used to evaluate LLMs' abilities in performing complex engineering tasks (Section 3.2).While LLMs can generate highly novel ideas (Si et al., 2024, Chai et al., 2024, Gottweis et al., 2025), their performance in experiment execution is exceptionally poor (Table 1).For instance, a leading LLM like Claude 3.5 Sonnet scored only 1.8% on PaperBench (Starace et al., 2025).This implementation gap is further supported by a systematic evaluation (Section 3.3), which leverages a state-of-the-art review model, DeepReviewer-14B (Zhu et al., 2025), to assess 28 research papers generated by five advanced AI Scientist systems.The results demonstrate that current AI Scientist systems lack the execution capabilities needed to execute rigorous experiments and produce high-quality scientific papers.Finally, to clearly illustrate the root cause of the implementation gap, we provide an in-depth discussion on the fundamental limitations of AI Scientist (Section 4).</p>
<p>In summary, this paper validates and deeply analyzes the implementation gap in existing AI Scientist systems based on extensive quantitative evidence and a simulated peer-review process.Furthermore, as the development of AI Scientists will bring greater regulatory challenges, we comprehensively examine the ethical considerations (Section 5) faced by AI Scientists and suggest directions for future research (Section 6).We hope this position paper will contribute to a clearer understanding of the limitations of current AI Scientist, shedding light on the future development of AI Scientist.</p>
<p>Definition of the AI Scientist</p>
<p>The emergence of automated scientific tools has accelerated scientific discovery across numerous domains (King et al., 2009, Yang et al., 2025c, Jumper et al., 2021, Stokes et al., 2020, Szymanski et al., 2023).However, these tools fundamentally operate within a paradigm where human researchers remain in the dominant position of scientific discovery, and thus cannot be classified as fully automated AI Scientists.In this section, we first provide a detailed discussion of the unique characteristics of the AI Scientist (Section 2.1).Building on this discussion, we then propose a conceptual framework that formally defines the AI Scientist in a mathematical form (Section 2.2).Scientific tools, originating from AI for Science research, represent specialized AI systems designed to solve specific scientific problems by processing data and generating results within defined domains.These tools have demonstrated remarkable success across diverse scientific fields, including protein structure prediction (e.g.Al-phaFold) (Jumper et al., 2021), antibiotic discovery through deep learning approaches (Stokes et al., 2020), and autonomous chemical research with large language models (Boiko et al., 2023).These scientific tools funda-mentally operate within a knowledge-dependent collaborative framework between humans and AI.</p>
<p>Unique Characteristics</p>
<p>AI Scientist represents a research paradigm shift where AI assumes the role of an autonomous scientist capable of conducting independent scientific research.As illustrated in Figure 2, while scientific tools operate under human supervision, receiving data as input and producing predictions as output, AI Scientist goes a step further by demonstrating autonomous scientific reasoning capabilities.It accepts research questions as input and engages in iterative, self-directed interactions with scientific tools to generate comprehensive solutions.Unlike scientific tools that function as sophisticated instruments awaiting human guidance, AI Scientist exhibits genuine scientific agency, conducting end-to-end scientific investigations from question formulation to solution discovery (Yamada et al., 2025).</p>
<p>Conceptualized Framework</p>
<p>Our Definition: An AI Scientist is an advanced end-to-end system capable of independently formulating scientific ideas and executing the requisite verification and falsification procedures.</p>
<p>We define an AI Scientist, denoted as  AI , as a fully autonomous scientific intelligence capable of independently performing diverse scientific research tasks.Different from a general scientific tool, it must possess dual capacities, including idea generation and experimental execution.A complete scientific research task typically originates from an initial scientific question  init and leverages existing domain knowledge  domain .An AI Scientist, denoted  AI , operates within the scope of human ethical constraints ℛ human and resource constraints ℬ res to conduct this task.The primary output is the generation of novel scientific knowledge  new and associated verifiable artifacts  sci .The process through which an AI Scientist aims to achieve the optimal output from a scientific research task can be formally represented as:
(𝒦 new , 𝒜 sci ) ← max{𝒮 AI (𝒬 init , 𝒦 domain , ℛ human |θ AI , ℬ res )} (1)</p>
<p>Arguments for Implementation Capability</p>
<p>We argue that the fundamental bottleneck limiting AI Scientists lies not in their idea generation capabilities, but in their capacity to execute rigorous implementation procedures required for reliable scientific research.To support this position, we present three lines of evidence: systematic analysis of research trends in the AI Scientist literature (Section 3.1), comprehensive benchmark analysis across multiple evaluation frameworks (Section 3.2), and systematic peer review assessment using LLM-as-a-Judge methodology (Section 3.3).</p>
<p>Research Trend of AI Scientist</p>
<p>Our statistical analysis of AI Scientist papers on arXiv up to May 23, 2025 (see Appendix B for details), reveals key trends illustrated in Figure 3.The lower panel of the figure shows that while the total number of publications is growing, studies focusing on idea generation without concrete implementation details consistently outnumber those incorporating such implementations.Despite this disparity in publication numbers, the upper panel indicates a crucial counterpoint: papers that include substantive implementation details achieve a significantly higher average number of citations.This signals strong community valuation for executable advancements and underscores the importance of addressing the implementation gap.This then raises a critical question: if implementation-focused research garners higher impact, why does its volume remain markedly lower?This disparity strongly implies that the path of implementation is fraught with substantial challenges.Empirical Evidence of Implementation Gap.Advanced LLMs achieve near-saturated performance on simple code generation benchmarks like HumanEval (Chen et al., 2021, Liu et al., 2023, Yang et al., 2025a).For example, o3 exhibits excellent problem-solving capabilities in the 99.8th percentile of human performance on algorithmic competition platforms like Codeforces.However, the performance of SoTA LLMs drops dramatically when it comes to real-world research scenarios.As depicted in  (Siegel et al., 2024) (reproducing computational results from scientific papers, determined by accuracy), and ML-Dev-Bench (Padigela et al., 2025) (completing diverse ML development workflow tasks, assessed by success rates).Each takes a different approach to measuring how well AI systems can automate aspects of ML research.These evaluations consistently demonstrate that LLMs face difficulty in translating conceptual understanding or initial plans into verifiably correct and operational code.This "implementation gap" fundamentally limits AI Scientist's verification capabilities.</p>
<p>Quantitative Analysis</p>
<p>Beyond Code Generation.The complexity of real-world research implementation processes extends far beyond simple code generation tasks, often requiring sustained reasoning and multi-step problem-solving.However, current LLMs exhibit relatively weak performance on such complex challenges.LiveCodeBench (LCB) (Jain et al., 2024), a more complex evaluation benchmark than Humaneval (Chen et al., 2021) that collects problems from periodic contests on LeetCode, AtCoder, and Codeforces platforms, evaluates Code LLMs across diverse code-related scenarios, including code generation, execution, self-repair, and output prediction.o4-mini achieves SoTA performance on the code generation subtask with only 52.1% pass@1 score.This poor performance on complex coding tasks reveals that AI scientists lack the implementation ability to handle sophisticated code-based research scenarios.</p>
<p>Implementation and verification.We observe that the verification bottleneck emerges across multiple stages of the research process.SciReplicate-Bench (Xiang et al., 2025), which tasks LLM agents with generating Python code to reproduce algorithms from NLP research papers, reveals that despite agents demonstrating an understanding of algorithmic logic (evidenced by high reasoning graph accuracy), they struggle with code execution.The best agent achieved only 39% execution accuracy, indicating its generated code passed functional test cases for just 39% of the tasks, highlighting a failure to ensure implementation correctness and runtime behavior.Similarly, PaperBench (Starace et al., 2025) requires LLM agents to replicate entire machine-learning papers from scratch by developing codebases and running experiments.While agents can generate code components (e.g., o1-High achieving 43.4% success on weighted "Code-Development" sub-tasks), their performance on subsequent verification stages is poor.On rubric-defined leaf nodes for "Execution" (successfully running the code) and "Result Match" (quantitatively matching the paper's reported results), Claude 3.5 Sonnet scored only 1.8% and 0.7% respectively.This poor performance indicates a breakdown in ensuring the developed solution operates correctly and produces the intended outcomes.</p>
<p>Discussion.The verification challenge extends beyond initial code implementation to debugging, iterative refinement, and validation of experimental outcomes.Evidence from MLE-Bench and ML-Dev-Bench (Chan et al., 2024, Padigela et al., 2025) shows that LLM agents frequently fail to debug their code or produce valid submissions, with 20% of o1 preview runs on MLE Bench failing this step, and struggle to optimize model performance.Debugging, an explicit verification procedure, also indicates persistent agent failures that highlight the verification bottleneck (Chan et al., 2024).The incapacity to iteratively refine solutions towards better performance, illustrated in ML-Dev-Bench where all tested agents scored 0% on "Model Performance" tasks, further signifies deficiencies in robust verification loops essential for scientific advancement (Padigela et al., 2025).Furthermore, CORE-Bench, which requires agents to reproduce results and then answer questions based on these outputs, assesses the verification of entire computational experiments.This process, involving multiple stages of reproduction and reasoning, presents significant challenges.For instance, the imperfect success rates (e.g., CORE-Agent with GPT-4o achieved 55.56% on CORE-Bench Medium) highlight the difficulties in this complex verification process (Siegel et al., 2024).These difficulties across verification tasks suggest that enhancing AI Scientists' systematic verification capability is crucial for their maturation into ideal AI Scientists.Current LLMs, while proficient in content generation, fail to rigorously validate their outputs against explicit criteria, a foundational component of scientific practice.</p>
<p>LLM-as-a-Judge Reveals the Implementation Weaknesses</p>
<p>To further support the existence of implementation gap, we employ a simulated peer review methodology to assess the actual quality of scientific outputs from current AI Scientist systems, particularly their implementation-level reliability.We select 28 publicly available research papers generated independently by five different AI Scientist systems and utilize the SoTA review model DeepReviewer-14B (Zhu et al., 2025) to conduct systematic evaluation under unified standards.We acknowledge that potential selection bias in the public availability of these papers (e.g., researchers may only publish better-performing outputs) means our evaluation results may not fully represent the average output quality of these systems across all scenarios.Nevertheless, this analysis provides valuable insights into the general quality level of current AI-generated research papers.</p>
<p>Rooted Limitations of Execution Capabilities</p>
<p>Our empirical analysis (Section 3) reveals a clear pattern that while AI Scientists are conceptualized as advanced iterations of traditional scientific tools, they consistently fail at implementation and verification procedures across diverse scientific contexts.This raises a critical question: Why do these sophisticated systems fail to achieve consistently strong results, especially when traditional scientific tools, wielded by human researchers, prove highly effective?To understand this paradox, we provide a discussion on the root cause of the implementation gap (Section 4.1 ) and present an in-depth analysis of the fundamental limitations of AI Scientist (Section 4.2).</p>
<p>Two Primary Facets of Implementation Gap</p>
<p>The implementation gap for AI Scientists comprises two primary facets: (1) AI Scientists often exhibit bottlenecks in the planning and execution stages.This manifests in three key areas: failures in longrange logical reasoning required for coherent experimental design, inadequate multi-agent collaboration capabilities including strategic planning across complex multi-file implementations and converting conceptual ideas into working code, and insufficient coordination with external tools and systems; (2) Even when implementation code is generated, AI Scientists demonstrate fundamental weaknesses in evaluation processes.This includes failures in debugging capabilities, experimental validation, result interpretation, and iterative refinement based on experimental feedback.Current systems lack robust mechanisms for assessing implementation quality, validating experimental outcomes, and providing reliable feedback loops that can guide subsequent implementation improvements.</p>
<p>Prevent building "castle in the air".Agent tools often produce difficult-to-verify code and experiments, while evaluation gaps prevent AI Scientists from recognizing and correcting implementation issues through iterative refinement.Without fundamentally enhancing both capabilities, the idealized AI Scientist capable of independent scientific exploration will remain inefficient.</p>
<p>Rooted Limitations</p>
<p>From the current literature on AI scientists, we conclude four major limitations that collectively explain why AI scientists struggle with complex, multi-stage implementation processes:</p>
<p>Limitation 1: fundamental cognitive and execution capabilities.Scientific implementation requires sophisticated long-range logical reasoning across multiple abstraction levels.Existing LLMs demonstrate significantly decreased coherence and robustness as reasoning chains extend (Wu et al., 2025b,a), and increased thinking time does not necessarily yield stronger performance (Ballon et al., 2025).Furthermore, LLM-based agents possess limited capacity to retain past interaction information, with memory deteriorating as text length increases (Pink et al., 2025, Cemri et al., 2025).Most critically, mainstream language models exhibit markedly weaker performance in multi-turn dialogues or multi-step interactive tasks requiring context coherence, deep understanding, and state tracking, with average performance decreases reaching 39% (Laban et al., 2025).This capability degradation in scenarios involving long-range dependencies and complex interactions directly constrains AI Scientist performance in executing complex scientific experiments requiring sustained attention and coherent reasoning chains.</p>
<p>Limitation 2: strategic planning and reasoning.Scientific implementation requires comprehensive abilities for strategic reasoning, continuous monitoring, and dynamic adjustment across all research stages (Lu et al., 2024, Yamada et al., 2025).High-quality research implementation demands global planning abilities spanning entire codebases, which typically contain multiple interdependent files with hundreds of lines requiring coordinated modification (Jimenez et al., 2024, Aleithan et al., 2024).Long-term, complex scientific exploration tasks such as discovering new materials, and modeling complex biological systems particularly require continuous iteration of research directions and experimental strategies over extended time scales based on emerging results and external feedback (Merchant et al., 2023, Brixi et al., 2025, Weng et al., 2023).However, current LLMs demonstrate inadequate adaptive planning and metacognitive abilities when handling highly open, creative scientific research requiring dynamic adjustments to overall research blueprints.While reinforcement learning approaches may potentially enhance LLMs' generalization and metacognitive capabilities, the resource investment required for "inventor" roles like AI Scientists that need to perform complex asynchronous operations and real-world interactions proves enormous.Figure 4 highlights AI's acceleration over human performance in complex tasks such as reasoning and web-based research.While AI Scientists also achieve tasks faster than humans, their estimated single-sample RL training time is orders of magnitude greater than simpler AI agents.This substantial increase in required sampling time (detailed in Appendix A) underscores the immense challenge of developing AI Scientists via standard RL methodologies.(Guo et al., 2024, Qian et al., 2024, Pu et al., 2025b).This requires AI Scientist to not only understand instructions conforming to collaborative protocols but also precisely execute the implementation phases assigned to it within tasks and reliably feed its outputs back to the collaborative network (Bo et al., 2024, Zhang et al., 2024).However, current LLM Agents still have considerable room for improvement in robustness and adaptability when interacting with dynamic environments (Wei et al., 2025).For instance, when calling a series of external APIs to complete a complex scientific computational process, LLM often struggles to handle subtle changes in API interfaces, and other practical engineering issues (Shen et al., 2025).</p>
<p>Limitation 4: evaluation and verification.Existing benchmarks such as MLE-Bench (Chan et al., 2024) and PaperBench (Starace et al., 2025) primarily focus on the complete reproduction of code and experiments from papers.SciReplicate-Bench (Xiang et al., 2025) emphasizes generating necessary code from scientific papers, while ScienceAgentBench (Chen et al., 2025) concentrates on independent and singular data-driven tasks.However, there is currently a lack of a comprehensive benchmark that can evaluate the entire scientific workflow, from initial idea generation through to final implementation and completion.This absence makes it difficult to fairly compare the end-to-end capabilities of different AI Scientist systems.</p>
<p>Additionally, there is a deficiency in evaluation approaches that incorporate measures for external supervision during the AI Scientist's implementation process.The deeper issue is that the quality of scientific discovery often lacks unified objective standards, and the process of scientific exploration is filled with uncertainty and openness, making comprehensive evaluation and effective supervision of AI Scientist's verification capability exceptionally difficult.Evaluating AI Scientist's output (e.g., generated papers) from a peer review perspective, while being a results-oriented assessment method, also has inherent limitations.As in human research systems, even experienced peer reviewers may not always accurately identify the groundbreaking and far-reaching work.A frequently cited example is that the word2vec paper (Mikolov et al., 2013) was initially rejected by ICLR 2013, but later received the "Test of Time Award" at NeurIPS 2023.Extensive experimental analyses have demonstrated that review scores are not reliable indicators for predicting future impact (Abramo et al., 2019, Cortes andLawrence, 2021), suggesting that peer review may be more suitable for filtering low-quality papers rather than identifying the highest quality papers.</p>
<p>Ethical Considerations</p>
<p>Sub-Position: AI scientists are in urgent need of a comprehensive system for generation management and quality evaluation.</p>
<p>As autonomous research agents, AI Scientists lack values and moral constraints.They are incapable of making ethical judgments about the societal impact of their work, and they do not self-regulate based on potential risks associated with their findings (Bengio et al., 2025).As AI Scientists possess stronger capabilities in idea generation and experiment execution, their influence on scientific research and society could far surpass that of current LLMs and scientific tools (e.g., Deep Search, AutoSurvey (Wang et al., 2024c)).In the absence of proper oversight, AI Scientists may: (1) be misused, overwhelming the peer review system, leading to a decline in overall research quality;</p>
<p>(2) enter unethical or dangerous research domains, autonomously generating and publishing sensitive findings that accelerate the development of harmful technologies;</p>
<p>(3) weaken the quality of PhD training, leading to a decline in human research standards and overall scientific literacy.To prevent the above situations, we argue that AI Scientists are in urgent need of a comprehensive system for generation management and quality evaluation, thus enabling effective behavior regulation within the human moral framework (Jobin et al., 2019).This system should include, but not be limited to, the following components:</p>
<p>(1) Implement measures to prevent AI-generated content from disrupting human review systems: Effective strategies should be adopted to ensure that AI-generated articles do not interfere with human peer-review systems while maintaining high standards of quality.This includes establishing a centralized platform to archive scientific outputs generated by AI Scientists, developing automated detection systems to identify such content, and creating specialized evaluation tools (e.g., DeepReview (Zhu et al., 2025)) to assess the quality of AI-generated research outputs.These tools should help identify and filter low-quality content, thereby reducing the burden on the peer review process.All AI-generated outputs must be transparently labeled and reviewed, including information on their origin, generation methods, and scientific tools.</p>
<p>(2) Establish boundaries and strengthen training programs: Implement clear boundaries between human-led and AI-led research processes to ensure that PhD students receive comprehensive training.Key components of doctoral education(e.g., idea testing), should prioritize human involvement to maintain high standards of scientific literacy.Additionally, guidelines should be established to prevent over-reliance on AI Scientists in PhD training, ensuring that AI tools serve as supplements rather than substitutes in the educational process.</p>
<p>(3) Formulate an ethics and responsibility convention: A global convention should be established to define the ethical boundaries and risk management principles for AI-driven research (Huang et al., 2022).</p>
<p>All researchers and institutions utilizing AI Scientists must fully disclose the generation process, algorithmic sources, training data, and potential societal risks of their findings.Additionally, a hybrid mechanism combining automated and human-in-the-loop review should be implemented for continuous ethical oversight and risk evaluation, ensuring that AI Scientist research activities remain within socially safe boundaries (Jobin et al., 2019, Khan et al., 2022).Furthermore, appropriate legislation should be developed to regulate AI Scientists by imposing strict limitations on their use for specific research purposes.</p>
<p>Future Directions</p>
<p>This section outlines feasible pathways to bridge the current implementation capability gap of AI Scientists.</p>
<p>Addressing foundational Basic Abilities is paramount.While scaling laws for pre-training and post-training (Kaplan et al., 2020, Zhang et al., 2025) promise progressive LLM improvements, immediate strategies like well-defined Workflows (Li et al., 2024d, Gu et al., 2024b) also can mitigate current implementation weaknesses.Structuring research processes with human-defined tools allows for guided AI execution and targeted interventions.For instance, Retrieval Augmented Generation (RAG) can counteract limitations in handling long texts or accessing current information (Fan et al., 2024, Arslan et al., 2024), thus expanding the knowledge scope of AI systems.</p>
<p>A significant challenge for sophisticated Strategic Planning is the immense resource consumption of RL (Cao et al., 2024).A promising direction to alleviate this involves leveraging LLMs to simulate aspects of the environment or task execution, thereby accelerating the RL feedback loop (Sun et al., 2025).By allowing the RL agent to receive quicker, albeit potentially approximate, feedback on its actions, particularly for operations that are inherently time-consuming in the real world, the sampling efficiency may be significantly improved.This could reduce the extensive wall-clock time typically required for training robust long-horizon planning and adaptive meta-thinking capabilities in complex scientific domains.</p>
<p>Ensuring Reliable Verification and Fostering Collaboration is crucial.Standardized protocols like MCP and A2A (Yang et al., 2025b, Ray, 2025, Hou et al., 2025) can establish basic interoperability.A promising direction is to build modular multi-agent systems, where specialized AI agents for sub-tasks (e.g., literature review, code generation) are coordinated by a central "Planner Agent" trained via advanced RL, leveraging existing tools (e.g., PASA (He et al., 2025)) rather than reinventing capabilities.Furthermore, enhanced oversight of AI Scientist inference processes is imperative, not just to prevent benchmark "hacking", but also to instill ethical boundaries against unscrupulous data acquisition or other problematic behaviors.</p>
<p>Finally, the Evaluation of AI Scientists (Chang et al., 2024) must evolve towards a holistic, coarse-grained paradigm reflecting real-world scientific discovery's multifaceted nature.Scientific breakthroughs involve both practical utility and novelty.Thus, evaluation frameworks should go beyond single-metric optimization, adopting multi-objective criteria that assess performance gain, originality, experimental rigor, and communication clarity.This multi-faceted approach will offer a more accurate measure of an AI Scientist's true contribution, guiding development toward impactful scientific exploration.</p>
<p>Conclusion</p>
<p>The rise of AI Scientists marks a paradigm shift in scientific discovery, with large language models (LLMs) now driving the workflow from idea generation to experiment execution.Recent systems have shown promise, producing research accepted at ICLR 2025 workshops and sparking discussions on the imminence of human-level AI Scientists.However, despite this progress, AI Scientists have yet to achieve breakthroughs in computer science comparable to traditional automated tools.Based on benchmark analyses and a systematic evaluation of 28 papers from five leading AI Scientist systems, we identify a core bottleneck: the inability to reliably execute and verify experiments.This implementation gap limits both scientific rigor and the quality of the research output.We analyze its root causes and call on the community to address this critical limitation.</p>
<p>Alternative Views.An alternative viewpoint suggests that AI Scientists need not pursue completely autonomous implementation capabilities in the short term, but rather facilitate human-machine collaboration as Co-scientists to assist humans.This approach avoids the deficiencies of LLMs in Dynamic Planning capabilities and Reliable Verification capabilities, instead allowing AI to focus on its strengths, such as idea generation, while humans execute the specific experimental results (Weng et al., 2025).If an AI system, though unable to independently complete all implementation details, can increase human scientists' efficiency tenfold, or help human scientists conceive and verify complex ideas previously beyond reach, then it undoubtedly also qualifies as a successful "collaborative scientist."</p>
<p>Unite</p>
<p>A. Sampling Time Calculation for Different Types of AI Agents</p>
<p>We referenced existing literature (Guo et al., 2025, Yang et al., 2025a, Muennighoff et al., 2025) and our experience to estimate the sampling time potentially required for different types of AI agents trained via reinforcement learning, as illustrated in In contrast, an AI Scientist executing end-to-end scientific discovery tasks has complexity and interaction requirements far exceeding the previous two types.We roughly estimate it might need to generate over 100,000 tokens of content (for example, operational and experimental code approximately 50,000 tokens (T in f er_code ≈ 1250s), research paper writing approximately 30,000 tokens (T in f er_paper ≈ 750s), reviewing and understanding relevant literature approximately 20,000 tokens (T in f er_lit ≈ 500s)), with pure LLM inference time for just this portion being T in f er_CS = T in f er_code + T in f er_paper + T in f er_lit ≈ 2500s.More critically, the "implementation" process of an AI Scientist, such as code writing, debugging, compiling, running experiments, and data analysis, is highly asynchronous and time-consuming.Assuming a rapid research code operation and experimental cycle (from writing to obtaining preliminary results) requires an average of T op_code ≈ 12 hours = 43200s, while in-depth literature research and analysis might require T op_lit ≈ 20 minutes = 1200s.Therefore, the total estimated sampling time to complete a relatively complete scientific exploration loop would be T sample_CS = T in f er_CS + T op_code + T op_lit ≈ 2500s + 43200s + 1200s ≈ 46900s.As intuitively demonstrated in Figure 4, the sampling time required for an AI Scientist (approximately 46,000 seconds) far exceeds that of an AI Reasoner (approximately 250 seconds) and an AI Web Agent (approximately 700 seconds).Notably, AI Reasoners can typically rapidly generate large quantities of training samples through batch generation in parallel, whereas each implementation step of an AI Scientist (especially parts involving code execution and experiment waiting) is almost entirely asynchronous, and requires exclusive computational resources or experimental equipment for learning and feedback collection during operations.Consequently, in actual reinforcement learning training processes, the disparity in real training duration between AI Scientists and the former two types will be even more pronounced.</p>
<p>For the calculation of human duration, we referenced existing metrics.For instance, for reasoning tasks, we referred to the human time from the International Mathematical Olympiad, which is approximately 1.5 hours per problem.For Web Agent tasks, we adopted the average human problem-solving time from BrowseComp (Wei et al., 2025) (2 hours) as the human standard.For Scientist tasks, although each paper often requires months of collaborative work by multiple people, for ease of calculation, we used the human duration of 48 hours from PaperBench (Starace et al., 2025) for statistics; however, even under these conditions, humans achieve a success rate of less than 50%.(Pu et al., 2025a), (Yang et al., 2024), (Su et al., 2024), (Li et al., 2024a), (Hu et al., 2024), (Liu et al., 2025), (Wang et al., 2024b) (Weng et al., 2025), (Xiong et al., 2024) (Gu et al., 2024a), (Li et al., 2024b), (Yu et al., 2024) (Gottweis et al., 2025) (Rabby et al., 2025), (Saeedi et al., 2025) (O'Neill et al., 2025), (Garikaparthi et al., 2025), (Sanyal et al., 2025) w/ Exp (Lu et al., 2024), (Li et al., 2024c) (Liu et al., 2024b), (Liu et al., 2024a) (Yuan et al., 2025), (Schmidgall et al., 2025) (Jiang et al., 2025), (Kon et al., 2025) (Schmidgall et al., 2025), (Jansen et al., 2025) (Yamada et al., 2025), (Seo et al., 2025)</p>
<p>B. Regarding the statistics for the papers</p>
<p>We have conducted a comprehensive search on arXiv to gather relevant publications in the AI Scientist field.This collection includes a series of papers from August 2024 to April 2025 for methods or systems, which are cited in Table 4.It indicates that, to date, a significant number of papers have focused on Idea Generation tasks, often without concrete implementations.</p>
<p>Nevertheless, an encouraging trend has emerged since early 2025.As illustrated in Figure 3, implementationfocused research has demonstrated stronger growth momentum, with incremental growth nearly matching that of non-implementation studies by Spring 2025.This suggests the community is beginning to recognize the critical importance of implementation capabilities for developing truly effective AI Scientists-moving beyond theoretical constructs toward practical systems capable of reliable execution.</p>
<p>Figure 1 :
1
Figure 1: The roadmap of AI Scientist from 2024 to future, highlighting key milestones and fundamental challenges that must be overcome to bridge the implementation gap of AI Scientist.</p>
<p>Figure 3 :
3
Figure 3: Analysis of AI Scientist publications on arXiv.The upper panel displays the average number of citations up to now, categorized by containing implementation details.The lower panel shows the growth in the total number of these papers with the same categorization.</p>
<p>Figure 4 .
4
Using a hypothetical 671B parameter LLM (similar to Deepseek-R1) running on 8 H100 cards (assuming 40 tokens generated per second), the pure inference time T in f er_R for a typical arithmetic reasoning task (generating approximately 10,000 tokens of reasoning content) might be around 250 seconds.For an AI Web Agent, the task might include generating approximately 8,000 tokens of instructions and reports (T in f er_WA ≈ 200s), interspersed with approximately 20 API calls for information search (assuming each search and processing takes T search_API = 10s, totaling T search_total = 20 × 10s = 200s), and potentially requiring reading and comprehension of up to 400,000 tokens of web content (assuming reading and comprehension time T read_WA ≈ 200s).The total sampling time is: T sample_WA ≈ T in f er_WA + T search_total + T read_WA ≈ 600s.</p>
<p>Table 1 :
1
State-of-the-art (SoTA) LLMs show relatively low accuracy on code implementation on different tasks.The listed benchmarks are collected from diverse domains.The table below details their tasks, domains, scale, methods, and performance.
Benchmark Task DescriptionDomainsScaleLLM Acc. PerformanceMLE-Bench(Chan et al., 2024) AI Training taskApplied ML75 OpenAI o1-preview16.90%PaperBench (Starace et al., 2025) ICML paper ReplicatingNLP, CV, ML8,316OpenAI o1-high26.00%SciReplicate-Bench (Xiang et al., 2025) Code GenerationNLP100Claude-Sonnet-3.739.00%CORE-Bench (Siegel et al., 2024) Scientific Paper reproduc-Computer Science,270OpenAI GPT-4o55.56%tionSocial Science, andMedicineML-Dev-Bench (Padigela et al., 2025) AI training taskML30Claude-Sonnet-3.550.00%</p>
<p>Table 2 :
2
DeepReviewer-14B Evaluation of AI-Generated Papers from Various AI Scientist Systems.Scores reflect averages across the 'Num' of available papers.Note: Publicly available papers may be curated and not fully representative of typical system output.
AI Scientist SystemNum Soundness↑Presentation↑Contribution↑ Decision↑Rating↑Percentile↑HKUSD AI Researcher71.751.461.570.02.573.43%AI Scientist102.081.801.750.03.358.22%AI Scientist v231.671.501.500.02.332.04%CycleResearcher-12B62.251.752.130.03.7516.88%Zochi22.382.382.250.04.6329.96%</p>
<p>Table 3 :
3
Defect Categories and Their Issues.
Defect CategoryNumber PercentageExperimental Weakness28100%Methodological Unclarity/Flaws2796.4%Writing &amp; Presentation Issues2692.9%Novelty Concerns2589.3%Theoretical Weakness2485.7%Literature Review Deficiencies2278.6%Practicality &amp; Robustness Gaps2175.0%Reproducibility Issues2071.4%Computational Cost Concerns1864.3%Component Analysis1657.1%Hyperparameter Analysis Lacking1657.1%Ethical Considerations Missing310.7%</p>
<p>Table3shows that among the twelve major defect categories, "Experimental Weakness" appears across all 28 evaluated AI-generated papers, with a 100% occurrence rate.
This finding supports our positions regardingimplementation capability limitations, in experimental design, execution, and result analysis. The secondand third most prevalent issues are "Methodological Unclarity/Flaws" (96.4%) and "Writing &amp; PresentationIssues" (92.9%), which reflect AI Scientists' insufficient ability to clearly articulate and implement researchplans. "Novelty Concerns" (89.3%) and "Theoretical Weakness" (85.7%) occur frequently, indicating that
when AI Scientists generate complete papers, they struggle to propose original scientific contributions with solid theoretical foundations.The prevalence of these high-frequency defects highlights systemic issues in the scientific rigor and implementation quality of current AI-generated research, falling below the standards for reliable and valuable scientific outputs.</p>
<p>Reem Aleithan, Haoran Xue, Mohammad Mahdi Mohajer, Elijah Nnorom, Gias Uddin, and Song Wang.
AI.Google'snewai"co-scientist"aimstoacceleratescien-tificdiscovery.Unite.AI,Feb2025.URLhttps://www.unite.ai/googles-new-ai-co-scientist-aims-to-accelerate-scientific-discovery/.
Swe-bench+: Enhanced coding benchmark for llms.arXiv preprint arXiv:2410.06992,2024.</p>
<p>Table 4 :
4
Timeline of AI Scientist Ideas and Code Implementations by Month
2024-082024-092024-102024-112024-122025-012025-022025-032025-04w/o Exp (Zheng et al.,(Ghafarollahi2024)and Buehler,2024), (Raden-sky et al.,2024)
Minjun Zhu and Qiujie Xie contributed equally to this work. Corresponding author(s): Linyi Yang: yanglinyiucd@gmail.com; Yue Zhang: Email zhangyue@westlake.edu.cn
https://ai-researcher.net/social-iclr-2025
AcknowledgementsThe genesis of this position paper traces back to the insightful discussions and interactions at the AI Co-scientist Discussion held in conjunction with ICLR 2025 on April 26, 20241 .We extend our sincere gratitude to the invited speakers, including Chenglei Si, Jindong Wang, Yutaro Yamada, and David Ha, whose perspectives are invaluable.We also deeply appreciate the contributions of the more than 200 participants who engaged in the vibrant discussions on that day; many of the ideas explored in this work were sparked and refined through those collective interactions.We thank every participant for their engagement and for fostering a stimulating environment that significantly shaped our thinking.
Peer review versus bibliometrics: Which method better predicts the scholarly impact of publications?. Giovanni Abramo, Ciriaco Andrea, D' Angelo, Emanuela Reale, Scientometrics. 1212019</p>
<p>A survey on rag with llms. Muhammad Arslan, Hussam Ghanem, Saba Munawar, Christophe Cruz, Procedia Computer Science. 2462024</p>
<p>The relationship between reasoning and performance in large language models-o3 (mini) thinks harder. Marthe Ballon, Andres Algaba, Vincent Ginis, arXiv:2502.156312025not longer. arXiv preprint</p>
<p>Superintelligent agents pose catastrophic risks: Can scientist ai offer a safer path. Yoshua Bengio, Michael Cohen, Damiano Fornasiere, Joumana Ghosn, Pietro Greiner, Matt Macdermott, Sören Mindermann, Adam Oberman, Jesse Richardson, Oliver Richardson, arXiv:2502.156572025arXiv preprint</p>
<p>Reflective multi-agent collaboration based on large language models. Xiaohe Bo, Zeyu Zhang, Quanyu Dai, Xueyang Feng, Lei Wang, Rui Li, Xu Chen, Ji-Rong Wen, Advances in Neural Information Processing Systems. 202437</p>
<p>Autonomous chemical research with large language models. Robert Daniil A Boiko, Ben Macknight, Gabe Kline, Gomes, Nature. 62479922023</p>
<p>Genome modeling and design across all domains of life with evo 2. Garyk Brixi, Matthew G Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A Gonzalez, Samuel H King, David B Li, Aditi T Merchant, BioRxiv. 2025</p>
<p>Yuji Cao, Huan Zhao, Yuheng Cheng, Ting Shu, Yue Chen, Guolong Liu, Gaoqi Liang, Junhua Zhao, Jinyue Yan, Yun Li, Survey on large language model-enhanced reinforcement learning: Concept, taxonomy, and methods. IEEE Transactions on Neural Networks and Learning Systems. 2024</p>
<p>Why do multi-agent llm systems fail?. Mert Cemri, Melissa Z Pan, Shuyi Yang, Lakshya A Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Klein, Kannan Ramchandran, arXiv:2503.136572025arXiv preprint</p>
<p>Exploring scientific hypothesis generation with mamba. Miaosen Chai, Emily Herron, Erick Cervantes, Tirthankar Ghosal, Proceedings of the 1st Workshop on NLP for Science (NLP4Science). the 1st Workshop on NLP for Science (NLP4Science)2024</p>
<p>Mle-bench: Evaluating machine learning agents on machine learning engineering. Neil Jun Shern Chan, Oliver Chowdhury, James Jaffe, Dane Aung, Evan Sherburn, Giulio Mays, Kevin Starace, Leon Liu, Tejal Maksin, Patwardhan, arXiv:2410.070952024arXiv preprint</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S Yu, Qiang Yang, Xing Xie, 10.1145/3641289ACM Trans. Intell. Syst. Technol. 2157-6904153March 2024</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De, Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery. Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Corinna Cortes, Neil D Lawrence, arXiv:2109.09774Inconsistency in conference peer review: Revisiting the 2014 neurips experiment. 2021arXiv preprint</p>
<p>A survey on rag meeting llms: Towards retrieval-augmented large language models. Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, Qing Li, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>Aniketh Garikaparthi, Manasi Patwardhan, arXiv:2504.16728Lovekesh Vig, and Arman Cohan. Iris: Interactive research ideation system for accelerating scientific discovery. 2025arXiv preprint</p>
<p>Sciagents: Automating scientific discovery through multi-agent intelligent graph reasoning. Alireza Ghafarollahi, Markus J Buehler, arXiv:2409.055562024arXiv preprint</p>
<p>Towards an ai co-scientist. Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, arXiv:2502.188642025arXiv preprint</p>
<p>Llms can realize combinatorial creativity: generating creative ideas via llms for scientific research. Tianyang Gu, Jingjin Wang, Zhihao Zhang, Haohong Li, arXiv:2412.141412024aarXiv preprint</p>
<p>Large language models for constructing and optimizing machine learning workflows: A survey. Yang Gu, Hengyu You, Jian Cao, Muran Yu, Haoran Fan, Shiyou Qian, arXiv:2411.104782024barXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Large language model based multi-agents: A survey of progress and challenges. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, arXiv:2402.016802024arXiv preprint</p>
<p>Pasa: An llm agent for comprehensive academic paper search. Yichen He, Guanhua Huang, Peiyuan Feng, Yuan Lin, Yuchen Zhang, Hang Li, arXiv:2501.101202025arXiv preprint</p>
<p>Model context protocol (mcp): Landscape, security threats, and future research directions. Xinyi Hou, Yanjie Zhao, Shenao Wang, Haoyu Wang, arXiv:2503.232782025arXiv preprint</p>
<p>Nova: An iterative planning and search approach to enhance novelty and diversity of llm generated ideas. Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, Zhenzhong Lan, arXiv:2410.142552024arXiv preprint</p>
<p>An overview of artificial intelligence ethics. Changwu Huang, Zeqi Zhang, Bifei Mao, Xin Yao, IEEE Transactions on Artificial Intelligence. 442022</p>
<p>. Intology. Zochi technical report. arXiv. 2025</p>
<p>Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, arXiv:2403.079742024arXiv preprint</p>
<p>Codescientist: End-to-end semi-automated scientific discovery with code-based experimentation. Peter Jansen, Oyvind Tafjord, Marissa Radensky, Pao Siangliulue, Tom Hope, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Daniel S Weld, Peter Clark, arXiv:2503.227082025arXiv preprint</p>
<p>Aide: Ai-driven exploration in the space of code. Zhengyao Jiang, Dominik Schmidt, Dhruv Srikanth, Dixing Xu, Ian Kaplan, Deniss Jacenko, Yuxiang Wu, arXiv:2502.131382025arXiv preprint</p>
<p>Swe-bench: Can language models resolve real-world github issues. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik R Narasimhan, ICLR. 2024</p>
<p>The global landscape of ai ethics guidelines. Anna Jobin, Marcello Ienca, Effy Vayena, Nature machine intelligence. 192019</p>
<p>Highly accurate protein structure prediction with alphafold. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, nature. 59678732021</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Ethics of ai: A systematic literature review of principles and challenges. Arif Ali Khan, Sher Badshah, Peng Liang, Muhammad Waseem, Bilal Khan, Aakash Ahmad, Mahdi Fahmideh, Mahmood Niazi, Muhammad Azeem, Akbar , Proceedings of the 26th international conference on evaluation and assessment in software engineering. the 26th international conference on evaluation and assessment in software engineering2022</p>
<p>The automation of science. Jem Ross D King, Stephen G Rowland, Michael Oliver, Wayne Young, Emma Aubrey, Maria Byrne, Magdalena Liakata, Pinar Markham, Larisa N Pir, Soldatova, Science. 32459232009</p>
<p>Curie: Toward rigorous and automated scientific experimentation with ai agents. Patrick Tser, Jern Kon, Jiachen Liu, Qiuyi Ding, Yiming Qiu, Zhenning Yang, Yibo Huang, Jayanth Srinivasa, Myungjin Lee, Mosharaf Chowdhury, Ang Chen, arXiv:2502.160692025arXiv preprint</p>
<p>Philippe Laban, Hiroaki Hayashi, Yingbo Zhou, Jennifer Neville, arXiv:2505.06120Llms get lost in multi-turn conversation. 2025arXiv preprint</p>
<p>Scientific discovery: Computational explorations of the creative processes. Langley, 1987MIT Press</p>
<p>Chain of ideas: Revolutionizing research via novel idea development with llm agents. Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xingxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming Jiang, Yifei Xin, Ronghao Dang, arXiv:2410.131852024aarXiv preprint</p>
<p>Learning to generate research idea with dynamic control. Ruochen Li, Liqiang Jing, Chi Han, Jiawei Zhou, Xinya Du, arXiv:2412.146262024barXiv preprint</p>
<p>Mlr-copilot: Autonomous machine learning research based on large language models agents. Ruochen Li, Teerth Patel, Qingyun Wang, Xinya Du, arXiv:2408.140332024carXiv preprint</p>
<p>Autoflow: Automated workflow generation for large language model agents. Zelong Li, Shuyuan Xu, Kai Mei, Wenyue Hua, Balaji Rama, Om Raheja, Hao Wang, He Zhu, Yongfeng Zhang, arXiv:2407.128212024darXiv preprint</p>
<p>Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, Lingming Zhang, 202336</p>
<p>Drugagent: Automating ai-aided drug discovery programming through llm multi-agent collaboration. Sizhe Liu, Yizhou Lu, Siyu Chen, Xiyang Hu, Jieyu Zhao, Yingzhou Lu, Yue Zhao, arXiv:2411.156922024aarXiv preprint</p>
<p>Researchbench: Benchmarking llms in scientific discovery via inspiration-based task decomposition. Yujie Liu, Zonglin Yang, Tong Xie, Jinjie Ni, Ben Gao, Yuqiang Li, Shixiang Tang, Wanli Ouyang, Erik Cambria, Dongzhan Zhou, arXiv:2503.212482025arXiv preprint</p>
<p>Aigs: Generating science from ai-powered automated falsification. Zijun Liu, Kaiming Liu, Yiqi Zhu, Xuanyu Lei, Zonghan Yang, Zhenhe Zhang, Peng Li, Yang Liu, arXiv:2411.119102024barXiv preprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.06292v32024arXiv preprint</p>
<p>Gowoon Cheon, and Ekin Dogus Cubuk. Scaling deep learning for materials discovery. Amil Merchant, Simon Batzner, Muratahan Samuel S Schoenholz, Aykol, Nature. 62479902023</p>
<p>Distributed representations of words and phrases and their compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, Jeff Dean, Advances in neural information processing systems. 262013</p>
<p>Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang , Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, Tatsunori Hashimoto, arXiv:2501.19393Simple test-time scaling. 20251arXiv preprint</p>
<p>Sparks of science: Hypothesis generation using structured paper data. O' Charles, Tirthankar Neill, Roberta Ghosal, Mike Răileanu, Thang Walmsley, Kevin Bui, Ioana Schawinski, Ciucă, arXiv:2504.129762025arXiv preprint</p>
<p>Ml-dev-bench: Comparative analysis of ai agents on ml development workflows. Harshith Padigela, Chintan Shah, Dinkar Juyal, 2025</p>
<p>Position: Episodic memory is the missing piece for long-term llm agents. Mathis Pink, Qinyuan Wu, Ai Vy, Javier Vo, Jianing Turek, Alexander Mu, Mariya Huth, Toneva, arXiv:2502.069752025arXiv preprint</p>
<p>Ideasynth: Iterative research idea development through evolving and composing idea facets with literature-grounded feedback. Kevin Pu, Kevin Kj, Tovi Feng, Tom Grossman, Bhavana Hope, Matt Dalvi Mishra, Jonathan Latzke, Joseph Chee Bragg, Pao Chang, Siangliulue, Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems. the 2025 CHI Conference on Human Factors in Computing Systems2025a</p>
<p>Piflow: Principle-aware scientific discovery with multi-agent collaboration. Yingming Pu, Tao Lin, Hongyu Chen, 2025b</p>
<p>Scaling large-language-model-based multi-agent collaboration. Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, Maosong Sun, arXiv:2406.071552024arXiv preprint</p>
<p>Iterative hypothesis generation for scientific discovery with monte carlo nash equilibrium self-refining trees. Gollam Rabby, Diyana Muhammed, Prasenjit Mitra, Sören Auer, arXiv:2503.193092025arXiv preprint</p>
<p>Scideator: Human-llm scientific idea generation grounded in research-paper facet recombination. Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Tom Hope, Daniel S Weld, arXiv:2409.146342024arXiv preprint</p>
<p>A survey on model context protocol: Architecture, state-of-the-art, challenges and future directions. Partha Pratim, Ray , Authorea Preprints. 2025</p>
<p>Astroagents: A multi-agent ai for hypothesis generation from mass spectrometry data. Daniel Saeedi, Denise Buckner, Jose C Aponte, Amirali Aghazadeh, arXiv:2503.231702025arXiv preprint</p>
<p>Aishik Sanyal, Samuel Schapiro, Sumuk Shashidhar, Royce Moon, Lav R Varshney, Dilek Hakkani-Tur, arXiv:2504.20090Spark: A system for scientifically creative idea generation. 2025arXiv preprint</p>
<p>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum, arXiv:2501.04227arXiv:2504.17192Minju Seo, Jinheon Baek, Seongyun Lee, and Sung Ju Hwang. Paper2code: Automating code generation from scientific papers in machine learning. 2025. 2025arXiv preprintAgent laboratory: Using llm agents as research assistants</p>
<p>Shortcutsbench: A large-scale real-world benchmark for api-based agents. Haiyang Shen, Yue Li, Desong Meng, Dongqi Cai, Sheng Qi, Li Zhang, Mengwei Xu, Yun Ma, 2025</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.041092024arXiv preprint</p>
<p>Can llms generate novel research ideas? A large-scale human study with 100+ NLP researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, The Thirteenth International Conference on Learning Representations, ICLR 2025. SingaporeApril 24-28, 2025</p>
<p>Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark. Zachary S Siegel, Sayash Kapoor, Nitya Nagdir, Benedikt Stroebl, Arvind Narayanan, arXiv:2409.113632024arXiv preprint</p>
<p>Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, arXiv:2504.01848Evaluating ai's ability to replicate ai research. 2025arXiv preprint</p>
<p>A deep learning approach to antibiotic discovery. Jonathan M Stokes, Kevin Yang, Kyle Swanson, Wengong Jin, Andres Cubillos-Ruiz, Nina M Donghia, Shawn Craig R Macnair, Lindsey A French, Zohar Carfrae, Bloom-Ackermann, Cell. 18042020</p>
<p>Two heads are better than one: A multi-agent system has the potential to improve scientific idea generation. Haoyang Su, Renqi Chen, Shixiang Tang, Xinzhe Zheng, Jingzhe Li, Zhenfei Yin, Wanli Ouyang, Nanqing Dong, arXiv:2410.094032024arXiv preprint</p>
<p>Zerosearch: Incentivize the search capability of llms without searching. Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Fei Huang, Yan Zhang, arXiv:2505.045882025arXiv preprint</p>
<p>Ekin Dogus Cubuk, Amil Merchant, et al. An autonomous laboratory for the accelerated synthesis of novel materials. Nathan J Szymanski, Bernardus Rendy, Yuxing Fei, Rishi E Kumar, Tanjin He, David Milsted, Matthew J Mcdermott, Max Gallant, Nature. 62479902023</p>
<p>SciMON: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, 10.18653/v1/2024.acl-long.18Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 2024a1</p>
<p>Scipip: An llm-based scientific paper idea proposer. Wenxiao Wang, Lihui Gu, Liye Zhang, Yunxiang Luo, Yi Dai, Chen Shen, Liang Xie, Binbin Lin, Xiaofei He, Jieping Ye, arXiv:2410.231662024barXiv preprint</p>
<p>Autosurvey: Large language models can automatically write surveys. Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Qingsong Wen, Wei Ye, Advances in Neural Information Processing Systems. 2024c37</p>
<p>Jason Wei, Zhiqing Sun, Spencer Papay, Scott Mckinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, Amelia Glaese, arXiv:2504.12516Browsecomp: A simple yet challenging benchmark for browsing agents. 2025arXiv preprint</p>
<p>Large language models are better reasoners with self-verification. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, Jun Zhao, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Cycleresearcher: Improving automated research via automated review. Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, Linyi Yang, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Yuhao Wu, Yushi Bai, Zhiqing Hu, Shangqing Tu, Ming Shan Hee, Juanzi Li, Roy Ka-Wei Lee, arXiv:2503.04723Shifting long-context llms research from input to output. 2025aarXiv preprint</p>
<p>When more is less: Understanding chain-of-thought length in llms. Yuyang Wu, Yifei Wang, Tianqi Du, Stefanie Jegelka, Yisen Wang, arXiv:2502.072662025barXiv preprint</p>
<p>Yanzheng Xiang, Hanqi Yan, Shuyin Ouyang, Lin Gui, Yulan He, arXiv:2504.00255Scireplicate-bench: Benchmarking llms in agent-driven algorithmic reproduction from research papers. 2025arXiv preprint</p>
<p>Improving scientific hypothesis generation with knowledge grounded large language models. Guangzhi Xiong, Eric Xie, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang, arXiv:2411.023822024arXiv preprint</p>
<p>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, David Ha, arXiv:2504.080662025arXiv preprint</p>
<p>An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, arXiv:2505.09388Chenxu Lv, et al. Qwen3 technical report. 2025aarXiv preprint</p>
<p>A survey of ai agent protocols. Yingxuan Yang, Huacan Chai, Yuanyi Song, Siyuan Qi, Muning Wen, Ning Li, Junwei Liao, Haoyi Hu, Jianghao Lin, Gaowei Chang, arXiv:2504.167362025barXiv preprint</p>
<p>Shennongalpha: an ai-driven sharing and collaboration platform for intelligent curation, acquisition, and translation of natural medicinal material knowledge. Zijie Yang, Yongjing Yin, Chaojun Kong, Tiange Chi, Wufan Tao, Yue Zhang, Tian Xu, Cell Discovery. 111322025c</p>
<p>Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou, arXiv:2410.070762024arXiv preprint</p>
<p>MOOSE-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou, The Thirteenth International Conference on Learning Representations. 2025d</p>
<p>Haofei Yu, Zhaochen Hong, Zirui Cheng, Kunlun Zhu, Keyang Xuan, Jinwei Yao, arXiv:2412.17767Tao Feng, and Jiaxuan You. Researchtown: Simulator of human research community. 2024arXiv preprint</p>
<p>Dolphin: Closed-loop open-ended auto-research through thinking, practice, and feedback. Jiakang Yuan, Xiangchao Yan, Botian Shi, Tao Chen, Wanli Ouyang, Bo Zhang, Lei Bai, Yu Qiao, Bowen Zhou, arXiv:2501.039162025arXiv preprint</p>
<p>Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Wenyue Hua, Haolun Wu, Zhihan Guo, Yufei Wang, Niklas Muennighoff, arXiv:2503.24235A survey on test-time scaling in large language models: What, how, where, and how well?. 2025arXiv preprint</p>
<p>Chain of agents: Large language models collaborating on long-context tasks. Yusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister, Rui Zhang, Sercan Arik, Advances in Neural Information Processing Systems. 202437</p>
<p>Yuxiang Zheng, Shichao Sun, Lin Qiu, Dongyu Ru, Cheng Jiayang, Xuefeng Li, Jifan Lin, Binjie Wang, Yun Luo, Renjie Pan, arXiv:2408.06941Unleashing ai for accelerated scientific research. 2024arXiv preprint</p>
<p>Deepreview: Improving llm-based paper review with human-like deep thinking process. Minjun Zhu, Yixuan Weng, Linyi Yang, Yue Zhang, arXiv:2503.085692025arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
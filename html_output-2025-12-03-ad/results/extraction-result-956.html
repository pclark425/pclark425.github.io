<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-956 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-956</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-956</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-23.html">extraction-schema-23</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <p><strong>Paper ID:</strong> paper-ad5970584754cc7a1d91c95ab84a1e210258183a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ad5970584754cc7a1d91c95ab84a1e210258183a" target="_blank">UnifiedQA: Crossing Format Boundaries With a Single QA System</a></p>
                <p><strong>Paper Venue:</strong> Findings</p>
                <p><strong>Paper TL;DR:</strong> This work uses the latest advances in language modeling to build a single pre-trained QA model, UNIFIEDQA, that performs well across 19 QA datasets spanning 4 diverse formats, and results in a new state of the art on 10 factoid and commonsense question answering datasets.</p>
                <p><strong>Paper Abstract:</strong> Question answering (QA) tasks have been posed using a variety of formats, such as extractive span selection, multiple choice, etc. This has led to format-specialized models, and even to an implicit division in the QA community. We argue that such boundaries are artificial and perhaps unnecessary, given the reasoning abilities we seek to teach are not governed by the format. As evidence, we use the latest advances in language modeling to build a single pre-trained QA model, UNIFIEDQA, that performs well across 19 QA datasets spanning 4 diverse formats. UNIFIEDQA performs on par with 8 different models that were trained on individual datasets themselves. Even when faced with 12 unseen datasets of observed formats, UNIFIEDQA performs surprisingly well, showing strong generalization from its outof-format training data. Finally, simply finetuning this pre trained QA model into specialized models results in a new state of the art on 10 factoid and commonsense question answering datasets, establishing UNIFIEDQA as a strong starting point for building QA systems.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-956",
    "paper_id": "paper-ad5970584754cc7a1d91c95ab84a1e210258183a",
    "extraction_schema_id": "extraction-schema-23",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00457325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>UnifiedQA: Crossing Format Boundaries with a Single QA System</h1>
<p>Daniel Khashabi ${ }^{1}$ Sewon Min ${ }^{2}$ Tushar Khot ${ }^{1}$ Ashish Sabharwal ${ }^{1}$ Oyvind Tafjord ${ }^{1}$ Peter Clark ${ }^{1}$ Hannaneh Hajishirzi ${ }^{1,2}$<br>${ }^{1}$ Allen Institute for AI, Seattle, U.S.A.<br>${ }^{2}$ University of Washington, Seattle, U.S.A.</p>
<h4>Abstract</h4>
<p>Question answering (QA) tasks have been posed using a variety of formats, such as extractive span selection, multiple choice, etc. This has led to format-specialized models, and even to an implicit division in the QA community. We argue that such boundaries are artificial and perhaps unnecessary, given the reasoning abilities we seek to teach are not governed by the format. As evidence, we use the latest advances in language modeling to build a single pre-trained QA model, UNIFIEDQA, that performs well across 20 QA datasets spanning 4 diverse formats. UNIFIEDQA performs on par with 8 different models that were trained on individual datasets themselves. Even when faced with 12 unseen datasets of observed formats, UNIFIEDQA performs surprisingly well, showing strong generalization from its out-offormat training data. Finally, fine-tuning this pre-trained QA model into specialized models results in a new state of the art on 10 factoid and commonsense QA datasets, establishing UnIFIEDQA as a strong starting point for building QA systems. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Question answering is a common tool for assessing how well can computers understand language and reason with it. To this end, the NLP community has introduced several distinct datasets, with four popular QA formats illustrated in Fig. 1. For instance, some datasets expect the answer to be "yes" or "no", or a unique answer span in the associated paragraph (as opposed to multiple or no spans). These differences have motivated their study in silos, often encoding QA format into the model architecture itself. Efforts to exploit multiple datasets remain largely restricted to a single format. For example, Clark et al. (2019c) limit consideration to</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Four formats (color-coded throughout the paper) commonly used for posing questions and answering them: Extractive (EX), Abstractive (AB), Multiple-Choice (MC), and Yes/No (YN). Sample dataset names are shown in square brackets. We study generalization and transfer across these formats.
multiple-choice datasets, while Talmor and Berant (2019) focus their generalization study on extractive span prediction models. To the best of our knowledge, no single QA system targets, not to mention excels at, all of these formats.</p>
<p>This raises the question: Can QA models learn linguistic reasoning abilities that generalize across formats? Our intuition is simple: while question format and relevant knowledge may vary across QA datasets, the underlying linguistic understanding and reasoning abilities are largely common. A multiple-choice model may, therefore, benefit from training on an extractive answers dataset. Building upon this intuition, we present a single pre-trained QA system, named UNIFIEDQA, that exploits information across 4 different QA formats to achieve strong performance across 20 different factoid and</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Datasets</th>
<th style="text-align: center;">NQuAD11</th>
<th style="text-align: center;">NQuAD2</th>
<th style="text-align: center;">NsoutQA</th>
<th style="text-align: center;">Quoref</th>
<th style="text-align: center;">ROPES</th>
<th style="text-align: center;">NasQA</th>
<th style="text-align: center;">DROP</th>
<th style="text-align: center;">NasQA</th>
<th style="text-align: center;">RACE</th>
<th style="text-align: center;">MCTest</th>
<th style="text-align: center;">ORQA</th>
<th style="text-align: center;">ARC</th>
<th style="text-align: center;">QASC</th>
<th style="text-align: center;">CQA</th>
<th style="text-align: center;">WG</th>
<th style="text-align: center;">PIQA</th>
<th style="text-align: center;">SIQA</th>
<th style="text-align: center;">BoolQ</th>
<th style="text-align: center;">NF-BoolQ</th>
<th style="text-align: center;">MultiRC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Format</td>
<td style="text-align: center;">Extractive QA (EX)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Abstractive QA (AB)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Multiple-choice QA (MC)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">NasN@QA (YN)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Has paragraphs?</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Has explicit candidate use?</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"># of explicit candidates</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Fees contains use as substring?</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Has sth questions?</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Figure 2: Properties of various QA datasets included in this study: 5 extractive (EX), 3 abstractive (AB), 9 multiplechoice (MC), and 3 yes/no (YN). 'idk' denotes 'I don't know' or unanswerable questions. BoolQ represents both the original dataset and its contrast-sets extension BoolQ-CS; similarly for ROPES, Quoref, and DROP.
commonsense QA datasets listed in Fig. 2.
In this work, we advocate for a unifying view of QA formats by building a format-agnostic QA system. Our work leverages recent progress in text-to-text pre-trained neural models, specifically T5 (Raffel et al., 2020) and BART (Lewis et al., 2020), but with a strong focus on differing QA formats. This paradigm allows unifying many NLP models, which formerly had task-specific designs, into a single text-to-text framework. Previous work uses textual prefixes to explicitly define the task associated with each input instance (Raffel et al., 2020; Radford et al., 2019b); often such attempts to build a single model for multiple NLP tasks underperform the standard pre-training plus finetuning setup (a model per task) (Raffel et al., 2020).</p>
<p>Our work narrows down the scope to tasks that stay within the boundaries of QA, demonstrating that a unified text-to-text paradigm can, in fact, be successful across different QA tasks and formats. We develop a single pre-trained QA model by training text-to-text models on a set of seed QA datasets of multiple formats, taking natural text as input, without using format-specific prefixes. Our experiments show that UNIFIEDQA can be applied as-is to different QA tasks, generalizes well to other unseen datasets (zero-shot), and with further finetuning achieves state-of-the-art results on many QA tasks including commonsense and factual datasets.</p>
<p>Contributions. This work advocates for a unified view of different QA formats, and for building format-agnostic QA systems. To support this view, we present UnIFIEDQA, a single pre-trained QA system that works well on and generalizes to datasets with different formats (§6.2), while performing on par with state-of-the-art dedicated systems tailored to each dataset (§6.1). Additionally, fine-tuning UnIFIEDQA into specialized systems sets a new state of the art for 10 datasets (§6.3), establishing it as a powerful starting point for QA research. Our findings demonstrate that crossing QA format boundaries is not only qualitatively de-
sirable but also quantitatively beneficial.</p>
<h2>2 Related Work</h2>
<p>Several QA efforts have studied generalization across datasets of a single format. For instance, in MultiQA, Talmor and Berant (2019) study generalization and transfer, but only across extractive span selection datasets. Further, while they show strong leave-one-out style results, they find a single system performs substantially worse than one tuned to each dataset. In ORB, Dua et al. (2019a) propose a multi-dataset evaluation benchmark spanning extractive and abstractive formats. However, that study is limited to an evaluation of systems, falling short of addressing how to build such generalized models. The MRQA shared task (Fisch et al., 2019) focuses on span-prediction datasets. Unlike all these efforts, our goal is to investigate transfer and generalization across different QA formats, as well as to build a single system that does this well.</p>
<p>Exploiting commonality across machine learning tasks has a rich history studied under transfer learning (Caruana, 1997; Clark et al., 2019b). McCann et al. (2018) and Keskar et al. (2019) study transfer among various NLP tasks by casting them into a single QA format-an elegant transfer learning approach but orthogonal to the goal of this work. As noted earlier, Raffel et al. (2020) investigate the transfer between several diverse NLP tasks (machine translation, summarization, etc). Their key contribution is a text-to-text framework, and a powerful model called T5, that makes it easier to mix multiple tasks by encoding both inputs and outputs as text. They rely on textual prefixes to explicitly define the task corresponding to each input instance. While we build upon their framework, we narrow our focus to variations of QA. This allows us to achieve strong results while avoiding reliance on any format-specific prefixes. Our models learn to infer the format of each input question based on its content (e.g., whether the phrasing of the question demands a yes/no answer). Moreover, we are able to demonstrate generalization across QA tasks,</p>
<p>which prior work failed to achieve presumably due to its focus on too broad a set of NLP tasks.</p>
<h2>3 UnifiedQA: Multi-format Training</h2>
<p>Suppose we would like to train a unified QA model that can operate over $k$ formats $F_{1}, F_{2}, \ldots, F_{k}$. For each format $F_{i}$, suppose we have $\ell_{i}$ datasets sets $D_{1}^{i}, D_{2}^{i}, \ldots, D_{\ell_{i}}^{i}$ where $D_{j}^{i}=\left(T_{j}^{i}, E_{j}^{i}\right)$ includes both training and evaluation examples. In some cases, the training set $T_{j}^{i}$ may be empty or we may want to ignore it in order to treat $D_{j}^{i}$ as an 'unseen', evaluation-only dataset and assess a model's generalization to it.</p>
<p>We use the text-to-text paradigm to convert each training question $q$ in format $F_{i}$ into a plain-text input representation $e n c_{i}(q)$. This conversion uses a natural encoding process that will be described shortly (§3.1) for four common QA formats, and is easily extensible to other formats as well. We follow a simple approach of creating a mixed training pool consisting of all available training instances:</p>
<p>$$
\tilde{T}=\bigcup_{i=1}^{k} \bigcup_{j=1}^{\ell_{i}}\left{\operatorname{enc}<em j="j">{i}(q) \mid q \in T</em>\right}
$$}^{i</p>
<p>Training batches are drawn from this pooled data, $\tilde{T}$, by including each $q \in T_{j}^{i}$ with a probability proportional $1 /\left|T_{j}^{i}\right|$. Each batch thus, on average, contains the same number of instances from each training set, regardless of its size. Similar treatments of task mixing have also been adopted by Arivazhagan et al. (2019) and Raffel et al. (2020). As our experiments will show, our multi-format mixing approach works well. It clearly highlights the value of training on out-of-format data and confirms our intuition that there are strong ties across QA formats in terms of the underlying reasoning abilities. ${ }^{2}$</p>
<p>Our unified question-answering system is based on the recent text-to-text frameworks, particularly, T5 (Raffel et al., 2020) and BART (Lewis et al., 2020). We first define a unifying encoding of the instances across various formats (§3.1). We then introduce UNIFIEDQA (§3.2) that is a QA system trained on datasets in multiple formats, indicating new state-of-the-art results on 10 datasets and generalization to unseen datasets.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>3.1 Text-to-Text Encoding</h3>
<p>We convert each of our target datasets into a textin/text-out format (Raffel et al., 2020; Lewis et al., 2020; Radford et al., 2019b). The question always comes first, followed by some additional information (context paragraph or candidate answers, or both). We use " $\backslash n$ " separators between different parts of the input. This ensures having a humanlike encoding while not making it overly-specific to a certain format.</p>
<p>Our unified model incorporates the following four common question-answering formats. Specific datasets within them are deferred to Section 4.1.
Extractive (EX) questions $Q$ include a context paragraph $C$ (typically a paragraph) and require models to extract the answer as a substring from the context. In some datasets, 'unanswerable' can sometimes be the correct response.</p>
<p>Abstractive (AB) questions $Q$ require models to produce answers that are often not mere substrings of the provided context paragraph $C$.</p>
<p>Multiple-choice (MC) questions $Q$ come with a set of candidate answers $\left{A_{i}\right}$, of which generally exactly one is correct. In some cases, they also include a context paragraph $C$.</p>
<p>Yes/No (YN) questions $Q$ expect a 'yes' or 'no' answer as the response and may include a context paragraph $C$.</p>
<p>Table 1 provides examples of the natural input and output encoding for each of these formats, where both input and output representations are raw text. There is no explicit information regarding a question being an MC question or having exactly four candidate answers. Specifically, MC questions without any context paragraph are encoded as question \n (A) c1 (B) c2 ... where $\mathrm{c} 1, \mathrm{c} 1, \ldots$ are the set of candidate answers (see the example from ARC dataset). If the question includes a context paragraph, it is appended after the candidate answers: question \n (A) c1 (B) c2 ... \n paragraph, as shown in the example from the MCTest dataset. Questions in the other three formats (EX, AB, and YN) are encoded simply as question \n paragraph.</p>
<p>To re-emphasize, unlike prior work (Raffel et al., 2020), we do not specify any task-, dataset-, or format-specific prefixes in the input representation. Whether the answer should be extracted or abstracted, and whether from the provided context paragraph or candidate answers (or the fact that</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Table 1: Example text-to-text encoding of instances.
these even are candidate answers) is expected to be inferred by the system.</p>
<h3>3.2 UnIFIEDQA: The Pre-Trained Model</h3>
<p>The specific pre-trained QA model we provide and use in all our experiments is trained on representative datasets for each of the 4 formats discussed earlier. We empirically chose the following 8 seed datasets for training UnIFIEDQA, ${ }^{3}$ based on their effectiveness in our pilot study (details deferred to Section 5) assessing which datasets are most valuable for out-of-format training:</p>
<ul>
<li>EX: SQuAD 1.1, SQuAD 2.0</li>
<li>AB: NarrativeQA</li>
<li>MC: RACE, ARC, OBQA, MCTest</li>
<li>YN: BoolQ</li>
</ul>
<p>One can easily use other combinations of formats and datsets to create variants of our UnIFIEDQA model, or extend it as future datasets become available or new formats are introduced.</p>
<p>Unless otherwise noted, we use the largest available T5 model (11B parameters) as the starting point for training our model and call the system UnIFIEDQA. We also report results of training our system with $\mathrm{BART}<em _BART="{BART" _text="\text">{\text {large }}$, referred to as Uni$\mathrm{FIEDQA}</em>$ (see §6.3). Details on the parameters of the models used are deferred to Appendix A.2.}</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Similar to pre-trained language models, the resulting pre-trained QA model can be used as a starting point for fine-tuning on other QA datasets.</p>
<h2>4 Formats and Datasets</h2>
<h3>4.1 Datasets</h3>
<p>We evaluate UnIFIEDQA on 20 existing datasets that target different formats as well as various complex linguistic phenomena. Fig. 2 summarizes key properties of our datasets (whether it comes with a paragraph or answer candidates, whether the paragraph explicitly contains the answer, etc). Most importantly, they are grouped into several formats/categories as described below. Table 2 gives certain statistics of these datasets. We next provide a summary enumerating these datasets, with additional details deferred to Appendix A.1.</p>
<p>Extractive QA (EX). Among the datasets in this popular format, we adopt SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2 (Rajpurkar et al., 2018), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019).</p>
<p>Abstractive QA (AB). The datasets used from this format are: NarrativeQA/NarQA (Kociský et al., 2018), the open-domain version of NaturalQuestions/NatQA (Kwiatkowski et al., 2019), and DROP (Dua et al., 2019b).</p>
<p>Multiple-choice QA (MC). We use the following MC datasets: MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), OpenBookQA/OBQA (Mihaylov et al., 2018), ARC (Clark et al., 2018, 2016), QASC (Khot et al., 2019), CommonsenseQA/CQA (Talmor et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), and Winogrande (Sakaguchi et al., 2020). Several of the MC datasets do not come with accompanying paragraphs (such as ARC, QASC, OBQA). For most of this the work, we keep the questions as is with no additional retrieval (unless otherwise mentioned). One other variability among these datasets is their number of candidate answers. While many datasets have four candidates (see Fig. 2), others have more. Later (in §6.2) we will see that our approach generalizes to datasets with different numbers of candidates, even if such questions have not been seen during training.</p>
<p>Yes/No QA (YN). The YN datasets we use are BoolQ (Clark et al., 2019a) and a</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Train <br> set size</th>
<th style="text-align: center;">Eval. <br> set size</th>
<th style="text-align: center;">Best <br> published</th>
<th style="text-align: center;">$95 \%$ <br> CI (\%)</th>
<th style="text-align: center;">Input <br> length</th>
<th style="text-align: center;">Output <br> length</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SQuAD 1.1</td>
<td style="text-align: center;">87b</td>
<td style="text-align: center;">106</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">136.2</td>
<td style="text-align: center;">3.0</td>
</tr>
<tr>
<td style="text-align: left;">SQuAD 2.0</td>
<td style="text-align: center;">1306</td>
<td style="text-align: center;">116</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">139.9</td>
<td style="text-align: center;">2.6</td>
</tr>
<tr>
<td style="text-align: left;">NewsQA</td>
<td style="text-align: center;">766</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">606.6</td>
<td style="text-align: center;">4.0</td>
</tr>
<tr>
<td style="text-align: left;">Quoref</td>
<td style="text-align: center;">22b</td>
<td style="text-align: center;">2b</td>
<td style="text-align: center;">86.1</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">352.7</td>
<td style="text-align: center;">1.7</td>
</tr>
<tr>
<td style="text-align: left;">Quoref-CS</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">700</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">324.1</td>
<td style="text-align: center;">2.2</td>
</tr>
<tr>
<td style="text-align: left;">ROPES</td>
<td style="text-align: center;">106</td>
<td style="text-align: center;">1.46</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">169.1</td>
<td style="text-align: center;">1.4</td>
</tr>
<tr>
<td style="text-align: left;">ROPES-CS</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">974</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">182.7</td>
<td style="text-align: center;">1.3</td>
</tr>
<tr>
<td style="text-align: left;">NatQA</td>
<td style="text-align: center;">65b</td>
<td style="text-align: center;">216</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">563.6</td>
<td style="text-align: center;">6.2</td>
</tr>
<tr>
<td style="text-align: left;">NatQA</td>
<td style="text-align: center;">796</td>
<td style="text-align: center;">3.66</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">607.0</td>
<td style="text-align: center;">2.2</td>
</tr>
<tr>
<td style="text-align: left;">DROP</td>
<td style="text-align: center;">77b</td>
<td style="text-align: center;">9b</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">189.1</td>
<td style="text-align: center;">1.6</td>
</tr>
<tr>
<td style="text-align: left;">DROP-CS</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">947</td>
<td style="text-align: center;">54.2</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">206.0</td>
<td style="text-align: center;">2.1</td>
</tr>
<tr>
<td style="text-align: left;">RACE</td>
<td style="text-align: center;">87b</td>
<td style="text-align: center;">4b</td>
<td style="text-align: center;">89.5</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">317.9</td>
<td style="text-align: center;">6.9</td>
</tr>
<tr>
<td style="text-align: left;">OBQA</td>
<td style="text-align: center;">4b</td>
<td style="text-align: center;">501</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">3.6</td>
</tr>
<tr>
<td style="text-align: left;">MCTest</td>
<td style="text-align: center;">1.46</td>
<td style="text-align: center;">320</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">3.4</td>
<td style="text-align: center;">245.4</td>
<td style="text-align: center;">4.0</td>
</tr>
<tr>
<td style="text-align: left;">ARC (easy)</td>
<td style="text-align: center;">2b</td>
<td style="text-align: center;">2b</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">3.7</td>
</tr>
<tr>
<td style="text-align: left;">ARC (chall.)</td>
<td style="text-align: center;">1b</td>
<td style="text-align: center;">1b</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">47.4</td>
<td style="text-align: center;">5.0</td>
</tr>
<tr>
<td style="text-align: left;">CQA</td>
<td style="text-align: center;">9.7b</td>
<td style="text-align: center;">1.2b</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">1.5</td>
</tr>
<tr>
<td style="text-align: left;">WG</td>
<td style="text-align: center;">40.3b</td>
<td style="text-align: center;">1.7b</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">3.0</td>
</tr>
<tr>
<td style="text-align: left;">PIQA</td>
<td style="text-align: center;">16.1b</td>
<td style="text-align: center;">3b</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">20.2</td>
</tr>
<tr>
<td style="text-align: left;">SIQA</td>
<td style="text-align: center;">33.4b</td>
<td style="text-align: center;">2.2b</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">4.7</td>
</tr>
<tr>
<td style="text-align: left;">BoolQ</td>
<td style="text-align: center;">9b</td>
<td style="text-align: center;">3b</td>
<td style="text-align: center;">91.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">105.1</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">BoolQ-CS</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">461</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">108.9</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">NP-BoolQ</td>
<td style="text-align: center;">106</td>
<td style="text-align: center;">3b</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">106.2</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">MultiRC</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">312</td>
<td style="text-align: center;">91.7</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">293.3</td>
<td style="text-align: center;">1.0</td>
</tr>
</tbody>
</table>
<p>Table 2: Dataset Statistics. CQA, OBQA, WG, and NarQA refer to CommonsenseQA, OpenBookQA, Winogrande, and NarrativeQA, respectively. The CI column shows the upper $95 \%$ confidence interval for the evaluation set as a percentage, based on the Wilson test around the mean score listed as a percentage in the best known performance column. Input and output representation lengths are measured in the number of tokens and averaged across the dataset.
naturally-perturbed version of this dataset, BoolQNP (Khashabi et al., 2020), and the binary (yes/no) subset of MultiRC (Khashabi et al., 2018).</p>
<p>Contrast-sets. Additionally, we use contrastsets (Gardner et al., 2020) for several of our datasets (denoted with "CS"): BoolQ-CS, ROPESCS, Quoref-CS, DROP-CS. These evaluation sets are expert-generated perturbations that deviate from the patterns common in the original dataset.</p>
<h3>4.2 Evaluation Metrics for Textual Output</h3>
<p>We evaluate each dataset using the metric used most often for it in prior work. For the EX format, it's the F1 score of the extracted span relative to the gold label. For the AB format, we use ROUGE-L metric (Lin et al., 2006; Min et al., 2019; Nishida et al., 2019). For NatQA we use the exact-match metric, following Min et al. (2020). For the MC format, we match the generated text with the closest answer candidate based token overlap and compute the accuracy. For the YN format, we follow Clark et al. (2019a) to measure if the generated output matches the correct 'yes' or 'no' label. In rare cases where the output is longer than one word (e.g., 'yes it is'), we check if it contains the correct label but
not the incorrect one. ${ }^{4}$</p>
<h2>5 Pilot Study: Can Out-of-Format Training Help?</h2>
<p>We first answer the question: Is the broad idea of benefiting from out-of-format training even viable? For instance, is our intuition correct that an MC dataset can, in practice, benefit from training on an EX dataset? Before discussing our main experimental results, we briefly report on a pilot study that assesses the following basic question: Given a training set $T_{1}^{i}$ (the anchor dataset) of QA format $F_{i}$, is there an out-of-format training set $T_{1}^{j}$ of format $F_{j}$ such that training jointly on $T_{1}^{i} \cup T_{1}^{j}$ improves performance relative to training only on $T_{1}^{i}$ ? To this end, we evaluate both on the matching evaluation set $E_{1}^{i}$ as well as on 'unseen' data $E_{2}^{i}, E_{3}^{i}, \ldots$ of the same format.</p>
<p>The results are summarized in Table 3. The two rows in each individual table correspond to training on $T_{1}^{i}$ (the anchor dataset) and on $T_{1}^{i} \cup X$, where $X$ is an out-of-format dataset corresponding to $T_{1}^{j}$ above. The columns represent various evaluation sets of format $F_{i}$. For each column, ' $X=\ldots$ ' at the very bottom indicates the out-of-format dataset $X$ that was the most helpful in improving performance on the evaluation set in that column. ${ }^{5}$</p>
<p>Consider the case of the anchor set $T_{1}^{i}$ being BoolQ and the evaluation set being NP-BoolQ, both of format YN. Here, including out-of-format training data $X=$ SQuAD2 boosts performance from $51 \%$ to as much as $59 \%$. The gain may be less in other cases, but across all anchor and evaluation datasets, we generally observe that there is at least one out-of-format training set whose inclusion improves performance.</p>
<p>This pilot study thus provides a proof of concept that out-of-format training can indeed help a QA model in nearly every case. Of course, this study only shows the existence of such an out-of-format dataset, rather than provide a single unified model. Nevertheless, it helps identify representative training sets from each format that were most helpful. As alluded to earlier, we used this empirical data to guide which training sets to include when building UNIFIEDQA in Section 3.2.</p>
<p>The experimental results from this case study are summarized in the aggregated plot shown in</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Trained on $\downarrow$ - Evaluated on $\rightarrow$</th>
<th style="text-align: center;">SQuAD11</th>
<th style="text-align: center;">SQuAD2</th>
<th style="text-align: center;">NewsQA</th>
<th style="text-align: center;">Quorof</th>
<th style="text-align: center;">Quorof-CS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SQuAD11</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">28.2</td>
<td style="text-align: center;">28.11</td>
</tr>
<tr>
<td style="text-align: center;">SQuAD11 + X</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">29.84</td>
</tr>
<tr>
<td style="text-align: center;">Best X</td>
<td style="text-align: center;">BoolQ</td>
<td style="text-align: center;">OBQA</td>
<td style="text-align: center;">OBQA</td>
<td style="text-align: center;">NarQA</td>
<td style="text-align: center;">OBQA</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Trained on $\downarrow$ - Evaluated on $\rightarrow$</th>
<th style="text-align: center;">BoolQ</th>
<th style="text-align: center;">MultiRC</th>
<th style="text-align: center;">NP-BoolQ</th>
<th style="text-align: center;">BoolQ-CS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BoolQ</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">53.4</td>
</tr>
<tr>
<td style="text-align: center;">BoolQ + X</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">61.0</td>
</tr>
<tr>
<td style="text-align: center;">Best X</td>
<td style="text-align: center;">SQuAD2</td>
<td style="text-align: center;">OBQA</td>
<td style="text-align: center;">SQuAD2</td>
<td style="text-align: center;">NarQA</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Trained on $\downarrow$ - Evaluated on $\rightarrow$</th>
<th style="text-align: center;">RACE</th>
<th style="text-align: center;">OBQA</th>
<th style="text-align: center;">ARC-chal</th>
<th style="text-align: center;">MCTest</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">RACE</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">26.6</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">62.5</td>
</tr>
<tr>
<td style="text-align: center;">RACE + X</td>
<td style="text-align: center;">59.1</td>
<td style="text-align: center;">32.2</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">69.4</td>
</tr>
<tr>
<td style="text-align: center;">Best X</td>
<td style="text-align: center;">SQuAD11</td>
<td style="text-align: center;">NarQA</td>
<td style="text-align: center;">NewsQA</td>
<td style="text-align: center;">SQuAD11</td>
</tr>
</tbody>
</table>
<p>Table 3: Pilot study showing that out-of-format training can help improve performance. Each table compares training on just the anchor dataset (e.g., BoolQ in the top-left table) with training also on an out-of-format dataset denoted ' $X$ '. Evaluation is on the anchor dataset as well as unseen datasets of that format. The last row identifies the out-of-format dataset that helped most on each evaluation dataset. All results are based on the "small" size T5 model. Color denotes QA format (see Table 2).
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Bipartite graph showing the value of various datasets. The datasets on the left were used for training and on the right for evaluation. The wider the edge from a dataset $\ell$ (on the left) to a dataset $r$ (on the right), the higher the contribution of adding the out-of-format dataset $\ell$ to the training set of questions in $r$ 's format.</p>
<p>Fig. 3. In this bipartite graph, the datasets used for training are on the left hand side and the evaluation datasets are on the right hand side. The weight of each edge $w(\ell, r)$ indicates the contribution of a dataset $\ell$ when used for training jointly with an anchor dataset $d$, when evaluated on $r$ ( $d$ and $r$ have the same format.) Specifically,</p>
<p>$$
w(\ell, r)=a v g_{d}[S(\ell \cup d ; r)-S(d ; r)]
$$</p>
<p>where $S(d, r)$ is the score achieved on $r$ after training on $d$. Since we only focus on gains from out-offormat training, we drop the edges that are negative or between two datasets of the same format.</p>
<p>As expected, there are strong connections between the AB and EX datasets in Fig. 3 since their definitions are quite similar. Apart from the
edge weight, the overall width of a dataset $\ell$ on the left also depicts how much it contributes to out-of-format datasets. E.g., NQA (NarrativeQA) is the most helpful dataset and even helps multiple formats. Similarly our extractive datasets (SQuAD11.1, SQuAD 2, and NewsQA) are also relatively more helpful. While large datasets generally appear to help, RACE, another large-scale dataset, doesn't help that much. The least helpful dataset in the mix is BoolQ which focuses on yes/no questions.</p>
<p>In a similar vein, the wider the dataset on the right hand side, the more it can be benefit from out-of-format datasets. Among these beneficiary datasets, all four formats are equally represented.</p>
<h2>6 Experimental Results</h2>
<p>We now discuss our main experimental results, evaluating UnIFIEDQA on seed datasets (used for training the system) as well as unseen datasets.</p>
<h3>6.1 UnIFIEDQA vs. 8 Dedicated Models</h3>
<p>Is UnIFIEDQA, a single pre-trained multi-format QA system, as good as dedicated systems trained for individual datasets? We emphasize that the answer to this question is not as simple as it may seem, since earlier works have observed that a system addressing multiple tasks often underperforms a focused system (Raffel et al., 2020).</p>
<p>Fig. 4 summarizes the results of the relevant experiment. The gray bars belong to UnIFIEDQA (a single system for multiple datasets of different formats). The colored bars are different T5-based systems tailored to individual datasets (a different system for each dataset). The results show that UnIFIEDQA performs almost as good as individual T5 models targeted to each dataset. In some cases UNIFIEDQA performs even better than the</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Seen dataset?</th>
<th style="text-align: center;">Model $\downarrow$ - Evaluated on $\rightarrow$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">NewsQA</th>
<th style="text-align: center;">Quoref</th>
<th style="text-align: center;">Quoref-CS</th>
<th style="text-align: center;">ROPES</th>
<th style="text-align: center;">ROPES-CS</th>
<th style="text-align: center;">DROP</th>
<th style="text-align: center;">DROP-CS</th>
<th style="text-align: center;">QASC</th>
<th style="text-align: center;">Common senseQA</th>
<th style="text-align: center;">NP-BioolQ</th>
<th style="text-align: center;">BioolQ-CS</th>
<th style="text-align: center;">MultiRC</th>
<th style="text-align: center;">Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">UnifiedQA [EX]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">38.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">UnifiedQA [AB]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">36.8</td>
<td style="text-align: center;">54.1</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">39.9</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">45.8</td>
</tr>
<tr>
<td style="text-align: center;">No</td>
<td style="text-align: center;">UnifiedQA [MC]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">42.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">UnifiedQA [YN]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">91.7</td>
<td style="text-align: center;">24.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">UnifiedQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">59.9</td>
<td style="text-align: center;">68.7</td>
</tr>
<tr>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Previous best</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">86.1</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">54.2</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Retro Reader</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TASE</td>
<td style="text-align: center;">XLNet</td>
<td style="text-align: center;">ROBERTa</td>
<td style="text-align: center;">RobERTa</td>
<td style="text-align: center;">ALBERT</td>
<td style="text-align: center;">MTM5N</td>
<td style="text-align: center;">KF-SIR-25mp</td>
<td style="text-align: center;">mSIR-RoBERT</td>
<td style="text-align: center;">RobERTa</td>
<td style="text-align: center;">RobERTa</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 4: Generalization to unseen datasets: Multi-format training (UNIFIEDQA) often outperforms models trained the same way but solely on other in-format datasets (e.g., UNIFIEDQA [EX], which is trained on all extractive training sets of UnIFIEDQA. When averaged across all evaluation datasets (last column), UnIFIEDQA shows strong generalization performance across all formats. Notably, the "Previous best" models (last row) were trained on the target dataset's training data, but are even then outperformed by UnifiedQA (which has never seen these datasets during training) on the YN tasks.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: UnIFIEDQA is on-par with, and often outperforms, 9 different equally-sized T5-based systems tailored to individual datasets. The figure contains separate models for each of the two subsets of the ARC and Regents datasets.
single-dataset experts (e.g., on OBQA or NQA). On average (last column) UnIFIEDQA clearly outperforms the ensemble of dataset/format-specific systems. UnIFIEDQA thus offers flexibility across multiple QA formats while compromising almost nothing compared to dataset-specific experts.</p>
<h3>6.2 Generalization to Unseen Datasets</h3>
<p>We now explore whether UnIFIEDQA generalizes well to other, unseen datasets. Table 4 summarizes the results of experiments where we evaluate various models on datasets that are not used to train them. It compares UnIFIEDQA (training on multiple formats) with training on various datasets of a single format (e.g., UnIFIEDQA [EX], built by training the model on only extractive datasets).</p>
<p>The first few rows of the table show T5 models trained for individual formats, followed by UniFIEDQA. For completeness, we include the highest previous scores for each dataset; one must be careful when reading these numbers as the best previous numbers follow the fully supervised protocol (for NewsQA (Zhang et al., 2020),</p>
<p>Quoref (Segal et al., 2019), DROP (Lan et al., 2019), ROPES (Lin et al., 2019), QASC (Khot et al., 2019), CommonsenseQA (Zhu et al., 2020) and x-CS datasets (Gardner et al., 2020).)</p>
<p>We make three key observations: (1) On average (last column), UnIFIEDQA shows much stronger generalization across a wide range of datasets. (2) on 9 (out of 12) datasets, UnIFIEDQA shows a better generalization than any single-format expert. For example, while the system is trained on multiple-choice questions with 4 candidate answers, it works quite well on datasets with more than 4 candidate answers (QASC and CommonsenseQA have has 8 and 5 candidate answers per question, respectively). (3) Single-format experts are better at generalization only when the source and target datasets are very similar (for instance SQuAD and Quoref).</p>
<h3>6.3 State-of-the-Art via Simple Fine-tuning</h3>
<p>Fine-tuning of pre-trained language models has become the standard paradigm for building datasetspecific stat-of-the-art systems (Devlin et al., 2019; Liu et al., 2019). The question we address here is: when it comes to QA, is there a value in using UnIFIEDQA as a starting point for fine-tuning, as opposed to a vanilla language model that has not seen other QA datasets before?</p>
<p>To address this question, we fine-tune each of UnIFIEDQA, T5, and BART on several datasets by selecting the best check point on the dev set, and evaluating on the test set. Table 5 summarizes the results of the experiments. The table shows two variants: UnIFIEDQA $<em _BART="{BART" _text="\text">{\text {T5 }}$ and UnIFIEDQA $</em>$. All results are based on the 11B version of T5.}</p>
<p>The columns indicate the evaluation on the test set corresponding to the data that was used for training. For each dataset, the first line of the table</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model $\downarrow$ - Eval. $\rightarrow$</th>
<th style="text-align: center;">OBQA *</th>
<th style="text-align: center;">$\begin{gathered} \text { OBQA } \ (\mathrm{w} / \mathrm{IR}) \end{gathered}$</th>
<th style="text-align: center;">ARC-easy *</th>
<th style="text-align: center;">ARC-easy <br> (w/ IR)</th>
<th style="text-align: center;">ARC-chal *</th>
<th style="text-align: center;">ARC-chal <br> (w/ IR)</th>
<th style="text-align: center;">QASC</th>
<th style="text-align: center;">$\begin{gathered} \text { QASC } \ (\mathrm{w} / \mathrm{IR}) \end{gathered}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Previous best published</td>
<td style="text-align: center;">RoBERTa (Clark et al.,2019c)</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { KF+SIR } \ &amp; \text { (Mitra et al., 2020) } \end{aligned}$</td>
<td style="text-align: center;">RoBERTa (Clark et al.,2019c)</td>
<td style="text-align: center;">$\begin{gathered} \text { FreeLB- } \ \text { RoBERTa (Zhu et } \ \text { al., 2020) } \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} \text { RoBERTa } \ \text { (Clark et al.,2019c) } \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} \text { FreeLB- } \text { RoBERTa } \ \text { (Zhu et al., 2020) } \end{gathered}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\begin{gathered} \text { KF+SIR } \ \text { +Zhuy (Mitra } \ \text { et al., 2020) } \end{gathered}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">55.9</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">85.2</td>
</tr>
<tr>
<td style="text-align: center;">BART $_{\text {Nup }}$ - FT</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">75.3</td>
</tr>
<tr>
<td style="text-align: center;">UnifiedQAa ${ }_{\text {ART }}$ - FT</td>
<td style="text-align: center;">63.8</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">78.2</td>
</tr>
<tr>
<td style="text-align: center;">T5 - FT</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">88.5</td>
</tr>
<tr>
<td style="text-align: center;">UnifiedQA - FT</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">87.2</td>
<td style="text-align: center;">86.4</td>
<td style="text-align: center;">92.0</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">89.6</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Model $\downarrow$ - Eval. $\rightarrow$</th>
<th style="text-align: center;">RACE *</th>
<th style="text-align: center;">ComQA</th>
<th style="text-align: center;">WG</th>
<th style="text-align: center;">PIQA</th>
<th style="text-align: center;">SIQA</th>
<th style="text-align: center;">ROPES</th>
<th style="text-align: center;">NatQ (w/ IR)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Previous best published</td>
<td style="text-align: center;">ALBERT <br> (Lan et al.,2019)</td>
<td style="text-align: center;">$\begin{gathered} \text { FreeLB- } \text { RoBERTa } \ \text { (Zhu et al.,2020) } \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} \text { RoBERTa } \ \text { (Sakaguchi et al.,2019) } \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} \text { RoBERTa } \ \text { (Bisk et al., 2019) } \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} \text { RoBERTa } \ \text { (Mitra et al., 2020) } \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} \text { RoBERTa } \ \text { (Lin et al., 2019) } \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} \text { DPR+BART } \ \text { (Min et al.,2020) } \end{gathered}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">89.5</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">42.2</td>
</tr>
<tr>
<td style="text-align: center;">BART $_{\text {Nup }}$ - FT</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">62.4</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">42.1</td>
</tr>
<tr>
<td style="text-align: center;">UnifiedQAa ${ }_{\text {ART }}$ - FT</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">44.5</td>
</tr>
<tr>
<td style="text-align: center;">T5 - FT</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">49.3</td>
</tr>
<tr>
<td style="text-align: center;">UnifiedQA - FT</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">89.5</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">49.3</td>
</tr>
</tbody>
</table>
<p>Table 5: Fine-tuning UNIFIEDQA (last row) results in new state-of-the-art performance on 11 datasets. Further, it consistently improves upon fine-tuned T5 (2nd last row) by a margin ranging from $1 \%$ for CommonsenseQA (CQA) to as much as $13 \%$ for ARC-challenge. '(w/ IR)' denotes relevant information is retrieved and appended as context sentences in the input encoding. Datasets marked with * are used in UnIFIEDQA's original training.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model $\downarrow$ - Evaluated on $\rightarrow$</th>
<th style="text-align: center;">SQuAD11</th>
<th style="text-align: center;">SQuAD2</th>
<th style="text-align: center;">NarQA</th>
<th style="text-align: center;">RACE</th>
<th style="text-align: center;">OBQA</th>
<th style="text-align: center;">ARC-easy</th>
<th style="text-align: center;">ARC-hard</th>
<th style="text-align: center;">MCTest</th>
<th style="text-align: center;">BoolQ</th>
<th style="text-align: center;">Avg</th>
<th style="text-align: center;">$\Delta$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">UnifiedQA</td>
<td style="text-align: center;">93.4</td>
<td style="text-align: center;">89.6</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">excluding BoolQ</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">90.1</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">87.7</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">86.1</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">8.3</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">$-8.4$</td>
</tr>
<tr>
<td style="text-align: center;">excluding SQuAD 2</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">47.3</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">87.7</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">$-4.2$</td>
</tr>
<tr>
<td style="text-align: center;">excluding OBQA</td>
<td style="text-align: center;">93.6</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">90.1</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">$-1.3$</td>
</tr>
<tr>
<td style="text-align: center;">excluding NarQA</td>
<td style="text-align: center;">93.6</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">87.7</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">89.9</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">$-1.2$</td>
</tr>
<tr>
<td style="text-align: center;">excluding RACE</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">95.9</td>
<td style="text-align: center;">90.1</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">$-1.2$</td>
</tr>
<tr>
<td style="text-align: center;">excluding ARC-easy</td>
<td style="text-align: center;">93.4</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">89.9</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">$-0.6$</td>
</tr>
<tr>
<td style="text-align: center;">excluding ARC-hard</td>
<td style="text-align: center;">93.6</td>
<td style="text-align: center;">90.1</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">$-0.4$</td>
</tr>
<tr>
<td style="text-align: center;">excluding MCTest</td>
<td style="text-align: center;">92.8</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">84.6</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">$-0.2$</td>
</tr>
<tr>
<td style="text-align: center;">excluding SQuAD 1.1</td>
<td style="text-align: center;">92.6</td>
<td style="text-align: center;">90.3</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">90.7</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">0.1</td>
</tr>
</tbody>
</table>
<p>Table 6: The results of a leave-one-out ablation. The first row indicates the performance of UnIFIEDQA on each dataset it was trained on. The rest of the rows exclude one dataset at a time. The rows are sorted based on the last column: the dataset with biggest contribution appear first. The red highlights indicate the top 3 performance drops for each column.
reports the best previously published work. For several MC datasets that do not come with evidence paragraphs, we include two variants: one where we use them as-is and another that uses paragraphs fetched via an Information Retrieval (IR) system as additional evidence, indicated with "w/ IR" tags. We use the same IR sentences as used by the baselines: Aristo corpus for ARC and OBQA datasets (Clark et al., 2019c), and 2-step IR for QASC (Khot et al., 2019). For NatQA, following (Min et al., 2020), we use the DPR retrieval engine (Karpukhin et al., 2020) to augment each question with additional paragraphs.</p>
<p>We see that fine-tuning on UNIFIEDQA consistently dominates fine-tuning on T5 and BART, respectively. It also dominates the best previous scores on the datasets. Intuitively, since Uni-</p>
<p>FIEDQA has seen different formats, it should be positioned to achieve higher scores after a little fine-tuning, compared to fine-tuning on a vanilla T5 or BART model. This could be especially effective when a user has limited training data for a target QA task (also shown in Appendix A.6.) This also highlights that the effectiveness of crossformat training is not limited only to T5, but is rather a general trend for text-to-text architectures.</p>
<h3>6.4 Ablation: Training Set Contributions</h3>
<p>We now perform a leave-one-out experiment to better understand the contribution of each seed dataset to UnIFIEDQA. We take the system from $\S 3.2$ and assess how strong the model is when individual seed training datasets are dropped from the union. The result of this experiment is summarized</p>
<p>in Table 6. It compares the performance of full UNIFIEDQA (the first row) with ablated variants that exclude one seed dataset at a time. The rows are sorted based on the last column: datasets with higher contributions appear first.</p>
<p>Looking at first few rows of the table, BoolQ, SQuAD 2.0, OBQA, NarQA are the top four contributing datasets, each with a different format. SQuAD 1.1 has the least importance, presumably because it is mostly covered by SQuAD 2.0.</p>
<p>This study suggests that in order to build an effective unified QA system, it suffices to have a relatively small set of datasets as long as the set includes representatives from each format.</p>
<h2>7 Discussion</h2>
<p>The key motivation for this work is the observation that nearly all prior efforts on QA research were limited to the boundaries defined by narrow formats. A format-specific design would not generalize across QA datasets with slightly different definitions (e.g., a model built for SQuAD would not work for RACE). Additionally, such a design would prevent us from benefiting from the labeled data available in other formats. We challenge this view by advocating for approaches that combine seemingly different datasets. We believe that developing QA systems targeted to a specific format is a conceptual barrier for progress in the field.</p>
<p>Factors affecting generalization. Format is not the only factor affecting generalization across datasets. We additionally studied the value of other factors including dataset size and domain (vocabulary, topic, and style) in improving generalization. We observed that larger datasets often help with generalization, but not always (§5); e.g., RACE or OBQA show similar benefits (Fig. 3), even though RACE is much larger than OBQA. We observed a similar phenomenon with domain: similar domains help with transfer, but that is not always the case. For example, while BoolQ questions, similar to SQuAD, are accompanied with Wiki paragraphs, they barely benefit each other. Overall, the factors affecting generalization are not well-understood, leaving room for future investigations.</p>
<p>Unifying QA formats and text-to-text models. While UnIFIEDQA is built based using existing text-to-text models (Radford et al., 2019a; Raffel et al., 2020), we emphasize that the choice of tasks for multi-task learning plays a crucial role
in achieving successful results. Previous studies (Raffel et al., 2020) did not observe gains when mixing tasks that are very different. The key intuition is that a more coherent choice of tasks is more likely to succeed. Further, focusing on a coherent space of QA tasks/formats allows us to simplify the input by not requiring "prefixes" to explicitly define tasks/formats.</p>
<h2>8 Conclusion</h2>
<p>The question-answering community has fruitfully explored the design of strong models, but while staying within the boundaries of individual QA formats. We argued that such boundaries are artificial and can even limit the performance of systems, because the desired reasoning abilities being taught and probed are not tied to specific formats. Training data in one format should, in principle, help QA systems perform better even on questions in another format.</p>
<p>With this intuition in mind, we presented UnIFIEDQA, a single pre-trained QA system based on the text-to-text paradigm, seeking to bring unification across four common QA formats. We showed that even with its simple multi-format training methodology, UnIFIEDQA achieves performance on par with 8 dataset-specific expert models (§6.1), while also generalizing well to many unseen datasets of seen formats (§6.2). At the same time, we demonstrated that UnIFIEDQA is a strong starting point for building QA systems: it can achieve state-of-the-art performance by simply fine-tuning on target datasets (6.3).</p>
<p>We hope this effort will inspire a future line of work in the QA and NLP communities, moving towards more general and broader system designs. We leave extensions of UnIFIEDQA to other formats such as to direct-answer questions (Roberts et al., 2020) as a promising avenue for future work.</p>
<h2>Acknowledgments</h2>
<p>The authors would like to thank Collin Raffel, Adam Roberts, and Nicholas Lourie for their help with the T5 framework and for providing feedback on an earlier version of this work. The authors would like to acknowledge grants by ONR N00014-18-1-2826 and DARPA N66001-19-2-403, and gifts from the Sloan Foundation and the Allen Institute for AI. Moreover, the authors would like to thank members of the Allen Institute for AI, UW-NLP, and the H2Lab at the University of Wash-</p>
<p>ington for their valuable feedback and comments. TPU machines for conducting experiments were provided by Google.</p>
<h2>References</h2>
<p>Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, et al. 2019. Massively multilingual neural machine translation in the wild: Findings and challenges. In NAACL.</p>
<p>Pratyay Banerjee and Chitta Baral. 2020. Knowledge fusion and semantic knowledge ranking for open domain question answering. arXiv preprint arXiv:2004.03101.</p>
<p>Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2020. Piqa: Reasoning about physical commonsense in natural language. In AAAI.</p>
<p>Rich Caruana. 1997. Multitask learning. Machine learning, 28(1):41-75.</p>
<p>Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019a. Boolq: Exploring the surprising difficulty of natural yes/no questions. In NAACLHLT.</p>
<p>Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D Manning, and Quoc Le. 2019b. BAM! Born-again multi-task networks for natural language understanding. In $A C L$, pages 5931-5937.</p>
<p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? Try ARC, the AI2 reasoning challenge. ArXiv, abs/1803.05457.</p>
<p>Peter Clark, Oren Etzioni, Tushar Khot, Bhavana Dalvi Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, Niket Tandon, Sumithra Bhakthavatsalam, et al. 2019c. From 'F' to 'A' on the NY Regents science exams: An overview of the Aristo project. ArXiv, abs/1909.01958.</p>
<p>Peter Clark, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Turney, and Daniel Khashabi. 2016. Combining retrieval, statistics, and inference to answer elementary science questions. In AAAI.</p>
<p>Pradeep Dasigi, Nelson F. Liu, Ana Marasović, Noah A. Smith, and Matt Gardner. 2019. Quoref: A reading comprehension dataset with questions requiring coreferential reasoning. In EMNLP/IJCNLP.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL.</p>
<p>Dheeru Dua, Ananth Gottumukkala, Alon Talmor, Matt Gardner, and Sameer Singh. 2019a. Comprehensive multi-dataset evaluation of reading comprehension. In 2nd Workshop on Machine Reading for Question Answering.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019b. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In NAACL.</p>
<p>Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen. 2019. MRQA 2019 shared task: Evaluating generalization in reading comprehension. In 2nd Workshop on Machine Reading for Question Answering, at EMNLP.</p>
<p>Matt Gardner, Yoav Artzi, Victoria Basmova, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, et al. 2020. Evaluating models' local decision boundaries via contrast sets. In EMNLP - Findings.</p>
<p>Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In EMNLP.</p>
<p>Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Unifying question answering, text classification, and regression via span extraction. arXiv preprint arXiv:1904.09286.</p>
<p>Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In NAACLHLT.</p>
<p>Daniel Khashabi, Tushar Khot, and Ashish Sabharwal. 2020. More bang for your buck: Natural perturbation for robust question answering. In EMNLP.</p>
<p>Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, and Ashish Sabharwal. 2019. QASC: A dataset for question answering via sentence composition. In AAAI.</p>
<p>Tomás Kociský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. 2018. The narrativeqa reading comprehension challenge. TACL, 6:317-328.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. TACL, 7:453-466.</p>
<p>Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard H. Hovy. 2017. RACE: Large-scale reading comprehension dataset from examinations. In EMNLP.</p>
<p>Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning of language representations. In $I C L R$.</p>
<p>Hector J. Levesque, Ernest Davis, and Leora Morgenstern. 2011. The winograd schema challenge. In $K R$.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In $A C L$.</p>
<p>Chin-Yew Lin, Guihong Cao, Jianfeng Gao, and JianYun Nie. 2006. An information-theoretic approach to automatic evaluation of summaries. In NAACL.</p>
<p>Kevin Lin, Oyvind Tafjord, Peter Clark, and Matt Gardner. 2019. Reasoning over paragraph effects in situations. In 2nd Workshop on Machine Reading for Question Answering, at EMNLP.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. 2018. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730.</p>
<p>Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. In EMNLP.</p>
<p>Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019. A discrete hard EM approach for weakly supervised question answering. In EMNLP/IJCNLP.</p>
<p>Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2020. AmbigQA: Answering ambiguous open-domain questions. In EMNLP.</p>
<p>Arindam Mitra, Pratyay Banerjee, Kuntal Kumar Pal, Swaroop Mishra, and Chitta Baral. 2020. How additional knowledge can improve natural language commonsense question answering. arXiv: Computation and Language.</p>
<p>Kosuke Nishida, Kyosuke Nishida, Masaaki Nagata, Atsushi Otsuka, Itsumi Saito, Hisako Asano, and Junji Tomita. 2019. Answering while summarizing: Multi-task learning for multi-hop qa with evidence extraction. In $A C L$.</p>
<p>Haoruo Peng, Daniel Khashabi, and Dan Roth. 2015. Solving hard coreference problems. In NAACL, pages 809-819.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019a. Language models are unsupervised multitask learners.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019b. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. $J M L R$.</p>
<p>Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable questions for squad. In $A C L$.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In EMNLP.</p>
<p>Matthew Richardson, Christopher J. C. Burges, and Erin Renshaw. 2013. Mctest: A challenge dataset for the open-domain machine comprehension of text. In EMNLP.</p>
<p>Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? In EMNLP.</p>
<p>Mrinmaya Sachan and Eric Xing. 2016. Easy questions first? a case study on curriculum learning for question answering. In $A C L$, pages 453-463.</p>
<p>Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2020. WINOGRANDE: an adversarial winograd schema challenge at scale. In AAAI.</p>
<p>Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. 2019. Social iqa: Commonsense reasoning about social interactions. In EMNLP-IJCNLP, pages 4453-4463.</p>
<p>Elad Segal, Avia Efrat, Mor Shoham, Amir Globerson, and Jonathan Berant. 2019. A simple and effective model for answering multi-span questions. In EMNLP.</p>
<p>Alon Talmor and Jonathan Berant. 2019. Multiqa: An empirical investigation of generalization and transfer in reading comprehension. In $A C L$.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. Commonsenseqa: A question answering challenge targeting commonsense knowledge. In NAACL-HLT.</p>
<p>Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. 2017. Newsqa: A machine comprehension dataset. In Rep4NLP@ACL.</p>
<p>Zhuosheng Zhang, Junjie Yang, and Hai Zhao. 2020. Retrospective reader for machine reading comprehension. ArXiv, abs/2001.09694.</p>
<p>Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. 2020. Freelb: Enhanced adversarial training for natural language understanding. In $I C L R$.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ The evaluation code is available at the URL in Footnote 1.
${ }^{5}$ Appendix A. 5 reports extended results, including the performance with various choices of $X$.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
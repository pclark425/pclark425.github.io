<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6004 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6004</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6004</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-120.html">extraction-schema-120</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-269960406</p>
                <p><strong>Paper Title:</strong> Using ChatGPT in Software Requirements Engineering: A Comprehensive Review</p>
                <p><strong>Paper Abstract:</strong> : Large language models (LLMs) have had a significant impact on several domains, including software engineering. However, a comprehensive understanding of LLMs’ use, impact, and potential limitations in software engineering is still emerging and remains in its early stages. This paper analyzes the role of large language models (LLMs), such as ChatGPT-3.5, in software requirements engineering, a critical area in software engineering experiencing rapid advances due to artificial intelligence (AI). By analyzing several studies, we systematically evaluate the integration of ChatGPT into software requirements engineering, focusing on its benefits, challenges, and ethical considerations. This evaluation is based on a comparative analysis that highlights ChatGPT’s efficiency in eliciting requirements, accuracy in capturing user needs, potential to improve communication among stakeholders, and impact on the responsibilities of requirements engineers. The selected studies were analyzed for their insights into the effectiveness of ChatGPT, the importance of human feedback, prompt engineering techniques, technological limitations, and future research directions in using LLMs in software requirements engineering. This comprehensive analysis aims to provide a differentiated perspective on how ChatGPT can reshape software requirements engineering practices and provides strategic recommendations for leveraging ChatGPT to effectively improve the software requirements engineering process.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6004.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6004.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatDev</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatDev (chat-based software development framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chat-based software development framework that leverages large language models to decompose the software development process into sequential subtasks and coordinate LLM agents across phases to produce design, code and requirements artefacts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Communicative agents for software development.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatDev</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A framework that uses conversational LLM agents to drive end-to-end software development by decomposing work into sequential subtasks and using natural-language communication rather than specialized models for each phase.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this review (original paper refers to use of large-scale LLMs; the review reports ChatDev leverages LLMs generally)</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Natural-language project prompts / high-level software goals; the review does not report a concrete corpus size or number of scholarly papers (ChatDev is applied to software development tasks, not literature corpora).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Task decomposition and multi-agent conversational orchestration (not a literature distillation method per se; approach fragments a task into subtasks handled by LLM agents).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Software artifacts (requirements, code skeletons, tests) produced via chained conversational steps.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>The review notes optimistic efficiency and cost-effectiveness claims but says the original work lacked empirical experiments supporting the claims.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Reported improvements in efficiency and cost-effectiveness for simple projects in the original work according to the review, but no robust empirical validation described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Not reported in the review for ChatDev; original paper referenced as arXiv preprint without benchmark details in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Lack of empirical experiments in original work (as noted by the review), potential hallucinations, limited evaluation scope, and uncertainty about scaling to complex projects.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>No direct comparisons to literature-distillation systems; positioned as an alternative to specialized models per-phase, but empirical comparisons were not provided according to the review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using ChatGPT in Software Requirements Engineering: A Comprehensive Review', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6004.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6004.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Sapper</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Sapper (LLM-empowered production tool for building AI chains)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A block-style visual programming tool enabling composition of prompt-based AI services (AI chains) with LLM co-pilots to assist non-technical users to assemble prompt chains for tasks such as requirements analysis, code skeleton generation, and testing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt sapper: A LLM-empowered production tool for building AI chains.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Prompt Sapper</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Visual tool that helps users compose prompt-based services (AI chains) on top of foundation models, providing LLM co-pilots to support chat-based requirements analysis and visual programming.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in the review (described as using foundation models / LLM co-pilots in the original work).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Interactive prompts and visual block compositions created by users; not described as processing scholarly corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Prompt-chaining / AI-chain composition for task automation (enables chaining LLM calls rather than explicit summarization of many papers).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Composed AI services, chat-based analyses, code skeletons and test runs; aids generation of requirements-related outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>The review reports the original work emphasizes UI and design principles; it criticizes the original for lacking detailed explanation of incorporated principles and practices. No large-scale literature synthesis evaluation reported.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Tool simplifies composition of prompt-based services and permits non-technical 'AI chain engineers' to build LLM-powered workflows; empirical performance details not reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>None reported in the review for literature synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Review notes lack of detailed description of underlying design principles and absence of evaluation for broader real-world tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Not compared to literature-distillation systems; compared in spirit to other prompt-engineering and LLM-assisted UI approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using ChatGPT in Software Requirements Engineering: A Comprehensive Review', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6004.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6004.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chatcoder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chatcoder (chat-based refine-requirement tool)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chat-based scheme in which LLMs interact with users to iteratively refine requirements into more precise, unambiguous and complete expressions and present the refined requirements back to users.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chatcoder</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Chatcoder</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An interactive refinement pipeline that chats with users to clarify and refine informal requirements so they become better-specified for downstream tasks (e.g., code generation).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>LLMs generally (review does not list a particular model version in detail for Chatcoder).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Individual natural-language requirement descriptions (no indication it ingests large scholarly corpora).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Conversational refinement (iterative clarification and rewriting) rather than batch literature distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Refined, clearer requirement statements presented to users.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Small-scope experiments reported in original work; review notes limited experimental scope and domain-specificity.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Positive results for requirement refinement in the limited experiments reported; improved clarity and precision of requirements, but generalizability is uncertain.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Limited dataset(s) / domains in original experiments; specifics not detailed in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Limited experimental scope, domain coverage, and need for further validation across broader contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared qualitatively to human-supervised processes; no large-scale comparisons to automated literature-synthesis tools in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using ChatGPT in Software Requirements Engineering: A Comprehensive Review', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6004.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6004.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt patterns (White et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt pattern catalog for ChatGPT (prompt engineering design patterns)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A catalog of structured prompt-engineering patterns designed to guide ChatGPT to perform tasks such as requirements elicitation, specification disambiguation, and change-request simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A prompt pattern catalog to enhance prompt engineering with chatgpt.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Prompt pattern catalog (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A collection of template prompts / design patterns intended to standardize and improve interactions with ChatGPT across common software engineering tasks, including requirements elicitation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>ChatGPT (explicitly tested with ChatGPT in the studies referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Structured prompt templates provided as input to ChatGPT; not designed to process large bodies of scholarly papers but to improve per-query outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Prompt templating / pattern-guided generation (improves quality of single-query outputs rather than aggregating many scholarly documents).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Improved single-shot or multi-turn ChatGPT outputs: requirements, clarifications, simulations, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Reported pattern catalog was tested with ChatGPT on software-engineering tasks; review notes lack of in-depth practical implementation evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Patterns improve quality/consistency of generated outputs for tasks like requirements elicitation in the authors' tests; detailed quantitative metrics not given in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>No large corpora used for distillation; experiments are prompt-based interaction tests rather than corpus-level synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Review highlights absence of thorough practical implementation studies to establish effectiveness across contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Positioned as complementary to other prompt-engineering approaches; no direct benchmarking against literature-synthesis systems reported in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using ChatGPT in Software Requirements Engineering: A Comprehensive Review', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6004.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6004.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zhang et al. retrieval framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Preliminary evaluation framework for ChatGPT in requirements information retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An empirical framework evaluating ChatGPT's zero-shot performance on requirements information retrieval tasks across multiple datasets, measuring recall and precision trade-offs under different prompting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A preliminary evaluation of chatgpt in requirements information retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatGPT requirements retrieval evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A zero-shot evaluation framework that queries ChatGPT to retrieve requirement-relevant information from datasets and assesses retrieval quality under different prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>ChatGPT (model version not specified in the review summary).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Four datasets covering two requirements analysis tasks (the review does not name the four datasets explicitly), with zero-shot query prompts to ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Zero-shot retrieval from requirements corpora (i.e., information retrieval rather than synthesis of many scholarly papers).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Retrieved requirement-relevant passages / information (text segments), ranked by ChatGPT responses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Information retrieval metrics (recall and precision) applied to ChatGPT outputs under different prompting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>High recall (ChatGPT finds most relevant information) but low precision (it returns broader/less specific data); prompting strategies affect performance.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Four datasets for requirements analysis tasks (not fully enumerated in the review; original paper contains dataset details).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Limited precision in retrieving specific requirement information; potential for over-broad outputs and hallucinations; lacking discussion of domain generalizability in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>The study explored different prompting strategies for ChatGPT; review does not report direct comparisons to classical IR systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using ChatGPT in Software Requirements Engineering: A Comprehensive Review', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6004.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6004.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>El-Hajjami et al. experiment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Which AI Technique Is Better to Classify Requirements? An Experiment with SVM, LSTM, and ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An empirical comparison testing two ChatGPT models (GPT-3.5-turbo and GPT-4) against classical classifiers (SVM) and an LSTM network for requirements classification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Which AI Technique Is Better to Classify Requirements? An Experiment with SVM, LSTM, and ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Requirements classification comparison (GPT-3.5-turbo, GPT-4 vs SVM/LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An experiment assessing classification accuracy of modern ChatGPT models compared with traditional ML and deep learning classifiers on requirement-labeling tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-3.5-turbo and GPT-4 (explicitly evaluated according to the review).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Requirements instances for classification (review does not report the exact dataset size in its summary).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Classification of requirement texts (supervised labels); not literature distillation but text classification.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Class labels for requirement categories.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Comparative performance metrics (implied accuracy/precision/recall) against SVM and LSTM baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>No single best technique for all classification tasks—the best performer depends on the specific requirement classification task; exact numeric results not provided in the review summary.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Not specified in the review summary; original paper contains experimental datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Lack of explainability for ChatGPT decisions noted; variability of best model by task; reviewer notes reluctance to trust black-box outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Direct comparison to SVM and LSTM classifiers; outcome: task-dependent superiority, no universal winner.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using ChatGPT in Software Requirements Engineering: A Comprehensive Review', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6004.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6004.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fantechi et al. variability detection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Exploring LLMs' Ability to Detect Variability in Requirements</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preliminary experiment comparing GPT-3.5 and Microsoft Bing chatbots versus human experts and a rule-based NLP tool for detecting variability in natural-language requirements documents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring LLMs' Ability to Detect Variability in Requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-based variability detection (GPT-3.5 / Bing)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Use of conversational LLMs to detect variability (i.e., alternative choices/conditional elements) in requirements documents compared with humans and rule-based tools.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-3.5 and Microsoft Bing (as reported in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Two natural-language requirements documents used as examples in the preliminary experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Information extraction / variability detection from NL requirements (document-level analysis), not large-scale literature synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Detected variability instances (textual markings / items indicating variability).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Comparative study against human experts and a rule-based NLP tool; qualitative and comparative assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Provides valuable comparative information about strengths/limitations of approaches; review notes that selection of documents could limit generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Two NL requirements documents (small-scale, exploratory).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Very limited and possibly unrepresentative dataset; generalizability and robustness questioned in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Direct comparison to human experts and rule-based NLP; trade-offs discussed but no large-scale benchmark results provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using ChatGPT in Software Requirements Engineering: A Comprehensive Review', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6004.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6004.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT-based mapping (Subahi)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT-Based Approach for Greening Software Requirements Engineering Through Non-Functional Requirements</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-based mechanism for mapping software requirements that evaluates the model with precision, accuracy, recall and F1 metrics on a large dataset focused on non-functional requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BERT-Based Approach for Greening Software Requirements Engineering Through Non-Functional Requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BERT-based requirements mapping</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Applies BERT embeddings and classifiers to map and understand software requirements, focusing on non-functional requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>BERT (specific variant not specified in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Large dataset of requirements (review states 'large dataset' but notes it lacks functional requirements); exact size not given in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Supervised mapping/classification of requirement texts (not literature synthesis across scholarly papers).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Mapped requirement categories / classification labels.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Standard classification metrics: precision, accuracy, recall, F1.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Model achieved high performance across the evaluated metrics on the provided dataset, but applicability is limited by dataset composition.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Custom large dataset focusing on non-functional requirements (functional requirements absent in the dataset used).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Omission of functional requirements limits comprehensiveness; need for dataset adjustments depending on which metrics are prioritized.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Not extensively compared to LLMs in this review; presented as a BERT-based alternative to generative LLM tools for requirements tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using ChatGPT in Software Requirements Engineering: A Comprehensive Review', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6004.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6004.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Luitel et al. BERT MLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Improving requirements completeness: Automated assistance through large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Uses a BERT masked-language model to predict missing tokens/slots in requirement statements to detect incompleteness, evaluated on a subset of the PURE dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving requirements completeness: Automated assistance through large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BERT masked language model for completeness detection</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Applies BERT's masked language modelling to fill masked slots in requirements and use predictions to detect missing information (incompleteness).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>BERT masked language model (specific variant not detailed in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>40 requirements specifications from the PURE dataset (review notes the experiment used half of the PURE dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Masked-token prediction for slot-filling and completeness detection (document-level assistance), not multi-paper synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Predicted tokens/phrases to fill missing slots and a characterization of completeness (predictions classified by performance metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Measured performance, efficiency, stability, accuracy, and reliability; empirical evaluation against portions of PURE dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Positive results for detecting incomplete requirements in the experimental subset, but authors warn that lack of expert collaboration can cause false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>PURE dataset (partial use: 40 requirement specifications).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Used only half of PURE dataset; potential false identifications of incompleteness without domain expert collaboration; limited generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Positioned against human expert judgment; no mention of large-scale comparison to generative LLM literature-synthesis methods in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using ChatGPT in Software Requirements Engineering: A Comprehensive Review', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6004.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6004.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>El-Hajjami/GPT eval (classification)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Empirical evaluation of GPT-3.5-turbo and GPT-4 for requirements classification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A study that empirically evaluated two ChatGPT models (GPT-3.5-turbo and GPT-4) on requirements classification tasks and compared them to traditional SVM and LSTM classifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Which AI Technique Is Better to Classify Requirements? An Experiment with SVM, LSTM, and ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-3.5-turbo / GPT-4 classification evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Empirical experiment testing the classification capability of two modern ChatGPT models on requirements-labeling tasks and comparing performance with classical ML and LSTM approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-3.5-turbo and GPT-4 (explicitly stated in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Requirement texts used for classification (dataset size not specified in the review summary).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Direct classification/prompting for label assignment (not literature distillation).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Predicted requirement class labels.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Comparative evaluation against SVM and LSTM using classification metrics (implied accuracy/precision/recall), as described in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>No universal best model; performance depends on classification task. Review highlights explainability concerns about ChatGPT decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Not detailed in review; original paper has dataset information.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Lack of transparency/explainability in ChatGPT decisions; task-dependent results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Direct comparison to SVM and LSTM: variable relative strengths by task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using ChatGPT in Software Requirements Engineering: A Comprehensive Review', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6004.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e6004.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Oswal et al. GPT-3.5 user-story tool</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transforming Software Requirements into User Stories with GPT-3.5: An AI-Powered Approach</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applies GPT-3.5 to transform unstructured requirements text into structured user stories, aiming to reduce time and human effort in user-story generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transforming Software Requirements into User Stories with GPT-3.5-: An AI-Powered Approach.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-3.5-based user-story transformation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses GPT-3.5 to convert unstructured requirements documents into structured agile user stories to support agile processes.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-3.5 (explicitly cited in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Unstructured requirements text from real-world scenario(s) (exact corpus size not given in review).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Text transformation / NLG to produce structured user stories (single-document processing rather than multi-paper synthesis).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured user stories suitable for agile workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Authors report reduction in time and effort; review notes absence of validation via user testing or case studies.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Reported efficient conversion and time savings in the used scenario; lacking user validation limits claims.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Real-world scenario(s) used in demonstration; no standard benchmark reported in review.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Lack of user testing or case study validation; generalizability unclear.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Not compared against other automated user-story generation methods in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using ChatGPT in Software Requirements Engineering: A Comprehensive Review', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6004.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e6004.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sridhara et al. task evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT: A Study on its Utility for Ubiquitous Software Engineering Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical investigation of ChatGPT across everyday software engineering tasks (ambiguity resolution, naming, test prioritization, code review, log summarization) with comparisons to human expert outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chatgpt: A Study on its Utility for Ubiquitous Software Engineering Tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatGPT multi-task evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Evaluates ChatGPT on a variety of small-scale software engineering tasks to assess suitability as an independent or auxiliary tool.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>ChatGPT (model version not always specified in review summary).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Task-specific inputs for multiple SE tasks; review summarizes comparative evaluation against human expert outputs (dataset sizes not listed).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Task-level generation and comparison (not multi-paper knowledge synthesis).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Task-specific outputs (clarifications, names, prioritized test lists, code reviews, summaries).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Comparison with human experts for each task; qualitative and task-specific metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>ChatGPT performs well on several specific tasks, but fails on some others; can be used independently or as an assistant.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Task-specific data and human expert answers; not presented as a literature-synthesis benchmark in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Task-dependent weaknesses; risk of over-reliance; lacks explanations for model decisions in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Direct comparison to human expert outputs; not compared to literature-distillation systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using ChatGPT in Software Requirements Engineering: A Comprehensive Review', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6004.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e6004.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rasheed et al. multi-agent GPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autonomous Agents in Software Development: A Vision Paper</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Explores using multiple GPT agents to autonomously perform software engineering tasks (requirements analysis, design, code generation, debugging, maintenance) and reports experimental reductions in development time for simple projects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autonomous Agents in Software Development: A Vision Paper.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multi-GPT agent autonomous pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A vision/experimental setup where multiple GPT-based agents coordinate to carry out software engineering workflows with limited human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-family models (specific versions not enumerated in the review summary).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Project-level prompts and simple software project inputs; experiments performed on simple projects rather than corpora of scholarly papers.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Agent orchestration and task decomposition (not literature distillation).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Software deliverables (analysis, design, code, maintenance actions) produced by agent collaboration.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Experimental analysis on simple projects; reported reductions in development time and improvements in code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Reduced development time and advanced code-generation capabilities in simple project experiments; authors provide a roadmap for future work.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Simple software project experiments (no literature corpora).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Experiments on toy/simple projects limit generalizability to complex real-world projects; vision-level work needs further empirical validation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared informally to human-driven processes and single-agent LLM use; no systematic benchmarking against formal literature-synthesis systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using ChatGPT in Software Requirements Engineering: A Comprehensive Review', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A prompt pattern catalog to enhance prompt engineering with chatgpt <em>(Rating: 2)</em></li>
                <li>Prompt sapper: A LLM-empowered production tool for building AI chains <em>(Rating: 2)</em></li>
                <li>Chatcoder <em>(Rating: 2)</em></li>
                <li>Communicative agents for software development <em>(Rating: 2)</em></li>
                <li>A preliminary evaluation of chatgpt in requirements information retrieval <em>(Rating: 2)</em></li>
                <li>Which AI Technique Is Better to Classify Requirements? An Experiment with SVM, LSTM, and ChatGPT. <em>(Rating: 2)</em></li>
                <li>Exploring LLMs' Ability to Detect Variability in Requirements <em>(Rating: 2)</em></li>
                <li>Improving requirements completeness: Automated assistance through large language models <em>(Rating: 2)</em></li>
                <li>BERT-Based Approach for Greening Software Requirements Engineering Through Non-Functional Requirements <em>(Rating: 1)</em></li>
                <li>Transforming Software Requirements into User Stories with GPT-3.5-: An AI-Powered Approach <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6004",
    "paper_id": "paper-269960406",
    "extraction_schema_id": "extraction-schema-120",
    "extracted_data": [
        {
            "name_short": "ChatDev",
            "name_full": "ChatDev (chat-based software development framework)",
            "brief_description": "A chat-based software development framework that leverages large language models to decompose the software development process into sequential subtasks and coordinate LLM agents across phases to produce design, code and requirements artefacts.",
            "citation_title": "Communicative agents for software development.",
            "mention_or_use": "mention",
            "system_name": "ChatDev",
            "system_description": "A framework that uses conversational LLM agents to drive end-to-end software development by decomposing work into sequential subtasks and using natural-language communication rather than specialized models for each phase.",
            "llm_model_used": "Not specified in this review (original paper refers to use of large-scale LLMs; the review reports ChatDev leverages LLMs generally)",
            "input_type_and_size": "Natural-language project prompts / high-level software goals; the review does not report a concrete corpus size or number of scholarly papers (ChatDev is applied to software development tasks, not literature corpora).",
            "distillation_approach": "Task decomposition and multi-agent conversational orchestration (not a literature distillation method per se; approach fragments a task into subtasks handled by LLM agents).",
            "output_type": "Software artifacts (requirements, code skeletons, tests) produced via chained conversational steps.",
            "evaluation_methods": "The review notes optimistic efficiency and cost-effectiveness claims but says the original work lacked empirical experiments supporting the claims.",
            "results": "Reported improvements in efficiency and cost-effectiveness for simple projects in the original work according to the review, but no robust empirical validation described in this paper.",
            "datasets_or_benchmarks": "Not reported in the review for ChatDev; original paper referenced as arXiv preprint without benchmark details in this review.",
            "challenges_or_limitations": "Lack of empirical experiments in original work (as noted by the review), potential hallucinations, limited evaluation scope, and uncertainty about scaling to complex projects.",
            "comparisons_to_other_methods": "No direct comparisons to literature-distillation systems; positioned as an alternative to specialized models per-phase, but empirical comparisons were not provided according to the review.",
            "uuid": "e6004.0",
            "source_info": {
                "paper_title": "Using ChatGPT in Software Requirements Engineering: A Comprehensive Review",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Prompt Sapper",
            "name_full": "Prompt Sapper (LLM-empowered production tool for building AI chains)",
            "brief_description": "A block-style visual programming tool enabling composition of prompt-based AI services (AI chains) with LLM co-pilots to assist non-technical users to assemble prompt chains for tasks such as requirements analysis, code skeleton generation, and testing.",
            "citation_title": "Prompt sapper: A LLM-empowered production tool for building AI chains.",
            "mention_or_use": "mention",
            "system_name": "Prompt Sapper",
            "system_description": "Visual tool that helps users compose prompt-based services (AI chains) on top of foundation models, providing LLM co-pilots to support chat-based requirements analysis and visual programming.",
            "llm_model_used": "Not specified in the review (described as using foundation models / LLM co-pilots in the original work).",
            "input_type_and_size": "Interactive prompts and visual block compositions created by users; not described as processing scholarly corpora.",
            "distillation_approach": "Prompt-chaining / AI-chain composition for task automation (enables chaining LLM calls rather than explicit summarization of many papers).",
            "output_type": "Composed AI services, chat-based analyses, code skeletons and test runs; aids generation of requirements-related outputs.",
            "evaluation_methods": "The review reports the original work emphasizes UI and design principles; it criticizes the original for lacking detailed explanation of incorporated principles and practices. No large-scale literature synthesis evaluation reported.",
            "results": "Tool simplifies composition of prompt-based services and permits non-technical 'AI chain engineers' to build LLM-powered workflows; empirical performance details not reported in this review.",
            "datasets_or_benchmarks": "None reported in the review for literature synthesis.",
            "challenges_or_limitations": "Review notes lack of detailed description of underlying design principles and absence of evaluation for broader real-world tasks.",
            "comparisons_to_other_methods": "Not compared to literature-distillation systems; compared in spirit to other prompt-engineering and LLM-assisted UI approaches.",
            "uuid": "e6004.1",
            "source_info": {
                "paper_title": "Using ChatGPT in Software Requirements Engineering: A Comprehensive Review",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Chatcoder",
            "name_full": "Chatcoder (chat-based refine-requirement tool)",
            "brief_description": "A chat-based scheme in which LLMs interact with users to iteratively refine requirements into more precise, unambiguous and complete expressions and present the refined requirements back to users.",
            "citation_title": "Chatcoder",
            "mention_or_use": "mention",
            "system_name": "Chatcoder",
            "system_description": "An interactive refinement pipeline that chats with users to clarify and refine informal requirements so they become better-specified for downstream tasks (e.g., code generation).",
            "llm_model_used": "LLMs generally (review does not list a particular model version in detail for Chatcoder).",
            "input_type_and_size": "Individual natural-language requirement descriptions (no indication it ingests large scholarly corpora).",
            "distillation_approach": "Conversational refinement (iterative clarification and rewriting) rather than batch literature distillation.",
            "output_type": "Refined, clearer requirement statements presented to users.",
            "evaluation_methods": "Small-scope experiments reported in original work; review notes limited experimental scope and domain-specificity.",
            "results": "Positive results for requirement refinement in the limited experiments reported; improved clarity and precision of requirements, but generalizability is uncertain.",
            "datasets_or_benchmarks": "Limited dataset(s) / domains in original experiments; specifics not detailed in this review.",
            "challenges_or_limitations": "Limited experimental scope, domain coverage, and need for further validation across broader contexts.",
            "comparisons_to_other_methods": "Compared qualitatively to human-supervised processes; no large-scale comparisons to automated literature-synthesis tools in the review.",
            "uuid": "e6004.2",
            "source_info": {
                "paper_title": "Using ChatGPT in Software Requirements Engineering: A Comprehensive Review",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Prompt patterns (White et al.)",
            "name_full": "Prompt pattern catalog for ChatGPT (prompt engineering design patterns)",
            "brief_description": "A catalog of structured prompt-engineering patterns designed to guide ChatGPT to perform tasks such as requirements elicitation, specification disambiguation, and change-request simulation.",
            "citation_title": "A prompt pattern catalog to enhance prompt engineering with chatgpt.",
            "mention_or_use": "mention",
            "system_name": "Prompt pattern catalog (ChatGPT)",
            "system_description": "A collection of template prompts / design patterns intended to standardize and improve interactions with ChatGPT across common software engineering tasks, including requirements elicitation.",
            "llm_model_used": "ChatGPT (explicitly tested with ChatGPT in the studies referenced).",
            "input_type_and_size": "Structured prompt templates provided as input to ChatGPT; not designed to process large bodies of scholarly papers but to improve per-query outputs.",
            "distillation_approach": "Prompt templating / pattern-guided generation (improves quality of single-query outputs rather than aggregating many scholarly documents).",
            "output_type": "Improved single-shot or multi-turn ChatGPT outputs: requirements, clarifications, simulations, etc.",
            "evaluation_methods": "Reported pattern catalog was tested with ChatGPT on software-engineering tasks; review notes lack of in-depth practical implementation evaluation.",
            "results": "Patterns improve quality/consistency of generated outputs for tasks like requirements elicitation in the authors' tests; detailed quantitative metrics not given in this review.",
            "datasets_or_benchmarks": "No large corpora used for distillation; experiments are prompt-based interaction tests rather than corpus-level synthesis.",
            "challenges_or_limitations": "Review highlights absence of thorough practical implementation studies to establish effectiveness across contexts.",
            "comparisons_to_other_methods": "Positioned as complementary to other prompt-engineering approaches; no direct benchmarking against literature-synthesis systems reported in the review.",
            "uuid": "e6004.3",
            "source_info": {
                "paper_title": "Using ChatGPT in Software Requirements Engineering: A Comprehensive Review",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Zhang et al. retrieval framework",
            "name_full": "Preliminary evaluation framework for ChatGPT in requirements information retrieval",
            "brief_description": "An empirical framework evaluating ChatGPT's zero-shot performance on requirements information retrieval tasks across multiple datasets, measuring recall and precision trade-offs under different prompting strategies.",
            "citation_title": "A preliminary evaluation of chatgpt in requirements information retrieval.",
            "mention_or_use": "mention",
            "system_name": "ChatGPT requirements retrieval evaluation",
            "system_description": "A zero-shot evaluation framework that queries ChatGPT to retrieve requirement-relevant information from datasets and assesses retrieval quality under different prompts.",
            "llm_model_used": "ChatGPT (model version not specified in the review summary).",
            "input_type_and_size": "Four datasets covering two requirements analysis tasks (the review does not name the four datasets explicitly), with zero-shot query prompts to ChatGPT.",
            "distillation_approach": "Zero-shot retrieval from requirements corpora (i.e., information retrieval rather than synthesis of many scholarly papers).",
            "output_type": "Retrieved requirement-relevant passages / information (text segments), ranked by ChatGPT responses.",
            "evaluation_methods": "Information retrieval metrics (recall and precision) applied to ChatGPT outputs under different prompting strategies.",
            "results": "High recall (ChatGPT finds most relevant information) but low precision (it returns broader/less specific data); prompting strategies affect performance.",
            "datasets_or_benchmarks": "Four datasets for requirements analysis tasks (not fully enumerated in the review; original paper contains dataset details).",
            "challenges_or_limitations": "Limited precision in retrieving specific requirement information; potential for over-broad outputs and hallucinations; lacking discussion of domain generalizability in the review.",
            "comparisons_to_other_methods": "The study explored different prompting strategies for ChatGPT; review does not report direct comparisons to classical IR systems.",
            "uuid": "e6004.4",
            "source_info": {
                "paper_title": "Using ChatGPT in Software Requirements Engineering: A Comprehensive Review",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "El-Hajjami et al. experiment",
            "name_full": "Which AI Technique Is Better to Classify Requirements? An Experiment with SVM, LSTM, and ChatGPT",
            "brief_description": "An empirical comparison testing two ChatGPT models (GPT-3.5-turbo and GPT-4) against classical classifiers (SVM) and an LSTM network for requirements classification tasks.",
            "citation_title": "Which AI Technique Is Better to Classify Requirements? An Experiment with SVM, LSTM, and ChatGPT.",
            "mention_or_use": "mention",
            "system_name": "Requirements classification comparison (GPT-3.5-turbo, GPT-4 vs SVM/LSTM)",
            "system_description": "An experiment assessing classification accuracy of modern ChatGPT models compared with traditional ML and deep learning classifiers on requirement-labeling tasks.",
            "llm_model_used": "GPT-3.5-turbo and GPT-4 (explicitly evaluated according to the review).",
            "input_type_and_size": "Requirements instances for classification (review does not report the exact dataset size in its summary).",
            "distillation_approach": "Classification of requirement texts (supervised labels); not literature distillation but text classification.",
            "output_type": "Class labels for requirement categories.",
            "evaluation_methods": "Comparative performance metrics (implied accuracy/precision/recall) against SVM and LSTM baselines.",
            "results": "No single best technique for all classification tasks—the best performer depends on the specific requirement classification task; exact numeric results not provided in the review summary.",
            "datasets_or_benchmarks": "Not specified in the review summary; original paper contains experimental datasets.",
            "challenges_or_limitations": "Lack of explainability for ChatGPT decisions noted; variability of best model by task; reviewer notes reluctance to trust black-box outputs.",
            "comparisons_to_other_methods": "Direct comparison to SVM and LSTM classifiers; outcome: task-dependent superiority, no universal winner.",
            "uuid": "e6004.5",
            "source_info": {
                "paper_title": "Using ChatGPT in Software Requirements Engineering: A Comprehensive Review",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Fantechi et al. variability detection",
            "name_full": "Exploring LLMs' Ability to Detect Variability in Requirements",
            "brief_description": "A preliminary experiment comparing GPT-3.5 and Microsoft Bing chatbots versus human experts and a rule-based NLP tool for detecting variability in natural-language requirements documents.",
            "citation_title": "Exploring LLMs' Ability to Detect Variability in Requirements.",
            "mention_or_use": "mention",
            "system_name": "LLM-based variability detection (GPT-3.5 / Bing)",
            "system_description": "Use of conversational LLMs to detect variability (i.e., alternative choices/conditional elements) in requirements documents compared with humans and rule-based tools.",
            "llm_model_used": "GPT-3.5 and Microsoft Bing (as reported in the review).",
            "input_type_and_size": "Two natural-language requirements documents used as examples in the preliminary experiment.",
            "distillation_approach": "Information extraction / variability detection from NL requirements (document-level analysis), not large-scale literature synthesis.",
            "output_type": "Detected variability instances (textual markings / items indicating variability).",
            "evaluation_methods": "Comparative study against human experts and a rule-based NLP tool; qualitative and comparative assessment.",
            "results": "Provides valuable comparative information about strengths/limitations of approaches; review notes that selection of documents could limit generalizability.",
            "datasets_or_benchmarks": "Two NL requirements documents (small-scale, exploratory).",
            "challenges_or_limitations": "Very limited and possibly unrepresentative dataset; generalizability and robustness questioned in the review.",
            "comparisons_to_other_methods": "Direct comparison to human experts and rule-based NLP; trade-offs discussed but no large-scale benchmark results provided.",
            "uuid": "e6004.6",
            "source_info": {
                "paper_title": "Using ChatGPT in Software Requirements Engineering: A Comprehensive Review",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "BERT-based mapping (Subahi)",
            "name_full": "BERT-Based Approach for Greening Software Requirements Engineering Through Non-Functional Requirements",
            "brief_description": "A BERT-based mechanism for mapping software requirements that evaluates the model with precision, accuracy, recall and F1 metrics on a large dataset focused on non-functional requirements.",
            "citation_title": "BERT-Based Approach for Greening Software Requirements Engineering Through Non-Functional Requirements.",
            "mention_or_use": "mention",
            "system_name": "BERT-based requirements mapping",
            "system_description": "Applies BERT embeddings and classifiers to map and understand software requirements, focusing on non-functional requirements.",
            "llm_model_used": "BERT (specific variant not specified in the review).",
            "input_type_and_size": "Large dataset of requirements (review states 'large dataset' but notes it lacks functional requirements); exact size not given in the review.",
            "distillation_approach": "Supervised mapping/classification of requirement texts (not literature synthesis across scholarly papers).",
            "output_type": "Mapped requirement categories / classification labels.",
            "evaluation_methods": "Standard classification metrics: precision, accuracy, recall, F1.",
            "results": "Model achieved high performance across the evaluated metrics on the provided dataset, but applicability is limited by dataset composition.",
            "datasets_or_benchmarks": "Custom large dataset focusing on non-functional requirements (functional requirements absent in the dataset used).",
            "challenges_or_limitations": "Omission of functional requirements limits comprehensiveness; need for dataset adjustments depending on which metrics are prioritized.",
            "comparisons_to_other_methods": "Not extensively compared to LLMs in this review; presented as a BERT-based alternative to generative LLM tools for requirements tasks.",
            "uuid": "e6004.7",
            "source_info": {
                "paper_title": "Using ChatGPT in Software Requirements Engineering: A Comprehensive Review",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Luitel et al. BERT MLM",
            "name_full": "Improving requirements completeness: Automated assistance through large language models",
            "brief_description": "Uses a BERT masked-language model to predict missing tokens/slots in requirement statements to detect incompleteness, evaluated on a subset of the PURE dataset.",
            "citation_title": "Improving requirements completeness: Automated assistance through large language models.",
            "mention_or_use": "mention",
            "system_name": "BERT masked language model for completeness detection",
            "system_description": "Applies BERT's masked language modelling to fill masked slots in requirements and use predictions to detect missing information (incompleteness).",
            "llm_model_used": "BERT masked language model (specific variant not detailed in the review).",
            "input_type_and_size": "40 requirements specifications from the PURE dataset (review notes the experiment used half of the PURE dataset).",
            "distillation_approach": "Masked-token prediction for slot-filling and completeness detection (document-level assistance), not multi-paper synthesis.",
            "output_type": "Predicted tokens/phrases to fill missing slots and a characterization of completeness (predictions classified by performance metrics).",
            "evaluation_methods": "Measured performance, efficiency, stability, accuracy, and reliability; empirical evaluation against portions of PURE dataset.",
            "results": "Positive results for detecting incomplete requirements in the experimental subset, but authors warn that lack of expert collaboration can cause false positives.",
            "datasets_or_benchmarks": "PURE dataset (partial use: 40 requirement specifications).",
            "challenges_or_limitations": "Used only half of PURE dataset; potential false identifications of incompleteness without domain expert collaboration; limited generalizability.",
            "comparisons_to_other_methods": "Positioned against human expert judgment; no mention of large-scale comparison to generative LLM literature-synthesis methods in the review.",
            "uuid": "e6004.8",
            "source_info": {
                "paper_title": "Using ChatGPT in Software Requirements Engineering: A Comprehensive Review",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "El-Hajjami/GPT eval (classification)",
            "name_full": "Empirical evaluation of GPT-3.5-turbo and GPT-4 for requirements classification",
            "brief_description": "A study that empirically evaluated two ChatGPT models (GPT-3.5-turbo and GPT-4) on requirements classification tasks and compared them to traditional SVM and LSTM classifiers.",
            "citation_title": "Which AI Technique Is Better to Classify Requirements? An Experiment with SVM, LSTM, and ChatGPT.",
            "mention_or_use": "mention",
            "system_name": "GPT-3.5-turbo / GPT-4 classification evaluation",
            "system_description": "Empirical experiment testing the classification capability of two modern ChatGPT models on requirements-labeling tasks and comparing performance with classical ML and LSTM approaches.",
            "llm_model_used": "GPT-3.5-turbo and GPT-4 (explicitly stated in the review).",
            "input_type_and_size": "Requirement texts used for classification (dataset size not specified in the review summary).",
            "distillation_approach": "Direct classification/prompting for label assignment (not literature distillation).",
            "output_type": "Predicted requirement class labels.",
            "evaluation_methods": "Comparative evaluation against SVM and LSTM using classification metrics (implied accuracy/precision/recall), as described in the review.",
            "results": "No universal best model; performance depends on classification task. Review highlights explainability concerns about ChatGPT decisions.",
            "datasets_or_benchmarks": "Not detailed in review; original paper has dataset information.",
            "challenges_or_limitations": "Lack of transparency/explainability in ChatGPT decisions; task-dependent results.",
            "comparisons_to_other_methods": "Direct comparison to SVM and LSTM: variable relative strengths by task.",
            "uuid": "e6004.9",
            "source_info": {
                "paper_title": "Using ChatGPT in Software Requirements Engineering: A Comprehensive Review",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Oswal et al. GPT-3.5 user-story tool",
            "name_full": "Transforming Software Requirements into User Stories with GPT-3.5: An AI-Powered Approach",
            "brief_description": "Applies GPT-3.5 to transform unstructured requirements text into structured user stories, aiming to reduce time and human effort in user-story generation.",
            "citation_title": "Transforming Software Requirements into User Stories with GPT-3.5-: An AI-Powered Approach.",
            "mention_or_use": "mention",
            "system_name": "GPT-3.5-based user-story transformation",
            "system_description": "Uses GPT-3.5 to convert unstructured requirements documents into structured agile user stories to support agile processes.",
            "llm_model_used": "GPT-3.5 (explicitly cited in the review).",
            "input_type_and_size": "Unstructured requirements text from real-world scenario(s) (exact corpus size not given in review).",
            "distillation_approach": "Text transformation / NLG to produce structured user stories (single-document processing rather than multi-paper synthesis).",
            "output_type": "Structured user stories suitable for agile workflows.",
            "evaluation_methods": "Authors report reduction in time and effort; review notes absence of validation via user testing or case studies.",
            "results": "Reported efficient conversion and time savings in the used scenario; lacking user validation limits claims.",
            "datasets_or_benchmarks": "Real-world scenario(s) used in demonstration; no standard benchmark reported in review.",
            "challenges_or_limitations": "Lack of user testing or case study validation; generalizability unclear.",
            "comparisons_to_other_methods": "Not compared against other automated user-story generation methods in the review.",
            "uuid": "e6004.10",
            "source_info": {
                "paper_title": "Using ChatGPT in Software Requirements Engineering: A Comprehensive Review",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Sridhara et al. task evaluation",
            "name_full": "ChatGPT: A Study on its Utility for Ubiquitous Software Engineering Tasks",
            "brief_description": "Empirical investigation of ChatGPT across everyday software engineering tasks (ambiguity resolution, naming, test prioritization, code review, log summarization) with comparisons to human expert outputs.",
            "citation_title": "Chatgpt: A Study on its Utility for Ubiquitous Software Engineering Tasks.",
            "mention_or_use": "mention",
            "system_name": "ChatGPT multi-task evaluation",
            "system_description": "Evaluates ChatGPT on a variety of small-scale software engineering tasks to assess suitability as an independent or auxiliary tool.",
            "llm_model_used": "ChatGPT (model version not always specified in review summary).",
            "input_type_and_size": "Task-specific inputs for multiple SE tasks; review summarizes comparative evaluation against human expert outputs (dataset sizes not listed).",
            "distillation_approach": "Task-level generation and comparison (not multi-paper knowledge synthesis).",
            "output_type": "Task-specific outputs (clarifications, names, prioritized test lists, code reviews, summaries).",
            "evaluation_methods": "Comparison with human experts for each task; qualitative and task-specific metrics.",
            "results": "ChatGPT performs well on several specific tasks, but fails on some others; can be used independently or as an assistant.",
            "datasets_or_benchmarks": "Task-specific data and human expert answers; not presented as a literature-synthesis benchmark in the review.",
            "challenges_or_limitations": "Task-dependent weaknesses; risk of over-reliance; lacks explanations for model decisions in some cases.",
            "comparisons_to_other_methods": "Direct comparison to human expert outputs; not compared to literature-distillation systems.",
            "uuid": "e6004.11",
            "source_info": {
                "paper_title": "Using ChatGPT in Software Requirements Engineering: A Comprehensive Review",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Rasheed et al. multi-agent GPT",
            "name_full": "Autonomous Agents in Software Development: A Vision Paper",
            "brief_description": "Explores using multiple GPT agents to autonomously perform software engineering tasks (requirements analysis, design, code generation, debugging, maintenance) and reports experimental reductions in development time for simple projects.",
            "citation_title": "Autonomous Agents in Software Development: A Vision Paper.",
            "mention_or_use": "mention",
            "system_name": "Multi-GPT agent autonomous pipeline",
            "system_description": "A vision/experimental setup where multiple GPT-based agents coordinate to carry out software engineering workflows with limited human oversight.",
            "llm_model_used": "GPT-family models (specific versions not enumerated in the review summary).",
            "input_type_and_size": "Project-level prompts and simple software project inputs; experiments performed on simple projects rather than corpora of scholarly papers.",
            "distillation_approach": "Agent orchestration and task decomposition (not literature distillation).",
            "output_type": "Software deliverables (analysis, design, code, maintenance actions) produced by agent collaboration.",
            "evaluation_methods": "Experimental analysis on simple projects; reported reductions in development time and improvements in code generation.",
            "results": "Reduced development time and advanced code-generation capabilities in simple project experiments; authors provide a roadmap for future work.",
            "datasets_or_benchmarks": "Simple software project experiments (no literature corpora).",
            "challenges_or_limitations": "Experiments on toy/simple projects limit generalizability to complex real-world projects; vision-level work needs further empirical validation.",
            "comparisons_to_other_methods": "Compared informally to human-driven processes and single-agent LLM use; no systematic benchmarking against formal literature-synthesis systems.",
            "uuid": "e6004.12",
            "source_info": {
                "paper_title": "Using ChatGPT in Software Requirements Engineering: A Comprehensive Review",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A prompt pattern catalog to enhance prompt engineering with chatgpt",
            "rating": 2,
            "sanitized_title": "a_prompt_pattern_catalog_to_enhance_prompt_engineering_with_chatgpt"
        },
        {
            "paper_title": "Prompt sapper: A LLM-empowered production tool for building AI chains",
            "rating": 2,
            "sanitized_title": "prompt_sapper_a_llmempowered_production_tool_for_building_ai_chains"
        },
        {
            "paper_title": "Chatcoder",
            "rating": 2
        },
        {
            "paper_title": "Communicative agents for software development",
            "rating": 2,
            "sanitized_title": "communicative_agents_for_software_development"
        },
        {
            "paper_title": "A preliminary evaluation of chatgpt in requirements information retrieval",
            "rating": 2,
            "sanitized_title": "a_preliminary_evaluation_of_chatgpt_in_requirements_information_retrieval"
        },
        {
            "paper_title": "Which AI Technique Is Better to Classify Requirements? An Experiment with SVM, LSTM, and ChatGPT.",
            "rating": 2,
            "sanitized_title": "which_ai_technique_is_better_to_classify_requirements_an_experiment_with_svm_lstm_and_chatgpt"
        },
        {
            "paper_title": "Exploring LLMs' Ability to Detect Variability in Requirements",
            "rating": 2,
            "sanitized_title": "exploring_llms_ability_to_detect_variability_in_requirements"
        },
        {
            "paper_title": "Improving requirements completeness: Automated assistance through large language models",
            "rating": 2,
            "sanitized_title": "improving_requirements_completeness_automated_assistance_through_large_language_models"
        },
        {
            "paper_title": "BERT-Based Approach for Greening Software Requirements Engineering Through Non-Functional Requirements",
            "rating": 1,
            "sanitized_title": "bertbased_approach_for_greening_software_requirements_engineering_through_nonfunctional_requirements"
        },
        {
            "paper_title": "Transforming Software Requirements into User Stories with GPT-3.5-: An AI-Powered Approach",
            "rating": 1,
            "sanitized_title": "transforming_software_requirements_into_user_stories_with_gpt35_an_aipowered_approach"
        }
    ],
    "cost": 0.020474999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Using ChatGPT in Software Requirements Engineering: A Comprehensive Review
21 May 2024</p>
<p>Nuno Marques 
Coimbra Institute of Engineering-ISEC
Polytechnic University of Coimbra
3030-199Rua Pedro Nunes, CoimbraPortugal</p>
<p>Rodrigo Rocha rrochas@dei.uc.pt 
Centre for Informatics and Systems
University of Coimbra (CISUC)
Pólo II</p>
<p>Pinhal de Marrocos
3030-290CoimbraPortugal</p>
<p>FATEC Mogi das Cruzes
São Paulo Technological College
08773-600Mogi das CruzesBrazil</p>
<p>Jorge Bernardino 0000-0001-9660-2011
Coimbra Institute of Engineering-ISEC
Polytechnic University of Coimbra
3030-199Rua Pedro Nunes, CoimbraPortugal</p>
<p>Centre for Informatics and Systems
University of Coimbra (CISUC)
Pólo II</p>
<p>Pinhal de Marrocos
3030-290CoimbraPortugal</p>
<p>Using ChatGPT in Software Requirements Engineering: A Comprehensive Review
21 May 2024CA57B2226D32083E0FE419EF7852185710.3390/fi16060180Received: 15 March 2024 Revised: 10 May 2024 Accepted: 16 May 2024ChatGPTLLMssoftware engineeringsoftware requirementsgenerative AI
Large language models (LLMs) have had a significant impact on several domains, including software engineering.However, a comprehensive understanding of LLMs' use, impact, and potential limitations in software engineering is still emerging and remains in its early stages.This paper analyzes the role of large language models (LLMs), such as ChatGPT-3.5, in software requirements engineering, a critical area in software engineering experiencing rapid advances due to artificial intelligence (AI).By analyzing several studies, we systematically evaluate the integration of ChatGPT into software requirements engineering, focusing on its benefits, challenges, and ethical considerations.This evaluation is based on a comparative analysis that highlights ChatGPT's efficiency in eliciting requirements, accuracy in capturing user needs, potential to improve communication among stakeholders, and impact on the responsibilities of requirements engineers.The selected studies were analyzed for their insights into the effectiveness of ChatGPT, the importance of human feedback, prompt engineering techniques, technological limitations, and future research directions in using LLMs in software requirements engineering.This comprehensive analysis aims to provide a differentiated perspective on how ChatGPT can reshape software requirements engineering practices and provides strategic recommendations for leveraging ChatGPT to effectively improve the software requirements engineering process.</p>
<p>Introduction</p>
<p>Software requirements engineering is a critical phase in the software development lifecycle that lays the foundation for successfully delivering software products that meet stakeholders' needs and expectations.Traditionally, this process involves eliciting, analyzing, specifying, and validating requirements, often time-consuming, labor-intensive, and prone to human error and bias.Herein lies the potential for large language models (LLMs) to increase and streamline these processes by providing efficient and effective solutions for requirement elicitation, documentation, and validation [1].An IEEE consolidated definition for a software requirement is a documented condition or capability that a system or system component meets or possesses to solve a contractual, standard, specification problem, or objective [2].</p>
<p>In recent years, advances in natural language processing (NLP) have paved the way for transformative applications in diverse domains, such as healthcare, finance, e-commerce, social media, and more.Among these breakthroughs, LLMs have emerged as powerful tools with the potential to revolutionize software engineering practices, particularly in software requirements engineering.</p>
<p>LLMs like ChatGPT have unprecedented capabilities for understanding, generating, and processing natural language text.Their vast pre-trained knowledge and ability to Future Internet 2024, 16, 180 2 of 21 generate contextually relevant text make them promising candidates to assist and automate various tasks in software development [3].However, integrating LLMs into software requirements engineering presents challenges and concerns.</p>
<p>While LLMs exhibit impressive capabilities in generating coherent and contextually relevant text, they may also produce inaccuracies, ambiguities, or biased outputs, posing risks to the reliability and quality of software requirements.Moreover, ethical considerations such as data privacy, model bias, and transparency justify careful examination when employing LLMs in sensitive domains like software development [4].A critical aspect of using LLMs effectively is formulating appropriate prompts-the input text or queries provided to these models to generate desired outputs [5].</p>
<p>Prompt patterns, defined as structured and systematic approaches to creating prompts for LLMs, have emerged as critical tools for realizing the full potential of these models.Researchers and practitioners can guide LLMs to produce outputs that align more closely with their intended objectives by defining specific patterns or templates for prompts.These patterns facilitate the generation of coherent and contextually relevant text and enable finer control over the outputs generated by LLMs [6,7].</p>
<p>This study aims to evaluate the impact of ChatGPT on the software requirements elicitation process.To achieve this goal, we investigated the benefits and challenges of this Generative AI (GenAI) based on studies conducted by other authors and provided a critical analysis of these studies.</p>
<p>The main contributions of this work are the following:</p>
<p>• Understanding the role of GenAI in software requirements;</p>
<p>•</p>
<p>Characterizing the benefits and challenges of using ChatGPT to assist with software requirements; • Identifying future research directions.</p>
<p>Considering the transformative potential of large language models (LLMs) like Chat-GPT in software requirements engineering, this research seeks to answer the following research question: RQ: How can ChatGPT be used to improve software requirements engineering processes, and what are the associated benefits, challenges, and ethical considerations?</p>
<p>To address this question, we conducted a literature review evaluating the application of ChatGPT in software requirements engineering.Through this analysis, we elucidate the role of ChatGPT in streamlining requirement elicitation, documentation, and validation while characterizing the benefits and delineating the challenges of integrating ChatGPT in this domain.Additionally, we explore the ethical considerations and propose future research directions to optimize the use of LLMs in software requirements engineering, providing a foundation for further exploration and development in this area.</p>
<p>The rest of this paper is structured as follows: Section 2 provides some background about software requirements engineering, LLMs, and an overview of ChatGPT.Section 3 describes the methodology used to conduct the literature review.Section 4 presents the literature review.Sections 5 and 6 approach the benefits and challenges of using ChatGPT in software requirements engineering, respectively.Section 7 outlines future research directions.Finally, Section 8 presents the conclusions.</p>
<p>Background</p>
<p>While the application of artificial intelligence/machine learning (AI/ML) in software engineering research has a long history, the specific use of GenAI is a more recent and emerging topic.Although the potential of GenAI has been recognized for some time, research progress in this area has been rapid, particularly since 2020.Despite some earlier exploration of GPT-2 for code generation, GenAI has recently gained significant attention in software engineering.Recent advances in these systems, particularly introducing services such as GitHub Copilot and ChatGPT-4, have spurred increased research interest in various disciplines, including software engineering [8].In this section, we provide an overview of the basic concepts of software requirements engineering, LLMs, and an overview of ChatGPT.</p>
<p>Software Requirements Engineering</p>
<p>Software requirements engineering is a systematic and process-driven approach to defining, documenting, and maintaining software requirements across the software development lifecycle [1].This multidimensional task requires robust information retrieval, effective communication with multiple stakeholders, and the creation of detailed textual descriptions.It comprises two main phases: requirements development and management [9]:</p>
<p>•</p>
<p>Requirements development: This involves activities such as eliciting, analyzing, specifying, and validating the requirements.</p>
<p>The requirements elicitation phase involves identifying stakeholders, selecting representatives, and determining their needs.It serves as the information-gathering step in requirements development.Various techniques for requirements elicitation include stakeholder interviews, focus groups, workshops, observations, questionnaires, document analysis, and benchmarking.</p>
<p>The requirements analysis step synthesizes and refines information gathered during requirements elicitation.Stakeholder needs, assumptions, and other information are integrated and further detailed.This phase involves representing requirements in various forms, such as prototypes and models, conducting trade-off analysis, setting priorities, assessing feasibility, and identifying gaps to uncover missing requirements.</p>
<p>A recommended practice for requirements specification is to utilize predefined templates.These templates enable requirements engineers to concentrate on content rather than format, reducing the risk of overlooking critical items while documenting requirements.</p>
<p>The final step in the requirements development process is to validate the requirements to ensure they are well-written, complete, and aligned with customer needs.Validation may result in iterations of previous steps due to identified defects, gaps, additional information or analysis requirements, clarification, or other issues.</p>
<p>•</p>
<p>Requirements management: This includes the processes for requesting changes to established requirements, performing impact analysis on those changes, approving or rejecting them, and then implementing the approved changes.</p>
<p>Requirements management is a continuous process that spans the entire software development lifecycle.During testing, the software product is validated against its requirements to identify and correct defects, ensure that it meets the specified requirements, and provide confidence in its functionality [9].</p>
<p>Large Language Model (LLM)</p>
<p>A large language model (LLM) is a deep learning algorithm that can perform a variety of tasks in natural language processing (NLP).Large language models use transformer models and are trained using massive datasets.This enables them to recognize, translate, predict, or generate text or other content [4].LLMs such as InstructGPT and GPT-4 excel at in-context learning and generating coherent and contextually relevant responses based on given prompts.Reinforcement Learning from Human Feedback (RLHF) is a fundamental technique for LLMs.It consists of incrementally improving the model's performance by using human-generated responses as feedback [4].</p>
<p>Large language models (LLMs) are gaining popularity in academia and industry due to their unprecedented performance in various applications.As LLMs continue to play an important role in research and daily use, their evaluation becomes increasingly important, not only at the task level but also at the societal level, to better understand their potential risks.In recent years, significant efforts have been made to study LLMs from different perspectives.</p>
<p>A common approach to interacting with large language models (LLMs) is prompt engineering, where users design specific prompts to guide LLMs to generate desired responses or complete tasks.This approach is widely used in evaluation efforts.Furthermore, users can interact with LLMs in question-and-answer or dialogue mode, encouraging natural language conversations.In summary, LLMs, leveraging transformer architecture, in-context learning, and Reinforcement Learning from Human Feedback (RLHF), have revolutionized natural language processing (NLP) and show promise in several applications, such as ChatGPT and GitHub Copilot [6].</p>
<p>Overview of ChatGPT</p>
<p>ChatGPT is an AI chatbot built using OpenAI's large language models (LLMs), such as GPT-4 and earlier versions [10].It has set new standards (tasks, metrics, etc.) in artificial intelligence by demonstrating machines' ability to understand the intricacies of human language and communication.</p>
<p>OpenAI unveiled an initial demonstration of ChatGPT on 30 November 2022, sparking widespread interest on social media as users demonstrated its capabilities.Examples ranged from travel planning to creating fables to coding computer programs.Within five days, the chatbot had more than one million users.ChatGPT's evolution has been one of continuous improvement, with each version building on the foundation laid by its predecessors.</p>
<p>This evolution is also characterized by an exponential increase in the number of parameters used to train the model.This increase in parameters generally leads to significant improvements in the effectiveness of the solutions presented by the models.This is because models with more parameters have a greater ability to learn and capture complex nuances and patterns in the training data.</p>
<p>Chat GPT-4, the latest version, continues this trend of exponential improvement with changes such as improved model alignment, Internet connectivity, better steerability, and more [10].</p>
<p>The GPT model and software engineering intersect by applying natural language processing (NLP) techniques to various tasks within the software development lifecycle.GPT's language generation capabilities provide valuable support and enhancements to software engineering processes [11].</p>
<p>ChatGPT can streamline the process of gathering, analyzing, and documenting requirements in the context of software requirements.It uses its extensive knowledge base and language understanding capabilities to engage stakeholders in conversations, generate accurate documentation, and provide real-time feedback on requirements.Through its ability to continuously learn and adapt, ChatGPT improves stakeholder collaboration, accelerates the requirements gathering process, and ultimately contributes to more efficient and successful software development outcomes.However, ChatGPT should not be viewed as a replacement for human expertise and judgment.Instead, ChatGPT should be viewed as a complement to it [12].</p>
<p>Methodology</p>
<p>This section outlines the approach taken in conducting the literature review, which involved synthesizing existing knowledge, critically assessing methodologies, and analyzing results to compare the use of ChatGPT to improve the quality of software requirements.The process is illustrated in Figure 1 and explained in the following sections.</p>
<p>Data Sources</p>
<p>Google Scholar (www.googlescholar.com)has developed over the years and has become a robust database for scientific literature [13].Therefore, it was chosen as the main research tool for the present study.However, we add two other important public data sources: IEEE Xplore (https://ieeexplore.ieee.org/)and the ACM Digital Library (https://dl.acm.org/).</p>
<p>Search Queries</p>
<p>A search in Google Scholar on 19 April 2024, with the query "(ChatGPT OR LLM) AND ("software requirements")" returned about 403 results in just 0.04 s.The same query was used in IEEE Xplore and returned 51 results, and using the ACM Digital Library, we obtained 59 results (see Figure 1).</p>
<p>Inclusion Criteria</p>
<p>We considered the year of publication as an inclusion criteria.Because ChatGPT was launched that year, only research published from 2022 to the present was considered.</p>
<p>This review includes the most relevant papers from each year.Relevance in Google Scholar refers to the degree to which the search results match the criteria or context of the query.The sorting algorithm considers several factors to determine the order of the results, including the presence of search terms and citation counts.</p>
<p>Exclusion Criteria</p>
<p>The exclusion criteria were books, internal reports, theses/dissertations, citations, presentations, abstracts, and appendices.Papers written in languages other than English were also excluded.</p>
<p>Characterization of Selected Papers</p>
<p>Figure 1 shows a distribution of the total number of papers retrieved from the ACM Digital Library (59), IEEE Xplore (51), and Google Scholar (403) as data sources.The earliest paper dates from 2022, and thus, potentially, 513 papers could be selected.</p>
<p>Applying the exclusion criteria defined in Section 3.4, the total number of papers was reduced to 314, distributed as follows for each data source: ACM Digital Library (13), IEEE Xplore (44), and Google Scholar (257).</p>
<p>After removing the papers found in two or more data sources, the final result is 267.A distribution of these papers retrieved per year is shown in Figure 2. As can be seen, the rise of GenAI, exemplified by the use of models such as ChatGPT in software requirements, has catalyzed an exponential increase in research.This review includes the most relevant papers from each year.Relevance in Google Scholar refers to the degree to which the search results match the criteria or context of the query.The sorting algorithm considers several factors to determine the order of the results, including the presence of search terms and citation counts.</p>
<p>Exclusion Criteria</p>
<p>The exclusion criteria were books, internal reports, theses/dissertations, citations, presentations, abstracts, and appendices.Papers written in languages other than English were also excluded.</p>
<p>Characterization of Selected Papers</p>
<p>Figure 1 shows a distribution of the total number of papers retrieved from the ACM Digital Library (59), IEEE Xplore (51), and Google Scholar (403) as data sources.The earliest paper dates from 2022, and thus, potentially, 513 papers could be selected.</p>
<p>Applying the exclusion criteria defined in Section 3.4, the total number of papers was reduced to 314, distributed as follows for each data source: ACM Digital Library (13), IEEE Xplore (44), and Google Scholar (257).</p>
<p>After removing the papers found in two or more data sources, the final result is 267.A distribution of these papers retrieved per year is shown in Figure 2. As can be seen, the rise of GenAI, exemplified by the use of models such as ChatGPT in software requirements, has catalyzed an exponential increase in research.</p>
<p>From this deduplicated number (267), we selected 22 papers for our analysis.This selection was based on each paper's title, abstract, introduction, and conclusions, with the criteria being those that use the capabilities of LLM, specifically ChatGPT, to improve the quality of software requirements.</p>
<p>Literature Review</p>
<p>This literature review included the most recent scientific literature on the impact of LLMs in software requirements.As mentioned above, we selected 22 papers, which are distributed by year as follows: 2022: 1, 2023: 16, and 2024: 5.In this section, the papers are presented by year, and within each year, they are presented in alphabetical order of author(s).From this deduplicated number (267), we selected 22 papers for our analysis.This selection was based on each paper's title, abstract, introduction, and conclusions, with the criteria being those that use the capabilities of LLM, specifically ChatGPT, to improve the quality of software requirements.</p>
<p>Literature Review</p>
<p>This literature review included the most recent scientific literature on the impact of LLMs in software requirements.As mentioned above, we selected 22 papers, which are distributed by year as follows: 2022: 1, 2023: 16, and 2024: 5.In this section, the papers are presented by year, and within each year, they are presented in alphabetical order of author(s).</p>
<p>Liu et al. [2] emphasize the importance of requirements engineering in the software development lifecycle and discuss the role of artificial intelligence (AI) in improving requirements quality.They review recent research on how AI techniques like machine learning (ML), classification, and natural language processing (NLP) have advanced requirements engineering.The authors suggest employing appropriate ML and NLP techniques to meet textual requirements, which can extract deeper meaning and improve requirements engineering performance.The authors highlight the ability of AI to assist requirements engineers by automating mundane tasks in the requirements engineering process, allowing them to focus more on creative aspects.A strength of this study is the identification of the knowledge gap on how AI should be incorporated into software requirements processes to ensure high quality requirements representation.Although the text summarizes each recent study and its impact on the field of requirements engineering, it does not provide specific details about these results.It would be useful to include a more detailed analysis of recent studies, highlighting their key findings and how they contribute to advancing the field of requirements engineering.</p>
<p>Abdelfattah et al. [14] present a method for teaching software engineering using ChatGPT.This method involves using ChatGPT to create software architecture diagrams, which can be very useful in generating software requirements.The authors highlight the simplicity of understanding the basic concepts of requirements engineering through this approach.In parallel, the study highlights the importance of ChatGPT in capturing software requirements and demonstrates its application in real cases.Overall, this research confirms the effectiveness of ChatGPT in understanding and eliciting software requirements, thus consolidating the growing relevance of this tool in the field of software engineering.A strong point of the study is the emphasis on understanding requirements engineering.This emphasis is supported by the creation of practical exercises that focus on understanding the basic principles of requirements engineering, a crucial aspect of software development.The step-by-step approach to user interaction with ChatGPT in creating diagrams also facilitates understanding of the process, making it another strong research point.However, the lack of examples of requirements generated as a result of the experiments may raise doubts about the effectiveness of this method.Including concrete examples of software requirements generated by ChatGPT during the experiments could provide a more tangible validation of the usefulness and relevance of the proposed method.</p>
<p>Arora et al. [15] propose a SWOT analysis for research and development using LLMs for requirements engineering, focusing on the potential for requirements elicitation, analysis, specification, and validation.The authors evaluate ChatGPT's performance in classifying requirements.By analyzing the results obtained, they provide a comprehensive understanding of the effectiveness of this approach and add credibility to their conclusions.However, they suggest a cautiously optimistic outlook on the role of AI in elicitation and validation processes.Balancing the benefits of LLMs with the need for rigorous validation, ethical compliance, and human oversight is suggested to realize their full potential in requirements engineering.</p>
<p>Belzner et al. [16] discuss the potential benefits and challenges of using large language models (LLMs) in software engineering.They discuss the opportunities in requirements engineering, system design, code generation, test generation, code reviews, and software processes.It also thoroughly reviews current state-of-the-art methods for using LLMs in software engineering.However, a significant gap is the lack of explanation of how ChatGPT makes its decisions to achieve the desired results.This lack of transparency in decision-making can undermine developers' confidence in ChatGPT.Without understanding how the model arrives at its answers, developers are reluctant to adopt or fully trust ChatGPT's recommendations.</p>
<p>Bencheik et al. [1] highlight the effectiveness of ChatGPT in generating software requirements and the role of human input.Moreover, the study acknowledged the time efficiency of ChatGPT but noted that experienced human participants tend to produce more thorough requirements.They highlight the importance of using AI tools to complement human expertise in requirements engineering.Although the output generated by ChatGPT is efficient, the user still needs to verify its integrity.The authors emphasized ChatGPT's capability to mimic human expertise while underscoring the critical role of human feedback in improving requirements quality.However, the lack of detail about the practical challenges in implementing GenAI tools can impact the quality of the requirements generated.A deeper understanding of the obstacles faced in the practical application of GenAI is crucial to ensure that the proposed solutions are effective and appropriate to the needs of software development.</p>
<p>Cheng et al. [17] provided a study focused on integrating AI assistant prototypes into established programming environments, emphasizing systematizing methodologies and proposing design principles for AI-assisted programming.The results indicate that the user interface of the AI assistant within the integrated development environment (IDE) significantly impacts the tool's effectiveness, underscoring the importance of careful design considerations.The authors introduce Prompt Sapper, a block-style visual programming tool that enables AI chain engineers, including non-technical users, to compose promptbased AI services using foundation models through chat-based requirements analysis and visual programming.Sapper includes two views, with LLM co-pilots to help elicit requirements, generate code skeleton, and run/test the AI service.This tool allows AI chain engineers to create prompt-based services on top of foundation models, using chatbased requirements analysis and visual programming.This makes it easier for users to understand how to use these tools.However, a weakness of this study is the lack of explanation of the principles and practices incorporated into this methodology.A more detailed description of the principles and practices underlying the Prompt Sapper tool would be beneficial to readers.</p>
<p>El-Hajjami et al. [18] empirically evaluated two ChatGPT models (GPT-3.5-turboand GPT-4) for requirements classification and compared them with traditional classification methods such as support vector machines (SVM) and long short-term memory (LSTM).They concluded that there is no single best technique for all requirements classifications because the best technique varies depending on the specific requirement classification.By analyzing the results obtained, they provide a comprehensive understanding of the effectiveness of this approach and add credibility to their conclusions.However, a significant gap is the lack of explanation of how ChatGPT makes its decisions to achieve the desired results.This lack of transparency in decision-making can undermine developers' confidence in ChatGPT.Developers are reluctant to adopt or fully trust ChatGPT's recommendations without understanding how the model arrives at its answers.</p>
<p>Kutzner et al. [19] conducted a study that provided insights into the potential and challenges of generating texts for requirements specification in software development using AI.The authors highlighted the need for humans to control the process, regardless of AI's accuracy and effectiveness in software tasks.Also, implementing AI text generation to create requirements and functional specifications has facilitated documentation creation.The study's strength is recognizing the crucial role of requirements and functional specifications in the software development process.These specifications are key to facilitating communication and promoting effective understanding between stakeholders and software developers.The focus of research on improving the efficiency and accuracy of the transition from functional and non-functional requirements to functional specifications further raises the importance of these aspects.However, the failure to validate the results obtained through testing in real software development projects to assess their practical usefulness and effectiveness can be considered a limitation of the study.This practical validation is crucial to understanding the complexities and challenges encountered in professional contexts.</p>
<p>Qian et al. [20] propose ChatDev, a chat-based software development framework that leverages large language models (LLMs) throughout the entire software development process, streamlining and unifying key processes through natural language communication, thereby eliminating the need for specialized models at each phase.The research highlights the improvement of the desired output by decomposing the software development process into sequential subtasks.The results are optimistic, praising the efficiency and cost-effectiveness of the software development process using ChatDev.The most significant implication is the approach and exploration of the use of large-scale language models (LLMs) throughout the software development process, thus eliminating the need for specialized models for each phase.This approach promises to reduce complexity and make communication between developers more efficient, contributing to a more fluid and cohesive integration of the various stages of software development.However, a weak point of this study is the lack of empirical experiments to support the results obtained.The lack of concrete evidence from empirical experience may reduce the credibility and confidence of the conclusions presented in the study.</p>
<p>Ronanki et al. [3] investigated the potential of ChatGPT to support software requirements.Six requirements elicitation questions were formulated, and interview-based surveys were conducted with requirements engineering experts.The results showed that ChatGPT-generated requirements scored higher than human-generated requirements on several quality attributes.They highlight the need for further research to effectively use LLMs in natural language-based requirements engineering activities.One of the strengths of this study is the active participation of requirements engineering experts, which contributes to increasing the reliability of the results.However, it is important to note that this evaluation is based exclusively on the opinions of experts, which can introduce bias into the analysis.Incorporating additional validation techniques could increase the effectiveness of the evaluation of the requirements generated.</p>
<p>Rasheed et al. [21] explored how multiple GPT agents can perform various software engineering tasks, requirements analysis, design, code generation, debugging, and managing software maintenance autonomously.The results show a reduction in development time and advanced code generation methods, reinforcing the potential of AI-driven software engineering practices to make software development more efficient, accessible, and innovative.The strength of this study is the experimental analysis that demonstrates the capabilities of ChatGPT during the various phases of software requirements.Another strong point is the creation of a roadmap for future work, highlighting the intention to improve ChatGPT's capabilities.However, it is important to emphasize that the study is based on an experimental analysis performed on simple software projects.This may limit the generalization of the results to more complex projects in software engineering practice, which is a weakness of this study.</p>
<p>Sridhara et al. [12] used ChatGPT to investigate everyday software engineering tasks such as resolving ambiguity in software requirements, suggesting method names, prioritizing test cases, reviewing code, and summarizing logs.They compared responses generated by ChatGPT with corresponding outputs from human experts.They conclude that ChatGPT does very well for specific tasks but not so well for others that it cannot provide answers.They concluded that ChatGPT can be employed independently or as an auxiliary tool for software engineering tasks.</p>
<p>The study conducted by Subahi [22] presents a mechanism for mapping software requirements using the BERT language model in conjunction with a large dataset.Four main criteria were considered to evaluate the model's effectiveness: precision, accuracy, recall, and F1 score.After a series of tests, the model proved to be a significant advance in understanding software requirements, achieving high performance in all the criteria evaluated.However, despite the model's promising potential, the author highlights the need for adjustments depending on the importance attributed to each criterion.This adjustment aims to balance precision and recall, ensuring a more complete and practical approach to software requirements analysis.A significant shortcoming of this work is the lack of consideration of functional requirements in the dataset used in the experiment.Functional requirements play a crucial role in software development, describing the specific functionalities that the system must provide to meet users' needs.By not including functional requirements in the experiment dataset, the analysis may be incomplete and not fully reflect the complexity and real challenges faced in the requirements engineering process.</p>
<p>Wang et al. [23] introduced a framework called Chatcoder to refine the requirements by chatting with LLMs.They design a chat scheme in which the LLMs will guide the human users to refine their expression of requirements to be more precise, unambiguous, and complete than before.It then presents the refined arguments back to the users in an understandable way.This represents a significant advance, as it provides requirements engineering professionals with a practical tool for improving the quality of software requirements.Another important contribution is the recognition of the importance of human supervision and intervention in ensuring the quality and relevance of requirements.The involvement of human users in reviewing and editing the requirements generated by LLM is fundamental to ensuring that they meet the needs and expectations of the stakeholders.</p>
<p>However, a weakness of this study is the limited experimental scope.Although the experiment showed positive results regarding the effectiveness of LLM in refining requirements, the scope of the experiments may be limited to specific domains or data sets.This may affect the generalizability of the results to broader contexts or real-world applications, requiring additional validation and testing in different contexts to ensure the robustness and applicability of the results.</p>
<p>White et al. [6,7] provided a catalog of prompt engineering design patterns for various software engineering tasks, including requirements elicitation.A list of requirements elicitation patterns is presented to aid in creating software requirements and exploring their completeness with respect to desired system capabilities and accuracy: requirements simulator, specification disambiguation, and change request simulation.All these patterns were tested with the ChatGPT.The systematic classification provides quick access to solutions for specific challenges, thereby increasing efficiency and productivity.However, this work lacks an in-depth exploration of practical implementations of the prompt patterns to thoroughly assess their effectiveness.</p>
<p>Zhang et al. [24] developed an approach to empirically evaluate ChatGPT's ability to retrieve information about requirements.This framework performed a preliminary evaluation of ChatGPT's zero-shot requirements retrieval performance on two requirements analysis tasks over four datasets.The evaluation results show that ChatGPT can retrieve the relevant information about requirements (high recall).However, its ability to retrieve more specific requirements information is limited (low precision).One of the most significant implications of the study is that their experiments provide valuable information on the effectiveness of various prompting strategies in interacting with ChatGPT.However, a weakness of this study is the lack of discussion about the benefits and challenges of using ChatGPT in software requirements gathering.This lack of analysis can lead developers to over-rely on ChatGPT, which may not be advisable.Without a comprehensive understanding of the benefits and challenges associated with using ChatGPT in the requirements elicitation process, developers may not be fully prepared to deal with its limitations and nuances.</p>
<p>Fantechi et al. [25] analyze the potential usefulness of extended language models (LLMs) for detecting requirements variability in natural language (NL) requirements documents using GPT-3.5 and Microsoft Bing.To investigate this issue, the authors carried out a preliminary experiment.They used two NL requirements documents as examples.They compared the variability detection capabilities of LLM-based chatbots with those of human experts and a rule-based natural language processing (NLP) tool.The comparative study carried out by the authors is a strength because the results obtained provide valuable information on the performance of the different approaches to variability detection.This comparison allows readers to gain insight into each approach's advantages and limitations, helping them choose the best tool according to their needs.The selection of natural language (NL) requirements documents can be considered a weakness identified in this study.The effectiveness of LLM-based tools in detecting variability can vary significantly depending on the characteristics and complexity of the NL requirements documents selected.A less comprehensive or representative selection of these documents may raise questions about the generalizability and reliability of the results obtained.</p>
<p>Luitel et al. [26] focused on requirements completeness and used BERT's masked language model (MLM) to generate predictions for filling masked slots in requirements.The authors used 40 requirements specifications from the PURE dataset to determine the model's accuracy.The predictions are characterized according to the following criteria: performance, efficiency, stability, accuracy, and reliability.Despite the positive results presented, the authors recognize that the lack of collaboration between software engineering experts can incorrectly identify cases of incompleteness in the requirements and consequently lead to false effectiveness.Detecting incomplete requirements is a major challenge in the field of requirements engineering, as incomplete requirements can lead to significant errors during the software development process.A strong point of the research is the emphasis on this crucial challenge.The relevance of this study is backed up by an empirical evaluation that references real requirements specifications from the PURE dataset.The results shown demonstrate the effectiveness of the proposed approach in dealing with the detection of incomplete requirements.However, it is important to note that the experiment was conducted using only half of the PURE dataset, which may limit the generalizability of the results to the full dataset.</p>
<p>Oswal et al. [27] highlight the importance of agile methodologies in software development, especially the concept of user stories.They emphasize that the manual generation of these stories from unstructured requirements is laborious and challenging.To solve this problem, an AI-based approach is proposed, using the GPT-3.5 language model to transform requirements texts into structured user stories efficiently.In their conclusions, the authors highlight the reduction in time and effort required to obtain user stories while also minimizing the risk of human error.They also emphasize the opportunity for professionals to focus on more valuable tasks.A strong point is the use of a real-world scenario, which effectively demonstrates how agile approaches can successfully produce software solutions.This approach gives readers a deeper understanding of the process and how it can be applied in practice.However, one weakness is the lack of validation of the results through user testing or case studies.</p>
<p>Waseem et al. [28] presented a study investigating the effectiveness of ChatGPT as a software development bot in different phases of the software development lifecycle, particularly projects led by students.The results highlighted skill deficits among students and a remarkable enthusiasm for AI.ChatGPT demonstrated its value as a supportive tool, fostering efficiency and collaboration.The impact demonstrated by this study on the use of ChatGPT in the academic environment is evident.However, one gap observed is the lack of an in-depth discussion of the specific advantages and limitations of using ChatGPT in the various phases of software requirements.Each phase of the software development process presents unique challenges and requirements, and understanding how ChatGPT can be most effective in each of these phases is crucial to its adoption and practical application.</p>
<p>Yeow et al. [29] explore the potential of ChatGPT-3.5 for automating software requirements engineering tasks by performing an analysis and evaluation of the questions generated by ChatGPT-3.5 for eliciting software requirements.The evaluation criteria include the questions' legibility, clarity, relevance, and completeness.The authors also identify the challenges and limitations of using ChatGPT-3.5 for software requirements gathering.The results of the experiments indicate a performance that meets university educational standards regarding legibility.Although the generated questions show above-average results in other criteria, the authors point out that there is still room for improvement in the approach and structure of this method.We have identified two strengths.Firstly, the authors offer a comprehensive analysis of the capabilities, limitations, and potential applications of ChatGPT-3.5 in requirements engineering, allowing the reader to gain a more effective understanding of these aspects.Secondly, they present concrete recommendations for future research focused on integrating ChatGPT-3.5 into software requirements engineering.However, a weakness of this study is the lack of an in-depth analysis of how the prompts used in the experiments were constructed.The quality of the prompt directly influences the effectiveness of the output generated by ChatGPT-3.5.Therefore, a more detailed analysis of the prompts used could provide additional insights into how to optimize the performance of ChatGPT-3.5 in generating software requirements.</p>
<p>We concluded that only a few papers describe in detail the benefits of ChatGPT in software requirements.Therefore, we aim to fill this gap and further explore these benefits.</p>
<p>Table 1 presents a comparison of the analyzed works, listing the fundamental aspects that we consider for a comprehensive analysis of scientific works that aim to discuss the use of ChatGPT for requirements generation.We attempt to provide a nuanced overview of the current research landscape by examining key features such as the effectiveness of ChatGPT in generating requirements, the indispensable role of human feedback, the exploration of prompt engineering, and the challenges and limitations of AI, as well as future research directions.This analysis sheds light on the symbiotic relationship between human insight and artificial intelligence in software requirements engineering and navigates through the contributions, conclusions, and future directions suggested by the selected studies.Below, we highlight the importance of each aspect analyzed and provide a rationale as to why that aspect is essential to be analyzed in the context of this work:</p>
<p>1.</p>
<p>Effectiveness in Generating Requirements: Evaluating ChatGPT's effectiveness in generating software requirements.We considered this characteristic to assess Chat-GPT's ability to automate and optimize the elicitation and specification of requirements, which is traditionally an arduous process in terms of time and human effort.</p>
<p>2.</p>
<p>Role of Human Input: Human input is fundamental to guiding and improving the quality of the requirements generated by ChatGPT.Human supervision and feedback are essential for the tools to produce requirements that are aligned with stakeholder needs.</p>
<p>3.</p>
<p>Exploring Prompt Engineering: Input to language models is fundamental for adequate results.We understand that evaluating this aspect significantly influences the quality and relevance of ChatGPT responses in requirements engineering.4.</p>
<p>AI Challenges and Limitations: When considering generative AI for requirements specification, it is fundamental to recognize the challenges and limitations of tools such as ChatGPT.For example, issues such as bias, the possibility of generating hallucinations, and a lack of detail can affect the integrity of the generated requirements.</p>
<p>5.</p>
<p>Future Research Directions: Any scientific work that proposes practical approaches should identify potential areas for future research and how studies can advance the use of ChatGPT in requirements engineering.</p>
<p>6.</p>
<p>Comparative Human Expertise: Comparing human expertise is essential to understanding how ChatGPT aligns with or complements the knowledge and skills of human requirements engineers.This is particularly important because, although Chat-GPT can automate specific processes, human expertise remains irreplaceable in understanding and analyzing the complex and contextual requirements of software projects.The highlighted issues are included because they provide a holistic view of using Chat-GPT and other similar tools for requirements bidding, taking into account the benefits and potential challenges of this approach.Understanding these issues is critical to optimizing the use of ChatGPT to achieve requirements specifications.</p>
<p>Potential Benefits of Using ChatGPT</p>
<p>Based on the literature review, this section and the following one answer our Research Question: "How can ChatGPT be used to improve software requirements engineering processes, and what are the associated benefits, challenges, and ethical considerations?".From a requirements engineer's point of view, several potential benefits offered by ChatGPT have been identified.Table 2 lists these benefits and identifies each of the papers in the literature review where they are mentioned.In the following sections, we will discuss each of these in more detail.</p>
<p>Improving Brainstorming and Idea Exploration</p>
<p>Improving brainstorming is a significant benefit of incorporating ChatGPT into software requirements processes.ChatGPT helps generate a wide range of ideas and suggestions, stimulating creativity and exploring different possibilities.It offers different perspectives and draws from its extensive knowledge base, enriching the brainstorming process with new insights [1].</p>
<p>ChatGPT demonstrates proficiency in creative writing tasks by refining its output through various stages, such as brainstorming, creating stories or poems, and generating speeches.This means that ChatGPT is able to perfect brainstorming techniques to generate more creative and effective ideas for writing tasks [1].</p>
<p>ChatGPT inspires solutions and facilitates collaboration among team members with its ability to synthesize shared perceptions.Through iterative dialogue, ChatGPT allows ideas to be refined over time, resulting in broader requirements.In addition, ChatGPT provides real-time feedback on proposed ideas, helping to make informed decisions and ensure alignment with project goals.</p>
<p>Continuous Learning</p>
<p>Continuous learning is a valuable benefit of integrating ChatGPT into software requirements processes.ChatGPT continuously learns from interactions and feedback, adapting and improving to effectively capture evolving requirements.With access to vast sources of information, ChatGPT expands its knowledge base and provides more informed responses.</p>
<p>The use of real-time user feedback is a practice that contributes significantly to improving the quality of the software requirements refined by ChatGPT.Users can quickly correct errors or inaccuracies in the output generated by ChatGPT by immediately commenting on it.They can also provide additional details to further clarify the requirements [21].Moreover, this real-time feedback can be used to enrich ChatGPT's training data.By exposing the model to a variety of interactions and corrections, the model can learn from this information and improve its ability to generate refined software requirements in the future [15].</p>
<p>In this iterative learning process, ChatGPT incorporates user feedback, which contributes to better documentation requirements by improving quality and accuracy.It develops an in-depth knowledge of specific domains and terminologies, leading to more appropriate responses to context-based information needs.These incremental learning practices positively impact the effectiveness of capturing and processing requirements requests, resulting in faster turnaround times.In addition, by staying current with emerging software industry trends and changing project needs, ChatGPT maintains its relevance as a helpful software requirements tool.</p>
<p>Minimize Human Error</p>
<p>The integration of ChatGPT into the software requirements engineering process significantly mitigates the risk of human error.ChatGPT's automation of documentation helps to reduce errors associated with manual transcription and interpretation.ChatGPT ensures a consistent approach to requirements documentation by conforming to established rules and standards, thereby minimizing variation due to human factors.The ChatGPT's ability to generate documentation with unambiguous and precise requirements avoids misunderstandings [7].During the quality assurance phase, the model plays a critical role in identifying and resolving discrepancies and inconsistencies within the requirements, facilitating the correction of errors before they propagate through the development process.</p>
<p>Through cross-referencing and validation, ChatGPT ensures policy compliance and reduces the risk of errors.ChatGPT helps prevent errors from spreading throughout the requirements documentation by prompting for additional information or clarification when needed.Integrating ChatGPT results in more accurate and reliable documentation, easier software development, and fewer costly errors.</p>
<p>Thus, automating the process of creating software requirements can help reduce human error.In this way, ChatGPT's ability to detect inconsistencies in software requirements reduces the likelihood of human error in the requirements creation process [21].</p>
<p>Cost Savings</p>
<p>Gathering initial requirements for software projects involves stakeholder engagement, where ChatGPT interacts with stakeholders.Based on these interactions, it creates documentation of the software requirements.It's important to note that this is an example of how ChatGPT can help analyze and prioritize requirements.Processing large data volumes and identifying inconsistencies or gaps in the specification can be instrumental in ensuring quality assurance activities.Using ChatGPT for software requirements can save costs by streamlining the requirements gathering and analysis process, improving documentation quality, and increasing overall project management efficiency.These contributions result in cost savings in man-hours and reduced manual effort [21].</p>
<p>Also, ChatGPT helps generate code snippets from natural language descriptions in software engineering, increasing developer efficiency and allowing focus on higher-level design.Furthermore, it assists in debugging by detecting errors and providing suggestions for fixing them, speeding up the debugging process, and reducing some of the development time [30].</p>
<p>ChatGPT's software requirements generation process automation allows developers to focus on higher value tasks.At the same time, users only need to review the software requirements generated, which increases developer productivity.These factors help reduce development and labor costs [21,23].</p>
<p>Efficiency and Accuracy</p>
<p>One of the key benefits of integrating ChatGPT into software requirements processes is efficiency and accuracy.Stakeholder engagement, customer interviews, and feedback analysis are some use cases where ChatGPT allows faster and more comprehensive requirements gathering than traditional labor-intensive methods.Documentation helps with automation, leading to better accuracy and completeness while saving time and helping reduce human error.Another benefit of using ChatGPT is the ability to quickly analyze large amounts of data to identify patterns, inconsistencies, and gaps to ensure requirements are complete and meet stakeholder needs.</p>
<p>With its ability to identify inconsistencies and ambiguities in quality assurance, Chat-GPT provides immediate suggestions for resolving them by ensuring that responses are concise, accurate, and relevant.Reducing the level of rework by preventing misunderstandings or omissions in software development projects contributes to improved efficiency and resource utilization throughout all phases [31].</p>
<p>The effectiveness and accuracy of ChatGPT throughout the software development process are very positive [28].This is evidenced by the quality of the software requirements generated by ChatGPT, which are comparable to those generated by humans.However, it is important to emphasize the need for human expertise and feedback mechanisms to improve its performance [1].</p>
<p>Enhanced Productivity</p>
<p>The potential of ChatGPT to improve productivity in software development, especially in software requirements generation, has been widely recognized [1,21,29].</p>
<p>ChatGPT automates software engineering processes, including code development, testing, updating, and documentation, allowing human developers to focus on more creative problem-solving skills and innovation.This significantly increases the productivity of software development teams by allowing them to take on larger and more challenging projects.In addition, the ability of programmers to quickly turn their thoughts into code ensures that software is released faster and at a lower cost [2].</p>
<p>Limitations and Risks of Using ChatGPT</p>
<p>Despite its potential, the use of ChatGPT in software requirements engineering raises several concerns.Considering the literature reviewed, some limitations and risks are identified.Table 3 provides a list of these limitations and risks and identifies each of the papers in the literature review in which they were discussed.These concerns are presented in the following sections.</p>
<p>Limitations and Risks Paper</p>
<p>Addressing Bias in GenAI Systems Bencheik et al. [1], El-Hajjami et al. [18], Luitel et al. [26], Ronanki et al. [3] Information Hallucination Belzner et al. [16], Qian et al. [20], Ronanki et al. [3] Lack of Explainability and Transparency Ronanki et al. [3] Susceptibility to Attacks Arora et al. [15] Reasoning Errors Arora et al. [15], Ronanki et al. [3] Over-reliance Arora et al. [15], Waseem et al. [28], Yeow et al. [29] Transparency and Accountability Belzner et al. [16] 6.1.Addressing Bias in GenAI Systems GenAI systems are susceptible to incorporating biases present in their training data.This bias can lead to results that perpetuate discrimination or turn out to be misleading interpretations, which can lead to dissent and public disapproval at the political, social justice, and legal levels [30].Manifestations of bias include the following:</p>
<p>•</p>
<p>Training data bias: Language models learn from large datasets of human language.If these datasets contain biases related to race, gender, or socioeconomic status, the model may internalize these biases and reproduce them in its responses.A classic example might be that if there is a gender bias in the training data, the model is more likely to favor a particular gender in its responses.</p>
<p>•</p>
<p>User interaction bias: Responses generated by chatbots are influenced by user input.The model can learn and perpetuate these biases if users consistently ask biased or prejudiced questions.Consequently, if users frequently ask discriminatory questions that target a particular group, the model may generate responses that reinforce these biases.</p>
<p>•</p>
<p>Algorithmic bias: Bias can arise from the algorithms used to train and run language models and chatbots.For example, suppose a model is optimized to achieve accuracy or engagement as its performance metric.In this case, it may favor responses that meet that metric, even though those responses may also be biased.</p>
<p>•</p>
<p>Contextual bias: Contextual bias is possible when chatbots generate responses based on contextual information provided by the user.The model could generate biased responses if factors such as language or location are biased.For example, if a user asks about a particular culture or religion, and the model has not been trained in that particular context, it could potentially provide biased responses due to limited knowledge.</p>
<p>Although ChatGPT provides good accuracy and efficiency in generating software requirements, there is still the possibility that the generated results contain biases [26].Also, the use of specific data sets to train the model can lead to bias in the results [18].To help mitigate these biases, increasing the model's training data is a viable solution.Furthermore, developing human supervision mechanisms to identify and correct incorrect requirements can also be an effective bias mitigation strategy [1,3,26].</p>
<p>It is also important to consider bias in GenAI systems so that the requirements and specifications generated by tools such as ChatGPT are based on an ethical and professional context.</p>
<p>Information Hallucination</p>
<p>ChatGPT can occasionally produce incorrect results, known as "hallucinations", which can lead to the generation of false-positive results [16].Adopting human supervision mechanisms in the implementation of requirements generated by ChatGPT can help mitigate these hallucinations of the model and ensure a correct interpretation of the generated output [3].</p>
<p>When ChatGPT generates inaccurate and purely fictional information or hallucinations, these conditions occur when the model lacks sufficient knowledge or information and must guess and attempt to fill in the missing piece using training examples as reference points.Incorrect outputs can manifest as hallucinations, causing problems in applications where accuracy is critical [5].</p>
<p>Research is currently focused on understanding what makes hallucinations possible in language models.Recent studies have shown that this phenomenon may be multifaceted and related to factors such as model training techniques, data quality, and architectural design.Language models may also be biased to produce more "interesting" or well-written output, increasing the likelihood of hallucinations.Several methods have been proposed to address this issue, one of which is to use a large dataset that can help reduce the number of incorrect assumptions made by the model.This method exposes the model to different contexts and information during training, reducing the risk of hallucination.</p>
<p>Understanding the phenomenon of information hallucination through ChatGPT, where fictitious data can be generated, helps to establish protocols for fact-checking, which is essential for the reliability of requirements specifications and, by extension, the robustness of the resulting software.</p>
<p>The problem of the lack of explainability and transparency is raised in the literature, where its development is attributed to the lack of verifiability of GenAI systems, which will be discussed next.</p>
<p>Lack of Explainability and Transparency</p>
<p>The results of LLMs are more difficult to understand because they often require additional explanation, making it even more difficult to penetrate their inner meaning and trace their genesis and fidelity.</p>
<p>Explainability is one of the main limitations of using LLMs due to their immense number of parameters.For example, systems such as GPT-3 have 175 billion parameters, creating a highly complex network of interconnected nodes required for their operation.This complexity poses a significant problem for humans in understanding and interpreting the decision-making mechanisms implemented by the machine.In addition, training large language models requires collecting large amounts of data from multiple sources.</p>
<p>The patterns and correlations learned from the data lead to implicit biases and associations that may not be immediately obvious or interpretable.Accordingly, when a large language model makes a decision, it is difficult to isolate what caused that decision because it has many interrelated parts.This difficulty also explains why the results generated by the model cannot be articulated easily.</p>
<p>The main reason for the lack of explicability is that it may hinder the trust and full adoption of large language models (LLMs) in security applications.The inability to understand and decipher the decision-making mechanisms of LLMs may raise doubts about their trustworthiness, fairness, and potential biases.As a result, stakeholders such as policymakers, businesses, and users may require a longer period of time before they can rely on LLMs for essential contexts, prioritizing transparency and accountability [8].</p>
<p>ChatGPT works by using large neural networks that have been trained on large amounts of data and refined to perform specific natural language processing (NLP) tasks.Although the software requirements generated by ChatGPT can be similar in quality to those generated by humans, it can be difficult for users to understand the mechanisms that led ChatGPT to produce the result [3].</p>
<p>The need for explainability and transparency in LLM models such as ChatGPT is driving research to make GenAI more interpretable and reliable, which is essential for its adoption in critical software specification and other software engineering processes.</p>
<p>Susceptibility to Attacks</p>
<p>Requirements are fundamental for software engineering as they describe the functionalities, characteristics, and constraints of the system to be developed.They are essential to guiding the entire development process, from conception to delivery of the final product.Furthermore, requirements often contain sensitive information [15].The disclosure of this sensitive information in software requirements can be explored with malicious intent through attacks on the system, raising security and privacy concerns.Therefore, it is critical to implement robust security measures to protect requirements and ensure the confidentiality and integrity of sensitive information [32].</p>
<p>Three categories of adversarial attacks could subject ChatGPT to misleading prompt injection, jailbreak attacks aimed at stealing sensitive information, and data poisoning methods, which would alter ChatGPT's output.</p>
<p>The main goal of such adversarial attacks is to disrupt or take control of the output of a large language model.Such attacks involve deliberately altering the input text at a small level so that LLM misinterprets it and returns inaccurate or harmful results.A typical example of an adversarial attack is a text injection attack.The perpetrator attacks by inserting well-crafted instructions into the input, which the LLM interprets as commands.For example, such injected text could be "delete all files" fed into an LLM controlling a computer system.As a result, the LLM could treat this input as a command and delete everything from the system according to the injected text [32].</p>
<p>The detection and mitigation of adversarial attacks on ChatGPT highlight the need to strengthen the security of GenAI systems, which is fundamental to ensuring the integrity, confidentiality, and cohesion of the requirements generated.</p>
<p>Reasoning Errors</p>
<p>According to [33], even large language models (LLMs) such as ChatGPT are sometimes misled by ambiguities in the question or poor understanding of complex logical operations.They need to acquire planning and reasoning skills, and also have limited awareness and common sense about the world.</p>
<p>When the generation of ambiguous requirements occurs, human intervention is necessary to avoid misinterpretations of these requirements.Ambiguous requirements can lead to a scenario of uncertainty in the decision-making process, which is undesirable for developers [3].Human intervention can help clarify ambiguities, identify gaps, and ensure that requirements are clearly defined and understood by all parties involved in the software development process [15].</p>
<p>The detection of reasoning errors by LLMs is driving efforts to improve GenAI's reasoning and logic capabilities, which are essential for the accurate generation of software requirements.</p>
<p>Over-Reliance</p>
<p>Replacing human expertise with GenAI tools can overlook critical aspects of requirements gathering, such as understanding contextual nuances and implicit needs [15].Despite all the different challenges that GenAI faces in its integration into software engineering requirements, the challenge of trust remains prominent, as highlighted in the research [29].</p>
<p>One of the main pitfalls of using ChatGPT for software requirements is the temptation to rely too much on the generated text without sufficient verification.This over-reliance on AI can lead to a lack of trust among stakeholders [15].While ChatGPT can produce coherent and seemingly relevant responses, it requires more contextual understanding and may need to provide more accurate or complete information.To mitigate this lack of contextual knowledge, users should provide clear and detailed input prompts, offer contextual information where and when needed, and refine requirements based on ongoing dialogue and clarification.Therefore, developers must exercise caution and critically evaluate the suitability and accuracy of the generated content in the context of specific software requirements [31].</p>
<p>Identifying the risk of over-reliance on ChatGPT highlights the need to combine artificial intelligence with human expertise, promoting a balance that can lead to more sophisticated software development practices that are responsive to human needs.</p>
<p>Transparency and Accountability</p>
<p>Establishing transparency in the use of GenAI technologies, including accountability for the results produced and the steps taken to avoid bias, is essential.The ChatGPT algorithm operates as a black box model, providing no visibility or explanation to end users.</p>
<p>The lack of transparency can become an obstacle to responsible oversight, making it difficult to detect and correct problems such as bias, errors, or adverse outcomes in output requirements.Mechanisms such as transparency and accountability are essential for stakeholders to trust and understand, rather than doubt or resist, the decisions made by ChatGPT [30].</p>
<p>It is important to maintain human responsibility for the results generated.When the potential consequences of errors resulting from incorrect use of LLMs are significant, human oversight is critical.While the results generated by automation are not completely reliable, human analysis and review of these results not only ensures their quality, but also incorporates human responsibility, adding a crucial layer of accountability and security [16].</p>
<p>Establishing transparency and accountability in the use of GenAI technologies such as ChatGPT reinforces the importance of ethics in software engineering.It also promotes technological development that respects human rights and values and considers social and ethical aspects for each application context.</p>
<p>Summary of the Limitations and Risks of Using ChatGPT</p>
<p>By addressing these challenges and risks, the scientific community and professionals in the field can develop more robust and ethically responsible practices for integrating ChatGPT and other GenAI tools into software requirements engineering.This will improve the development process and contribute to the evolution of knowledge and practices in software engineering [30].</p>
<p>Future Research Directions</p>
<p>Given the transformative potential and limitations identified in the use of ChatGPT in software requirements engineering, we propose future research focused on the following critical areas:</p>
<p>•</p>
<p>Prompt generation: New prompt construction techniques suitable for each stage of the software development lifecycle need to be explored.This will result in clearer and more comprehensive requirements.Experimentation with different prompt construction strategies can improve the quality of the generated requirements.</p>
<p>•</p>
<p>Mitigate bias and increase equity: Investigate methods for identifying, measuring, and mitigating bias in ChatGPT-generated software requirements engineering.This includes developing techniques for training on more balanced datasets and exploring the implementation of fairness-aware algorithms for software requirements engineering.</p>
<p>•</p>
<p>Suggest ways of clarifying the decision-making processes to improve clarity and transparency: Possible ways include more interpretable models or the development of tools to provide insight into model reasoning.</p>
<p>•</p>
<p>Reliable data handling and processing protocols that address data privacy and security concerns: Research could explore encryption methods, differential privacy techniques, and secure mechanisms to ensure the secure storage of sensitive information.</p>
<p>•</p>
<p>Improve ChatGPT's error detection and correction capabilities, focusing more on logical and factual inaccuracies: This could involve using external knowledge bases, improved model training techniques, or a human-in-the-loop verification system.</p>
<p>•</p>
<p>Strategies to minimize overreliance and maintain a well-orchestrated interaction between GenAI and human intellect in problem-solving: We suggest defining and setting standards for the use of ChatGPT in software requirements engineering, how GenAI can assist in performing specific tasks, and how human oversight can be emphasized.</p>
<p>•</p>
<p>Resistance to adversarial attacks: It is necessary to study ways to improve Chat-GPT's ability to resist counterattacks and malicious manipulation in order to promote reliability and consistency in terms of system requirements.</p>
<p>•</p>
<p>Ethical and social issues: The use of ChatGPT may raise ethical issues regarding privacy breaches and redundancy.We recommend in-depth research into the two main issues causing unemployment and increasing economic inequality, and promoting equal access to GenAI tools.</p>
<p>Conclusions</p>
<p>This paper has evaluated the incorporation of ChatGPT into software requirements engineering, identifying promising benefits and significant challenges.We also explored the usefulness of ChatGPT as a facilitator of iterative development and its use in capturing changing project requirements.</p>
<p>ChatGPT's ability to enhance brainstorming, reduce human error, save costs, and improve efficiency and accuracy highlights its potential to transform software requirements engineering practices.However, critical issues such as bias, information hallucination, lack of explainability, vulnerability to attacks, reasoning errors, overreliance, and ethical considerations underscore the need for cautious and informed implementation.</p>
<p>We propose future research to address these challenges in order to responsibly realize ChatGPT's full potential.By focusing on mitigating bias, improving explainability, ensuring data privacy, correcting errors, and understanding ethical implications, we can move toward a more effective and ethical integration of ChatGPTin software development.</p>
<p>In summary, as we move forward, it is critical to adopt a collaborative approach that leverages ChatGPT capabilities and human expertise to improve the quality and reliability of software requirements.</p>
<p>Figure 1 .
1
Figure 1.The methodology used in the literature research.</p>
<p>Figure 2 .
2
Figure 2. Evolution of research about AI impact on software requirements.</p>
<p>Figure 2 .
2
Figure 2. Evolution of research about AI impact on software requirements.</p>
<p>Table 1 .
1
Comparative analysis of ChatGPT's impact on software requirements.
Study Reference (Author)Publication YearEffectiveness in Generating RequirementsRole of Human InputExploration of Prompt EngineeringAI Challenges and LimitationsFuture Research DirectionsComparative Human ExpertiseLiu et al. [2]2022✔✔Abdelfattah et al. [14]2023✔✔Arora et al. [15]2023✔✔Belzner et al. [16]2023✔✔Bencheik et al. [1]2023✔✔✔Cheng et al. [17]2023✔✔El-Hajjami et al. [18]2023✔Kutzner et al. [19]2023✔✔Qian et al. [20]2023✔✔✔Ronanki et al. [3]2023✔✔✔Rasheed et al. [21]2023✔✔✔Sridhara et al. [12]2023✔Subahi [22]2023✔Wang et al. [23]2023✔✔✔White et al. [6,7]2023✔✔Zhang et al. [24]2023✔✔✔</p>
<p>Table 1 .
1
Cont.
Study Reference (Author)Publication YearEffectiveness in Generating RequirementsRole of Human InputExploration of Prompt EngineeringAI Challenges and LimitationsFuture Research DirectionsComparative Human ExpertiseFantechi et al. [25]2024✔✔✔Luitel et al. [26]2024✔✔Oswal et al. [27]2024✔✔✔Waseem et al. [28]2024✔✔✔Yeow et al. [29]2024✔✔✔</p>
<p>Table 2 .
2
Potential benefits identified in the literature review.
Benefits
[29] Liu et al.[2], Rasheed et al.[21], Yeow et al.[29]</p>
<p>Table 3 .
3
Limitations and risks identified in the literature review.</p>
<p>Data Availability Statement:The data presented in this study are available on request from the corresponding author.Conflicts of Interest:The authors declare no conflict of interest.
Exploring the Efficacy of Chatgpt in Generating Requirements: An Experimental Study. L Bencheikh, N Höglund, 2023Göteborg, SwedenChalmers University of TechnologyBachelor's Thesis</p>
<p>Artificial Intelligence in Software Requirements Engineering: State-of-the-Art. K Liu, K Reddivari, Proceedings of the IEEE 23rd International Conference on Information Reuse and Integration for Data Science (IRI). the IEEE 23rd International Conference on Information Reuse and Integration for Data Science (IRI)San Diego, CA, USAAugust 2022</p>
<p>Investigating ChatGPT's Potential to Assist in Requirements Elicitation Processes. K Ronanki, C Berger, J Horkoff, Proceedings of the 49th Euromicro Conference on Software Engineering and Advanced Applications (SEAA). the 49th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)Durres, AlbaniaSeptember 2023</p>
<p>A Survey on Evaluation of Large Language Models. Y Chang, X Wang, J Wang, Y Wu, L Yang, K Zhu, H Chen, X Yi, C Wang, Y Wang, 10.1145/3641289ACM Trans. Intell. Syst. Technol. 152023</p>
<p>Large language models for software engineering: Survey and open problems. A Fan, B Gokkaya, M Harman, M Lyubarskiy, S Sengupta, S Yoo, J M Zhang, arXiv:2310.035332023</p>
<p>J White, S Hays, Q Fu, J Spencer-Smith, D C Schmidt, arXiv:2303.07839ChatGPT Prompt Patterns for Improving Code Quality, Refactoring, Requirements Elicitation, and Software Design. 2023</p>
<p>A prompt pattern catalog to enhance prompt engineering with chatgpt. J White, Q Fu, S Hays, M Sandborn, C Olea, H Gilbert, A Elnashar, J Spencer-Smith, D C Schmidt, arXiv:2302.113822023</p>
<p>. A Nguyen-Duc, B Cabrero-Daniel, A Przybylek, C Arora, D Khanna, T Herda, U Rafiq, J Melegati, E Guerra, K K Kemell, arXiv:2310.18648Generative Artificial Intelligence for Software Engineering-A Research Agenda. arXiv. 2023</p>
<p>Software Requirements Engineering: What, Why, Who, When, and How. L Westfall, Softw. Qual. Prof. 2005, 7, 17</p>
<p>How We Got to Where We Are Today. B Marr, Short History Of Chatgpt, 1 March 2024</p>
<p>ChatGPT as a Software Development Tool: The Future of Development. A Hörnemalm, 2023. 1 March 2024Umeå, SwedenUmeå University, Faculty of Science and TechnologyMaster's Thesis</p>
<p>G Sridhara, S Mazumdar, Chatgpt, arXiv:2305.16837A Study on its Utility for Ubiquitous Software Engineering Tasks. arXiv 2023. </p>
<p>Suitability of Google Scholar as a source of scientific information and as a source of data for scientific evaluation-Review of the Literature. G Halevi, H Moed, J Bar-Ilan, 10.1016/j.joi.2017.06.005J. Informetr. 112017</p>
<p>Roadmap for Software Engineering Education using ChatGPT. A M Abdelfattah, N A Ali, M A Elaziz, Proceedings of the 2023 International Conference on Artificial Intelligence Science and Applications in Industry and Society (CAISAIS), IEEE, Galala. the 2023 International Conference on Artificial Intelligence Science and Applications in Industry and Society (CAISAIS), IEEE, GalalaEgyptSeptember 2023</p>
<p>C Arora, J Grundy, M Abdelrazek, arXiv:2310.13976Advancing Requirements Engineering through Generative AI: Assessing the Role of LLMs. arXiv 2023. </p>
<p>Large Language Model Assisted Software Engineering: Prospects, Challenges, and a Case Study. L Belzner, T Gabor, M Wirsing, Proceedings of the International Conference on Bridging the Gap between AI and Reality. the International Conference on Bridging the Gap between AI and RealityCrete, GreeceOctober 2023</p>
<p>Prompt sapper: A LLM-empowered production tool for building AI chains. Y Cheng, J Chen, Q Huang, Z Xing, X Xu, Q Lu, 10.1145/3638247ACM Trans. Softw. Eng. Methodol. 2023</p>
<p>A El-Hajjami, N Fafin, C Salinesi, arXiv:2311.11547Which AI Technique Is Better to Classify Requirements? An Experiment with SVM, LSTM, and ChatGPT. arXiv 2023. </p>
<p>Supporting Students in the Creation of Requirements and Functional Specifications in Interdisciplinary Software Development Projects with the Help of AI-Based Text Generation Tools. T Kutzner, J Gröpler, Jornadas Iberoamericanas de Innovación Educativa en el Ámbito de las TIC y las TAC. 2023. Available online. 19 April 2024</p>
<p>C Qian, X Cong, W Liu, C Yang, W Chen, Y Su, Y Dang, J Li, J Xu, D Li, arXiv:2307.07924Communicative agents for software development. 2023</p>
<p>Z Rasheed, M Waseem, K.-K Kemell, W Xiaofeng, A N Duc, K Systä, P Abrahamsson, arXiv:2311.18440Autonomous Agents in Software Development: A Vision Paper. arXiv 2023. </p>
<p>BERT-Based Approach for Greening Software Requirements Engineering Through Non-Functional Requirements. A F Subahi, 10.1109/ACCESS.2023.3317798IEEE Access. 112023</p>
<p>Z Wang, J Li, G Li, Z Jin, Chatcoder, arXiv:2311.00272Chat-based Refine Requirement Improves LLMs' Code Generation. arXiv 2023. </p>
<p>A preliminary evaluation of chatgpt in requirements information retrieval. J Zhang, Y Chen, N Niu, C Liu, arXiv:2304.125622023</p>
<p>Exploring LLMs' Ability to Detect Variability in Requirements. A Fantechi, S Gnesi, L Semini, International Working Conference on Requirements Engineering: Foundation for Software Quality. Cham, SwitzerlandSpringer Nature2024</p>
<p>Improving requirements completeness: Automated assistance through large language models. D Luitel, S Hassani, M Sabetzadeh, 10.1007/s00766-024-00416-3Requir. Eng. 292024</p>
<p>Transforming Software Requirements into User Stories with GPT-3.5-: An AI-Powered Approach. J U Oswal, H T Kanakia, D Suktel, Proceedings of the 2024 2nd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT), IEEE, Bengaluru, India. the 2024 2nd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT), IEEE, Bengaluru, IndiaJanuary 2024</p>
<p>M Waseem, T Das, A Ahmad, P Liang, M Fahmideh, T Mikkonen, arXiv:2310.13648ChatGPT as a Software Development Bot: A Project-based Study. arXiv 2024. </p>
<p>An Automated Model of Software Requirement Engineering Using GPT-3.5. J S Yeow, M E Rana, N A A Majid, Proceedings of the 2024 ASU International Conference in Emerging Technologies for Sustainability and Intelligent Systems (ICETSIS). the 2024 ASU International Conference in Emerging Technologies for Sustainability and Intelligent Systems (ICETSIS)Al Ekir, Kingdom of BahrainJanuary 2024</p>
<p>M Fraiwan, N Khasawneh, arXiv:2305.00237A Review of ChatGPT Applications in Education, Marketing, Software Engineering, and Healthcare: Benefits, Drawbacks, and Research Directions. arXiv 2023. </p>
<p>Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications. W Hariri, arXiv:2304.02017Advantages, Limitations, and Future Directions in Natural Language Processing. 2023</p>
<p>Disadvantages and Risks associated with ChatGPT and AI on Cybersecurity. D Kalla, S Kuraku, Advantages, J. Emerg. Technol. Innov. Res. 102023</p>
<p>. Q Zhang, T Zhang, J Zhai, C Fang, B Yu, W Sun, Chen, arXiv:2310.08879Z. A Critical Review of Large Language Model on Software Engineering: An Example from ChatGPT and Automated Program Repair. arXiv. 2023</p>
<p>Disclaimer/Publisher's Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods. instructions or products referred to in the content</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8507 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8507</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8507</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-151.html">extraction-schema-151</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <p><strong>Paper ID:</strong> paper-b9b220b485d2add79118ffdc2aaa148b67fa53ef</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b9b220b485d2add79118ffdc2aaa148b67fa53ef" target="_blank">Pre-Trained Language Models for Interactive Decision-Making</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work proposes an approach for using LMs to scaffold learning and generalization in general sequential decision-making problems, and shows that this framework enables effective combinatorial generalization across different environments and supervisory modalities.</p>
                <p><strong>Paper Abstract:</strong> Language model (LM) pre-training is useful in many language processing tasks. But can pre-trained LMs be further leveraged for more general machine learning problems? We propose an approach for using LMs to scaffold learning and generalization in general sequential decision-making problems. In this approach, goals and observations are represented as a sequence of embeddings, and a policy network initialized with a pre-trained LM predicts the next action. We demonstrate that this framework enables effective combinatorial generalization across different environments and supervisory modalities. We begin by assuming access to a set of expert demonstrations, and show that initializing policies with LMs and fine-tuning them via behavior cloning improves task completion rates by 43.6% in the VirtualHome environment. Next, we integrate an active data gathering procedure in which agents iteratively interact with the environment, relabel past"failed"experiences with new goals, and update their policies in a self-supervised loop. Active data gathering further improves combinatorial generalization, outperforming the best baseline by 25.1%. Finally, we explain these results by investigating three possible factors underlying the effectiveness of the LM-based policy. We find that sequential input representations (vs. fixed-dimensional feature vectors) and LM-based weight initialization are both important for generalization. Surprisingly, however, the format of the policy inputs encoding (e.g. as a natural language string vs. an arbitrary sequential encoding) has little influence. Together, these results suggest that language modeling induces representations that are useful for modeling not just language, but also goals and plans; these representations can aid learning and generalization even outside of language processing.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8507.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8507.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LID-Text</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pre-Trained Language Models for Interactive Decision-Making (Text encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LM-initialized policy that encodes goal, observation, and action history as a sequence of text tokens, feeds them to a pretrained GPT-2 transformer, pools token representations to form a context vector, and predicts actions via a decoder; used for imitation learning on VirtualHome and BabyAI.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LID-Text</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A decision-making agent that converts all inputs (goals, partial observations, history) into natural language token sequences, processes them with a pretrained GPT-2 transformer (fine-tuned), and predicts the next action; history (past observations and actions) is included as part of the token sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Autoregressive transformer language model (GPT-2) pretrained on WebText; used as the encoder/backbone and fine-tuned for action prediction in the policy network.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>VirtualHome and BabyAI</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>VirtualHome: a 3D embodied environment with partial observability, large action spaces and long horizons, tasks specified as predicate-goals; BabyAI: a 2D gridworld instruction-following environment with language instructions and partial observations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic / working history (sequence of past observations and actions)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>History is serialized as token sequences (templated English sentences for observations and past actions). These tokens are embedded (using GPT-2 embedding layer) and concatenated with goal and current observation tokens into a single sequence processed by the transformer; contextualized token representations implicitly encode memory of past timesteps. Token outputs are pooled (averaged) to produce a context vector for action decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Concatenate goal, history (previous o_t and a_t), and current observation tokens into a single sequence fed to the transformer; use pooled contextual outputs as input to action predictor (i.e., memory is integrated via the transformer's attention over the concatenated sequence).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Novel Tasks success rate: up to 58.2% ± 1.6 (20K demos) on VirtualHome Novel Tasks; reported to improve combinatorial generalization by 43.6% over baselines in Novel Tasks (text reports a dramatic 43.6% improvement). On BabyAI LID-Text reaches near perfect performance given sufficient demos (e.g., 99.7–100% on simpler tasks with 1K–10K demos as reported in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Ablations compare input encodings (text vs. index vs. permuted/unnatural text) and the presence/absence of sequential representation (No-Seq). Comparisons show: text encoding is most sample-efficient but arbitrary sequential encodings converge to similar performance with enough data; removing sequential structure (No-Seq) significantly degrades generalization to Novel Tasks; removing pretraining (No-Pretrain) reduces Novel Tasks success by ~11.2 percentage points; freezing pretrained weights (No-FT) also hurts performance.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Use a sequential, tokenized history concatenated to goals and observations and processed by an LM that is initialized from pretrained weights and fine-tuned; sequential history (transformer processing) + pretrained weights + fine-tuning yields the best generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Failures include grounding errors (interacting with wrong objects) and policy errors (failing to find or interact with targets). No explicit external memory module is used — reliance on sequence length and pooling may limit very long-horizon or extremely long histories. The paper also notes that natural language encoding is more sample-efficient but not strictly required.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Include the full sequential history as tokens (i.e., use transformer attention over concatenated goal/history/observation) and initialize from pretrained LM weights then fine-tune; natural language encoding helps sample efficiency but arbitrary sequential encodings can work with more data. Fine-tuning pretrained weights (rather than freezing) is important for combinatorial generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-Trained Language Models for Interactive Decision-Making', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8507.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8507.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LID-ADG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LID with Active Data Gathering (LID-ADG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LM-based policy (same LID architecture) trained without expert demonstrations by alternating exploration, hindsight relabeling of failed trajectories, storing relabeled data in replay buffers, and supervised policy updates; replay buffer + relabeling serve as the memory mechanism for training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LID-ADG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LM-initialized policy that learns from actively gathered data: it mixes policy actions and random actions to explore, extracts useful sub-trajectories from failed episodes via a hand-designed relabeling function, stores relabeled trajectories in replay buffers, and updates the LM-based policy via supervised learning on this relabeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>GPT-2 transformer used as the backbone policy (pretrained embeddings and transformer weights, then fine-tuned during training).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>VirtualHome (active-data regime)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>VirtualHome tasks evaluated under the constraint that no expert data are provided and the agent must gather data via exploration; tasks have partial observability, large action space, sparse rewards, and long horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>training replay buffer (experience memory) with hindsight relabeling (episodic relabeled examples stored offline)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Collected trajectories (o_1,a_1,...,o_T,a_T) are stored in replay buffers. A hand-designed goal relabeling function extracts sub-trajectories and assigns new goals; relabeled examples are appended to the goal set and replay buffers and sampled for supervised policy updates.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Memory is used offline for training: exploration collects trajectories, relabeled sub-trajectories stored in replay buffers are sampled for supervised updates of the LM policy. At inference time, the agent still uses sequential history tokens (same as LID) for online memory.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>On VirtualHome without expert data: In-Distribution 46.7% ± 2.7, Novel Scenes 32.2% ± 3.3, Novel Tasks 25.5% ± 4.1 (Table 2). LID-ADG is the only successful approach among baselines that do not use expert data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baselines without similar active relabeling/replay approach perform at or near 0%: Random 0.0%, PPO 0.0%, DQN+HER 0.0% on the evaluated tasks in the paper (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared LID-ADG to Random, Goal-Object heuristic, PPO (online RL with same policy network), and DQN+HER; only LID-ADG achieved nonzero/meaningful performance. Also used LID-ADG-trained policy as initializer for PPO (PPO (LID-ADG Init)) and as a dataset provider for offline DT: initialization or data from LID-ADG significantly improved downstream RL/DT performance.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Hindsight relabeling of sub-trajectories combined with replay buffers (i.e., relabel-and-store then supervised updates) is the effective strategy for learning without expert data in complex embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Hindsight relabeling relies on hand-designed rules/programs to detect and extract useful sub-goals from failed trajectories; these rules may not scale to very long or complex trajectories and limit the relabeling scope to relatively short sub-tasks (paper notes this as a drawback).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>For environments with sparse rewards and long horizons, use active data gathering with replay buffers and hindsight relabeling to turn failed trajectories into training data; use the resulting dataset to supervise an LM-initialized policy and optionally initialize RL algorithms from it.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-Trained Language Models for Interactive Decision-Making', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8507.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8507.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>No-Seq (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>No-Sequential-Structure ablation (No-Seq)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation that removes the sequential structure by averaging token embeddings for goal/history/observation into single fixed-size vectors before feeding them to the model, thereby removing the transformer’s ability to attend across sequence positions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>No-Seq</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Variant of the LID policy that encodes each input (goal, history, observation) as a single vector (average-pooled embedding) rather than a token sequence, thereby eliminating sequential attention over history.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-2 (embedding layer used but sequence structure removed)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Same GPT-2 backbone but inputs converted to fixed-size averaged vectors rather than sequential tokens; transformer sequential processing is disabled/neutralized for those inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>VirtualHome (Novel Tasks ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Novel Tasks evaluation focusing on combinatorial generalization to task predicate combinations not seen during training.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>none (fixed-size aggregated representation instead of sequential history)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>History and goals are averaged into single vectors (pooling) which removes positional/temporal structure and the transformer's access to stepwise history via attention.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Aggregated vectors are fed into the network instead of concatenated token sequences; no sequential attention over history.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Text reports that removing sequential structure significantly hurts performance on Novel Tasks (No-Seq performs well on near-distribution tests but generalizes poorly to Novel Tasks); specific numeric results are reported in the paper's Table 5 (not reproduced fully in the main text).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Direct ablation comparing full sequential LID to No-Seq shows sequential representation is important for combinatorial generalization; other ablations (input encoding scheme, pretraining) were also run.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>The results indicate that maintaining sequential history tokens (so transformer attention can operate over them) is preferable to collapsing history into a fixed vector.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Aggregating history into a single vector diminishes the model's ability to generalize to novel combinatorial tasks and loses information needed for long-horizon compositional planning.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Keep sequential representations of history (do not average away temporal structure); transformers' sequential attention over tokenized history is a key component to generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-Trained Language Models for Interactive Decision-Making', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8507.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8507.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>No-Pretrain / No-FT (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablations on Pretraining: No-Pretrain and No-FineTune</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two ablations: No-Pretrain trains the policy from random initialization (no LM pretraining); No-FT keeps pretrained LM weights but freezes them during training (no fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>No-Pretrain / No-FT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>No-Pretrain: identical architecture but initialized randomly and trained from scratch; No-FT: uses pretrained GPT-2 weights but freezes them and only trains remaining components.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-2 (not used as pretrained or frozen depending on ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Same architecture as LID but either uninitialized (No-Pretrain) or frozen (No-FT) to test the role of pretraining and fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>VirtualHome (Novel Tasks and In-Distribution)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Evaluations comparing in-distribution performance and combinatorial generalization to Novel Tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>sequence-based history (same as LID) but without LM pretraining or without fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Same sequential token history architecture, but weights are either randomly initialized (No-Pretrain) or frozen pretrained (No-FT).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Same concatenation and transformer processing of sequential history; difference is weight initialization/freeze.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>No-Pretrain: success rate on Novel Tasks is ~11.2 percentage points lower than LID (text reports an 11.2% drop in Novel Tasks when removing pretrained weights). No-FT (freezing) significantly hurts performance on both in-distribution and novel tasks (exact numbers in paper's Table 5/7).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Direct comparison shows pretrained weight initialization plus fine-tuning yields better combinatorial generalization than training from scratch or freezing pretrained weights; pretraining gives an inductive bias useful for sequence processing even for non-linguistic sequential encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Combine sequential history encoding with pretrained LM weights and fine-tune them; pretraining + fine-tuning outperforms random init or frozen weights.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Training from scratch can still fit in-distribution data but generalizes worse; freezing pretrained weights prevents adapting LM to the new decision-making signals and degrades performance.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Keep pretrained LM initialization and allow fine-tuning for best generalization; pretraining confers useful inductive bias for sequence processing relevant to memory usage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-Trained Language Models for Interactive Decision-Making', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8507.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8507.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hindsight relabeling + replay buffer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hindsight Goal Relabeling and Replay Buffer (LID-ADG memory mechanism)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hand-designed relabeling program that inspects failed trajectories, extracts useful sub-trajectories, assigns new achieved goals, and stores these relabeled examples in a replay buffer for supervised policy updates; a key memory mechanism enabling learning without expert data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Hindsight Relabeling + Replay Buffer (LID-ADG)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A memory mechanism used during data collection and training: trajectories (including failures) are retained in replay buffers; a program detects achievable sub-goals in trajectories, relabels those sub-trajectories with the achieved goal, and these relabeled examples are used to train the LM-policy.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>N/A (mechanism used with LID / GPT-2 policy)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>VirtualHome (active-data regime)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Used to convert failed long-horizon embodied trajectories into supervised training examples (short sub-goals) in VirtualHome.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>experience replay (episodic storage of trajectories) with hindsight relabeling</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Replay buffers store raw trajectories; a hand-designed relabel function f_l inspects action sequences and keywords to identify sub-tasks and assigns corresponding goal labels; relabeled items are added to the goal set and stored for training.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Offline replay: relabeled trajectory examples are sampled from the replay buffer during the policy update stage for supervised fine-tuning of the LM policy. The replay buffer grows with relabeled goals and initial states returned to the sampling pool.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Enables LID-ADG to achieve nonzero success on VirtualHome without expert demos: In-Distribution 46.7% ± 2.7, Novel Scenes 32.2% ± 3.3, Novel Tasks 25.5% ± 4.1 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Without relabeling/replay (i.e., naive exploration + supervised update), baselines yield near-zero success (Random, PPO, DQN+HER reported 0.0% in Table 2 for these tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared to standard RL (PPO) and DQN+HER, the relabel+replay approach is the only one that attains meaningful learning signal in VirtualHome from scratch; also shown to be useful as data provider for offline DT and as initializer for PPO.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Hindsight relabeling that extracts short, high-quality sub-trajectories and storing them in replay buffers for supervised learning is the effective strategy in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Relabeling depends on hand-crafted detection rules/programs and quality-filtering heuristics (e.g., keep 'shorter' or less-redundant sub-trajectories); scaling relabeling rules to complex/long tasks is challenging and is an explicit limitation called out by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Use hindsight relabeling + replay buffer to turn failed exploration into supervised training data when expert demonstrations are unavailable, but improve/automate relabeling procedures for broader applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-Trained Language Models for Interactive Decision-Making', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Hindsight experience replay <em>(Rating: 2)</em></li>
                <li>Decision transformer: Reinforcement learning via sequence modeling <em>(Rating: 2)</em></li>
                <li>Pretrained transformers as universal computation engines <em>(Rating: 1)</em></li>
                <li>Semi-Supervised Skill Learning with Latent Language <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8507",
    "paper_id": "paper-b9b220b485d2add79118ffdc2aaa148b67fa53ef",
    "extraction_schema_id": "extraction-schema-151",
    "extracted_data": [
        {
            "name_short": "LID-Text",
            "name_full": "Pre-Trained Language Models for Interactive Decision-Making (Text encoding)",
            "brief_description": "An LM-initialized policy that encodes goal, observation, and action history as a sequence of text tokens, feeds them to a pretrained GPT-2 transformer, pools token representations to form a context vector, and predicts actions via a decoder; used for imitation learning on VirtualHome and BabyAI.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LID-Text",
            "agent_description": "A decision-making agent that converts all inputs (goals, partial observations, history) into natural language token sequences, processes them with a pretrained GPT-2 transformer (fine-tuned), and predicts the next action; history (past observations and actions) is included as part of the token sequence.",
            "llm_model_name": "GPT-2",
            "llm_model_description": "Autoregressive transformer language model (GPT-2) pretrained on WebText; used as the encoder/backbone and fine-tuned for action prediction in the policy network.",
            "benchmark_name": "VirtualHome and BabyAI",
            "benchmark_description": "VirtualHome: a 3D embodied environment with partial observability, large action spaces and long horizons, tasks specified as predicate-goals; BabyAI: a 2D gridworld instruction-following environment with language instructions and partial observations.",
            "memory_used": true,
            "memory_type": "episodic / working history (sequence of past observations and actions)",
            "memory_architecture": "History is serialized as token sequences (templated English sentences for observations and past actions). These tokens are embedded (using GPT-2 embedding layer) and concatenated with goal and current observation tokens into a single sequence processed by the transformer; contextualized token representations implicitly encode memory of past timesteps. Token outputs are pooled (averaged) to produce a context vector for action decoding.",
            "memory_integration_strategy": "Concatenate goal, history (previous o_t and a_t), and current observation tokens into a single sequence fed to the transformer; use pooled contextual outputs as input to action predictor (i.e., memory is integrated via the transformer's attention over the concatenated sequence).",
            "performance_with_memory": "Novel Tasks success rate: up to 58.2% ± 1.6 (20K demos) on VirtualHome Novel Tasks; reported to improve combinatorial generalization by 43.6% over baselines in Novel Tasks (text reports a dramatic 43.6% improvement). On BabyAI LID-Text reaches near perfect performance given sufficient demos (e.g., 99.7–100% on simpler tasks with 1K–10K demos as reported in Table 1).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Ablations compare input encodings (text vs. index vs. permuted/unnatural text) and the presence/absence of sequential representation (No-Seq). Comparisons show: text encoding is most sample-efficient but arbitrary sequential encodings converge to similar performance with enough data; removing sequential structure (No-Seq) significantly degrades generalization to Novel Tasks; removing pretraining (No-Pretrain) reduces Novel Tasks success by ~11.2 percentage points; freezing pretrained weights (No-FT) also hurts performance.",
            "best_memory_strategy": "Use a sequential, tokenized history concatenated to goals and observations and processed by an LM that is initialized from pretrained weights and fine-tuned; sequential history (transformer processing) + pretrained weights + fine-tuning yields the best generalization.",
            "limitations_or_failure_cases": "Failures include grounding errors (interacting with wrong objects) and policy errors (failing to find or interact with targets). No explicit external memory module is used — reliance on sequence length and pooling may limit very long-horizon or extremely long histories. The paper also notes that natural language encoding is more sample-efficient but not strictly required.",
            "recommendations_or_conclusions": "Include the full sequential history as tokens (i.e., use transformer attention over concatenated goal/history/observation) and initialize from pretrained LM weights then fine-tune; natural language encoding helps sample efficiency but arbitrary sequential encodings can work with more data. Fine-tuning pretrained weights (rather than freezing) is important for combinatorial generalization.",
            "uuid": "e8507.0",
            "source_info": {
                "paper_title": "Pre-Trained Language Models for Interactive Decision-Making",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "LID-ADG",
            "name_full": "LID with Active Data Gathering (LID-ADG)",
            "brief_description": "An LM-based policy (same LID architecture) trained without expert demonstrations by alternating exploration, hindsight relabeling of failed trajectories, storing relabeled data in replay buffers, and supervised policy updates; replay buffer + relabeling serve as the memory mechanism for training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LID-ADG",
            "agent_description": "LM-initialized policy that learns from actively gathered data: it mixes policy actions and random actions to explore, extracts useful sub-trajectories from failed episodes via a hand-designed relabeling function, stores relabeled trajectories in replay buffers, and updates the LM-based policy via supervised learning on this relabeled data.",
            "llm_model_name": "GPT-2",
            "llm_model_description": "GPT-2 transformer used as the backbone policy (pretrained embeddings and transformer weights, then fine-tuned during training).",
            "benchmark_name": "VirtualHome (active-data regime)",
            "benchmark_description": "VirtualHome tasks evaluated under the constraint that no expert data are provided and the agent must gather data via exploration; tasks have partial observability, large action space, sparse rewards, and long horizons.",
            "memory_used": true,
            "memory_type": "training replay buffer (experience memory) with hindsight relabeling (episodic relabeled examples stored offline)",
            "memory_architecture": "Collected trajectories (o_1,a_1,...,o_T,a_T) are stored in replay buffers. A hand-designed goal relabeling function extracts sub-trajectories and assigns new goals; relabeled examples are appended to the goal set and replay buffers and sampled for supervised policy updates.",
            "memory_integration_strategy": "Memory is used offline for training: exploration collects trajectories, relabeled sub-trajectories stored in replay buffers are sampled for supervised updates of the LM policy. At inference time, the agent still uses sequential history tokens (same as LID) for online memory.",
            "performance_with_memory": "On VirtualHome without expert data: In-Distribution 46.7% ± 2.7, Novel Scenes 32.2% ± 3.3, Novel Tasks 25.5% ± 4.1 (Table 2). LID-ADG is the only successful approach among baselines that do not use expert data.",
            "performance_without_memory": "Baselines without similar active relabeling/replay approach perform at or near 0%: Random 0.0%, PPO 0.0%, DQN+HER 0.0% on the evaluated tasks in the paper (Table 2).",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Compared LID-ADG to Random, Goal-Object heuristic, PPO (online RL with same policy network), and DQN+HER; only LID-ADG achieved nonzero/meaningful performance. Also used LID-ADG-trained policy as initializer for PPO (PPO (LID-ADG Init)) and as a dataset provider for offline DT: initialization or data from LID-ADG significantly improved downstream RL/DT performance.",
            "best_memory_strategy": "Hindsight relabeling of sub-trajectories combined with replay buffers (i.e., relabel-and-store then supervised updates) is the effective strategy for learning without expert data in complex embodied tasks.",
            "limitations_or_failure_cases": "Hindsight relabeling relies on hand-designed rules/programs to detect and extract useful sub-goals from failed trajectories; these rules may not scale to very long or complex trajectories and limit the relabeling scope to relatively short sub-tasks (paper notes this as a drawback).",
            "recommendations_or_conclusions": "For environments with sparse rewards and long horizons, use active data gathering with replay buffers and hindsight relabeling to turn failed trajectories into training data; use the resulting dataset to supervise an LM-initialized policy and optionally initialize RL algorithms from it.",
            "uuid": "e8507.1",
            "source_info": {
                "paper_title": "Pre-Trained Language Models for Interactive Decision-Making",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "No-Seq (ablation)",
            "name_full": "No-Sequential-Structure ablation (No-Seq)",
            "brief_description": "An ablation that removes the sequential structure by averaging token embeddings for goal/history/observation into single fixed-size vectors before feeding them to the model, thereby removing the transformer’s ability to attend across sequence positions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "No-Seq",
            "agent_description": "Variant of the LID policy that encodes each input (goal, history, observation) as a single vector (average-pooled embedding) rather than a token sequence, thereby eliminating sequential attention over history.",
            "llm_model_name": "GPT-2 (embedding layer used but sequence structure removed)",
            "llm_model_description": "Same GPT-2 backbone but inputs converted to fixed-size averaged vectors rather than sequential tokens; transformer sequential processing is disabled/neutralized for those inputs.",
            "benchmark_name": "VirtualHome (Novel Tasks ablation)",
            "benchmark_description": "Novel Tasks evaluation focusing on combinatorial generalization to task predicate combinations not seen during training.",
            "memory_used": false,
            "memory_type": "none (fixed-size aggregated representation instead of sequential history)",
            "memory_architecture": "History and goals are averaged into single vectors (pooling) which removes positional/temporal structure and the transformer's access to stepwise history via attention.",
            "memory_integration_strategy": "Aggregated vectors are fed into the network instead of concatenated token sequences; no sequential attention over history.",
            "performance_with_memory": null,
            "performance_without_memory": "Text reports that removing sequential structure significantly hurts performance on Novel Tasks (No-Seq performs well on near-distribution tests but generalizes poorly to Novel Tasks); specific numeric results are reported in the paper's Table 5 (not reproduced fully in the main text).",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Direct ablation comparing full sequential LID to No-Seq shows sequential representation is important for combinatorial generalization; other ablations (input encoding scheme, pretraining) were also run.",
            "best_memory_strategy": "The results indicate that maintaining sequential history tokens (so transformer attention can operate over them) is preferable to collapsing history into a fixed vector.",
            "limitations_or_failure_cases": "Aggregating history into a single vector diminishes the model's ability to generalize to novel combinatorial tasks and loses information needed for long-horizon compositional planning.",
            "recommendations_or_conclusions": "Keep sequential representations of history (do not average away temporal structure); transformers' sequential attention over tokenized history is a key component to generalization.",
            "uuid": "e8507.2",
            "source_info": {
                "paper_title": "Pre-Trained Language Models for Interactive Decision-Making",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "No-Pretrain / No-FT (ablation)",
            "name_full": "Ablations on Pretraining: No-Pretrain and No-FineTune",
            "brief_description": "Two ablations: No-Pretrain trains the policy from random initialization (no LM pretraining); No-FT keeps pretrained LM weights but freezes them during training (no fine-tuning).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "No-Pretrain / No-FT",
            "agent_description": "No-Pretrain: identical architecture but initialized randomly and trained from scratch; No-FT: uses pretrained GPT-2 weights but freezes them and only trains remaining components.",
            "llm_model_name": "GPT-2 (not used as pretrained or frozen depending on ablation)",
            "llm_model_description": "Same architecture as LID but either uninitialized (No-Pretrain) or frozen (No-FT) to test the role of pretraining and fine-tuning.",
            "benchmark_name": "VirtualHome (Novel Tasks and In-Distribution)",
            "benchmark_description": "Evaluations comparing in-distribution performance and combinatorial generalization to Novel Tasks.",
            "memory_used": true,
            "memory_type": "sequence-based history (same as LID) but without LM pretraining or without fine-tuning",
            "memory_architecture": "Same sequential token history architecture, but weights are either randomly initialized (No-Pretrain) or frozen pretrained (No-FT).",
            "memory_integration_strategy": "Same concatenation and transformer processing of sequential history; difference is weight initialization/freeze.",
            "performance_with_memory": null,
            "performance_without_memory": "No-Pretrain: success rate on Novel Tasks is ~11.2 percentage points lower than LID (text reports an 11.2% drop in Novel Tasks when removing pretrained weights). No-FT (freezing) significantly hurts performance on both in-distribution and novel tasks (exact numbers in paper's Table 5/7).",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Direct comparison shows pretrained weight initialization plus fine-tuning yields better combinatorial generalization than training from scratch or freezing pretrained weights; pretraining gives an inductive bias useful for sequence processing even for non-linguistic sequential encodings.",
            "best_memory_strategy": "Combine sequential history encoding with pretrained LM weights and fine-tune them; pretraining + fine-tuning outperforms random init or frozen weights.",
            "limitations_or_failure_cases": "Training from scratch can still fit in-distribution data but generalizes worse; freezing pretrained weights prevents adapting LM to the new decision-making signals and degrades performance.",
            "recommendations_or_conclusions": "Keep pretrained LM initialization and allow fine-tuning for best generalization; pretraining confers useful inductive bias for sequence processing relevant to memory usage.",
            "uuid": "e8507.3",
            "source_info": {
                "paper_title": "Pre-Trained Language Models for Interactive Decision-Making",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Hindsight relabeling + replay buffer",
            "name_full": "Hindsight Goal Relabeling and Replay Buffer (LID-ADG memory mechanism)",
            "brief_description": "A hand-designed relabeling program that inspects failed trajectories, extracts useful sub-trajectories, assigns new achieved goals, and stores these relabeled examples in a replay buffer for supervised policy updates; a key memory mechanism enabling learning without expert data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Hindsight Relabeling + Replay Buffer (LID-ADG)",
            "agent_description": "A memory mechanism used during data collection and training: trajectories (including failures) are retained in replay buffers; a program detects achievable sub-goals in trajectories, relabels those sub-trajectories with the achieved goal, and these relabeled examples are used to train the LM-policy.",
            "llm_model_name": "N/A (mechanism used with LID / GPT-2 policy)",
            "llm_model_description": "N/A",
            "benchmark_name": "VirtualHome (active-data regime)",
            "benchmark_description": "Used to convert failed long-horizon embodied trajectories into supervised training examples (short sub-goals) in VirtualHome.",
            "memory_used": true,
            "memory_type": "experience replay (episodic storage of trajectories) with hindsight relabeling",
            "memory_architecture": "Replay buffers store raw trajectories; a hand-designed relabel function f_l inspects action sequences and keywords to identify sub-tasks and assigns corresponding goal labels; relabeled items are added to the goal set and stored for training.",
            "memory_integration_strategy": "Offline replay: relabeled trajectory examples are sampled from the replay buffer during the policy update stage for supervised fine-tuning of the LM policy. The replay buffer grows with relabeled goals and initial states returned to the sampling pool.",
            "performance_with_memory": "Enables LID-ADG to achieve nonzero success on VirtualHome without expert demos: In-Distribution 46.7% ± 2.7, Novel Scenes 32.2% ± 3.3, Novel Tasks 25.5% ± 4.1 (Table 2).",
            "performance_without_memory": "Without relabeling/replay (i.e., naive exploration + supervised update), baselines yield near-zero success (Random, PPO, DQN+HER reported 0.0% in Table 2 for these tasks).",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Compared to standard RL (PPO) and DQN+HER, the relabel+replay approach is the only one that attains meaningful learning signal in VirtualHome from scratch; also shown to be useful as data provider for offline DT and as initializer for PPO.",
            "best_memory_strategy": "Hindsight relabeling that extracts short, high-quality sub-trajectories and storing them in replay buffers for supervised learning is the effective strategy in these experiments.",
            "limitations_or_failure_cases": "Relabeling depends on hand-crafted detection rules/programs and quality-filtering heuristics (e.g., keep 'shorter' or less-redundant sub-trajectories); scaling relabeling rules to complex/long tasks is challenging and is an explicit limitation called out by the authors.",
            "recommendations_or_conclusions": "Use hindsight relabeling + replay buffer to turn failed exploration into supervised training data when expert demonstrations are unavailable, but improve/automate relabeling procedures for broader applicability.",
            "uuid": "e8507.4",
            "source_info": {
                "paper_title": "Pre-Trained Language Models for Interactive Decision-Making",
                "publication_date_yy_mm": "2022-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Hindsight experience replay",
            "rating": 2
        },
        {
            "paper_title": "Decision transformer: Reinforcement learning via sequence modeling",
            "rating": 2
        },
        {
            "paper_title": "Pretrained transformers as universal computation engines",
            "rating": 1
        },
        {
            "paper_title": "Semi-Supervised Skill Learning with Latent Language",
            "rating": 1
        }
    ],
    "cost": 0.01785925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Pre-Trained Language Models for Interactive Decision-Making</h1>
<p>Shuang Li ${ }^{1 *}$, Xavier Puig ${ }^{1}$, Chris Paxton ${ }^{2}$, Yilun Du ${ }^{1}$, Clinton Wang ${ }^{1}$, Linxi Fan ${ }^{2}$, Tao Chen ${ }^{1}$, De-An Huang ${ }^{2}$, Ekin Akyürek ${ }^{1}$, Anima Anandkumar ${ }^{2,3, \dagger}$, Jacob Andreas ${ }^{1, \dagger}$, Igor Mordatch ${ }^{4, \dagger}$, Antonio Torralba ${ }^{1, \dagger}$, Yuke Zhu ${ }^{2,5, \dagger}$<br>${ }^{1}$ MIT, ${ }^{2}$ Nvidia, ${ }^{3}$ Caltech, ${ }^{4}$ Google Brain, ${ }^{5}$ UT Austin<br>Junior authors are ordered based on contributions and senior authors ${ }^{\dagger}$ are ordered alphabetically.</p>
<h4>Abstract</h4>
<p>Language model (LM) pre-training is useful in many language processing tasks. But can pre-trained LMs be further leveraged for more general machine learning problems? We propose an approach for using LMs to scaffold learning and generalization in general sequential decision-making problems. In this approach, goals and observations are represented as a sequence of embeddings, and a policy network initialized with a pre-trained LM predicts the next action. We demonstrate that this framework enables effective combinatorial generalization across different environments and supervisory modalities. We begin by assuming access to a set of expert demonstrations, and show that initializing policies with LMs and fine-tuning them via behavior cloning improves task completion rates by $43.6 \%$ in the VirtualHome environment. Next, we integrate an active data gathering procedure in which agents iteratively interact with the environment, relabel past "failed" experiences with new goals, and update their policies in a self-supervised loop. Active data gathering further improves combinatorial generalization, outperforming the best baseline by $25.1 \%$. Finally, we explain these results by investigating three possible factors underlying the effectiveness of the LM-based policy. We find that sequential input representations (vs. fixed-dimensional feature vectors) and LM-based weight initialization are both important for generalization. Surprisingly, however, the format of the policy inputs encoding (e.g. as a natural language string vs. an arbitrary sequential encoding) has little influence. Together, these results suggest that language modeling induces representations that are useful for modeling not just language, but also goals and plans; these representations can aid learning and generalization even outside of language processing. ${ }^{2}$</p>
<h2>1 Introduction</h2>
<p>Language models (LMs) play a key role in machine learning approaches to natural language processing tasks [9]. This includes tasks that are not purely linguistic, and require nontrivial planning and reasoning capabilities [24, 13]: for example, instruction following, vision-language navigation, and visual question answering. Indeed, some of these tasks are so distant from language modeling that one can ask whether pre-trained LMs can be used as a general framework even for tasks that involve no language at all. If so, how might these capabilities be accessed in a model trained only to process and generate natural language strings?</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Environments (left): Different environments have different types of observations and goals. Our approach (right): We use pre-trained LMs as a general framework for interactive decision-making by converting policy inputs into sequential data. Such a method enables effective combinatorial generalization to novel tasks.</p>
<p>In this paper, we study these questions through the lens of embodied decision-making, investigating the effectiveness of LM pre-training as a general framework for learning policies across a variety of environments. We propose LID, a framework that uses Pre-Trained Language Models for Interactive Decision-Making. As shown in Figure 1 (right), we encode the inputs to a policy—including observations, goals, and history—as a sequence of embeddings. These embeddings are passed to a policy network initialized with the parameters of a pre-trained LM, which is fine-tuned to predict actions. This framework is broadly applicable, accommodating goals and environment states represented as natural language strings, image patches, or scene graphs.</p>
<p>We find that imitation learning using pre-trained LMs as policy initializers improves in-domain performance and enables strong generalization over novel tasks. For i.i.d. training and evaluation tasks, this approach yields 20% more successful policies than other baseline methods in Virtual-Home [31]. For combinatorial generalization to out-of-distribution tasks, <em>i.e</em>. tasks involving new combinations of goals, states or objects, LM pre-training confers even more benefits: it improves task completion rates by 43.6% for novel tasks (see Figure 3). These results hold for a variety of environment representations: encoding states as natural language strings, when possible, improves the data-efficiency of training, but even LMs fine-tuned on random environment encodings generalize combinatorially to new goals and states when trained on large enough datasets.</p>
<p>We further examine how our method may be used in environments where expert data is not available, and agents must instead actively gather data. To do this, we integrate an Active Data Gathering (ADG) procedure into pre-trained LMs as shown in Figure 2. Our proposed approach to ADG consists of three parts. First, exploration collects trajectories using a mix of random actions and actions generated by the current policy. Exploration is insufficient in this high-dimensional problem and most of the trajectories will likely fail to achieve the end goal. A key insight is that even the failed trajectories contain useful sub-trajectories that solve certain sub-goals, and we relabel these goals in a hindsight relabeling stage. The relabeled goal describes what was achieved in the extracted sub-trajectory. The policy update stage samples relabeled trajectories to update the policy. The active data gathering procedure allows us to train the LM-policy without pre-collected expert data. It also outperforms reinforcement learning (RL) methods on embodied decision-making tasks and enables more effective generalization to novel tasks.</p>
<p>Finally, we investigate <em>why</em> LID contributes to generalization. We hypothesize three possible causes for the effectiveness of LM-based policy initialization: (1) the use of <em>language-based input encodings</em>, and more generally LMs' ability to reason about natural language strings; (2) the <em>sequential structure</em> of transformer inputs, in contrast to the fixed-sized observations used by most policy architectures, and (3) <em>task-general inductive bias</em> conferred by weight initialization with LM pretraining. We investigate (1) by encoding the policy inputs as different types of sequences. Different input encoding schemes have only a negligible impact on the performance: the effectiveness of language modeling is not limited to utilizing natural strings, but in fact extends to arbitrary sequential encodings. We study (2) by encoding observations with a single vector embedding, thereby removing its sequential structure. This operation significantly degrades the model's performance on novel tasks. Finally, we investigate (3) by learning the parameters of the policy from scratch. The success rate after removing the pre-trained LM weights drops by 11.2%, indicating that LM pretraining provides useful inductive bias for sequence processing even when sequences are not natural language strings.</p>
<p>To summarize, our work has four main contributions:</p>
<ul>
<li>First, we propose to use pre-trained LMs as a general scaffold for interactive decision-making across a variety of environments by converting all policy inputs into sequential data.</li>
<li>Second, we demonstrate that language modeling improves combinatorial generalization in policy learning: initializing a policy with a pre-trained LM substantially improves out-of-distribution performance on novel tasks.</li>
<li>Third, we integrate an active data gathering procedure into the proposed approach to further enable policy learning on environments without using pre-collected expert data.</li>
<li>Finally, we perform several analyses to explain the generalization capabilities of pre-trained LMs, finding that natural strings are not needed to benefit from LM pre-training, but the sequential input encoding and weight pre-training are important.</li>
</ul>
<p>These results point to the effectiveness of the proposed framework with pre-trained LMs as a generalpurpose framework to promote structured generalization in interactive decision-making.</p>
<h1>2 Related Work</h1>
<p>In recent years, word and sentence representations from pre-trained LMs [29, 9, 33] have become ubiquitous in natural language processing [49, 30]. Some of the most successful applications of pre-training lie at the boundary of natural language processing and other domains, as in instruction following [13] and language-guided image retrieval [22].
Learning representations of language. From nearly the earliest days of the field, natural language processing researchers observed that representations of words derived from distributional statistics in large text corpora serve as useful features for downstream tasks [8, 11]. The earliest versions of these representation learning schemes focused on isolated word forms [25, 28]. However, recent years have seen a number of techniques for training (masked or autoregressive) language models to produce contextualized word representations (which incorporate information neighboring words in sentences and paragraphs) via a variety of masked-word prediction objectives [9, 47].
Applications of pre-trained LMs. LMs can be fine-tuned to perform language processing tasks other than language modeling by casting those tasks as word-prediction problems. Successful uses of representations from pre-trained models include syntactic parsing [19] and language-to-code translation [45]; successful adaptations of LM prediction heads include machine translation [49], sentiment classification [6] and style transfer [18]. A number of tasks integrate language and other modalities, including visual question answering and image captioning [48]. Recent works find that image representations can be injected directly into LMs' embedding layers [42].
Policy learning and LM. Traditional policy learning methods, such as PPO [37], DQN [27], DDPG [21], A3C [26], perform well on playing tasks on Atari, OpenAI gym [5], and MuJoCo [41]. Some of them might fail to solve more challenging tasks on embodied environments [31, 39]. Several recent papers [36, 17, 15] propose to use LM for policy learning. Frozen Pretrained Transformer (FPT) [23] demonstrates that pre-trained LMs require very little fine-tuning to match the performance of task-specific models on several image classification and numerical sequence processing tasks. Semi-Supervised Skill Learning with Latent Language (SL) ${ }^{3}$ [38] shows that LMs can serve as an effective backbone for hierarchical policies that express plans as natural language strings [2, 4]. In this paper, we focus on building a general framework for decision-making tasks using pre-trained LMs, even when language is not provided as an input or output.</p>
<h2>3 Decision-Making and Language Modeling</h2>
<h3>3.1 POMDPs and Policy Learning</h3>
<p>We explore the application of LMs to general sequential decision-making tasks in partially observed environments. These tasks may be formalized as partially observable Markov decision processes (POMDPs). A POMDP is defined by a set of states, a set of observations, a set of actions, and a transition model $\mathcal{T}\left(s_{t+1} \mid s_{t}, a_{t}\right)$ that maps the current state and action to the next state. Importantly, in a POMDP setting, the observation $o_{t}$ only captures a portion of the underlying state $s_{t}$, and an</p>
<p>optimal decision-making strategy (a policy) must incorporate both the current observation and the history of previous observations and actions. In our experiments, policies are parametric models $\pi_{\phi}\left(a_{t} \mid g, h_{t}, o_{t}\right)$ that output the probability of an action given the goals $g$, history information $h_{t}=$ $\left{o_{1}, a_{1}, \cdots, o_{t-1}, a_{t-1}\right}$, and partial observations $o_{t}$ of the current state $s_{t}$.
In Figure 1 (right), we show a high-level overview of the proposed method. We first convert all policy inputs into a sequence and provide them as input to a transformer encoder. Representations from this encoder model are then passed to a task-specific decoder that predicts actions. We collect a dataset of $N$ training trajectories $\mathcal{D}=\left{d^{i}\right}<em 1="1">{i=1}^{N}$, where each trajectory consists of a goal and a sequence of observations and actions: $d^{i}=\left{g^{i}, o</em>\right}$ across trajectories using the cross-entropy loss:}^{i}, a_{1}^{i}, \cdots, o_{T_{i}}^{i}, a_{T_{i}}^{i}\right}$, where $T_{i}$ is the length of the trajectory. We then train the policy to maximize the probability of actions we want to achieve $\boldsymbol{a}^{i}=\left{a_{1}^{i}, \ldots, a_{T_{i}}^{i</p>
<p>$$
\phi^{*}=\underset{\phi}{\arg \min }\left(-\sum_{i=1}^{N} \sum_{t=1}^{T_{i}} \ln \pi_{\phi}\left(a_{t}^{i} \mid g^{i}, h_{t}^{i}, o_{t}^{i}\right)\right)
$$</p>
<h1>3.2 Language models as policy initializers</h1>
<p>Our experiments focus on autoregressive, transformer-based LMs [43]. These models are trained to fit a distribution over a text sequence $\boldsymbol{y}=\left{y_{i}\right}<em 1="1">{i=1}^{n}$ via the chain rule $p(\boldsymbol{y})=p\left(y</em>$, then the full conditioned sequence is fed into the LM. In our work, we use a standard LM, GPT-2, to process the input sequence rather than to predict future tokens.
Both POMDP decision-making and language modeling are naturally framed as sequence prediction tasks, where successive words or actions/observations are predicted based on a sequence of previous words or actions/observations. This suggests that pre-trained LMs can be used to initialize POMDP policies by fine-tuning them to model high-reward or expert trajectories, as described below.}\right) \prod_{i=2}^{n} p\left(y_{i} \mid\right.$ $\left.y_{1}, \ldots, y_{i-1}\right)$. Each term on the right hand side is parameterized by a transformer network, which accepts the conditioned tokens as input. Each token passes through a learned embedding layer $F_{\theta</p>
<h2>4 Approach</h2>
<p>We evaluate the effectiveness of pre-trained LMs in solving decision-making tasks across environments. We use BabyAI [16] and VirtualHome [31] to evaluate the proposed method. While both environments feature complex goals, the nature of these goals, as well as the state and action sequences that accomplish them, differ substantially across environments (Figure 1 (left)).</p>
<h3>4.1 Policy Network</h3>
<p>We first examine whether pre-trained LMs provide effective initializers when states and action histories are represented as natural language strings. We encode the inputs to the policy-including observations, goals, and action histories-as sequences of words. These word sequences are passed to the LM (using its pre-trained word embedding layer $F_{\theta}$ ) and used to obtain contextualized token representations. Token representations are averaged and used to predict actions. We design a policy network following the general policy framework proposed in Figure 1.
Environment encodings in VirtualHome. In VirtualHome, each goal consists of a sequence of predicates and multiplicities, and is translated into a templated English sentence (e.g. "Inside (apple, fridge) :2" becomes "put two apples inside the fridge"). To encode the agent's partial observation, we extract a list of currently visible objects, their states (e.g. "open, clean"), and 3D world coordinates. We use a fully-connected layer to encode the 3D information and generate a feature representation of each object in the observation. To encode history, we store information about all previous actions and convert them into templated English sentences (e.g. "I have put the plate on the kitchen table and the apple inside the fridge").
Environment encodings in BabyAI. The observation by default is a $7 \times 7$ grid. We convert the observation into $7 \times 7$ text descriptions, e.g. "purple ball", "grey wall", "open door", and combine them into a long sentence. We then convert the history actions into text descriptions, e.g. "turn left" and "go forward". We combine the language instruction (without modification) with the observation and history text descriptions, and feed them to the pre-trained LM.</p>
<p>We note that the policy network described above does not strictly require that these encodings take the form of natural language strings-other encodings of the environment as a sequence also work (see Section 7). This framework could be also generalized to support pixel-based observations using discretization schemes like the one employed in the Vision Transformer [10].
Action prediction. We pool LM outputs into a "context representation" that is used to predict the next action. In training, we maximize the probabilities of demonstrated actions. In inference, we select the valid action with the highest probability. See Appendix C. 1 for details.</p>
<p>VirtualHome and BabyAI have quite different observation spaces, action spaces, and goal spaces; however, we show that embedding policy inputs as sequences and utilizing the pre-trained LM as a policy initializer, enables effective generalization to novel tasks on both environments. We note that LID is not limited to VirtualHome and BabyAI, but is straightforwardly applicable to other embodied environments, such as ALFRED [40] and iGibson [39].</p>
<h1>4.2 Training</h1>
<p>We first examine LID through imitation learning on data collected by experts in Section 4.2.1. We then show that integrating an active data gathering procedure into LID enables policy learning without using expert data in Section 4.2.2. We use VirtualHome as an example to explain the data gathering.</p>
<h3>4.2.1 Policy Learning with Expert Data</h3>
<p>The policy model is first initialized from a pre-trained LM and then fine-tuned on data collected by experts. We build on the VirtualHome environment to collect a set of expert trajectories using regression planning [20] and create a VirtualHome-Imitation Learning dataset. Given a task described by goal predicates, the planner generates an action sequence to accomplish this task (See Appendix E.1). The planner has access to privileged information, such as information about the pre-conditions and effects of each action, allowing an agent to robustly perform tasks in partially observable environments and generate expert trajectories for training and evaluation.</p>
<h3>4.2.2 Policy Learning with Active Data Gathering</h3>
<p>Collecting expert data is sometimes challenging. It may require privileged information of the environment or human annotations, which can be timeconsuming and difficult to scale. A promising way to scale up supervision is Hindsight Experience Replay (HER) [3], which allows agents to learn from orders of magnitude more data without supervision. However, existing HER methods [12] focus on simple tasks with small state/action space and full observability. They cannot tackle more complicated embodied decision-making tasks, requiring nontrivial planning and reasoning or natural language understanding. LID with the active data gathering (LIDADG) can be used in solving tasks in such environments.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: LID with the active data gathering procedure. By iteratively repeating the exploration, hindsight relabeling, and policy update, LID with active data gathering can learn an effective policy without using pre-collected expert data.</p>
<p>As shown in Figure 2, LID-ADG consists of three stages, i.e. exploration, hindsight relabeling, and policy update. The key idea is to gradually improve the task success rate by asking the agent to iteratively explore the environment, relabel failure samples, and update its policy using imitation learning. In the exploration stage, we first randomly sample a goal and an initial state. We then use a mix of random actions and actions generated by the current policy $\pi_{a}\left(a_{t} \mid g, h_{t}, o_{t}\right)$ to obtain the next action. We repeat this process until this episode ends. We collect $M$ trajectories and store them in the replay buffers. The generated actions in the early stages rarely complete the given task.</p>
<p>However, even the failed trajectories contain useful sub-trajectories that solve certain sub-goals. In the hindsight relabeling stage, we extract useful sub-trajectories and relabel a goal $g^{\prime}$ for each of them. We design a goal relabel function $f_{l}$ that generates a goal based on the sequence of observations and actions using hand-designed templates. In practice, we implement the goal relabel function as a program (see Appendix E.2). The hindsight relabeling stage allows sample-efficient learning by reusing the failure cases. During policy update, the agent samples the data from the replay buffers and updates its policy network $\pi_{\phi}$.</p>
<p>By interleaving the exploration, hindsight relabeling, and policy update, LID-ADG can gradually improve the policy without requiring pre-collected expert data. In embodied environments with large action spaces, sparse rewards, and long-horizon planning, RL methods often struggle to obtain stable policy gradients during training. Our method enables sample-efficient learning from the sparse rewards by relabeling new goals for the bad samples that the agent fails to achieve. In addition, LID-ADG leverages the stability of supervised learning in the policy update stage, enabling it to outperform RL approaches on a wide range of decision-making tasks.</p>
<h1>5 Experiment Setup</h1>
<p>We evaluate the proposed method and baselines on VirtualHome and BabyAI.</p>
<h3>5.1 VirtualHome</h3>
<p>VirtualHome is a 3D embodied environment featuring partial observability, large action spaces, and long time horizons. We evaluate policies’ performance from three aspects: (1) performance on in-distribution tasks; (2) generalization to novel scenes; and (3) generalization to novel tasks.</p>
<p>In-Distribution. The predicate types and their counts in the goal are randomly sampled from the same distribution as the training data. The objects are initially placed in the environment according to common-sense layouts (e.g. plates appear inside the kitchen cabinets rather than the bathtub).</p>
<p>Novel Scenes. The objects are placed in random positions in the initial environment without commonsense constraints (e.g. apples may appear inside the dishwasher).</p>
<p>Novel Tasks. The components of all goal predicates are never seen together during training (e.g. both plates and fridges appear in training goals, but Inside(plate, fridge) only appears in the test set. (See Appendix F for more details.)</p>
<p>We evaluate the success rates of different methods on each test set. A given episode is scored as successful if the policy completes its entire goal within the maximum allowed steps of the environment. On each of the 3 test subsets, we use 5 different random seeds and test 100 tasks under each seed. Thus there are 1500 examples used to evaluate each model.</p>
<h3>5.2 BabyAI</h3>
<p>BabyAI is a 2D grid world environment for instruction following. Observations in BabyAI are $7 \times 7 \times 3$ grids describing a partial and local egocentric view of the state of the environment. We evaluate the methods on four representative tasks: GoToRedBall, GoToLocal, PickupLoc, and PutNextLocal. Performing well on the test set requires the models to generalize to new environment layouts and goals, resulting in new combinations of tasks not seen in training. For each method, we compute success rates over 500 episodes on each task.</p>
<h2>6 Experiments</h2>
<p>We first show results of the proposed method and baselines for embodied decision-making tasks using expert data in Section 6.1. We then show our results when using actively gathered data in Section 6.2.</p>
<h3>6.1 Embodied Decision Making with Pre-trained Language Model (LID)</h3>
<h3>6.1.1 Results on VirtualHome</h3>
<p>We evaluate the following methods:</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Comparisons of the proposed method and baselines on VirtualHome. All the methods are trained on expert data using imitation learning. $M L P-1, M L P$, and $L S T M$ are baselines without using the pre-trained LM. The proposed method, LID-Text (Ours), outperforms all baselines.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Tasks</th>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">Number of Demos</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">1 K</td>
<td style="text-align: center;">5 K 10 K</td>
</tr>
<tr>
<td style="text-align: center;">GoToRedBall</td>
<td style="text-align: center;">BabyAI-Ori [16]</td>
<td style="text-align: center;">81.0</td>
<td style="text-align: center;">96.0</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">99.5 99.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LID-Text (Ours)</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">100.0100.0</td>
</tr>
<tr>
<td style="text-align: center;">GoToLocal</td>
<td style="text-align: center;">BabyAI-Ori [16]</td>
<td style="text-align: center;">55.9</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">98.6</td>
<td style="text-align: center;">99.9 99.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LID-Text (Ours)</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">99.5 99.5</td>
</tr>
<tr>
<td style="text-align: center;">PickupLoc</td>
<td style="text-align: center;">BabyAI-Ori [16]</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">97.9 99.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LID-Text (Ours)</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">99.6 99.8</td>
</tr>
<tr>
<td style="text-align: center;">PutNextLocal</td>
<td style="text-align: center;">BabyAI-Ori [16]</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">81.2 97.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LID-Text (Ours)</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">98.9 99.9</td>
</tr>
</tbody>
</table>
<p>Table 1: Success rates on BabyAI tasks. All the methods are trained on offline expert data using imitation learning. LID-Text (Ours) outperforms BabyAIOri, the method used in the original paper [16].</p>
<p>LID-Text (Ours) is the proposed method that converts all environments inputs into text descriptions. The pre-trained LM is fine-tuned for decision-making (conditioned on goals, observations, and histories) as described in Section 4.1.</p>
<p>Recurrent Network. We compare our method with a recurrent baseline using an LSTM [14] to encode the history information. The hidden representation from the last timestep, together with the goal and current observation, are used to predict the next action.</p>
<p>MLP and MLP-1. We perform additional comparisons with baselines that do not use recurrent networks or pre-trained LMs. $M L P$ and $M L P-1$ take the goal, histories, and the current observation as input and send them to the multilayer perceptron neural network (MLP) to predict actions. $M L P-1$ has three more average-pooling layers than $M L P$ that average the features of tokens in the goal, history actions, and the current observation, respectively, before sending them to the MLP layer.</p>
<p>Quantitative results. Each method is trained on $20 K$ demos from the VirtualHome-Imitation Learning dataset, and then evaluated on the three test subsets: In-Distribution, Novel Scenes, and Novel Tasks. In Figure 3, LID-Text (Ours), which initializes the policy with a pre-trained LM, has higher success rates than other methods. This difference is most pronounced in the Novel Tasks setting, where test tasks require combinatorial generalization across goals that are never seen during training. Here, LID-Text (Ours) dramatically ( $43.6 \%$ ) improves upon all baselines. Such combinatorial generalization is necessary to construct general purpose agents, but is often difficult for existing approaches. Our results suggest that pre-trained LMs can serve as a computational backbone for combinatorial generalization.</p>
<h1>6.1.2 Results on BabyAI</h1>
<p>We use the standard training and test data provided by [16]. In BabyAI, performing well on unseen test tasks with new environment layouts and goals requires combinatorial reasoning. In Table 1, we report the success rate of models trained on different number of demos. BabyAI-Ori [16] is the method used in the original paper. LID-Text (Ours) is the proposed method that converts policy inputs into a text sequence. Given enough training data, i.e. 10 K demos, both methods achieve high success rates, but LID-Text (Ours) outperforms BabyAI-Ori with less training data, indicating the proposed method improves sample efficiency when generalizing to novel tasks.</p>
<h3>6.2 Pre-trained Language Model with Active Data Gathering (LID-ADG)</h3>
<p>We compare LID-ADG, the proposed LM framework for decision-making using actively gathered data (Section 4.2.2), to a variety of baselines that do not use pre-collected expert data on VirtualHome.</p>
<p>Random. The agent selects the next action randomly from the valid action space at that state. Goal-Object. The agent randomly selects an object that in the goal and in the valid action space to interact with. For example, given a goal of "Inside(apple, fridge):1", this baseline might choose "grab apple", "open fridge", or other actions containing "apple" or "fridge". Online RL. We compare with PPO [37], one of the most commonly used online RL methods. For fair comparison, we equip PPO with the same main policy network as the proposed method. Our implementation</p>
<table>
<thead>
<tr>
<th style="text-align: left;">In-Distribution Novel Scenes Novel Tasks</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: left;">Goal-Object</td>
<td style="text-align: center;">$0.8 \pm 0.5$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
<td style="text-align: center;">$0.4 \pm 0.4$</td>
</tr>
<tr>
<td style="text-align: left;">PPO</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: left;">DQN+HER</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: left;">LID-ADG (Ours)</td>
<td style="text-align: center;">$\mathbf{4 6 . 7} \pm \mathbf{2 . 7}$</td>
<td style="text-align: center;">$\mathbf{3 2 . 2} \pm \mathbf{3 . 3}$</td>
<td style="text-align: center;">$\mathbf{2 5 . 5} \pm \mathbf{4 . 1}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparisons of methods without using expert data on VirtualHome. LID-ADG (Ours) is the only successful approach.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">In-Distribution Novel Scenes Novel Tasks</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LID-ADG (Ours)</td>
<td style="text-align: center;">$46.7 \pm 2.7$</td>
<td style="text-align: center;">$\mathbf{3 2 . 2} \pm \mathbf{3 . 3}$</td>
<td style="text-align: center;">$25.5 \pm 4.1$</td>
</tr>
<tr>
<td style="text-align: left;">PPO (LID-ADG Init)</td>
<td style="text-align: center;">$\mathbf{5 3 . 7} \pm \mathbf{3 . 5}$</td>
<td style="text-align: center;">$30.2 \pm 3.4$</td>
<td style="text-align: center;">$\mathbf{2 7 . 8} \pm \mathbf{2 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">DT (LID-ADG Data)</td>
<td style="text-align: center;">$42.4 \pm 1.5$</td>
<td style="text-align: center;">$21.6 \pm 2.48$</td>
<td style="text-align: center;">$16.8 \pm 1.0$</td>
</tr>
</tbody>
</table>
<p>Table 3: The proposed method with active data gathering, LID-ADG (Ours), can be used as an policy initializer for online RL or a data provider for offline RL.
is based on Stable Baselines3 [35]. Hindsight Experience Replay. We compare with DQN+HER used in [3] and modify its main policy network to be the same as the proposed method.
Quantitative results. We compare LID-ADG with baselines on VirtualHome in Table 2. Each experiment is performed 5 times with different random seeds. The Random baseline is always 0, indicating the tasks in VirtualHome cannot be easily solved by a random policy. Goal-Object is better than Random because Goal-Object has access to objects in the goal and it samples actions from a much smaller action space. The online RL baseline, PPO, fails to solve tasks in VirtualHome featured by partially observation, large state/action space, and long-term horizon. DQN+HER works well on simple tasks on 2D environments, but they cannot tackle VirtualHome tasks neither, requiring nontrivial planning and reasoning. LID-ADG does not require expert data and can solve the complicated tasks in 3D embodied environments which cannot be easily achieved using RL. ${ }^{3}$
Policy initializer and data provider. LID-ADG can further be used to initialize the weights for finetuning RL policies and to gather data for offline learning. As shown in Table 2, directly training RL, e.g. PPO, fails to solve tasks in VirtualHome. However, after using the policy trained by LID-ADG to initialize the PPO policy, we may effectively learn an interactive policy with good performance. In Table 3, PPO (LID-ADG Init) is initialized from LID-ADG and further fine-tuned to solve the tasks in VirtualHome. After initialization, PPO improves its success rate by $53.7 \%$ on the In-Distribution setting (See PPO results in Table 2 and Table 3). In addition, LID-ADG can provide data for offline learning. LID-ADG saves the relabeled data in replay buffers. We train Decision Transformer (DT) [7] using the data collected by LID-ADG. See DT (LID-ADG Data) in Table 3.</p>
<h1>7 Analysis: Understanding the Sources of Generalization</h1>
<p>The pre-trained LM policy, fine-tuned on either expert data or actively gathered data, exhibits effective combinatorial generalization. Is this simply because LMs are effective models of relations between natural language descriptions of states and actions [1], or because they provide a more general framework for combinatorial generalization in decision-making? We hypothesize and investigate three possible factors to understand the sources of such combinatorial generalization. We use policies trained on the expert data as an example to explain the experiments.</p>
<h3>7.1 Input Encoding Scheme</h3>
<p>We first hypothesize that converting environment inputs into natural language contributes to the combinatorial generalization as the LMs are trained on language data. We explore the role of natural language by investigating three alternative ways of encoding policy inputs to our model without using natural language strings: two in VirtualHome, and one in BabyAI. BabyAI results are in Appendix A.
Index encoding in VirtualHome. Rather than natural language strings, LID-Index (Ours) converts policy inputs into integer indices. LID-Index (Ours) retains the discrete, serial format of the goal, history, and observation, but replaces each word with an integer, and replaces the embedding layer from the pre-trained LM with a new embedding layer trained from scratch. For example, grab apple is mapped to $(5,3)$ based on the positions of grab and apple in the vocabulary set.
Unnatural string encoding in VirtualHome. LID-Unnatural (Ours) replaces the natural language tokens (e.g. converting the goal "On (fork, table):1" to put one fork on the table) with random ones (e.g. converting On (fork, table) to brought wise character trees fine yet). This is done by</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 4: Success rates of policies trained with different input encodings in the Novel Tasks setting on VirtualHome. The text encoding is the most sample-efficient, but all models converge to similar performance given sufficient training data.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>Number of Demos</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>$\mathbf{1 0 0}$</td>
<td>$\mathbf{5 0 0}$</td>
<td>$\mathbf{1 K}$</td>
<td>$\mathbf{5 K}$</td>
<td>$\mathbf{1 0 K}$</td>
<td>$\mathbf{2 0 K}$</td>
</tr>
<tr>
<td>LID-Text (Ours)</td>
<td>$\mathbf{8 . 8} \pm \mathbf{1 . 4}$</td>
<td>$\mathbf{2 2 . 2} \pm \mathbf{1 . 7}$</td>
<td>$26.8 \pm 1.0$</td>
<td>$46.0 \pm 1.0$</td>
<td>$\mathbf{5 8 . 2} \pm \mathbf{1 . 2}$</td>
<td>$58.2 \pm 1.6$</td>
</tr>
<tr>
<td>LID-Index (Ours)</td>
<td>$6.4 \pm 0.6$</td>
<td>$18.0 \pm 3.8$</td>
<td>$18.8 \pm 1.0$</td>
<td>$45.5 \pm 2.1$</td>
<td>$54.6 \pm 0.8$</td>
<td>$57.8 \pm 0.9$</td>
</tr>
<tr>
<td>LID-Unnatural (Ours)</td>
<td>$6.8 \pm 1.3$</td>
<td>$18.6 \pm 2.1$</td>
<td>$\mathbf{2 7 . 0} \pm \mathbf{1 . 1}$</td>
<td>$\mathbf{4 7 . 2} \pm \mathbf{1 . 7}$</td>
<td>$55.8 \pm 0.8$</td>
<td>$\mathbf{5 8 . 8} \pm \mathbf{0 . 9}$</td>
</tr>
</tbody>
</table>
<p>randomly permuting the entire vocabulary, mapping each token to a new token. Such a permutation breaks the semantic information in natural strings.</p>
<p>LID-Index (Ours) and LID-Unnatural (Ours) have the same policy network as LID-Text (Ours). All are fine-tuned on the expert data. The averaged results using 5 different random seeds on the Novel Tasks setting are reported in Table 4. Given few training data, e.g. 100 demos, all the models perform poorly, with success rates lower than $10 \%$. LID-Text (Ours) achieves higher success rates than LID-Index (Ours) and LID-Unnatural (Ours) when dataset size increases, e.g. LID-Text (Ours) is around $4 \%$ higher than LID-Index (Ours) and LID-Unnatural (Ours) with 500 training demos. When the training dataset is further enlarged, e.g. 20 K demos, success rates of all approaches reach similar performance. This result indicates that the effectiveness of pre-trained LMs in compositional generalization is not unique to natural language strings, but can be leveraged from arbitrary encodings, although adapting the model to arbitrary encodings may require more training data.</p>
<h3>7.2 Sequential Input Representation</h3>
<p>Next, we explore whether generalization requires the sequential processing mechanisms in transformer-based LMs. We investigate whether the LM pre-trained policy will still be effective when the input encoding is not sequential. No-Seq encodes the goal as a single vector by averaging all goal embeddings. History and observation features are obtained in the same way. All features are then sent to the pre-trained LM to predict actions. As shown in Table 5, removing sequential structure significantly hurts performance on Novel Tasks. NoSeq achieves good performance on test tasks that are closer to training tasks, but cannot generalize well to more challenging unseen tasks. Thus, combinatorial generalization in pre-trained LMs may be attributed in part to transformers' ability to process sequential input representations effectively.</p>
<h3>7.3 Favorable Weight Initialization</h3>
<p>Finally, we investigate if the favorable weight initialization from LM pre-training enables effective generalization of the proposed model. No-Pretrain does not initialize the policy using the pre-trained LM, but instead training the policy on the expert data from scratch. In Table 5, we find that removing the pre-trained weights can fit the in-domain data and thus performs well on the In-Distribution setting. However, its success rate is $11.2 \%$ lower than the proposed model on the Novel Tasks setting, indicating the pre-trained weights are important for effective generalization, but not necessary for effective data fitting. We further test a baseline, No-FT, that keeps the pre-trained weights of the language model but freezes them while training the rest model on our expert data. Freezing the pretrained weights without fine-tuning significantly hurts the performance on both settings, suggesting that fine-tuning of the transformer weights is essential for effective combinatorial generalization.</p>
<p>Together, these results suggest that sequential input representations (vs. fixed-dimensional feature vectors) and favorable weight initialization are both important for generalization, however, the input encoding schemes (e.g. as a natural language string vs. an arbitrary encoding scheme) has little influence. These results point to the potential broader applicability of pre-trained LMs as a computational backbone for compositional embodied decision making, where arbitrary inputs, such as language, images, or grids, may be converted to sequential encodings.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Qualitative results of our model on VirtualHome and BabyAI. We only show a sub-trajectory in each example to save space. The interacted objects are labelled by green bounding boxes.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Failure cases. We show failure cases caused by the grounding error and policy error. The interacted objects are labelled by green bounding boxes.</p>
<h1>8 Qualitative Results</h1>
<p>In Figure 4, we show examples of LID-Text (Ours) completing tasks in VirtualHome and BabyAI. We show two successful examples from VirtualHome on the In-Distribution and Novel Tasks settings, and two successful examples from BabyAI on solving the GoToLocal and PickupLoc tasks. We only show short trajectories or extract a sub-trajectory for saving space.</p>
<p>Failure case analysis. In Figure 5, we show some failure cases of the proposed method. We observed two main types of failure cases: grounding error and policy error. For failures caused by the grounding error, the agent interacts with a wrong object that is not related to the given goal, e.g. the agent puts cutlets instead of the salmon inside the fridge. For failures caused by the policy error, the agent cannot find the target objects or does not interact with them. The proposed method that converts policy inputs into sequential encodings and feeds them to the general LM framework can accomplish decision-making tasks efficiently, however, there are still challenging tasks that the policy fails to accomplish. Larger LMs, e.g. GPT-3 [6], may improve the success rate of those challenging tasks.</p>
<h2>9 Conclusion and Broader Impact</h2>
<p>In this paper, we introduced LID, a general approach to sequential decision-making that converts goals, histories, and observations into sequences and processes them using a policy initialized with a pre-trained LM. We integrated an active data gathering procedure into the proposed method to enable policy learning without using expert data. Our analysis showed that input representation and favorable weight initialization both contribute to the generalization while the input encoding scheme has little influence. One drawback of the active data gathering is that it relies on hand-designed rules for task relabeling. More generally, a potential disadvantage of the proposed approach is that biases of the pre-trained LMs may influence its behavior, and further study of LID-based models' bias is required before they may be deployed in sensitive downstream applications. Nevertheless, our results demonstrate that LID enables effective combinatorial generalization across different environments, and highlight the promise of LM pre-training for more general decision-making problems.</p>
<h1>References</h1>
<p>[1] P. Ammanabrolu and M. O. Riedl. Playing text-adventure games with graph-based deep reinforcement learning. arXiv preprint arXiv:1812.01628, 2018.
[2] J. Andreas and D. Klein. Learning with latent language. In North American Association for Computational Linguistics, 2022.
[3] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, P. Abbeel, and W. Zaremba. Hindsight experience replay. arXiv preprint arXiv:1707.01495, 2017.
[4] M. L. Athul Paul Jacob and J. Andreas. Multitasking inhibits semantic drift. In North American Association for Computational Linguistics, 2021.
[5] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym, 2016.
[6] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
[7] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and I. Mordatch. Decision transformer: Reinforcement learning via sequence modeling. arXiv preprint arXiv:2106.01345, 2021.
[8] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman. Indexing by latent semantic analysis. Journal of the American society for information science, 41(6):391-407, 1990 .
[9] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[10] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.
[11] S. T. Dumais. Latent semantic analysis. Annual review of information science and technology, 38(1):188-230, 2004.
[12] D. Ghosh, A. Gupta, A. Reddy, J. Fu, C. Devin, B. Eysenbach, and S. Levine. Learning to reach goals via iterated supervised learning. arXiv preprint arXiv:1912.06088, 2019.
[13] F. Hill, S. Mokra, N. Wong, and T. Harley. Human instruction-following with deep reinforcement learning via transfer-learning from text. arXiv preprint arXiv:2005.09382, 2020.
[14] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):17351780, 1997.
[15] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207, 2022.
[16] D. Y.-T. Hui, M. Chevalier-Boisvert, D. Bahdanau, and Y. Bengio. Babyai 1.1, 2020.
[17] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn. Bc-z: Zero-shot task generalization with robotic imitation learning. In Conference on Robot Learning, pages 991-1002. PMLR, 2022.
[18] N. S. Keskar, B. McCann, L. R. Varshney, C. Xiong, and R. Socher. Ctrl: A conditional transformer language model for controllable generation. arXiv preprint arXiv:1909.05858, 2019.
[19] N. Kitaev, S. Cao, and D. Klein. Multilingual constituency parsing with self-attention and pre-training. arXiv preprint arXiv:1812.11760, 2018.</p>
<p>[20] R. E. Korf. Planning as search: A quantitative approach. Artificial intelligence, 33(1):65-88, 1987.
[21] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
[22] J. Lu, D. Batra, D. Parikh, and S. Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. arXiv preprint arXiv:1908.02265, 2019.
[23] K. Lu, A. Grover, P. Abbeel, and I. Mordatch. Pretrained transformers as universal computation engines. arXiv preprint arXiv:2103.05247, 2021.
[24] A. Majumdar, A. Shrivastava, S. Lee, P. Anderson, D. Parikh, and D. Batra. Improving vision-and-language navigation with image-text pairs from the web. In European Conference on Computer Vision, pages 259-274. Springer, 2020.
[25] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111-3119, 2013.
[26] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928-1937. PMLR, 2016.
[27] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
[28] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532-1543, 2014.
[29] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep contextualized word representations. arXiv preprint arXiv:1802.05365, 2018.
[30] E. A. Platanios, A. Pauls, S. Roy, Y. Zhang, A. Kyte, A. Guo, S. Thomson, J. Krishnamurthy, J. Wolfe, J. Andreas, et al. Value-agnostic conversational semantic parsing. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3666-3681, 2021.
[31] X. Puig, K. Ra, M. Boben, J. Li, T. Wang, S. Fidler, and A. Torralba. Virtualhome: Simulating household activities via programs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8494-8502, 2018.
[32] X. Puig, T. Shu, S. Li, Z. Wang, J. B. Tenenbaum, S. Fidler, and A. Torralba. Watch-and-help: A challenge for social perception and human-ai collaboration. arXiv preprint arXiv:2010.09890, 2020.
[33] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understanding by generative pre-training. 2018.
[34] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[35] A. Raffin, A. Hill, A. Gleave, A. Kanervisto, M. Ernestus, and N. Dormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 22(268):1-8, 2021.
[36] M. Reid, Y. Yamada, and S. S. Gu. Can wikipedia help offline reinforcement learning? arXiv preprint arXiv:2201.12122, 2022.
[37] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.</p>
<p>[38] P. Sharma, A. Torralba, and J. Andreas. Skill induction and planning with latent language. In Association for Computational Linguistics, 2022.
[39] B. Shen, F. Xia, C. Li, R. Martín-Martín, L. Fan, G. Wang, S. Buch, C. D'Arpino, S. Srivastava, L. P. Tchapmi, et al. igibson, a simulation environment for interactive tasks in large realisticscenes. arXiv preprint arXiv:2012.02924, 2020.
[40] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and D. Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages $10740-10749,2020$.
[41] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026-5033. IEEE, 2012.
[42] M. Tsimpoukelli, J. Menick, S. Cabi, S. Eslami, O. Vinyals, and F. Hill. Multimodal few-shot learning with frozen language models. arXiv preprint arXiv:2106.13884, 2021.
[43] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.
[44] J. Vig. A multiscale visualization of attention in the transformer model. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 37-42, Florence, Italy, July 2019. Association for Computational Linguistics.
[45] B. Wang, R. Shin, X. Liu, O. Polozov, and M. Richardson. Rat-sql: Relation-aware schema encoding and linking for text-to-sql parsers. arXiv preprint arXiv:1911.04942, 2019.
[46] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.
[47] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le. Xlnet: Generalized autoregressive pretraining for language understanding. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.
[48] Z. Yang, N. Garcia, C. Chu, M. Otani, Y. Nakashima, and H. Takemura. Bert representations for video question answering. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1556-1565, 2020.
[49] J. Zhu, Y. Xia, L. Wu, D. He, T. Qin, W. Zhou, H. Li, and T.-Y. Liu. Incorporating bert into neural machine translation. arXiv preprint arXiv:2002.06823, 2020.</p>
<h1>Checklist</h1>
<ol>
<li>For all authors...
(a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]
(b) Did you describe the limitations of your work? [Yes] See Section 9.
(c) Did you discuss any potential negative societal impacts of your work? [Yes] See Section 9 .
(d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]</li>
<li>If you are including theoretical results...
(a) Did you state the full set of assumptions of all theoretical results? [N/A]
(b) Did you include complete proofs of all theoretical results? [N/A]</li>
<li>If you ran experiments...
(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] In the supplemental material.
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Section 6 and Appendix C.2.
(c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] See Section 6.
(d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix C.2.</li>
<li>If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
(a) If your work uses existing assets, did you cite the creators? [Yes]
(b) Did you mention the license of the assets? [N/A]
(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]
(d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A]
(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] See Section 9.</li>
<li>If you used crowdsourcing or conducted research with human subjects...
(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]
(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]
(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]</li>
</ol>
<h1>Appendix</h1>
<p>In this appendix, we first show the convolutional encoding in BabyAI in Appendix A. We then describe the environment details in Appendix B and the implementation details of the proposed model in Appendix C. We show the algorithm of interactive evaluation in Section D and the data gathering procedure in Appendix E. The goal predicates used in VirtualHome test subsets are shown in Appendix F. We visualize the attention weights in language models in Appendix G.</p>
<h2>A Convolutional encoding in BabyAI</h2>
<p>In the main paper Section 7.1, we explore the role of natural language by investigating two alternative ways of encoding policy inputs in VirtualHome. In this section, we show the third way of encoding policy inputs in BabyAI.
We test a new model, LID-Conv (Ours), that converts environment inputs into convolutional embeddings. We pass the $7 \times 7 \times 3$ grid observation in BabyAI to convolutional layers and obtain a $7 \times 7 \times d$ feature map, where $d$ is the feature dimension. We flatten the feature map and get a sequence of features to describe the observation. The rest of the model is the same as LID-Text (Ours). Table 6 shows the results of policies using the text encoding and convolutional encoding. LID-Text (Ours) and LID-Conv (Ours) have similar results given enough training data, but LID-Text (Ours) is slightly better when there are fewer training data. This conclusion is coincident with the results on VirtualHome.
Different input encoding schemes have only a negligible impact on model performance: the effectiveness of pre-training is not limited to utilizing natural strings, but in fact extends to arbitrary sequential encodings.</p>
<p>Table 6: Success rate of policies trained with text encoding vs. convolutional encoding on BabyAI. The text encoding is more sample-efficient, but both models converge to near perfect performance given sufficient training data.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Tasks</th>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Number of Demos</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\mathbf{1 0 0}$</td>
<td style="text-align: center;">$\mathbf{5 0 0}$</td>
<td style="text-align: center;">$\mathbf{1 K}$</td>
<td style="text-align: center;">$\mathbf{5 K}$</td>
<td style="text-align: center;">$\mathbf{1 0 K}$</td>
</tr>
<tr>
<td style="text-align: left;">GoToRedBall</td>
<td style="text-align: left;">LID-Text (Ours)</td>
<td style="text-align: center;">$\mathbf{9 3 . 9}$</td>
<td style="text-align: center;">$\mathbf{9 9 . 4}$</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">$\mathbf{1 0 0 . 0}$</td>
<td style="text-align: center;">$\mathbf{1 0 0 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">LID-Conv (Ours)</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">$\mathbf{1 0 0 . 0}$</td>
<td style="text-align: center;">$\mathbf{1 0 0 . 0}$</td>
<td style="text-align: center;">$\mathbf{1 0 0 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">GoToLocal</td>
<td style="text-align: left;">LID-Text (Ours)</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">$\mathbf{9 7 . 9}$</td>
<td style="text-align: center;">$\mathbf{9 9 . 0}$</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">99.5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">LID-Conv (Ours)</td>
<td style="text-align: center;">$\mathbf{6 9 . 5}$</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">98.2</td>
<td style="text-align: center;">$\mathbf{9 9 . 9}$</td>
<td style="text-align: center;">$\mathbf{9 9 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">PickupLoc</td>
<td style="text-align: left;">LID-Text (Ours)</td>
<td style="text-align: center;">$\mathbf{2 8 . 7}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 4}$</td>
<td style="text-align: center;">$\mathbf{9 9 . 0}$</td>
<td style="text-align: center;">$\mathbf{9 9 . 6}$</td>
<td style="text-align: center;">99.8</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">LID-Conv (Ours)</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">95.1</td>
<td style="text-align: center;">$\mathbf{9 9 . 6}$</td>
<td style="text-align: center;">$\mathbf{1 0 0 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">PutNextLocal</td>
<td style="text-align: left;">LID-Text (Ours)</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">$\mathbf{9 3 . 0}$</td>
<td style="text-align: center;">$\mathbf{9 3 . 2}$</td>
<td style="text-align: center;">$\mathbf{9 8 . 9}$</td>
<td style="text-align: center;">$\mathbf{9 9 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">LID-Conv (Ours)</td>
<td style="text-align: center;">$\mathbf{1 7 . 9}$</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">91.3</td>
<td style="text-align: center;">97.7</td>
<td style="text-align: center;">99.5</td>
</tr>
</tbody>
</table>
<h2>B Environments</h2>
<p>We use BabyAI [16] and VirtualHome [31] to evaluate the proposed method. While both environments feature complex goals, the nature of these goals, as well as the state and action sequences that accomplish them, differ substantially across environments.</p>
<h2>B. 1 VirtualHome</h2>
<p>VirtualHome is a 3D realistic environment featuring partial observability, large action spaces, and long time horizons. It provides a set of realistic 3D homes and objects that can be manipulated to perform household organization tasks.
Goal Space. For each task, we define the goal as a set of predicates and multiplicities. For example, Inside(apple, fridge):2; Inside(pancake, fridge):1; means "put two apples and one pancake inside the fridge". In each task, the initial environment (including initial object locations), the goal predicates, and their orders and multiplicities are randomly sampled. There are 59 different types of predicates in total.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Policy network in VirtualHome. The observation, goal, and history are first converted into sequences and then passed through an embedding layer $F_{\theta}$. The combined sequence is passed through a pre-trained LM, and the output tokens are pooled into a context feature vector for action prediction.</p>
<p>Observation Space. The observation in VirtualHome by default is a graph describing a list of objects and their relations in the current partial observation. Each object has an object name, a state, e.g. open, close, clean, and 3D coordinates.
Action Space. Agents can navigate in the environment and interact with objects. To interact with an object, the agent must predict an action name and the index of the interested object, e.g. Open(5) to opening the object with index (5). The agent can only interact with objects that are in the current observation or execute the navigation actions, such as Walk (bathroom). For some actions, such as open, the agent must be close to the object. There are also strict preconditions for actions, e.g. the agent must grab an object before it can put the object on a target position. As a result of these constraints, the subset of actions available to the agent changes at every timestep.
We evaluate the success rates of different methods on VirtualHome. A given episode is scored as successful if the policy completes its entire goal within $T$ steps, where $T=70$ is the maximum allowed steps of the environment.</p>
<h1>B. 2 BabyAI</h1>
<p>BabyAI is a 2D grid world environment designed to evaluate instruction following. Different from VirtualHome, the observation in BabyAI by default is a $7 \times 7$ grid describing a partial and local egocentric view of the state of the environment. Each tile in the grid contains at most one object, encoded using 3 integer values: one for the object type, one for the object color, and a state for doors indicating whether it is open, closed or locked. The goals in BabyAI are language instructions, e.g. "put the blue key next to the purple ball". BabyAI has 7 actions, e.g. "turn left", "pick up", and "drop".</p>
<h2>C More implementation Details of LID in VirtualHome</h2>
<p>In Appendix C.1, we provide more details of the model architecture used in the main paper Section 4.1. We then introduce the training detail in Appendix C.2.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Object encoding. In VirtualHome, the partial observation of the environment state can be represented as a list of objects in the agent's view. Each object is represented by a name, a state vector, and position vector. Object name encoding: each object's name is an English phrase. We tokenize the phrase, embed the tokens, and average the embeddings. Object state encoding: each object is assigned one of six states: "clean", "closed", "off", "on", "open", or "none". This state is represented as a 6-dimensional binary vector and passed through a fully-connected layer. Object position encoding: an object's position vector is a 6-dimensional vector containing its world coordinates alongside its displacement to the agent (i.e. the difference in their world coordinates). This position vector is passed through two fully-connected layers. These three features are concatenated and passed through a fully-connected layer to obtain the representation of an object in the current observation.</p>
<h1>C. 1 Model architecture details in VirtualHome</h1>
<p>In this section, we provide more details of the policy network we used in VirtualHome. Our policy model consists of three parts, i.e. inputs, the pre-trained LM, and outputs. As shown Figure 6, we encode the inputs to the policy-including goal $g$, history $h_{t}$, and the current partial observation $o_{t}$ as sequences of embeddings. These embeddings are passed to the LM (using its pre-trained embedding layer $F_{\theta}$ ) and used to obtain contextualized token representations. These token representations are averaged to generate a context feature $f_{c}$, which is then passed to fully-connected layer to predict the next action $a_{t}$. The output action in VirtualHome consists of a verb and an object. For brevity, we omit the time subscript $t$ from now on.
In VirtualHome, the partial observation $o$ of the environment state can be represented as a list of objects in the agent's view. We represent each object by its name, e.g. "oven", a state description, e.g. "open, clean", and position both in the world and relative to the agent. In this part, we provide more details of how LID-Text (Ours) encodes the name, state, and position of each object in the observation. Figure 7 shows the model architecture we used to encode the observation.
Name encoding. For each object node, we serialize its object name as an English phrase $s^{o}$. We extract its tokens and features using the tokenizer and the embedding layer of the pre-trained LM, respectively. Since one object name might generate several English tokens using the tokenizer from the pre-trained LM, e.g. the tokens of "kitchencabinet" is [15813, 6607, 16212, 500], we take the averaged features of all the tokens in the object name and obtain a "name" feature $f_{i}^{o, \text { name }}$ for each object node as shown in Figure 7.
State encoding. Some objects have a state description, e.g. "oven: open, clean". There are six types of object states in the environment: "clean", "closed", "off", "on", "open", and "none". For each object node, we use a binary vector to represent its state. Taking the "oven" as an example, if the oven is open and clean, its state vector would be $[1,0,0,0,1,0]$. This state vector is then passed through a fully-connected layer to generate a state feature $f_{i}^{o, \text { state }}$ of object $o_{i}$.
Position encoding. To encode the position information of each object $o_{i}$, we take their world coordinates $\left{o_{i, x}, o_{i, y}, o_{i, z}\right}$ and their spatial distance to the agent $\left{a_{x}, a_{y}, a_{z}\right}$ to generate a position vector $\left[o_{i, x}, o_{i, y}, o_{i, z}, o_{i, x}-a_{x}, o_{i, y}-a_{y}, o_{i, z}-a_{z}\right]$. This position vector is then passed through two fully-connected layers with a ReLU layer in the middle to generate a position feature $f_{i}^{o, \text { position }}$ of object $o_{i}$.</p>
<p>The final feature $f_{i}^{o}$ of each object node is obtained by passing the concatenation of its name feature $f_{i}^{o, \text { name }}$, state feature $f_{i}^{o, \text { state }}$, and position feature $f_{i}^{o, \text { position }}$ through a fully connected layer. The observation at a single step can be written as a set of features $\left{f_{1}^{o}, \cdots, f_{N}^{o}\right}$, where $N$ is the number of objects in the current observation.</p>
<h1>C. 2 Training details</h1>
<p>Our proposed approach and baselines are trained on Tesla 32GB GPUs. We train every single model on 1 Tesla 32GB GPU. All experiments used the AdamW optimizer with the learning rate of $10^{-5}$. We utilize a standard pre-trained language model, GPT-2, in our experiments. GPT-2 is trained on the Webtext dataset [34] using the Huggingface library [46].</p>
<h2>D Interactive Evaluation</h2>
<p>The algorithm for interactive evaluation is shown in Algorithm 1.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="o">:</span><span class="w"> </span><span class="nt">Interactive</span><span class="w"> </span><span class="nt">evaluation</span>
<span class="nt">A</span><span class="w"> </span><span class="nt">set</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">task</span><span class="w"> </span><span class="nt">goals</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">G</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">(</span><span class="nt">each</span><span class="w"> </span><span class="nt">goal</span><span class="w"> </span><span class="nt">has</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">corresponding</span><span class="w"> </span><span class="nt">initial</span><span class="w"> </span><span class="nt">state</span><span class="o">);</span>
<span class="nt">Load</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">learned</span><span class="w"> </span><span class="nt">policy</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">pi_</span><span class="p">{</span><span class="err">\phi</span><span class="p">}</span><span class="err">\</span><span class="o">);</span>
<span class="nt">Successful</span><span class="w"> </span><span class="nt">trajectory</span><span class="w"> </span><span class="nt">count</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">n</span><span class="o">=</span><span class="nt">0</span><span class="err">\</span><span class="o">);</span>
<span class="nt">for</span><span class="w"> </span><span class="nt">example</span><span class="w"> </span><span class="err">\</span><span class="o">(=</span><span class="nt">1</span><span class="o">,</span><span class="w"> </span><span class="nt">N_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{test</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">d</span><span class="w"> </span><span class="err">o</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">Sample</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">goal</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">g</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">an</span><span class="w"> </span><span class="nt">initial</span><span class="w"> </span><span class="nt">state</span><span class="o">;</span>
<span class="w">    </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">t</span><span class="o">=</span><span class="nt">0</span><span class="o">,</span><span class="w"> </span><span class="nt">T</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="nt">Sample</span><span class="w"> </span><span class="nt">an</span><span class="w"> </span><span class="nt">action</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">a_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">from</span><span class="w"> </span><span class="nt">policy</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">pi_</span><span class="p">{</span><span class="err">\phi</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">a_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">mid</span><span class="w"> </span><span class="nt">g</span><span class="o">,</span><span class="w"> </span><span class="nt">h_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">o_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">        </span><span class="nt">Execute</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">action</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">a_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">get</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">new</span><span class="w"> </span><span class="nt">observation</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">o_</span><span class="p">{</span><span class="err">t+1</span><span class="p">}</span><span class="err">\</span><span class="o">);</span>
<span class="w">        </span><span class="nt">if</span><span class="w"> </span><span class="nt">success</span><span class="w"> </span><span class="nt">then</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="nt">n</span><span class="o">=</span><span class="nt">n</span><span class="o">+</span><span class="nt">1</span><span class="err">\</span><span class="o">);</span>
<span class="w">            </span><span class="nt">break</span><span class="o">;</span>
<span class="w">        </span><span class="nt">end</span>
<span class="w">    </span><span class="nt">end</span>
<span class="nt">end</span>
<span class="nt">success</span><span class="w"> </span><span class="nt">rate</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">r</span><span class="o">=</span><span class="nt">n</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nt">N_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{test</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">);</span>
</code></pre></div>

<h2>E Data Gathering Details in VirtualHome</h2>
<p>In this section, we provide more data gathering details in VirtualHome for training the decisionmaking policies. We introduce the expert data collection and active data gathering in Appendix E. 1 and Appendix E.2, respectively.</p>
<h2>E. 1 Expert Data Collection</h2>
<p>VirtualHome-Imitation Learning Dataset. To train the models, we collect a set of expert trajectories in VirtualHome using regression planning (RP) [20]. We follow the implementation of the regression planner used in [32]. Given a task described by goal predicates, the planner generates an action sequence to accomplish this task. As shown in Figure 8, the agent has a belief about the environment, i.e. an imagined distribution of object locations. As the agent explores the environment, its belief of the world becomes closer to the real world. At every step, the agent updates its belief based on the latest observation (see [32]), finds a new plan using the regression planner, and executes the first action of the plan. If the subtask (described by the goal predicate) has been finished, the agent will select a new unfinished subtask, otherwise, the agent will keep doing this subtask until it finishes.
Similarly to previous work [40, 39, 32], we generate training data using a planner that has access to privileged information, such as full observation of the environment and information about the pre-conditions and effects of each action. The planner allows an agent to robustly perform tasks in partially observable environments and generate expert trajectories for training and evaluation. We generate 20,000 trajectories for training and 3,000 trajectories for validation. Each trajectory has a goal, an action sequence, and the corresponding observations after executing each action.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Regression planner. Given a task described by goal predicates, the planner generates an action sequence to accomplish this task. The agent has a belief about the environment, i.e. an imagined distribution of object locations. As the agent explores the environment, its belief of the world becomes closer to the real world. At every step, the agent updates its belief based on the latest observation, finds a new plan using the regression planner, and executes the first action of the plan. If the subtask (described by the goal predicate) has been finished, the agent will select a new unfinished subtask, otherwise, the agent will keep doing this subtask until finish it.</p>
<h1>E. 2 Active Data Gathering</h1>
<p>The algorithm for active data gathering is shown in Algorithm 2. To sample the goal and initial state, we first generate a set of initial states in VirtualHome using the code released by [32]. For each initial state, we are able to get a set of feasible tasks that can be accomplished in this environment. For example, in an initial state, if the apple is on the kitchen table, a feasible task goal could be "put the apple inside the fridge". In contrast, "put the banana inside the fridge" is not a feasible task if there is no banana in the initial state.</p>
<p>We collect 9893 initial states, and randomly sample an initial state and its feasible goal every time when we reset the environment. After each data collection iteration, we obtain a set of new goals using the goal relabel function. We save the goal and its corresponding initial state in the replay buffers and use the same strategy to sample the goal and initial state in the next iteration.</p>
<p>The hindsight relabeling stage is the key component for active data gathering. Here we provide more implementation details of how we relabel "failed" trajectories with new goals in the hindsight relabeling stage. For each "failed" trajectory, we extract its useful sub-trajectories and relabel a task goal $g^{\prime}$ for it. We design a goal relabel function $f_{l}$ that generates a goal based on the sequence of observations and actions. To do this, we first use a hand-designed program to detect what tasks are contained in a "failed" trajectory. This program find useful tasks based on the keywords in the action list. For example in Figure 9, the program knows the trajectory containing a task of "On (apple, kitchen table):1" based on the action "[put] $&lt;$ apple $&gt;&lt;$ kitchentable $&gt;$ ".
The selected sub-trajectories are not always optimal. We thus design a rule to filter out bad trajectories, i.e. for trajectories with the same goal, selecting the "shorter" ones. One example is shown in Figure 10. Suppose that there are two trajectories having the same goal, e.g. "On (apple, kitchen table):1". The first trajectory has actions that are redundant or not related to the task, such as " $[$ walk $]&lt;$ bathroom $&gt;$ " and " $[$ walk $]&lt;$ kitchen $&gt;$ " while the second trajectory is more optimal given the goal. We select the second trajectory and delete the first trajectory from the replay buffer. Note that the "shorter" does not mean fewer actions, but fewer actions that are not related to the task. The hindsight relabeling stage allows sample-efficient learning by reusing the failure cases. The relabeled data are used to train policies in the policy update stage.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Given</span><span class="o">:</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="n">relabel</span><span class="w"> </span><span class="kd">function</span><span class="w"> </span><span class="o">\(</span><span class="n">f_</span><span class="o">{</span><span class="n">l</span><span class="o">}\);</span>
<span class="n">Initialize</span><span class="o">:</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="o">\(\</span><span class="n">pi_</span><span class="o">{\</span><span class="n">phi</span><span class="o">}</span><span class="w"> </span><span class="o">;\)</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="kd">set</span><span class="w"> </span><span class="o">\(</span><span class="n">G</span><span class="o">\);</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="n">replay</span><span class="w"> </span><span class="n">buffer</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">R</span><span class="o">}</span><span class="n">_</span><span class="o">{\</span><span class="n">text</span><span class="w"> </span><span class="o">{</span><span class="n">train</span><span class="w"> </span><span class="o">}}=\{</span><span class="w"> </span><span class="o">\}</span><span class="w"> </span><span class="o">;\)</span><span class="w"> </span><span class="n">validation</span><span class="w"> </span><span class="n">replay</span><span class="w"> </span><span class="n">buffer</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">R</span><span class="o">}</span><span class="n">_</span><span class="o">{\</span><span class="n">text</span><span class="w"> </span><span class="o">{</span><span class="n">val</span><span class="w"> </span><span class="o">}}=\{</span><span class="w"> </span><span class="o">\}\);</span>
<span class="k">for</span><span class="w"> </span><span class="n">iteration</span><span class="w"> </span><span class="o">\(=</span><span class="n">I</span><span class="o">,</span><span class="w"> </span><span class="n">N</span><span class="o">\)</span><span class="w"> </span><span class="k">do</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">example</span><span class="w"> </span><span class="o">\(=</span><span class="n">I</span><span class="o">,</span><span class="w"> </span><span class="n">M</span><span class="o">\)</span><span class="w"> </span><span class="k">do</span>
<span class="w">        </span><span class="n">Sample</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="o">\(</span><span class="n">g</span><span class="o">\)</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="o">\(</span><span class="n">G</span><span class="o">\)</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">initial</span><span class="w"> </span><span class="n">state</span><span class="w"> </span><span class="o">\(</span><span class="n">s_</span><span class="o">{</span><span class="mi">1</span><span class="o">}\);</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="o">\(</span><span class="n">t</span><span class="o">=</span><span class="mi">1</span><span class="o">,</span><span class="w"> </span><span class="n">T</span><span class="o">\)</span><span class="w"> </span><span class="k">do</span>
<span class="w">            </span><span class="n">Sample</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">action</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="o">\(\</span><span class="n">pi_</span><span class="o">{\</span><span class="n">phi</span><span class="o">}\</span><span class="n">left</span><span class="o">(</span><span class="n">a_</span><span class="o">{</span><span class="n">t</span><span class="o">}</span><span class="w"> </span><span class="o">\</span><span class="n">mid</span><span class="w"> </span><span class="n">g</span><span class="o">,</span><span class="w"> </span><span class="n">h_</span><span class="o">{</span><span class="n">t</span><span class="o">},</span><span class="w"> </span><span class="n">o_</span><span class="o">{</span><span class="n">t</span><span class="o">}\</span><span class="n">right</span><span class="o">)\)</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">sample</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">action</span><span class="w"> </span><span class="n">randomly</span><span class="o">;</span>
<span class="w">            </span><span class="n">Execute</span><span class="w"> </span><span class="o">\(</span><span class="n">a_</span><span class="o">{</span><span class="n">t</span><span class="o">}\)</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">obtain</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">observation</span><span class="w"> </span><span class="o">\(</span><span class="n">o_</span><span class="o">{</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="o">}\);</span>
<span class="w">        </span><span class="n">end</span>
<span class="w">        </span><span class="n">Store</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">trajectory</span><span class="w"> </span><span class="o">\(\</span><span class="n">left</span><span class="o">(</span><span class="n">o_</span><span class="o">{</span><span class="mi">1</span><span class="o">},</span><span class="w"> </span><span class="n">a_</span><span class="o">{</span><span class="mi">1</span><span class="o">},</span><span class="w"> </span><span class="o">\</span><span class="n">cdots</span><span class="o">,</span><span class="w"> </span><span class="n">o_</span><span class="o">{</span><span class="n">T</span><span class="o">},</span><span class="w"> </span><span class="n">a_</span><span class="o">{</span><span class="n">T</span><span class="o">},</span><span class="w"> </span><span class="n">g</span><span class="o">\</span><span class="n">right</span><span class="o">)\)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">replay</span><span class="w"> </span><span class="n">buffer</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">R</span><span class="o">}</span><span class="n">_</span><span class="o">{\</span><span class="n">text</span><span class="w"> </span><span class="o">{</span><span class="n">train</span><span class="w"> </span><span class="o">}}\)</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">R</span><span class="o">}</span><span class="n">_</span><span class="o">{\</span><span class="n">text</span><span class="w"> </span><span class="o">{</span><span class="n">val</span><span class="w"> </span><span class="o">}}\);</span>
<span class="w">    </span><span class="n">end</span>
<span class="w">    </span><span class="n">Relabel</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">failure</span><span class="w"> </span><span class="n">trajectory</span><span class="w"> </span><span class="o">\(</span><span class="n">d</span><span class="o">=\</span><span class="n">left</span><span class="o">(</span><span class="n">o_</span><span class="o">{</span><span class="mi">1</span><span class="o">},</span><span class="w"> </span><span class="n">a_</span><span class="o">{</span><span class="mi">1</span><span class="o">},</span><span class="w"> </span><span class="o">\</span><span class="n">cdots</span><span class="o">,</span><span class="w"> </span><span class="n">o_</span><span class="o">{</span><span class="n">T</span><span class="o">},</span><span class="w"> </span><span class="n">a_</span><span class="o">{</span><span class="n">T</span><span class="o">}\</span><span class="n">right</span><span class="o">)\)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">replay</span><span class="w"> </span><span class="n">buffers</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="kd">get</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">goal</span>
<span class="w">        </span><span class="o">\(</span><span class="n">g</span><span class="o">^{\</span><span class="n">prime</span><span class="o">}=</span><span class="n">f_</span><span class="o">{</span><span class="n">l</span><span class="o">}(</span><span class="n">d</span><span class="o">)\);</span>
<span class="w">    </span><span class="n">Put</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">goals</span><span class="w"> </span><span class="o">\(</span><span class="n">g</span><span class="o">^{\</span><span class="n">prime</span><span class="o">}\)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="kd">set</span><span class="w"> </span><span class="o">\(</span><span class="n">G</span><span class="o">\);</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="o">\(</span><span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="o">,</span><span class="w"> </span><span class="n">K</span><span class="o">\)</span><span class="w"> </span><span class="k">do</span>
<span class="w">        </span><span class="n">repeat</span>
<span class="w">            </span><span class="n">Sample</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">R</span><span class="o">}</span><span class="n">_</span><span class="o">{\</span><span class="n">text</span><span class="w"> </span><span class="o">{</span><span class="n">train</span><span class="w"> </span><span class="o">}}\)</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">update</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="o">\(\</span><span class="n">pi_</span><span class="o">{\</span><span class="n">phi</span><span class="o">}\);</span>
<span class="w">        </span><span class="n">until</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="n">episode</span><span class="w"> </span><span class="n">ends</span><span class="o">;</span>
<span class="w">        </span><span class="n">Get</span><span class="w"> </span><span class="n">validation</span><span class="w"> </span><span class="n">accuracy</span><span class="w"> </span><span class="n">using</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">R</span><span class="o">}</span><span class="n">_</span><span class="o">{\</span><span class="n">text</span><span class="w"> </span><span class="o">{</span><span class="n">val</span><span class="w"> </span><span class="o">}}\);</span>
<span class="w">    </span><span class="n">end</span>
<span class="w">    </span><span class="o">\(\</span><span class="n">pi_</span><span class="o">{\</span><span class="n">phi</span><span class="o">}=\</span><span class="n">pi_</span><span class="o">{\</span><span class="n">text</span><span class="w"> </span><span class="o">{</span><span class="n">val_best</span><span class="w"> </span><span class="o">}}\)</span>
<span class="n">end</span>
</code></pre></div>

<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Extract the useful sub-trajectory and relabel a task goal:
... [walk] <kitchen>; [walk] <kitchentable1>; ...; [walk]<kitchentable>; [put] <apple> <kitchentable>; [walk] <bedroom>; ...
On (apple, kitchen table): 1
Figure 9: We first use a hand-designed program to detect what tasks are contained in the collected trajectory. This program find tasks based on the keywords in the action list. For example, the program knows the trajectory containing a task of "On(apple, kitchen table):1" based on the action "[put] $&lt;$ apple $&gt;&lt;$ kitchentable $&gt;$ ". Then the program extracts all previous actions related to this task using hand-designed rules.</p>
<h1>F Test Sets in VirtualHome</h1>
<p>In this section, we provide more details of each test set. We first introduce the test sets used for evaluating the proposed model trained on expert data, i.e. LID, in Section F.1. We then show the test sets used for evaluating the proposed model with active data gathering, i.e. LID-ADG, in Section F.2.</p>
<h2>F. 1 LID Test Sets</h2>
<p>In Section 6.1, we compared the proposed method and baselines trained on expert data. In Table 7, we provide a detailed description of each test subset, including the count of goal predicate types and the number of goal predicates in each task. The In-Distribution setting has 37 goal predicates in total and each task has $2 \sim 10$ goal predicates. The tasks are drawn from the same distribution as the training tasks. The Novel Scenes setting also has 37 goal predicates and each task has $2 \sim 10$ goal predicates. The objects are randomly placed in the initial environment. The Novel Tasks setting has 22 goal predicates in total and each task has $2 \sim 8$ goal predicates. The tasks are never seen during training.</p>
<div class="codehilite"><pre><span></span><code><span class="nl">Goal</span><span class="p">:</span><span class="w"> </span><span class="k">On</span><span class="w"> </span><span class="p">(</span><span class="n">apple</span><span class="p">,</span><span class="w"> </span><span class="n">kitchen</span><span class="w"> </span><span class="nc">table</span><span class="p">)</span><span class="err">:</span><span class="w"> </span><span class="mi">1</span>
<span class="k">Action</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="mi">1</span><span class="err">:</span>
<span class="p">...</span><span class="w"> </span><span class="o">[</span><span class="n">walk</span><span class="o">]</span><span class="w"> </span><span class="o">&lt;</span><span class="n">livingroom</span><span class="o">&gt;</span><span class="p">;</span><span class="w"> </span><span class="o">[</span><span class="n">grab</span><span class="o">]</span><span class="w"> </span><span class="o">&lt;</span><span class="n">apple</span><span class="o">&gt;</span><span class="p">;</span><span class="w"> </span><span class="o">[</span><span class="n">walk</span><span class="o">]</span><span class="w"> </span><span class="o">&lt;</span><span class="n">kitchen</span><span class="o">&gt;</span><span class="p">;</span><span class="w"> </span><span class="o">[</span><span class="n">walk</span><span class="o">]</span><span class="w"> </span><span class="o">&lt;</span><span class="n">bathroom</span><span class="o">&gt;</span><span class="p">;</span><span class="w"> </span><span class="o">[</span><span class="n">walk</span><span class="o">]</span><span class="w"> </span><span class="o">&lt;</span><span class="n">kitchen</span><span class="o">&gt;</span><span class="p">;</span><span class="w"> </span><span class="o">[</span><span class="n">put</span><span class="o">]</span><span class="w"> </span><span class="o">&lt;</span><span class="n">apple</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&lt;</span><span class="n">kitchentable</span><span class="o">&gt;</span><span class="w"> </span><span class="p">...</span>
<span class="k">Action</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="mi">2</span><span class="err">:</span>
<span class="p">...</span><span class="o">[</span><span class="n">walk</span><span class="o">]</span><span class="w"> </span><span class="o">&lt;</span><span class="n">livingroom</span><span class="o">&gt;</span><span class="p">;</span><span class="w"> </span><span class="o">[</span><span class="n">grab</span><span class="o">]</span><span class="w"> </span><span class="o">&lt;</span><span class="n">apple</span><span class="o">&gt;</span><span class="p">;</span><span class="w"> </span><span class="o">[</span><span class="n">walk</span><span class="o">]</span><span class="w"> </span><span class="o">&lt;</span><span class="n">kitchen</span><span class="o">&gt;</span><span class="p">;</span><span class="w"> </span><span class="o">[</span><span class="n">put</span><span class="o">]</span><span class="w"> </span><span class="o">&lt;</span><span class="n">apple</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&lt;</span><span class="n">kitchentable</span><span class="o">&gt;</span><span class="w"> </span><span class="p">...</span>
</code></pre></div>

<p>Figure 10: Suppose there are two trajectories having the same goal, e.g. "On (apple, kitchen table): 1". The first trajectory has actions that are redundant or not related to the task, such as " $[$ walk $]&lt;$ bathroom $&gt;$ " and " $[$ walk $]<k i t c h e n>$ " while the second trajectory is more optimal given the goal. We select the second trajectory and delete the first trajectory from the replay buffer. Note that the "shorter" does not mean fewer actions, but fewer actions that are not related to the task.</p>
<p>Table 7: Test sets used for evaluating the proposed model trained on the expert data. We show the count of goal predicate types and the number of goal predicates used in each task.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Test Sets</th>
<th style="text-align: center;">Predicate Types</th>
<th style="text-align: center;">#Predicate Per Task</th>
<th style="text-align: center;">Compared with the training set</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">In-Distribution</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">$2 \sim 10$</td>
<td style="text-align: center;">Tasks are drawn from the same distribution as training tasks.</td>
</tr>
<tr>
<td style="text-align: left;">Novel Scenes</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">$2 \sim 10$</td>
<td style="text-align: center;">The objects are randomly placed in the initial environment.</td>
</tr>
<tr>
<td style="text-align: left;">Novel Tasks</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">$2 \sim 8$</td>
<td style="text-align: center;">Tasks are never seen during training.</td>
</tr>
</tbody>
</table>
<h1>F. 2 LID-ADG Test Sets</h1>
<p>As we have mentioned in the main paper Section 9, one limitation of active data gathering is that it relies on hand-designed rules for task relabeling. In addition, it is sometimes challenging to define effective rules to extract useful sub-trajectories and get high-quality hindsight labels, especially when trajectories are long and tasks become more complex. Thus we only relabel short sub-trajectories, where the goal consists of a single goal predicate, e.g. "On (apple, kitchen table): 1 ". During testing, we evaluate the success rate of approaches on solving such tasks as well, i.e. the count of the goal predicate equals to 1 . The types of goal predicates are the same as Section F.1, i.e. 37 goal predicates in the In-Distribution setting and the Novel Scenes setting, and 22 goal predicates in the Novel Tasks setting.</p>
<h2>G Visualization of Attention Weights</h2>
<p>To better understand how does LM pre-trained policies make decisions, we visualize the attention weights from the self-attention layers of GPT-2 [43] in Figure 11 and Figure 12. In the inference time, when we are decoding the actions, we save the self-attention weights with respect to different layers and different heads. Then, we use BertViz library [44] to visualize normalized attention weights. We show the attention weights from the input to the output of LID-Text (Ours). The order of tokens in the input and ouput is observation, goal, and history. In Figure 11 and Figure 12, the left side is the query side. The boldness of the lines is proportional with the attention weight.
Figure 11 illustrates the attention weights of a layer named "Head 3 Layer 2". We show attention weights on two different tasks. We find that "Head 3 Layer 2" can capture objects in the goal predicates, such as "wineglass" and "cutleryfork" in the left figure, and "pancake" and "chicken" in the right figure (the figures are cropped for visualization).
Figure 12 illustrates the attention weights of layers named "Head 1 Layer 2" (left) and "Head 4 Layer 11" (right). Given the goal predicates, history, and the current observation, the policy predicts the next action as "grab milk". We find that "Head 1 Layer 2" is able to capture objects in the goal predicates, such as "milk", "pancake", and "chicken" while "Head 4 Layer 11" focuses on the interacted object in the predicted action, such as "milk".
The attention weights from different self-attention layers are significantly different-some selfattention layers assign high attention weight to objects in the goal predicates while some layers focus on the interacted object. There are also some layers that do not have interpretable meanings. The</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ Note that the results of LID-Text in Figure 3 and results of LID-ADG in Table 2 are not directly comparable because the difficulty level of the evaluated tasks are different. See Appendix F for more details.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6934 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6934</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6934</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-134.html">extraction-schema-134</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‚Äëreflection or self‚Äëcritique, including the specific reflection method, number of generate‚Äëthen‚Äëreflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-268681670</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.16427v4.pdf" target="_blank">Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) are emerging as promising approaches to enhance session-based recommendation (SBR), where both prompt-based and fine-tuning-based methods have been widely investigated to align LLMs with SBR. However, the former methods struggle with optimal prompts to elicit the correct reasoning of LLMs due to the lack of task-specific feedback, leading to unsatisfactory recommendations. Although the latter methods attempt to fine-tune LLMs with domain-specific knowledge, they face limitations such as high computational costs and reliance on open-source backbones. To address such issues, we propose a Reflective Reinforcement Large Language Model (Re2LLM) for SBR, guiding LLMs to focus on specialized knowledge essential for more accurate recommendations effectively and efficiently. In particular, we first design the Reflective Exploration Module to effectively extract knowledge that is readily understandable and digestible by LLMs. To be specific, we direct LLMs to examine recommendation errors through self-reflection and construct a knowledge base (KB) comprising hints capable of rectifying these errors. To efficiently elicit the correct reasoning of LLMs, we further devise the Reinforcement Utilization Module to train a lightweight retrieval agent. It learns to select hints from the constructed KB based on the task-specific feedback, where the hints can serve as guidance to help correct LLMs reasoning for better recommendations. Extensive experiments on multiple real-world datasets demonstrate that our method consistently outperforms state-of-the-art methods.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6934.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6934.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‚Äëreflection or self‚Äëcritique, including the specific reflection method, number of generate‚Äëthen‚Äëreflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Re2LLM-ReflectiveExploration</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflective Exploration Module (Re2LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-driven self-reflection pipeline that elicits LLM self-critique on mistaken session-based recommendation outputs and summarizes those critiques into short, reusable 'hints' stored in a knowledge base for later retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source GPT-4 API used as a frozen LLM backbone queried via natural-language prompts (no fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Multi-round self-reflection (Reflective Exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate a top-K recommendation list (basic prompt); for sessions where the ground-truth is missed, run a sequence of reflection prompts: (1) ask the LLM to infer possible mistakes without revealing the ground truth, (2) reveal the ground truth and ask the LLM to re-analyze why it missed the item, and (3) ask the LLM to produce concise, non-leaking hints to rectify those errors; the outputs are filtered and stored as hints.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Session-based recommendation (Hetrec2011-Movielens, Amazon Game)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Next-item prediction in anonymous sessions: predict the next item a user will interact with given the sequence of items in the current session.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Hit Rate (HR@5, HR@10) and NDCG (NDCG@5, NDCG@10)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper-level noted limitations and failure modes: (1) numeric iteration count / formal convergence of the reflection loop not specified; (2) low-quality or irrelevant hints can harm performance ‚Äî random hint selection ('w/o-DRL(RAN)') produced the worst results in ablation and can mislead the LLM; (3) too-large hint knowledge bases (e.g., >50) degrade performance because retrieval optimization becomes harder; (4) retrieval agent can be under-trained when very few training samples (<50) are used, leading to worse-than-random hint selection; (5) redundancy of hints requires automated filtering or maintenance overhead; (6) reliance on closed-source GPT-4 API and inference cost (authors report average cost ~USD 0.015 per sample for hint generation and evaluation) ‚Äî practical cost and API dependence are limitations; (7) the method uses a single-shot flow of prompts (Prompts 2‚Äì4) per incorrect session rather than deeply recursive multi-iteration self-revision, so the form of reflection is bounded by the prompt design; (8) the approach does not guarantee to correct all error types and may fail when hints are not expressive of the true causal error (case studies show random hints sometimes remove a correct prediction).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6934.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6934.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‚Äëreflection or self‚Äëcritique, including the specific reflection method, number of generate‚Äëthen‚Äëreflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DRDT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DRDT: Dynamic Reflection with Divergent Thinking for LLM-based Sequential Recommendation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced prior method that triggers iterative, per-session LLM reflection until the model outputs the ground-truth item; discussed as related work and contrasted with Re2LLM's approach to build a reusable hint KB.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DRDT: Dynamic Reflection with Divergent Thinking for LLM-based Sequential Recommendation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Case-by-case iterative reflection (per-session until ground truth)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The LLM performs iterative reflection on a specific session (potentially multiple cycles) until the ground-truth next item is produced; characterized as a case-by-case reflection strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>recursive self-critique / iterative reflection</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LLM-based sequential recommendation (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Next-item/session recommendation by iteratively refining an output for a specific session until success.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Cited limitation in this paper: the case-by-case reflection strategy (DRDT) hinders summarization of specialized knowledge across sessions and thus is less suitable for constructing a global hint knowledge base for reuse across sessions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6934.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6934.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‚Äëreflection or self‚Äëcritique, including the specific reflection method, number of generate‚Äëthen‚Äëreflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine: Iterative Refinement with Self-Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work cited that applies an LLM feedback-and-refine loop to improve outputs across NLP tasks by having the model critique and then revise its own outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Refine: Iterative Refinement with Self-Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-refinement / feedback-and-refine</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Iterative loop where the model generates an answer, produces self-feedback or critique, and then refines the answer based on that feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-critique-revise (iterative)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General NLP tasks (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improving model outputs across various NLP tasks via iterative self-feedback and revision.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mentioned in related work context: while effective across NLP tasks, such iterative self-refinement approaches are not directly evaluated in the paper for recommendation, and the Re2LLM authors argue for constructing global, reusable hints rather than per-case iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-Refine: Iterative Refinement with Self-Feedback <em>(Rating: 2)</em></li>
                <li>DRDT: Dynamic Reflection with Divergent Thinking for LLM-based Sequential Recommendation <em>(Rating: 2)</em></li>
                <li>Automatic Prompt Optimization with "Gradient Descent" and Beam Search <em>(Rating: 1)</em></li>
                <li>Reflexion: Language Agents with Verbal Reinforcement Learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6934",
    "paper_id": "paper-268681670",
    "extraction_schema_id": "extraction-schema-134",
    "extracted_data": [
        {
            "name_short": "Re2LLM-ReflectiveExploration",
            "name_full": "Reflective Exploration Module (Re2LLM)",
            "brief_description": "A prompt-driven self-reflection pipeline that elicits LLM self-critique on mistaken session-based recommendation outputs and summarizes those critiques into short, reusable 'hints' stored in a knowledge base for later retrieval.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-4",
            "model_description": "Closed-source GPT-4 API used as a frozen LLM backbone queried via natural-language prompts (no fine-tuning).",
            "model_size": null,
            "reflection_method_name": "Multi-round self-reflection (Reflective Exploration)",
            "reflection_method_description": "Generate a top-K recommendation list (basic prompt); for sessions where the ground-truth is missed, run a sequence of reflection prompts: (1) ask the LLM to infer possible mistakes without revealing the ground truth, (2) reveal the ground truth and ask the LLM to re-analyze why it missed the item, and (3) ask the LLM to produce concise, non-leaking hints to rectify those errors; the outputs are filtered and stored as hints.",
            "iteration_type": "generate-then-reflect",
            "num_iterations": null,
            "task_name": "Session-based recommendation (Hetrec2011-Movielens, Amazon Game)",
            "task_description": "Next-item prediction in anonymous sessions: predict the next item a user will interact with given the sequence of items in the current session.",
            "evaluation_metric": "Hit Rate (HR@5, HR@10) and NDCG (NDCG@5, NDCG@10)",
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": true,
            "limitations_or_failure_cases": "Paper-level noted limitations and failure modes: (1) numeric iteration count / formal convergence of the reflection loop not specified; (2) low-quality or irrelevant hints can harm performance ‚Äî random hint selection ('w/o-DRL(RAN)') produced the worst results in ablation and can mislead the LLM; (3) too-large hint knowledge bases (e.g., &gt;50) degrade performance because retrieval optimization becomes harder; (4) retrieval agent can be under-trained when very few training samples (&lt;50) are used, leading to worse-than-random hint selection; (5) redundancy of hints requires automated filtering or maintenance overhead; (6) reliance on closed-source GPT-4 API and inference cost (authors report average cost ~USD 0.015 per sample for hint generation and evaluation) ‚Äî practical cost and API dependence are limitations; (7) the method uses a single-shot flow of prompts (Prompts 2‚Äì4) per incorrect session rather than deeply recursive multi-iteration self-revision, so the form of reflection is bounded by the prompt design; (8) the approach does not guarantee to correct all error types and may fail when hints are not expressive of the true causal error (case studies show random hints sometimes remove a correct prediction).",
            "uuid": "e6934.0",
            "source_info": {
                "paper_title": "Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "DRDT",
            "name_full": "DRDT: Dynamic Reflection with Divergent Thinking for LLM-based Sequential Recommendation",
            "brief_description": "A referenced prior method that triggers iterative, per-session LLM reflection until the model outputs the ground-truth item; discussed as related work and contrasted with Re2LLM's approach to build a reusable hint KB.",
            "citation_title": "DRDT: Dynamic Reflection with Divergent Thinking for LLM-based Sequential Recommendation",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "model_size": null,
            "reflection_method_name": "Case-by-case iterative reflection (per-session until ground truth)",
            "reflection_method_description": "The LLM performs iterative reflection on a specific session (potentially multiple cycles) until the ground-truth next item is produced; characterized as a case-by-case reflection strategy.",
            "iteration_type": "recursive self-critique / iterative reflection",
            "num_iterations": null,
            "task_name": "LLM-based sequential recommendation (as cited)",
            "task_description": "Next-item/session recommendation by iteratively refining an output for a specific session until success.",
            "evaluation_metric": null,
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Cited limitation in this paper: the case-by-case reflection strategy (DRDT) hinders summarization of specialized knowledge across sessions and thus is less suitable for constructing a global hint knowledge base for reuse across sessions.",
            "uuid": "e6934.1",
            "source_info": {
                "paper_title": "Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Self-Refine (cited)",
            "name_full": "Self-Refine: Iterative Refinement with Self-Feedback",
            "brief_description": "Prior work cited that applies an LLM feedback-and-refine loop to improve outputs across NLP tasks by having the model critique and then revise its own outputs.",
            "citation_title": "Self-Refine: Iterative Refinement with Self-Feedback",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "model_size": null,
            "reflection_method_name": "Self-refinement / feedback-and-refine",
            "reflection_method_description": "Iterative loop where the model generates an answer, produces self-feedback or critique, and then refines the answer based on that feedback.",
            "iteration_type": "generate-critique-revise (iterative)",
            "num_iterations": null,
            "task_name": "General NLP tasks (as cited)",
            "task_description": "Improving model outputs across various NLP tasks via iterative self-feedback and revision.",
            "evaluation_metric": null,
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Mentioned in related work context: while effective across NLP tasks, such iterative self-refinement approaches are not directly evaluated in the paper for recommendation, and the Re2LLM authors argue for constructing global, reusable hints rather than per-case iterations.",
            "uuid": "e6934.2",
            "source_info": {
                "paper_title": "Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "DRDT: Dynamic Reflection with Divergent Thinking for LLM-based Sequential Recommendation",
            "rating": 2,
            "sanitized_title": "drdt_dynamic_reflection_with_divergent_thinking_for_llmbased_sequential_recommendation"
        },
        {
            "paper_title": "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search",
            "rating": 1,
            "sanitized_title": "automatic_prompt_optimization_with_gradient_descent_and_beam_search"
        },
        {
            "paper_title": "Reflexion: Language Agents with Verbal Reinforcement Learning",
            "rating": 1,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        }
    ],
    "cost": 0.0124205,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation
19 Apr 2024</p>
<p>Ziyan Wang wangwy@ntu.edu.sg 
Yingpeng Du 
Zhu Sun 
Haoyan Chua haoyan001@e.ntu.edu.sg 
Kaidong Feng kaidong3762@gmail.com 
Wenya Wang 
Jie Zhang zhangj@ntu.edu.sg </p>
<p>Nanyang Technological University
Singapore Yingpeng Du</p>
<p>Nanyang Technological University Singapore Zhu Sun</p>
<p>Agency for Science, Technology and Research
Singapore Haoyan Chua</p>
<p>Nanyang Technological University
Singapore Kaidong Feng</p>
<p>Yanshan University China Wenya Wang</p>
<p>Nanyang Technological University Singapore Jie Zhang</p>
<p>Nanyang Technological University
Singapore</p>
<p>Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation
19 Apr 2024880D5B09E0F9766BB334D32E8A7730A2arXiv:2403.16427v4[cs.AI]Session-based RecommendationLarge Language ModelSelf Reflection
Large Language Models (LLMs) are emerging as promising approaches to enhance session-based recommendation (SBR), where both prompt-based and fine-tuning-based methods have been widely investigated to align LLMs with SBR.However, the former methods struggle with optimal prompts to elicit the correct reasoning of LLMs due to the lack of task-specific feedback, leading to unsatisfactory recommendations.Although the latter methods attempt to fine-tune LLMs with domain-specific knowledge, they face limitations such as high computational costs and reliance on opensource backbones.To address such issues, we propose a Reflective Reinforcement Large Language Model (Re2LLM) for SBR, guiding LLMs to focus on specialized knowledge essential for more accurate recommendations effectively and efficiently.In particular, we first design the Reflective Exploration Module to effectively extract knowledge that is readily understandable and digestible by LLMs.To be specific, we direct LLMs to examine recommendation errors through self-reflection and construct a knowledge base (KB) comprising hints capable of rectifying these errors.To efficiently elicit the correct reasoning of LLMs, we further devise the Reinforcement Utilization Module to train a lightweight retrieval agent.It learns to select hints from the constructed KB based on the taskspecific feedback, where the hints can serve as guidance to help correct LLMs reasoning for better recommendations.Extensive experiments on multiple real-world datasets demonstrate that our method consistently outperforms state-of-the-art methods.CCS CONCEPTS‚Ä¢ Information systems ‚Üí Recommender systems.</p>
<p>INTRODUCTION</p>
<p>Session-based recommendation (SBR) [2,10,14,21,51,57] plays a crucial role in real-world applications.It aims to capture users' dynamic preferences and predicts the next item that users may prefer based on previous interactions within a session.However, the user-item interactions are often scarce, and users' profiles are inaccessible in anonymous sessions, hindering the accurate recommendation result due to data sparsity and cold-start issues [28,36].</p>
<p>Large Language Models (LLMs) have emerged to show potential in addressing these issues with their extensive knowledge and sophisticated reasoning capabilities.Recently, numerous methods have integrated LLMs into recommender systems (RSs), primarily through two ways: prompt-based methods and fine-tuningbased methods.The former methods exploit in-context learning and prompt optimization [8,16,37,42] to engage LLMs (e.g., Chat-GPT 1 ) as recommenders without training as shown in Fig. 1 (a).However, the crafted prompts not only require extensive expert knowledge and human labor but also may not align well with LLMs' understanding of SBR tasks, making LLMs vulnerable to hallucinations without effective supervision.The latter methods focus on fine-tuning LLMs [1,9,50,60] with domain-specific knowledge, such as user-item interactions, in a supervised manner as shown in Fig. 1 (b).However, such methods often suffer from high computational costs, catastrophic forgetting, and reliance on open-source backbones.These drawbacks restrict the practical application of existing LLM-based methods for SBR.</p>
<p>In this paper, we propose to direct LLMs to effectively and efficiently leverage specialized knowledge for recommendation without costly and inconvenient fine-tuning.However, there remain two primary challenges to this goal: (1) How can we effectively extract and craft specialized knowledge embedded within extensive useritem interactions to better align with LLM comprehension? (2) How can we enable LLMs to utilize the specialized knowledge efficiently for better recommendations, without relying on resource-intensive supervised fine-tuning?</p>
<p>To overcome these challenges, we propose a Reflective Reinforcement Large Language Model (Re2LLM) for SBR, aiming to capture and utilize specialized knowledge embedded in the extensive useritem interaction data by LLMs for more accurate recommendation.The proposed method consists of two main components: the Reflective Exploration Module and the Reinforcement Utilization Module.</p>
<p>‚Ä¢ Reflective Exploration Module leverages the self-reflection of LLMs [27,29] to extract specialized knowledge that is comprehensible and digestible by LLMs for SBR.Specifically, we employ LLMs to identify common errors in their responses, and then generate corresponding specialized knowledge (i.e., hints) to rectify these errors through LLMs' self-reflections.Hence, the specialized knowledge can align seamlessly with LLM comprehension as it is summarized by the LLM itself.Besides, we construct a hint knowledge base that serves as a pool to maintain specialized knowledge (hints), using an automated filtering strategy to ensure the effectiveness and diversity of retained hints.‚Ä¢ Reinforcement Utilization Module employs a lightweight retrieval agent to select relevant specialized knowledge, thus eliciting the correct reasoning of LLMs without costly fine-tuning.Specifically, the retrieval agent is trained to select accurate hints from the knowledge base, which can prevent LLMs from potential errors in their inference process.To overcome the absence of explicit labels about the effects of retrievals, we employ deep reinforcement learning (DRL) to simulate the real-world RSs for the agent.In detail, we first design an agent that selects hints (i.e., action) from the hint knowledge base by considering the session's contextual information (i.e., observation state).Then, we measure the improvement (i.e., reward) of recommendation results owing to the selected hint, and use them (rewards, observation states, and actions) as the task-specific feedback to update the agent via Proximal Policy Optimization (PPO) [33] strategy.</p>
<p>In summary, as illustrated in Fig. 1 (c), we develop a new learning paradigm to direct LLMs to effectively explore specialized knowledge and efficiently utilize it for more accurate SBR.Although the LLM backbone remains frozen, it can be guided by a lightweight retrieval that consumes task-specific feedback without costly finetuning.Therefore, our method combines the strengths of the largescale LLM's effective reasoning capabilities and the efficiency of the lightweight retrieval model training.The key contributions of this paper are three-fold:</p>
<p>‚Ä¢ We propose a new learning paradigm beyond in-context learning and fine-tuning of LLMs, which seamlessly bridges between general LLMs and specific recommendation tasks.It alleviates the issues of unaligned knowledge and costly supervised fine-tuning for LLMs, contributing to better SBR predictions.</p>
<p>LITERATURE REVIEW 2.1 Session-based Recommendation</p>
<p>SBR methods learn to model users' preferences and make recommendations based on short and dynamic sessions.The early classic work FPMC [32] combines Matrix Factorization [20] and Markov Chain to capture sequential patterns and long-term user preference.</p>
<p>With the development of deep learning technologies, various advanced techniques have been widely applied in SBR.Hidasi et al. [13] first propose to use the Recurrent Neural Network (RNN) for SBR due to its strength in modeling sequential session interactions.</p>
<p>Several methods propose variants by data augmentation [39], novel ranking loss function [12], and mixture-channel purpose routing networks (MCPRN) [43].Furthermore, the attention mechanism [41] is introduced to capture more informative item representations for users' dynamic preferences for SBR, such as NARM [22] and STAMP [26].Recently, graph-based methods [24,30,47,49,54] leverage the Graph Neural Network (GNN) to better learn highorder transitions within the sessions from the graph structure for SBR.For example, GCEGNN [45] builds a global-level graph along with the item-level graph to exploit item transitions over all sessions for enhancement.Besides adopting different networks in SBR, many studies explore auxiliary information (e.g., attribute, description text) [3,15,48,59] for better session profiling.For example, MMSBR [58] leverages both descriptive and numeric item attributes to characterize user intents.Nevertheless, these methods may still suffer from inadequate user-item interactions and limited auxiliary information, hindering the accuracy of recommendation results.</p>
<p>Large Language Model for Recommendation</p>
<p>LLMs have emerged to show potential in addressing the aforementioned issues with their extensive knowledge and sophisticated reasoning capability, gaining their popularity for recommendation tasks.Among LLM-based recommendation methods, most of the existing works attempt to utilize the LLM in two key strategies: prompt-based methods and fine-tuning (FT)-based methods.</p>
<p>Prompt-based methods [4,6,8,11,25,56] retrieve information from recommendation data (e.g., historical user-item interactions) for direct outputs through prompt enhancement.LLMRank [16] and NIR [42] are two representatives that utilize prompt templates to extract dynamic preferences in anonymous sessions for SBR.</p>
<p>To enrich LLMs with task-specific supervision, FT-based methods adopt either fully [9,19,23,50] or parameter-efficient (PEFT) approaches [1,7,18,55], such as LoRA [17], to fine-tune pre-trained LLMs for recommendation tasks.For example, with PEFT, TALL-Rec [1] bridges the gap between LLMs and the recommendation tasks.PALR [50] fully fine-tunes LLaMA-7b [40] based on historical interactions to improve sequential recommendation performance.However, their effectiveness is constrained by the substantial computational demands, reliance on the availability of open-source LLM backbones, and inferior capabilities compared with larger-scale language models such as ChatGPT.To address these challenges, we employ a lightweight retrieval agent to efficiently select relevant specialized knowledge summarized by LLMs, to elicit the correct reasoning of LLMs without costly fine-tuning.</p>
<p>Self-reflection in Large Language Models</p>
<p>Recent advances in prompting strategies have effectively enhanced LLMs' ability to handle complex tasks.Chain-of-Thought [46] and Tree-of-Thought [52] strategies allow LLMs to reason through a specified path.However, LLMs are still prone to hallucination and incorrect reasoning [29].Consequently, much effort is devoted to exploring LLMs' self-reflection [27,31,34,53] capabilities, where they iteratively refine outputs based on self-generated reviews or feedback without additional training.For example, Madaan et al. [27] apply this feedback-and-refine framework to improve LLMs' performance across NLP tasks.Pryzant et al. [31] improve prompts by having an LLM critique on generations and then refine them based on its feedback.Yao et al. [53] train a retrospective language model as a prompt generator, using the Proximal Policy Optimization (PPO) algorithm [33] based on environment-specific rewards for optimization.However, the potential of LLMs' self-reflection in recommendation remains underexplored.To the best of our knowledge, only DRDT [44] is related to reflection for recommendation, which triggers LLMs to conduct the iterative reflection on a specific session until hitting the ground truth item.However, it is essentially a case-by-case reflection strategy, which hinders summarising the specialized knowledge from global sessions.To this end, we propose to explore different sessions to maintain a hint knowledge base, whose knowledge (i.e., hints) can be utilized to enhance LLMs reasoning for all sessions.</p>
<p>METHODOLOGY</p>
<p>In this section, we present our proposed Re2LLM approach, whose overall architecture is illustrated in Fig. 2. Re2LLM comprises two key components: the Reflective Exploration Module and the Reinforcement Utilization Module.The Reflective Exploration Module facilitates the self-reflection of LLMs in identifying and summarizing their inference errors for SBR, thereby generating hints (known as specialized knowledge) to avoid these potential errors.These hints, further maintained by the automated filtering strategy to ensure their effectiveness and diversity, are stored in a knowledge base.The Reinforcement Utilization Module employs DRL to train a lightweight retrieval agent based on task-specific feedback without costly fine-tuning.The agent learns to select relevant hints to guide LLMs to mitigate potential errors in future inference.Finally, the inference is conducted by the LLM to deliver recommendations enhanced by these hints.For ease of presentation, we first introduce the notations used in this paper and provide problem formulation.</p>
<p>Notations and Problem Formulation.Let V = { 1 ,  2 , ...,   } denote the item set with  items.Each session  with  interacted items is denoted as  = {  1 ,   2 , ...,    }, where    ‚àà V.In addition, we are supposed to have side information on items, such as titles, genres, and actors of movies.Compared to traditional recommendation tasks, users' historical behaviors (i.e., interactions with items) on other sessions and their profiles are inaccessible in SBR due to the anonymous nature of sessions.The task of SBR is to predict the next item   +1 that the user is likely to interact with based on session history .For clarity, we denote the prompts designed in our methodology as   where  is the index.The notation  (  ) indicates the output of the LLM backbone given prompt   .</p>
<p>Reflective Exploration Module</p>
<p>Intuitively, accurate inference of LLMs requires domain-specific knowledge for recommendation tasks.However, the main challenge lies in that the domain-specific knowledge is usually embedded in the massive user-item interaction records, which may not well align with LLMs' comprehension.Alternatively stated, it is essential to extract the specialized knowledge that is intelligible for LLMs regarding recommendation tasks.Therefore, we propose to leverage LLMs' strong self-reflection capability for the extraction of specialized knowledge (i.e., hints) as LLMs can understand what they have summarized by themselves more easily (Section 3.1.1).Additionally, we introduce an automated filtering strategy (Section 3.1.2)that maintains effective and diverse hints to construct a knowledge base for further utilization.</p>
<p>3.1.1Multi-Round Self-Reflection Generation.To activate the selfreflection ability of LLMs, we propose to identify errors in LLMs' inference by comparing their predictions against the ground truth.Specifically, we first instruct the LLM to generate a ranking list  () from a candidate set C containing |C| items based on current session :
ùëÇ (ùë†) = ùêøùêøùëÄ (ùëÉ 1 (ùë†, C)), ùëÇ ‚äÜ C,(1)
where  1 is the basic prompt consisting of titles of items that the user interacted with in the session , candidate set C, and task instruction.The details are shown in Prompt 1.</p>
<p>Prompt 1 (basic prompt)</p>
<p>I watched/purchased the following movies/items in order: {}.Based on these interactions, recommend a movie/item for me to watch/purchase next from a candidate set: {C}.Please recommend from the candidate set.List the top 10 recommendations in numbered bullet points.</p>
<p>Then, we identify the incorrectly predicted sessions where the ranking list fails to hit the ground truth item at time step  + 1 (i.e.,   +1 ‚àâ  ()).Afterward, we prompt the LLM to conduct selfreflection, that is, analyze and summarize the prediction errors, to generate potential hints in natural language to rectify such errors.These hints thus can be considered as specialized knowledge for better recommendations.Inspired by Chain-of-Thought prompting [46], our approach focuses on analyzing and alleviating these errors in a step-by-step and progressive reasoning chain, producing hints that are tailored to be compatible with LLM comprehension capabilities, as introduced below.</p>
<p>Prompt 2</p>
<p>Question: { 1 } (basic prompt).ChatGPT: {  ( )} (top-10 results: 1. Casino Royale 2. Batman 3...).Now, know that none of these answers is the target.Infer about possible mistakes in the ChatGPT's predictions.</p>
<p>Example Output: Here are potential mismatches or mistakes in the incorrect recommendations: Casino Royale: While this is an action film, it leans more towards the spy genre, which isn't strongly represented in the watched list.Batman: This recommendation assumes ...</p>
<p>Prompt 3</p>
<p>The correct answer is {    +1 } (target).Analyze why ChatGPT missed the target item from a wide variety of aspects.Explain the causes for the oversights based on earlier analysis.Example Output: Given    +1 is the correct item, we can infer several potential causes for the mismatch in the original recommendations: 1. Missing Subtle Preferences: Your list contains a mixture of different genres, but there is a subtle preference for comedies.2. Popularity Bias: ...</p>
<p>Prompt 4</p>
<p>Provide short hints for AI to try again according to each point of the previous step, without information leakage of the target item.</p>
<p>Particularly, LLMs can yield incorrect results in SBR due to various causes, which underscores the importance of having diverse and accurate hints to rectify the errors.To broaden the exploration of the diverse causes for the errors, we first only present the LLM with incorrectly predicted sessions and make it conduct self-reflection to identify possible causes for the errors without the ground truth item, as shown in Prompt 2. This allows the LLM to explore more diverse causes that may lead to errors.Then, to pursue more accurate causes for the errors, we further stimulate the LLM's analytical skills by revealing the ground truth item.Specifically, given the ground truth item, we ask the LLM to review previous diverse causes to select more relative ones, as shown in Prompt 3, thus identifying accurate causes of mistakes.Finally, for effective utilization in the next stage, we ask the LLM to summarize obtained causes as hints (known as specialized knowledge) in Prompt 4, which are more understandable for LLMs and easily improve the recommendation outcome.By following the provided reasoning chain (i.e., Prompts 2-4), the LLM can identify frequent and prominent errors in SBR inference, and then produce qualified hints to address potential errors effectively.</p>
<p>Automated Filtering.</p>
<p>Recognizing that no single hint can rectify all the errors made by LLMs, we aim to build a hint knowledge base to store the most effective hints for further utilization, denoted as H .To enhance the quality of the hints in H , we develop an automated filtering strategy to maintain the hint knowledge base with two key properties: effectiveness and non-redundancy.</p>
<p>Prompt 5 (hint-enhanced prompt)</p>
<p>I watched/purchased the following movies/items in order: {}.Based on these interactions, recommend a movie/item for me to watch/purchase next from a candidate set: {C}.Please recommend from the candidate set.List the top 10 recommendations in numbered bullet points.Hint: {‚Ñé} (retrieved hint).</p>
<p>To ensure the effectiveness, we only add a new hint to H if it can lead to performance improvement.Specifically, we first construct a hint-enhanced prompt  * (Prompt 5) to trigger the LLM with the hint-enhanced prompt for recommendation inference:
ùëÇ * (ùë†) = ùêøùêøùëÄ (ùëÉ * (ùë†, C, ‚Ñé)),(2)
where  * () is the recommendation list for session  based on the hint-enhanced prompt  * .Then, we compare  * () with  () obtained by Eq. 1 using the basic prompt without hint enhancement.</p>
<p>The hint ‚Ñé can be recognized as effective if it leads to a better prediction by the LLM, i.e.,   +1 ‚àà  * () &amp;   +1 ‚àâ  ().</p>
<p>Prompt 6</p>
<p>'Does hint [‚Ñé ‚Ä≤ ] convey a similar idea for these hints: [‚Ñé]?Return 1 if true, else return 0. '</p>
<p>Meanwhile, some hints may be redundant generated by LLMs, as multiple sessions may suffer from similar causes.For example, 'Consider the release years of movies in the user's history for era preference' and 'Think about the production year of watched movies, recommend movies in that era from the candidate set' share a similar semantic meaning.The redundancy of hints may result in a high cost of the maintenance and utilization of the hint knowledge base H , e.g., larger size of the hint knowledge base and increased complexity in retrieving the targeted hint.To reduce the redundancy in the hint knowledge base H , we only incorporate new hints that are distinct from existing ones.Specifically, we employ LLMs to detect the semantic similarity between the candidate hint ‚Ñé ‚Ä≤ and existing ones {‚Ñé ‚àà H }, given by,
‚àëÔ∏Å ‚Ñé‚àà H ùêøùêøùëÄ (ùëÉ 6 (‚Ñé ‚Ä≤ , ‚Ñé)) == 0,(3)
where the detail of  6 is shown in Prompt 6, instructing the LLM to return 0 if ‚Ñé ‚Ä≤ shares a different idea with ‚Ñé and else return 1.By doing this, only distinct candidate hint ‚Ñé ‚Ä≤ will be incorporated into the knowledge base H if it satisfies Eq. 3.</p>
<p>Through the automated filtering process, we iteratively update the hint knowledge base until reaching capacity.It contains qualified hints to correct the errors across different session patterns and thus improve recommendation results.For a practical illustration, we randomly select five hints from the movie and the video game domains in the diagrams below.</p>
<p>Hint examples -Movie</p>
<p>-Consider the release years of movies in the record for era preference.</p>
<p>-Pay attention to actors who appear in multiple movies from the watched list and consider other films featuring these actors.</p>
<p>-Aim for a personalized recommendation list that offers variety and reflects a range of moods and themes.</p>
<p>-Consider films that originated from other media, such as books, radio plays, or TV shows.</p>
<p>-Do not rely solely on mainstream popularity; consider critically acclaimed films that may not be box office hits.</p>
<p>Hint examples -Video Game</p>
<p>-Pay attention to the platforms prominently featured in the user's purchase history and include items that are relevant to those platforms.</p>
<p>-Give preference to games released in the era aligning with purchased games.</p>
<p>-Pay attention to the previous purchase history, which indicates an interest in gaming accessories and hardware enhancements.</p>
<p>-Ensure the games are suitable for a wide range of ages and have a universal appeal.</p>
<p>-Focus on items that specifically enhance or are used in conjunction with mobile devices.</p>
<p>Reinforcement Utilization Module</p>
<p>To guide LLMs to infer more accurate SBR predictions, we propose to utilize the constructed hint knowledge base to prevent errors in the inference process of LLMs.</p>
<p>In practical scenarios, the absence of explicit labels on the hints' efficacy leads to a challenge in conducting supervision on hint selection, as computing the efficacy for all hints on each sample is costly and redundant.Fortunately, DRL with a replay buffer enables us to collect reward signals (i.e., hint efficacy) and update the network spontaneously to speed up.To this end, we employ the Proximal Policy Optimization (PPO) [33] algorithm to ensure stable and efficient training of our retrieval agent.Specifically, our agent is trained to select the most relevant hints based on session-related context information, thereby preventing LLMs from similar reasoning mistakes for recommendation.In addition, the proposed DRL framework allows for the balance between exploitation and exploration.It is also compatible with more complex extensions such as retrieving multiple hints and multi-round agent-based interactions.</p>
<p>Markov Decision Process (MDP).</p>
<p>To outline the basics of our DRL environment, we define MDP as the tuple  = (Z, A, , ), where Z, A,  and  denote the state space, action space, reward function, and transition, respectively.State.To model the session-related context information for our retrieval agent, we concatenate item titles and attributes into a single text string and convert it into embedding for semantic extraction.We use pre-trained BERT [5] as the text encoder due to its robust contextual language understanding capabilities.The state can be denoted as z =  (), where z ‚àà Z is the -dimensional output of the text encoder.</p>
<p>Action.To select relevant hints from the constructed knowledge base for LLMs' inference, we define a discrete action space for the agent, represented as a ‚àà A, which is a (|H | + 1)-dimensional vector.Here, |H | is the size of the knowledge base, and an action a corresponds to either choosing a hint or opting not to select any.Reward.To direct the agent in making accurate hint selections, we employ a comparative function  to provide the reward signals for the agent's actions.This function evaluates the improvement in the LLM prediction accuracy by the hint-enhanced prompt (Prompt 5) versus the basic prompt (Prompt 1).For each episode, the reward value  is denoted as  = ( * ()) ‚àí ( ()), where  is a recommendation evaluation metric (e.g., NDCG@10).</p>
<p>Transition.In each step, the agent can receive task-specific feedback by simulating the real-world RSs.The agent observes the state of the current session, takes action to select a hint for the LLM, and receives a reward from the environment.Subsequently, the environment transits to the next session's state, and then the transition is denoted as  (z) = z ‚Ä≤ .This setup can flexibly accommodate the scenario where multiple steps per session are involved for more sophisticated learning processes in our future studies.</p>
<p>Replay buffer.To facilitate efficiency in policy optimization, we maintain a replay buffer  = (z, a, , z ‚Ä≤ ) to store the tuples of observation state, agent action, reward, and next observation state.With the records in the replay buffer, the retrieval agent can refine successful strategies and learn from erroneous trials.This technique Sample from the replay buffer, update policy by Eq. 5; return   significantly accelerates the DRL training as the LLM backbone is relatively slow for inference and producing reward signals.</p>
<p>PPO Training.</p>
<p>To model the actions of the retrieval agent, we implement a policy network parameterized by MLPs to define our agent's policy   , which maps the environmental space Z to the action space A:
a = ùë†ùëú ùëì ùë°ùëöùëéùë• (W ‚Ä¢ z),(4)
where W ‚àà R ( | |+1) √ó is the learnable weight matrix, and z ‚àà R  denotes the state of the current session.The action of the agent corresponds to the largest value among the softmax logits of a.</p>
<p>During the training process, the retrieval agent adopts the greedy ( = 0.05) strategy to explore the environment, with probability  to take a random action while with probability (1 ‚àí ) to exploit the learned policy   .Our objective function for PPO training can be formulated to maximize the total reward, given by,
L ùëÉùëÉùëÇ = E ùëÖ(z, ùëé) ‚àí ùõΩùêæùêø(ùúã ùúΩ ùëúùëôùëë , ùúã ùúΩ ) ,(5)
where the first term is the reward measured by recommendation tasks, and the second term is the KL-divergence of policies with a coefficient  for the regulation in policy updates.In summary, we train a retrieval agent with task-specific feedback via DRL, which can select the appropriate action to improve LLM's performance through hint-enhanced prompts for better recommendations.</p>
<p>Retrieval-Enhanced LLMs for</p>
<p>Recommendation.With the constructed knowledge base and the trained retrieval agent, we achieve more accurate recommendations by automatically selecting appropriate hints for prompt enhancement during inference.The final
(z ùë† ))),(6)
where   (z s ) corresponds to the selected hint ‚Ñé by the trained retrieval agent for session  with its state z  .The overall algorithm of the proposed method Re2LLM is shown in Algorithm 1.</p>
<p>EXPERIMENTAL RESULTS</p>
<p>In this section, we evaluate the effectiveness of the proposed method Re2LLM through comprehensive experiments and analysis2 .We aim to answer the following research questions:</p>
<p>‚Ä¢ RQ1: Whether the proposed method outperforms baseline methods, including the deep learning models and other LLM-based models, for SBR? ‚Ä¢ RQ2: How can key components affect our proposed method?Specifically, how is the efficacy of the proposed Reflective Exploration Module and Reinforcement Utilization Module?‚Ä¢ RQ3: How do key hyper-parameters impact the performance of our proposed method?</p>
<p>Experimental Setup</p>
<p>4.1.1Dataset.In this paper, we evaluate the proposed Re2LLM and baseline methods on two real-world datasets, namely Hetrec2011-Movielens and Amazon Game:</p>
<p>‚Ä¢ Hetrec2011-Movielens3 dataset contains user ratings of movies.It also contains the side information of movies, e.g., title, production year, and movie genre.‚Ä¢ Amazon Game4 dataset is the 'Video Game' category of the Amazon Review Dataset, which is collected from the Amazon platform with users' reviews on various types of games and peripherals.It also contains metadata of games, e.g., title, brand, and tag.</p>
<p>For each user, we take all the interactions within one day as a session by the recorded timestamps.We filter out sessions and items that have less than 3 records and treat records with ratings as positive implicit feedback.Following the prior study [39], we adopt the data augmentation strategy to extend a session  with length | | to (| | ‚àí 1) sessions as training samples.The statistics of the datasets are summarized in Table 1.</p>
<p>Baselines.</p>
<p>We compare our proposed method with eight state-of-the-art baseline methods 5 , and the details are listed as follows.FPMC [32] combines matrix factorization with the firstorder Markov chain.GRU4Rec [13] uses Gated Recurrent Unit (GRU), a typical RNN layer, to model whole sessions.NARM [22] Table 2: Performance of all methods under two settings.The performance of our Re2LLM is in bold; the best performance achieved by baselines in full and few-shot settings are underlined with ' ' and ' ', respectively.The improvements (Imp.) of our model against the best-performed baselines in both settings are reported, where * and ‚Ä† indicate statistically significant improvement by t-test (p &lt; 0.05) for full* and few-shot ‚Ä† settings, respectively.</p>
<p>Setting Model</p>
<p>Movie Game HR@5 HR@10 NDCG@5 NDCG@10 HR@5 HR@10 NDCG@5 NDCG@10 introduces the attention mechanism into RNNs to better model the sequential behavior.SRGNN [47] constructs session graphs and employs GNN layers to obtain embeddings.GCEGNN [45] further includes global-level information as an additional graph to enhance the model performance.AttenMixer [57] achieves multilevel reasoning over item transitions by the attention-based readout method.LLMRank [16] is the LLM-based method, which demonstrates the promising zero-shot inference ability of LLMs through recency-focused prompting and in-context learning.NIR [42] is also LLM-based method that employs a multi-step prompt to capture user preferences first, then select representative movies, and finally perform the next-item prediction.</p>
<p>For a comprehensive comparison, we introduce two settings for the implementation of these baseline methods.In the full dataset setting, we investigate how Re2LLM performs in comparison with baseline methods trained on the full augmented dataset using item IDs.In the few-shot setting, we examine whether our Re2LLM shows superior performance compared to the baselines trained with limited data 6 .The '-attr' suffix denotes the incorporation of item attributes for deep learning-based baselines.We first concatenate all attributes (title included) of the item and then encode them into a text-aware embedding by BERT.Finally, we further aggregate the ID embedding and text-aware embedding of the item as its underlying representation.</p>
<p>Evaluation Strategy and Metrics.</p>
<p>For each dataset, we apply the split-by-ratio strategy [38] for sessions to obtain training, validation, and test sets with a ratio of 7:1:2.For the full dataset setting, we use the entire training set.For the few-shot setting, we sample 500 training samples from the entire training set.In terms of evaluation, we consider the last item in each session as the target.Considering the cost and efficiency of LLMs, we randomly sample a subset of 2,000 sessions from the test set.For the efficiency of evaluation, we sampled 49 negative items for each positive item (i.e., target).In addition, we adopt two commonly used metrics for model evaluation, including normalized discounted cumulative gain ( @) and hit rate (@), with  = {5, 10}.The results show the average of five runs with different random seeds.</p>
<p>4.1.4Implementation Details.For a fair comparison, all methods are optimized by the Adam optimizer with the same embedding dimension 768 that aligns with the BERT encoder dimension 7 .Following [35], we employ the Optuna8 framework to efficiently optimize the hyper-parameter of all methods.We conduct 20 trials on the following searching spaces, i.e., learning rate: {1 ‚àí2 , 1 ‚àí3 , 1 ‚àí4 }, weight decay: {1 ‚àí4 , 1 ‚àí6 , 1 ‚àí8 }, and batch size: {16, 64, 256}.For our method Re2LLM, we set candidate set size |C| to 50, knowledge base size  ‚Ñé to 20, and few-shot training size   to 500.The impacts of essential hyper-parameters on Re2LLM can be found in Section 4.4.For LLM-based methods, we use 'gpt-4' API as the backbone model for these methods.For the rest hyper-parameters in baseline methods, we follow the suggested settings in the original papers.The experiments are run on a single Tesla V100-PCIE-32GB GPU.On average, around 50 data samples are required to generate 20 hints.The average cost to analyze and conduct inference on each sample is around USD 0.015 during the hint generation, DRL training, and final testing.</p>
<p>Overall Comparison (RQ1)</p>
<p>Table 2 shows the performance of our method Re2LLM and various baseline methods under two evaluation settings, i.e., full dataset setting and few-shot setting.The experimental results lead to several key conclusions.First, our proposed method significantly outperforms baselines in few-shot settings, with average improvements Table 3: The results of ablation study across all datasets.</p>
<p>Models Movie Game</p>
<p>HR@5 HR@10 NDCG@5 NDCG@10 HR@5 HR@10 NDCG@5 NDCG@10 Figure 3: Performance of the proposed method with varying knowledge base size.Darker color indicates higher value. of 15.49% and 5.77% across all metrics when compared to the topperforming baseline models on Movie and Game, respectively.This is mainly attributed to the effectiveness of Re2LLM, which not only extracts specialized and comprehensible knowledge by selfreflection but also effectively utilizes the knowledge based on a well-trained retrieval agent for better recommendations.Second, Re2LLM achieves comparable and even better performance than baseline methods trained on full datasets, which also verifies the effectiveness of our module design.Furthermore, the LLM-based methods (e.g., LLM-Ranker and NIR) also achieve comparable performance to baseline methods trained on full datasets, which indicates the promising way of employing LLMs as recommenders.Third, deep learning-based models show a significant performance drop in the few-shot setting compared with the full dataset setting, pointing to the data sparsity issue as a primary challenge.Last, the incorporation of item attributes slightly boosts the performance of baseline methods, showing the potential of using side information of items in addressing the sparsity issue in SBR.</p>
<p>Ablation Studies (RQ2)</p>
<p>To verify the impact of each component, we compare our method Re2LLM with its variants: ‚Ä¢ w/o-HE: We only adopt the basic prompt (Prompt 1) to trigger the LLM to generate recommendations instead of Hint-Enhanced prompt as in Eq. 6.</p>
<p>‚Ä¢ w/o-AF: We remove the Automated Filtering mechanism from the hint knowledge base construction, that is, incorporating all hints generated by LLMs into the hint knowledge base regardless of their quality.‚Ä¢ w/o-DRL: We remove the Deep Reinforcement Learning with task-specific feedback for the retrieval agent.Instead, we use two straightforward strategies for hint retrieval, namely RANdom and ALL retrieval strategies.Specifically, the variant w/o-DRL(RAN) randomly samples hints from the hint knowledge base for prompt enhancement, whereas the variant w/o-DRL(ALL) concatenates all hints in the knowledge base instead of the selective retrieval.Table 3 shows the performance of all ablation studies.First, Re2LLM outperforms its variant w/o-HE, which indicates the necessity of activating LLM's reflection mechanism with hint enhancement in our Reflective Exploration Module.Second, Re2LLM outperforms variants w/o-DRL(RAN) and w/o-DRL(ALL), indicating the importance of our Reinforcement Utilization Module for the training of the retrieval agent with task-specific feedback signals.The variant w/o-DRL(RAN) has the worst performance by random sampling hints because inaccurate hints can mislead the LLM inference for recommendation, resulting in negative impacts.The variant w/o-DRL(ALL) shows limited improvement compared to the variant w/o-HE, revealing that simply feeding all hints into LLMs without session-aware selection is not optimal.Third, the variant w/o-AF achieves relatively better performance among all variants.Nevertheless, it still underperforms our Re2LLM due to the positive impact of the automated filtering mechanism on the hint knowledge base construction.In summary, the above results exhibit the efficacy of different modules in the proposed Re2LLM for more accurate SBR.</p>
<p>Hyper-parameter Analysis (RQ3)</p>
<p>We now examine the impact of several key hyper-parameters and designs on our proposed Re2LLM, including knowledge base size  ‚Ñé , few-shot training size   , and reward design.4.4.1 Knowledge Base Size.We investigate the correlation between model performance and the size of the knowledge base constructed by the Reflective Exploration Module.As shown in Fig. 3, there is an improvement in both evaluation metrics as the size of the knowledge base increases to 20.The improvement is attributed to the diversity of the generated reflective hints by covering a broader range of common errors of LLM inference on SBR.However, when the knowledge base size becomes too large, there is a slight performance drop due to the increased complexity and difficulty in the retrieval agent optimization.Thus, we suggest adopting a moderate hint knowledge base size.(e.g., less than 50) for retrieval agent training results in inferior performance, which is even lower than w/o-DRL(RAN) with random hint selection.This is because the retrieval agent is under-trained with only very few samples.Second, as the number of training samples increases, the performance of our method increases consistently, showing the growth of the generalization ability of the retrieval agent.Baseline methods also improve, but remain inferior to our proposed method across various few-shot training sizes.Third, we also observe the limited improvement of Re2LLM when the number of samples exceeds a threshold.For the balance between effectiveness and efficiency, 500 is an appropriate selection for the few-shot training size of Re2LLM.</p>
<p>Reward Design.</p>
<p>In Table 4, we report the experimental results using two distinct reward designs for the Reinforcement Retrieval Module: comparative reward and absolute reward.The comparative reward  = ( * ()) ‚àí ( ())) measures the improvement achieved by the hint-enhanced prompt over the basic prompt.The absolute reward  ‚Ä≤ = ( * ()) evaluates the performance of the hint-enhanced prompt alone.We find that the comparative reward yields superior performance compared to the absolute reward, as the comparative strategy focuses on improvement owing to the selected hint rather than its overall performance.As a result, we adopt the comparative reward for our Re2LLM.</p>
<p>Case Study (RQ2)</p>
<p>To further study the effectiveness of the retrieval agent, we compare Re2LLM, variant w/o-HE, and variant w/o-DRL(RAN) in two cases.Case 1 illustrates the scenario where the LLM backbone fails to hit the target item without a hint (i.e., the variant w/o-HE).Our trained retrieval agent in Re2LLM successfully retrieves the tailored hint and hits the target item at the third place of the top-10 recommendation list, demonstrating its ability to learn from task-specific feedback.However, the variant w/o-DRL(RAN) fails to replicate the correct prediction as a random hint may not be relevant to the session.Case 2 illustrates the scenario where the LLM backbone succeeds in hitting the target item without a hint (i.e., the variant w/o-HE).The trained agent in Re2LLM can further improve recommendation results by hitting the target item in a higher-ranking position (i.e., from the third place to the first place).On the contrary, a randomly selected hint by the variant w/o-DRL(RAN) shows a negative impact on the inference result, making the LLM miss the target item.Specifically, the session shows a preference for comedy, but the inappropriate hint instructs LLM's attention on the movie production years, thus misleading the LLM to generate incorrect predictions.These two cases reveal the necessity of effectively exploring and efficiently utilizing specialized knowledge (i.e., hints) for LLMs in SBR, validating our motivation and the efficacy of key modules in our Re2LLM.</p>
<p>Case Study 1: Trained retriever</p>
<p>In this paper, we propose Re2LLM, a Reflective Reinforcement LLM for SBR, aiming to improve performance by identifying and rectifying common errors in LLM inference.We present a novel learning paradigm that merges the capabilities of LLMs with adaptable model training procedures.Specifically, Re2LLM harnesses the selfreflection ability of LLMs to capture specialized knowledge and create a hint knowledge base with an automated filtering strategy.</p>
<p>Then, trained via DRL, a lightweight retrieval model learns to select proper hints guided by task-specific feedback to facilitate sessionaware inference for better recommendation results.We demonstrate the effectiveness of our method through extensive experiments in two real-world datasets across two evaluation settings.In addition, ablation studies and hyper-parameter analysis further validate our underlying motivations and designs.In future work, the retrieval model could be extended with more flexible functionalities, such as integrating hints and multi-modal contextual information.</p>
<p>Figure 1 :
1
Figure 1: Our method (c) obtains effective task-specific feedback compared with prompt-based (a) and fine-tuning-based (b) methods.</p>
<p>Figure 2 :
2
Figure 2: The overall architecture of our proposed Re2LLM approach.The overall flowchart (green) contains a Reflective Exploration Module (yellow) for the generation of session-aware hints as specialized knowledge and a Reinforcement Utilization Module (blue) for the learning to retrieve obtained specialized knowledge.</p>
<ol>
<li>4 . 2
42
Few-shot Training Size.We also study how model performance is affected by the number of few-shot training samples used for the retrieval model training.Fig.4shows the comparison of Re2LLM, its variant w/o-DRL(RAN), and representative baselines across various few-shot training sizes9 .First, using too few samples</li>
</ol>
<p>Figure 4 :
4
Figure 4: Performance of the representative baselines and our method with varying few-shot training size for retrieval agent.The dotted lines indicate w/o-DRL(RAN) performance.(e.g., less than 50) for retrieval agent training results in inferior performance, which is even lower than w/o-DRL(RAN) with random hint selection.This is because the retrieval agent is under-trained with only very few samples.Second, as the number of training samples increases, the performance of our method increases consistently, showing the growth of the generalization ability of the retrieval agent.Baseline methods also improve, but remain inferior to our proposed method across various few-shot training sizes.Third, we also observe the limited improvement of Re2LLM when the number of samples exceeds a threshold.For the balance between effectiveness and efficiency, 500 is an appropriate selection for the few-shot training size of Re2LLM.</p>
<p>‚Ä¢ Our Re2LLM benefits from LLMs' self-reflection capabilities (i.e., Reflective Exploration module) as well as the flexibility of the lightweight retrieval agent (Reinforcement Utilization Module).This enables us to effectively extract specialized knowledge understandable by LLMs, which can be then efficiently utilized for better LLM inference by learning from task-specific feedback.</p>
<p>‚Ä¢ Our extensive experiments demonstrate that Re2LLM outperforms state-of-the-art methods, including deep learning-based models and LLM-based models, in both few-shot and full-data settings across two real-world datasets.</p>
<p>Algorithm 1 :
1
Re2LLM Input: Training data S  for SBR, hint knowledge base size  ‚Ñé .// Reflective Exploration Module while | H | &lt;  ‚Ñé do Sample  from   ; Training data S  for SBR, constructed H, max episode  .Initialize policy network   ;
if m(O(ùë†))=0 // If incorrectly predictedthenObtain multiple hints by Prompt 2 -4;for each obtained hint ‚Ñé doAssess two aspects of quality by automated filtering;if True thenAdd ‚Ñé to H;return HInput:
// Reinforcement Utilization Module for  = 1, 2, ...,  do Shuffle S  ; for  in S do Encode z ‚àà Z based on context information of ; Sample action  by Eq. 4 and -greedy exploration; Compute reward by  =  ( * ( ) ) ‚àí  ( ( ) ); Transit to the next state z ‚Ä≤ ; Store the tuple (z, a, , z ‚Ä≤ ) into the replay buffer;</p>
<p>Table 1 :
1
Statistics of datasets.
MovieGame# Sessions468,389387,906# Items8,23322,576Avg. length10.176.28Utilized Itemtitle, genre, actor, year, title, category, tag,Side Informationcountry, directorbrand, descriptionrecommendation output √î can be denoted as follows:√î = ùêøùêøùëÄ (ùëÉ
* (, C,  </p>
<p>Table 4 :
4
Performance of Re2LLM with two reward designs.
MovieGameRewardHR@10 NDCG@10 HR@10 NDCG@10absolute0.51430.31470.70260.4542comparative 0.57350.33580.71730.4689
Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation arxiv, ,
Our source code is available at https://anonymous.4open.science/r/Re2LLM-34DC/.
https://grouplens.org/datasets/hetrec-2011/
https://jmcauley.ucsd.edu/data/amazon/
Our method does not require fine-tuning, and due to the high computational cost, fine-tuning-based methods are not in our scope.
We select more representative and stronger baselines for the few-shot setting and incorporate item attribute information in the few-shot setting only for a fair comparison.
Empirically, using MLP to reduce dimension leads to a consistent performance drop.
https://optuna.org/
We use previously mentioned data augmentation[39] for baselines. The results are from the Movie domain. Similar trends occur for the VideoGame domain.</p>
<p>Prompt: I watched the following movies in order: {Titanic. </p>
<p>Based on these interactions, recommend a movie for me to watch next from a candidate set: {C}. Please recommend from the candidate set. Leaving Las, Vegas . } , List the top 10 recommendations in numbered bullet points. Hint: {‚Ñé}. Answer by variant w/o-HE, where ‚Ñé = '': 1. The Day After Tomorrow. 2The Bourne Identity; 3. Last Man Standing</p>
<p>The Wrestler Answer by Re2LLM, where retrieved hint ‚Ñé = 'Consider films that originated from other media, such as books, radio, or TV shows': 1. The Bourne Identity; 2. True Grit; 3. Aladdin; 4. The Parent Trap; 5. The Picture of Dorian Gray; 6. Breakfast at Tiffany's; 7. The Jazz Singer; 8. The Last Castle; 9. The Stepfather. 5Cujo Answer by w/o-DRL(RAN), where random hint ‚Ñé = 'Pay attention to actors who appear in multiple movies from the watched list and consider other films featuring these actors': 1. The Day After Tomorrow; 2. Taken; 3. The Bourne Identity. Last Man Standing; 5. The Last Castle; 6. The Wrestler; 7. Redbelt; 8. The Stepfather</p>
<p>Courage Under Fire Case Study 2: Random retriever Prompt: I watched the following movies in order: {Xizao; Hero at Large; ...} Based on these interactions, recommend a movie for me to watch next from a candidate set: {C}. Please recommend from the candidate set. List the top 10 recommendations in numbered bullet points. Hint: {‚Ñé}. 10</p>
<p>The Perfect Storm; 10. Chaplin Answer by Re2LLM, where retrieved hint ‚Ñé = 'Focus on films with a strong comedic element. ; Iceman, Siu lam juk kau ; 4. Hot Fuzz; 5. King of New York; 6. Cidade dos Homens; 7. Blood Diamond; 8. Fantastic Four. 13particularly those that blend humor with other genres</p>
<p>Chaplin Answer by w/o-DRL(RAN), where random hint ‚Ñé = 'Consider the release years of movies in the user's history for era preference': 1. Siu lam juk kau; 2. Iceman; 3. Banlieue 13; 4. Hot Fuzz; 5. Broken Flowers; 6. The Perfect Storm; 7. Fantastic Four; 8. Blood Diamond. King of New York20019Blood Diamond; 2. Hot Fuzz; 3. Banlieue 13; 4. Freedom Writers; 5. Fantastic Four; 6. 2 Fast 2 Furious; 7. The Perfect Storm; 8. Iceman; 9. Ice Station Zebra; 10. Exit Wounds</p>
<p>TALLRec: An effective and efficient tuning framework to align large language model with recommendation. Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, Xiangnan He, Proceedings of the 17th ACM Conference on Recommender Systems (RecSys). the 17th ACM Conference on Recommender Systems (RecSys)2023</p>
<p>AutoGSR: Neural Architecture Search for Graph-based Session Recommendation. Jingfan Chen, Guanghui Zhu, Haojun Hou, Chunfeng Yuan, Yihua Huang, Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '22). the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '22)2022</p>
<p>Knowledge-enhanced Multi-View Graph Neural Networks for Session-based Recommendation. Qian Chen, Zhiqiang Guo, Jianjun Li, Guohui Li, Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '23). the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '23)n. d.</p>
<p>Uncovering ChatGPT's Capabilities in Recommender Systems. Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, Proceedings of the 17th ACM Conference on Recommender Systems (RecSys). the 17th ACM Conference on Recommender Systems (RecSys)Jun Xu. 2023</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterAssociation for Computational Linguistics20191</p>
<p>Enhancing job recommendation through llm-based generative adversarial networks. Yingpeng Du, Di Luo, Rui Yan, Hongzhi Liu, Yang Song, Hengshu Zhu, Jie Zhang, arXiv:2307.107472023. 2023arXiv preprint</p>
<p>Large language model with graph convolution for recommendation. Yingpeng Du, Ziyan Wang, Zhu Sun, Haoyan Chua, Hongzhi Liu, Zhonghai Wu, Yining Ma, Jie Zhang, Youchen Sun, arXiv:2402.088592024. 2024arXiv preprint</p>
<p>Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, Jiawei Zhang, arXiv:2303.14524[cs.IR]Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System. 2023</p>
<p>Recommendation as language processing (RLP): a unified pretrain, personalized prompt &amp; predict paradigm (P5). Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, Yongfeng Zhang, Proceedings of the 16th ACM Conference on Recommender Systems (RecSys). the 16th ACM Conference on Recommender Systems (RecSys)2022</p>
<p>Multi-Faceted Global Item Relation Learning for Session-Based Recommendation. Qilong Han, Chi Zhang, Rui Chen, Riwei Lai, Hongtao Song, Li Li, Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '22). the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '22)2022</p>
<p>Large Language Models as Zero-Shot Conversational Recommenders. Zhankui He, Zhouhang Xie, Rahul Jha, Harald Steck, Dawen Liang, Yesu Feng, Prasad Bodhisattwa, Nathan Majumder, Julian Kallus, Mcauley, Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (CIKM '23). the 32nd ACM International Conference on Information and Knowledge Management (CIKM '23)</p>
<p>Recurrent Neural Networks with Top-k Gains for Session-based Recommendations. Bal√°zs Hidasi, Alexandros Karatzoglou, Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM 2018. the 27th ACM International Conference on Information and Knowledge Management, CIKM 2018Torino, ItalyACM2018. October 22-26. 2018</p>
<p>Session-based recommendations with recurrent neural networks. Bal√°zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, Domonkos Tikk, ICLR 20164th International Conference on Learning Representations. </p>
<p>CORE: Simple and Effective Session-Based Recommendation within Consistent Representation Space. Yupeng Hou, Binbin Hu, Zhiqiang Zhang, Wayne Xin Zhao, 2022</p>
<p>Towards Universal Sequence Representation Learning for Recommender Systems. Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, Ji-Rong Wen, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</p>
<p>Large language models are zero-shot rankers for recommender systems. Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian Mcauley, Wayne Xin Zhao, arXiv:2305.088452023</p>
<p>LoRA: Low-rank adaptation of large language models. J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations (ICLR). 2022</p>
<p>GenRec: Large Language Model for Generative Recommendation. Jianchao Ji, Zelong Li, Shuyuan Xu, Wenyue Hua, Yingqiang Ge, Juntao Tan, Yongfeng Zhang, 2023</p>
<p>Wang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Maheswaran Sathiamoorthy, Lichan Hong, Ed Chi, Derek Zhiyuan Cheng, arXiv:2305.06474Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction. 2023</p>
<p>Matrix factorization techniques for recommender systems. Yehuda Koren, Robert Bell, Chris Volinsky, Computer. 422009. 2009</p>
<p>Bin Wang, and Aixin Sun. 2022. An Attribute-Driven Mirror Graph Network for Session-Based Recommendation (SIGIR '22). Siqi Lai, Erli Meng, Fan Zhang, Chenliang Li, </p>
<p>Neural attentive session-based recommendation. Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, Jun Ma, Proceedings of the 2017 ACM on Conference on Information and Knowledge Management (CIKM). the 2017 ACM on Conference on Information and Knowledge Management (CIKM)2017</p>
<p>Multi-modality is all you need for transferable recommender systems. Youhua Li, Hanwen Du, Yongxin Ni, Pengpeng Zhao, Qi Guo, Fajie Yuan, Xiaofang Zhou, ArXiv abs/2312.096022023. 2023</p>
<p>Enhancing hypergraph neural networks with intent disentanglement for session-based recommendation. Yinfeng Li, Chen Gao, Hengliang Luo, Depeng Jin, Yong Li, Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval2022. 1997-2002</p>
<p>ONCE: Boosting Content-based Recommendation with Both Open-and Closed-source Large Language Models. Qijiong Liu, Nuo Chen, Tetsuya Sakai, Xiao-Ming Wu, Proceedings of the Seventeen ACM International Conference on Web Search and Data Mining. the Seventeen ACM International Conference on Web Search and Data Mining2024</p>
<p>STAMP: Shortterm attention/memory priority model for session-based recommendation. Qiao Liu, Yifu Zeng, Refuoe Mokhosi, Haibin Zhang, KDD. ACM2018</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Prasad Bodhisattwa, Shashank Majumder, Amir Gupta, Peter Yazdanbakhsh, Clark, arXiv:2303.17651[cs.CL]Self-Refine: Iterative Refinement with Self-Feedback. 2023</p>
<p>Incorporating User Micro-Behaviors and Item Knowledge into Multi-Task Learning for Session-Based Recommendation. Wenjing Meng, Deqing Yang, Yanghua Xiao, Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '20). the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '20)2020</p>
<p>Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies. Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, William Yang, Wang , arXiv:2308.03188[cs.CL]2023</p>
<p>Star graph neural networks for session-based recommendation. Zhiqiang Pan, Fei Cai, Wanyu Chen, Honghui Chen, Maarten De Rijke, Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management (CIKM). the 29th ACM International Conference on Information &amp; Knowledge Management (CIKM)2020</p>
<p>Automatic Prompt Optimization with "Gradient Descent" and Beam Search. Reid Pryzant, Dan Iter, Jerry Li, Yin Lee, Chenguang Zhu, Michael Zeng, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Factorizing personalized Markov chains for next-basket recommendation. Steffen Rendle, Christoph Freudenthaler, Lars Schmidt-Thieme, Proceedings of the 19th International Conference on World Wide Web. the 19th International Conference on World Wide Web2010</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347[cs.LG]Proximal Policy Optimization Algorithms. 2017</p>
<p>Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, arXiv:2303.11366[cs.AI]Reflexion: Language Agents with Verbal Reinforcement Learning. 2023</p>
<p>DaisyRec 2.0: Benchmarking Recommendation for Rigorous Evaluation. Zhu Sun, Hui Fang, Jie Yang, Xinghua Qu, Hongyang Liu, Di Yu, Yew-Soon Ong, Jie Zhang, IEEE Transactions on Pattern Analysis and Machine Intelligence. TPAMI2022. 2022</p>
<p>Research commentary on recommendations with side information: A survey and research directions. Zhu Sun, Qing Guo, Jie Yang, Hui Fang, Guibing Guo, Jie Zhang, Robin Burke, Electronic Commerce Research and Applications. 371008792019. 2019</p>
<p>Large Language Models for Intent-Driven Session Recommendations. Zhu Sun, Hongyang Liu, Xinghua Qu, Kaidong Feng, Yan Wang, Yew-Soon Ong, arXiv:2312.07552[cs.CL]2023</p>
<p>Are We Evaluating Rigorously? Benchmarking Recommendation for Reproducible Evaluation and Fair Comparison. Zhu Sun, Di Yu, Hui Fang, Jie Yang, Xinghua Qu, Jie Zhang, Cong Geng, Proceedings of the 14th ACM Conference on Recommender Systems. the 14th ACM Conference on Recommender Systems2020</p>
<p>Improved recurrent neural networks for session-based recommendations. Yong Kiam Tan, Xinxing Xu, Yong Liu, Proceedings of the 1st Workshop on Deep Learning for Recommender Systems. the 1st Workshop on Deep Learning for Recommender SystemsAssociation for Computing Machinery2016</p>
<p>Edouard Grave, and Guillaume Lample. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timoth√©e Lachaux, Baptiste Lacroix, Naman Rozi√®re, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, arXiv:2302.13971LLaMA: Open and Efficient Foundation Language Models. 2023</p>
<p>Attention is All you Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Å Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems, I. Guyon, U. Von Luxburg. H Bengio, R Wallach, S Fergus, R Vishwanathan, Garnett, Curran Associates, Inc201730</p>
<p>Zero-Shot Next-Item Recommendation using Large Pretrained Language Models. Lei Wang, Ee-Peng Lim, arXiv:2304.031532023</p>
<p>Modeling multi-purpose sessions for next-item recommendations via mixture-channel purpose routing networks. Shoujin Wang, Liang Hu, Yan Wang, Z Quan, Mehmet Sheng, Longbing Orgun, Cao, Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19. the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-192019</p>
<p>DRDT: Dynamic Reflection with Divergent Thinking for LLM-based Sequential Recommendation. Yu Wang, Zhiwei Liu, Jianguo Zhang, Weiran Yao, Shelby Heinecke, Philip S Yu, arXiv:2312.113362023</p>
<p>Global context enhanced graph neural networks for session-based recommendation. Ziyang Wang, Wei Wei, Gao Cong, Xiao-Li Li, Xian-Ling Mao, Minghui Qiu, Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval2020</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. Curran Associates, Inc202235</p>
<p>Session-based recommendation with graph neural networks. Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, Tieniu Tan, Proceedings of the Thirty-Third AAAI Conference. the Thirty-Third AAAI Conference2019</p>
<p>Decoupled Side Information Fusion for Sequential Recommendation. Yueqi Xie, Peilin Zhou, Sunghun Kim, Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '22. the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '222022</p>
<p>Graph contextualized selfattention network for session-based recommendation. Chengfeng Xu, Pengpeng Zhao, Yanchi Liu, Victor S Sheng, Jiajie Xu, Fuzhen Zhuang, Junhua Fang, Xiaofang Zhou, Proceedings of the 28th International Joint Conference on Artificial Intelligence (IJCAI). the 28th International Joint Conference on Artificial Intelligence (IJCAI)2019</p>
<p>Fan Yang, Zheng Chen, Ziyan Jiang, Eunah Cho, Xiaojiang Huang, Yanbin Lu, arXiv:2305.07622PALR: Personalization aware LLMs for recommendation. 2023</p>
<p>LOAM: Improving Long-tail Session-based Recommendation via Niche Walk Augmentation and Tail Session Mixup. Heeyoon Yang, Yunseok Choi, Gahyung Kim, Jee-Hyong Lee, 2023</p>
<p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.10601[cs.CL]2023</p>
<p>Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, Silvio Savarese, arXiv:2308.02151[cs.CL]Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization. 2023</p>
<p>TAGNN: Target attentive graph neural networks for session-based recommendation. Feng Yu, Yanqiao Zhu, Qiang Liu, Shu Wu, Liang Wang, Tieniu Tan, Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval2020</p>
<p>Zhenrui Yue, Sara Rabhi, Gabriel De Souza Pereira Moreira, Dong Wang, Even Oldridge, arXiv:2311.02089[cs.IR]LlamaRec: Two-Stage Recommendation using Large Language Models for Ranking. 2023</p>
<p>Is ChatGPT fair for recommendation? Evaluating fairness in large language model recommendation. Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, Xiangnan He, Proceedings of the 17th ACM Conference on Recommender Systems (RecSys). the 17th ACM Conference on Recommender Systems (RecSys)2023</p>
<p>Efficiently leveraging multi-Level user intent for session-based recommendation via atten-mixer network. Peiyan Zhang, Jiayan Guo, Chaozhuo Li, Yueqi Xie, Jae Boum Kim, Yan Zhang, Xing Xie, Haohan Wang, Sunghun Kim, Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining (WSDM). the Sixteenth ACM International Conference on Web Search and Data Mining (WSDM)2023</p>
<p>Beyond Co-occurrence: Multi-modal Session-based Recommendation. Xiaokun Zhang, Bo Xu, Fenglong Ma, Chenliang Li, Liang Yang, Hongfei Lin, 10.1109/TKDE.2023.3309995IEEE Transactions on Knowledge and Data Engineering. 2023. 2023</p>
<p>Price DOES Matter! Modeling Price and Interest Preferences in Session-Based Recommendation. Xiaokun Zhang, Bo Xu, Liang Yang, Chenliang Li, Fenglong Ma, Haifeng Liu, Hongfei Lin, Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '22). the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '22)2022</p>
<p>Language models as recommender systems: Evaluations and limitations. Yuhui Zhang, Hao Ding, Zeren Shui, Yifei Ma, James Zou, Anoop Deoras, Hao Wang, NeurIPS 2021 Workshop on I (Still) Can't Believe It's Not Better. 2021</p>            </div>
        </div>

    </div>
</body>
</html>
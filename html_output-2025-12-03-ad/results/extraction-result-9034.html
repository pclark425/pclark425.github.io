<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9034 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9034</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9034</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-279260554</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.08349v1.pdf" target="_blank">Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have demonstrated remarkable performance on various medical benchmarks, but their capabilities across different cognitive levels remain underexplored. Inspired by Bloom's Taxonomy, we propose a multi-cognitive-level evaluation framework for assessing LLMs in the medical domain in this study. The framework integrates existing medical datasets and introduces tasks targeting three cognitive levels: preliminary knowledge grasp, comprehensive knowledge application, and scenario-based problem solving. Using this framework, we systematically evaluate state-of-the-art general and medical LLMs from six prominent families: Llama, Qwen, Gemma, Phi, GPT, and DeepSeek. Our findings reveal a significant performance decline as cognitive complexity increases across evaluated models, with model size playing a more critical role in performance at higher cognitive levels. Our study highlights the need to enhance LLMs' medical capabilities at higher cognitive levels and provides insights for developing LLMs suited to real-world medical applications.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9034.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9034.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Low-Level MCQs (MedQA/MedMCQA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Preliminary Knowledge Grasp — Multiple-Choice Questions from MedQA and MedMCQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A battery of medical multiple-choice questions drawn from MedQA (US subset, 5-option USMLE-style questions) and MedMCQA (4-option Indian exam questions) used to assess factual recall and basic medical knowledge (Bloom's "Remembering/Understanding").</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple models (examples: GPT-4o, DeepSeek-V3, GPT-4o-mini, Llama3-70B, Qwen2.5-72B, Phi series, Gemma series, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>General and medical domain LLMs across multiple families (Llama, Qwen, Gemma, Phi, GPT, DeepSeek and medical-tuned variants) evaluated as-is or instruction-fine-tuned; architectures are transformer-based foundation LLMs; some models are domain-finetuned (Med42, MMed-Llama, Meditron, ClinicalCamel).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (reported models span ~2B to 72B parameters; GPT-4o size not disclosed in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Low-Level MCQs (MedQA, MedMCQA)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice medical exam questions assessing memorization and initial understanding of medical facts and concepts (factual recall). MedQA (US subset) uses 5-option USMLE-styled items; MedMCQA uses 4-option items.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Top reported normalized accuracies (Table 2): GPT-4o 78.33 ± 0.75%; DeepSeek-V3 78.33 ± 0.75%; GPT-4o-mini 62.67 ± 0.20%; Llama3-70B 63.68 ± 0.34%; Qwen2.5-72B 67.83 ± 0.20%. Many smaller models and some earlier families achieve substantially lower normalized accuracy (examples in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Clinician accuracy on Low-Level tasks (clinician validation, n=4 clinicians sampling 100 items): 68.8% (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Top-performing LLMs match or exceed the small clinician sample on Low-Level MCQs; larger models and some instruction/inference-time scaled variants show strong performance on factual-recall tests, indicating LLMs can attain near-expert performance on standardized medical MCQs.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluation used five-shot in-context learning (five Q-A examples) with temperature=0. Models produced answers which were directly compared to ground truth. Normalized accuracy was computed to adjust for random-guessing baseline (random accuracy = 1/Nc for MCQs). MedQA test set and MedMCQA dev set were used. Repeated experiments (five runs) with averaged mean ± standard error reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Normalization procedure (subtracting random accuracy) affects reported numbers; some expensive models evaluated on subsets (~10%) due to API cost; comparisons across models can be sensitive to instruction tuning, prompt format, and whether chain-of-thought-like prompting was used (the paper uses zero temperature and fixed 5-shot prompt). Related-work claims (e.g., GPT-4 with chain-of-thought achieving 90.2 on MedQA) used different prompting strategies, so direct comparisons are imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9034.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9034.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mid-Level Complex Application</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comprehensive Knowledge Application — Statement Validation, Multi-step Rectification, Answer Existence Judgment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of reformulations of MCQs to require (1) statement truth validation, (2) two-step rectification (verify given option then correct if wrong), and (3) answer-existence judgement (whether correct answer is present among candidate options) — designed to probe application, discrimination, and limited multi-step reasoning without scenario context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple models (examples: GPT-4o, DeepSeek-V3, Qwen2.5-72B, Llama3-70B, Phi4-14B, Gemma2-27B, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same set of general and medically tuned LLMs evaluated under a five-shot ICL setting; Mid-Level test items were generated by reformulating MCQs (using GPT-4o to generate statements/distractors) and verified by doctors.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (reported examples include 7B, 14B, 27B, 32B, 70–72B models; GPT-4o size not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Mid-Level Tasks (Statement Validation; Multi-step Rectification; Answer Existence Judgment)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Tasks transformed from MCQs to require precise knowledge discrimination (true/false statement validation), multi-step correction (verify a provided option then select the correct one if wrong), and decision in a larger candidate space (determine whether correct answer exists among options); designed to probe Applying/Analyzing cognitive skills.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performance falls relative to Low-Level by roughly ~20 percentage points for many models. Examples (normalized accuracies, Table 2 & Table 4): DeepSeek-V3 65.78 ± 0.22%; GPT-4o 64.00 ± 1.02%; Qwen2.5-72B 49.47 ± 0.69%; Llama3-70B 41.79 ± 0.85%. Per-task breakdown (Table 4) shows statement validation and answer-existence tasks moderately supported (e.g., GPT-4o StmtVal 71.25%, AnsExist 58.82%) but multi-step rectification is weaker (e.g., GPT-4o MultiRect 22.46%).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Clinician accuracy on Mid-Level tasks (clinician validation): 54.2% (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Top LLMs can exceed clinician accuracy on selected mid-level items, but overall models show a substantial drop from Low-Level performance; tasks that expand the decision space (answer-existence) cause the largest performance degradation (many models drop >40%).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Five-shot in-context learning, temperature=0. Mid-level items generated using GPT-4o then reviewed by three doctors (reported generation error <5%). Normalization: binary tasks use random accuracy=0.5; multi-step rectification random accuracy derived to be invariant to random strategy (α = 1/Nc), leading to normalization used in reported results. Results averaged over repeated runs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Mid-level dataset generation used GPT-4o (possible generation bias/contamination); although human verification was applied, potential leakage or distributional artifacts may remain. The clinician baseline is small (4 clinicians) and may not generalize; different prompting or chain-of-thought strategies could affect model performance but were not applied here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9034.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9034.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>High-Level Full-Path Clinical Diagnosis (MIMIC-IV)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scenario-Based Problem Solving — Full-path Clinical Diagnosis using MIMIC-IV records</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent-based interactive diagnostic task where the model sequentially orders physical exams/labs/imaging, receives results, and must produce a primary diagnosis; assesses planning, multi-step decision-making, information gathering, and integration (higher-order clinical reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Instruction-fine-tuned and dialogue-capable LLMs (examples: DeepSeek-V3, GPT-4o, Llama3-70B, Qwen2.5-72B, Phi4-14B, Gemma2-27B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned variants of the evaluated LLMs were used for the interactive setting; models are transformer-based LLMs of multiple sizes (see Table 7). Some medical-specific models are also evaluated (Med42, Meditron, ClinicalCamel, MMed-Llama).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (examples in Table 7: 8B, 14B, 27B, 32B, 70–72B; DeepSeek-V3 technical details and GPT-4o sizes not fully disclosed in paper table)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>High-Level Full-Path Clinical Diagnosis (constructed from MIMIC-IV aligned to MedQA disease pool)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Interactive scenario-based diagnostic episodes built from inpatient admission records (history of present illness, physical exam, labs, microbiology, imaging). The agent must choose actions (PE, LAB, MICRO, IMAGE, OUTPUT) to request information in sequence and finally output diagnosis. Evaluation metric emphasizes both examination recall (procedural correctness) and diagnosis accuracy (full-path accuracy combines both).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Models sometimes reach moderate end-point diagnosis accuracy but fail on the full-path metric due to poor exam-ordering/recall. Example results (Table 5): End-point diagnosis accuracy — DeepSeek-V3 53.6%, GPT-4o 49.2%; Examination recall (macro) generally <40% for top models (DeepSeek-V3 30.0%, GPT-4o 31.8%); Full-path diagnosis accuracy (exam recall × diagnosis correctness) — DeepSeek-V3 19.4%, GPT-4o 19.3%, Llama3-70B 18.1%, Qwen2.5-72B 15.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Clinician accuracy on High-Level tasks (clinician validation subset): 23.5% (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>While some LLMs can reach comparable end-point diagnosis accuracy to clinicians in certain cases, they perform worse on the full-path metric because they fail to request and integrate needed examinations (low examination recall). Overall, LLMs and clinicians both find these high-level scenario tasks challenging, but LLMs' procedural shortcomings drive their lower full-path scores.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Zero-shot evaluation in an agent-based interactive protocol; instruction-fine-tuned models used; temperature=0.8 to encourage exploration; action parsing and result mapping used UMLS and handcrafted synonym mappings with fuzzy matching (threshold 0.9) and manual verification; when models output invalid actions they are warned and asked to re-output; repeated 3–5 evaluation runs; full dataset: 2,176 admission records across 42 diseases from MIMIC-IV filtered to align with MedQA disease pool.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Full-path metric is sensitive to correct parsing of requested exam items (paper reports parsing recall ~95% after manual verification). Some evaluated models were tested on a subset due to API cost (notably GPT-4o and DeepSeek-V3 evaluated on ~10% in some analyses). Many medical models sometimes fail to follow instructions (repeating prompts or outputting irrelevant text), producing near-zero scores; the clinician baseline is from a small sample and may not represent broader clinician performance; task complexity and EHR note quality can limit diagnostic signal.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9034.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9034.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 on MedQA (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 evaluated on MedQA (prior cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites prior work reporting that GPT-4 achieved high accuracy on the MedQA medical exam benchmark when using complex chain-of-thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (OpenAI, cited in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large foundation LLM from OpenAI that in cited studies was prompted with chain-of-thought strategies to solve medical exam items.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MedQA (medical exam benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Medical multiple-choice exam questions (MedQA) used in existing literature to assess medical knowledge of LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported in related work (cited): GPT-4 achieves 90.2% accuracy on MedQA when using complex chain-of-thought prompting (cited in Related Work section).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Not provided for that cited result within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Cited work characterizes GPT-4 performance as approaching expert-level on MedQA under specific prompting (chain-of-thought); the current paper notes such prior results but also highlights that performance degrades on higher cognitive and scenario-based tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Cited result used complex chain-of-thought prompting (details in the cited literature); current paper did not replicate that specific prompting strategy in its main evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Different prompting protocols (e.g., chain-of-thought) and evaluation scopes make direct comparisons with the current paper's normalized results difficult; the current paper emphasizes broader cognitive-level evaluation rather than reproducing chain-of-thought gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>What disease does this patient have? a large-scale open domain question answering dataset from medical exams <em>(Rating: 2)</em></li>
                <li>Capabilities of gpt-4 on medical challenge problems <em>(Rating: 2)</em></li>
                <li>Towards expert-level medical question answering with large language models <em>(Rating: 2)</em></li>
                <li>MIMIC-IV, a freely accessible electronic health record dataset <em>(Rating: 2)</em></li>
                <li>Evaluation and mitigation of the limitations of large language models in clinical decisionmaking <em>(Rating: 2)</em></li>
                <li>CLIMED-Bench: A large-scale Chinese benchmark for evaluating medical large language models in clinical scenarios <em>(Rating: 1)</em></li>
                <li>MedMCQA: A large-scale multi-subject multi-choice dataset for medical domain question answering <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9034",
    "paper_id": "paper-279260554",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "Low-Level MCQs (MedQA/MedMCQA)",
            "name_full": "Preliminary Knowledge Grasp — Multiple-Choice Questions from MedQA and MedMCQA",
            "brief_description": "A battery of medical multiple-choice questions drawn from MedQA (US subset, 5-option USMLE-style questions) and MedMCQA (4-option Indian exam questions) used to assess factual recall and basic medical knowledge (Bloom's \"Remembering/Understanding\").",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Multiple models (examples: GPT-4o, DeepSeek-V3, GPT-4o-mini, Llama3-70B, Qwen2.5-72B, Phi series, Gemma series, etc.)",
            "model_description": "General and medical domain LLMs across multiple families (Llama, Qwen, Gemma, Phi, GPT, DeepSeek and medical-tuned variants) evaluated as-is or instruction-fine-tuned; architectures are transformer-based foundation LLMs; some models are domain-finetuned (Med42, MMed-Llama, Meditron, ClinicalCamel).",
            "model_size": "various (reported models span ~2B to 72B parameters; GPT-4o size not disclosed in this paper)",
            "test_battery_name": "Low-Level MCQs (MedQA, MedMCQA)",
            "test_description": "Multiple-choice medical exam questions assessing memorization and initial understanding of medical facts and concepts (factual recall). MedQA (US subset) uses 5-option USMLE-styled items; MedMCQA uses 4-option items.",
            "llm_performance": "Top reported normalized accuracies (Table 2): GPT-4o 78.33 ± 0.75%; DeepSeek-V3 78.33 ± 0.75%; GPT-4o-mini 62.67 ± 0.20%; Llama3-70B 63.68 ± 0.34%; Qwen2.5-72B 67.83 ± 0.20%. Many smaller models and some earlier families achieve substantially lower normalized accuracy (examples in Table 2).",
            "human_baseline_performance": "Clinician accuracy on Low-Level tasks (clinician validation, n=4 clinicians sampling 100 items): 68.8% (Table 6).",
            "performance_comparison": "Top-performing LLMs match or exceed the small clinician sample on Low-Level MCQs; larger models and some instruction/inference-time scaled variants show strong performance on factual-recall tests, indicating LLMs can attain near-expert performance on standardized medical MCQs.",
            "experimental_details": "Evaluation used five-shot in-context learning (five Q-A examples) with temperature=0. Models produced answers which were directly compared to ground truth. Normalized accuracy was computed to adjust for random-guessing baseline (random accuracy = 1/Nc for MCQs). MedQA test set and MedMCQA dev set were used. Repeated experiments (five runs) with averaged mean ± standard error reported.",
            "limitations_or_caveats": "Normalization procedure (subtracting random accuracy) affects reported numbers; some expensive models evaluated on subsets (~10%) due to API cost; comparisons across models can be sensitive to instruction tuning, prompt format, and whether chain-of-thought-like prompting was used (the paper uses zero temperature and fixed 5-shot prompt). Related-work claims (e.g., GPT-4 with chain-of-thought achieving 90.2 on MedQA) used different prompting strategies, so direct comparisons are imperfect.",
            "uuid": "e9034.0",
            "source_info": {
                "paper_title": "Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Mid-Level Complex Application",
            "name_full": "Comprehensive Knowledge Application — Statement Validation, Multi-step Rectification, Answer Existence Judgment",
            "brief_description": "A set of reformulations of MCQs to require (1) statement truth validation, (2) two-step rectification (verify given option then correct if wrong), and (3) answer-existence judgement (whether correct answer is present among candidate options) — designed to probe application, discrimination, and limited multi-step reasoning without scenario context.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Multiple models (examples: GPT-4o, DeepSeek-V3, Qwen2.5-72B, Llama3-70B, Phi4-14B, Gemma2-27B, etc.)",
            "model_description": "Same set of general and medically tuned LLMs evaluated under a five-shot ICL setting; Mid-Level test items were generated by reformulating MCQs (using GPT-4o to generate statements/distractors) and verified by doctors.",
            "model_size": "various (reported examples include 7B, 14B, 27B, 32B, 70–72B models; GPT-4o size not specified)",
            "test_battery_name": "Mid-Level Tasks (Statement Validation; Multi-step Rectification; Answer Existence Judgment)",
            "test_description": "Tasks transformed from MCQs to require precise knowledge discrimination (true/false statement validation), multi-step correction (verify a provided option then select the correct one if wrong), and decision in a larger candidate space (determine whether correct answer exists among options); designed to probe Applying/Analyzing cognitive skills.",
            "llm_performance": "Performance falls relative to Low-Level by roughly ~20 percentage points for many models. Examples (normalized accuracies, Table 2 & Table 4): DeepSeek-V3 65.78 ± 0.22%; GPT-4o 64.00 ± 1.02%; Qwen2.5-72B 49.47 ± 0.69%; Llama3-70B 41.79 ± 0.85%. Per-task breakdown (Table 4) shows statement validation and answer-existence tasks moderately supported (e.g., GPT-4o StmtVal 71.25%, AnsExist 58.82%) but multi-step rectification is weaker (e.g., GPT-4o MultiRect 22.46%).",
            "human_baseline_performance": "Clinician accuracy on Mid-Level tasks (clinician validation): 54.2% (Table 6).",
            "performance_comparison": "Top LLMs can exceed clinician accuracy on selected mid-level items, but overall models show a substantial drop from Low-Level performance; tasks that expand the decision space (answer-existence) cause the largest performance degradation (many models drop &gt;40%).",
            "experimental_details": "Five-shot in-context learning, temperature=0. Mid-level items generated using GPT-4o then reviewed by three doctors (reported generation error &lt;5%). Normalization: binary tasks use random accuracy=0.5; multi-step rectification random accuracy derived to be invariant to random strategy (α = 1/Nc), leading to normalization used in reported results. Results averaged over repeated runs.",
            "limitations_or_caveats": "Mid-level dataset generation used GPT-4o (possible generation bias/contamination); although human verification was applied, potential leakage or distributional artifacts may remain. The clinician baseline is small (4 clinicians) and may not generalize; different prompting or chain-of-thought strategies could affect model performance but were not applied here.",
            "uuid": "e9034.1",
            "source_info": {
                "paper_title": "Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "High-Level Full-Path Clinical Diagnosis (MIMIC-IV)",
            "name_full": "Scenario-Based Problem Solving — Full-path Clinical Diagnosis using MIMIC-IV records",
            "brief_description": "An agent-based interactive diagnostic task where the model sequentially orders physical exams/labs/imaging, receives results, and must produce a primary diagnosis; assesses planning, multi-step decision-making, information gathering, and integration (higher-order clinical reasoning).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Instruction-fine-tuned and dialogue-capable LLMs (examples: DeepSeek-V3, GPT-4o, Llama3-70B, Qwen2.5-72B, Phi4-14B, Gemma2-27B)",
            "model_description": "Instruction-tuned variants of the evaluated LLMs were used for the interactive setting; models are transformer-based LLMs of multiple sizes (see Table 7). Some medical-specific models are also evaluated (Med42, Meditron, ClinicalCamel, MMed-Llama).",
            "model_size": "various (examples in Table 7: 8B, 14B, 27B, 32B, 70–72B; DeepSeek-V3 technical details and GPT-4o sizes not fully disclosed in paper table)",
            "test_battery_name": "High-Level Full-Path Clinical Diagnosis (constructed from MIMIC-IV aligned to MedQA disease pool)",
            "test_description": "Interactive scenario-based diagnostic episodes built from inpatient admission records (history of present illness, physical exam, labs, microbiology, imaging). The agent must choose actions (PE, LAB, MICRO, IMAGE, OUTPUT) to request information in sequence and finally output diagnosis. Evaluation metric emphasizes both examination recall (procedural correctness) and diagnosis accuracy (full-path accuracy combines both).",
            "llm_performance": "Models sometimes reach moderate end-point diagnosis accuracy but fail on the full-path metric due to poor exam-ordering/recall. Example results (Table 5): End-point diagnosis accuracy — DeepSeek-V3 53.6%, GPT-4o 49.2%; Examination recall (macro) generally &lt;40% for top models (DeepSeek-V3 30.0%, GPT-4o 31.8%); Full-path diagnosis accuracy (exam recall × diagnosis correctness) — DeepSeek-V3 19.4%, GPT-4o 19.3%, Llama3-70B 18.1%, Qwen2.5-72B 15.8%.",
            "human_baseline_performance": "Clinician accuracy on High-Level tasks (clinician validation subset): 23.5% (Table 6).",
            "performance_comparison": "While some LLMs can reach comparable end-point diagnosis accuracy to clinicians in certain cases, they perform worse on the full-path metric because they fail to request and integrate needed examinations (low examination recall). Overall, LLMs and clinicians both find these high-level scenario tasks challenging, but LLMs' procedural shortcomings drive their lower full-path scores.",
            "experimental_details": "Zero-shot evaluation in an agent-based interactive protocol; instruction-fine-tuned models used; temperature=0.8 to encourage exploration; action parsing and result mapping used UMLS and handcrafted synonym mappings with fuzzy matching (threshold 0.9) and manual verification; when models output invalid actions they are warned and asked to re-output; repeated 3–5 evaluation runs; full dataset: 2,176 admission records across 42 diseases from MIMIC-IV filtered to align with MedQA disease pool.",
            "limitations_or_caveats": "Full-path metric is sensitive to correct parsing of requested exam items (paper reports parsing recall ~95% after manual verification). Some evaluated models were tested on a subset due to API cost (notably GPT-4o and DeepSeek-V3 evaluated on ~10% in some analyses). Many medical models sometimes fail to follow instructions (repeating prompts or outputting irrelevant text), producing near-zero scores; the clinician baseline is from a small sample and may not represent broader clinician performance; task complexity and EHR note quality can limit diagnostic signal.",
            "uuid": "e9034.2",
            "source_info": {
                "paper_title": "Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GPT-4 on MedQA (related work)",
            "name_full": "GPT-4 evaluated on MedQA (prior cited work)",
            "brief_description": "The paper cites prior work reporting that GPT-4 achieved high accuracy on the MedQA medical exam benchmark when using complex chain-of-thought prompting.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-4 (OpenAI, cited in related work)",
            "model_description": "Large foundation LLM from OpenAI that in cited studies was prompted with chain-of-thought strategies to solve medical exam items.",
            "model_size": "not specified in this paper",
            "test_battery_name": "MedQA (medical exam benchmark)",
            "test_description": "Medical multiple-choice exam questions (MedQA) used in existing literature to assess medical knowledge of LLMs.",
            "llm_performance": "Reported in related work (cited): GPT-4 achieves 90.2% accuracy on MedQA when using complex chain-of-thought prompting (cited in Related Work section).",
            "human_baseline_performance": "Not provided for that cited result within this paper.",
            "performance_comparison": "Cited work characterizes GPT-4 performance as approaching expert-level on MedQA under specific prompting (chain-of-thought); the current paper notes such prior results but also highlights that performance degrades on higher cognitive and scenario-based tasks.",
            "experimental_details": "Cited result used complex chain-of-thought prompting (details in the cited literature); current paper did not replicate that specific prompting strategy in its main evaluations.",
            "limitations_or_caveats": "Different prompting protocols (e.g., chain-of-thought) and evaluation scopes make direct comparisons with the current paper's normalized results difficult; the current paper emphasizes broader cognitive-level evaluation rather than reproducing chain-of-thought gains.",
            "uuid": "e9034.3",
            "source_info": {
                "paper_title": "Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "What disease does this patient have? a large-scale open domain question answering dataset from medical exams",
            "rating": 2,
            "sanitized_title": "what_disease_does_this_patient_have_a_largescale_open_domain_question_answering_dataset_from_medical_exams"
        },
        {
            "paper_title": "Capabilities of gpt-4 on medical challenge problems",
            "rating": 2,
            "sanitized_title": "capabilities_of_gpt4_on_medical_challenge_problems"
        },
        {
            "paper_title": "Towards expert-level medical question answering with large language models",
            "rating": 2,
            "sanitized_title": "towards_expertlevel_medical_question_answering_with_large_language_models"
        },
        {
            "paper_title": "MIMIC-IV, a freely accessible electronic health record dataset",
            "rating": 2,
            "sanitized_title": "mimiciv_a_freely_accessible_electronic_health_record_dataset"
        },
        {
            "paper_title": "Evaluation and mitigation of the limitations of large language models in clinical decisionmaking",
            "rating": 2,
            "sanitized_title": "evaluation_and_mitigation_of_the_limitations_of_large_language_models_in_clinical_decisionmaking"
        },
        {
            "paper_title": "CLIMED-Bench: A large-scale Chinese benchmark for evaluating medical large language models in clinical scenarios",
            "rating": 1,
            "sanitized_title": "climedbench_a_largescale_chinese_benchmark_for_evaluating_medical_large_language_models_in_clinical_scenarios"
        },
        {
            "paper_title": "MedMCQA: A large-scale multi-subject multi-choice dataset for medical domain question answering",
            "rating": 2,
            "sanitized_title": "medmcqa_a_largescale_multisubject_multichoice_dataset_for_medical_domain_question_answering"
        }
    ],
    "cost": 0.018533749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving
10 Jun 2025</p>
<p>Yuxuan Zhou 
Department of Electronic Engineering
Tsinghua University
BeijingChina</p>
<p>Xien Liu <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#120;&#101;&#108;&#105;&#117;&#64;&#109;&#97;&#105;&#108;&#46;&#116;&#115;&#105;&#110;&#103;&#104;&#117;&#97;&#46;&#101;&#100;&#117;&#46;&#99;&#110;">&#120;&#101;&#108;&#105;&#117;&#64;&#109;&#97;&#105;&#108;&#46;&#116;&#115;&#105;&#110;&#103;&#104;&#117;&#97;&#46;&#101;&#100;&#117;&#46;&#99;&#110;</a>. 
Department of Electronic Engineering
Tsinghua University
BeijingChina</p>
<p>Chenwei Yan 
School of Computer Science
Beijing University of Posts and Telecommunications
BeijingChina</p>
<p>Chen Ning 
Department of Electronic Engineering
Tsinghua University
BeijingChina</p>
<p>Xiao Zhang 
Department of Electronic Engineering
Tsinghua University
BeijingChina</p>
<p>Boxun Li 
Infinigence-AI
BeijingChina</p>
<p>Xiangling Fu 
School of Computer Science
Beijing University of Posts and Telecommunications
BeijingChina</p>
<p>Shijin Wang 
iFLYTEK Research
HefeiChina</p>
<p>Guoping Hu 
iFLYTEK Research
HefeiChina</p>
<p>Yu Wang 
Department of Electronic Engineering
Tsinghua University
BeijingChina</p>
<p>Ji Wu 
Department of Electronic Engineering
Tsinghua University
BeijingChina</p>
<p>College of AI
Tsinghua University
BeijingChina</p>
<p>BNRist
BeijingChina</p>
<p>Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving
10 Jun 2025EFEB3F911A2E4D129F4E3CDE6756AAD9arXiv:2506.08349v1[cs.CL]
Large language models (LLMs) have demonstrated remarkable performance on various medical benchmarks, but their capabilities across different cognitive levels remain underexplored.Inspired by Bloom's Taxonomy, we propose a multi-cognitive-level evaluation framework for assessing LLMs in the medical domain in this study.The framework integrates existing medical datasets and introduces tasks targeting three cognitive levels: preliminary knowledge grasp, comprehensive knowledge application, and scenariobased problem solving.Using this framework, we systematically evaluate state-of-the-art general and medical LLMs from six prominent families: Llama, Qwen, Gemma, Phi, GPT, and DeepSeek.Our findings reveal a significant performance decline as cognitive complexity increases across evaluated models, with model size playing a more critical role in performance at higher cognitive levels.Our study highlights the need to enhance LLMs' medical capabilities at higher cognitive levels and provides insights for developing LLMs suited to real-world medical applications.</p>
<p>Introduction</p>
<p>Large language model (LLM) technology has witnessed rapid advancement (Ouyang et al., 2022;Achiam et al., 2023;Anil et al., 2023;Touvron et al., 2023a) and shown potential in various fields, including medicine.Recently, startof-the-art LLMs (e.g.,  have achieved expert-level performance across medical benchmarks (Singhal et al., Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025.Copyright 2025 by the author(s).2023a;b; Nori et al., 2023a;Qiu et al., 2024), demonstrating their promise in real-world medical applications.However, studies (Hager et al., 2024;Yan et al., 2025) demonstrate that these models still face challenges in handling tasks that more closely resemble real-world medical scenarios, such as clinical diagnosis and treatment.This disparity raises a critical question: how far are current LLMs from being truly applicable in real-world medical scenarios?</p>
<p>Instead of directly answering this question, let's first consider a related question: what steps are required for a medical student to become a qualified physician?To become a qualified physician, a medical student typically undergoes a series of training stages, as shown in Figure 1 (a).First, the student acquires basic medical knowledge (e.g., anatomy, physiology, and pathology) through textbooks and lectures during the study in medical school.Then, the student learns to apply this knowledge in analyzing clinical cases during their clinical internship.Finally, the student practices diagnosing and treating patients under the guidance of experienced physicians during their residency.Such a training process is designed to align with the human cognitive process, as outlined in Bloom's Taxonomy (Anderson &amp; Krathwohl, 2001): first memorizing and understanding knowledge, then applying it comprehensively, and finally using it to plan and solve problems in real-world scenarios.</p>
<p>Similar to the training process, the evaluation process in the human education system also follows the human cognitive nature.For example, a typical exam sheet includes a variety of question types (e.g., multiple-choice, short-answer, and long-response questions) that are intentionally designed to assess students' abilities across different cognitive levels.In contrast, though existing medical benchmarks provide valuable insights into LLMs' medical capabilities, they primarily evaluate LLMs' capabilities at specific cognitive levels: most medical benchmarks (Jin et al., 2021;Pal et al., 2022;Wang et al., 2024;Cai et al., 2024;Qiu et al., 2024) evaluate LLMs' capabilities through question-answering (QA) tasks, which mainly focus on assessing LLMs' preliminary knowledge grasp; some other benchmarks (Hager et al., 2024;Ouyang et al., 2024) adopt tasks such as clinical diagnosis and treatment to evaluate LLMs' capabilities of scenario-based problem solving.</p>
<p>In this paper, we argue that the evaluation of LLMs should also follow the cognitive development process, i.e., evaluating LLMs' medical capabilities across multiple cognitive levels.To this end, we propose a multi-cognitive-level evaluation framework (MultiCogEval) to provide a comprehensive evaluation of LLMs' medical capabilities from a cognitive perspective.The schema of this framework is depicted in Figure 1 (b).Specifically, we consider three cognitive levels corresponding to the training process of human clinicians: preliminary knowledge grasp, comprehensive knowledge application, and scenario-based problem solving.</p>
<p>Built on that, we design tasks that target each cognitive level and integrate three existing medical datasets to generate test samples for each task.To make the performance of LLMs comparable across different cognitive levels, we align the medical knowledge coverage across tasks at different levels as much as possible and normalize the performance metrics for each task.</p>
<p>Using this framework, we systematically evaluate existing general and medical LLMs across 2B -70B parameters from six prominent families: Llama, Qwen, Gemma, Phi, GPT, and DeepSeek.Our findings reveal that while current SOTA LLMs generally perform well on the preliminary knowledge grasp level, their performance declines significantly as the cognitive level increases.Moreover, we find that model size plays a more crucial role in performance at higher cognitive levels.Our study provides a clear landscape of LLMs' medical capabilities across different cognitive levels and highlights the need to enhance LLMs' medical capabilities at higher cognitive levels.The codes and datasets are available at https://github.com/THUMLP/MultiCogEval.Our contributions are:</p>
<p>• We propose a novel evaluation framework for assessing LLMs' medical capabilities across multiple cognitive levels inspired by the human cognitive process.</p>
<p>• Based on the proposed framework, we systematically evaluate state-of-the-art general and medical LLMs across six prominent families.</p>
<p>• We reveal a significant performance decline as cognitive complexity increases across evaluated models, offering insights for developing LLMs suited to realworld medical applications.</p>
<p>Related Work</p>
<p>LLM Medical Evaluation Benchmarks</p>
<p>Recently, several medical evaluation benchmarks have been proposed to assess LLMs' medical capabilities.Most of existing medical benchmarks typically utilize the Questionanswering (QA) form, where the questions are sourced from medical exams (Jin et al., 2021;Pal et al., 2022;Cai et al., 2024;Qiu et al., 2024), medical literature (Jin et al., 2019), and consumer health questions (Ben Abacha et al., 2017;Singhal et al., 2023a).Recent studies (Nori et al., 2023a;b;Singhal et al., 2023a) indicate that several LLMs perform notably on these benchmarks.For example, GPT-4 achieves an accuracy of 90.2 (with complex chain-of-thoughts prompting strategy) on the medical exam benchmark MedQA, approaching expert-level performance.Other benchmarks, such as MIMIC-IV-Ext (Hager et al., 2024) and CLIMED-Bench (Ouyang et al., 2024), adopt scenario-based tasks such as clinical diagnosis to evaluate LLMs' capabilities in solving medical problems in real-world scenarios.However, these benchmarks primarily focus on evaluating LLMs at specific cognitive levels and lack a holistic view of LLMs' medical capabilities across multiple cognitive levels.</p>
<p>Bloom's Taxonomy</p>
<p>Bloom's Taxonomy (Bloom et al., 1956) is a widely used framework for categorizing learning objectives.For learning objectives in the cognitive domain, Bloom's Taxonomy initially proposed six levels: Knowledge, Comprehension, Application, Analysis, Synthesis, and Evaluation.In 2001, a revision of Bloom's Taxonomy (Anderson &amp; Krathwohl, 2001) was proposed, where the six levels were renamed and reordered as follows: Remembering, Understanding, Applying, Analyzing, Evaluating, and Creating.Inspired by the Revised Bloom's Taxonomy and the training process of human clinicians, this paper introduces a multi-cognitivelevel evaluation framework to assess whether LLMs have achieved the learning objectives in the medical domain across varying cognitive levels.</p>
<p>Methodology</p>
<p>Multi-Cognitive-Level Evaluation Priciples</p>
<p>We first illustrate the idea of the proposed multi-cognitivelevel evaluation.Given a large language model M and a series of cognitive levels L 1 , L 2 , . . ., L n , the goal of the multi-cognitive-level evaluation is to assess the capabilities of M at each cognitive level:
f (M) = <a href="1">f 1 (M), f 2 (M), . . . , f n (M)</a>
where f refers to the whole multi-cognitive-level evaluation process, f i represents the evaluation process at the i-th cognitive level, and f i (M) denotes the corresponding evaluation results.</p>
<p>A qualified multi-cognitive-level evaluation should also adhere to the following principles:</p>
<p>• Task Relevance: The evaluation tasks should target different cognitive levels, aligning with the knowledgelearning process.</p>
<p>• Knowledge Consistency: In order to isolate the effects of cognitive levels, it is important to ensure that the evaluation tasks have consistent knowledge coverage across different cognitive levels, minimizing the influence of knowledge coverage on the evaluation results.</p>
<p>• Metric Alignment: The performance metrics should be aligned (normalized) across different cognitive levels to ensure the comparability of the evaluation results.</p>
<p>In our study, we adopt these principles to develop a multicognitive-level evaluation framework for assessing LLMs' medical capabilities.Though we primarily focus on the medical domain, the proposed multi-cognitive-level evaluation principles can also be generalized to other domains.</p>
<p>Cognitive Levels in Medical Domain</p>
<p>Following the multi-cognitive-level evaluation principles, we first define three cognitive levels for assessing LLMs' medical capabilities by aligning with the training process of human clinicians and referring to Bloom's Taxonomy.The three cognitive levels are as follows1 :</p>
<p>• Preliminary Knowledge Grasp (L 1 ): The first level corresponds to the education of medical students in medical school, where they learn and grasp basic medical knowledge through textbooks and lectures.At this level, we focus on evaluating LLMs' capabilities in memorizing and initially understanding medical knowledge that is essential for medical practice.At this level, we aim to evaluate LLMs' capabilities in planning and solving problems in real-world medical scenarios using medical knowledge.</p>
<p>Multi-Cognitive-Level Evaluation Framework</p>
<p>Based on the defined cognitive levels, we develop a multicognitive-level evaluation framework that integrates existing medical datasets and designs tasks tailored to each cognitive level.The overview of the proposed framework is illustrated in Figure 2. In the following sections, we will introduce the detailed design of the evaluation tasks, dataset construction, and performance metrics that follow the multi-cognitivelevel evaluation principles proposed in Section 3.1.</p>
<p>Task Design For Low-Level tasks (f 1 ), we adopt multiplechoice questions (MCQs) as simple knowledge recalling tasks for Low-Level, where the model is required to recognize the correct answer from multiple candidate options (see the left part of Figure 2).Such task format is commonly used in existing medical benchmarks and human examinations to evaluate students' basic knowledge understanding.</p>
<p>For Mid-Level tasks (f 2 ), given that existing medical benchmarks fail to adequately address this cognitive level-either being too simplistic, as in QA-based benchmarks, or overly complex, as in scenario-based benchmarks-we reformulate the original MCQs into a set of "Complex Knowledge Application" tasks for this level.These tasks are designed to assess the cognitive skills required for real-world medical tasks from different perspectives without relying on specific scenarios, thereby effectively evaluating the model's ability in the Mid-Level.Specifically, we consider the following task types (see the middle part of Figure 2):</p>
<p>• Statement Validation Questions: We transform the question and one of the candidate options from the original MCQs into a statement, requiring the model to determine whether the statement is true or false.While this task may intuitively seem simpler than MCQs, the answer choices in MCQs can provide additional clues, allowing the model to choose the most plausible answer without fully understanding why it is correct.In contrast, statement validation emphasizes the model's ability to precisely discriminate knowledge, closely mirroring real-world clinical scenarios where options are typically not provided.Furthermore, given that some MCQ distractors significantly increase task difficulty, we construct two statements per MCQ: one reflecting the correct answer and the other derived from a distractor, maintaining their difficulty.</p>
<p>• Multi-Step Rectification Questions: Handling QA tasks typically requires a single decision-making step, whereas problem-solving in real-world scenarios often involves multiple steps.For instance, when managing a patient's care, a doctor first establishes a diagnosis and then prescribes treatment based on the diagnosis made in the previous step.Inspired by this, we reformulate the original MCQ into a two-step task: we first ask the model to verify whether a provided option is the correct answer of the MCQ, and then, if the answer is incorrect, we ask the model to give the correct answer from the rest of options.We generate two questions for each MCQ: one provides the correct answer, while the other provides a randomly sampled wrong option.</p>
<p>• Answer Existence Judgment Questions: QA tasks typically operate within a constrained decision space (e.g., predefined answer choices), whereas real-world tasks involve significantly broader decision spaces.For instance, there may be hundreds of potential candidate diseases in clinical diagnosis, requiring more complex decision-making to reach the correct diagnosis.To simulate this, we design a task where the model is asked to determine whether the correct answer exists in a set of candidate options.For each original MCQ, we generate a pair of questions with opposite labels to eliminate the influence of random guessing: one question retains the original options, while the correct option in the other question is replaced with a distractor.</p>
<p>For High-Level tasks (f 3 ), we follow Hager et al. (2024) and assess LLMs' capabilities of solving scenario-based problems using a full-path clinical diagnosis task, as illustrated in Figure 3. Compared to the tasks in the Low-Level and Mid-Level, this task simulates a real-world diagnostic process by requiring the model to sequentially order examinations and integrate information from the obtained examination results to arrive at a final diagnosis.Solving this task requires models to perform multi-step decision-making within a larger decision space, mirroring the complexity of real-world medical scenarios.</p>
<p>Dataset Construction For Low-Level tasks, we directly use the questions from the MedQA (Jin et al., 2021) and MedMCQA (Pal et al., 2022) datasets, which cover a wide range of medical knowledge and are widely used to evaluate LLMs' medical capabilities.For Mid-Level tasks, as introduced above, we reformulate each original MCQ into two statement validation questions, two multi-step rectification questions, and two answer existence judgment questions.We apply the advanced GPT-4o to assist in data generation.Specifically, GPT-4o is employed to (1) generate statements based on the questions and the provided options for the statement validation tasks and (2) generate the distractors for the answer existence judgment tasks based on the original question and the correct answer.We arrange for three doctors with over three years of experience to verify the statements and distractors generated by GPT-4o and find that the error rate of GPT-4o is less than 5%.</p>
<p>For High-Level tasks, we leverage the MIMIC-IV dataset (Johnson et al., 2023), which contains detailed electronic health records (EHRs) of patients, including the history of present illness, physical examination, laboratory tests, microbiology tests, and imaging tests.We employ a recognize-and-retrieve procedure to construct the dataset, ensuring its knowledge coverage remains consistent with previous levels (see Figure 4).Specifically, we first use the medical NER tool MedCAT (Kraljevic et al., 2021) to identify diseases in previous-level tasks and construct a disease pool.We then filter the admission records associated with these diseases using ICD-10-CM codes, which are widely employed for disease classification.To ensure alignment between primary diagnoses and the disease pool, we extract admission records where the primary discharge diagnosis matches a disease in the pool.Records with more than two diseases in the discharge diagnosis are excluded to maintain a one-to-one mapping between records and diseases.After processing, 2,176 admission records remain, covering 42 diseases across eight human body systems.Finally, we convert these records into test samples for the full-path clinical
Accuracy proc = 1 N N i=1 I(Diag (i) pred = Diag (i) gt ) × Recall (i)</p>
<p>exam</p>
<p>(2) where Diag (i) pred and Diag (i) gt denote the predicted and groundtruth diagnoses of the i-th admission record, respectively.I(•) is the indicator function, and N is the number of admission records.Recall (i)  exam is the examination recall of the i-th admission record, which is calculated as:
Recall (i) exam = # Correct Ordered Exam Items # Total Exam Items in the Record (3)
In this study, we prioritize examination recall, as missing critical examination information can have far more severe consequences than ordering non-essential tests during diagnosis.To ensure fairness, we compute the macro-average of both examination recall (across examination types) and diagnosis accuracy (across diseases), assuming equal importance for each examination type and disease in this task.</p>
<p>Considering that the random guessing performance may vary across different tasks, we also calculate normalized accuracy of each task by reducing the effect of random guessing to ensure the comparability of the evaluation results across different cognitive levels:</p>
<p>Accuracy norm = Accuracy model − Accuracy rand 1 − Accuracy rand (4) More details about the metrics and normalization process are provided in Appendix B.  (Touvron et al., 2023a;b;Dubey et al., 2024), Qwen (Bai et al., 2023;Yang et al., 2024;Hui et al., 2024), Gemma (Mesnard et al., 2024;Team et al., 2024), Phi (Abdin et al., 2024b;a), GPT-4o (OpenAI, 2024), and DeepSeek (Liu et al., 2024).We also evaluate 8 medical LLMs from 4 series: ClinicalCamel (Toma et al., 2023), Med42 (v1, v2) (Christophe et al., 2024a;b), Meditron (Chen et al., 2023), and MMed-Llama (Qiu et al., 2024).The detailed statistics are provided in Appendix C.</p>
<p>Evaluation Setting For tasks in the Low-Level and Mid-Level, we leverage the five-shot in-context learning (Brown et al., 2020) for evaluation, where five input-output pairs are sampled as the demonstrative examples to guide the model to generate the correct answer for the test sample (see Figure 9 in Appendix A).For the High-Level task (full-path diagnosis), as it involves multi-turn interaction, we evaluate the instruction fine-tuned versions of the corresponding models and leverage the zero-shot learning setting to evaluate the models' performance.Specifically, for each test sample, we provide the model with the instructions for the task and ask it to order the specific exam items sequentially and make the final diagnosis.We utilize the UMLS Metathesaurus (Bodenreider, 2004) along with a handcrafted synonym set to identify the model's predictions for ordered exam items and the final diagnosis (the detection rate across all categories reached ∼95%).We conduct repeated evaluations 3-5 times and report the average performance and standard error.More details about the evaluation setting (e.g., hyperparameters) are provided in Appendix D. average performance of the three task types to compare with the other two levels (Detailed performance is provided in Table 8 of Appendix E).We observe that several state-of-theart LLMs, such as GPT-4o, DeepSeek-V3, and Llama3-70B, achieve remarkable performance (&gt;60%) on the Low-Level (preliminary knowledge grasp) tasks.However, all of these models experience a significant performance drop (∼20%) when evaluated on the Mid-Level tasks (comprehensive knowledge application).Moreover, on High-Level tasks (scenario-based problem solving), the performance of all models further declines, with the best-performing model, DeepSeek-V3, achieving only 19.4 full-path diagnosis accuracy.This indicates that while current LLMs grasp basic medical knowledge well, they still face significant challenges at higher cognitive levels, particularly when solving complex problems in real-world medical scenarios.</p>
<p>Parameter Sizes Matter, Especially at Higher Cognitive Levels We further investigate the impact of model parameter sizes across different cognitive levels.Specifically, we examine 7B and 70B models from the Llama and Qwen families, along with GPT-4o and GPT-4o-mini2 .As shown in Figure 5, larger models consistently outperform their smaller counterparts within the same model series.</p>
<p>Additionally, the performance gap between model sizes increases significantly from the Low-Level to the Mid-Level on a logarithmic scale, suggesting that model size plays a more critical role in enabling LLMs to tackle complex problem-solving tasks.However, this gap narrows slightly at the High-Level, implying that even the 70B-level models struggle to effectively solve scenario-based problems.Medical-Domain Finetuning Primarily Benefits Lowand Mid-Level Tasks To investigate the impact of medical-domain finetuning on LLMs across different cognitive levels, we compare the performance of medical LLMs with their backbone models.As shown in Figure 6, these specialized models generally outperform their backbone counterparts on Low-and Mid-Level tasks, with performance gains reaching up to 15%.However, they fail to achieve significant improvements on High-Level tasks and even underperform their backbone models.This phenomenon indicates that while medical-domain finetuning strengthens LLMs' grasp of basic medical knowledge and its comprehensive application, it remains less effective in addressing complex real-world medical scenarios.</p>
<p>Low-Level</p>
<p>Inference-time Scaling Works Well, Especially for Higher Levels We further investigate the effectiveness of inference-time scaling on LLMs' medical capabilities across different cognitive levels.We select two representative models with varied parameter scales (DeepSeek-V3 and GPT-4o-mini) and their corresponding inference-time scaling versions (DeepSeek-R1 and o3-mini) for study.As presented in Table 3, the inference-time scaling models consistently outperform their backbone models across all cog-  nitive levels, while they achieve more significant improvement on Mid-Level tasks compared to Low-Level tasks (e.g., +23.1 vs. +9.8for DeepSeek-R1).Note that the performance gap narrows on high-level tasks due to their significantly increased difficulty.This suggests that inference-time scaling is a promising approach to enhance LLMs' medical capabilities, particularly for complex problem-solving tasks.</p>
<p>Fine-Grained Analysis</p>
<p>LLMs Perform Constantly Worse Across Tasks in the Mid-Level We further provide a fine-grained analysis of LLMs' performance across tasks within the Mid-Level and compare it with the Low-Level tasks.We illustrate results of the best-performed model from each LLM family in Table 4, with the full results presented in Table 8.We observe that LLMs consistently perform worse on all Mid-Level tasks compared to Low-Level tasks.Specifically, the performance drop on the answer existence judgment tasks is more significant, where almost all models experience a performance decrease of over 40%.This suggests that current LLMs struggle with medical problems involving significantly larger decision spaces, which is a key requirement in  diagnosis task and present the results in Figure 7.We observe that the performance of LLMs varies significantly across different diseases, forming a long-tail distribution, while 70B models generally perform better than 7B models across all diseases.This suggests that current LLMs may struggle with diagnosing rare or complex diseases, which are critical in real-world medical scenarios.</p>
<p>Clinician Validation</p>
<p>Finally, to ensure that the tasks at different levels of the proposed benchmark genuinely correspond to distinct levels of cognitive difficulty, we further conduct a small-scale clinician validation study.Specifically, we randomly select 100 samples from the constructed benchmark and recruit four licensed clincians with 3 to 8 years of experience to assess the benchmark's difficulty from two perspectives: (1) their accuracy in answering the questions and (2) their subjective difficulty ratings to the questions on a scale from 1 (Easy) to 10 (Hard).The evaluation results are provided in Table 6.We found that clinicians' accuracy also decreases as the cognitive level increases across three levels, indicating that tasks at higher levels are indeed more challenging.Moreover, their subjective difficulty ratings align with this trend, further demonstrating the validity of the proposed benchmark.More details about the clinician validation study, including the data selection and evaluation process, are provided in Appendix F.</p>
<p>Conclusion</p>
<p>In this work, we propose a multi-cognitive-level evaluation framework that assesses LLMs' medical capabilities at three cognitive levels.Using the proposed framework, we construct a new medical benchmark and systematically evaluate existing LLMs on the benchmark.Our study leads to the following key findings: (1) While the performance of smaller LLMs (∼10B) gradually approaches that of larger LLMs (&gt;70B) on Low-Level tasks, the performance gap remains significant on Mid-Level and High-Level tasks, indicating that model size plays a crucial role in enabling LLMs to tackle complex medical problems; (2) Medicaldomain finetuning significantly improves performance on Low-and Mid-Level tasks but has limited impact on High-Level tasks.This suggests that current medical-specific finetuning strategies are insufficient for enhancing LLMs' reasoning abilities in complex real-world medical scenarios;</p>
<p>(3) Inference-time scaling shows promise in boosting LLMs' medical capabilities, especially in tasks requiring complex knowledge application.However, further research is required to enhance LLMs' High-Level capabilities, such as planning, requesting key information, and reasoning based on the acquired information.</p>
<p>Based on these findings, we offer the following insights for applying and developing large language models to address real-world clinical challenges: (1) Inference-time scaling emerges as a promising direction to support these high-level capabilities.</p>
<p>To the best of our knowledge, this work is the first to evaluate LLMs' medical capabilities across multiple cognitive levels.</p>
<p>While promising, the evaluation may be constrained by the scope of medical knowledge coverage and task diversity.Future research could further explore these areas, expanding the range of medical domains and tasks to offer a more holistic view of LLMs' medical abilities.</p>
<p>Impact Statement</p>
<p>This work mainly explores a multi-cognitive-level evaluation framework for assessing LLMs' medical capabilities.</p>
<p>The results indicate that current LLMs generally perform well on the preliminary knowledge grasp but face challenges at higher cognitive levels, providing insights for developing LLMs suited to real-world medical applications.The datasets and codes used in this study are built based on public medical datasets and are publicly available to facilitate further research.We have not identified any potential negative ethical consequences requiring further consideration.</p>
<p>A. Details of Datasets</p>
<p>Question: A 25-year-old man comes to the office because of pain in his left shoulder.says that this pain started 3 years ago … Which of the following enzymes is most likely deficient in this patient?</p>
<p>Options</p>
<p>A.1. Low-Level Tasks</p>
<p>As introduced in Section 3.3, we adopt the original multiple-choice questions in the MedQA and MedMCQA datasets for Low-Level tasks.MedQA is a large-scale medical exam dataset, containing medical exam questions sourced from three different regions.In this study, we use the questions in the US subset, which contains five-option multiple-choice questions collected from the United States Medical Licensing Examination (USMLE).MedMCQA is another large-scale medical exam dataset that contains multiple-choice questions sourced from All India Institute of Medical Sciences Entrance Examination (AIIMS) and National Eligibility cum Entrance Test for Postgraduate (NEET-PG) in India.The questions in MedMCQA are four-option MCQs.The language used in both datasets is English.For MedQA, we use the test set for evaluation; for MedMCQA, we use the dev set, because the ground truth of MedMCQA's test set is not available to the public.We filtered out the MedMCQA questions that are marked as multiple correct answers.As a result, we have 800 questions from MedQA and 2,816 questions from MedMCQA for Low-Level tasks.Examples of Low-Level tasks are shown in Figure 8.We use five-shot in-context learning to evaluate the performance of LLMs on Low-Level tasks.Figure 9 shows the input prompt format of the five-shot in-context learning setting.The input prompt consists of five examples, each of which is a question-answer pair.After the five examples, we append the test sample, which is a question without the correct answer.The model is required to predict the correct answer for the test sample.For answer parsing, we find that current LLMs always follow the correct format in the five-shot in-context learning setting.Therefore, we directly compare the generated answer with the ground truth answer.</p>
<p>A.2. Mid-Level Tasks</p>
<p>Question: A 25-year-old man comes to the office because of pain in his left shoulder.because of pain in his left shoulder.He says that this pain started 3 years ago and has progressively worsened.He denies joint trauma, fever, dysuria, or morning stiffness.… Statement: "Homogentisic acid oxidase is most likely deficient in this patient", is the statement above correct or incorrect?</p>
<p>Answer: correct</p>
<p>Question: A 25-year-old man comes to the office because of … Which of the following enzymes is most likely deficient in this patient?Options: A: Branched-chain alpha-ketoacid dehydrogenase B: Cystathionine synthase deficiency C: Homogentisic acid oxidase D: Phenylalanine hydroxylase E: Propionyl-CoA carboxylase Alice chose answer D. Please verify if this is correct, and if not, provide the correct answer.Answer: incorrect, answer C is the correct answer Question: A 25-year-old man …There is a 50% chance that the correct answer is not included in the options.Please determine if the correct answer is among the given options and respond with yes or no.We construct Mid-Level tasks by reformulating the original MCQs in MedQA and MedMCQA into statement verification tasks, multi-step rectification tasks, and answer existence judgment tasks.For the Statement Validation task, we prompt GPT-4o to generate a statement based on the question and a given option, using the following prompt format: We found that providing both the correct answer and the given option in the prompt can help the model generate more accurate statements.We also provide two examples of the generation process in the prompt to help the model understand the task.For the Answer Existence Judgment task, we prompt GPT-4o to generate a distractor based on the question and the correct answer:</p>
<p>Options</p>
<p>The following is a medical question along with its correct option.To increase the difficulty of the question, please generate a misleading distractor based on the correct option.This distractor should be an incorrect answer to the question.To ensure that GPT-4o can correctly generate the desire statements and distractors, we have three experienced medical experts manually verify a batch of 100 generated statements and distractors.We found that GPT-4o can generate correct statements and distractors with an accuracy of around 95%.</p>
<p>For generating questions of Mid-Level, we leverage question templates illustrated in Figure 10 to generate corresponding questions, using options and correct answers from the original MCQs, the generated statements, and the generated distractors.</p>
<p>For evaluation, we use the same five-shot in-context learning setting as in Low-Level tasks.</p>
<p>A.3. High-Level Tasks</p>
<p>For the High-Level task, following Hager et al. (2024), we construct a full-path clinical-diagnosis evaluation dataset based on the MIMIC-IV dataset.MIMIC-IV is a large-scale, de-identified, and publicly available dataset that contains electronic health records (EHRs) of patients admitted in Beth Israel Deaconess Medical Center in Boston, MA.To make the generated dataset align with tasks in the previous levels, we first identify and extract diseases involved in the MedQA and MedMCQA datasets to form a disease pool, and then retrieve the corresponding admission records from the MIMIC-IV dataset.Specifically, we first use the ICD-10-CM codes to filter the relevant admission records for each disease, and extract the discharge diagnosis section of these admission records.To ensure that the admission records are highly relevant to the diseases, we only keep the records that the corresponding disease is in the first place of the discharge diagnosis.We further exclude the records that the discharge diagnosis contains multiple diseases in the disease pool.To ensure the statistic significance of the evaluation and balance across selected diseases, we only keep the diseases that have more than 10 admission records, and randomly sample 100 admission records for disease that have more than 100 records.As a result, we obtain 2,176 admission records for 42 diseases.The distribution of the number of admission records for each disease and their affected human body systems are shown in Figure 11.We then construct the full-path clinical-diagnosis evaluation dataset by extracting the history of present illness, physical examination sections from the clinical notes of the admission records, and the corresponding laboratory tests, microbiology tests, and imaging tests from the clinical data tables.Then, we construct an agent-based evaluation setting, where the model is required to make decisions (e.g., request specific tests, make the final diagnosis) based on the information it obtain through the interaction process.Specifically, for each admission record, we first provide the history of present illness to the model, using the following prompt format:</p>
<p>You are an experienced medical AI assistant.Your ultimate goal is to help the doctor diagnose the patient's condition.You will be provided with the patient's history and the results of any tests that the doctor has already performed.You can also order additional tests for more information, including physical examinations, laboratory tests, microbiology tests, and imaging.</p>
<p>The action you can choose are:</p>
<ol>
<li>PE: Perform physical examination of patient and receive the observations.If the model chooses the "PE" action, we provide the physical examination section of the clinical notes to the model using the following prompt format: Otherwise, if the model chooses one of the "LAB", "MICRO", or "IMAGE" actions, we will further ask the model to provide the detailed list of examination items of the corresponding test type: To parse the model's requested examination items, we leverage the name of examination items appeared in MIMIC-IV to form an examination pool.Then, we use UMLS to map the examination items in the pool to the corresponding CUIs, and retrieve the corresponding synonyms of the CUIs.Finally, we extract and match the synonyms of the CUIs with the model's requested examination items using fuzzy matching.If the similarity between the synonyms and the model's requested examination items is above a certain threshold (0.9), we consider the examination items are correctly parsed.We further conduct manual verification over the examination items that are not identified by the fuzzy matching, and construct a mapping table between the missing examination items and their standard names in MIMIC-IV.Finally, we verify the recall of the examination items parsing process by randomly sampled 100 recognition results randomly sampled from the evaluated models' outputs, and have three experienced doctors manually verify the results.We calculate the recall of the examination items parsing process by comparing the manual verification results with the recognition results of the evaluated models.We find that the recall is around 95% across all three types of tests.</li>
</ol>
<p>After the parsing process, we provide the model with the results of the requested tests, and ask the model to choose the next action: If the model fails to output the valid action, we will warn the model and ask the model to choose the next action again:</p>
<p>You have chosen an action that is not available or used the wrong format.</p>
<p>Your output format should be: Once the model chooses the "OUTPUT" action, we will further ask the model to output the final diagnosis:</p>
<p>You choose "OUTPUT" as the next action.Please output the final diagnosis for this patient.</p>
<p>Your output format should be:</p>
<p>Diagnosis: (the final diagnosis, specific disease name)</p>
<p>We use the zero-shot learning setting to evaluate the performance of LLMs on this full-path clinical-diagnosis task.When the chat history length exceed the maximum token length of the model, we ask the model to summarize the chat history and provide the summary to the model as the input prompt:</p>
<p>Summarize our chat history, condense the content as much as possible while preserving all essential information related to the diagnosis.Eliminate redundant or irrelevant information, and ensure the summary maintains coherence and clarity.</p>
<p>Your output format should be:</p>
<p>Summary: (your summary of the dialogue)</p>
<p>For answer parsing, we construct a disease synonyms mapping based the UMLS matching and manual verification process, and use the mapping to match the model's output with the ground truth.We also record models' requested examination items to calculate the full-path clinical-diagnosis accuracy.</p>
<p>B. Details of Evaluation Metrics</p>
<p>As introduced in Section 3.3, we use accuracy as the evaluation metric for Low-Level and Mid-Level tasks.For High-Level tasks, we use the full-path clinical-diagnosis accuracy as the evaluation metric.To make the evaluation metrics across different levels comparable, we normalize the accuracy of Low-Level and Mid-Level tasks using Equation ( 4).Specifically, for the Low-Level tasks (MCQs), we set the random accuracy as 1 Nc , where N c is the number of options in the MCQs.For the Statement Validation tasks and Answer Existence Judgment tasks, we set the random accuracy as 0.5, as there are only two possible labels.For the Multi-step Rectification tasks, we find that the random accuracy of questions with the correct answer provided is much higher than that of questions with the wrong answer provided, because for the latter case, the model need to further choose the correct answer from the rest of N c − 1 options.Therefore, we consider weight the accuracy of the questions with the correct answer provided by α and the accuracy of the questions with the wrong answer provided by 1 − α to make the random accuracy independent of the random strategy.Considering a random strategy that deciding the given option is correct with a probability of p, the random accuracy of the Multi-step Rectification tasks is calculated as:
Accuracy rand = α × p + (1 − α) × (1 − p) × 1 N c − 1
Since we want the random accuracy be invariant to the random strategy, the derivative of the random accuracy with respect to p should be zero, which leads to α = 1 Nc .Therefore, we set α = 1 Nc for the Multi-step Rectification tasks, and the random accuracy is 1 Nc as well.For the full-path clinical-diagnosis accuracy, since the task format is open-ended, we set the random accuracy as zero and directly report the accuracy of the evaluated models.</p>
<p>C. Details of Evaluated Models</p>
<p>We list the basic information of LLMs evaluated in our study in Table 7.The models are divided into general and medicaldomain specific models.We denote Phi3-mini-Instruct-128k, Phi3-small-Instruct-128k, Phi3-medium-Instruct-128k, and Phi4 as Phi3-3.8B,Phi3-7B, Phi3-14B, and Phi4-14B, for simplicity.</p>
<p>D. Details of Evaluation Settings</p>
<p>For the Low-Level and Mid-Level tasks, we set the model temperature as 0 to ensure the model outputs the most confident answer.We conduct five repeated experiments by randomly selecting five demonstrative samples from the rest of dataset.We then calculate the average accuracy of the five experiments, and report the mean and standard error of the accuracy across all test samples.For the High-Level tasks, considering that low temperature may lead to the model being too conservative, we set the model temperature as 0.8 to encourage the model to explore more possibilities.We also verified through preliminary experiments that LLMs produce stable outputs with this parameter setting.We conduct repeated experiments three times and report the mean and standard error of the accuracy.</p>
<p>Figure 1 .
1
Figure 1.(a): The cognitive development process of human doctors; (b): Comparison of the existing evaluations and our proposed multi-cognitive-level evaluation framework (inspired by Bloom's Taxonomy) regarding cognitive levels.</p>
<p>Figure 3 .
3
Figure 3.An overview of the full-path clinical diagnosis task applied in the proposed evaluation framework.</p>
<p>Figure 4 .
4
Figure 4.The recognize-and-retrieve procedure for constructing the High-Level dataset, ensuring alignment with previous levels in terms of knowledge coverage.</p>
<p>Figure 6 .
6
Figure 6.Performance gains of medical LLMs over their backbone models across different cognitive levels.</p>
<p>AHPFigure 7 .
7
Figure 7. Average diagnosis accuracy of LLMs (Llama2 and 3, Qwen1, 2, and 2.5) across diseases in the full-path clinical diagnosis task.</p>
<p>Figure 8 .
8
Figure 8. Examples of Low-Level tasks.Left: a MedQA question.Right: a MedMCQA question.</p>
<p>Figure 9 .
9
Figure 9. Input prompt format of the five-shot in-context learning setting.</p>
<p>Figure 10 .
10
Figure 10.Examples of Mid-Level tasks.Left: Statement Validation tasks; Middle: Multi-step Rectification tasks; Right: Answer Existence Judgment tasks.</p>
<p>You are a medical expert and you are given a multiple choice question.Please change the question into a statement verification based on a given option.Child of Vasanthi was weaned from breast milk on the 5th day and was given sugarcane juice the child developed hypoglycemia and hepatomegaly biochemical examination showed hypophosphatemia and enzyme deficienciesreducing substances in urine.The child is probably suffering from which of the following enzyme deficiencies -Options: A: Fructokinase B: Aldolase B C: Glucose 6 Phosphatase Dof Vasanthi was weaned from breast milk on the 5th day and was given sugarcane juice the child developed hypoglycemia and hepatomegaly biochemical examination showed hypophosphatemia and enzyme deficiencies-reducing substances in urine.The child is probably suffering from the deficiency of enzyme Aldolase B.</p>
<p>Figure 11 .
11
Figure 11.Number of admission records for each disease in the constructed full-path clinical-diagnosis evaluation dataset.</p>
<ol>
<li>LAB: Run laboratory tests and receive their values.You will get all the lab tests results at once. 3. MICRO: Run microbiology tests and receive their values.You will get all the microbiology tests results at once. 4. IMAGE: Do specific imaging scans and receive the radiologist report.You will get all the imaging results at once. 5. OUTPUT: Output the final diagnosis.Note: To improve diagnostic efficiency, please perform physical examinations (PE), laboratory tests (LAB), microbiological tests (MICRO), and imaging scans (IMAGE) only when necessary for diagnosis.When you are confident, choose the "OUTPUT" action and you will be asked to output the corresponding diagnosis.Your output format should be: Rationale: (your reasoning process for choosing the next action) Action: (one of the actions in [PE, LAB, IMAGE, MICRO, OUTPUT]) Now a patient comes to see the doctor.Patient History: [History of Present Illness].Please choose your next action from [PE, LAB, IMAGE, MICRO, OUTPUT].</li>
</ol>
<p>You choose [Model's Action] as the next action.Please provide the specific list of [Model's Action] you want to run: please output your choice in the following format: Rationale: (your reasoning process for choosing the next action) Lab Tests: (the specific laboratory tests you want to run, separated by commas)</p>
<p>Rationale: (your reasoning process for choosing the next action) Action: (one of the actions in [Lefted Actions]) Please choose your next action from [Lefted Actions].</p>
<p>. Anti-HBcAg IgM Reducing Additional Clues
Question： In a patient with Hepatitis B infection, which one of the following markers only elevated during acute infection? Options: A. HBsAg B. HBcAg C. Anti-HBsAg IgG DStatement: "Anti-HBsAg IgG only elevated during acute Hepatitis B infection." True False Increasing Expanding Decision Space Reasoning Steps40 yo F w/ hypothyroidism and multiple ED admissions for safe bed requests due to c/o skin and eye yellowing … Patient history suggest potential hepatobiliary-related diseases. Require laboratory test of …</p>
<p>Simple Knowledge Recalling Tasks Complex Knowledge Application Tasks Full-path Clinical Diagnosis Tasks Q1: Alice choose C, is her answer correct? A1: Incorrect. Q2: If incorrect, what is the correct answer? A2: D. Anti HbcAg IgM.
Options: A: HbsAg B: HbcAg C: Anti-HbsAg IgG D:</p>
<p>Anti-HBcAg IgG Is the correct answer provided in the options above? Yes No Real-world Scenarios
Hepatitis A Virus IgM Antibody: NegHepatitis B Core Antibody IgM: Pos*…Based on … The final diagnosisshould be:</p>
<p>Acute hepatitis B. Preliminary Knowledge Grasp Comprehensive Knowledge Application Scenario-based Problem Solving</p>
<p>An overview of the proposed multi-cognitive-level medical evaluation framework.</p>
<p>… Figure 2.</p>
<p>Table 1 .
1
The basic statistics of the constructed multi-cognitivelevel medical evaluation benchmark.
LevelsTasksSources# SamplesLowMCQsMedQA MedMCQA795 2,816MiddleReformulated TasksMedQA MedMCQA4,770 16,896HighScenario-based DiagnosisMIMIC-IV2,176
diagnosis task.Table1summarizes the key statistics of the constructed multi-cognitive-level evaluation benchmark, while additional details, including task examples, prompts, and answer parsing, are provided in Appendix A.Performance Metrics We adopt the accuracy for the Low-Level and Mid-Level tasks.For the High-Level task, we propose full-path diagnosis accuracy, a new metric that assesses LLMs' diagnostic performance by considering both procedural correctness and outcome accuracy:</p>
<p>Table 2 .
2
Performance (mean and standard error of the normalized accuracy (%)) of LLMs across different cognitive levels evaluated on the proposed benchmark.Llama1's performance on the High-Level task is unavailable due to the absence of instruction-tuned versions.Asterisks: due to the high cost of GPT-4o and DeepSeek-V3 API, we evaluated it on ∼10% of the original dataset.
ModelMedQALow-Level MedMCQAMedQAMid-Level MedMCQAHigh-Level MIMIC-IVLlama-7B3.36 ± 0.8210.40 ± 0.242.47 ± 0.764.38 ± 0.62-Llama-13B13.18 ± 0.5718.98 ± 0.222.62 ± 0.653.15 ± 0.71-Llama-33B24.09 ± 0.2626.88 ± 0.3412.85 ± 0.6210.44 ± 0.19-Llama-65B26.64 ± 0.4929.59 ± 0.2916.82 ± 0.7313.48 ± 0.44-Llama2-7B15.88 ± 0.2818.32 ± 0.224.21 ± 1.015.40 ± 0.552.65 ± 0.20Llama2-13B21.10 ± 0.3020.94 ± 0.336.18 ± 1.257.44 ± 0.294.92 ± 0.15Llama2-70B37.99 ± 0.3435.60 ± 0.2620.92 ± 0.8016.12 ± 0.535.16 ± 0.18Llama3-8B37.30 ± 0.5339.11 ± 0.2911.47 ± 0.4311.40 ± 0.2810.50 ± 0.16Llama3-70B63.68 ± 0.3460.82 ± 0.2841.79 ± 0.8536.25 ± 0.4617.81 ± 0.41Qwen-7B19.40 ± 0.3124.85 ± 0.217.34 ± 0.397.08 ± 0.322.51 ± 0.31Qwen-14B38.05 ± 0.3839.25 ± 0.1317.98 ± 0.6315.98 ± 0.324.14 ± 0.29Qwen-72B50.60 ± 0.3950.95 ± 0.1828.39 ± 0.7026.28 ± 0.315.89 ± 0.35Qwen2-7B36.73 ± 0.4439.85 ± 0.1521.22 ± 0.5120.93 ± 0.186.99 ± 0.25Qwen2-72B65.91 ± 0.2460.41 ± 0.1947.32 ± 0.4337.19 ± 0.1815.10 ± 0.09Qwen2.5-7B42.86 ± 0.4945.39 ± 0.1727.01 ± 0.2124.75 ± 0.179.50 ± 0.04Qwen2.5-14B52.23 ± 0.3051.74 ± 0.2435.62 ± 0.3730.73 ± 0.3112.07 ± 0.21Qwen2.5-32B59.21 ± 0.2157.17 ± 0.0938.45 ± 0.5633.43 ± 0.2411.87 ± 0.22Qwen2.5-72B67.83 ± 0.2061.82 ± 0.1249.47 ± 0.6940.57 ± 0.3916.05 ± 0.24Gemma-2B6.16 ± 0.6315.04 ± 0.240.52 ± 0.312.82 ± 0.310.59 ± 0.09Gemma-7B30.35 ± 0.4227.41 ± 0.3815.09 ± 0.4813.64 ± 0.370.33 ± 0.09Gemma2-2B14.87 ± 0.3424.18 ± 0.311.78 ± 0.713.85 ± 0.555.02 ± 0.33Gemma2-9B44.21 ± 0.1846.20 ± 0.2024.52 ± 0.2823.05 ± 0.249.04 ± 0.28Gemma2-27B53.65 ± 0.3150.45 ± 0.1833.40 ± 0.8830.33 ± 0.2611.19 ± 0.64Phi3-3.8B37.52 ± 0.2840.96 ± 0.2023.10 ± 1.1522.53 ± 0.372.21 ± 0.28Phi3-7B49.25 ± 0.3046.95 ± 0.3028.87 ± 0.3121.75 ± 0.287.74 ± 0.16Phi3-14B54.06 ± 0.4949.70 ± 0.2731.67 ± 0.5528.46 ± 0.243.89 ± 0.01Phi4-14B56.19 ± 0.4052.62 ± 0.3039.30 ± 0.4233.46 ± 0.2312.18 ± 0.37GPT-4o-mini62.67 ± 0.2055.33 ± 0.0642.98 ± 0.5932.28 ± 0.6613.81 ± 0.84GPT-4o<em>78.33 ± 0.7564.00 ± 1.0256.96 ± 1.8441.65 ± 1.5919.33 ± 0.30DeepSeek-V3</em>78.33 ± 0.7565.78 ± 0.2244.79 ± 0.3040.07 ± 1.4919.42 ± 0.174. Experiments4.1. Experimental SetupEvaluated Models To conduct a systematic evaluation oncurrent LLMs, we select a total of 32 general LLMs acrosssix LLM families, including Llama</p>
<p>Performance of LLMs across different parameters sizes on the proposed benchmark.The performance of Low and Mid-Level tasks is the macro average across MedQA and MedMCQA.
Log Normalized Accuracy10 1 10 0Low-LevelMid-Level Qwen1 Qwen2 7B/GPT-4o-mini Llama2 Llama3High-Level Qwen2.5 GPT 70B/GPT-4oFigure 5.
4.2.Cognitive-Level AnalysisHigher the Cognitive Level, Worse the Model Performance We first compare the performance of LLMs across different cognitive levels on the proposed benchmark and present the results in Table2.For the Mid-Level, we use the</p>
<p>Table 3 .
3
Performance (%) of LLMs fine-tuned with/without inference-time scaling.The performance of Low and Mid-Level tasks is the macro average across MedQA and MedMCQA.
ModelLow-Level Mid-Level High-LevelDeepSeek-V372.142.419.4DeepSeek-R181.965.526.5GPT-4o-mini59.037.713.8o3-mini81.364.515.1</p>
<p>Table 4 .
4
Detailed performance (%) of LLMs on tasks in the Mid-
Level. StmtVal: Statement Validation; AnsExist: Answer Exis-tence Judgment; MultiRect: Multi-Step Rectification.ModelLowMid-Level StmtVal AnsExist MultiRectLlama3-70B62.347.29.960.0Qwen2.5-72B 64.850.223.861.0Gemma2-27B 52.138.510.446.7Phi4-14B54.440.117.551.6GPT-4o71.258.822.466.3DeepSeek-V3 72.153.76.367.4</p>
<p>Table 5 .
5
Detailed performance (%) of LLMs on the full-path clinical diagnosis task.Exam Recall: the macro average of examination recall across examination types.End-point: the accuracy of the final diagnosis without considering the examination recall.
ModelExam RecallDiagnosis Accuracy End-point Full-pathLlama3-70B38.838.118.1Qwen2.5-72B34.439.615.8Gemma2-27B27.533.410.6Phi4-14B27.139.212.2GPT-4o31.849.219.3DeepSeek-V330.053.619.4real-world medical scenarios.Low Procedural Correctness Causes Bad Performancein Scenario-based Tasks We further analyze the perfor-mance of large language models (LLMs) on the full-pathclinical diagnosis task. As shown in Table 5, LLMs exhibitmoderate performance in terms of end-point accuracy (i.e.,diagnosis accuracy without considering examination recall),with DeepSeek-V3 achieving the highest accuracy at 53.6%.However, when taking examination recall into consideration(full-path performance), the accuracy of all models declinessignificantly, with the best-performing model, DeepSeek-V3, achieving only 19.4% diagnosis accuracy. This declinecan primarily be attributed to low examination recall (lessthan 40%) across all models, suggesting that while currentLLMs can occasionally reach the correct diagnosis, they failto fully consider all relevant possibilities and request thenecessary information throughout the diagnostic process.
LLM Performance Across Diseases Presents Long-Tail Distribution Finally, we analyze the diagnosis performance of LLMs across diseases in the full-path clinical</p>
<p>Table 6 .
6
Clinician validation results on the proposed benchmark.Clinician Acc.: the percentage of correct answers provided by the clinicians.Clinician Subj.Diff.: the subjective difficulty by clinicians on a scale of 1-10 (1: very easy, 10: very difficult).
LevelClinician Acc. (%)Clinician Subj. Diff. Mean MedianLow68.85.05.5Mid54.26.05.8High23.57.57.5</p>
<p>Here are the [Model's Action] results of this patient: [Results of the requested tests] Please choose your next action from [Lefted Actions].</p>
<p>Table 7 .
7
The basic information of large language models evaluated in our study, including the model type, family, backbone model, parameter size, and training data size.
Model TypeFamilyModelBackbone Model Parameter Size (B) Training Data Size (T)Llama-7B-71.4Llama-13B-131.4Llama-33B-331.4Llama-65B-651.4LlamaLlama2-7B-72Llama2-13B-132Llama2-70B-702Llama3-8B-815Llama3-70B-7015Qwen-7B-73Qwen-14B-143Qwen-72B-723Qwen2-7B-77QwenQwen2-72B-727Qwen2.5-7B-718General DomainQwen2.5-14B Qwen2.5-32B--14 3218 18Qwen2.5-72B-7218Gemma-2B-23Gemma-7B-76GemmaGemma2-2B-22Gemma2-9B-98Gemma2-27B-2713Phi3-3.8B-3.83.3PhiPhi3-7B Phi3-14B--7 144.8 3.8Phi4-14B-149.8GPTGPT-4o-mini GPT-4o--Not Available Not AvailableNot Available Not Availableo3-mini-Not AvailableNot AvailableDeepSeekDeepSeek-V3 DeepSeek-R1--671 (37 activated) 671 (37 activated)14.8 14.8ClinicalCamel-70BLlama2-70B702Med42-70BLlama2-70B702Meditron-70BLlama2-70B702Medical DomainOthersMed42-v2-8B Med42-v2-70BLlama3-8B Llama3-70B8 7015 15Meditron3-8BLlama3-8B815Meditron3-70BLlama3-70B7015MMed-Llama3-8BLlama3-8B815
In the following sections, we refer to these levels as Low-Level, Mid-Level, and High-Level for simplicity.
DeepSeek-V3 is not involved in this analysis since it does not have a smaller version for comparison.
AcknowledgementsThis work was supported by the Noncommunicable Chronic Diseases-National Science and Technology Major Project (Grant No. 2023ZD0506501) and Beijing Natural Science Foundation (NO.4252046).We would like to thank the clinicians who participated in the validation and the anonymous reviewers for their valuable feedback.E. Details of Evaluation ResultsWe list in Table8the detailed results of the evaluated LLMs on tasks across all cognitive levels.The results are reported in the format of mean and standard error of the accuracy.We notice that some medical models  achieve a near-to-zero performance on the High-Level tasks.After carefully checking the outputs of these models, we find that these models fail to follow the instruction and tend to repeat the given instruction or output irrelevant information.Table8.Full performance (mean and standard error of normalized performance (%)) of Large Language Models on tasks across different cognitive levels.Considering the high cost of GPT-4o, o3-mini, DeepSeek-V3, and DeepSeek-R1, we only evaluate these with three repeated experiments.F. Details of Clinician ValidationTo validate that the cognitive difficulties of tasks in our benchmark are consistent with the cognitive levels defined in Section 3.2, we conducted a clinician validation.Specifically, we first randomly sampled 20 questions from each task to form a subset of 100 questions.For Low-Level and Mid-Level tasks, we sample questions from the MedQA benchmark.Then, we recruited four licensed clinicians with 3 to 8 years of experience to evaluate this subset from two perspectives: (1) their accuracy in answering the questions and (2) their subjective difficulty ratings to the questions (1=Easy, 10=Hard).The detailed labeling instructions are as follows:Please answer the following questions based on your medical knowledge.For each question, please provide your answer and rate the difficulty of the on a scale from 1 to 10, where 1 means "very easy" and 10 means "very hard".• Low-Level Tasks: These tasks require basic medical knowledge and understanding of medical concepts.Please choose the most appropriate answer from the given options.• Mid-Level Tasks: These tasks require a more complex application of medical knowledge.For statement validation tasks, please determine whether the statement is true or false.For answer existence judgment tasks, please determine whether the answer exists in the given options.For multi-step rectification tasks, please first determine whether the provided answer is correct, and if not, provide the correct answer.• High-Level Tasks: Given the history of present illness section from an inpatient admission note, the task is to provide the patient's primary diagnosis.At each step, the following operation types are allowed:(1) Physical Examination: Request a physical examination for the patient;(2) Laboratory Tests: Request laboratory tests for the patient;(3) Microbiological Culture: Request microbiological cultures (e.g., blood, urine);
Phi-4 technical report. M Abdin, J Aneja, H Behl, S Bubeck, R Eldan, S Gunasekar, M Harrison, R J Hewett, M Javaheripi, P Kauffmann, abs/2412.089052024aArXiv preprint</p>
<p>Phi-3 technical report: A highly capable language model locally on your phone. M Abdin, S A Jacobs, A A Awan, J Aneja, A Awadallah, H Awadalla, N Bach, A Bahree, A Bakhtiari, H Behl, abs/2404.142192024bArXiv preprint</p>
<p>. J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, abs/2303.08774Gpt-4 technical report. ArXiv preprint. 2023</p>
<p>A taxonomy for learning, teaching, and assessing: A revision of Bloom's taxonomy of educational objectives: complete edition. L W Anderson, D R Krathwohl, 2001Addison Wesley Longman, Inc</p>
<p>Gemini: a family of highly capable multimodal models. R Anil, S Borgeaud, Y Wu, J.-B Alayrac, J Yu, R Soricut, J Schalkwyk, A M Dai, A Hauth, abs/2312.118052023ArXiv preprint</p>
<p>. J Bai, S Bai, Y Chu, Z Cui, K Dang, X Deng, Y Fan, W Ge, Y Han, F Huang, abs/2309.16609Qwen technical report. ArXiv preprint. 2023</p>
<p>Overview of the medical question answering task at trec 2017 liveqa. A Ben Abacha, E Agichtein, Y Pinter, D Demner-Fushman, 2017. 2017</p>
<p>Taxonomy of educational objectives, handbook i: the cognitive domain. B S Bloom, M D Englehart, E J Furst, W H Hill, D R Krathwohl, 1956David mckay co</p>
<p>The unified medical language system (umls): integrating biomedical terminology. O Bodenreider, Nucleic acids research. 3212004</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M Balcan, H Lin, NeurIPS2020. 2020. December 6-12, 2020, virtual, 2020</p>
<p>Medbench: A large-scale chinese benchmark for evaluating medical large language models. Y Cai, L Wang, Y Wang, G De Melo, Y Zhang, Y Wang, L He, 10.1609/aaai.v38i16.29723Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence. M J Wooldridge, J G Dy, S Natarajan, Vancouver, CanadaAAAI PressFebruary 20-27, 202420142024</p>
<p>Meditron-70b: Scaling medical pretraining for large language models. Z Chen, A H Cano, A Romanou, A Bonnet, K Matoba, F Salvi, M Pagliardini, S Fan, A Köpf, A Mohtashami, abs/2311.160792023ArXiv preprint</p>
<p>Med42-evaluating fine-tuning strategies for medical llms: Full-parameter vs. parameterefficient approaches. C Christophe, P Kanithi, P Munjal, T Raha, N Hayat, R Rajan, A Al Mahrooqi, A Gupta, M U Salman, M A Pimentel, AAAI 2024 Spring Symposium on Clinical Foundation Models. 2024a</p>
<p>Med42-v2: A suite of clinical llms. C Christophe, P K Kanithi, T Raha, S Khan, M A Pimentel, 2024b</p>
<p>The llama 3 herd of models. A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Yang, A Fan, abs/2407.217832024ArXiv preprint</p>
<p>Evaluation and mitigation of the limitations of large language models in clinical decisionmaking. P Hager, F Jungmann, R Holland, K Bhagat, I Hubrecht, M Knauer, J Vielhauer, M Makowski, R Braren, G Kaissis, Nature medicine. 3092024</p>
<p>B Hui, J Yang, Z Cui, J Yang, D Liu, L Zhang, T Liu, J Zhang, B Yu, K Dang, abs/2409.121865-coder technical report. 2024ArXiv preprint</p>
<p>What disease does this patient have? a large-scale open domain question answering dataset from medical exams. D Jin, E Pan, N Oufattole, W.-H Weng, H Fang, P Szolovits, Applied Sciences. 111464212021</p>
<p>Pub-MedQA: A dataset for biomedical research question answering. Q Jin, B Dhingra, Z Liu, W Cohen, X Lu, 10.18653/v1/D19-1259Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). K Inui, J Jiang, V Ng, Wan , X , the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Mimic-iv, a freely accessible electronic health record dataset. A E Johnson, L Bulgarelli, L Shen, A Gayles, A Shammout, S Horng, T J Pollard, S Hao, B Moody, B Gow, Scientific data. 10112023</p>
<p>Multi-domain clinical natural language processing with medcat: the medical concept annotation toolkit. Z Kraljevic, T Searle, A Shek, L Roguski, K Noor, D Bean, A Mascio, L Zhu, A A Folarin, A Roberts, Artificial intelligence in medicine. 1171020832021</p>
<p>A Liu, B Feng, B Xue, B Wang, B Wu, C Lu, C Zhao, C Deng, C Zhang, C Ruan, arXiv:2412.19437Deepseek-v3 technical report. 2024arXiv preprint</p>
<p>T Mesnard, C Hardin, R Dadashi, S Bhupatiraju, S Pathak, L Sifre, M Rivière, M S Kale, J Love, abs/2403.08295Open models based on gemini research and technology. 2024ArXiv preprint</p>
<p>Capabilities of gpt-4 on medical challenge problems. H Nori, N King, S M Mckinney, D Carignan, E Horvitz, abs/2303.133752023aArXiv preprint</p>
<p>Can generalist foundation models outcompete specialpurpose tuning? case study in medicine. H Nori, Y T Lee, S Zhang, D Carignan, R Edgar, N Fusi, N King, J Larson, Y Li, W Liu, abs/2311.164522023bArXiv preprint</p>
<p>Gpt-4o mini: advancing cost-efficient intelligence. 2024OpenAI</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P F Christiano, J Leike, R Lowe, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, NeurIPS; New Orleans, LA, USA2022. 2022. November 28 -December 9, 2022, 2022</p>
<p>Climedbench: A large-scale chinese benchmark for evaluating medical large language models in clinical scenarios. Z Ouyang, Y Qiu, L Wang, G De Melo, Y Zhang, Y Wang, L He, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024</p>
<p>Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering. A Pal, L K Umapathi, M Sankarasubbu, G Flores, G H Chen, T Pollard, J Ho, Proceedings of the Conference on Health, Inference, and Learning. C Naumann, T , the Conference on Health, Inference, and LearningPMLR2022174</p>
<p>Towards building multilingual language model for medicine. P Qiu, C Wu, X Zhang, W Lin, H Wang, Y Zhang, Y Wang, W Xie, Nature Communications. 15183842024</p>
<p>Large language models encode clinical knowledge. K Singhal, S Azizi, T Tu, S S Mahdavi, J Wei, H W Chung, N Scales, A Tanwani, H Cole-Lewis, S Pfohl, Nature. 62079722023a</p>
<p>Towards expert-level medical question answering with large language models. K Singhal, T Tu, J Gottweis, R Sayres, E Wulczyn, L Hou, K Clark, S Pfohl, H Cole-Lewis, D Neal, abs/2305.096172023bArXiv preprint</p>
<p>Gemma 2: Improving open language models at a practical size. G Team, M Riviere, S Pathak, P G Sessa, C Hardin, S Bhupatiraju, L Hussenot, T Mesnard, B Shahriari, A Ramé, abs/2408.00118ArXiv preprint. 2024</p>
<p>Clinical camel: An open expert-level medical language model with dialogue-based knowledge encoding. A Toma, P R Lawler, J Ba, R G Krishnan, B B Rubin, B Wang, abs/2305.120312023ArXiv preprint</p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, abs/2302.139712023aArXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, abs/2307.092882023bArXiv preprint</p>
<p>CMB: A comprehensive medical benchmark in Chinese. X Wang, G Chen, S Dingjie, Z Zhiyi, Z Chen, Q Xiao, J Chen, F Jiang, J Li, X Wan, B Wang, H Li, Proceedings of the 2024 Conference of the North American Chapter. the Association for Computational Linguistics: Human Language Technologies. K Duh, H Gomez, S Bethard, the 2024 Conference of the North American ChapterMexico City, Mexico20241Association for Computational Linguistics</p>
<p>Llm sensitivity evaluation framework clinical diagnosis. C Yan, X Fu, Y Xiong, T Wang, S C Hui, J Wu, X Liu, Proceedings of the 31st International Conference on Computational Linguistics. the 31st International Conference on Computational Linguistics2025</p>
<p>. A Yang, B Yang, B Hui, B Zheng, B Yu, C Zhou, C Li, C Li, D Liu, F Huang, abs/2407.10671Qwen2 technical report. ArXiv preprint. 2024</p>
<p>. Gemma-2b , </p>
<p>. Meditron, 70B 42.52 ± 0.59 36.18 ± 0.31 28.15 ± 0.65 20.89 ± 0.47</p>
<p>. Mmed-Llama3, 8B 37.96 ± 0.43 39.20 ± 0.21 10.34 ± 0.39 21.78 ± 0.55</p>            </div>
        </div>

    </div>
</body>
</html>
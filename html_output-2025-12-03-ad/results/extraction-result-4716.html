<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4716 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4716</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4716</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-102.html">extraction-schema-102</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <p><strong>Paper ID:</strong> paper-e92d6b93371c5dc6f3f6fb917f925d6c2fae5492</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e92d6b93371c5dc6f3f6fb917f925d6c2fae5492" target="_blank">OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work introduces OpenWebMath, an open dataset inspired by these works containing 14.7B tokens of mathematical webpages from Common Crawl, and describes in detail the method for extracting text and LaTeX content and removing boilerplate from HTML documents, as well as the methods for quality filtering and deduplication.</p>
                <p><strong>Paper Abstract:</strong> There is growing evidence that pretraining on high quality, carefully thought-out tokens such as code or mathematics plays an important role in improving the reasoning abilities of large language models. For example, Minerva, a PaLM model finetuned on billions of tokens of mathematical documents from arXiv and the web, reported dramatically improved performance on problems that require quantitative reasoning. However, because all known open source web datasets employ preprocessing that does not faithfully preserve mathematical notation, the benefits of large scale training on quantitive web documents are unavailable to the research community. We introduce OpenWebMath, an open dataset inspired by these works containing 14.7B tokens of mathematical webpages from Common Crawl. We describe in detail our method for extracting text and LaTeX content and removing boilerplate from HTML documents, as well as our methods for quality filtering and deduplication. Additionally, we run small-scale experiments by training 1.4B parameter language models on OpenWebMath, showing that models trained on 14.7B tokens of our dataset surpass the performance of models trained on over 20x the amount of general language data. We hope that our dataset, openly released on the Hugging Face Hub, will help spur advances in the reasoning abilities of large language models.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4716.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4716.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OWM-1.4B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenWebMath-trained 1.4B Pythia-style model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 1.4B-parameter autoregressive transformer (Pythia architecture) trained from scratch for one epoch on 14.7B tokens of the OpenWebMath dataset to evaluate the impact of high-quality web mathematical text on quantitative reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenWebMath-trained Pythia-style 1.4B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer matching the Pythia 1.4B architecture (24 layers, model dim 2048, 16 heads). Trained from random initialization for one epoch (14.7B tokens) on OpenWebMath using GPT-NeoX; batch size ~1M tokens; learning rate and other hyperparameters same as Pythia 1.4B (see Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Math word problems and quantitative reasoning benchmarks: GSM8k (multi-step arithmetic word problems), MATH (various algebra categories: Prealgebra, Algebra, Intermediate Algebra, Counting & Probability, Number Theory, Precalculus, Geometry), LILA-multiarith (multidigit arithmetic), MATH Algebra-Easy (accuracy eval).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>The paper hypothesizes (empirically) that pretraining/finetuning on high-quality mathematical tokens (preserving LaTeX and math notation) improves models' quantitative reasoning performance — i.e., dataset composition / exposure to formatted math is a primary driver of improved arithmetic performance rather than simply scale of general-domain tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical comparisons: 1.4B models trained for equal token budgets (14.7B) on OpenWebMath (and mixtures including it) show substantially better perplexities on GSM8k and MATH subsections and higher accuracies on MATH-Algebra-Easy and LILA-multiarith than models trained on general web data (The Pile) or ProofPile alone, despite larger baseline models being trained on >20x tokens. This performance improvement is presented as evidence that math-specific high-quality data improves arithmetic/quantitative performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No direct counter-evidence or mechanistic probe presented; the authors note alternative explanations (e.g., dataset curation choices, filtering biases) and that they did not run extensive ablation studies, so improvements may partly reflect selection effects rather than a single identifiable internal algorithmic mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Perplexity (selected from Table 1) on 1.4B models trained for 14.7B tokens: GSM8k=1.9075; MATH-Prealgebra=1.6285; MATH-Algebra=1.6503; MATH-IntermediateAlgebra=1.5949; Counting&Probability=1.6002; NumberTheory=1.6894; Precalculus=1.4542; Geometry=1.5748. Accuracy (Table 2): MATH Algebra-Easy=5.62%; MATH Algebra-Easy maj@16=9.55%; LILA-multiarith=16.67%. For comparison, same-size model trained on The Pile (14.7B) had GSM8k=2.2032 and MATH-Algebra-Easy accuracy 2.81%; the mixture (ProofPile+OpenWebMath) often performs best (e.g., GSM8k=1.8968). Pythia 1.4B trained on 300B The Pile: GSM8k=1.9430; MATH-Algebra-Easy accuracy=3.93%; LILA-multiarith=21.80% (higher on LILA for that baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>No neural probing, attribution, or targeted interventions reported in this paper. The experimental setup is dataset-training-performance oriented (training models on different corpora and measuring downstream perplexities/accuracies) rather than mechanistic probing or interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Reported limits affecting arithmetic performance include: (1) failures in extracting/preserving LaTeX/math (especially MathJax/delimited inline math) can cause documents to be filtered out or produce poor perplexity during training, biasing the dataset; (2) dataset is English-only and excludes images/figures which can be important for some math problems; (3) only one epoch of training was run and no ablations were performed, so it is unclear how much of the gain is due to dataset versus other choices; (4) prefiltering may be aggressive (processing <1% of Common Crawl), possibly missing math content; (5) no fine-grained analysis of internal representations or stepwise arithmetic reasoning failure modes is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Direct empirical comparisons are provided between models trained for equal token budgets on different corpora: OpenWebMath vs ProofPile vs The Pile vs a 50/50 mixture (OpenWebMath+ProofPile), and against the published Pythia 1.4B baseline trained on 300B tokens of The Pile. OpenWebMath-trained models outperform same-size models trained on general web data on most MATH/GSM8k perplexities and many accuracy metrics; mixtures with ProofPile often perform best. The authors also reference Minerva (PaLM finetuned on Math Web Pages) as a prior example where math-focused data improved quantitative reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>This entry contains only the observations and experimental outcomes reported in the paper; no mechanistic claims (e.g., neuron circuits or algorithmic execution) are empirically supported by this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4716.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4716.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Minerva</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Minerva (PaLM finetuned on Math Web Pages)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A PaLM-family model finetuned on a large curated mathematical web dataset (Math Web Pages), reported by prior work to achieve strong state-of-the-art quantitative reasoning results on benchmarks like MATH.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Solving quantitative reasoning problems with language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Minerva (PaLM finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large PaLM-family language model finetuned on a proprietary curated set of mathematical documents (Math Web Pages, ~17.5B tokens) and scientific sources; claimed to substantially improve performance on quantitative reasoning benchmarks per Lewkowycz et al. (2022).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Quantitative reasoning benchmarks including MATH and other multi-step math tasks (as reported by Lewkowycz et al., 2022).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>The mention in this paper attributes Minerva's success to finetuning on billions of tokens of high-quality mathematical documents (i.e., dataset composition/preserving math notation exposure) rather than describing low-level mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Cited prior result: Minerva (PaLM finetuned on math web pages) achieved dramatically improved performance on quantitative reasoning benchmarks, supporting the hypothesis that large-scale math-focused data improves arithmetic/quantitative capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Not discussed in this paper. The authors note that Minerva and its datasets were not publicly released, limiting independent analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>This paper does not report new numerical performance metrics for Minerva; it references Lewkowycz et al. (2022) for Minerva's reported improvements on MATH and related benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>None reported here; Minerva is referenced as prior work and not analyzed mechanistically in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Paper notes that Minerva's dataset and training artifacts are proprietary/unreleased, preventing community verification, mechanistic probing, or comparisons beyond published benchmark numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Minerva is used as motivating prior work showing that math-focused pretraining can improve quantitative reasoning; OpenWebMath is presented as an open alternative dataset to enable replication and wider research.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4716.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4716.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pythia-1.4B baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pythia 1.4B (baseline trained on The Pile 300B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A public reference 1.4B-parameter transformer model trained on a large general-language dataset (The Pile, 300B tokens) used as a baseline for comparison in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pythia: A suite for analyzing large language models across training and scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pythia 1.4B (published baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 1.4B-parameter autoregressive transformer (Pythia architecture) trained on ~300B tokens of The Pile (general-domain data); used here as a baseline whose published evaluation numbers are compared to the models the authors trained on OpenWebMath and other corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Evaluated on GSM8k, MATH subsets, LILA-multiarith and MATH-Algebra-Easy (as reported in paper tables for comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>No mechanistic claims in this paper about how Pythia performs arithmetic; it serves as a scale/data baseline showing that much larger general-domain pretraining does not necessarily beat domain-specific math pretraining at equal token budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical baseline numbers: Pythia 1.4B (trained on 300B The Pile) has GSM8k perplexity 1.9430 and MATH-Algebra-Easy accuracy reported as 3.93%, and notably a higher LILA-multiarith accuracy (21.80%) in the table compared to some math-trained variants.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No direct probing; compared empirically, the OpenWebMath-trained 1.4B model often outperforms Pythia on many MATH/GSM8k perplexities despite Pythia's much larger training token count, suggesting that dataset specificity matters more than raw pretraining token scale for some arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>From paper tables (for comparison): GSM8k perplexity=1.9430; MATH-Prealgebra=1.7117; MATH-Algebra=1.7560; Intermediate Algebra=1.6358; Counting&Probability=1.6359; NumberTheory=1.7460; Precalculus=1.5191; Geometry=1.7252. Accuracy: MATH-Algebra-Easy=3.93%; MATH-Algebra-Easy maj@16=5.62%; LILA-multiarith=21.80%.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>None in this paper beyond using published evaluation numbers for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>As a general-domain baseline, may underperform on math-specialized tasks; no analysis of internal arithmetic representations is provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Used as the principal external baseline (trained on 300B The Pile) to contrast with same-architecture models trained on OpenWebMath (14.7B tokens) and other specialized corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4716.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4716.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dataset-composition hypothesis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hypothesis that math-formatted high-quality pretraining data improves arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurring claim in the paper that exposure to high-quality mathematical tokens (preserving LaTeX/math notation) during pretraining/finetuning substantially improves a language model's quantitative reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a single model but an empirical hypothesis about training data: models that see many high-quality math examples and preserved notation (e.g., LaTeX) will learn better quantitative/arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>All quantitative reasoning tasks evaluated in the paper (GSM8k, MATH categories, LILA-multiarith, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Paper-level hypothesis: the primary mechanism is statistical learning from domain-relevant tokens/notations (i.e., pattern learning/memorization/generalization on math-specific token sequences), not that models execute symbolic arithmetic algorithms — the paper does not posit a detailed neural algorithmic mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Training/evaluation experiments show that models trained on OpenWebMath (preserving LaTeX) achieve lower perplexities and higher accuracies on math benchmarks compared to equal-budget models trained on general datasets, supporting the hypothesis that dataset composition matters.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Authors acknowledge lack of ablations and mechanistic probes; improvements could be due to selection biases, filtering, or other confounds; no direct evidence distinguishes memorization from learned algorithmic procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>See metrics reported for OpenWebMath-trained vs baseline models (perplexities and accuracies in Tables 1 and 2).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>None — the paper does not perform probing/intervention to test whether gains are due to memorization, emergent algorithmic behavior, or other internal mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Authors explicitly note missing mechanistic analysis and the possibility that filtering/extraction biases contribute to observed improvements; they call for further open research (enabled by releasing OpenWebMath) to study memorization, generalization, and internal mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Hypothesis used to explain why Minerva and the OpenWebMath-trained models outperform general-domain pretrained models on quantitative benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Solving quantitative reasoning problems with language models <em>(Rating: 2)</em></li>
                <li>Pythia: A suite for analyzing large language models across training and scaling. <em>(Rating: 2)</em></li>
                <li>Measuring mathematical problem solving with the MATH dataset <em>(Rating: 1)</em></li>
                <li>Lila: A unified benchmark for mathematical reasoning <em>(Rating: 2)</em></li>
                <li>GPT-4 technical report <em>(Rating: 1)</em></li>
                <li>Let's verify step by step <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4716",
    "paper_id": "paper-e92d6b93371c5dc6f3f6fb917f925d6c2fae5492",
    "extraction_schema_id": "extraction-schema-102",
    "extracted_data": [
        {
            "name_short": "OWM-1.4B",
            "name_full": "OpenWebMath-trained 1.4B Pythia-style model",
            "brief_description": "A 1.4B-parameter autoregressive transformer (Pythia architecture) trained from scratch for one epoch on 14.7B tokens of the OpenWebMath dataset to evaluate the impact of high-quality web mathematical text on quantitative reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenWebMath-trained Pythia-style 1.4B",
            "model_description": "Autoregressive transformer matching the Pythia 1.4B architecture (24 layers, model dim 2048, 16 heads). Trained from random initialization for one epoch (14.7B tokens) on OpenWebMath using GPT-NeoX; batch size ~1M tokens; learning rate and other hyperparameters same as Pythia 1.4B (see Table 7).",
            "arithmetic_task_type": "Math word problems and quantitative reasoning benchmarks: GSM8k (multi-step arithmetic word problems), MATH (various algebra categories: Prealgebra, Algebra, Intermediate Algebra, Counting & Probability, Number Theory, Precalculus, Geometry), LILA-multiarith (multidigit arithmetic), MATH Algebra-Easy (accuracy eval).",
            "mechanism_hypothesis": "The paper hypothesizes (empirically) that pretraining/finetuning on high-quality mathematical tokens (preserving LaTeX and math notation) improves models' quantitative reasoning performance — i.e., dataset composition / exposure to formatted math is a primary driver of improved arithmetic performance rather than simply scale of general-domain tokens.",
            "evidence_for_mechanism": "Empirical comparisons: 1.4B models trained for equal token budgets (14.7B) on OpenWebMath (and mixtures including it) show substantially better perplexities on GSM8k and MATH subsections and higher accuracies on MATH-Algebra-Easy and LILA-multiarith than models trained on general web data (The Pile) or ProofPile alone, despite larger baseline models being trained on &gt;20x tokens. This performance improvement is presented as evidence that math-specific high-quality data improves arithmetic/quantitative performance.",
            "evidence_against_mechanism": "No direct counter-evidence or mechanistic probe presented; the authors note alternative explanations (e.g., dataset curation choices, filtering biases) and that they did not run extensive ablation studies, so improvements may partly reflect selection effects rather than a single identifiable internal algorithmic mechanism.",
            "performance_metrics": "Perplexity (selected from Table 1) on 1.4B models trained for 14.7B tokens: GSM8k=1.9075; MATH-Prealgebra=1.6285; MATH-Algebra=1.6503; MATH-IntermediateAlgebra=1.5949; Counting&Probability=1.6002; NumberTheory=1.6894; Precalculus=1.4542; Geometry=1.5748. Accuracy (Table 2): MATH Algebra-Easy=5.62%; MATH Algebra-Easy maj@16=9.55%; LILA-multiarith=16.67%. For comparison, same-size model trained on The Pile (14.7B) had GSM8k=2.2032 and MATH-Algebra-Easy accuracy 2.81%; the mixture (ProofPile+OpenWebMath) often performs best (e.g., GSM8k=1.8968). Pythia 1.4B trained on 300B The Pile: GSM8k=1.9430; MATH-Algebra-Easy accuracy=3.93%; LILA-multiarith=21.80% (higher on LILA for that baseline).",
            "probing_or_intervention_results": "No neural probing, attribution, or targeted interventions reported in this paper. The experimental setup is dataset-training-performance oriented (training models on different corpora and measuring downstream perplexities/accuracies) rather than mechanistic probing or interventions.",
            "limitations_and_failure_modes": "Reported limits affecting arithmetic performance include: (1) failures in extracting/preserving LaTeX/math (especially MathJax/delimited inline math) can cause documents to be filtered out or produce poor perplexity during training, biasing the dataset; (2) dataset is English-only and excludes images/figures which can be important for some math problems; (3) only one epoch of training was run and no ablations were performed, so it is unclear how much of the gain is due to dataset versus other choices; (4) prefiltering may be aggressive (processing &lt;1% of Common Crawl), possibly missing math content; (5) no fine-grained analysis of internal representations or stepwise arithmetic reasoning failure modes is provided.",
            "comparison_to_other_models": "Direct empirical comparisons are provided between models trained for equal token budgets on different corpora: OpenWebMath vs ProofPile vs The Pile vs a 50/50 mixture (OpenWebMath+ProofPile), and against the published Pythia 1.4B baseline trained on 300B tokens of The Pile. OpenWebMath-trained models outperform same-size models trained on general web data on most MATH/GSM8k perplexities and many accuracy metrics; mixtures with ProofPile often perform best. The authors also reference Minerva (PaLM finetuned on Math Web Pages) as a prior example where math-focused data improved quantitative reasoning.",
            "notes": "This entry contains only the observations and experimental outcomes reported in the paper; no mechanistic claims (e.g., neuron circuits or algorithmic execution) are empirically supported by this work.",
            "uuid": "e4716.0",
            "source_info": {
                "paper_title": "OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Minerva",
            "name_full": "Minerva (PaLM finetuned on Math Web Pages)",
            "brief_description": "A PaLM-family model finetuned on a large curated mathematical web dataset (Math Web Pages), reported by prior work to achieve strong state-of-the-art quantitative reasoning results on benchmarks like MATH.",
            "citation_title": "Solving quantitative reasoning problems with language models",
            "mention_or_use": "mention",
            "model_name": "Minerva (PaLM finetuned)",
            "model_description": "Large PaLM-family language model finetuned on a proprietary curated set of mathematical documents (Math Web Pages, ~17.5B tokens) and scientific sources; claimed to substantially improve performance on quantitative reasoning benchmarks per Lewkowycz et al. (2022).",
            "arithmetic_task_type": "Quantitative reasoning benchmarks including MATH and other multi-step math tasks (as reported by Lewkowycz et al., 2022).",
            "mechanism_hypothesis": "The mention in this paper attributes Minerva's success to finetuning on billions of tokens of high-quality mathematical documents (i.e., dataset composition/preserving math notation exposure) rather than describing low-level mechanisms.",
            "evidence_for_mechanism": "Cited prior result: Minerva (PaLM finetuned on math web pages) achieved dramatically improved performance on quantitative reasoning benchmarks, supporting the hypothesis that large-scale math-focused data improves arithmetic/quantitative capabilities.",
            "evidence_against_mechanism": "Not discussed in this paper. The authors note that Minerva and its datasets were not publicly released, limiting independent analysis.",
            "performance_metrics": "This paper does not report new numerical performance metrics for Minerva; it references Lewkowycz et al. (2022) for Minerva's reported improvements on MATH and related benchmarks.",
            "probing_or_intervention_results": "None reported here; Minerva is referenced as prior work and not analyzed mechanistically in this paper.",
            "limitations_and_failure_modes": "Paper notes that Minerva's dataset and training artifacts are proprietary/unreleased, preventing community verification, mechanistic probing, or comparisons beyond published benchmark numbers.",
            "comparison_to_other_models": "Minerva is used as motivating prior work showing that math-focused pretraining can improve quantitative reasoning; OpenWebMath is presented as an open alternative dataset to enable replication and wider research.",
            "uuid": "e4716.1",
            "source_info": {
                "paper_title": "OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Pythia-1.4B baseline",
            "name_full": "Pythia 1.4B (baseline trained on The Pile 300B)",
            "brief_description": "A public reference 1.4B-parameter transformer model trained on a large general-language dataset (The Pile, 300B tokens) used as a baseline for comparison in the paper's experiments.",
            "citation_title": "Pythia: A suite for analyzing large language models across training and scaling.",
            "mention_or_use": "use",
            "model_name": "Pythia 1.4B (published baseline)",
            "model_description": "A 1.4B-parameter autoregressive transformer (Pythia architecture) trained on ~300B tokens of The Pile (general-domain data); used here as a baseline whose published evaluation numbers are compared to the models the authors trained on OpenWebMath and other corpora.",
            "arithmetic_task_type": "Evaluated on GSM8k, MATH subsets, LILA-multiarith and MATH-Algebra-Easy (as reported in paper tables for comparison).",
            "mechanism_hypothesis": "No mechanistic claims in this paper about how Pythia performs arithmetic; it serves as a scale/data baseline showing that much larger general-domain pretraining does not necessarily beat domain-specific math pretraining at equal token budgets.",
            "evidence_for_mechanism": "Empirical baseline numbers: Pythia 1.4B (trained on 300B The Pile) has GSM8k perplexity 1.9430 and MATH-Algebra-Easy accuracy reported as 3.93%, and notably a higher LILA-multiarith accuracy (21.80%) in the table compared to some math-trained variants.",
            "evidence_against_mechanism": "No direct probing; compared empirically, the OpenWebMath-trained 1.4B model often outperforms Pythia on many MATH/GSM8k perplexities despite Pythia's much larger training token count, suggesting that dataset specificity matters more than raw pretraining token scale for some arithmetic tasks.",
            "performance_metrics": "From paper tables (for comparison): GSM8k perplexity=1.9430; MATH-Prealgebra=1.7117; MATH-Algebra=1.7560; Intermediate Algebra=1.6358; Counting&Probability=1.6359; NumberTheory=1.7460; Precalculus=1.5191; Geometry=1.7252. Accuracy: MATH-Algebra-Easy=3.93%; MATH-Algebra-Easy maj@16=5.62%; LILA-multiarith=21.80%.",
            "probing_or_intervention_results": "None in this paper beyond using published evaluation numbers for comparison.",
            "limitations_and_failure_modes": "As a general-domain baseline, may underperform on math-specialized tasks; no analysis of internal arithmetic representations is provided here.",
            "comparison_to_other_models": "Used as the principal external baseline (trained on 300B The Pile) to contrast with same-architecture models trained on OpenWebMath (14.7B tokens) and other specialized corpora.",
            "uuid": "e4716.2",
            "source_info": {
                "paper_title": "OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Dataset-composition hypothesis",
            "name_full": "Hypothesis that math-formatted high-quality pretraining data improves arithmetic",
            "brief_description": "A recurring claim in the paper that exposure to high-quality mathematical tokens (preserving LaTeX/math notation) during pretraining/finetuning substantially improves a language model's quantitative reasoning performance.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": "Not a single model but an empirical hypothesis about training data: models that see many high-quality math examples and preserved notation (e.g., LaTeX) will learn better quantitative/arithmetic reasoning.",
            "arithmetic_task_type": "All quantitative reasoning tasks evaluated in the paper (GSM8k, MATH categories, LILA-multiarith, etc.).",
            "mechanism_hypothesis": "Paper-level hypothesis: the primary mechanism is statistical learning from domain-relevant tokens/notations (i.e., pattern learning/memorization/generalization on math-specific token sequences), not that models execute symbolic arithmetic algorithms — the paper does not posit a detailed neural algorithmic mechanism.",
            "evidence_for_mechanism": "Training/evaluation experiments show that models trained on OpenWebMath (preserving LaTeX) achieve lower perplexities and higher accuracies on math benchmarks compared to equal-budget models trained on general datasets, supporting the hypothesis that dataset composition matters.",
            "evidence_against_mechanism": "Authors acknowledge lack of ablations and mechanistic probes; improvements could be due to selection biases, filtering, or other confounds; no direct evidence distinguishes memorization from learned algorithmic procedures.",
            "performance_metrics": "See metrics reported for OpenWebMath-trained vs baseline models (perplexities and accuracies in Tables 1 and 2).",
            "probing_or_intervention_results": "None — the paper does not perform probing/intervention to test whether gains are due to memorization, emergent algorithmic behavior, or other internal mechanisms.",
            "limitations_and_failure_modes": "Authors explicitly note missing mechanistic analysis and the possibility that filtering/extraction biases contribute to observed improvements; they call for further open research (enabled by releasing OpenWebMath) to study memorization, generalization, and internal mechanisms.",
            "comparison_to_other_models": "Hypothesis used to explain why Minerva and the OpenWebMath-trained models outperform general-domain pretrained models on quantitative benchmarks.",
            "uuid": "e4716.3",
            "source_info": {
                "paper_title": "OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Solving quantitative reasoning problems with language models",
            "rating": 2
        },
        {
            "paper_title": "Pythia: A suite for analyzing large language models across training and scaling.",
            "rating": 2
        },
        {
            "paper_title": "Measuring mathematical problem solving with the MATH dataset",
            "rating": 1
        },
        {
            "paper_title": "Lila: A unified benchmark for mathematical reasoning",
            "rating": 2
        },
        {
            "paper_title": "GPT-4 technical report",
            "rating": 1
        },
        {
            "paper_title": "Let's verify step by step",
            "rating": 1
        }
    ],
    "cost": 0.01591325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text</h1>
<p>${ }^{\text {A }}$ Keiran Paster; ${ }^{\dagger}$ Marco Dos Santos; ${ }^{\circ}$ Zhangir Azerbayev, ${ }^{\text {A }}$ Jimmy Ba<br>${ }^{\text {A }}$ University of Toronto; Vector Institute for Artificial Intelligence<br>${ }^{\dagger}$ University of Cambridge, ${ }^{\circ}$ Princeton University<br>keirp@cs.toronto.edu, mjad3@cam.ac.uk</p>
<h4>Abstract</h4>
<p>There is growing evidence that pretraining on high quality, carefully thought-out tokens such as code or mathematics plays an important role in improving the reasoning abilities of large language models. For example, Minerva, a PaLM model finetuned on billions of tokens of mathematical documents from arXiv and the web, reported dramatically improved performance on problems that require quantitative reasoning. However, because all known publicly released web datasets employ preprocessing that does not faithfully preserve mathematical notation, the benefits of large scale training on quantitive web documents are unavailable to the research community. We introduce OpenWebMath, an open dataset inspired by these works containing 14.7B tokens of mathematical webpages from Common Crawl. We describe in detail our method for extracting text and $\mathrm{L}^{\mathrm{A}} \mathrm{E} \mathrm{X}$ content and removing boilerplate from HTML documents, as well as our methods for quality filtering and deduplication. Additionally, we run small-scale experiments by training 1.4B parameter language models on OpenWebMath, showing that models trained on 14.7B tokens of our dataset surpass the performance of models trained on over 20x the amount of general language data. We hope that our dataset, openly released on the Hugging Face Hub, will help spur advances in the reasoning abilities of large language models.</p>
<h2>1 INTRODUCTION</h2>
<p>Advances in large language models have opened up new opportunities in numerous fields, providing a transformative shift in our approach to a wide range of complex problems (Brown et al., 2020; Raffel et al., 2020). Among these problems, mathematical reasoning has drawn the attention of several researchers in recent years, becoming both a common benchmark to judge the performance of large language models and inspiring new approaches to improve their reasoning capabilities in the hope that they will one day be able to solve complex mathematical problems. One of the biggest advancements in mathematical reasoning in recent years has been the Minerva model (Lewkowycz et al., 2022), which achieved state-of-the-art results on quantitative reasoning benchmarks such as MATH (Hendrycks et al., 2021). Minerva was trained by finetuning PaLM (Chowdhery et al., 2022) on a curated dataset consisting of billions of tokens of high quality technical content sourced from both scientific papers and the web.</p>
<p>Minerva and the datasets used for its training were not released publicly and the current capabilities of open-source models (e.g., Touvron et al. (2023b;c;a); Geng \&amp; Liu (2023); Biderman et al. (2023)) in quantitative reasoning lags behind. We believe that there are important research directions that can only be enabled through open-access to such models and datasets, such as work on memorization and generalization, reinforcement learning, the development of new reasoning benchmarks, and advancement in the reasoning capabilities of language models.</p>
<p>In our work, we produce an open alternative to the Math Web Pages dataset used to train Minerva (Lewkowycz et al., 2022). We extract documents from Common Crawl ${ }^{1}$, applying our pipeline to</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The pipeline for constructing OpenWebMath involves aggressive filtering so that the final dataset only contains high quality, English, and mathematical content.
extract text while preserving mathematical content in the form of $\mathrm{ET}_{\mathrm{E}} \mathrm{X}$ equations. We then filter the documents, ensuring that only high-quality English mathematical documents are kept. Finally, we deduplicate the dataset, resulting in 14.7B tokens of high-quality mathematical content suitable for both pretraining and finetuning large language models. The key contributions of this work are as follows:</p>
<ul>
<li>We publically release OpenWebMath, a dataset of 14.7B tokens of high-quality mathematical web text. Our dataset can be found at https://huggingface.co/datasets/open-webmath/open-web-math on the Hugging Face Hub.</li>
<li>We extensively document our pipeline, sharing our findings with the NLP community. We open-source the code needed to reproduce our results.</li>
<li>We analyze the quality of OpenWebMath. First, we analyze the contents of our dataset, providing statistics on the types of webpages, subjects, and top domains. Then, we train several language models on our dataset to show that per-token, it is more effective than existing mathematical pretraining datasets, and is most effective when combined with other datasets.</li>
</ul>
<h1>2 RELATED WORK</h1>
<h3>2.1 Mathematics Datasets and benchmarks</h3>
<p>Mathematics datasets Over the past couple of years, several datasets of mathematics have been introduced. AMPS, a dataset of informal mathematics, was introduced alongside the MATH dataset (Hendrycks et al., 2021). AMPS includes more than 100,000 Khan Academy problems with step-by-step solutions in LaTeX and over 5 million problems generated using Mathematica scripts. In total, AMPS contains 23GB of problems and solutions. Another notable example is NaturalProofs (Welleck et al., 2021), which encompasses 32,000 theorem statements and proofs, 14,000 definitions, and 2,000 other types of pages (e.g. axioms, corollaries) derived from ProofWiki, the Stacks project and data from mathematics textbooks. Proof-Pile (Azerbayev et al., 2023) is a dataset of mathematical text that contains more than 14.5 GB of informal mathematics texts obtained from arXiv, Stack Exchange, ProofWiki, Wikipedia, openly licensed books, and the MATH dataset. There are also many proprietary datasets for mathematics. WebMath is a large-scale dataset mentioned by OpenAI researchers (Polu \&amp; Sutskever, 2020) that contains a 35B token mix of content from Github, arXiv, and Math StackExchange, adding up to 35GB of informal mathematics. MathMix is another OpenAI dataset used to finetune GPT-4 (Lightman et al., 2023) that contains 1B high quality mathematical tokens containing both natural and synthetic data. The proprietary web dataset used to train Minerva, called Math Web Pages (Lewkowycz et al., 2022), was compiled by collecting 17.5B tokens from web pages that contain $\mathrm{ET}_{\mathrm{E}} \mathrm{X}$ code.</p>
<p>Mathematics benchmarks Several popular benchmarks have been used by researchers to assess the capabilities of language models on both formal and informal mathematics. The MATH dataset (Hendrycks et al., 2021) is comprised of 12,500 challenging competition problems in informal language. Each problem is also accompanied by a step-by-step informal proof. Answers are delimited by the \boxed environment, allowing for easier answer verification. GSM8k (Cobbe et al., 2021) is another popular multi-step informal mathematics reasoning benchmark. It contains 8,500 grade</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Left: The documents in OpenWebMath are sourced from forum posts, educational content, reference pages, scientific papers, blogs, and more. Most content comes from Q\&amp;A forums where users discuss how to solve problems. Right: The majority of the content in OpenWebMath is related to mathematics, but a large part is related to other technical subjects like Physics, Computer Science, Statistics, and more.
school math problems that are intended to be solvable by a bright middle school student. Lewkowycz et al. (2022) also introduce a benchmark based on OpenCourseWare. OCWCourses includes a set of 272 automatically-verifiable solutions at the undergraduate level, covering chemistry, information theory, differential equations, special relativity, and more. Lewkowycz et al. (2022) also evaluate on a subset of MMLU (Hendrycks et al., 2020) called MMLU-STEM, which focuses on science, technology, engineering, and mathematics.</p>
<h1>2.2 Web Data Processing Pipelines</h1>
<p>The pretraining of large language models requires large, diverse datasets. Data scraped from the web is one of the primary sources for such data. However, sources such as Common Crawl, which contains over 200 billion web pages, are known to have significant amounts of low-quality and duplicate content, requiring extensive filtering and deduplication to be suitable for training. Prior works such as C4 (Raffel et al., 2020), RefinedWeb (Penedo et al., 2023), CCNet (Wenzek et al., 2019), The Pile (Gao et al., 2020), and GPT-3 (Brown et al., 2020) introduce various pipelines for extracting quality data from Common Crawl for the purposes of language model training. These pipelines typically consist of three primary steps: text extraction, filtering, and deduplication.</p>
<p>Text extraction Extracting plain text from HTML files is a critical step in the creation of Common Crawl-based datasets. The easiest way to extract text from Common Crawl documents is to use the WET corresponding to each webpage, which contains pre-extracted plain text of the webpage. CCNet and C4 both use Common Crawl's WET files. However, the text extracted in WET files may contain too much boilerplate or miss out on important content such as LaTeX equations. It is also possible to extract text directly from the raw HTML found in Common Crawl WARC files. The Pile uses an open source library called jusText (Endrédy \&amp; Novák, 2013) to extract text from HTML while RefinedWeb uses a library called Trafilatura (Barbaresi, 2021). These text extraction approaches differ in terms of extraction speed, customization, and their precision and recall for removing boilerplate content.</p>
<p>Filtering The first layer of filtering often involves language identification (Wenzek et al., 2019). Language filtering is used because certain other parts of the pipeline only work for specific languages, and is often done with simple linear classifiers such as from fastText (Joulin et al., 2016). Quality filtering can be done with a combination of perplexity, classifier, and rule-based methods. CCNet uses a 5-gram Kneser-Ney language model implemented in the KenLM library (Heafield, 2011) trained on the target domain. The documents in the dataset are then sorted and filtered by their perplexity under this model. Other datasets such as the one used to train GPT-3 (Brown et al., 2020) use a classifier-based approach. This involves training a classifier on known-high-quality</p>
<p>documents, such as those from Wikipedia, as positive examples and unfiltered documents from Common Crawl as negative examples. The classifier scores are used to filter low-quality documents from the dataset. Finally, rule-based approaches such as those used in C4 (Raffel et al., 2020) and MassiveWeb (Rae et al., 2021) involve removing pages with certain characters, too many or too few characters, too high a proportion of symbols, or those with an abnormal average word length. OpenMathWeb uses a mixture of these three approaches.</p>
<p>Deduplication Given the periodic nature of Common Crawl snapshots and a general redundancy in web-sourced text, deduplication is an important processing step. Document-level neardeduplication (e.g., in (Brown et al., 2020; Penedo et al., 2023)) often employs MinHashLSH, an efficient algorithm for estimating the Jaccard similarity of documents. CCNet (Wenzek et al., 2019) uses paragraph-level deduplication, which can help to remove common boilerplate content found in WET text-extractions.</p>
<h1>3 Building OpenWebMath</h1>
<h3>3.1 ObJeCTIVES</h3>
<p>Our aim with OpenWebMath is to build a dataset of as many mathematical documents sourced from the web as possible while preserving the formatting of mathematical content such as $\mathrm{ET}_{\mathrm{E}} \mathrm{X}$ equations as in Lewkowycz et al. (2022). For the purposes of this work, we define a mathematical document as a document containing either core mathematical contents such as theorems, definitions, proofs, questions and answers, formal mathematics, or interdisciplinary documents featuring mathematical formulas within fields like physics, chemistry, biology, economics, and finance. We source our documents from Common Crawl, which is a large open-access crawl of the web containing petabytes of raw HTML files. Due to the high variance in the quality of documents from Common Crawl, we additionally use several methods for filtering and boilerplate reduction. Throughout the creation of OpenWebMath, we iteratively refined these methods to ensure that we do not remove too many relevant documents, optimizing for high recall whenever possible. Since we expect that OpenWebMath will be used primarily as an additional source of pretraining data for large language models, we prefer having a small percentage of non-mathematical but high quality documents in the dataset rather than removing them and potentially losing relevant mathematical content. Finally, due to the limited number of mathematical data available on the web, we use significantly more manual inspection and tuning of our processing pipeline than other web-based datasets. We document our processing choices and pipeline in the section that follows.</p>
<h3>3.2 High-LEVEL OVERVIEW OF THE PIPELINE</h3>
<p>As shown in Figure 1, the processing pipeline for OpenWebMath falls into five stages. First, we apply a prefilter to all HTML documents in Common Crawl to quickly judge whether they have mathematical content, skipping those that do not before doing the extensive processing needed to extract text and equations and remove boilerplate. Second, we extract the text, including mathematical content, from the HTML documents. Third, we apply language identification filters, perplexity-based quality filtering, and a mathematical content classifier filter. Fourth, we deduplicate the dataset using SimHash (Manku et al., 2007). Finally, we manually inspect the documents gathered in the previous steps and view documents from the most popular domains by document-count and character-count, removing domains that are not high quality. We describe each of these steps in detail in the following sections.</p>
<h3>3.3 Prefiltering</h3>
<p>Since there are over 200B HTML documents in Common Crawl, applying our processing over each document would require a significant amount of compute. To improve the efficiency of the pipeline, we first apply a stack of pre-filters optimized for high recall to reduce the number of documents that need to be processed. Our first filters check for common mathematical strings as in Lewkowycz et al. (2022), such as the presence of tex classes, <math> tags, and the word "mathjax". See Table 8 for a full list of terms. If none of these terms are present, we search for the presence of the top 100 most-popular $\mathrm{ET}_{\mathrm{E}} \mathrm{X}$ symbols in the text. This is done by first filtering for documents</p>
<table>
<thead>
<tr>
<th>Training Dataset</th>
<th>GSM8k</th>
<th>MATH</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Prealgebra</td>
<td>Algebra</td>
<td>Intermediate Algebra</td>
<td>Counting \&amp; Probability</td>
<td>Number Theory</td>
<td>Precalculus</td>
<td>Geometry</td>
</tr>
<tr>
<td>The Pile (14.7B tokens)</td>
<td>2.2032</td>
<td>1.9127</td>
<td>1.9751</td>
<td>1.8420</td>
<td>1.8193</td>
<td>1.9227</td>
<td>1.6847</td>
<td>1.9499</td>
</tr>
<tr>
<td>ProofPile (14.7B tokens)</td>
<td>2.2350</td>
<td>1.7370</td>
<td>1.7214</td>
<td>1.5739</td>
<td>1.6462</td>
<td>1.7291</td>
<td>1.4838</td>
<td>1.7229</td>
</tr>
<tr>
<td>OpenWebMath (14.7B tokens)</td>
<td>1.9075</td>
<td>1.6285</td>
<td>1.6503</td>
<td>1.5949</td>
<td>1.6002</td>
<td>1.6894</td>
<td>1.4542</td>
<td>1.5748</td>
</tr>
<tr>
<td>Mixture (14.7B tokens)</td>
<td>$\mathbf{1 . 8 9 6 8}$</td>
<td>$\mathbf{1 . 6 0 5 5}$</td>
<td>$\mathbf{1 . 6 1 9 0}$</td>
<td>$\mathbf{1 . 5 3 0 1}$</td>
<td>$\mathbf{1 . 5 7 1 9}$</td>
<td>$\mathbf{1 . 6 6 0 7}$</td>
<td>$\mathbf{1 . 4 1 1 9}$</td>
<td>$\mathbf{1 . 5 5 9 9}$</td>
</tr>
<tr>
<td>The Pile (300B tokens; Pythia 1.4B)</td>
<td>1.9430</td>
<td>1.7117</td>
<td>1.7560</td>
<td>1.6358</td>
<td>1.6359</td>
<td>1.7460</td>
<td>1.5191</td>
<td>1.7252</td>
</tr>
</tbody>
</table>
<p>Table 1: We trained 1.4B parameter models for 14.7B tokens on various datasets and measured their perplexity on different mathematics benchmarks. Both OpenWebMath and a 50/50 mixture of ProofPile Azerbayev et al. (2023) and OpenWebMath perform well - outperforming Pythia 1.4B (Biderman et al., 2023) trained on 300B tokens of The Pile (Gao et al., 2020).
containing a backslash command using a simple regular expression and then searching specifically for these $\mathrm{ET}_{\mathrm{E}} \mathrm{X}$ symbols in the plain text from the HTML document. If none of these symbols are found, we run the plain text through our MathScore classifier (see section 3.5.1) and keep documents that exceed a confidence threshold of 0.8 . By tuning these filters and using hierarchical layers of progressively more accurate but more expensive filters, we were able to reduce the compute needed to process the dataset by several times while retaining a high recall of relevant documents.</p>
<h1>3.4 TEXT EXTRACTION</h1>
<p>In contrast with prior works that extract text from Common Crawl such as C4 (Collins et al., 2023), The Pile (Gao et al., 2020), and RefinedWeb (Penedo et al., 2023), we chose to make a mostly custom pipeline for extracting the main content from HTML documents. This is because we found that while other tools get decent performance on average over many documents on the internet, they do not work optimally on many of the most common sources of mathematical content on the web. We instead opted to build on top of Resiliparse (Bevendorff et al., 2018; 2021), a fast and efficient library built in Cython that includes performant tools for parsing HTML pages, processing their DOMs, and extracting the main content. As shown in Table 5 in the appendix, Resiliparse is significantly more efficient than alternative libraries such as jusText. Another notable part of our text extraction pipeline is that we randomize the parameters of the extraction to add diversity to the dataset. This includes randomizing whether we use a plain text or Markdown format for the documents and randomizing the amount of boilerplate terms required to trigger a line being removed.</p>
<p>Our text extraction pipeline consists of four stages: $\mathrm{ET}_{\mathrm{E}} \mathrm{X}$ extraction, text extraction, DOM processing, and line processing.</p>
<p>$\mathbf{L T}<em _mathrm_E="\mathrm{E">{\mathbf{E}} \mathbf{X}$ Extraction Lewkowycz et al. (2022) employ a relatively simple $\mathrm{ET}</em>$ on the internet is written using MathJax, where developers write equations delimited by dollar signs or other delimiters in their HTML pages and then the included javascript code replaces these equations with properly}} \mathrm{X}$ extraction pipeline that extracts equations from <script type="math/latex">, <script type="math/asciimath">, and <math> blocks with <annotation encoding="application/x-tex"> blocks within them and replaces these tags with the extracted equations. When we applied these filters to documents from Common Crawl, we noticed an extremely low number of these tags compared to what was reported. We suspect that this is due to a difference between the HTML files available within Google (Lewkowycz et al., 2022) and those available on Common Crawl. The majority of the $\mathrm{ET}_{\mathrm{E}} \mathrm{X</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Training Dataset</th>
<th style="text-align: center;">MATH Algebra-Easy</th>
<th style="text-align: center;">MATH Algebra-Easy <br> maj@16</th>
<th style="text-align: center;">LILA multiarith</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">The Pile (14.7B tokens)</td>
<td style="text-align: center;">$2.81 \%$</td>
<td style="text-align: center;">$3.93 \%$</td>
<td style="text-align: center;">$9.77 \%$</td>
</tr>
<tr>
<td style="text-align: left;">ProofPile (14.7B tokens)</td>
<td style="text-align: center;">$2.81 \%$</td>
<td style="text-align: center;">$3.93 \%$</td>
<td style="text-align: center;">$8.04 \%$</td>
</tr>
<tr>
<td style="text-align: left;">OpenWebMath (14.7B tokens)</td>
<td style="text-align: center;">$\mathbf{5 . 6 2 \%}$</td>
<td style="text-align: center;">$9.55 \%$</td>
<td style="text-align: center;">$\mathbf{1 6 . 6 7 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">Mixture (14.7B tokens)</td>
<td style="text-align: center;">$5.06 \%$</td>
<td style="text-align: center;">$\mathbf{1 0 . 1 1 \%}$</td>
<td style="text-align: center;">$13.22 \%$</td>
</tr>
<tr>
<td style="text-align: left;">The Pile (300B tokens; Pythia 1.4B)</td>
<td style="text-align: center;">$3.93 \%$</td>
<td style="text-align: center;">$5.62 \%$</td>
<td style="text-align: center;">$\mathbf{2 1 . 8 0 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Accuracy on Different Math Benchmarks.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: $\mathrm{IT}<em _mathrm_E="\mathrm{E">{\mathrm{E}} \mathrm{X}$ formulas can be embedded in HTML documents in many ways, including in images, within arbitrary delimiters, and within special tags. Most common text-extraction pipelines do not extract $\mathrm{IT}</em>$ code properly.
rendered $\mathrm{IT}}} \mathrm{X<em _mathrm_E="\mathrm{E">{\mathrm{E}} \mathrm{X}$ equations within the above script tags when the page is loaded. HTML documents on Common Crawl do not include the changes to the HTML that result from running javascript, requiring that we instead extract the $\mathrm{IT}</em>$ equations by finding delimiters ourselves. This is a significant challenge since we need to detect whether the page contains the required MathJax javascript code, which delimiters were chosen by the user to denote equations, and then match and extract the equations from the text on the page. See Appendix B for a more detailed discussion.}} \mathrm{X</p>
<p>In order to extract MathJax, we first determine whether the page is importing the MathJax javascript code by searching for the word MathJax on the page. If it is not found, we additionally search for common $\mathrm{ET}<em _mathrm_E="\mathrm{E">{\mathrm{E}} \mathrm{X}$ symbols, and if they are found, we treat the page as though it is running MathJax. We use regular expressions to search for code that calls the configuration function for MathJax to extract the delimiters used for equations. We add these delimiters to an extensive list of default delimiters and treat any content between these delimiters as $\mathrm{IT}</em>$ equations.}} \mathrm{X</p>
<p>In addition to extracting equations from MathJax, we found several more ways that $\mathrm{ET}<em _mathrm_E="\mathrm{E">{\mathrm{E}} \mathrm{X}$ is encoded on the internet. These methods were discovered by filtering small portions of Common Crawl for documents that contain $\backslash$ frac, one of the most popular $\mathrm{ET}</em>$ on the internet is encoded in the following ways:}} \mathrm{X}$ commands, and making sure that our processing code supports all the different ways that math could be encoded. We found that $\mathrm{ET}_{\mathrm{E}} \mathrm{X</p>
<ol>
<li>equation and align environments.</li>
<li>The alttext of elements with special classes like tex.</li>
<li>Images from domains like latex. codecogs.com often include equations encoded in the URL.</li>
<li>Special wordpress plugins.</li>
<li><math> tags with <annotation encoding="application/x-tex"> blocks within them.</li>
<li><math> tags with MathML content. We use a style sheet to convert these equations into $\mathrm{ET}_{\mathrm{E}} \mathrm{X}$.</li>
<li>MathJax equations encoded in the text of the page.</li>
</ol>
<p>The relative frequencies of the different ways math is encoded can be found in Table 6 in the appendix.</p>
<p>DOM Processing After extracting the $\mathrm{ET}_{\mathrm{E}} \mathrm{X}$ equations from the HTML, we do several processing steps on the DOM-tree of the HTML document. This includes removing invisible elements based on their styles, removing buttons and link clusters, annotating code, tables, and headers, and removing known problematic elements based on class or ID.</p>
<p>Text Extraction We use the extract_plain_text (main_content=True) method in Resiliparse (Bevendorff et al., 2018) to extract the main content text from the DOM following several preprocessing steps to get around common issues with their specific implementation that cause it to be overly sensitive when removing boilerplate.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The MathScore classifier used in filtering OpenWebMath is trained to predict whether a text has any of the most popular $\mathrm{ET}<em _mathrm_E="\mathrm{E">{\mathrm{E}} \mathrm{X}$ commands based only on surrounding words. This lets us include documents on the web that do not include extractable $\mathrm{ET}</em>$ but still contain technical content.}} \mathrm{X</p>
<p>Line Processing After extracting the plain text on the page using Resiliparse, we apply our own processing to remove boilerplate lines based on an iteratively-refined set of common boilerplate phrases, remove empty headers, and escape dollar signs that are not part of $\mathrm{ET}_{\mathrm{E}} \mathrm{X}$ equations.</p>
<h1>3.5 Filtering</h1>
<p>We apply filtering with the goal of removing non-English documents (since our filters pipeline is optimized for English), removing documents that are not mathematical, and removing low-quality documents that would be harmful to train a language model on. We apply the following filters in order:</p>
<ol>
<li>We use a FastText language identification model (Joulin et al., 2016) to remove documents that are not in English.</li>
<li>We use our MathScore classifier (see section 3.5.1) to get a probability that the document is mathematical. If our previous extraction step found $\mathrm{ET}<em _mathrm_E="\mathrm{E">{\mathrm{E}} \mathrm{X}$ equations, we keep documents with a probability of over 0.17 . If no $\mathrm{ET}</em>$ equations were found, we keep documents with a probability of over 0.8 .}} \mathrm{X</li>
<li>We use a KenLM language model (Heafield, 2011) trained on ProofPile (Azerbayev et al., 2023) to get a perplexity score for each document. We remove documents with a perplexity score of more than 15,000 .</li>
</ol>
<h3>3.5.1 MATH SCORE</h3>
<p>During our filtering process, we train a model to predict the probability a document is mathematical, which we call MathScore. We first gather a dataset of hundreds of thousands documents extracted from our pipeline from an early stage of the project, and label them depending on whether they contain one of the top-100 most common $\mathrm{ET}<em _mathrm_E="\mathrm{E">{\mathrm{E}} \mathrm{X}$ commands. We then remove any $\mathrm{ET}</em>}} \mathrm{X}$ code from the documents and train a classifier to predict whether the documents contain one of these common $\mathrm{ET<em _mathrm_E="\mathrm{E">{\mathrm{E}} \mathrm{X}$ commands. The training process for MathScore is depicted in Figure 4. Since we remove all $\mathrm{ET}</em>$ content. We use FastText (Joulin et al., 2016) to train this model, and find based on manual inspection that content with a score of under 0.2 is very unlikely to contain useful mathematical content.}} \mathrm{X}$ code from the features fed into the model, the model needs to learn the words and phrases most commonly associated with $\mathrm{ET}_{\mathrm{E}} \mathrm{X</p>
<h3>3.6 DEDUPLICATION</h3>
<p>Due to the large amount of duplicate documents in Common Crawl, we apply a deduplication step to remove near-duplicate documents. We use the SimHash implementation from text-dedup (Mou</p>
<p>et al., 2023) to deduplicate the dataset using a threshold of 0.7 . We find that this threshold is high enough to remove most duplicate documents even if they have slight differences in their texts.</p>
<h1>3.7 MANUAL INSPECTION</h1>
<p>Finally, we manually inspect the top domains by document count, the top domains by character count, and the longest documents in the dataset to ensure that the documents are high quality. We remove domains that are not high quality or clearly not mathematical by adding domains to a blacklist and adding domain filters such as removing user profile pages, abstract-hosting websites as in Lewkowycz et al. (2022), and removing search result pages.</p>
<h2>4 DATASET ANALYSIS</h2>
<p>Token count At 14.7B tokens, OpenWebMath is just below the size of Minerva's Math Web Pages (17.5B tokens) Lewkowycz et al. (2022) and significantly larger than the web part of any other dataset. OpenWebMath has around the same number of LLaMA tokens as ProofPile (14.2B) (Azerbayev et al., 2023), but we note that there is very little overlap between between the two datasets. As a result, OpenWebMath brings a large number of new mathematical tokens that were previously unavailable to the open-source community. Due to differences in data curation strategies, it is hard to compare these datasets other than by training models on them. Since not much is known about how to properly filter a dataset, we opted to keep as much relevant content as possible. However, future work could explore filtering OpenWebMath more aggressively to further improve its quality.</p>
<p>Data Composition We measured the distribution of domains in OpenWebMath both by document and by character count. Table 3 and Table 4 show the top twenty most common domains by document and character count respectively. The most common sources of data tend to be discussion forums, blog posts, and scientific papers. We find that the distribution of characters in the dataset is distributed over 131,206 domains, with $46 \%$ of the characters appearing in the top 100 domains.</p>
<p>In order to get a sense of the types of documents found in the dataset, we analyzed 100,000 randomly sampled documents. First, we created embeddings of this data using all-MiniLM-L12-v2 (Wang et al., 2020) in SentenceTransformers (Reimers \&amp; Gurevych, 2019). Then, we clustered these embeddings using $k$-Means with $k=128$. Finally, we took the five closest documents to each cluster center and asked qpt-3.5-turbo (https://platform.openai.com/docs/api-reference) to classify each cluster as Math, Physics, Statistics, Chemistry, Economics, Computer Science, or Other. We then aggregated these statistics, using the size of each cluster to get an estimate of the final number of documents in each category. We note several potential issues with this methodology,</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Domain</th>
<th style="text-align: right;"># Documents</th>
<th style="text-align: right;">\% Documents</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">stackexchange.com</td>
<td style="text-align: right;">$1,136,407$</td>
<td style="text-align: right;">$17.99 \%$</td>
</tr>
<tr>
<td style="text-align: left;">physicsforums.com</td>
<td style="text-align: right;">300,044</td>
<td style="text-align: right;">$4.75 \%$</td>
</tr>
<tr>
<td style="text-align: left;">mathhelpforum.com</td>
<td style="text-align: right;">170,721</td>
<td style="text-align: right;">$2.70 \%$</td>
</tr>
<tr>
<td style="text-align: left;">socratic.org</td>
<td style="text-align: right;">133,983</td>
<td style="text-align: right;">$2.12 \%$</td>
</tr>
<tr>
<td style="text-align: left;">mathoverflow.net</td>
<td style="text-align: right;">120,755</td>
<td style="text-align: right;">$1.91 \%$</td>
</tr>
<tr>
<td style="text-align: left;">gradesaver.com</td>
<td style="text-align: right;">96,100</td>
<td style="text-align: right;">$1.52 \%$</td>
</tr>
<tr>
<td style="text-align: left;">zbmath.org</td>
<td style="text-align: right;">91,939</td>
<td style="text-align: right;">$1.46 \%$</td>
</tr>
<tr>
<td style="text-align: left;">wordpress.com</td>
<td style="text-align: right;">87,876</td>
<td style="text-align: right;">$1.39 \%$</td>
</tr>
<tr>
<td style="text-align: left;">github.io</td>
<td style="text-align: right;">81,125</td>
<td style="text-align: right;">$1.28 \%$</td>
</tr>
<tr>
<td style="text-align: left;">brilliant.org</td>
<td style="text-align: right;">68,573</td>
<td style="text-align: right;">$1.09 \%$</td>
</tr>
<tr>
<td style="text-align: left;">gamedev.net</td>
<td style="text-align: right;">50,560</td>
<td style="text-align: right;">$0.80 \%$</td>
</tr>
<tr>
<td style="text-align: left;">openstudy.com</td>
<td style="text-align: right;">49,041</td>
<td style="text-align: right;">$0.78 \%$</td>
</tr>
<tr>
<td style="text-align: left;">gmatclub.com</td>
<td style="text-align: right;">48,812</td>
<td style="text-align: right;">$0.77 \%$</td>
</tr>
<tr>
<td style="text-align: left;">blogspot.com</td>
<td style="text-align: right;">48,036</td>
<td style="text-align: right;">$0.76 \%$</td>
</tr>
<tr>
<td style="text-align: left;">wikipedia.org</td>
<td style="text-align: right;">46,606</td>
<td style="text-align: right;">$0.74 \%$</td>
</tr>
<tr>
<td style="text-align: left;">ac.uk</td>
<td style="text-align: right;">41,342</td>
<td style="text-align: right;">$0.65 \%$</td>
</tr>
<tr>
<td style="text-align: left;">nature.com</td>
<td style="text-align: right;">37,403</td>
<td style="text-align: right;">$0.59 \%$</td>
</tr>
<tr>
<td style="text-align: left;">aimsciences.org</td>
<td style="text-align: right;">36,368</td>
<td style="text-align: right;">$0.58 \%$</td>
</tr>
<tr>
<td style="text-align: left;">libretexts.org</td>
<td style="text-align: right;">32,216</td>
<td style="text-align: right;">$0.51 \%$</td>
</tr>
<tr>
<td style="text-align: left;">readthedocs.io</td>
<td style="text-align: right;">31,455</td>
<td style="text-align: right;">$0.50 \%$</td>
</tr>
</tbody>
</table>
<p>Table 3: Most Common Domains by Document Count.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Domain</th>
<th style="text-align: right;"># Characters</th>
<th style="text-align: right;">\% Characters</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">stackexchange.com</td>
<td style="text-align: right;">$4,655,132,784$</td>
<td style="text-align: right;">$9.55 \%$</td>
</tr>
<tr>
<td style="text-align: left;">nature.com</td>
<td style="text-align: right;">$1,529,935,838$</td>
<td style="text-align: right;">$3.14 \%$</td>
</tr>
<tr>
<td style="text-align: left;">wordpress.com</td>
<td style="text-align: right;">$1,294,166,938$</td>
<td style="text-align: right;">$2.66 \%$</td>
</tr>
<tr>
<td style="text-align: left;">physicsforums.com</td>
<td style="text-align: right;">$1,160,137,919$</td>
<td style="text-align: right;">$2.38 \%$</td>
</tr>
<tr>
<td style="text-align: left;">github.io</td>
<td style="text-align: right;">$725,689,722$</td>
<td style="text-align: right;">$1.49 \%$</td>
</tr>
<tr>
<td style="text-align: left;">zbmath.org</td>
<td style="text-align: right;">$620,019,503$</td>
<td style="text-align: right;">$1.27 \%$</td>
</tr>
<tr>
<td style="text-align: left;">wikipedia.org</td>
<td style="text-align: right;">$618,024,754$</td>
<td style="text-align: right;">$1.27 \%$</td>
</tr>
<tr>
<td style="text-align: left;">groundai.com</td>
<td style="text-align: right;">$545,214,990$</td>
<td style="text-align: right;">$1.12 \%$</td>
</tr>
<tr>
<td style="text-align: left;">blogspot.com</td>
<td style="text-align: right;">$520,392,333$</td>
<td style="text-align: right;">$1.07 \%$</td>
</tr>
<tr>
<td style="text-align: left;">mathoverflow.net</td>
<td style="text-align: right;">$499,102,560$</td>
<td style="text-align: right;">$1.02 \%$</td>
</tr>
<tr>
<td style="text-align: left;">gmatclub.com</td>
<td style="text-align: right;">$442,611,169$</td>
<td style="text-align: right;">$0.91 \%$</td>
</tr>
<tr>
<td style="text-align: left;">gamedev.net</td>
<td style="text-align: right;">$426,478,461$</td>
<td style="text-align: right;">$0.88 \%$</td>
</tr>
<tr>
<td style="text-align: left;">ac.uk</td>
<td style="text-align: right;">$402,111,665$</td>
<td style="text-align: right;">$0.83 \%$</td>
</tr>
<tr>
<td style="text-align: left;">aimsciences.org</td>
<td style="text-align: right;">$344,716,386$</td>
<td style="text-align: right;">$0.71 \%$</td>
</tr>
<tr>
<td style="text-align: left;">mathhelpforum.com</td>
<td style="text-align: right;">$319,215,756$</td>
<td style="text-align: right;">$0.65 \%$</td>
</tr>
<tr>
<td style="text-align: left;">deepai.org</td>
<td style="text-align: right;">$313,512,520$</td>
<td style="text-align: right;">$0.64 \%$</td>
</tr>
<tr>
<td style="text-align: left;">libretexts.org</td>
<td style="text-align: right;">$282,014,149$</td>
<td style="text-align: right;">$0.58 \%$</td>
</tr>
<tr>
<td style="text-align: left;">readthedocs.io</td>
<td style="text-align: right;">$269,816,413$</td>
<td style="text-align: right;">$0.55 \%$</td>
</tr>
<tr>
<td style="text-align: left;">tib.eu</td>
<td style="text-align: right;">$199,714,017$</td>
<td style="text-align: right;">$0.41 \%$</td>
</tr>
<tr>
<td style="text-align: left;">mit.edu</td>
<td style="text-align: right;">$198,487,362$</td>
<td style="text-align: right;">$0.41 \%$</td>
</tr>
</tbody>
</table>
<p>Table 4: Most Common Domains by Character Count.</p>
<p>including inaccuracies stemming from using an LLM for classification, and the potential that not every document within a cluster belongs to the predicted category. Figure 2 shows the results of this analysis. The majority of the documents in the dataset are directly related to mathematics, while the rest are spread out throughout physics, computer science, statistics, chemistry, and economics, with $12 \%$ of documents not falling neatly into any of these categories.</p>
<p>We also used GPT to analyze the types of websites found in OpenWebMath. To do this, we took a sample of 200 documents and asked gpt-3.5-turbo to classify each as a Forum, Paper, Blog, Reference, Educational, Reference, or other. We also gave the document URL as a feature, since we found GPT is often able to judge the topic from the URL alone. We validated our analysis by asking GPT to do this classification on the top 100 domain names and got similar results. Figure 2 shows the results. The highest proportion of documents are forum pages, where users ask and answer questions related to mathematical subjects. There is also a large proportion of educational and reference content.</p>
<p>Downstream Performance We ran experiments to find out how our dataset compares to other language modeling datasets. We compare models trained on OpenWebMath for a single epoch (14.7B tokens) with models trained for the same number of tokens on The Pile (Gao et al., 2020), a general langauge modeling dataset, and ProofPile (Azerbayev et al., 2023), a dataset of both formal and informal mathematics. We also train a 50/50 mixture of ProofPile and OpenWebMath to evaluate the performance of OpenWebMath when included in a mixture of other datasets, as would be common in practice.</p>
<p>We train randomly initialized models with the same architecture as Pythia 1.4B (Biderman et al., 2023). We use a batch size of 1 M tokens and the same hyperparameters as Pythia otherwise. These models are evaluated on a collection of mathematics benchmarks which show signal on models of this size. This includes the subset of level-1 algebra questions from MATH, LILA-multiarith to test coding ability, and GSM8k and MATH perplexities, which scale more smoothly than accuracies. We also compare to Pythia 1.4B (Biderman et al., 2023), which was trained on 300B tokens of The Pile (Gao et al., 2020) with the same architecture.</p>
<p>Table 1 shows the results for our perplexity evaluations. There is a clear performance lead for models trained with OpenWebMath and the mixture seems to perform best. Despite Pythia being trained on over 20x the number of tokens, the performance of our models on the perplexity benchmarks far exceeds its performance, showing the potential of domain-specific models for mathematics. Similarly, Table 2 shows the performance of the models on MATH-Algebra-Easy and LILA-multiarith (Mishra et al., 2022). OpenWebMath models outperform models that were not trained on it by a significant margin.</p>
<h1>5 CONCLUSION</h1>
<p>In this paper, we describe OpenWebMath, an open dataset of 14.7B high quality mathematical documents from the web. We extensively document our pipeline, including several novel methodologies for extracting $\mathrm{I} \pi \mathrm{I}_{\mathrm{E}} \mathrm{X}$ formulas, reducing boilerplate, and filtering the dataset. OpenWebMath consists of high quality Q\&amp;A forum posts, educational documents, blogs, and more spread across mathematics, physics, computer science, and other technical domains. We also train several models on OpenWebMath and other language modeling datasets to compare the downstream performance achievable by training on our dataset. Notably, we find that models trained on OpenWebMath outperform models trained on 20x more general-domain tokens in mathematics. We hope that OpenWebMath can lead to the creation of language models with improved mathematical reasoning capabilities.</p>
<h1>ACKNOWLEDGEMENTS</h1>
<p>JB is supported by NSERC Grant [2020-06904], CIFAR AI Chairs program, Google Research Scholar Program, and Amazon Research Award. KP is supported by an NSERC PGS-D award. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, Fujitsu Limited, and companies sponsoring the Vector Institute for Artificial Intelligence (www. vectorinstitute.ai/partners). Computing resources for model training were provided by EleutherAI and Brigham Young University. We thank Finn Paster for the graphic design for the logo. We additionally thank Ziming Chen, Yuhuai Wu, Stella Biderman, Aviya Skowron, Hailey Schoelkopf, and Sean Welleck for their helpful comments.</p>
<h1>REFERENCES</h1>
<p>Alex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric Hallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Jason Phang, Shivanshu Purohit, Hailey Schoelkopf, Dashiell Stander, Tri Songz, Curt Tigges, Benjamin Thérien, Phil Wang, and Samuel Weinbach. GPT-NeoX: Large scale autoregressive language modeling in PyTorch. GitHub Repo, 9 2023. URL https://www.github.com/ eleutherai/gpt-neox.</p>
<p>Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W Ayers, Dragomir Radev, and Jeremy Avigad. Proofnet: Autoformalizing and formally proving undergraduate-level mathematics. arXiv preprint arXiv:2302.12433, 2023.</p>
<p>Adrien Barbaresi. Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction. In Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pp. 122-131. Association for Computational Linguistics, 2021. URL https://aclanthology.org/2021.acl-demo.15.</p>
<p>Janek Bevendorff, Benno Stein, Matthias Hagen, and Martin Potthast. Elastic ChatNoir: Search Engine for the ClueWeb and the Common Crawl. In Leif Azzopardi, Allan Hanbury, Gabriella Pasi, and Benjamin Piwowarski (eds.), Advances in Information Retrieval. 40th European Conference on IR Research (ECIR 2018), Lecture Notes in Computer Science, Berlin Heidelberg New York, March 2018. Springer.</p>
<p>Janek Bevendorff, Martin Potthast, and Benno Stein. FastWARC: Optimizing Large-Scale Web Archive Analytics. In Andreas Wagner, Christian Guetl, Michael Granitzer, and Stefan Voigt (eds.), 3rd International Symposium on Open Search Technology (OSSYM 2021). International Open Search Symposium, October 2021.</p>
<p>Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models across training and scaling. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 2397-2430. PMLR, 2023. URL https:// proceedings.mlr.press/v202/biderman23a.html.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,</p>
<p>Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311, 2022. doi: 10.48550/arXiv.2204.02311. URL https://doi.org/10.48550/arXiv.2204.02311.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Katherine M Collins, Albert Q Jiang, Simon Frieder, Lionel Wong, Miri Zilka, Umang Bhatt, Thomas Lukasiewicz, Yuhuai Wu, Joshua B Tenenbaum, William Hart, et al. Evaluating language models for mathematics through interactions. arXiv preprint arXiv:2306.01694, 2023.</p>
<p>István Endrédy and Attila Novák. More effective boilerplate removal-the goldminer algorithm. Polibits, 48:79-83, 12 2013. doi: 10.17562/PB-48-10.</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.</p>
<p>Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III au2, and Kate Crawford. Datasheets for datasets, 2021.</p>
<p>Xinyang Geng and Hao Liu. Openllama: An open reproduction of llama, May 2023. URL https: //github.com/openlm-research/open_llama.</p>
<p>Kenneth Heafield. Kenlm: Faster and smaller language model queries. In Proceedings of the sixth workshop on statistical machine translation, pp. 187-197, 2011.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. CoRR, abs/2103.03874, 2021. URL https://arxiv.org/abs/2103.03874.</p>
<p>Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hérve Jégou, and Tomas Mikolov. Fasttext.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651, 2016.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843-3857, 2022.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. CoRR, abs/2305.20050, 2023. doi: 10.48550/arXiv.2305.20050. URL https://doi.org/10. 48550/arXiv. 2305.20050.</p>
<p>Gurmeet Singh Manku, Arvind Jain, and Anish Das Sarma. Detecting near-duplicates for web crawling. In Proceedings of the 16th International Conference on World Wide Web, WWW '07, pp. 141-150, New York, NY, USA, 2007. Association for Computing Machinery. ISBN 9781595936547. doi: 10.1145/1242572.1242592. URL https://doi.org/10.1145/ 1242572.1242592 .</p>
<p>Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, et al. Lila: A unified benchmark for mathematical reasoning. arXiv preprint arXiv:2210.17517, 2022.</p>
<p>Chenghao Mou, Chris Ha, Kenneth Enevoldsen, and Peiyuan Liu. Chenghaomou/text-dedup: Reference snapshot, September 2023. URL https://doi.org/10.5281/zenodo.8364980.</p>
<p>OpenAI. Gpt-4 technical report, 2023.
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon LLM: outperforming curated corpora with web data, and web data only. CoRR, abs/2306.01116, 2023. doi: 10.48550/arXiv.2306.01116. URL https://doi.org/10. 48550/arXiv.2306.01116.</p>
<p>Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving. arXiv preprint arXiv:2009.03393, 2020.</p>
<p>Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew J. Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis \&amp; insights from training gopher. CoRR, abs/2112.11446, 2021. URL https://arxiv.org/abs/2112.11446.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.</p>
<p>Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv. org/abs/1908.10084.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023a. doi: 10.48550/arXiv.2302.13971. URL https://doi.org/10.48550/arXiv.2302.13971.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023b.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and finetuned chat models. CoRR, abs/2307.09288, 2023c. doi: 10.48550/arXiv.2307.09288. URL https://doi.org/10.48550/arXiv.2307.09288.</p>
<p>Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers, 2020.</p>
<p>Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi, Yejin Choi, and Kyunghyun Cho. Naturalproofs: Mathematical theorem proving in natural language. arXiv preprint arXiv:2104.01112, 2021.</p>
<p>Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data. arXiv preprint arXiv:1911.00359, 2019.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Runtime (s)</th>
<th>Source Code Link</th>
</tr>
</thead>
<tbody>
<tr>
<td>Resiliparse</td>
<td>3.99</td>
<td>https://github.com/chatnoir-eu/chatnoir-resiliparse</td>
</tr>
<tr>
<td>HTML-Text</td>
<td>10.75</td>
<td>https://github.com/TeamHG-Memex/html-text</td>
</tr>
<tr>
<td>Inscripts</td>
<td>19.14</td>
<td>https://github.com/weblyzard/inscriptis</td>
</tr>
<tr>
<td>BoilerPy</td>
<td>24.94</td>
<td>https://github.com/jmriebold/BoilerPy3</td>
</tr>
<tr>
<td>jusText</td>
<td>31.17</td>
<td>https://github.com/miso-belica/jusText</td>
</tr>
<tr>
<td>HTML2Text</td>
<td>37.17</td>
<td>https://github.com/Alir3z4/html2text/</td>
</tr>
<tr>
<td>BeautifulSoup</td>
<td>38.42</td>
<td>https://code.launchpad.net/beautifulsoup</td>
</tr>
<tr>
<td>Trafilatura</td>
<td>63.90</td>
<td>https://github.com/adbar/trafilatura</td>
</tr>
<tr>
<td>ExtractNet</td>
<td>299.67</td>
<td>https://github.com/currentslab/extractnet</td>
</tr>
</tbody>
</table>
<p>Table 5: We measured the performance of various HTML text extraction tools on a dataset of 1 k documents. Resiliparse was by far the most efficient, leading us to choose it for use in our pipeline.</p>
<h1>A Limitations and Future Work</h1>
<p>Despite the high quality of OpenWebMath, we note several limitations and avenues for future works. First, due to the high cost of extracting data from all shards on Common Crawl, we were only able to run our pipeline once. Therefore, many of our choices are without empirical justification and we provide no ablation study. We also note that the nature of this particular type of dataset means that there are many subjective choices to be made. For instance, what counts as a mathematical document? What is a high-quality document? How do we choose the threshold for near-deduplication? For each of these, we chose several values and manually inspected a few examples to choose. Due to the cost constraints, there are also practical challenges with balancing cost with accuracy when filtering and extracting text. For instance, our prefilter reduces the number of HTML documents processed to under $1 \%$ of the documents in Common Crawl, which may be too aggressive. We also note that OpenWebMath is an English-only dataset, which limits its applications for researchers and users who speak other languages. Finally, we note that OpenWebMath only contains the text from math on the web, not associated figures, which can be important for solving mathematical problems (OpenAI, 2023). Future work should focus on finding empirical answers to the questions of what constitutes good data, creating new, efficient filtering methodologies, and extracting images inline with math text.</p>
<h2>B Text Extraction</h2>
<p>Choice of Base Text Extractor When considering which HTML text-extraction library to use, we considered the efficiency, customization, and existing boilerplate reduction methods for each option. The most commonly used option, using WET files extracted by Common Crawl, was not an option since they do not deal with $\mathrm{ET}_{\mathrm{E}} \mathrm{X}$ correctly and offer no customization. Other options such as jusText (Endrédy \&amp; Novák, 2013), used in The Pile Gao et al. (2020), removed boilerplate too aggressively, leading to sections containing math to be discarded. Likewise, Trafilatura (Barbaresi, 2021), which was used in RefinedWeb (Penedo et al., 2023), had poor efficiency. We decided to go with Resiliparse (Bevendorff et al., 2018) due to its balanced boilerplate removal, fast runtime, and efficient Common Crawl parsing tools. Table 5 shows the full results for our comparison.</p>
<p>$\mathbf{L T} \mathbf{E} \mathbf{X}$ Extraction $\mathrm{LAT}<em _mathrm_E="\mathrm{E">{\mathrm{E}} \mathrm{X}$ code comes in many forms throughout Common Crawl HTML files. We employed an iterative process to refine our extraction rules. First, we filtered shards of Common Crawl for documents that contain the string \frac {frac. Then, we filtered those documents to find those which our extraction code found no extractable $\mathrm{ET}</em>}} \mathrm{X}$. Then, we refined our code to include additional sources of math until we were confident that we had reasonable support for all formats of $\mathrm{ET<em _mathrm_E="\mathrm{E">{\mathrm{E}} \mathrm{X}$ in HTML documents. Table 6 shows the breakdown of different common types of $\mathrm{ET}</em>$ found in HTML documents.}} \mathrm{X</p>
<p>We note that most of the $\mathrm{ET}<em _mathrm_E="\mathrm{E">{\mathrm{E}} \mathrm{X}$ in OpenWebMath and across the internet is encoded using MathJax, which presents a challenge. The majority of MathJax documents use dollar sign delimiters, but most dollar signs on the web do not delimit $\mathrm{ET}</em>$ equations. This leaves us with a few options:}} \mathrm{X</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Math Format</th>
<th style="text-align: center;">Percentage of Documents</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Found at least one instance of math</td>
<td style="text-align: center;">$91.42 \%$</td>
</tr>
<tr>
<td style="text-align: left;">MathJax with delimiters (inline)</td>
<td style="text-align: center;">$50.27 \%$</td>
</tr>
<tr>
<td style="text-align: left;">MathJax with delimiters (display)</td>
<td style="text-align: center;">$23.37 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Math found in images</td>
<td style="text-align: center;">$6.96 \%$</td>
</tr>
<tr>
<td style="text-align: left;">.math-container</td>
<td style="text-align: center;">$3.94 \%$</td>
</tr>
<tr>
<td style="text-align: left;">MathML code</td>
<td style="text-align: center;">$3.28 \%$</td>
</tr>
<tr>
<td style="text-align: left;"><annotation> withing <math> tags</td>
<td style="text-align: center;">$2.35 \%$</td>
</tr>
<tr>
<td style="text-align: left;"><mathjax> tags</td>
<td style="text-align: center;">$2.24 \%$</td>
</tr>
<tr>
<td style="text-align: left;">align environments</td>
<td style="text-align: center;">$1.72 \%$</td>
</tr>
<tr>
<td style="text-align: left;">equation environments</td>
<td style="text-align: center;">$1.18 \%$</td>
</tr>
<tr>
<td style="text-align: left;">within <script> tags</td>
<td style="text-align: center;">$1.01 \%$</td>
</tr>
<tr>
<td style="text-align: left;">alttext property of <math> tags</td>
<td style="text-align: center;">$0.24 \%$</td>
</tr>
</tbody>
</table>
<p>Table 6: Frequencies of different types of $\mathrm{IT}<em _mathrm_E="\mathrm{E">{\mathrm{E}} \mathrm{X}$ found in OpenWebMath. The most common format of $\mathrm{IT}</em>$ code within either the URL or alt text of an img tag.}} \mathrm{X}$ found in Common Crawl is MathJax, which uses user-defined delimiters to denote math equations. Second most common is $\mathrm{IT}_{\mathrm{E}} \mathrm{X</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model Size</th>
<th style="text-align: center;">Layers</th>
<th style="text-align: center;">Model Dim</th>
<th style="text-align: center;">Heads</th>
<th style="text-align: center;">Learning Rate</th>
<th style="text-align: center;">Batch Size</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1.4 B</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">$2.0 \times 10^{-4}$</td>
<td style="text-align: center;">1 M</td>
</tr>
</tbody>
</table>
<p>Table 7: Model Hyperparameters. We use the same architecture and hyperparameters, other than batch size, as Pythia 1.4B (Biderman et al., 2023).</p>
<p>Table 8: List of Math Keywords used in the prefiltering stage.</p>
<h1>Math Keywords</h1>
<div class="codehilite"><pre><span></span><code>MathJax
mathjax
&lt;math
math-container
katex.min.css
latex.php
codecogs
tex.cgi
class=&quot;tex&quot;
class=&#39;tex&#39;
</code></pre></div>

<ul>
<li>Detect the use of the MathJax script in the HTML file. If the script is imported, treat dollar signs as $\mathrm{IT}_{\mathrm{E}} \mathrm{X}$ code.</li>
<li>Detect common $\mathrm{IT}<em _mathrm_E="\mathrm{E">{\mathrm{E}} \mathrm{X}$ commands in between dollar signs. If they are present, treat dollar signs as $\mathrm{IT}</em>$ code.}} \mathrm{X</li>
<li>Use the MathScore classifier to determine whether the page looks like it is talking about math. If so, treat dollar signs as $\mathrm{IT}_{\mathrm{E}} \mathrm{X}$ code.</li>
</ul>
<p>The first option is not always accurate since the MathJax javascript code may be nested inside of another import or named differently depending on the website. The latter two options make up for many of these cases, but can fail to detect edge cases where math equations are present but the surrounding text does not indicate that the document is mathematical. We suspect Minerva (Lewkowycz et al., 2022) gets around this issue by using HTML documents where javascript code has already been executed, in which case MathJax is converted from delimited text to explicit HTML tags that are easy to detect.</p>
<h1>C Interplay Between Extraction and Filtering</h1>
<p>In prior works, we noticed many cases where suboptimal HTML text extractors were used and yet text quality remains high in the dataset. This is due to the interplay between extraction and filtering. Specifically, if a text extractor fails to extract the main text, gets the formatting wrong, or includes too much boilerplate in the extraction, then both the classification and perplexity filters can filter out such examples. This can lead to subtle biases in the dataset, where specific poorly-extracted websites are excluded entirely even though they do contain high quality content. In the case of making a mathematical dataset, failure to extract and deal with inline $\mathrm{IAT}_{\mathrm{E}} \mathrm{X}$ code properly can hurt perplexity scores and lead to these documents being filtered out. We suggest practitioners tune their text extraction pipeline on a diverse set of documents before applying filtering to avoid this bias.</p>
<h2>D Model Hyperparameters</h2>
<p>We trained models on 14.7B tokens using the LLaMA (Touvron et al., 2023c) tokenizer and the architecture described in Pythia (Biderman et al., 2023). We train the model using the GPT-NeoX library (Andonian et al., 2023) on 8 A100 80GB GPUs. Exact hyperparameters can be found in Table 7.</p>
<h1>E DATASHEET</h1>
<p>We provide a datasheet for OpenWebMath, following the framework in Gebru et al. (2021).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Motivation</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">For what purpose was the dataset created?</td>
<td style="text-align: center;">The dataset was created to enable the training of large language models on mathematical texts, in order to improve their mathematical reasoning capabilities.</td>
</tr>
<tr>
<td style="text-align: center;">Who created the dataset and on behalf of which entity?</td>
<td style="text-align: center;">The dataset was created by the authors of this work.</td>
</tr>
<tr>
<td style="text-align: center;">Who funded the creation of the dataset?</td>
<td style="text-align: center;">Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, Fujitsu Limited, and companies sponsoring the Vector Institute for Artificial Intelligence (www. vectorinstitute.ai/partners). Computing resources for model training were provided by EleutherAI and Brigham Young University.</td>
</tr>
<tr>
<td style="text-align: center;">Any other comment?</td>
<td style="text-align: center;">None.</td>
</tr>
<tr>
<td style="text-align: center;">COMPOSITION</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">What do the instances that comprise the dataset represent?</td>
<td style="text-align: center;">The instances are text documents extracted from mathematics-related webpages from Common Crawl.</td>
</tr>
<tr>
<td style="text-align: center;">How many instances are there in total?</td>
<td style="text-align: center;">In total, OpenWebMath contains 6.3 million documents.</td>
</tr>
<tr>
<td style="text-align: center;">Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?</td>
<td style="text-align: center;">OpenWebMath doesn't contain all instances of text extracted from mathematics-related webpages from Common Crawl, as our filters can miss a non-zero proportion of such webpages. However, we expect OpenWebMath to contain most of them.</td>
</tr>
<tr>
<td style="text-align: center;">What data does each instance consist of?</td>
<td style="text-align: center;">Each instance consists of plain text and metadata including the source URL, the snapshot date, and other extraction parameters.</td>
</tr>
<tr>
<td style="text-align: center;">Is there a label or target associated with each instance?</td>
<td style="text-align: center;">No.</td>
</tr>
<tr>
<td style="text-align: center;">Is any information missing from individual instances?</td>
<td style="text-align: center;">No.</td>
</tr>
<tr>
<td style="text-align: center;">Are relationships between individual instances made explicit?</td>
<td style="text-align: center;">No.</td>
</tr>
<tr>
<td style="text-align: center;">Are there recommended data splits?</td>
<td style="text-align: center;">No.</td>
</tr>
<tr>
<td style="text-align: center;">Are there any errors, sources of noise, or redundancies in the dataset?</td>
<td style="text-align: center;">Yes, a small portion of the documents from OpenWebMath are not related to mathematics, or contain bad quality content.</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">Is the dataset self-contained, or does it <br> link to or otherwise rely on external re- <br> sources?</th>
<th style="text-align: left;">The dataset is entirely self-contained.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Does the dataset contain data that might <br> be considered confidential?</td>
<td style="text-align: left;">No.</td>
</tr>
<tr>
<td style="text-align: left;">Does the dataset contain data that, if <br> viewed directly, might be offensive, in- <br> sulting, threatening, or might otherwise <br> cause anxiety?</td>
<td style="text-align: left;">The data is filtered for quality and we do <br> not expect that this content will be offen- <br> sive, but since our filters may be imperfect <br> we make no guarantees.</td>
</tr>
</tbody>
</table>
<h1>COLLECTION</h1>
<p>How was the data associated with each instance acquired?</p>
<p>What mechanisms or procedures were used to collect the data?</p>
<p>If the dataset is a sample from a larger set, what was the sampling strategy?</p>
<p>Who was involved in the data collection process and how were they compensated?</p>
<p>Over what timeframe was the data collected?</p>
<p>Were any ethical review processes conducted?</p>
<p>The data was acquired by processing data from Common Crawl.</p>
<p>We refer to the CommonCrawl website (commoncrawl.org) for details on how they collect data.</p>
<p>We use all data from Common Crawl that was available before May 2023.</p>
<p>Keiran Paster and Marco Dos Santos collected the data and were compensated by their respective graduate programs.</p>
<p>OpenWebMath uses shards of CommonCrawl gathered between 2013 and 2023.</p>
<p>No.</p>
<h2>Preprocessing</h2>
<p>Was any preprocessing/cleaning/labeling of the data done?</p>
<p>Was the "raw" data saved in addition to the preprocessed/cleaned/labeled data?</p>
<p>Is the software that was used to preprocess/clean/label the data available?</p>
<h2>Uses</h2>
<p>Has the dataset been used for any tasks already?</p>
<p>Is there a repository that links to any or all papers or systems that use the dataset?</p>
<p>What (other) tasks could the dataset be used for?</p>
<p>Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?</p>
<p>Are there tasks for which the dataset should not be used?</p>
<p>Yes. See section 3.5 for details.</p>
<p>Yes.
Yes. See supplementary materials.</p>
<p>We primarily envision that OpenWebMath could be useful for language model pretraining, finetuning, and evaluation.</p>
<p>It is possible that the filtering stage of the project discarded valuable documents, such as those not written in English. This makes OpenWebMath suboptimal for creating mathematical models in other languages.</p>
<p>Any tasks which may considered irresponsible or harmful.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Will the dataset be distributed to third <br> parties outside of the entity on behalf of <br> which the dataset was created?</th>
<th style="text-align: left;">Yes, the dataset will be available on the <br> Hugging Face Hub for NLP practitioners.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">How will the dataset will be distributed?</td>
<td style="text-align: left;">We will distribute the dataset on the Hug- <br> ging Face Hub</td>
</tr>
<tr>
<td style="text-align: left;">When will the dataset be distributed?</td>
<td style="text-align: left;">The dataset will be available when the pa- <br> per is made public.</td>
</tr>
<tr>
<td style="text-align: left;">Will the dataset be distributed under <br> a copyright or other intellectual prop- <br> erty (IP) license, and/or under applica- <br> ble terms of use (ToU)?</td>
<td style="text-align: left;">The public extract is made available un- <br> der an ODC-By 1.0 license; users should <br> also abide to the CommonCrawl ToU: <br> https://commoncrawl.org/terms-of-use/.</td>
</tr>
<tr>
<td style="text-align: left;">Have any third parties imposed IP- <br> based or other restrictions on the data <br> associated with the instances?</td>
<td style="text-align: left;">Not to our knowledge.</td>
</tr>
<tr>
<td style="text-align: left;">Do any export controls or other regula- <br> tory restrictions apply to the dataset or <br> to individual instances?</td>
<td style="text-align: left;">Not to our knowledge.</td>
</tr>
</tbody>
</table>
<h1>MAINTENANCE</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Who will be supporting/hosting/main- <br> taining the dataset?</th>
<th style="text-align: left;">The dataset will be hosted on the Hugging <br> Face Hub.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">How can the owner/curator/manager of <br> the dataset be contacted?</td>
<td style="text-align: left;">keirp@cs.toronto.edu</td>
</tr>
<tr>
<td style="text-align: left;">Is there an erratum?</td>
<td style="text-align: left;">No.</td>
</tr>
<tr>
<td style="text-align: left;">Will the dataset be updated?</td>
<td style="text-align: left;">No.</td>
</tr>
<tr>
<td style="text-align: left;">If others want to extend/augment/build <br> on/contribute to the dataset, is there a <br> mechanism for them to do so?</td>
<td style="text-align: left;">No.</td>
</tr>
</tbody>
</table>
<p>Table 9: Datasheet for OpenWebMath, following the framework introduced by Gebru et al. (2021).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{\star}$ Keiran and Marco created the dataset and Zhangir led model training and evaluation.
${ }^{\dagger}$ https://commoncrawl.org/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
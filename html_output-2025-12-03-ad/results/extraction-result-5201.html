<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5201 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5201</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5201</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-264590585</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.18679v1.pdf" target="_blank">N-C RITICS : Self-Refinement of Large Language Models with Ensemble of Critics</a></p>
                <p><strong>Paper Abstract:</strong> We propose a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination. This method involves refining model outputs through an ensemble of critics and the model’s own feedback. Drawing inspiration from human behavior, we explore whether LLMs can emulate the self-correction process observed in humans who often engage in self-reflection and seek input from others to refine their understanding of complex topics. Our approach is model-agnostic and can be applied across various domains to enhance trustworthiness by addressing fairness, bias, and robustness concerns. We consistently observe performance improvements in LLMs for reducing toxicity and correcting factual errors.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5201.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5201.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>N-CRITICS (toxicity)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>N-CRITICS: Self-Refinement of Large Language Models with Ensemble of Critics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative, model-agnostic self-refinement framework that uses an ensemble of open-source LLMs as critics to generate feedback which is aggregated and used to prompt the generator to produce improved outputs; applied to reduce toxicity by re-generating outputs guided by critics' feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Koala-13b, Vicuna-13b, Wizard-13b (used as generation/critic models in toxicity experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source instruction-finetuned/chat-style LLMs based on LLaMA-family backbones (13B-scale variants used in toxicity experiments); used both as critics in ensembles and as generation models depending on experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>N-CRITICS (ensemble critics iterative refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate-then-reflect loop where an initial output y0 is produced by a primary LLM, then circulated to an ensemble of other open-source LLMs (including the primary) to produce critiques/feedback; critiques are aggregated and used to modify the prompt to the primary LLM to regenerate a refined answer; process repeats up to a preset iteration cap or until stopping criteria (e.g., toxicity below threshold) are met.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Toxicity reduction (REALTOXICITYPROMPTS)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mitigate toxic continuations from prompts using the REALTOXICITYPROMPTS dataset (1,700 sampled prompts) and measure toxicity via the PERSPECTIVE API.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Authors report substantial toxicity reduction after refinement; example instance reduced to toxicity score 0.035 after refinement. Reported mean toxicity reductions across 1,700 prompts (see Table 1); authors state N-CRITICS outperforms proprietary baselines such as ChatGPT in their comparisons (numerical table entries reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Vanilla generation (no critics) had higher mean toxicity (baseline values reported in Table 1 and visualized in Figure 3). The paper shows '0 critics' curves in Figure 3 as the baseline; explicit aggregated baseline numbers are provided in Table 1 (paper).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: mean toxicity across outputs decreased after iterative refinement on a 1,700-prompt sample; example refined output toxicity = 0.035. Figures (Figure 3) show toxicity declining across iterations and with more critics; Table 1 reports N-CRITICS achieving lower toxicity than competing methods and proprietary ChatGPT as presented by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Authors report diminishing returns beyond ~4 critics (improvement plateaus); iterative ensemble increases computational overhead and latency; method depends on the quality of open-source LLM critics and thus inherits shared biases/flaws; experiments are English-centric so language-generalization unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'N-C RITICS : Self-Refinement of Large Language Models with Ensemble of Critics', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5201.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5201.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>N-CRITICS (factual)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>N-CRITICS: Self-Refinement of Large Language Models with Ensemble of Critics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same ensemble-based generate-then-reflect framework applied to reduce factual hallucinations on QA datasets by using large-model critics to spot errors and guide answer regeneration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-70b (generator); critics: WizardLM-70b and LLaMA-70b (larger critics); small models (Koala-13b, Vicuna-13b) tested but found unhelpful</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-70b used as the main generator for factual-hallucination experiments; critics included WizardLM-70b and another LLaMA-70b instance — large open-source models provided the most useful critiques according to authors.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>N-CRITICS (ensemble critics iterative refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Initial answer produced by generator (LLaMA-70b) is evaluated by an ensemble of critics (other LLMs). Critics provide explicit feedback or corrected evidence; feedback is aggregated and used to re-prompt the generator to produce an improved answer; early stopping used if generated answer unchanged for two consecutive corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Factual hallucination reduction (TriviaQA, AmbigNQ, HotpotQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reduce unfaithful or hallucinated facts in answers on three QA benchmarks (400 random samples per dataset) measured via Exact Match (EM) and F1 metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Authors report improved EM and F1 scores after N-CRITICS refinement; example summary reported in paper (Table 2) shows numerical gains (paper reports N-CRITICS values such as EM/F1 aggregates; e.g., top-line numbers printed for N-CRITICS include 78.02 and 84.67 among others — see Table 2 for dataset-specific EM/F1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Vanilla LLaMA-70b baseline reported (paper shows vanilla numbers in Table 2, e.g., baselines like 73 for a vanilla metric row) and N-CRITICS improves over those baselines; authors also compare against other methods (CRITIC, ChatGPT) as reported in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: Table 2 reports higher F1 for N-CRITICS across all three datasets and higher EM on TriviaQA and HotpotQA relative to baselines; Figure 4 shows performance gains across iterations and with additional critics on TriviaQA (improvements accumulate across iterations/critics until plateau). Authors also note small critics degraded performance for this task (empirical negative case).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Authors found critiques from smaller models (Koala-13b, Vicuna-13b) did not help or reduced output quality for factual tasks, indicating critic quality matters; computational cost is substantial for large-model ensembles; dependency on open-source model pool and English-only evaluation; early-stopping heuristics required (stop if answer unchanged for 2 iterations).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'N-C RITICS : Self-Refinement of Large Language Models with Ensemble of Critics', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5201.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5201.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine: Iterative Refinement with Self-Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work where a pre-trained LLM uses self-generated feedback to iteratively refine its own outputs (the same model acts as both generator and critic).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Refine: Iterative Refinement with Self-Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>single pre-trained LLM (same model as critic and generator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Approach uses a single LLM to both produce an answer and generate critiques/feedback for that answer, enabling iterative self-correction without external tools.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>self-refine / self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate-then-reflect where the same LLM examines its own prior output, produces feedback or error diagnoses, and then uses that feedback to produce a revised output; iterative loop until stopping condition.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>general iterative self-correction (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Broad method applied to iterative correction tasks; paper is cited as demonstration that self-feedback can be used end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Cited by the authors as prior work showing that iterative self-feedback from the same LLM can improve outputs (no numerical details reproduced in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mentioned as an existing approach in related work; this paper positions N-CRITICS as complementary by using ensembles of distinct LLM critics rather than only self-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'N-C RITICS : Self-Refinement of Large Language Models with Ensemble of Critics', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5201.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5201.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior method that adopts iterative self-correction with an episodic memory buffer storing past feedback and model outputs to prevent repeating errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM agent with an episodic memory component (as described in original Reflexion paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Agent architecture that augments an LLM with a memory buffer that records prior feedback and outcomes to guide future self-corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>iterative self-correction with episodic memory</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The model iteratively generates outputs and verbal feedback, stores interactions and feedback in memory, and uses stored history to inform future corrections and avoid repeated mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>iterative agent improvement (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>General improvement of agent behavior using verbal reinforcement and memory of past feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Mentioned in related work as an approach that improved performance by using memory of prior feedback to prevent repeated errors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Cited as related work; N-CRITICS differs by using an ensemble of separate LLM critics and focusing on open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'N-C RITICS : Self-Refinement of Large Language Models with Ensemble of Critics', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5201.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5201.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CRITIC (Gou et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior method that generates critics using a suite of specialized tools (e.g., code interpreters, calculators, search) to interactively critique and help correct LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs with external tool interaction (per CRITIC paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CRITIC uses LLMs in combination with external specialized tools to create critiques/evidence which are then used to refine generator outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>tool-interactive critiquing</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>A critic generation pipeline that leverages external deterministic tools (e.g., executors, calculators, search) to evaluate model outputs and produce feedback which guides model refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>tool-assisted self-correction (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Use of external tool feedback to detect and correct model errors across tasks such as code, math, and factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Cited as prior art showing tool-backed critics can help correct outputs; N-CRITICS compares itself to CRITIC in evaluations (table comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Referenced limitations include reliance on external tools and possibly proprietary resources; N-CRITICS emphasizes open-source critics without external proprietary tools.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'N-C RITICS : Self-Refinement of Large Language Models with Ensemble of Critics', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5201.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5201.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Edit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Edit: Fault-aware code editor for code generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior approach that executes generated code (via a code interpreter) and uses execution feedback as a source of critique to refine code outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Edit: Fault-aware code editor for code generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM + code execution environment (per Self-Edit paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method couples an LLM code generator with an executor/interpreter to obtain runtime feedback (errors/results) used to iteratively edit code.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>execution-feedback-based self-edit</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate code, execute it, collect runtime errors or outputs as feedback, and use that feedback to iteratively refine the code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>code generation refinement (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reducing code generation errors by using execution trace/errors as feedback for iterative correction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Cited in related work as an example where external execution feedback improves generated code quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mentioned as related work; differs from N-CRITICS which focuses on ensembles of LLM critics rather than executing code.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'N-C RITICS : Self-Refinement of Large Language Models with Ensemble of Critics', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-Refine: Iterative Refinement with Self-Feedback <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing <em>(Rating: 2)</em></li>
                <li>Self-Edit: Fault-aware code editor for code generation <em>(Rating: 1)</em></li>
                <li>Improving language models via plug-and-play retrieval feedback <em>(Rating: 1)</em></li>
                <li>Read, revise, repeat: A system demonstration for human-in-the-loop iterative text revision <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5201",
    "paper_id": "paper-264590585",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "N-CRITICS (toxicity)",
            "name_full": "N-CRITICS: Self-Refinement of Large Language Models with Ensemble of Critics",
            "brief_description": "An iterative, model-agnostic self-refinement framework that uses an ensemble of open-source LLMs as critics to generate feedback which is aggregated and used to prompt the generator to produce improved outputs; applied to reduce toxicity by re-generating outputs guided by critics' feedback.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Koala-13b, Vicuna-13b, Wizard-13b (used as generation/critic models in toxicity experiments)",
            "model_description": "Open-source instruction-finetuned/chat-style LLMs based on LLaMA-family backbones (13B-scale variants used in toxicity experiments); used both as critics in ensembles and as generation models depending on experiment.",
            "reflection_method_name": "N-CRITICS (ensemble critics iterative refinement)",
            "reflection_method_description": "Generate-then-reflect loop where an initial output y0 is produced by a primary LLM, then circulated to an ensemble of other open-source LLMs (including the primary) to produce critiques/feedback; critiques are aggregated and used to modify the prompt to the primary LLM to regenerate a refined answer; process repeats up to a preset iteration cap or until stopping criteria (e.g., toxicity below threshold) are met.",
            "num_iterations": 4,
            "task_name": "Toxicity reduction (REALTOXICITYPROMPTS)",
            "task_description": "Mitigate toxic continuations from prompts using the REALTOXICITYPROMPTS dataset (1,700 sampled prompts) and measure toxicity via the PERSPECTIVE API.",
            "performance_with_reflection": "Authors report substantial toxicity reduction after refinement; example instance reduced to toxicity score 0.035 after refinement. Reported mean toxicity reductions across 1,700 prompts (see Table 1); authors state N-CRITICS outperforms proprietary baselines such as ChatGPT in their comparisons (numerical table entries reported in paper).",
            "performance_without_reflection": "Vanilla generation (no critics) had higher mean toxicity (baseline values reported in Table 1 and visualized in Figure 3). The paper shows '0 critics' curves in Figure 3 as the baseline; explicit aggregated baseline numbers are provided in Table 1 (paper).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: mean toxicity across outputs decreased after iterative refinement on a 1,700-prompt sample; example refined output toxicity = 0.035. Figures (Figure 3) show toxicity declining across iterations and with more critics; Table 1 reports N-CRITICS achieving lower toxicity than competing methods and proprietary ChatGPT as presented by the authors.",
            "limitations_or_failure_cases": "Authors report diminishing returns beyond ~4 critics (improvement plateaus); iterative ensemble increases computational overhead and latency; method depends on the quality of open-source LLM critics and thus inherits shared biases/flaws; experiments are English-centric so language-generalization unknown.",
            "uuid": "e5201.0",
            "source_info": {
                "paper_title": "N-C RITICS : Self-Refinement of Large Language Models with Ensemble of Critics",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "N-CRITICS (factual)",
            "name_full": "N-CRITICS: Self-Refinement of Large Language Models with Ensemble of Critics",
            "brief_description": "Same ensemble-based generate-then-reflect framework applied to reduce factual hallucinations on QA datasets by using large-model critics to spot errors and guide answer regeneration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-70b (generator); critics: WizardLM-70b and LLaMA-70b (larger critics); small models (Koala-13b, Vicuna-13b) tested but found unhelpful",
            "model_description": "LLaMA-70b used as the main generator for factual-hallucination experiments; critics included WizardLM-70b and another LLaMA-70b instance — large open-source models provided the most useful critiques according to authors.",
            "reflection_method_name": "N-CRITICS (ensemble critics iterative refinement)",
            "reflection_method_description": "Initial answer produced by generator (LLaMA-70b) is evaluated by an ensemble of critics (other LLMs). Critics provide explicit feedback or corrected evidence; feedback is aggregated and used to re-prompt the generator to produce an improved answer; early stopping used if generated answer unchanged for two consecutive corrections.",
            "num_iterations": 3,
            "task_name": "Factual hallucination reduction (TriviaQA, AmbigNQ, HotpotQA)",
            "task_description": "Reduce unfaithful or hallucinated facts in answers on three QA benchmarks (400 random samples per dataset) measured via Exact Match (EM) and F1 metrics.",
            "performance_with_reflection": "Authors report improved EM and F1 scores after N-CRITICS refinement; example summary reported in paper (Table 2) shows numerical gains (paper reports N-CRITICS values such as EM/F1 aggregates; e.g., top-line numbers printed for N-CRITICS include 78.02 and 84.67 among others — see Table 2 for dataset-specific EM/F1).",
            "performance_without_reflection": "Vanilla LLaMA-70b baseline reported (paper shows vanilla numbers in Table 2, e.g., baselines like 73 for a vanilla metric row) and N-CRITICS improves over those baselines; authors also compare against other methods (CRITIC, ChatGPT) as reported in Table 2.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: Table 2 reports higher F1 for N-CRITICS across all three datasets and higher EM on TriviaQA and HotpotQA relative to baselines; Figure 4 shows performance gains across iterations and with additional critics on TriviaQA (improvements accumulate across iterations/critics until plateau). Authors also note small critics degraded performance for this task (empirical negative case).",
            "limitations_or_failure_cases": "Authors found critiques from smaller models (Koala-13b, Vicuna-13b) did not help or reduced output quality for factual tasks, indicating critic quality matters; computational cost is substantial for large-model ensembles; dependency on open-source model pool and English-only evaluation; early-stopping heuristics required (stop if answer unchanged for 2 iterations).",
            "uuid": "e5201.1",
            "source_info": {
                "paper_title": "N-C RITICS : Self-Refinement of Large Language Models with Ensemble of Critics",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Self-Refine",
            "name_full": "Self-Refine: Iterative Refinement with Self-Feedback",
            "brief_description": "Prior work where a pre-trained LLM uses self-generated feedback to iteratively refine its own outputs (the same model acts as both generator and critic).",
            "citation_title": "Self-Refine: Iterative Refinement with Self-Feedback",
            "mention_or_use": "mention",
            "model_name": "single pre-trained LLM (same model as critic and generator)",
            "model_description": "Approach uses a single LLM to both produce an answer and generate critiques/feedback for that answer, enabling iterative self-correction without external tools.",
            "reflection_method_name": "self-refine / self-feedback",
            "reflection_method_description": "Generate-then-reflect where the same LLM examines its own prior output, produces feedback or error diagnoses, and then uses that feedback to produce a revised output; iterative loop until stopping condition.",
            "num_iterations": null,
            "task_name": "general iterative self-correction (prior work)",
            "task_description": "Broad method applied to iterative correction tasks; paper is cited as demonstration that self-feedback can be used end-to-end.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": null,
            "evidence_of_improvement": "Cited by the authors as prior work showing that iterative self-feedback from the same LLM can improve outputs (no numerical details reproduced in this paper).",
            "limitations_or_failure_cases": "Mentioned as an existing approach in related work; this paper positions N-CRITICS as complementary by using ensembles of distinct LLM critics rather than only self-feedback.",
            "uuid": "e5201.2",
            "source_info": {
                "paper_title": "N-C RITICS : Self-Refinement of Large Language Models with Ensemble of Critics",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: Language agents with verbal reinforcement learning",
            "brief_description": "Prior method that adopts iterative self-correction with an episodic memory buffer storing past feedback and model outputs to prevent repeating errors.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "mention",
            "model_name": "LLM agent with an episodic memory component (as described in original Reflexion paper)",
            "model_description": "Agent architecture that augments an LLM with a memory buffer that records prior feedback and outcomes to guide future self-corrections.",
            "reflection_method_name": "iterative self-correction with episodic memory",
            "reflection_method_description": "The model iteratively generates outputs and verbal feedback, stores interactions and feedback in memory, and uses stored history to inform future corrections and avoid repeated mistakes.",
            "num_iterations": null,
            "task_name": "iterative agent improvement (prior work)",
            "task_description": "General improvement of agent behavior using verbal reinforcement and memory of past feedback.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": null,
            "evidence_of_improvement": "Mentioned in related work as an approach that improved performance by using memory of prior feedback to prevent repeated errors.",
            "limitations_or_failure_cases": "Cited as related work; N-CRITICS differs by using an ensemble of separate LLM critics and focusing on open-source models.",
            "uuid": "e5201.3",
            "source_info": {
                "paper_title": "N-C RITICS : Self-Refinement of Large Language Models with Ensemble of Critics",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "CRITIC (Gou et al.)",
            "name_full": "CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing",
            "brief_description": "Prior method that generates critics using a suite of specialized tools (e.g., code interpreters, calculators, search) to interactively critique and help correct LLM outputs.",
            "citation_title": "CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing",
            "mention_or_use": "mention",
            "model_name": "LLMs with external tool interaction (per CRITIC paper)",
            "model_description": "CRITIC uses LLMs in combination with external specialized tools to create critiques/evidence which are then used to refine generator outputs.",
            "reflection_method_name": "tool-interactive critiquing",
            "reflection_method_description": "A critic generation pipeline that leverages external deterministic tools (e.g., executors, calculators, search) to evaluate model outputs and produce feedback which guides model refinement.",
            "num_iterations": null,
            "task_name": "tool-assisted self-correction (prior work)",
            "task_description": "Use of external tool feedback to detect and correct model errors across tasks such as code, math, and factuality.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": null,
            "evidence_of_improvement": "Cited as prior art showing tool-backed critics can help correct outputs; N-CRITICS compares itself to CRITIC in evaluations (table comparisons).",
            "limitations_or_failure_cases": "Referenced limitations include reliance on external tools and possibly proprietary resources; N-CRITICS emphasizes open-source critics without external proprietary tools.",
            "uuid": "e5201.4",
            "source_info": {
                "paper_title": "N-C RITICS : Self-Refinement of Large Language Models with Ensemble of Critics",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Self-Edit",
            "name_full": "Self-Edit: Fault-aware code editor for code generation",
            "brief_description": "Prior approach that executes generated code (via a code interpreter) and uses execution feedback as a source of critique to refine code outputs.",
            "citation_title": "Self-Edit: Fault-aware code editor for code generation",
            "mention_or_use": "mention",
            "model_name": "LLM + code execution environment (per Self-Edit paper)",
            "model_description": "Method couples an LLM code generator with an executor/interpreter to obtain runtime feedback (errors/results) used to iteratively edit code.",
            "reflection_method_name": "execution-feedback-based self-edit",
            "reflection_method_description": "Generate code, execute it, collect runtime errors or outputs as feedback, and use that feedback to iteratively refine the code generation.",
            "num_iterations": null,
            "task_name": "code generation refinement (prior work)",
            "task_description": "Reducing code generation errors by using execution trace/errors as feedback for iterative correction.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": null,
            "evidence_of_improvement": "Cited in related work as an example where external execution feedback improves generated code quality.",
            "limitations_or_failure_cases": "Mentioned as related work; differs from N-CRITICS which focuses on ensembles of LLM critics rather than executing code.",
            "uuid": "e5201.5",
            "source_info": {
                "paper_title": "N-C RITICS : Self-Refinement of Large Language Models with Ensemble of Critics",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing",
            "rating": 2,
            "sanitized_title": "critic_large_language_models_can_selfcorrect_with_toolinteractive_critiquing"
        },
        {
            "paper_title": "Self-Edit: Fault-aware code editor for code generation",
            "rating": 1,
            "sanitized_title": "selfedit_faultaware_code_editor_for_code_generation"
        },
        {
            "paper_title": "Improving language models via plug-and-play retrieval feedback",
            "rating": 1,
            "sanitized_title": "improving_language_models_via_plugandplay_retrieval_feedback"
        },
        {
            "paper_title": "Read, revise, repeat: A system demonstration for human-in-the-loop iterative text revision",
            "rating": 1,
            "sanitized_title": "read_revise_repeat_a_system_demonstration_for_humanintheloop_iterative_text_revision"
        }
    ],
    "cost": 0.01448975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>N-CRITICS: Self-Refinement of Large Language Models with Ensemble of Critics</p>
<p>Sajad Mousavi 
Ricardo Luna Gutierrez 
Desik Rengarajan 
AshwinVineet Gundecha 
Ramesh Babu 
Avisek Naug 
Antonio Guillen 
Soumyendu Sarkar 
N-CRITICS: Self-Refinement of Large Language Models with Ensemble of Critics
18FEF6B2F78FB7DA8ED265E656DE3885
We propose a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination.This method involves refining model outputs through an ensemble of critics and the model's own feedback.Drawing inspiration from human behavior, we explore whether LLMs can emulate the self-correction process observed in humans who often engage in self-reflection and seek input from others to refine their understanding of complex topics.Our approach is model-agnostic and can be applied across various domains to enhance trustworthiness by addressing fairness, bias, and robustness concerns.We consistently observe performance improvements in LLMs for reducing toxicity and correcting factual errors.</p>
<p>Introduction</p>
<p>The recent impressive results achieved by LLMs have led to a substantial surge in their utilization and investigation (OpenAI, 2023;Touvron et al., 2023;Chowdhery et al., 2022;Zhao et al., 2023).However, as a consequence of this heightened exposure, it is important to ensure their accuracy and safety.This concern is particularly significant in light of the demonstrated potential for LLMs to exhibit unfaithful and toxic behavior (Deshpande et al., 2023;Lin et al., 2022;Maynez et al., 2020).</p>
<p>Various methods have been proposed to tackle this problem (Pan et al., 2023).From training-time correction (Xu et al., 2023;Liu and Liu, 2021;Li et al., 2019;Jauregi Unanue et al., 2021;Zelikman et al., 2022;Huang et al., 2022) to post output generation refinement (Madaan et al., 2023;Shinn et al., 2023;Zhang et al., 2023;Pan et al., 2023;Du et al., 2022;Yu et al., 2023;Gou et al., 2023;Paul et al., 2023;Le et al., 2022;Akyurek et al., 2023), these methods have shown the impact that iterative self-refinement and proper feedback can have on the performance of LLMs.</p>
<p>While numerous prior research endeavors have focused on iterative feedback, generated from external tools or the LLM itself, there exists a gap in the exploration of the potential benefits arising from an ensemble of distinct general-purpose Large Language Models (LLMs).Such an ensemble could evaluate LLM-generated output and subsequently offer feedback, which can help to reduce toxicity and rectify factual errors.</p>
<p>Taking inspiration from human behavior, where seeking feedback from others is a common practice for improvement, we introduce N-CRITICS, a self-correction framework designed for LLMs.N-CRITICS leverages an ensemble of critics, each represented by a distinct LLM, which can provide new evidence or feedback to correct the reasoning of the main generation model.In our framework, the</p>
<p>Input BlackBox LLM</p>
<p>Ouput</p>
<p>Refine</p>
<p>Self Critique Feedback generated output of an LLM is supplied to the ensemble of critics, and subsequently, we aggregate the critiques provided by the ensemble.These collected critiques are then used to prompt the generator to regenerate the output, taking into consideration the feedback received.This iterative process is repeated for a predefined number of cycles or until further refinement is deemed unnecessary.Figure 1 shows our overall approach.</p>
<p>Contrary to previous approaches, N-CRITICS is built on open-source models and does not rely on proprietary models and tools such as GPT (OpenAI, 2023) or Google search.We evaluate N-CRITICS on the REALTOXICITYPROMPTS (Gehman et al., 2020) dataset for toxicity.The AmbigNQ (Min et al., 2020), TriviaQA (Joshi et al., 2017) and HotpotQA (Yang et al., 2018) datasets were used to test factual hallucination.We show that N-CRITICS is able to improve the original output of the LLM model, increasing its accuracy and reducing toxicity.</p>
<p>Figure 1: Workflow of the N-CRITICS approach.The process begins with initializing the initial context and output, after which feedback is generated by engaging with an ensemble of opensource LLMs.This feedback then guides the refinement of the output through iterative steps until a satisfactory result is achieved or a stopping criterion is met.</p>
<p>Related Work</p>
<p>Refinements of LLM outputs can be achieved through various methods, involving both human and machine-generated verification and feedback.One effective technique is Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022), which fine-tunes LLMs to generate more compelling outputs for human consumption.Similarly, the use of iterative human feedback to refine model outputs has been explored in Du et al. (2022).While these approaches have been shown to improve the performance of LLM models, they rely on human intervention, which can be expensive and time-consuming.</p>
<p>To address this limitation, alternative automated approaches that do not require human intervention during the refinement process have been proposed (Pan et al., 2023).For instance, Self-Refine (Madaan et al., 2023) introduced an approach that involves utilizing a pre-trained Large Language Model (LLM) for comprehensive end-to-end self-correction.In this approach, the same LLM serves a dual role, both as the generator of output and as the provider of feedback.This generated feedback is subsequently employed to prompt the model to iteratively refine its previous output based on the received feedback.Similarly, Reflexion (Shinn et al., 2023) adopts iterative self-correction and proposes the incorporation of an episodic memory buffer to enhance performance.This memory buffer stores past feedback and corresponding model outputs with the aim of preventing the repetition of previous errors.</p>
<p>On the other hand, employing external tools as sources of feedback has also been a subject of exploration.Self-Edit (Zhang et al., 2023) executes code generated by an LLM in a code interpreter and provides the execution results as feedback.Logic-LM (Pan et al., 2023) proposes to address logical reasoning through a two-step process.Firstly, an LLM translates a natural language problem into a symbolic formulation, and subsequently, a deterministic symbolic solver is deployed to conduct inference on this formulation.The error messages returned by the solver are harnessed as feedback to refine the LLM-generated output.RARR (Du et al., 2022) and REFEED (Yu et al., 2023) leverage an external corpus of collected documents to look for evidence that corroborates or contradicts the outputs generated by the models.This acquired evidence is subsequently employed for model refinement.CRITIC (Gou et al., 2023) proposes the use of a suite of specialized tools for a variety of tasks such as code interpreters, calculators, or search engines to generate critics for the LLM's generated output.Moreover, approaches such as REFINER (Paul et al., 2023), CodeRL (Le et al., 2022) and RL4F (Akyurek et al., 2023) propose to train a specialized critic to provide feedback to the generator model.</p>
<p>In contrast to these approaches, our approach harnesses the collective knowledge of multiple generalpurpose LLMs, including the primary LLM in use.Moreover, in our work, we only use models that are part of the open-source community, which differ from many of the previous work which uses proprietary generation models and feedback tools.Our ensemble-based strategy enables automatic LLM output correction across various domains via an iterative process of feedback and correction, without training or fine-tuning, showcasing the versatility and effectiveness of our approach.</p>
<p>Algorithm 1 N-CRITICS algorithm end if 8: Refine the input prompt based on critiques to form x ′ .9: Obtain the improved output yi+1 by feeding x ′ to M. 10: Increment i by 1. 11: end while
Input: Prompt x, primary LLM model M, ensemble of external LLMs L = [L1, L2, L3, . . . ,</p>
<p>N-CRITICS: Ensemble of Critics</p>
<p>The capacity for LLMs to emulate the human self-correction process poses an engaging research query.Humans typically use a dual strategy: self-reflection and seeking external feedback when grappling with intricate thoughts or texts.Although LLMs have advanced impressively, their initial outputs often require improvement.Our proposal, inspired by the human text refinement process, presents two main strategies: self-reflection and critiquing.The overarching objective is to elevate the quality of LLM-generated content.An outline of our proposed framework, N-CRITICS, is depicted in Figure 1.</p>
<p>The methodology operates on an iterative feedback mechanism that doesn't require supervised training.Let x be a given input, the LLM, denoted as M, produces an initial output y0.This output y0 is then circulated through M itself and other LLMs for evaluation.The feedback obtained from the ensemble of critics is compiled, guiding M to refine its output into yi, where i is the iteration count.The iterations persist until a specific stopping criterion, relevant to the given task, is fulfilled.The N-CRITICS approach is elaborated in Algorithm 1.</p>
<p>Setting the Initial Context and Output: Our methodology draws from existing self-correction strategies (Pan et al., 2023), implementing chain-of-thought reasoning combined with few-shot and zero-shot prompting for in-context learning.For a given input x, a set of handcrafted examples are combined into the input prompt, directing the LLMs to yield an initial output y0.Depending on the nature of the task, the zero-shot strategy might be incorporated, enhancing the input prompt with "Let's think step by step" (Kojima et al., 2022).An example of such prompts, used during the refinement for the factual hallucination and toxicity reduction tasks, can be observed in Figure 2.</p>
<p>Feedback Generation from Ensemble of Critics: For feedback procurement, we harness multiple open-source LLMs.With the input x and its corresponding initial output y0, the primary model M interacts with other LLMs to evaluate y0.This feedback mechanism is tailored according to the specific task.Figure 2 displays instances of the prompts and the feedback acquired during the refinement for tasks like toxicity reduction and factual hallucination.</p>
<p>Output Correction via Feedback:</p>
<p>The critiquing strategy tackles the noted inconsistencies and issues prevalent in LLM outputs.It mirrors the human practices of consulting experts or tools to assess and improve upon initial drafts.The critique process starts with the LLM's initial output, which is then assessed by appropriate tools to evaluate its various dimensions.Feedback from this assessment is used to revise the output.This iterative mechanism, leveraging both introspective and external feedback, fine-tunes LLM-generated content, bridging the gap between machine-generated and human-curated content.</p>
<p>Experiments and Results</p>
<p>We evaluate N-CRITICS on two distinct tasks: Toxicity reduction, which focuses on improving the overall health (and safety) of the LLM model output, and factual hallucination reduction, which ensures the accuracy of generated content.For our assessments, we leverage several open-sourced base LLMs, which include: LLaMA-70b (Touvron et al., 2023): A widely recognized LLM by Meta., WizardLM-70b and 13b (Xu et al., 2023): A variant of LLaMA trained with the innovative Evol-Instruct method on intricate instruction data., Koala-13b (Geng et al., 2023): This model is fine-tuned on dialogue data extracted from the web and utilizes Meta's LLaMA as its foundation., and Vicuna-13b (Chiang et al., 2023): An open-sourced chatbot that's been fine-tuned using conversations from ShareGPT, a platform where users share their ChatGPT dialogues.</p>
<p>In our experiments, we consistently set the temperature parameter to p = 0.7 to encourage diverse outputs.We also cap the number of iterations, itr, at 4. Notably, we present the results of previous state-of-the-art approaches as originally reported.Replicating their outcomes would necessitate extensive training and inference using LLMs.Furthermore, the APIs for their LLM models are not freely accessible.</p>
<p>Factual Hallucination</p>
<p>Q: What was advertised with Eva Herzagovia using the slogan hello boys?A: Let's think step by step.Eva Herzagovia was advertised with the slogan hello boys.So the answer is: Eva Herzagovia.(Main Model Original Answer)</p>
<p>Is there any problem with answer 'A:'?Let's ask model 1: &gt; Q: What was advertised with Eva Herzagovia using the slogan hello boys?&gt; A: Let's think step by step.Eva Herzagovia was advertised with the slogan hello boys.So the answer is: Eva Herzagovia.</p>
<p>Feedback:</p>
<p>No, the answer is not correct.The correct answer should be the product or service that Eva Herzagovia was promoting with the slogan "hello boys," not Eva Herzagovia herself.(Model 1 Feedback)</p>
<p>Let's ask model 2: &gt; Q: What was advertised with Eva Herzagovia using the slogan hello boys?&gt; A: Let's think step by step.Eva Herzagovia was advertised with the slogan hello boys.So the answer is: Eva Herzagovia.</p>
<p>Feedback: No, the answer provided is not factually correct.Eva Herzagovia was not advertised with the slogan "hello boys."The correct answer is Wonderbra, as she was featured in a Wonderbra ad campaign with that slogan.(Model 2 Feedback)</p>
<p>Given the previous feedback from model 1 and model 2, revisit your previous answer.</p>
<p>Question: What was advertised with Eva Herzagovia using the slogan hello boys?</p>
<p>The product that was advertised with Eva Herzagovia using the slogan "helloboys" was Wonderbra.So the answer is: Wonderbra.(Main Model Refined Answer)</p>
<p>Toxicity Reduction</p>
<p>The input: "Your call is very important to us..." we are told, by automated voices that don't give a</p>
<p>The current toxic text continuation of input: damn about our frustrations or our time, just like the endless hold music that never seems to end, it's all a part of the modern customer service experience.(Main Model Original Answer)</p>
<p>The feedback: The current toxic text continuation has 57% toxicity of PROFANITY.Also, another oracle says that the text is more closely associated with Profanity, with 45%.Additionally, another source says that the text is closely associated with the category of Profanity.Also, another augur says that the text "damn about our frustrations or our time, just like the endless hold music that never seems to end, it's all a part of the modern customer service experience" can be categorized as "Toxicity" with a percentage of 25%.(Model 1,2, 3 and Main Model Feedback)</p>
<p>Considering the mentioned feedback, give a new healthier, safer, and more fluent continuation that does not contain toxicity, insult, profanity, identity attack, threats sexually explicit, etc. Ensure not to repeat the provided input text.</p>
<p>Start the new continuation only after "--&gt;" and ensure not repeat the previous toxic continuation text: "Your call is very important to us..." we are told, by automated voices that don't give a--&gt; thought to our frustrations or the value of our time.Instead, they guide us through an endless hold music experience, just like the never-ending loop of an eternal wait.This is the modern customer service experience, where every interaction is designed to make us feel valued and app reciated.(Main Model Refined Answer) New toxicity: 0.035 Toxicity Reduction: We evaluate the capacity of our proposed method, N-CRITICS, to mitigate potential toxicity in LLM-generated content.Using a random sample of 1,700 prompts from the non-toxic section of REALTOXICITYPROMPTS (Gehman et al., 2020)-a dataset deliberately designed to elicit toxic responses-we gauge the effectiveness of our technique.For a comprehensive assessment of toxicity levels, we employ the PERSPECTIVE API 2 in conjunction with three notable chatbots: Koala-13 (Geng et al., 2023), Vicuna-13 (Chiang et al., 2023), and Wizard-13b (Xu et al., 2023).To evaluate our results, we compute the mean toxicity across all the finalized outputs and present both dist-2 and dist-3 scores, representing the uniqueness of bigrams and trigrams respectively.The refinement process is halted either when the content's overall toxicity drops below 10% or when the maximum iteration limit is reached.</p>
<p>Table 1 showcases the performance of N-CRITICS relative to other leading methods.Evidently, our approach substantially reduces the toxicity levels in the LLM-generated outputs while ensuring diversity.Remarkably, N-CRITICS outperforms proprietary LLMs such as ChatGPT and supervised methods that necessitate training steps and data.In addition, we examined the impact of iterative correction as well as the number of LLMs as critics.Figure 3 traces the detoxification trajectory across iterations for varying numbers of critics.It is observable that as both the iteration frequency and the number of critics rise, the toxicity diminishes.However, beyond four critics, the improvement plateaus, suggesting the LLM might have tapped into its maximal capability for generating non-toxic content, regardless of additional feedback.</p>
<p>Factual Hallucination Reduction:</p>
<p>To assess the effectiveness of our proposed model in addressing the task of reducing factual hallucinations, we conducted experiments using three distinct datasets: TriviaQA (Joshi et al., 2017), AmbigNQ (Min et al., 2020) and HotpotQA (Yang et al., 2018).From each of these datasets, we randomly selected 400 samples for evaluation.We set a maximum of 3 refinement intentions.Moreover, as in shown CRITIC (Gou et al., 2023), we make an early stop in the refinement loop if the generated answer did not change for two consecutive corrections.We report the results of these experiments in terms of two key metrics: Exact Match (EM) and F1 scores.</p>
<p>The EM score is determined by assessing whether the model's prediction precisely matches the characters of the correct answer(s).If there is an exact match between the predicted output and the true answer(s), the EM score is set to 1; otherwise, it is assigned a value of 0.</p>
<p>We used Llama-70b (Touvron et al., 2023) as the base generator.For critics, we used WizardLM-70b (Xu et al., 2023) and Llama-70b.During the development of N-CRITICS, we found that the critiques from small models such as Koala-13b or Vicuna-13b did not help improve or even reduce the quality of the generated output for this task, hence the dependency on larger models used in our experimentation.</p>
<p>Table 2 shows the results obtained for these experiments.N-CRITICS effectively rectifies untruthful facts across all three distinct datasets.Moreover, N-CRITICS outperforms state-of-the-art methods in terms of F1 scores on all three datasets.For EM, N-CRITICS exhibits superior performance on TriviaQA and HotpotQA.Furthermore, we conducted an analysis to assess the impact of including additional critics and refinement iterations on the quality of the generated output.For this analysis we used TriviaQA.As illustrated in Figure 4, N-CRITICS achieves improvements in performance with the incorporation of additional critics and refinement iterations.(Gou et al., 2023).(Joshi et al., 2017) Limitations: Our work with N-CRITICS presents several noteworthy findings, yet it is not without its limitations.Dependence on Open-Sourced LLMs: A significant aspect of our method is its reliance on feedback from open-sourced LLMs.This means the quality and effectiveness of N-CRITICS are inherently tied to the caliber of these models.Shared biases or flaws among these LLMs could potentially influence the refined outputs.Nevertheless, with the rapid growth in the open-sourced LLM community, we believe some of these concerns may be alleviated in the future.</p>
<p>Computational Load: The iterative feedback and refinement process, especially when soliciting insights from multiple LLMs, can add computational overhead.As a consequence, the refinement process might experience delays, especially in resource-constrained environments.Language Specificity: Our current experiments predominantly revolve around English datasets.As such, the method's effectiveness and applicability in non-English contexts remain unknown.</p>
<p>N-Critic</p>
<p>F1</p>
<p>GPT-2 0.527 0.85 0.85 PPLM (Dathathri et al., 2019) 0.520 0.86 0.86 GeDi (Krause et al., 2021) 0.363 0.84 0.83 DEXPERT (Liu et al., 2021) 0.314 0.84 0.84 DAPT (Gururangan et al., 2020) 0.428 0.84 0.84 PPO (Lu et al., 2022) 0.218 0.79 0.82 QUARK (Lu et al., 2022) 0.196 0.80 0.84 Self-Correct (Welleck et al., 2022) 0</p>
<p>Conclusion</p>
<p>We introduced N-CRITICS, an innovative method leveraging feedback from open-sourced LLMs to iteratively refine model outputs, setting it apart from current self-refinement approaches (also, their underlying models are not free to use).Our evaluations across diverse tasks, ranging from hallucination and factual error mitigation to toxicity reduction, consistently underscore the merit of employing critiques from various LLMs to strengthen overall LLM performance.Looking ahead, we aim to broaden our evaluative lens to capture a wider array of errors, specifically those tied to flawed code and instances of unfaithful reasoning-where the conclusion strays from the established reasoning trajectory.While our current research predominantly centered on English datasets, a strategic expansion into multilingual tasks remains on our agenda as well.</p>
<p>Figure 2 :
2
Figure2: Left: Example prompt used for factual hallucination refinement in N-CRITICS.N-CRITICS evaluates the truthfulness and accuracy of a generated answer and collects feedback from its ensemble of critics.It utilizes this feedback to guide the generation of new improved output, with the ultimate goal of improving the truthfulness and accuracy of the response.Right: Example prompt for toxicity reduction with N-CRITICS.Feedback, identifying toxic elements in the output, is gathered from the ensemble (including the primary LLM) and used to guide the main LLM in mitigating such issues.</p>
<p>Figure 3 :
3
Figure 3: Left: Detoxification progress over iterations, Right: Variations in detoxification reduction across different critic numbers (0-4).</p>
<p>2 https://www.perspectiveapi.com/</p>
<p>Figure 4 :
4
Figure 4: Left: Factual hallucination reduction progress over iterations, Right: Variations in factual hallucination reduction across different critic numbers (0-2).Both evaluations were done using the TriviaQA dataset(Joshi et al., 2017)</p>
<p>Table 1 :
1
Toxicity reduction results.
N-Critic</p>
<p>Table 2 :
2
Factual hallucination reduction results.The results of the methods evaluated on ChatGPT are taken from CRITIC</p>
<p>.15 79.35 48.79 60.03 41.75 50.91
.1710.800.83ChatGPTChatGPT (OpenAI, 2023)0.3250.770.76CRITIC (Gou et al., 2023)0.1730.780.77Wizard-13BVanilla0.2130.937 0.892N-CRITICS0.0680.944 0.922Vanilla70.479.335.152.423.236.6CoT (Wei et al., 2022)72.979.244.258.633.746.1ReACT (Yao et al., 2023)63.769.847.661.234.947.9CRITIC (Gou et al., 2023) 75.181.750.064.938.750.5Llama-70bVanilla73
N-CRITICS 78.02 84.67 50.93 62.54 43.13 52.56</p>
<p>arXiv:2303.08774OpenAI, Gpt-4 technical report. 2023</p>
<p>H Touvron, T Lavril, G Izacard, X M M , -A L T L B R N G E H F A A R A J E Grave, G Lample, Llama: Open and efficient foundation language models. 2023</p>
<p>A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, P Schuh, K Shi, S Tsvyashchenko, J Maynez, A Rao, P Barnes, Y Tay, N Shazeer, V Prabhakaran, E Reif, N Du, B Hutchinson, R Pope, J Bradbury, J Austin, M Isard, G Gur-Ari, P Yin, T Duke, A Levskaya, S Ghemawat, S Dev, H Michalewski, X Garcia, V Misra, K Robinson, L Fedus, D Zhou, D Ippolito, D Luan, H Lim, B Zoph, A Spiridonov, R Sepassi, D Dohan, S Agrawal, M Omernick, arXiv:2204.02311Scaling language modeling with pathways. M Dai, T S Pillai, M Pellat, A Lewkowycz, E Moreira, R Child, O Polozov, K Lee, Z Zhou, X Wang, B Saeta, M Diaz, O Firat, M Catasta, J Wei, K Meier-Hellstern, D Eck, J Dean, Petrov, N. Fiedel, Palm2022</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, Y Du, C Yang, Y Chen, Z Chen, J Jiang, R Ren, Y Li, X Tang, Z Liu, P Liu, J.-Y Nie, J.-R Wen, arXiv:2303.18223A survey of large language models. 2023</p>
<p>A Deshpande, V Murahari, T Rajpurohit, A Kalyan, K Narasimhan, arXiv:2304.05335Toxicity in chatgpt: Analyzing persona-assigned language models. 2023</p>
<p>Measuring how models mimic human falsehoods. S Lin, J Hilton, O Evans, Truthfulqa , arXiv:2109.079582022</p>
<p>On faithfulness and factuality in abstractive summarization. J Maynez, S Narayan, B Bohnet, R Mcdonald, 10.18653/v1/2020.acl-main.173Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline2020</p>
<p>Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies. L Pan, M Saxon, W Xu, D Nathani, X Wang, W Y Wang, arXiv:2308.031882023</p>
<p>SESCORE2: Learning text generation evaluation via synthesizing realistic mistakes. W Xu, X Qian, M Wang, L Li, W Y Wang, 10.18653/v1/2023.acl-long.283Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>SimCLS: A simple framework for contrastive learning of abstractive summarization. Y Liu, P Liu, 10.18653/v1/2021.acl-short.135Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline20212Short Papers)</p>
<p>Deep reinforcement learning with distributional semantic rewards for abstractive summarization. S Li, D Lei, P Qin, W Y Wang, 10.18653/v1/D19-1623Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>BERTTune: Fine-tuning neural machine translation with BERTScore. I Jauregi Unanue, J Parnell, M Piccardi, 10.18653/v1/2021.acl-short.115Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline20212Short Papers)</p>
<p>Star: Bootstrapping reasoning with reasoning. E Zelikman, Y Wu, J Mu, N Goodman, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Large language models can self-improve. J Huang, S S Gu, L Hou, Y Wu, X Wang, H Yu, J Han, arXiv:2210.116102022</p>
<p>Self-Refine: Iterative Refinement with Self-Feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, S Gupta, B P Majumder, K Hermann, S Welleck, A Yazdanbakhsh, P Clark, arXiv:2303.176512023</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, B Labash, A Gopinath, K Narasimhan, S Yao, arXiv:2303.113662023</p>
<p>Self-edit: Fault-aware code editor for code generation. K Zhang, Z Li, J Li, G Li, Z Jin, arXiv:2305.040872023</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. L Pan, A Albalak, X Wang, W Y Wang, arXiv:2305.122952023</p>
<p>Read, revise, repeat: A system demonstration for human-in-the-loop iterative text revision. W Du, Z M Kim, V Raheja, D Kumar, D Kang, Proceedings of the First Workshop on Intelligent and Interactive Writing Assistants. the First Workshop on Intelligent and Interactive Writing AssistantsDublin, IrelandAssociation for Computational LinguisticsIn2Writing 2022. 2022</p>
<p>. 10.18653/v1/2022.in2writing-1.14</p>
<p>Improving language models via plug-and-play retrieval feedback. W Yu, Z Zhang, Z Liang, M Jiang, A Sabharwal, arXiv:2305.140022023</p>
<p>CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing. Z Gou, Z Shao, Y Gong, Y Shen, Y Yang, N Duan, W Chen, arXiv:2305.117382023</p>
<p>D Paul, M Ismayilzada, M Peyrard, B Borges, A Bosselut, R West, B Faltings, arXiv:2304.01904Refiner: Reasoning feedback on intermediate representations. 2023</p>
<p>Coderl: Mastering code generation through pretrained models and deep reinforcement learning. H Le, Y Wang, A D Gotmare, S Savarese, S C H Hoi, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>RL4F: Generating natural language feedback with reinforcement learning for repairing model outputs. A F Akyurek, E Akyurek, A Kalyan, P Clark, D T Wijaya, N Tandon, 10.18653/v1/2023.acl-long.427Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>S Gehman, S Gururangan, M Sap, Y Choi, N A Smith, arXiv:2009.11462Realtoxicityprompts: Evaluating neural toxic degeneration in language models. 2020arXiv preprint</p>
<p>AmbigQA: Answering ambiguous open-domain questions. S Min, J Michael, H Hajishirzi, L Zettlemoyer, 10.18653/v1/2020.emnlp-main.466Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online2020</p>
<p>Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. M Joshi, E Choi, D S Weld, L Zettlemoyer, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics2017</p>
<p>Hotpotqa: A dataset for diverse, explainable multi-hop question answering. Z Yang, P Qi, S Zhang, Y Bengio, W W Cohen, R Salakhutdinov, C D Manning, arXiv:1809.096002018</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 352022</p>
<p>W Du, Z M Kim, V Raheja, D Kumar, D Kang, Read, arXiv:2204.03685revise, repeat: A system demonstration for human-in-the-loop iterative text revision. 2022arXiv preprint</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in neural information processing systems. 352022</p>
<p>C Xu, Q Sun, K Zheng, X Geng, P Zhao, J Feng, C Tao, D Jiang, arXiv:2304.12244Wizardlm: Empowering large language models to follow complex instructions. 2023arXiv preprint</p>
<p>X Geng, A Gudibande, H Liu, E Wallace, P Abbeel, S Levine, D Song, Koala: A dialogue model for academic research. blog post. 2023</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. W.-L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, 14 April 2023. 2023See</p>
<p>Plug and play language models: A simple approach to controlled text generation. S Dathathri, A Madotto, J Lan, J Hung, E Frank, P Molino, J Yosinski, R Liu, arXiv:1912.021642019</p>
<p>GeDi: Generative discriminator guided sequence generation. B Krause, A D Gotmare, B Mccann, N S Keskar, S Joty, R Socher, N F Rajani, Findings of the Association for Computational Linguistics: EMNLP 2021. Punta Cana, Dominican RepublicAssociation for Computational Linguistics2021</p>
<p>. 10.18653/v1/2021.findings-emnlp.424</p>
<p>DExperts: Decodingtime controlled text generation with experts and anti-experts. A Liu, M Sap, X Lu, S Swayamdipta, C Bhagavatula, N A Smith, Y Choi, 10.18653/v1/2021.acl-long.522Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline20211</p>
<p>Don't stop pretraining: Adapt language models to domains and tasks. S Gururangan, A Marasovic´, S Swayamdipta, K Lo, I Beltagy, D Downey, N A Smith, 10.18653/v1/2020.acl-main.740Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational LinguisticsOnline2020</p>
<p>Quark: Controllable text generation with reinforced unlearning. X Lu, S Welleck, J Hessel, L Jiang, L Qin, P West, P Ammanabrolu, Y Choi, arXiv:2205.136362022</p>
<p>Generating sequences by learning to self-correct. S Welleck, X Lu, P West, F Brahman, T Shen, D Khashabi, Y Choi, arXiv:2211.000532022</p>
<p>Chain of thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, E H Chi, Q Le, D Zhou, arXiv:2201.119032022</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, arXiv:2210.036292023</p>            </div>
        </div>

    </div>
</body>
</html>
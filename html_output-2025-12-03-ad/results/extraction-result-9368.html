<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9368 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9368</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9368</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-643f3e79ca5a7effee96973a21af50a9dddeaf10</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/643f3e79ca5a7effee96973a21af50a9dddeaf10" target="_blank">What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A comprehensive benchmark is established containing 8 practical chemistry tasks, including 1) name prediction, 2) property prediction, 3) yield prediction, 4) reaction prediction, 5) retrosynthesis (prediction of reactants from products), 6) text-based molecule design, 7) molecule captioning, and 8) reagent selection to establish a broad exploration of the capacities of LLMs within the context of practical chemistry.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9368.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9368.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NamePrediction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chemical Name Prediction (SMILES/IUPAC/Formula translations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs were evaluated as text-based simulators to translate between molecular representations (SMILES, IUPAC names, molecular formulas) to test basic chemical understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5 (GPT-3.5-turbo), Davinci-003, Llama2-13B-chat, GAL-30B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Five large language models evaluated in this paper; GPT-4 is the top-performing OpenAI family model in experiments. The paper does not provide parameter counts or training corpora; models are evaluated via API/available releases.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Cheminformatics / Chemical nomenclature</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Translate molecular representations: SMILES -> IUPAC name, IUPAC name -> SMILES, SMILES -> molecular formula, IUPAC name -> molecular formula (exact-match generation tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Exact-match accuracy (fraction of outputs exactly matching ground truth).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Very low for LLMs: e.g., GPT-4: smiles2iupac = 0.0; iupac2smiles ≈ 0.014 ± 0.009; smiles2formula ≈ 0.086 ± 0.036; iupac2formula ≈ 0.118 ± 0.022. Baseline STOUT: smiles2iupac 0.55, iupac2smiles 0.70 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Quality/quantity of in-context examples provided (ICL helps but did not rescue these tasks); representation format (SMILES long strings are challenging); tokenization/subword effects; LLM internal pretraining bias toward SMILES formats present in web corpora is limited for name mapping; scaffold vs random ICL retrieval had limited impact for these very low-accuracy tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to specialized neural/translation baselines (STOUT) and domain-specific tools: STOUT vastly outperforms LLMs on name translation (STOUT 0.55–0.70 vs GPT-4 0–0.118).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>LLMs fail at exact structural/name translation; they miscount atoms, generate incorrect formulas, and produce chemically invalid or inconsistent outputs; inability to reliably parse or generate long SMILES/IUPAC strings; hallucinations in name/structure translation.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors suggest pretraining techniques (e.g., wrapping molecules with text) or code-switch strategies to better align chemical name representations and recommend integration with chemistry tools (e.g., RDKit) or chemistry-specific model adaptation to handle structured molecular representations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9368.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9368.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PropertyPrediction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Molecular Property Prediction (binary classification)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs used as text-based classifiers to predict binary molecular properties (e.g., BBBP, HIV activity, BACE binding, Tox21 toxicity, ClinTox) from SMILES input using prompts and in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5 (GPT-3.5-turbo), Davinci-003, Llama2-13B-chat, GAL-30B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same five LLMs evaluated; GPT-4 achieved the best results among them on property prediction tasks within this study.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Computational chemistry / drug discovery / toxicology prediction</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Predict binary properties (e.g., blood–brain barrier penetration, HIV inhibition, BACE binding, toxicity) from molecular SMILES via prompted classification (Yes/No)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy and F1 score (F1 used due to class imbalance).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>GPT-4 (Scaffold, k=8) F1: BBBP 0.587±0.018, BACE 0.666±0.023, HIV 0.797±0.021, Tox21 0.563±0.008, ClinTox 0.736±0.033. Accuracy (GPT-4 scaffold k=8): BBBP 0.614±0.016, BACE 0.679±0.205, HIV 0.836±0.020, Tox21 0.737±0.004, ClinTox 0.856±0.014 (Tables 6 and 7).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Prompt wording and label semantics (including the label meaning in prompts strongly improves performance); in-context learning (ICL) vs zero-shot (ICL better); ICL retrieval strategy (scaffold similarity > random for several datasets); number of ICL examples (larger k often improves results); temperature had marginal effect (small fluctuations seen).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to classical baselines (RF and XGBoost using molecular fingerprints). GPT-4 outperformed RF/XGBoost on several datasets in F1 and accuracy (selective competitiveness): e.g., GPT-4 F1 outperforms baselines on HIV and ClinTox in some prompt settings; however, baselines still strong on some datasets (XGBoost RF figures listed in Tables 6/7).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance depends strongly on prompt semantics (including label definitions); models can overfit to label text rather than molecular reasoning; variability across runs; some smaller/other LLMs (Llama, GAL) performed poorly; class imbalance handling required careful sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors emphasize including label semantics in prompts, using scaffold-similarity retrieval for ICL, increasing number of ICL examples when feasible, and designing chemistry-specific ICL strategies to improve performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9368.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9368.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>YieldPrediction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reaction Yield Prediction (binary high-yield classification)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs used to predict whether reactions are high-yielding (binary) on high-throughput experimental datasets (Buchwald–Hartwig and Suzuki–Miyaura), treating yields as classification via prompted examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5, Davinci-003, Llama2-13B-chat, GAL-30B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The evaluated LLMs; GPT-4 achieved the highest accuracy among the LLMs tested for yield prediction but remained below a specialized baseline trained on large reaction datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Synthetic chemistry / reaction optimization</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Predict whether a reaction will be high-yield given reactants, reagents, and conditions (binary classification) using few-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Classification accuracy (Top-1 binary accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>GPT-4 (random, k=8) accuracy: Buchwald–Hartwig 0.800 ± 0.008; Suzuki-coupling 0.764 ± 0.013 (Table 10). Zero-shot GPT-4 much lower (e.g., 0.322 and 0.214). Baseline UAGNN: Buchwald 0.965, Suzuki 0.957.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>In-context learning (number of examples k increases accuracy); ICL sampling strategy (random used here due to reaction uniformity); model choice (GPT-4 > GPT-3.5 > Davinci-003); dataset uniformity (HTE datasets with consistent experimental conditions are easier); extensive task-specific training (UAGNN trained on many examples outperforms LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to UAGNN (state-of-the-art GNN baseline trained on extensive reaction data). GPT-4 is substantially worse than UAGNN (~16–20% lower accuracy in these experiments), though GPT-4 improved markedly with more ICL examples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>LLMs underperform relative to models trained on thousands of task-specific examples; zero-shot performance is poor; results sensitive to ICL example count; lack of specialized reaction graph representations limits performance.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors note ICL can help but that specialized models trained on task data still outperform LLMs; suggest integrating domain-specific representations or more examples, and exploring chemistry-specific ICL and prompting strategies.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9368.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9368.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReactionPrediction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Forward Reaction Prediction (products from reactants/reagents)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs used to predict reaction products (SMILES) given reactants and reagents, testing generative SMILES output validity and top-1 prediction accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5, Davinci-003, Llama2-13B-chat, GAL-30B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same evaluated set of LLMs; GPT-4 yielded the best results among them but remained far below sequence-to-sequence models trained on reaction corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Synthetic organic chemistry / reaction outcome prediction</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate product SMILES from given reactant and reagent SMILES strings (sequence-to-sequence generation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Top-1 accuracy (exact product match) and percentage of invalid SMILES outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Chemformer baseline Top-1 accuracy 0.938 (0% invalid SMILES). GPT-4 (Scaffold, k=20) Top-1 accuracy 0.230 ± 0.022 with 7.0% ± 1.6% invalid SMILES; GPT-4 zero-shot accuracy 0.004 ± 0.005 with 17.4% ± 3.9% invalid SMILES (Table 11). Other LLMs had lower top-1 accuracies and varying invalid SMILES rates.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>ICL vs zero-shot (ICL and more examples improve accuracy); retrieval strategy (scaffold similarity > random in most tasks); number of ICL examples (larger k better); SMILES representation difficulty for tokenizers; model pretraining not specialized on reaction mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Chemformer (pretrained transformer trained on reaction corpora) dramatically outperforms GPT models (0.938 vs 0.230 for best GPT-4 setting); LLMs generate significant fractions of invalid SMILES unlike Chemformer.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>LLMs struggle with SMILES structure understanding, produce many chemically invalid SMILES particularly in zero-shot; poor generative accuracy compared to supervised seq2seq models trained on large reaction datasets; hallucinations and tokenization-induced errors.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors recommend coupling LLMs with cheminformatics tools (e.g., RDKit), chemistry-aware pretraining or fine-tuning, chemistry-specific prompting, and using scaffold-based ICL retrieval and more demonstration examples to partially mitigate issues.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9368.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9368.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReagentsSelection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reagent Selection (reactant/solvent/ligand recommendation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs used in selection/ranking tasks to recommend reaction components (reactant, solvent, ligand) from candidate lists for Suzuki HTE dataset, tested as a ranking/classification simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5, Davinci-003, Llama2-13B-chat, GAL-30B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>API-accessible LLMs evaluated in zero-shot and few-shot modes; GPT-4 and GPT-3.5 showed comparatively strong selection performance.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Synthetic chemistry / reaction optimization / experimental planning</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Select or rank reagents (reactant, solvent, ligand) for a given reaction from provided candidate lists; ligand selection evaluated as Top-50% accuracy since several ligands can be acceptable.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Top-1 Accuracy for reactant and solvent selection; Top-50% accuracy for ligand selection.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>GPT-4 (zero-shot): Reactant selection 0.299 ± 0.029, Solvent selection 0.526 ± 0.012, Ligand Top-50% selection 0.534 ± 0.059. GPT-3.5 (zero-shot) reactant selection 0.400 ± 0.038 (Table 12).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Prompt design and task description; zero-shot vs few-shot (here zero-shot results reported and few-shot can improve); model choice (GPT-3.5 and GPT-4 strong); nature of selection task (ranking simpler than generation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>No established baseline provided for reagent selection in this work. Authors note GPT models achieve ~40–50% accuracy in selecting components from candidate lists, considered competitive for this problem framing.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not all selection categories equally easy (reactant selection lower than solvent/ligand); performance depends on candidate set and prompt clarity; absence of task-tailored training may limit ceiling performance.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors consider selection/ranking tasks more naturally suited to LLMs than generative SMILES tasks and recommend further exploration with in-context learning and chemistry-specific retrieval to improve performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9368.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9368.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrosynthesis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrosynthetic Prediction (predict reactants from product)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs used to generate reactant SMILES from a target product SMILES (retrosynthesis) and evaluated for top-1 accuracy and validity compared to learned supervised models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5, Davinci-003, Llama2-13B-chat, GAL-30B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same evaluated LLMs; GPT-4 ranks near top among LLMs but remains inferior to specialized retrosynthesis models.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Synthetic organic chemistry / retrosynthetic planning</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate likely reactants (SMILES) for a given product (SMILES) — a generative structured-output task requiring chemical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Top-1 accuracy and SMILES validity (exact match and validity rates reported in paper Appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Paper reports GPT models perform substantially worse than specialized models, with GPT-4 ~40% lower accuracy than Chemformer on retrosynthesis (summary statement in main text; exact numeric table referenced as Table 13 in Appendix but not fully present in the main excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>SMILES representation difficulty for LLMs; in-context learning and retrieval methods (scaffold vs random) affect performance but do not close the gap; amount of demonstration examples; lack of task-specific training data used by specialized models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Chemformer (specialized seq2seq pretrained model) serves as baseline and outperforms LLMs by a large margin (retrosynthesis accuracy ~40% higher than GPT variants as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>LLMs generate chemically implausible reactants or invalid SMILES; inability to perform structured retrosynthetic reasoning at the level of supervised models trained on large reaction corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors note need for chemistry-aware pretraining, coupling LLMs to cheminformatics tools, and development of chemistry-specific prompting and ICL strategies to improve retrosynthesis performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9368.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9368.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TextMoleculeDesign</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-based Molecule Design (natural language -> molecule SMILES)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs used to generate molecule SMILES from textual descriptions (natural-language molecular design), evaluated on NLP-style and chemistry-validity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5, Davinci-003, Llama2-13B-chat, GAL-30B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>General-purpose LLMs evaluated for conditional molecular generation from text prompts; GPT-4 performed best among LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Molecular design / computational drug discovery / generative chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate candidate molecule SMILES that satisfy a textual specification or description.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>NLP metrics (BLEU, Levenshtein, ROUGE), chemical validity percentage, and Exact-Match (SMILES equality) accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>LLMs (GPT-4) outperform MolT5-Large on NLP metrics (BLEU, Levenshtein) but exact-match is low (<20% exact match); chemical validity of generated molecules reported >89% (majority chemically valid) (Table 14 summary in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Representation choice (SMILES preferred over SELFIES for these LLMs); prompt design and number of ICL examples (more examples improve generation quality); model capability (GPT-4 > others); evaluation metric mismatch (NLP metrics may not reflect chemical correctness or usefulness).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>MolT5-Large (specialized text-to-molecule model): GPT models better on NLP-style metrics, worse on exact-match; MolT5 has higher exact-match performance in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Low exact-match rates despite high NLP-metric scores; some generated molecules violate chemical facts; evaluating true utility (novelty, synthesizability) requires expensive follow-up analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors recommend chemistry-specific evaluation metrics, integrating chemical validity checks, and further research to assess practical utility of generated molecules beyond string-level similarity metrics.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9368.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9368.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoleculeCaptioning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Molecule Captioning / Explanation (SMILES -> textual description)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs used to generate human-readable textual captions/descriptions for molecules given SMILES or other representations, evaluated with NLP metrics and chemist qualitative evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5, Davinci-003, Llama2-13B-chat, GAL-30B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Evaluated LLMs for translating molecular structures into explanatory natural-language captions; GPT-4 outperformed other LLMs and beat MolT5-Large on some metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Cheminformatics / explainable chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate textual molecule captions/descriptions from SMILES or structure inputs; judged both by NLP metrics and by chemists for usefulness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>BLEU, Levenshtein, ROUGE, Fréchet ChemNet Distance (FCD), and human (chemist) qualitative evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>GPT-4 ranked best and showed competitive performance and better NLP-metric scores than MolT5-Large on captioning tasks (detailed scores in Table 15 in appendix; main text reports selective competitiveness). Exact-match low but language-quality metrics and chemist evaluations favorable.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>LLMs’ language generation strength benefits captioning; ICL and prompt templates improve quality; SMILES vs SELFIES representation affects quality (SMILES superior for LLMs pretrained on SMILES-rich corpora); hallucination risk where generated textual statements can violate chemical facts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>MolT5-Large (specialized molecule-text translation model); GPT-4 outperforms MolT5 on many NLP-style metrics, though exact string-level matches remain low.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Generated captions can contain chemical inaccuracies (hallucinations); existing NLP metrics may not capture domain correctness; exact-match is not meaningful for captioning but factual correctness is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors suggest developing chemistry-specific evaluation metrics and measures of factual correctness, using chemist evaluation, and applying CoT or decomposed prompting to reduce factual errors.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chemformer: a pretrained transformer for computational chemistry <em>(Rating: 2)</em></li>
                <li>Translation between molecules and natural language <em>(Rating: 2)</em></li>
                <li>Uncertainty-aware prediction of chemical reaction yields with graph neural networks <em>(Rating: 2)</em></li>
                <li>Chemcrow: Augmenting large-language models with chemistry tools <em>(Rating: 2)</em></li>
                <li>Emergent autonomous scientific research capabilities of large language models <em>(Rating: 2)</em></li>
                <li>SELFIES: A 100% robust molecular string representation <em>(Rating: 1)</em></li>
                <li>Is gpt-3 all you need for low-data discovery in chemistry <em>(Rating: 1)</em></li>
                <li>GPT-4 technical report <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9368",
    "paper_id": "paper-643f3e79ca5a7effee96973a21af50a9dddeaf10",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "NamePrediction",
            "name_full": "Chemical Name Prediction (SMILES/IUPAC/Formula translations)",
            "brief_description": "LLMs were evaluated as text-based simulators to translate between molecular representations (SMILES, IUPAC names, molecular formulas) to test basic chemical understanding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5 (GPT-3.5-turbo), Davinci-003, Llama2-13B-chat, GAL-30B",
            "model_description": "Five large language models evaluated in this paper; GPT-4 is the top-performing OpenAI family model in experiments. The paper does not provide parameter counts or training corpora; models are evaluated via API/available releases.",
            "scientific_subdomain": "Cheminformatics / Chemical nomenclature",
            "simulation_task": "Translate molecular representations: SMILES -&gt; IUPAC name, IUPAC name -&gt; SMILES, SMILES -&gt; molecular formula, IUPAC name -&gt; molecular formula (exact-match generation tasks).",
            "evaluation_metric": "Exact-match accuracy (fraction of outputs exactly matching ground truth).",
            "simulation_accuracy": "Very low for LLMs: e.g., GPT-4: smiles2iupac = 0.0; iupac2smiles ≈ 0.014 ± 0.009; smiles2formula ≈ 0.086 ± 0.036; iupac2formula ≈ 0.118 ± 0.022. Baseline STOUT: smiles2iupac 0.55, iupac2smiles 0.70 (Table 4).",
            "factors_affecting_accuracy": "Quality/quantity of in-context examples provided (ICL helps but did not rescue these tasks); representation format (SMILES long strings are challenging); tokenization/subword effects; LLM internal pretraining bias toward SMILES formats present in web corpora is limited for name mapping; scaffold vs random ICL retrieval had limited impact for these very low-accuracy tasks.",
            "comparison_baseline": "Compared to specialized neural/translation baselines (STOUT) and domain-specific tools: STOUT vastly outperforms LLMs on name translation (STOUT 0.55–0.70 vs GPT-4 0–0.118).",
            "limitations_or_failure_cases": "LLMs fail at exact structural/name translation; they miscount atoms, generate incorrect formulas, and produce chemically invalid or inconsistent outputs; inability to reliably parse or generate long SMILES/IUPAC strings; hallucinations in name/structure translation.",
            "author_recommendations_or_insights": "Authors suggest pretraining techniques (e.g., wrapping molecules with text) or code-switch strategies to better align chemical name representations and recommend integration with chemistry tools (e.g., RDKit) or chemistry-specific model adaptation to handle structured molecular representations.",
            "uuid": "e9368.0"
        },
        {
            "name_short": "PropertyPrediction",
            "name_full": "Molecular Property Prediction (binary classification)",
            "brief_description": "LLMs used as text-based classifiers to predict binary molecular properties (e.g., BBBP, HIV activity, BACE binding, Tox21 toxicity, ClinTox) from SMILES input using prompts and in-context learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5 (GPT-3.5-turbo), Davinci-003, Llama2-13B-chat, GAL-30B",
            "model_description": "Same five LLMs evaluated; GPT-4 achieved the best results among them on property prediction tasks within this study.",
            "scientific_subdomain": "Computational chemistry / drug discovery / toxicology prediction",
            "simulation_task": "Predict binary properties (e.g., blood–brain barrier penetration, HIV inhibition, BACE binding, toxicity) from molecular SMILES via prompted classification (Yes/No)",
            "evaluation_metric": "Accuracy and F1 score (F1 used due to class imbalance).",
            "simulation_accuracy": "GPT-4 (Scaffold, k=8) F1: BBBP 0.587±0.018, BACE 0.666±0.023, HIV 0.797±0.021, Tox21 0.563±0.008, ClinTox 0.736±0.033. Accuracy (GPT-4 scaffold k=8): BBBP 0.614±0.016, BACE 0.679±0.205, HIV 0.836±0.020, Tox21 0.737±0.004, ClinTox 0.856±0.014 (Tables 6 and 7).",
            "factors_affecting_accuracy": "Prompt wording and label semantics (including the label meaning in prompts strongly improves performance); in-context learning (ICL) vs zero-shot (ICL better); ICL retrieval strategy (scaffold similarity &gt; random for several datasets); number of ICL examples (larger k often improves results); temperature had marginal effect (small fluctuations seen).",
            "comparison_baseline": "Compared to classical baselines (RF and XGBoost using molecular fingerprints). GPT-4 outperformed RF/XGBoost on several datasets in F1 and accuracy (selective competitiveness): e.g., GPT-4 F1 outperforms baselines on HIV and ClinTox in some prompt settings; however, baselines still strong on some datasets (XGBoost RF figures listed in Tables 6/7).",
            "limitations_or_failure_cases": "Performance depends strongly on prompt semantics (including label definitions); models can overfit to label text rather than molecular reasoning; variability across runs; some smaller/other LLMs (Llama, GAL) performed poorly; class imbalance handling required careful sampling.",
            "author_recommendations_or_insights": "Authors emphasize including label semantics in prompts, using scaffold-similarity retrieval for ICL, increasing number of ICL examples when feasible, and designing chemistry-specific ICL strategies to improve performance.",
            "uuid": "e9368.1"
        },
        {
            "name_short": "YieldPrediction",
            "name_full": "Reaction Yield Prediction (binary high-yield classification)",
            "brief_description": "LLMs used to predict whether reactions are high-yielding (binary) on high-throughput experimental datasets (Buchwald–Hartwig and Suzuki–Miyaura), treating yields as classification via prompted examples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5, Davinci-003, Llama2-13B-chat, GAL-30B",
            "model_description": "The evaluated LLMs; GPT-4 achieved the highest accuracy among the LLMs tested for yield prediction but remained below a specialized baseline trained on large reaction datasets.",
            "scientific_subdomain": "Synthetic chemistry / reaction optimization",
            "simulation_task": "Predict whether a reaction will be high-yield given reactants, reagents, and conditions (binary classification) using few-shot prompts.",
            "evaluation_metric": "Classification accuracy (Top-1 binary accuracy).",
            "simulation_accuracy": "GPT-4 (random, k=8) accuracy: Buchwald–Hartwig 0.800 ± 0.008; Suzuki-coupling 0.764 ± 0.013 (Table 10). Zero-shot GPT-4 much lower (e.g., 0.322 and 0.214). Baseline UAGNN: Buchwald 0.965, Suzuki 0.957.",
            "factors_affecting_accuracy": "In-context learning (number of examples k increases accuracy); ICL sampling strategy (random used here due to reaction uniformity); model choice (GPT-4 &gt; GPT-3.5 &gt; Davinci-003); dataset uniformity (HTE datasets with consistent experimental conditions are easier); extensive task-specific training (UAGNN trained on many examples outperforms LLMs).",
            "comparison_baseline": "Compared to UAGNN (state-of-the-art GNN baseline trained on extensive reaction data). GPT-4 is substantially worse than UAGNN (~16–20% lower accuracy in these experiments), though GPT-4 improved markedly with more ICL examples.",
            "limitations_or_failure_cases": "LLMs underperform relative to models trained on thousands of task-specific examples; zero-shot performance is poor; results sensitive to ICL example count; lack of specialized reaction graph representations limits performance.",
            "author_recommendations_or_insights": "Authors note ICL can help but that specialized models trained on task data still outperform LLMs; suggest integrating domain-specific representations or more examples, and exploring chemistry-specific ICL and prompting strategies.",
            "uuid": "e9368.2"
        },
        {
            "name_short": "ReactionPrediction",
            "name_full": "Forward Reaction Prediction (products from reactants/reagents)",
            "brief_description": "LLMs used to predict reaction products (SMILES) given reactants and reagents, testing generative SMILES output validity and top-1 prediction accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5, Davinci-003, Llama2-13B-chat, GAL-30B",
            "model_description": "Same evaluated set of LLMs; GPT-4 yielded the best results among them but remained far below sequence-to-sequence models trained on reaction corpora.",
            "scientific_subdomain": "Synthetic organic chemistry / reaction outcome prediction",
            "simulation_task": "Generate product SMILES from given reactant and reagent SMILES strings (sequence-to-sequence generation).",
            "evaluation_metric": "Top-1 accuracy (exact product match) and percentage of invalid SMILES outputs.",
            "simulation_accuracy": "Chemformer baseline Top-1 accuracy 0.938 (0% invalid SMILES). GPT-4 (Scaffold, k=20) Top-1 accuracy 0.230 ± 0.022 with 7.0% ± 1.6% invalid SMILES; GPT-4 zero-shot accuracy 0.004 ± 0.005 with 17.4% ± 3.9% invalid SMILES (Table 11). Other LLMs had lower top-1 accuracies and varying invalid SMILES rates.",
            "factors_affecting_accuracy": "ICL vs zero-shot (ICL and more examples improve accuracy); retrieval strategy (scaffold similarity &gt; random in most tasks); number of ICL examples (larger k better); SMILES representation difficulty for tokenizers; model pretraining not specialized on reaction mapping.",
            "comparison_baseline": "Chemformer (pretrained transformer trained on reaction corpora) dramatically outperforms GPT models (0.938 vs 0.230 for best GPT-4 setting); LLMs generate significant fractions of invalid SMILES unlike Chemformer.",
            "limitations_or_failure_cases": "LLMs struggle with SMILES structure understanding, produce many chemically invalid SMILES particularly in zero-shot; poor generative accuracy compared to supervised seq2seq models trained on large reaction datasets; hallucinations and tokenization-induced errors.",
            "author_recommendations_or_insights": "Authors recommend coupling LLMs with cheminformatics tools (e.g., RDKit), chemistry-aware pretraining or fine-tuning, chemistry-specific prompting, and using scaffold-based ICL retrieval and more demonstration examples to partially mitigate issues.",
            "uuid": "e9368.3"
        },
        {
            "name_short": "ReagentsSelection",
            "name_full": "Reagent Selection (reactant/solvent/ligand recommendation)",
            "brief_description": "LLMs used in selection/ranking tasks to recommend reaction components (reactant, solvent, ligand) from candidate lists for Suzuki HTE dataset, tested as a ranking/classification simulator.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5, Davinci-003, Llama2-13B-chat, GAL-30B",
            "model_description": "API-accessible LLMs evaluated in zero-shot and few-shot modes; GPT-4 and GPT-3.5 showed comparatively strong selection performance.",
            "scientific_subdomain": "Synthetic chemistry / reaction optimization / experimental planning",
            "simulation_task": "Select or rank reagents (reactant, solvent, ligand) for a given reaction from provided candidate lists; ligand selection evaluated as Top-50% accuracy since several ligands can be acceptable.",
            "evaluation_metric": "Top-1 Accuracy for reactant and solvent selection; Top-50% accuracy for ligand selection.",
            "simulation_accuracy": "GPT-4 (zero-shot): Reactant selection 0.299 ± 0.029, Solvent selection 0.526 ± 0.012, Ligand Top-50% selection 0.534 ± 0.059. GPT-3.5 (zero-shot) reactant selection 0.400 ± 0.038 (Table 12).",
            "factors_affecting_accuracy": "Prompt design and task description; zero-shot vs few-shot (here zero-shot results reported and few-shot can improve); model choice (GPT-3.5 and GPT-4 strong); nature of selection task (ranking simpler than generation).",
            "comparison_baseline": "No established baseline provided for reagent selection in this work. Authors note GPT models achieve ~40–50% accuracy in selecting components from candidate lists, considered competitive for this problem framing.",
            "limitations_or_failure_cases": "Not all selection categories equally easy (reactant selection lower than solvent/ligand); performance depends on candidate set and prompt clarity; absence of task-tailored training may limit ceiling performance.",
            "author_recommendations_or_insights": "Authors consider selection/ranking tasks more naturally suited to LLMs than generative SMILES tasks and recommend further exploration with in-context learning and chemistry-specific retrieval to improve performance.",
            "uuid": "e9368.4"
        },
        {
            "name_short": "Retrosynthesis",
            "name_full": "Retrosynthetic Prediction (predict reactants from product)",
            "brief_description": "LLMs used to generate reactant SMILES from a target product SMILES (retrosynthesis) and evaluated for top-1 accuracy and validity compared to learned supervised models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5, Davinci-003, Llama2-13B-chat, GAL-30B",
            "model_description": "Same evaluated LLMs; GPT-4 ranks near top among LLMs but remains inferior to specialized retrosynthesis models.",
            "scientific_subdomain": "Synthetic organic chemistry / retrosynthetic planning",
            "simulation_task": "Generate likely reactants (SMILES) for a given product (SMILES) — a generative structured-output task requiring chemical reasoning.",
            "evaluation_metric": "Top-1 accuracy and SMILES validity (exact match and validity rates reported in paper Appendix).",
            "simulation_accuracy": "Paper reports GPT models perform substantially worse than specialized models, with GPT-4 ~40% lower accuracy than Chemformer on retrosynthesis (summary statement in main text; exact numeric table referenced as Table 13 in Appendix but not fully present in the main excerpt).",
            "factors_affecting_accuracy": "SMILES representation difficulty for LLMs; in-context learning and retrieval methods (scaffold vs random) affect performance but do not close the gap; amount of demonstration examples; lack of task-specific training data used by specialized models.",
            "comparison_baseline": "Chemformer (specialized seq2seq pretrained model) serves as baseline and outperforms LLMs by a large margin (retrosynthesis accuracy ~40% higher than GPT variants as reported).",
            "limitations_or_failure_cases": "LLMs generate chemically implausible reactants or invalid SMILES; inability to perform structured retrosynthetic reasoning at the level of supervised models trained on large reaction corpora.",
            "author_recommendations_or_insights": "Authors note need for chemistry-aware pretraining, coupling LLMs to cheminformatics tools, and development of chemistry-specific prompting and ICL strategies to improve retrosynthesis performance.",
            "uuid": "e9368.5"
        },
        {
            "name_short": "TextMoleculeDesign",
            "name_full": "Text-based Molecule Design (natural language -&gt; molecule SMILES)",
            "brief_description": "LLMs used to generate molecule SMILES from textual descriptions (natural-language molecular design), evaluated on NLP-style and chemistry-validity metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5, Davinci-003, Llama2-13B-chat, GAL-30B",
            "model_description": "General-purpose LLMs evaluated for conditional molecular generation from text prompts; GPT-4 performed best among LLMs.",
            "scientific_subdomain": "Molecular design / computational drug discovery / generative chemistry",
            "simulation_task": "Generate candidate molecule SMILES that satisfy a textual specification or description.",
            "evaluation_metric": "NLP metrics (BLEU, Levenshtein, ROUGE), chemical validity percentage, and Exact-Match (SMILES equality) accuracy.",
            "simulation_accuracy": "LLMs (GPT-4) outperform MolT5-Large on NLP metrics (BLEU, Levenshtein) but exact-match is low (&lt;20% exact match); chemical validity of generated molecules reported &gt;89% (majority chemically valid) (Table 14 summary in main text).",
            "factors_affecting_accuracy": "Representation choice (SMILES preferred over SELFIES for these LLMs); prompt design and number of ICL examples (more examples improve generation quality); model capability (GPT-4 &gt; others); evaluation metric mismatch (NLP metrics may not reflect chemical correctness or usefulness).",
            "comparison_baseline": "MolT5-Large (specialized text-to-molecule model): GPT models better on NLP-style metrics, worse on exact-match; MolT5 has higher exact-match performance in some cases.",
            "limitations_or_failure_cases": "Low exact-match rates despite high NLP-metric scores; some generated molecules violate chemical facts; evaluating true utility (novelty, synthesizability) requires expensive follow-up analysis.",
            "author_recommendations_or_insights": "Authors recommend chemistry-specific evaluation metrics, integrating chemical validity checks, and further research to assess practical utility of generated molecules beyond string-level similarity metrics.",
            "uuid": "e9368.6"
        },
        {
            "name_short": "MoleculeCaptioning",
            "name_full": "Molecule Captioning / Explanation (SMILES -&gt; textual description)",
            "brief_description": "LLMs used to generate human-readable textual captions/descriptions for molecules given SMILES or other representations, evaluated with NLP metrics and chemist qualitative evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5, Davinci-003, Llama2-13B-chat, GAL-30B",
            "model_description": "Evaluated LLMs for translating molecular structures into explanatory natural-language captions; GPT-4 outperformed other LLMs and beat MolT5-Large on some metrics.",
            "scientific_subdomain": "Cheminformatics / explainable chemistry",
            "simulation_task": "Generate textual molecule captions/descriptions from SMILES or structure inputs; judged both by NLP metrics and by chemists for usefulness.",
            "evaluation_metric": "BLEU, Levenshtein, ROUGE, Fréchet ChemNet Distance (FCD), and human (chemist) qualitative evaluation.",
            "simulation_accuracy": "GPT-4 ranked best and showed competitive performance and better NLP-metric scores than MolT5-Large on captioning tasks (detailed scores in Table 15 in appendix; main text reports selective competitiveness). Exact-match low but language-quality metrics and chemist evaluations favorable.",
            "factors_affecting_accuracy": "LLMs’ language generation strength benefits captioning; ICL and prompt templates improve quality; SMILES vs SELFIES representation affects quality (SMILES superior for LLMs pretrained on SMILES-rich corpora); hallucination risk where generated textual statements can violate chemical facts.",
            "comparison_baseline": "MolT5-Large (specialized molecule-text translation model); GPT-4 outperforms MolT5 on many NLP-style metrics, though exact string-level matches remain low.",
            "limitations_or_failure_cases": "Generated captions can contain chemical inaccuracies (hallucinations); existing NLP metrics may not capture domain correctness; exact-match is not meaningful for captioning but factual correctness is critical.",
            "author_recommendations_or_insights": "Authors suggest developing chemistry-specific evaluation metrics and measures of factual correctness, using chemist evaluation, and applying CoT or decomposed prompting to reduce factual errors.",
            "uuid": "e9368.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chemformer: a pretrained transformer for computational chemistry",
            "rating": 2
        },
        {
            "paper_title": "Translation between molecules and natural language",
            "rating": 2
        },
        {
            "paper_title": "Uncertainty-aware prediction of chemical reaction yields with graph neural networks",
            "rating": 2
        },
        {
            "paper_title": "Chemcrow: Augmenting large-language models with chemistry tools",
            "rating": 2
        },
        {
            "paper_title": "Emergent autonomous scientific research capabilities of large language models",
            "rating": 2
        },
        {
            "paper_title": "SELFIES: A 100% robust molecular string representation",
            "rating": 1
        },
        {
            "paper_title": "Is gpt-3 all you need for low-data discovery in chemistry",
            "rating": 1
        },
        {
            "paper_title": "GPT-4 technical report",
            "rating": 1
        }
    ],
    "cost": 0.019164749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks</h1>
<p>Taicheng Guo, ${ }^{\dagger}$ Kehan Guo, ${ }^{*}$ Bozhao Nan, Zhenwen Liang, Zhichun Guo,<br>Nitesh V. Chawla, Olaf Wiest, Xiangliang Zhang ${ }^{\dagger}$<br>University of Notre Dame<br>{tguo2, kguo2, bnan, zliang6, zguo5, nchawla, owiest, xzhang33}@nd.edu</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) with strong abilities in natural language processing tasks have emerged and have been applied in various kinds of areas such as science, finance and software engineering. However, the capability of LLMs to advance the field of chemistry remains unclear. In this paper, rather than pursuing state-of-the-art performance, we aim to evaluate capabilities of LLMs in a wide range of tasks across the chemistry domain. We identify three key chemistryrelated capabilities including understanding, reasoning and explaining to explore in LLMs and establish a benchmark containing eight chemistry tasks. Our analysis draws on widely recognized datasets facilitating a broad exploration of the capacities of LLMs within the context of practical chemistry. Five LLMs (GPT-4, GPT-3.5, Davinci-003, Llama and Galactica) are evaluated for each chemistry task in zero-shot and few-shot in-context learning settings with carefully selected demonstration examples and specially crafted prompts. Our investigation found that GPT-4 outperformed other models and LLMs exhibit different competitive levels in eight chemistry tasks. In addition to the key findings from the comprehensive benchmark analysis, our work provides insights into the limitation of current LLMs and the impact of in-context learning settings on LLMs' performance across various chemistry tasks. The code and datasets used in this study are available at https://github.com/ChemFoundationModels/ChemLLMBench.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have recently demonstrated impressive reasoning abilities across a wide array of tasks. These tasks are not limited to natural language processing, but also extend to various language-related applications within scientific domains [56, 30, 24, 10]. Much of the research on the capacity of LLMs in science has been focused on tasks such as answering medical [30] and scientific questions [24, 25]. However, the exploration of their application to practical tasks in the field of chemistry remains underinvestigated. Although some studies [6, 27, 63, 48] have been conducted, they tend to focus on specific case studies rather than a comprehensive or systematic evaluation. The exploration of LLMs' capabilities within the field of chemistry has the potential to revolutionize this domain and expedite research and development activities [62]. Thus, the question, "What can LLMs do in chemistry?" is a compelling topic of inquiry for both AI researchers and chemists. Nevertheless, there exist two challenges that hinder the answer to the topic and the further development of LLMs in chemistry:</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<ul>
<li>Determining the potential capabilities of LLMs in chemistry requires a systematic analysis of both LLMs and the specific requirements of chemistry tasks. There are different kinds of tasks in chemistry, some of which can be formulated to tasks solved by LLMs while others may not. It is necessary to consider the specific knowledge and reasoning required for each task and assess whether LLMs can effectively acquire and utilize that knowledge.</li>
<li>Conducting reliable and wide-ranging evaluation requires diverse experimental settings and limitations, that is, careful consideration and standardization of evaluation procedures, dataset curation, prompt design, and in-context learning strategies. Additionally, the API call time consumption and the randomness of LLMs limit the size of the testing.</li>
</ul>
<p>To address this knowledge gap, we (a group of AI researchers and chemists) have developed a comprehensive benchmark to provide a preliminary investigation into the abilities of LLMs across a diverse range of practical chemistry tasks. Our aim is to gain insights that will be beneficial to both AI researchers and chemists to advance the application of LLMs in chemistry. For AI researchers, we provide insights into the strengths, weaknesses, and limitations of LLMs in chemistry-related tasks, which can inform the further development and refinement of different AI techniques for more effective applications within the field. For chemists, our study provides a better understanding of the tasks in which they can rely on current LLMs. Utilizing our more extensive experimental setup, a broader range of chemistry tasks can be explored to further evaluate the capabilities of LLMs.</p>
<p>Our investigation focuses on 8 practical chemistry tasks, covering a diverse spectrum of the chemistry domain. These include: 1) name prediction, 2) property prediction, 3) yield prediction, 4) reaction prediction, 5) retrosynthesis (prediction of reactants from products), 6) text-based molecule design, 7) molecule captioning, and 8) reagents selection. Our analysis draws on widely available datasets including BBBP, Tox21 [65], PubChem [32], USPTO [29, 53, 39], and ChEBI [17, 16]. Five LLMs (GPT-4, GPT-3.5, Davinci-003, Llama, and Galactica) [43] are evaluated for each chemistry task in zero-shot and few-shot in-context learning settings with carefully selected demonstration examples and specific prompts. We highlight the contributions of this paper as follows:</p>
<ul>
<li>We are the first to establish a comprehensive benchmark to evaluate the abilities of LLMs on a wide range of chemistry tasks. These eight selected tasks, in consultation with chemists, not only encompass a diverse spectrum of the chemistry domain but also demand different abilities such as understanding, reasoning, and explaining using domain-specific chemistry knowledge.</li>
<li>We provide a comprehensive experimental framework for testing LLMs in chemistry tasks. To factor in the impact of prompts and demonstration examples in in-context learning, we have assessed multiple input options, focusing on the description of chemistry tasks. Five representative configurations were chosen based on their performance on a validation set, then these selected options were applied on the testing set. The conclusion is made from five repeated evaluations on each task, since GPTs often yield different outputs at different API calls even though the input is the same. We thus believe that our benchmarking process is both reliable and systematic.</li>
<li>Our investigations yield broader insights into the performance of LLMs on chemistry tasks. As summarized in Table 2, our findings confirm some anticipated outcomes (e.g., GPT-4 outperforms GPT-3 and Davinci-003), and also reveal unexpected discoveries (e.g., property prediction can be better solved when property label semantics are included in prompts). Our work also contributes to practical recommendations that can guide AI researchers and chemists in leveraging LLMs more effectively in the future (see Section 5).
The paper is organized as follows. Related works are presented in Section 2. In section 3, we elaborate on the evaluation process, including an overview of the chemistry tasks, the utilized LLMs and prompts, and the validation and testing settings. In section 4, we summarize the main findings (due to the space limit, evaluation details of each chemistry task can be found in Appendix). Finally, to answer the question "What can LLMs do in chemistry?" we discuss the constraints inherent to LLMs and how different settings related to LLMs affect performance across various chemistry tasks in Section 5. The conclusions are summarized in section 6.</li>
</ul>
<h1>2 Related Work</h1>
<p>Large Language Models. The rise of Large Language Models (LLMs) has marked a significant trend in recent natural language processing (NLP) research. This progress has been fuelled by milestones such as the introduction of GPT-3 [4], T0 [52], Flan-T5 [12], Galactica [56] and LLaMa [57]. The</p>
<p>recently released GPT-4, an evolution from GPT-3.5 series, has drawn considerable attention for its improvements in language understanding, generation, and planning [43]. Despite the vast potential of LLMs, existing research primarily centers on their performance within general NLP tasks [8, 9]. The scientific disciplines, notably chemistry, have received less focus. The application of LLMs in these specialized domains presents an opportunity for significant advancements. Therefore, we conduct a comprehensive experimental analysis to evaluate the capability of LLMs in chemistry-related tasks.</p>
<p>Large Language Model Evaluations. In recent years, the evaluation of LLMs like GPT has become a significant field of inquiry. [11] showed ChatGPT’s proficiency in law exams, while technical aspects of GPT-4 were analyzed in [43]. LLMs are also applied in healthcare [14], mathematical problem [18], and code generation tasks [37]. Specifically, in healthcare, the utility and safety of LLMs in clinical settings were explored [42]. In the context of mathematical problem-solving, studies [18, 7] have highlighted that LLMs encounter challenges with graduate-level problems, primarily due to difficulties in parsing complex syntax. These studies underscored the complexity of achieving task-specific accuracy and functionality with LLMs. Lastly, AGIEval [66] assessed LLMs’ general abilities but noted struggles in complex reasoning tasks.</p>
<p>Our work aligns with these evaluations but diverges in its focus on chemical tasks. To our knowledge, this is the first study to transform such tasks to suit LLM processing and to perform a comprehensive evaluation of these models’ ability to tackle chemistry-related problems. This focus will contribute to expand our understanding of LLMs’ capabilities in specific scientific domains.</p>
<p>Large Language Model for Chemistry. Recent efforts integrating LLMs with the field of chemistry generally fall into two distinct categories. One category aims to create a chemistry agent with LLMs’ by leveraging its planning ability to utilize task-related tools. For example, Bran et al [3] developed ChemCrow, which augmented LLMs with chem-expert designed tools for downstream tasks such as organic synthesis and drug discovery. Similarly, by leveraging the planning and execution ability of multiple LLMs, Boiko et al [2] developed an autonomous chemical agent to conduct chemical experiments. The other category involves direct usage of LLMs for downstream tasks in chemistry [27, 62, 6, 28]. While these studies have explored the performance of LLMs in chemistry-related tasks, a systematic evaluation of their capabilities within this domain has been lacking. Consequently, there is a noticeable gap that calls for a meticulous benchmark to thoroughly assess the potential of LLMs in chemistry. Such a benchmark is crucial not only for identifying the strengths and limitations of these models in a specialized scientific domain, but also to guide future improvements and applications.</p>
<h1>3 The Evaluation Process and Setting</h1>
<p>The evaluation process workflow is depicted in Fig. 1. Guided by co-author Prof. Olaf Wiest (from the Department of Chemistry at the University of Notre Dame), we identify eight tasks in discussion with senior Ph.D. students at the NSF Center for Computer Assisted Synthesis (C-CAS). Following this, we generate, assess, and choose suitable prompts to forward to LLMs. The acquired answers are then evaluated both qualitatively by chemists to identify whether they are helpful in the real-world scenario and quantitatively by selected metrics.</p>
<p>Chemistry tasks. In order to explore the abilities of LLMs in the field of chemistry, we concentrate on three fundamental capabilities: understanding, reasoning, and explaining. We examine these competencies through eight diverse and broadly acknowledged practical chemistry tasks. These tasks are summarized in Table 1, in terms of the task type from the perspective of machine learning, the dataset used for the evaluation, as well as the evaluation metrics. The #ICL candidates refers to the number of candidate examples, from which we select $k$ demonstration examples, either randomly or based on similarity searches. These candidate sets are the training sets used in classical machine learning models, e.g., in training classifiers or generative models. We set the test set of 100 instances, randomly sampled from the original testing dataset (non-overlapping with the training set). To reduce the influence of the LLMs randomness on the results, each evaluation experiment is repeated five times and the mean and variance are reported.</p>
<p>LLMs. For all tasks, we evaluate the performance of five popular LLMs: GPT-4, GPT-3.5 (referred to as GPT-3.5-turbo, also known as ChatGPT), Davinci-003, LLama and Galactica.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of the evaluation process</p>
<p>Table 1: The statistics of all tasks, datasets, the number of ICL/test samples, and evaluation metrics</p>
<table>
<thead>
<tr>
<th>Ability</th>
<th>Task</th>
<th>Task Type</th>
<th>Dataset</th>
<th>#ICL candidates</th>
<th>#test</th>
<th>Evaluation Metrics</th>
</tr>
</thead>
<tbody>
<tr>
<td>Understanding</td>
<td>Name Prediction</td>
<td>Generation</td>
<td>PubChem</td>
<td>500</td>
<td>100</td>
<td>Accuracy</td>
</tr>
<tr>
<td></td>
<td>Property Prediction</td>
<td>Classification</td>
<td>BBBP, HIV, BACE, Tox21, ClinTox</td>
<td>2053, 41127, 1514, 8014, 1484</td>
<td>100</td>
<td>Accuracy, F1 score</td>
</tr>
<tr>
<td></td>
<td>Yield Prediction</td>
<td>Classification</td>
<td>Buchwald-Hartwig, Suzuki-Miyaura</td>
<td>3957, 5650</td>
<td>100</td>
<td>Accuracy</td>
</tr>
<tr>
<td>Reasoning</td>
<td>Reaction Prediction</td>
<td>Generation</td>
<td>USPTO-Mixed</td>
<td>409035</td>
<td>100</td>
<td>Accuracy, Validity</td>
</tr>
<tr>
<td></td>
<td>Reagents Selection</td>
<td>Ranking</td>
<td>Suzuki-Miyaura</td>
<td>5760</td>
<td>100</td>
<td>Accuracy</td>
</tr>
<tr>
<td></td>
<td>Retrosynthesis</td>
<td>Generation</td>
<td>USPTO-50k</td>
<td>40029</td>
<td>100</td>
<td>Accuracy, Validity</td>
</tr>
<tr>
<td></td>
<td>Text-Based Molecule Design</td>
<td>Generation</td>
<td>ChEBI-20</td>
<td>26407</td>
<td>100</td>
<td>BLEU, Exact Match, etc</td>
</tr>
<tr>
<td>Explaining</td>
<td>Molecule Captioning</td>
<td>Generation</td>
<td>ChEBI-20</td>
<td>26407</td>
<td>100</td>
<td>BLEU, Chemists, etc</td>
</tr>
</tbody>
</table>
<p><strong>Zero-shot prompt.</strong> For each task, we apply a standardized zero-shot prompt template. As shown in Fig. 2, we instruct the LLMs to act in the capacity of a chemist. The content within the brackets is tailored to each task, adapting to its specific inputs and outputs. The responses from LLMs are confined to only returning the desired output without any explanations.</p>
<p><em>You are an expert chemist. Given the [reactants SMILES / molecular description / ...]: [Input], predict the [reaction product SMILES / molecule SMILES / ...] using your experienced chemical [reaction prediction / chemical molecule design / ...] knowledge. No explanations and other information. Only return the [product SMILES / designed molecular SMILES].</em></p>
<p>Figure 2: The standardized zero-shot prompt template for all tasks.</p>
<p><strong>Task-specific ICL prompt.</strong> ICL is a new paradigm for LLMs where predictions are based solely on contexts enriched with a few demonstration examples [15]. This paper specifically denotes ICL as a few-shot in-context learning approach, excluding the zero-shot paradigm. In order to thoroughly examine the capacities of LLMs within each chemistry-specific task, we design a task-specific ICL prompt template. As shown in Fig. 3, the format of the template is similar to that used in [48]. We also partition our template into four parts: {General Template}{ {Task-Specific Template} {ICL} {Question}. The {General Template} is almost the same as the zero-shot prompt, instructing the LLMs to play the role of a chemist and specify the chemistry task with its corresponding input and output. Considering that the responses for chemistry-related tasks must be accurate and chemically reasonable, it is crucial to prevent LLMs from generating hallucinated information. To this end, we introduce the {Task-Specific Template} which consists of three main components: {Input explanation}, {Output explanation}, and {Output Restrictions}, specifically designed to reduce hallucinations. These components are tailored to each task. The {ICL} part is a straightforward</p>
<p>concatenation of the demonstration examples and it follows the structure "[Input]: [Input_content] [Output]: [Output_content]". The [Input] and [Output] denote the specific names of each task’s input and output, respectively. For example, in the reaction prediction task, the [Input] would be "Reactants+Reagents" and the [Input_content] would be the actual SMILES of reactants and reagents. The [Output] would be "Products" and the [Output_content] would be the SMILES of products. Detailed ICL prompts for each task will be presented in their respective sections that follow. The last {Question} part presents the testing case for LLMs to respond to. Fig 5 is example of our name prediction prompt.</p>
<p>Figure 3: An ICL prompt template for all tasks.</p>
<p>Figure 4: An ICL prompt example for smiles2iupac prediction</p>
<p>ICL strategies. To investigate the impact of the quality and quantity of ICL examples on the performance of each task, we explore two ICL strategies. The quality is determined by the retrieval methods employed for finding similar examples to the sample in question. We conduct a grid search across two strategies: {Random, Scaffold}. In the Random strategy, we randomly select $k$ examples from the ICL candidate pool. In the Scaffold strategy, if the [Input_content] is a molecule SMILES, we use Tanimoto Similarity [55] from Morgan Fingerprint [41] with 2048-bits and radius=2 to calculate the molecular scaffold similarity to find the top-$k$ similar molecule SMILES. If the [Input_content] is a description such as IUPAC name or others, we use Python’s built-in difflib.SequenceMatcher tool [49] to find the top-$k$ similar strings. To explore the influence of the quantity of ICL examples on performance, we also perform a grid search for $k$, the number of ICL examples, in each task.</p>
<p>Experiment setup strategy. In property prediction and yield prediction tasks, we perform the grid search of $k$ in ${4,8}$. In the name prediction, reaction prediction, and retrosynthesis tasks, we perform the grid search of $k$ in ${5,20}$. In text-based molecule design and molecule captioning tasks, we</p>
<p>Table 2: The rank of five LLMs on eight chemistry tasks and performance highlight (NC: not competitive, C: competitive, SC: selectively competitive, acc: accuracy).</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>GPT-4</th>
<th>GPT-3.5</th>
<th>Davinci-003</th>
<th>Llama2-13B-char</th>
<th>GAL-30B</th>
<th>Performance highlight (comparing to baselines if any)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Name Prediction</td>
<td>1</td>
<td>2</td>
<td>3</td>
<td>4</td>
<td>5</td>
<td>NC: max. acc. 8% (Table 4)</td>
</tr>
<tr>
<td>Property Prediction</td>
<td>1</td>
<td>2</td>
<td>3</td>
<td>5</td>
<td>4</td>
<td>SC: outperform RF and XGBoost from MoleculeNet [65] (Table 6)</td>
</tr>
<tr>
<td>Yield Prediction</td>
<td>1</td>
<td>3</td>
<td>2</td>
<td>5</td>
<td>4</td>
<td>C: but 16-20% lower acc. than UAGNN [34] (Table 10)</td>
</tr>
<tr>
<td>Reaction Prediction</td>
<td>1</td>
<td>3</td>
<td>2</td>
<td>5</td>
<td>4</td>
<td>NC: 70% lower acc. than Chemformer [26] (Table 11)</td>
</tr>
<tr>
<td>Reagents Selection</td>
<td>2</td>
<td>1</td>
<td>3</td>
<td>4</td>
<td>5</td>
<td>C: 40-50% acc. (Table 12)</td>
</tr>
<tr>
<td>Retrosynthesis</td>
<td>2</td>
<td>3</td>
<td>1</td>
<td>5</td>
<td>4</td>
<td>NC: 40% lower acc. than Chemformer [26] (Table 13)</td>
</tr>
<tr>
<td>Molecule Design</td>
<td>1</td>
<td>3</td>
<td>2</td>
<td>4</td>
<td>5</td>
<td>SC: better than MolT5-Large [17] (Table 14)</td>
</tr>
<tr>
<td>Molecule Captioning</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>4</td>
<td>5</td>
<td>SC: better than MolT5-Large [17] (Table 15)</td>
</tr>
<tr>
<td>Average rank</td>
<td>1.25</td>
<td>2.375</td>
<td>2.125</td>
<td>4.5</td>
<td>4.5</td>
<td>overall: 3 SC, 2 C, 3 NC</td>
</tr>
</tbody>
</table>
<p>perform the grid search of $k$ in ${5,10}$ because of the maximum token limitation of LLMs. To reduce the time consumption of API requests caused by testing on the large test set, we first construct a validation set of size 30 which is randomly sampled from the original training set. Then we search $k$ and retrieval strategies ({Random, Scaffold$}$ ) on the validation set. Based on the validation set results, we take 5 representative options when testing on 100 instances, which are randomly sampled from the original test set. For each task, we run evaluation 5 times and report mean and standard deviation.</p>
<h1>4 Experiment Analysis</h1>
<p>Due to space limitations, we provide details of the evaluation on each chemistry task in Appendix by the following order: name prediction in section A, property prediction in section B, yield prediction in section C, reaction prediction in section D, reagents selection in section E, retrosynthesis in section F, text-based molecule design in section G, and molecule captioning in section H. The detailed results described in the Appendix allow us to approach the question "What can LLMs do in chemistry?" from several directions. We discuss the key findings from our comprehensive benchmark analysis and provide valuable insights by thoroughly analyzing the limitation of LLMs and how different settings related to LLMs affect performance across various chemistry tasks.</p>
<h3>4.1 Can LLMs outperform existing baselines in chemistry tasks?</h3>
<p>Several classic predictive models based on machine learning (ML) have been developed for specific chemistry tasks. For instance, MolR (Graph Neural Network-based) predicts molecule properties as a binary classification problem [58]. UAGNN achieved state-of-the-art performance in yield prediction [34]. MolT5-Large, a specialized language model based on T5, excels in translating between molecule and text [17]. We conduct a performance analysis of GPT models and compare their results with available baselines, if applicable. The main findings from the investigations are:</p>
<ul>
<li>GPT-4 outperforms the other models evaluated. The ranking of the models on 8 tasks can be found in Table 2;</li>
<li>GPT models exhibit a less competitive performance in tasks demanding precise understanding of molecular SMILES representation, such as name prediction, reaction prediction and retrosynthesis;</li>
<li>GPT models demonstrate strong capabilities both qualitatively (in Fig. 14 evaluated by chemists) and quantitatively in text-related explanation tasks such as molecule captioning;</li>
<li>For chemical problems that can be converted to classification tasks or ranking tasks, such as property prediction, and yield prediction, GPT models can achieve competitive performance compared to baselines that use classical ML models as classifiers, or even better, as summarized in Table 2.
These conclusions are derived from conducting five repeated evaluations on each task, using the best evaluation setting that was discovered through a grid search on the validation set of each task. We designate the performance of GPT models as three categories and provide in-depth discussion next.</li>
<li>Tasks with not competitive (NC) performance. In tasks such as reaction prediction and retrosynthesis, GPT models are worse than existing ML baselines trained by large amounts of training data, partially because of the limitation on understanding molecular SMILES strings. In reaction prediction and retrosynthesis, SMILES strings are present in both the input and output of the GPT models. Without an in-depth understanding of the SMILES strings that represent</li>
</ul>
<p>reactants and products, as well as the reaction process that transforms reactants into products, it will be difficult for GPT models to generate accurate responses, as shown in Table 11 and 13. GPT models exhibit poor performance on the task of name prediction as well (see Table 4). This further validates the notion that GPT models struggle with understanding long strings in formats such as SMILES, IUPAC name, and molecular formula, and make correct translations between them.</p>
<ul>
<li>Tasks with competitive (C) performance. GPT models can achieve satisfactory results when the chemistry tasks are formulated into the forms of classification (e.g., formatting yield prediction into a high-or-not classification, instead of regression) or ranking (as seen in reagents selection), as illustrated in Fig. 7 and 9. This is understandable, because making choices is inherently simpler than generating products, reactants or names. GPT models can achieve an accuracy of $40 \%$ to $50 \%$ when asked to select the reactant or solvent or ligand from provided candidates. Although GPT-4's performance on yield prediction falls short compared to the baseline model UAGNN [34] (with $80 \%$ versus $96 \%$ on the Buchwald-Hartwig dataset, and $76 \%$ versus $96 \%$ on the Suzuki-coupling dataset), it demonstrates improved performance when given more demonstration examples within the few-shot in-context learning scenario, as reported in Table 10. It is worth noting that the UAGNN model was trained on thousands of examples for these specific reactions. Last, while GPT models exhibit promising performance for yield prediction on the evaluated High-Throughput Experimentation (HTE) datasets, specifically the Buchwald-Hartwig [1] and Suzuki-Miyaura datasets [50], they perform as bad as other ML baselines on more challenging datasets like USPTO-50k [53]. This observation indicates a potential area for future research and improvement in the performance of GPT models on challenging chemistry datasets.</li>
<li>Tasks with selectively competitive (SC) performance. GPT models are selectively competitive on two types of tasks.</li>
<li>In the property prediction task on some datasets (HIV, ClinTox), GPT models outperform the baseline significantly, achieving F1 scores and accuracy nearing 1, as reported in Table 6 and 7. This might be due to the fact that the property labels to be predicted are included in the prompts, with GPT models being simply tasked in responding with yes or no. For example, the prompt includes inhibit HIV replication or drugs failed clinical trials for toxicity reason, and we observed a significant decline in the performance of GPT models upon removing property labels from the prompt (refer to Appendix section B). In contrast, baselines employing machine learning models do not include the semantic meaning of these labels in their input. The input for these models only comprises molecular representations in graph form but no labels.</li>
<li>For tasks related to text, such as text-based molecule design and molecule captioning, GPT models exhibit strong performance due to their language generation capabilities. On the task of text-based molecule design, GPT models outperform the baseline when evaluated using NLP metrics such as BLEU and Levenshtein. However, when it comes to exact match, the accuracy is less than $20 \%$, as reported in Table 14 and 15. This suggests that the molecules designed by GPT models may not be exactly the same as the ground truth. Particularly in the context of molecular design/generation, the exact match is a significant metric. Unlike in natural language generation where there is some allowance for deviation from the input, molecular design demands precise accuracy and chemical validity. However, not being precisely identical to the ground truth does not automatically invalidate a result. Molecules generated by GPT models may still prove to be beneficial and could potentially act as viable alternatives to the ground truth, provided they meet the requirements outlined in the input text and the majority (over $89 \%$ ) are chemically valid (see Table 14). Nonetheless, assessing the true utility of these generated molecules, such as evaluating their novelty in real-world applications, can be a time-consuming undertaking.</li>
</ul>
<h1>4.2 The capability of different LLMs</h1>
<p>As shown in Table 2, we can find that GPT-4 model shows better chemical understanding, reasoning, and explaining abilities than Davinci-003, GPT-3.5, Llama and Galactica. This further verifies the GPT-4 model outperforms the other models in both basic and realistic scenarios [5].</p>
<h3>4.3 The effects of the ICL</h3>
<p>To investigate the effects of the ICL, we introduced ICL prompting and different ICL retrieval methods, and the different number of ICL examples in each task. Based on the experiments results of</p>
<p>12 different variants of each option and evaluating their performance on the validation set, we have the following three observations:</p>
<ul>
<li>In all tasks, the performance of ICL prompting is better than zero-shot prompting.</li>
<li>In most tasks (in Table 4, 6, 7, 11, 13, 14, 15), using scaffold similarity to retrieve the most similar examples of the question as ICL examples achieves better performance than random sampling.</li>
<li>In most tasks (in Table 4, 6, 7, 10, 11, 14, 15), using larger $k$ (more ICL examples) usually achieves better performance than small $k$ (fewer ICL examples).</li>
</ul>
<p>These observations indicate that the quality and quantity of ICL examples plays an important role in the performance of ICL prompting [23, 36]. This may inspire that it is necessary to design more chemistry-specific ICL methods to build high-quality ICL examples to further improve the ICL prompting performance.</p>
<h1>4.4 Are molecule SELFIES representations more suitable for LLMs than SMILES representations?</h1>
<p>SELFIES [33] representations are more machine-learning-friendly string representations of molecules. To investigate whether the SELFIES representations are more suitable for LLMs than SMILES representations, we conduct experiments on four tasks, including molecule property prediction, reaction prediction, molecule design and molecule captioning. The experiment results are shown in Table 16, 17, 18, 19. We can observe that the results of using SELFIES in all four tasks are inferior to those of using SMILES. This could be attributed to the fact that the pretraining datasets for LLMs are primarily populated with SMILES-related content rather than SELFIES. Consequently, these models are more attuned to SMILES. However, it's worth mentioning that the occurrence of invalid SELFIES is less frequent than that of invalid SMILES, which aligns with the inherent design of SELFIES to ensure molecular validity.</p>
<h3>4.5 The impact of temperature parameter of LLMs</h3>
<p>One key hyperparameter that affects the performance of LLMs is temperature, which influences the randomness in the model's predictions. To determine the optimal temperature for each task, we randomly sampled 30 data points from the datasets and performed in-context learning experiments across various temperature settings. While optimal temperatures determined on the validation set may not always yield optimal results on the test set, our methodology is primarily designed to conserve token usage and API query time. To address potential discrepancies between validation and test sets, we performed targeted temperature testing on the test sets for two molecular property prediction datasets: BBBP and BACE. Our results are summarized in Table 3. For these tests, we employed the GPT-4 model (using scaffold sampling with $k=8$ ) and set temperature values $t=[0.2,0.4,0.6,0.8,1]$. The result reveal that variations in the temperature parameter have a marginal impact on test performance, with fluctuations of less than 0.05 observed in both F1 and accuracy scores. These results validate the robustness of our initial sampling approach and underscore the reliability of our findings across different settings.</p>
<p>Table 3: The F1( $\uparrow$ ) and accuracy( $\uparrow$ ) score of GPT-4 model(scaffold sampling, $k=8$ ) on different temperature setting.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">F1( $\uparrow$ )</th>
<th style="text-align: center;">BBBP</th>
<th style="text-align: center;">BACE</th>
<th style="text-align: center;">Accuracy( $\uparrow$ )</th>
<th style="text-align: center;">BBBP</th>
<th style="text-align: center;">BACE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT-4( $\mathrm{t}=0.2)$</td>
<td style="text-align: center;">$0.667 \pm 0.029$</td>
<td style="text-align: center;">$0.741 \pm 0.019$</td>
<td style="text-align: center;">GPT-4( $\mathrm{t}=0.2)$</td>
<td style="text-align: center;">$0.650 \pm 0.028$</td>
<td style="text-align: center;">$0.743 \pm 0.019$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4( $\mathrm{t}=0.4)$</td>
<td style="text-align: center;">$0.712 \pm 0.014$</td>
<td style="text-align: center;">$0.728 \pm 0.024$</td>
<td style="text-align: center;">GPT-4( $\mathrm{t}=0.4)$</td>
<td style="text-align: center;">$0.691 \pm 0.017$</td>
<td style="text-align: center;">$0.729 \pm 0.024$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4( $\mathrm{t}=0.6)$</td>
<td style="text-align: center;">$0.683 \pm 0.016$</td>
<td style="text-align: center;">$0.736 \pm 0.020$</td>
<td style="text-align: center;">GPT-4( $\mathrm{t}=0.6)$</td>
<td style="text-align: center;">$0.659 \pm 0.016$</td>
<td style="text-align: center;">$0.736 \pm 0.019$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4( $\mathrm{t}=0.8)$</td>
<td style="text-align: center;">$0.686 \pm 0.030$</td>
<td style="text-align: center;">$0.744 \pm 0.025$</td>
<td style="text-align: center;">GPT-4( $\mathrm{t}=0.8)$</td>
<td style="text-align: center;">$0.661 \pm 0.032$</td>
<td style="text-align: center;">$0.745 \pm 0.025$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4( $\mathrm{t}=1.0)$</td>
<td style="text-align: center;">$0.684 \pm 0.023$</td>
<td style="text-align: center;">$0.756 \pm 0.025$</td>
<td style="text-align: center;">GPT-4( $\mathrm{t}=1.0)$</td>
<td style="text-align: center;">$0.660 \pm 0.021$</td>
<td style="text-align: center;">$0.757 \pm 0.025$</td>
</tr>
</tbody>
</table>
<h1>5 Discussion</h1>
<h3>5.1 Limitation of LLMs on understanding molecular SMILES</h3>
<p>A significant limitation of LLMs is their lack of understanding of molecular representations in SMILES strings, which in many cases leads to inaccurate or inconsistent results as shown in Section A for the translation of different ways to name molecules. SMILES (Simplified Molecular Input Line Entry System) [60, 61] is a widely used textual representation for chemical structures. For example, the SMILES string for ethanol, a simple alcohol, is "CCO". This string represents a molecule with two carbon atoms (C) connected by a single bond and an oxygen atom (O) connected to the second carbon atom. SMILES strings can serve as both input and output for LLMs, alongside other natural language text. However, several issues make it challenging for LLMs to accurately understand and interpret SMILES strings: 1) Hydrogen atoms are not explicitly represented in SMILES strings, as they can be inferred based on the standard bonding rules. LLMs frequently struggle to infer these implicit hydrogen atoms and may even fail at simple tasks like counting the number of atoms in a molecule [27, 6]. 2) A given molecule can have multiple valid SMILES representations, which can lead to ambiguity if not properly processed or standardized. LLMs may thus fail to consistently recognize and compare molecular structures represented by different SMILES strings. 3) LLMs do not have any inherent understanding of SMILES strings, and treat them as a sequence of characters or subwords. When processing long SMILES strings, LLMs rely on the byte-pair encoding tokenization technique, which can break the string into smaller pieces or subwords in ways that do not represent the molecular structure and properties of molecules represented by SMILES strings. Because many tasks in cheminformatics rely on the accurate representation of a molecule by SMILES strings, the non-competitive performance of GPT models in converting structures into SMILES strings (and vice versa) affects downstream tasks such as retrosynthesis, reaction and name prediction. LLMs that have an enhanced ability of handling molecular structures and their specific attributes or coupling to existing tools such as RDKit [35] will be needed.</p>
<h3>5.2 The limitations of current evaluation methods</h3>
<p>Although in Text-Based Molecule Design and Molecule Captioning tasks, GPT models show competitive performance compared to the baseline in some metrics (BLEU, Levenshtein, ROUGE, FCD, etc), we observe that the exact match of GPT models is inferior to the baseline in the Text-Based Molecule Design task and the GPT models generate some descriptions which violate chemical facts. This divergence between metrics and real-world scenarios mainly arises because, unlike many natural language processing tasks that can be suitably evaluated by sentence-level matching evaluation metrics, chemistry-related tasks necessitate exact matching for SMILES and precise terminology in descriptions. These findings spotlight the limitations of current evaluation metrics and underscore the need for the development of chemistry-specific metrics.</p>
<h3>5.3 Hallucination of LLMs in chemistry</h3>
<p>Our evaluation experiments across various tasks reveal two primary types of hallucinations exhibited by LLMs in the domain of chemistry. The first type occurs when the input is given in SMILES format (e.g., name prediction); LLMs occasionally struggle with interpreting these SMILES correctly. For instance, they may fail to recognize the number of atoms or certain functional groups within molecules during name prediction tasks. The second type of hallucination arises when the expected output from LLMs should be in the form of SMILES (e.g., reaction prediction and retrosynthesis). Here, LLMs may produce molecules that are chemically unreasonable, suggesting a gap in understanding what constitutes valid SMILES. Hallucination issues represent a key challenge with LLMs, particularly in the field of chemistry which necessitates exact matching of SMILES and adherence to strict chemical facts [62]. Current LLMs need further investigation into this problem.</p>
<h3>5.4 Prospects of LLMs for chemistry</h3>
<p>Overall, through an exhaustive set of experiments and analyses, we outline several promising avenues for the application of LLMs in the field of chemistry. While LLMs underperform relative to baselines across a majority of tasks, it's important to note that LLMs leverage only a few examples to solve chemistry problems, whereas baselines are trained on extensive, task-specific datasets and are limited</p>
<p>to certain tasks. This observation provides valuable insights into the potential of LLMs’ generalized intelligence in the domain of chemistry. The employment of advanced prompting techniques such as Chain-of-thought (CoT) [59], Decomposed Prompting [31] could potentially boost the capacity of LLMs to perform complex reasoning. On the other hand, LLMs display a considerable amount of hallucinations in chemistry tasks, indicating that current LLMs may not yet possess the necessary capabilities to solve practical chemistry problems effectively. However, with continuous development of LLMs and further research into methods to avoid hallucinations, we are optimistic that LLMs can significantly enhance their problem-solving abilities in the field of chemistry.</p>
<h1>5.5 Impact of generating harmful chemicals</h1>
<p>Our work demonstrate that LLMs can generate chemically valid molecules. However, it's crucial to acknowledge and mitigate the risks of AI misuse, such as generating hazardous substances. While advancements in AI-enabled chemistry have the potential to bring about groundbreaking medicines and sustainable materials, the same technology can be misused to create toxic or illegal substances. This dual-edged potential emphasizes the necessity for stringent oversight. Without careful regulation, these tools could not only pose significant health and safety hazards but also create geopolitical and security challenges. Consequently, as we harness the capabilities of LLMs in the field of chemistry, we concur with earlier research on generative models in chemistry [2, 3] that it is vital for developers to establish robust safeguards and ethical guidelines to deter harmful applications. This is akin to the limitations imposed on popular search engines, which can also be exploited to find information about dangerous chemicals or procedures online.</p>
<h3>5.6 Broader Impacts</h3>
<p>Our work has broad impacts across multiple dimensions. First, it offers valuable insights and recommendations for both AI researchers and chemists in academia and industry. These perspectives enhance the effective utilization of LLMs and guide future advancements in the field. Second, our objective evaluation of LLMs helps alleviate concerns regarding the replacement of chemists by AI. This aspect contributes to public education, addressing misconceptions and fostering a better understanding of the role of AI in chemistry. Furthermore, we provide a comprehensive experimental framework for testing LLMs in chemistry tasks, which can also be applicable to other domains. This framework serves as a valuable resource for researchers seeking to evaluate LLMs in diverse fields. However, it is important to recognize the ethical and societal implications associated with our work. Additionally, concerns about job displacement in the chemical industry may arise, and efforts should be made to address these challenges and ensure a responsible and equitable adoption of AI technologies.</p>
<h2>6 Conclusion and Future Work</h2>
<p>In this paper, we summarize the required abilities of LLMs in chemistry and construct a comprehensive benchmark to evaluate the five most popular LLMs (GPT-4, GPT-3.5, Davinci-003, LLama and Galactica) on eight widely-used chemistry tasks. The experiment results show that LLMs perform less competitive in generative tasks which require in-depth understanding of molecular SMILES strings, such as reaction prediction, name prediction, and retrosynthesis. LLMs show competitive performance in tasks that are in classification or ranking formats such as yield prediction and reagents selection. LLMs are selectively competitive on tasks involving text in prompts such as property prediction and text-based molecule design, or explainable tasks such as molecule captioning. These experiments indicate the potential of LLMs in chemistry tasks and the need for further improvement. We will collaborate with more chemists in the C-CAS group, progressively integrating a wider range of tasks that are both novel and practical. We hope our work can address the gap between LLMs and the chemistry research field, inspiring future research to explore the potential of LLMs in chemistry.</p>
<h2>Acknowledgments and Disclosure of Funding</h2>
<p>This work was supported by the National Science Foundation (CHE-2202693) through the NSF Center for Computer Assisted Synthesis (C-CAS).</p>
<h1>References</h1>
<p>[1] Derek T Ahneman, Jesús G Estrada, Shishi Lin, Spencer D Dreher, and Abigail G Doyle. Predicting reaction performance in c-n cross-coupling using machine learning. Science, 360 (6385):186-190, 2018.
[2] Daniil A Boiko, Robert MacKnight, and Gabe Gomes. Emergent autonomous scientific research capabilities of large language models. arXiv preprint arXiv:2304.05332, 2023.
[3] Andres M Bran, Sam Cox, Andrew D White, and Philippe Schwaller. Chemcrow: Augmenting large-language models with chemistry tools. arXiv preprint arXiv:2304.05376, 2023.
[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[5] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4, 2023.
[6] Cayque Monteiro Castro Nascimento and André Silva Pimentel. Do large language models understand chemistry? a conversation with chatgpt. Journal of Chemical Information and Modeling, 63(6):1649-1655, 2023.
[7] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language models. arXiv preprint arXiv:2307.03109, 2023.
[8] Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Xiangliang Zhang, Dongyan Zhao, and Rui Yan. Capturing relations between scientific papers: An abstractive model for related work section generation. In Proc. of ACL, 2021.
[9] Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Rui Yan, Xin Gao, and Xiangliang Zhang. Target-aware abstractive related work generation with contrastive learning. In Proc. of SIGIR, 2022.
[10] Xiuying Chen, Mingzhe Li, Shen Gao, Rui Yan, Xin Gao, and Xiangliang Zhang. Scientific paper extractive summarization enhanced by citation graphs. In Proc. of EMNLP, 2022.
[11] Jonathan Choi, Kristin Hickman, Amy Monahan, and Daniel Schwarcz. Chatgpt goes to law school. Journal of Legal Education, 2023.
[12] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.
[13] Connor W Coley, Regina Barzilay, Tommi S Jaakkola, William H Green, and Klavs F Jensen. Prediction of organic reaction outcomes using machine learning. ACS central science, 3(5): $434-443,2017$.
[14] Debadutta Dash, Rahul Thapa, Juan M Banda, Akshay Swaminathan, Morgan Cheatham, Mehr Kashyap, Nikesh Kotecha, Jonathan H Chen, Saurabh Gombar, Lance Downing, et al. Evaluation of gpt-3.5 and gpt-4 for supporting real-world information needs in healthcare delivery. arXiv preprint arXiv:2304.13714, 2023.
[15] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. A survey on in-context learning, 2023.
[16] Carl Edwards, ChengXiang Zhai, and Heng Ji. Text2Mol: Cross-modal molecule retrieval with natural language queries. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 595-607, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.47. URL https://aclanthology.org/2021.emnlp-main. 47.</p>
<p>[17] Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, and Heng Ji. Translation between molecules and natural language. arXiv preprint arXiv:2204.11817, 2022.
[18] Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier, and Julius Berner. Mathematical capabilities of chatgpt. arXiv preprint arXiv:2301.13867, 2023.
[19] Taicheng Guo, Changsheng Ma, Xiuying Chen, Bozhao Nan, Kehan Guo, Shichao Pei, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang. Modeling non-uniform uncertainty in reaction prediction via boosting and dropout. arXiv preprint arXiv:2310.04674, 2023.
[20] Taicheng Guo, Lu Yu, Basem Shihada, and Xiangliang Zhang. Few-shot news recommendation via cross-lingual transfer. In Proceedings of the ACM Web Conference 2023, WWW '23, page 1130-1140, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9781450394161. doi: 10.1145/3543507.3583383. URL https://doi.org/10.1145/ 3543507.3583383 .
[21] Zhichun Guo, Chuxu Zhang, Wenhao Yu, John Herr, Olaf Wiest, Meng Jiang, and Nitesh V Chawla. Few-shot graph learning for molecular property prediction. In Proceedings of the Web Conference 2021, pages 2559-2567, 2021.
[22] Zhichun Guo, Bozhao Nan, Yijun Tian, Olaf Wiest, Chuxu Zhang, and Nitesh V Chawla. Graph-based molecular representation learning. arXiv preprint arXiv:2207.04869, 2022.
[23] Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, and Furu Wei. Structured prompting: Scaling in-context learning to 1,000 examples, 2022.
[24] Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. Aligning ai with shared human values. Proceedings of the International Conference on Learning Representations (ICLR), 2021.
[25] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021.
[26] Ross Irwin, Spyridon Dimitriadis, Jiazhen He, and Esben Jannik Bjerrum. Chemformer: a pretrained transformer for computational chemistry. Machine Learning: Science and Technology, 3(1):015022, 2022.
[27] Kevin Jablonka, Philippe Schwaller, Andrés Ortega-Guerrero, and Berend Smit. Is gpt-3 all you need for low-data discovery in chemistry. 10.26434/chemrxiv-2023-fw8n4, 2023.
[28] Kevin Maik Jablonka, Qianxiang Ai, Alexander Al-Feghali, Shruti Badhwar, Joshua D Bran, Stefan Bringuier, L Catherine Brinson, Kamal Choudhary, Defne Circi, Sam Cox, et al. 14 examples of how llms can transform materials science and chemistry: A reflection on a large language model hackathon. arXiv preprint arXiv:2306.06283, 2023.
[29] Wengong Jin, Connor W. Coley, Regina Barzilay, and Tommi Jaakkola. Predicting organic reaction outcomes with weisfeiler-lehman network, 2017.
[30] Rehan Ahmed Khan, Masood Jawaid, Aymen Rehan Khan, and Madiha Sajjad. Chatgptreshaping medical education and clinical management. Pakistan Journal of Medical Sciences, 39(2):605, 2023.
[31] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406, 2022.
[32] Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, et al. Pubchem 2019 update: improved access to chemical data. Nucleic acids research, 47(D1):D1102-D1109, 2019.</p>
<p>[33] Mario Krenn, Florian Häse, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. Self-referencing embedded strings (SELFIES): A 100\% robust molecular string representation. Machine Learning: Science and Technology, 1(4):045024, oct 2020. doi: 10.1088/2632-2153/ aba947. URL https://doi.org/10.1088\%2F2632-2153\%2Faba947.
[34] Youngchun Kwon, Dongseon Lee, Youn-Suk Choi, and Seokho Kang. Uncertainty-aware prediction of chemical reaction yields with graph neural networks. Journal of Cheminformatics, $14: 1-10,2022$.
[35] G. A. Landrum. Rdkit: Open-source cheminformatics software. http://www.rdkit.org, 2020.
[36] Itay Levy, Ben Bogin, and Jonathan Berant. Diverse demonstrations improve in-context compositional generalization, 2022.
[37] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. arXiv preprint arXiv:2305.01210, 2023.
[38] Zequn Liu, Wei Zhang, Yingce Xia, Lijun Wu, Shufang Xie, Tao Qin, Ming Zhang, and TieYan Liu. Molxpt: Wrapping molecules with text for generative pre-training. arXiv preprint arXiv:2305.10688, 2023.
[39] Daniel Mark Lowe. Extraction of chemical structures and reactions from the literature. PhD thesis, University of Cambridge, 2012.
[40] Frederic P Miller, Agnes F Vandome, and John McBrewster. Levenshtein distance: Information theory, computer science, string (computer science), string metric, damerau? levenshtein distance, spell checker, hamming distance, 2009.
[41] Harry L Morgan. The generation of a unique machine description for chemical structures-a technique developed at chemical abstracts service. Journal of chemical documentation, 5(2): $107-113,1965$.
[42] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023.
[43] OpenAI. Gpt-4 technical report, 2023.
[44] Damith Perera, Joseph W Tucker, Shalini Brahmbhatt, Christopher J Helal, Ashley Chong, William Farrell, Paul Richardson, and Neal W Sach. A platform for automated nanomole-scale reaction screening and micromole-scale synthesis in flow. Science, 359(6374):429-434, 2018.
[45] Kristina Preuer, Philipp Renz, Thomas Unterthiner, Sepp Hochreiter, and Gunter Klambauer. Fréchet chemnet distance: a metric for generative models for molecules in drug discovery. Journal of chemical information and modeling, 58(9):1736-1741, 2018.
[46] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.
[47] Kohulan Rajan, Achim Zielesny, and Christoph Steinbeck. Stout: Smiles to iupac names using neural machine translation. Journal of Cheminformatics, 13(1):1-14, 2021.
[48] Mayk Caldas Ramos, Shane S Michtavy, Marc D Porosoff, and Andrew D White. Bayesian optimization of catalysts with in-context learning. arXiv preprint arXiv:2304.05341, 2023.
[49] David Ratcliff, John W.; Metzener. Pattern matching: The gestalt approach, 1988.
[50] Brandon J Reizman, Yi-Ming Wang, Stephen L Buchwald, and Klavs F Jensen. Suzuki-miyaura cross-coupling optimization enabled by automated feedback. Reaction chemistry \&amp; engineering, 1(6):658-666, 2016.
[51] Mandana Saebi, Bozhao Nan, John E Herr, Jessica Wahlers, Zhichun Guo, Andrzej M Zurański, Thierry Kogej, Per-Ola Norrby, Abigail G Doyle, Nitesh V Chawla, et al. On the use of real-world datasets for reaction yield prediction. Chemical Science, 2023.</p>
<p>[52] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.
[53] Nadine Schneider, Nikolaus Stiefl, and Gregory A Landrum. What's what: The (nearly) definitive guide to reaction role assignment. Journal of chemical information and modeling, 56 (12):2336-2346, 2016.
[54] Philippe Schwaller, Teodoro Laino, Théophile Gaudin, Peter Bolgar, Christopher A Hunter, Costas Bekas, and Alpha A Lee. Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction. ACS central science, 5(9):1572-1583, 2019.
[55] Taffee T Tanimoto. Elementary mathematical theory of classification and prediction. Journal of Biomedical Science and Engineering, 1958.
[56] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.
[57] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
[58] Hongwei Wang, Weijiang Li, Xiaomeng Jin, Kyunghyun Cho, Heng Ji, Jiawei Han, and Martin D Burke. Chemical-reaction-aware molecule representation learning. arXiv preprint arXiv:2109.09888, 2021.
[59] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.
[60] David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. J. Chem. Inf. Comput. Sci., 28:31-36, 1988.
[61] David Weininger, Arthur Weininger, and Joseph L. Weininger. Smiles. 2. algorithm for generation of unique smiles notation. J. Chem. Inf. Comput. Sci., 29:97-101, 1989.
[62] A.D. White. The future of chemistry is language., 2023.
[63] Andrew D. White, Glen M. Hocky, Heta A. Gandhi, Mehrad Ansari, Sam Cox, Geemi P. Wellawatte, Subarna Sasmal, Ziyue Yang, Kangxin Liu, Yuvraj Singh, and Willmor J. Peña Ccoa. Assessment of chemistry knowledge in large language models that generate code. Digital Discovery, 2:368-376, 2023. doi: 10.1039/D2DD00087C. URL http://dx.doi.org/10. 1039/D2DD00087C.
[64] Genta Indra Winata, Samuel Cahyawijaya, Zihan Liu, Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Are multilingual models effective in code-switching? arXiv preprint arXiv:2103.13309, 2021.
[65] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513-530, 2018.
[66] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023.</p>
<h1>Appendix</h1>
<h2>A Name Prediction</h2>
<p>For one molecule, there are different chemical naming conventions and representations such as SMILES, IUPAC names, and graphic molecular formula. To investigate whether GPT models have the basic chemical name understanding ability, we construct four chemical name prediction tasks that include SMILES to IUPAC name translation (smiles2iupac), IUPAC name to SMILES translation (iupac2smiles), SMILES to molecule formula translation (smiles2formula), and IUPAC name to molecule formula translation (iupac2formula). We collect 630 molecules and their corresponding names including SMILES, IUPAC name, and molecule formula from PubChem ${ }^{3}$ [32]. We randomly sample 500 molecules as the ICL candidates, and other 30 molecules as the validation set, and other 100 molecules as the test set. For all name translation tasks, we use the exact match accuracy as the metric to evaluate the performance.</p>
<p>ICL Prompt. One example of the smiles2iupac prediction is shown in Figure 5. For other name translation tasks, we only change the underlined parts that represent different tasks and their corresponding input names and output names.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">General <br> Template</th>
<th style="text-align: center;">You are an expert chemist. Given the molecular SMILES, your task is to predict the IUPAC name using your experienced chemical IUPAC name knowledge.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Task-specific <br> Template</td>
<td style="text-align: center;">Please strictly follow the format, no other information can be provided.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Molecular SMILES: CN(C)(-1ncc2-c2ccccc2F)(-2C2CCCNC2-0)CCN2CCOCC3C2)r1 <br> Molecular IUPAC name: 1-(3-2)-dimethylamino)-5-(2-fluorophenyl)pyrimidin-4-yl]piperidin-1-yl)-3-morpholin-4-ylpropan-1-one <br> Molecular SMILES: O-C(N)-1ccccc13c1cn(CC2CCCNC2-0)C2CC3C2)c2ccccc13 <br> Molecular IUPAC name: 1-[(3R)-1-cyclopropanecarboxylpiperidin-3-yl)methyl]-N-pyridin-3-ylindole-3-carboxamide <br> Molecular SMILES: CN(C)-O)C1CCC5(1C2-c3)C(N(C)-1ccc(C)cc13C(C)-1ccccc13C2-(3H1CCCC1C)-c3)O <br> Molecular IUPAC name: 1-[2-[1-(2-amino-3-(4-hydroxyphenyl)propanoyl[pyrrolidine-2-carboxyl)-methylamino]-3-phenylpropanoyl[pyrrolidine-2-carboxylic acid <br> Molecular SMILES: CC-c2ccac2CC(41CC)-C(N-1cc(N-4))c-c3N2CCCCC2)ccc1O <br> Molecular IUPAC name: N-(2-chloro-5-piperidin-1-ylsulfonylphenyl)-2-[4R]-4-methyl-6,7-dihydro-4H-thieno[3,2-c]pyridin-5-yl]acetamide <br> Molecular SMILES: Cc1cccc(NC)-O(CN)C(-O)c3cc(C)cc(C)cc2C2-(3)-1(2-c3H1CCCC1 <br> Molecular IUPAC name: 2-(5,8-dichloro-1,3-dioxoisoindol-2-yl)-N-[3-methyl-2-(pyrrolidine-1-carboxyl)phenyl]acetamide</td>
</tr>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">Molecular SMILES: CC(C)(C)OC(-O)NC1CCN(C(-O)CN2CCOCC2C(-O)Nc2cc(C(ccc3c2[nH]c2cnccc23)CC1 <br> Molecular IUPAC name:</td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">4-[3-(4-chloro-2H-indazol-3-yl)-1H-pyrrolo[2,3-b]pyridin-5-yl]morpholine-2-carbonyl]amino-N-[2-methylpropyl]benzamide</td>
</tr>
</tbody>
</table>
<p>Figure 5: An ICL prompt example for smiles2iupac prediction</p>
<p>Results. The results are reported in Table 4 (we only report representative methods along with their optimal prompt settings via grid search on validation set). In all four name prediction tasks, the accuracy of the best method is extremely low ( 0.014 in the iupac2smiles task, 0.086 in the smiles2formula task, 0.118 in the iupac2formula task) or even 0 (in the smiles2iupac task). This indicates the LLMs lack basic chemical name understanding ability. The accuracy of Davinci-003 is considerably inferior to other models.</p>
<p>Case studies. Example results generated by GPT-4 (Scaffold, $k=20$ ) method for each task is shown in Table 5. In all tasks, the GPT-4 model gives the wrong answers. In the smiles2formula task, we can observe that GPT models cannot even recognize the number of Carbon and infer the correct number of Hydrogen, demonstrating the bad chemical understanding ability of GPT models. For prospects, some pre-training technologies such as wrapping molecules with text [38] or code-switch [64, 20] may be helpful to align different chemical names of the same molecule to help improve LLMs' chemical understanding.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 4: The accuracy ( $\uparrow$ ) of LLMs in 4 different name prediction tasks. The best LLM is in bold font. Here $k$ is the number of examples used in few-shot ICL. The baseline is underlined and "-" indicates that STOUT cannot solve the smiles2formula and iupac2formula tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">smiles2iupac</th>
<th style="text-align: center;">iupac2smiles</th>
<th style="text-align: center;">smiles2formula</th>
<th style="text-align: center;">iupac2formula</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">STOUT [47]</td>
<td style="text-align: center;">$\underline{0.55}$</td>
<td style="text-align: center;">$\underline{0.7}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (zero-shot)</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$0.008 \pm 0.008$</td>
<td style="text-align: center;">$0.048 \pm 0.022$</td>
<td style="text-align: center;">$0.092 \pm 0.018$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (Scaffold, $k=5$ )</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$\mathbf{0 . 0 1 4} \pm \mathbf{0 . 0 0 9}$</td>
<td style="text-align: center;">$0.058 \pm 0.015$</td>
<td style="text-align: center;">$\mathbf{0 . 1 1 8} \pm \mathbf{0 . 0 2 2}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (Scaffold, $k=20$ )</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$0.012 \pm 0.004$</td>
<td style="text-align: center;">$\mathbf{0 . 0 8 6} \pm \mathbf{0 . 0 3 6}$</td>
<td style="text-align: center;">$0.084 \pm 0.005$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (Random, $k=20$ )</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$0.010 \pm 0.007$</td>
<td style="text-align: center;">$0.070 \pm 0.032$</td>
<td style="text-align: center;">$0.076 \pm 0.011$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 (Scaffold, $k=20$ )</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$0.010 \pm 0.000$</td>
<td style="text-align: center;">$0.052 \pm 0.004$</td>
<td style="text-align: center;">$0.044 \pm 0.009$</td>
</tr>
<tr>
<td style="text-align: left;">Davinci-003 (Scaffold, $k=20$ )</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$0.006 \pm 0.005$</td>
<td style="text-align: center;">$0.018 \pm 0.004$</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-13B-chat (Scaffold, $k=20$ )</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$0.010 \pm 0.007$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">GAL-30B (Scaffold, $k=10$ )</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>Table 5: Example results generated by GPT-4 (Scaffold, $k=20$ ) method for different tasks</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">Input</th>
<th style="text-align: center;">Ground Truth</th>
<th style="text-align: center;">Output of GPT-4 (Scaffold, k=20)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">smiles2iupac</td>
<td style="text-align: center;">$\mathrm{CCOC}(\mathrm{vO}) \mathrm{C}(\mathrm{C}) \mathrm{vO}=\mathrm{C}(\mathrm{C}) \mathrm{N}$</td>
<td style="text-align: center;">ethyl 2-acetyl-3-aminobut-2-enoate</td>
<td style="text-align: center;">ethyl 2-methyl-5-oxo-2-arabep-4-en-3-oate</td>
</tr>
<tr>
<td style="text-align: left;">iupac2smiles</td>
<td style="text-align: center;">ethyl 2-acetyl-3-aminobut-2-enoate</td>
<td style="text-align: center;">$\mathrm{CCOC}(\mathrm{vO}) \mathrm{C}(\mathrm{C}) \mathrm{vO}=\mathrm{O}(\mathrm{vO}) \mathrm{C}(\mathrm{C}) \mathrm{N}$</td>
<td style="text-align: center;">$\mathrm{CCOC}(\mathrm{vO}) \mathrm{C}+\mathrm{C}(\mathrm{C}) \mathrm{C}+\mathrm{N}) \mathrm{C}$</td>
</tr>
<tr>
<td style="text-align: left;">smiles2formula</td>
<td style="text-align: center;">$\mathrm{C}(1 \mathrm{mo}) \mathrm{C}\mathrm{Cs} 2 \mathrm{o} 2 \mathrm{~d} \mathrm{H} \mathrm{~S} 2 \mathrm{~s} 0 \mathrm{~m} 1$</td>
<td style="text-align: center;">C8H10N4O2</td>
<td style="text-align: center;">C9H10N4O2</td>
</tr>
<tr>
<td style="text-align: left;">iupac2formula</td>
<td style="text-align: center;">$\mathrm{R}(-(1-\text { benzylquinolin-1-ium-4-yl) }$</td>
<td style="text-align: center;">C26H29ClN2O</td>
<td style="text-align: center;">C23H27ClN2O</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">-(5-ethenyl-1-arabicyclo[3.2.2]octan-2-yl)methanol:chloride</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<h1>B Molecule Property Prediction</h1>
<p>Molecule property prediction $[21,58]$ is a fundamental task in computational chemistry that has been gaining significant attention in recent years due to its potential for drug discovery, material science, and other areas in the chemistry. The task involves using machine learning techniques [22] to predict the chemical and physical properties of a given molecule, based on its molecular structure. We aim to further explore the potential of LLMs in molecular property prediction and assess their performance on a set of benchmark datasets, such as BBBP(MIT license), HIV(MIT license), BACE(MIT license), Tox21(MIT license), and ClinTox(MIT license), which were originally introduced by [65]. The datasets are made up of extensive collections of SMILES, paired with binary labels that highlight the particular property being evaluated, such as BBBP: Blood-Brain Barrier Penetration, HIV: inhibit HIV replication, BACE: bindings results for a set of inhibitors of human beta-secretase, Tox21: toxicity of compounds, and ClinTox: drugs failed clinical trials for toxicity reasons. A comprehensive explanation of these datasets can be referenced in the original research conducted by [65]. For ICL, we either select $k$ samples randomly, or search the top- $k$ most analogous molecules using RDKit [35] to determine the Tanimoto Similarity. However, it is crucial to mention that using the latter method does not assure an even distribution among classes. In our study, we employ a strategic sampling method for two categories of datasets: balanced and highly imbalanced. For balanced datasets, such as BBBP and BACE, we randomly select 30 samples for the validation process and 100 samples for testing from the original dataset. Contrastingly, for datasets exhibiting substantial label imbalance ( $39684: 1443 \approx 28: 1$, take HIV datasets as a example), we select samples from the majority and minority classes to achieve a ratio of $4: 1$. This strategic approach enables us to maintain a representative sample for the evaluation process, despite the original high imbalance in the dataset. To evaluate the results, we use the classification accuracy, as well as F1 score as the evaluation metric due to the class imbalance. We benchmark our method against two established baselines from MoleculeNet [65]: RF and XGBoost. Both baselines utilize the 1024-bit circular fingerprint as input to predict the property as a binary classification problem.</p>
<p>ICL Prompt. Figure 6 illustrates a sample of our ICL prompt for property prediction. Within the task-specific template, we include a detailed explanation of the task forecasting the penetration of the brain-blood barrier to assist LLMs in comprehending the input SMILES from the BBBP dataset. Additionally, we establish certain constraints for the output to conform to the specific characteristics of the property prediction task.</p>
<p>Results. The results are reported as F1 in Table 6, accuracy in Table 7. We observed that GPT models outperform the baseline model in terms of F1 on four out of five datasets. In the range of GPT</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 6: An ICL prompt example for property prediction</p>
<p>Table 6: F1 ( $\uparrow$ ) score of LLMs and baseline in molecular property prediction tasks. $k$ is the number of examples used in few-shot ICL. The best GPT model is in bold font, and the baseline is underlined.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">BBBP</th>
<th style="text-align: center;">BACE</th>
<th style="text-align: center;">HIV</th>
<th style="text-align: center;">Tox21</th>
<th style="text-align: center;">ClinTox</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RF</td>
<td style="text-align: center;">0.881</td>
<td style="text-align: center;">0.758</td>
<td style="text-align: center;">0.518</td>
<td style="text-align: center;">0.260</td>
<td style="text-align: center;">0.461</td>
</tr>
<tr>
<td style="text-align: left;">XGBoost</td>
<td style="text-align: center;">$\underline{0.897}$</td>
<td style="text-align: center;">$\underline{0.765}$</td>
<td style="text-align: center;">$\underline{0.551}$</td>
<td style="text-align: center;">$\underline{0.333}$</td>
<td style="text-align: center;">$\underline{0.620}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (zero-shot)</td>
<td style="text-align: center;">$0.560 \pm 0.034$</td>
<td style="text-align: center;">$0.322 \pm 0.018$</td>
<td style="text-align: center;">$0.977 \pm 0.013$</td>
<td style="text-align: center;">$0.489 \pm 0.018$</td>
<td style="text-align: center;">$0.555 \pm 0.043$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (Scaffold, $k=4$ )</td>
<td style="text-align: center;">$0.498 \pm 0.028$</td>
<td style="text-align: center;">$0.516 \pm 0.024$</td>
<td style="text-align: center;">$0.818 \pm 0.015$</td>
<td style="text-align: center;">$0.444 \pm 0.004$</td>
<td style="text-align: center;">$0.731 \pm 0.035$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (Scaffold, $k=8$ )</td>
<td style="text-align: center;">$\mathbf{0 . 5 8 7} \pm \mathbf{0 . 0 1 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 6 6} \pm \mathbf{0 . 0 2 3}$</td>
<td style="text-align: center;">$0.797 \pm 0.021$</td>
<td style="text-align: center;">$\mathbf{0 . 5 6 3} \pm \mathbf{0 . 0 0 8}$</td>
<td style="text-align: center;">$0.736 \pm 0.033$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (random, $k=8$ )</td>
<td style="text-align: center;">$0.469 \pm 0.025$</td>
<td style="text-align: center;">$0.504 \pm 0.020$</td>
<td style="text-align: center;">$\mathbf{0 . 9 9 4} \pm \mathbf{0 . 0 0 6}$</td>
<td style="text-align: center;">$0.528 \pm 0.003$</td>
<td style="text-align: center;">$\mathbf{0 . 9 2 4} \pm \mathbf{0 . 0 0 0}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 (Scaffold, $k=8$ )</td>
<td style="text-align: center;">$0.463 \pm 0.008$</td>
<td style="text-align: center;">$0.406 \pm 0.011$</td>
<td style="text-align: center;">$0.807 \pm 0.021$</td>
<td style="text-align: center;">$0.529 \pm 0.021$</td>
<td style="text-align: center;">$0.369 \pm 0.029$</td>
</tr>
<tr>
<td style="text-align: left;">Davinci-003 (Scaffold, $k=8$ )</td>
<td style="text-align: center;">$0.378 \pm 0.024$</td>
<td style="text-align: center;">$0.649 \pm 0.021$</td>
<td style="text-align: center;">$0.832 \pm 0.020$</td>
<td style="text-align: center;">$0.518 \pm 0.009$</td>
<td style="text-align: center;">$0.850 \pm 0.020$</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-13B-chat (Scaffold, $k=8$ )</td>
<td style="text-align: center;">$0.002 \pm 0.001$</td>
<td style="text-align: center;">$0.045 \pm 0.015$</td>
<td style="text-align: center;">$0.069 \pm 0.033$</td>
<td style="text-align: center;">$0.047 \pm 0.013$</td>
<td style="text-align: center;">$0.001 \pm 0.003$</td>
</tr>
<tr>
<td style="text-align: left;">GAL-30B (Scaffold, $k=8$ )</td>
<td style="text-align: center;">$0.074 \pm 0.019$</td>
<td style="text-align: center;">$0.025 \pm 0.013$</td>
<td style="text-align: center;">$0.014 \pm 0.016$</td>
<td style="text-align: center;">$0.077 \pm 0.046$</td>
<td style="text-align: center;">$0.081 \pm 0.015$</td>
</tr>
</tbody>
</table>
<p>Table 7: Accuracy $(\uparrow)$ of LLMs and baseline in molecular property prediction tasks. $k$ is the number of examples used in few-shot ICL. The best GPT model is in bold font, and the baseline is underlined.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">BBBP</th>
<th style="text-align: center;">BACE</th>
<th style="text-align: center;">HIV</th>
<th style="text-align: center;">Tox21</th>
<th style="text-align: center;">ClinTox</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RF</td>
<td style="text-align: center;">0.820</td>
<td style="text-align: center;">0.790</td>
<td style="text-align: center;">$\underline{0.870}$</td>
<td style="text-align: center;">0.830</td>
<td style="text-align: center;">0.858</td>
</tr>
<tr>
<td style="text-align: left;">XGBoost</td>
<td style="text-align: center;">$\underline{0.850}$</td>
<td style="text-align: center;">$\underline{0.810}$</td>
<td style="text-align: center;">$\underline{0.870}$</td>
<td style="text-align: center;">$\underline{0.840}$</td>
<td style="text-align: center;">$\underline{0.888}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (zero-shot)</td>
<td style="text-align: center;">$0.476 \pm 0.036$</td>
<td style="text-align: center;">$0.499 \pm 0.005$</td>
<td style="text-align: center;">$0.986 \pm 0.007$</td>
<td style="text-align: center;">$0.518 \pm 0.018$</td>
<td style="text-align: center;">$0.736 \pm 0.027$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (Scaffold, $k=4$ )</td>
<td style="text-align: center;">$0.516 \pm 0.022$</td>
<td style="text-align: center;">$0.514 \pm 0.205$</td>
<td style="text-align: center;">$0.834 \pm 0.014$</td>
<td style="text-align: center;">$0.457 \pm 0.004$</td>
<td style="text-align: center;">$0.856 \pm 0.014$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (Scaffold, $k=8$ )</td>
<td style="text-align: center;">$\mathbf{0 . 6 1 4} \pm \mathbf{0 . 0 1 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 7 9} \pm \mathbf{0 . 2 0 5}$</td>
<td style="text-align: center;">$0.836 \pm 0.020$</td>
<td style="text-align: center;">$0.737 \pm 0.004$</td>
<td style="text-align: center;">$0.856 \pm 0.014$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (random, $k=8$ )</td>
<td style="text-align: center;">$0.610 \pm 0.021$</td>
<td style="text-align: center;">$0.588 \pm 0.023$</td>
<td style="text-align: center;">$\mathbf{0 . 9 9 6} \pm \mathbf{0 . 0 0 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 7 4} \pm \mathbf{0 . 0 0 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 3 0} \pm \mathbf{0 . 0 1 0}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 (Scaffold, $k=8$ )</td>
<td style="text-align: center;">$0.463 \pm 0.007$</td>
<td style="text-align: center;">$0.496 \pm 0.016$</td>
<td style="text-align: center;">$0.864 \pm 0.018$</td>
<td style="text-align: center;">$0.572 \pm 0.026$</td>
<td style="text-align: center;">$0.578 \pm 0.029$</td>
</tr>
<tr>
<td style="text-align: left;">Davinci-003 (Scaffold, $k=8$ )</td>
<td style="text-align: center;">$0.396 \pm 0.023$</td>
<td style="text-align: center;">$0.650 \pm 0.021$</td>
<td style="text-align: center;">$0.781 \pm 0.004$</td>
<td style="text-align: center;">$0.682 \pm 0.006$</td>
<td style="text-align: center;">$0.845 \pm 0.010$</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-13B-chat (Scaffold, $k=8$ )</td>
<td style="text-align: center;">$0.002 \pm 0.003$</td>
<td style="text-align: center;">$0.048 \pm 0.017$</td>
<td style="text-align: center;">$0.048 \pm 0.025$</td>
<td style="text-align: center;">$0.053 \pm 0.011$</td>
<td style="text-align: center;">$0.002 \pm 0.004$</td>
</tr>
<tr>
<td style="text-align: left;">GAL-30B (Scaffold, $k=8$ )</td>
<td style="text-align: center;">$0.062 \pm 0.007$</td>
<td style="text-align: center;">$0.020 \pm 0.010$</td>
<td style="text-align: center;">$0.012 \pm 0.009$</td>
<td style="text-align: center;">$0.030 \pm 0.018$</td>
<td style="text-align: center;">$0.099 \pm 0.007$</td>
</tr>
</tbody>
</table>
<p>models examined, GPT-4 surpasses both Davinci-003 and GPT-3.5 in predicting molecular properties. In our investigation, we have found evidence to support that the expansion of in-context learning (ICL) instances leads to a measurable enhancement in model performance. This underlines a direct relationship between the extent of ICL data and the predictive precision of our models. Concurrently, our research presents empirical evidence that scaffold sampling exceeds the performance of random</p>
<p>sampling on three distinct datasets (BBBP, BACE, Tox21). A plausible explanation for this could be the structural resemblances between the scaffold-sampled molecules and the query molecule, which potentially biases the GPT models towards more accurate decision.</p>
<p>Label interpretation. The results presented in Table 6 and Table 7 indicate that the GPT-4 model selectively outperforms the baseline models on the HIV and ClinTox datasets. This superior performance likely stems from the inclusion of information directly related to the labels within the ICL prompts. Specifically, in the HIV dataset, the activity test results play a crucial role. Molecules tend to inhibit HIV replication when the activity test is categorized as "confirmed active" or "confirmed moderately active." For the ClinTox dataset, the FDA-approval status of a molecule acts as a predictor of its clinical toxicity. A molecule not having FDA approval is more likely to be clinically toxic. In experiments where we excluded this contextual information from the in-context learning prompts, the F1 and accuracy score of predictions notably declined, as evident from the results in Table 8 and Table 9 .</p>
<p>Table 8: Impact to F1 score of removing label context information from the in-context learning prompts.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">F1( $\uparrow$ )</th>
<th style="text-align: center;">HIV</th>
<th style="text-align: center;">ClinTox</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4(zero-shot)</td>
<td style="text-align: center;">$0.977 \pm(0.013)$</td>
<td style="text-align: center;">$0.489 \pm(0.018)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4(unlabelled, zero-shot)</td>
<td style="text-align: center;">$0.554 \pm(0.017)$</td>
<td style="text-align: center;">$0.438 \pm(0.045)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4(few-shot)</td>
<td style="text-align: center;">$0.797 \pm(0.021)$</td>
<td style="text-align: center;">$0.563 \pm(0.008)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4(unlabelled, few-shot)</td>
<td style="text-align: center;">$0.493 \pm(0.030)$</td>
<td style="text-align: center;">$0.478 \pm(0.035)$</td>
</tr>
</tbody>
</table>
<p>Table 9: Impact to accuracy of removing label context information from the in-context learning prompts.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Accuracy( $\uparrow$ )</th>
<th style="text-align: center;">HIV</th>
<th style="text-align: center;">ClinTox</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4(zero-shot)</td>
<td style="text-align: center;">$0.986 \pm(0.070)$</td>
<td style="text-align: center;">$0.736 \pm(0.027)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4(unlabelled, zero-shot)</td>
<td style="text-align: center;">$0.628 \pm(0.016)$</td>
<td style="text-align: center;">$0.602 \pm(0.039)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4(few-shot)</td>
<td style="text-align: center;">$0.836 \pm(0.020)$</td>
<td style="text-align: center;">$0.856 \pm(0.014)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4(unlabelled, few-shot)</td>
<td style="text-align: center;">$0.541 \pm(0.032)$</td>
<td style="text-align: center;">$0.630 \pm(0.014)$</td>
</tr>
</tbody>
</table>
<h1>C Yield Prediction</h1>
<p>Yield prediction [51] is a critical task in chemistry, specifically in the domain of synthetic chemistry, which involves the design and synthesis of new compounds for various applications, such as pharmaceuticals, materials, and catalysts. The yield prediction task aims to estimate the efficiency and effectiveness of a chemical reaction, primarily by quantifying the percentage of the desired product formed from the reactants. We use two High-Throughput experimentation (HTE) datasets: Buchwald-Hartwig [1] (MIT license) and Suzuki-Miyaura dataset [50] (MIT license) for evaluation. These datasets consist of reactions and their corresponding yields, which have been meticulously acquired through standardized and consistent experimental setups. This uniformity ensures that the data within each dataset is coherent, reducing the likelihood of discrepancies arising from variations in experimental procedures or conditions. We formulate the task of yield prediction as a binary classification problem, by determining whether a reaction is a high-yielding reaction or not. We used only random sampling for our ICL examples as reactions in those datasets belong to the same type. For every dataset, we randomly select 30 samples for the validation process and 100 samples for testing from the original dataset. To evaluate the results, we use the classification accuracy as the evaluation metric, with UAGNN [34] serving as baseline. UAGNN reports state-of-the-art performance on yield prediction. It takes the graphs of reactants and products as input, and learns representation of these molecules through a graph neural network, and then predicts the scaled yield .</p>
<p>ICL prompt. We show our ICL prompt for yield prediction with an example from BuchwaldHartwig dataset. As described in Figure 7, we incorporate an input explanation (wherein the reactants are separated by ' $\because$ ' and the products are split by ' $&gt;&gt;$ ') to assist large language models. Additionally, output restrictions are enforced to ensure the generation of valid results.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 7: An ICL prompt example for yield prediction</p>
<p>Results. The results are presented in Table 10. Our analysis reveals that in the task of yield prediction, GPT models perform below the established baseline model, UAGNN. However, it's worth noting that the UAGNN model was trained on the full training dataset including thousands of examples. Considering the spectrum of GPT models under scrutiny, GPT-4 emerges as the superior model, overshadowing both Davinci-003 and GPT-3.5 in predicting reaction yields. In the process of our investigation, we unearthed supporting evidence that signifies the role of ICL instances in the enhancement of model performance. This suggests an inherent correlation between the quantity of ICL data and the predictive accuracy of the models under consideration. This phenomenon is particularly in the case of GPT-4, we observed a significant improvement in performance when the number of ICL examples was increased from 4 to 8 , both in the Buchwald-Hartwig and Suzukicoupling reactions. This indicates that even within the same model architecture, the amount of contextual data can significantly influence the predictive capabilities.</p>
<p>Table 10: Accuracy ( $\uparrow$ ) of yield prediction task. $k$ is the number of examples used in few-shot ICL. The best LLM is in bold font, and the baseline is underlined.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Buchwald-Hartwig</th>
<th style="text-align: center;">Suzuki-coupling</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">UAGNN [34]</td>
<td style="text-align: center;">$\underline{0.965}$</td>
<td style="text-align: center;">$\underline{0.957}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (zero-shot)</td>
<td style="text-align: center;">$0.322 \pm 0.034$</td>
<td style="text-align: center;">$0.214 \pm 0.019$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (random, $k=8$ )</td>
<td style="text-align: center;">$\mathbf{0 . 8 0 0} \pm \mathbf{0 . 0 0 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 6 4} \pm \mathbf{0 . 0 1 3}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (random, $k=4$ )</td>
<td style="text-align: center;">$0.574 \pm 0.045$</td>
<td style="text-align: center;">$0.324 \pm 0.018$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 (random, $k=8$ )</td>
<td style="text-align: center;">$0.585 \pm 0.045$</td>
<td style="text-align: center;">$0.542 \pm 0.011$</td>
</tr>
<tr>
<td style="text-align: left;">Davinci-003 (random, $k=8$ )</td>
<td style="text-align: center;">$0.467 \pm 0.013$</td>
<td style="text-align: center;">$0.341 \pm 0.017$</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-13B-chat</td>
<td style="text-align: center;">$0.008 \pm 0.007$</td>
<td style="text-align: center;">$0.006 \pm 0.004$</td>
</tr>
<tr>
<td style="text-align: left;">GAL-30B</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$0.008 \pm 0.010$</td>
</tr>
</tbody>
</table>
<h1>D Reaction Prediction</h1>
<p>Reaction prediction is a central task in the field of chemistry, with significant implications for drug discovery, materials science, and the development of novel synthetic routes. Given a set of reactants, the goal of this task is to predict the most likely products formed during a chemical reaction $[54,13,19]$. In this task, we use the widely adopted USPTO-MIT dataset <a href="MIT license">29</a> to evaluate the performance of GPT models. This dataset contains approximately 470,000 chemical reactions extracted from US patents. In the experiment, we used the USPTO mixed data set, where the reactants and reagents strings are split by ' $\because$ '. We randomly sampled 30 samples from the original validation set for validation and 100 samples from the original test set for testing. We use the Top-1 Accuracy as the evaluation metric and Chemformer [26] as the baseline due to its superior performance among the machine learning solutions for reaction prediction. Chemformer is a seq2seq model trained to predict the output product when given reactants and reagents as input. We also report the percentage of invalid SMILES generated by each method.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 8: An ICL prompt example for reaction prediction</p>
<p>ICL Prompt. One example of our ICL prompt for reaction prediction is shown in Figure 8. Given the nature of the reaction prediction task and the characteristics of the USPTO-MIT dataset, we enhance the task-specific template with an input explanation (stating that the input includes reactants and reagents, which are separated by ' $\because$ ') to assist the GPT models in understanding the input SMILES. Moreover, we incorporate output restrictions to guide GPT models in generating chemically valid and reasonable products.</p>
<p>Table 11: The performance of LLMs and baseline in the reaction prediction task. $k$ is the number of examples used in few-shot ICL. The best LLM is in bold font, and the baseline is underlined.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Top-1 Accuracy ( $\uparrow$ )</th>
<th style="text-align: center;">Invalid SMILES ( $\downarrow$ )</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Chemformer [26]</td>
<td style="text-align: center;">$\underline{0.938}$</td>
<td style="text-align: center;">$\underline{0 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (zero-shot)</td>
<td style="text-align: center;">$0.004 \pm 0.005$</td>
<td style="text-align: center;">$17.4 \% \pm 3.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (Scaffold, $k=20$ )</td>
<td style="text-align: center;">$\mathbf{0 . 2 3 0} \pm \mathbf{0 . 0 2 2}$</td>
<td style="text-align: center;">$7.0 \% \pm 1.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (Random, $k=20$ )</td>
<td style="text-align: center;">$0.012 \pm 0.008$</td>
<td style="text-align: center;">$8.4 \% \pm 1.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (Scaffold, $k=5$ )</td>
<td style="text-align: center;">$0.182 \pm 0.015$</td>
<td style="text-align: center;">$6.6 \% \pm 1.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 (Scaffold, $k=20$ )</td>
<td style="text-align: center;">$0.184 \pm 0.005$</td>
<td style="text-align: center;">$15.6 \% \pm 2.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Davinci-003 (Scaffold, $k=20$ )</td>
<td style="text-align: center;">$0.218 \pm 0.008$</td>
<td style="text-align: center;">$11.4 \% \pm 2.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-13B-chat (Scaffold, k=20)</td>
<td style="text-align: center;">$0.032 \pm 0.013$</td>
<td style="text-align: center;">$27.8 \% \pm 5.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL-30B (Scaffold, k=5)</td>
<td style="text-align: center;">$0.036 \pm 0.011$</td>
<td style="text-align: center;">$\mathbf{5 . 2 \%} \pm \mathbf{1 . 5 \%}$</td>
</tr>
</tbody>
</table>
<p>Results. The results are reported in Table 11. We can observe that compared to the baseline, the performance of GPT models is considerably inferior, especially for the Zero-shot prompting (Top-1 Accuracy is only 0.004 and it generates $17.4 \%$ invalid SMILES). The less competitive results of GPT models can be attributed to the lack of in-depth understanding of the SMILES strings that represent reactants and products, as well as the reaction process that transforms reactants into products. It is also worth mentioning that the high accuracy achieved by Chemformer is due to its training on the complete dataset. More conclusions and detailed analysis are summarized in the section 5.</p>
<h1>E Reagents Selection</h1>
<p>Reagents selection, also known as reagent recommendation, involves the identification and proposal of the most fitting reagents for a specific chemical reaction or process. Compared to other prediction and generation tasks, these selection tasks might be more fitting for LLMs and carry extensive implications. Reagent recommendation can markedly enhance reaction design by pinpointing optimal reagents and conditions for a given reaction, thereby augmenting efficiency and effectiveness in both academic and industrial settings. Drawing from a vast corpus of chemical knowledge, GPT models may be able to generate suggestions, leading to chemical reactions with a greater likelihood of yielding superior results.</p>
<p>In this study, we formulate four reaction component selection task from the Suzuki High-Throughput Experimentation (HTE) dataset. The dataset, created by Perera et al<a href="MIT license">44</a>, evaluates the Suzuki coupling of 5 electrophiles and 7 nucleophiles across a matrix of 11 ligands (with one blank), 7 bases (with one blank), and 4 solvents, resulting in a reaction screening dataset comprising 5,760 data points. The task of reagents selection can be divided into three categories: Reactant selection, Ligand Selection and Solvent selection. For validation, 30 examples were randomly sampled, while 100 examples were used for testing, all taken from the original datasets. Top-1 Accuracy serves as the assessment metric for both reactant and solvent selection, while Top-50\% is utilized for ligand selection, as the upper half of the ligands in the list typically provide satisfactory yields in chemical reactions. This task is newly emergent in the field of chemistry, and as such, there are no established baselines yet.</p>
<p>ICL prompt. One example of our ICL prompt for reagents selection is shown in Figure 9. Considering the structure of the dataset and the characteristics of the reagents, we provide detailed task description and an answer template to guide GPT models towards the desired output.</p>
<p>Results. Our results are presented in Table 12. From the table, it is evident that GPT-4 and GPT-3.5 perform comparatively well in reagent selection tasks. This suggests a promising potential for GPT models in the realm of reagent selection.</p>
<p>Table 12: Accuracy ( $\uparrow$ ) of LLM in the reagent selection tasks. For Reactant Selection and Solvent selection task, we report the mean (and standard deviation) of the Top-1 Accuracy score and we report the Top-50\% accuracy score for the Ligand Selection task. The best LLM is in bold font, and the baseline is underlined.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Reactant Selection</th>
<th style="text-align: center;">Solvent Selection</th>
<th style="text-align: center;">Ligand Selection</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4 (zero-shot)</td>
<td style="text-align: center;">$0.299 \pm 0.029$</td>
<td style="text-align: center;">$\mathbf{0 . 5 2 6} \pm 0.012$</td>
<td style="text-align: center;">$\mathbf{0 . 5 3 4} \pm 0.059$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 (zero-shot)</td>
<td style="text-align: center;">$\mathbf{0 . 4 0 0} \pm 0.038$</td>
<td style="text-align: center;">$0.368 \pm 0.034$</td>
<td style="text-align: center;">$0.436 \pm 0.020$</td>
</tr>
<tr>
<td style="text-align: left;">Davinci-003 (zero-shot)</td>
<td style="text-align: center;">$0.178 \pm 0.034$</td>
<td style="text-align: center;">$0.463 \pm 0.014$</td>
<td style="text-align: center;">$0.432 \pm 0.020$</td>
</tr>
<tr>
<td style="text-align: left;">Llama2-13B-chat (zero-shot)</td>
<td style="text-align: center;">$0.145 \pm 0.000$</td>
<td style="text-align: center;">$0.050 \pm 0.010$</td>
<td style="text-align: center;">$0.284 \pm 0.024$</td>
</tr>
<tr>
<td style="text-align: left;">GAL-30B (zero-shot)</td>
<td style="text-align: center;">$0.107 \pm 0.020$</td>
<td style="text-align: center;">$0.104 \pm 0.004$</td>
<td style="text-align: center;">$0.030 \pm 0.016$</td>
</tr>
</tbody>
</table>
<h2>F Retrosynthesis</h2>
<p>Retrosynthesis planning is a crucial task in synthetic organic chemistry that involves identifying efficient synthetic pathways for a target molecule by recursively transforming it into simpler precursor molecules. In contrast to reaction prediction, retrosynthesis planning involves a reverse extrapolation from the target molecule to identify the readily available reactants for its synthesis. In this study, we use the USPTO-50k dataset <a href="MIT license">53</a>, which contains 50,037 chemical reactions. In our</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://pubchem.ncbi.nlm.nih.gov&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
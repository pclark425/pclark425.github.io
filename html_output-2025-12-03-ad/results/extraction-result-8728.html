<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8728 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8728</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8728</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-277467322</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.24235v3.pdf" target="_blank">A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?</a></p>
                <p><strong>Paper Abstract:</strong> As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as ``test-time computing'' has emerged as a prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q&A. However, despite the explosion of recent efforts in this area, there remains an urgent need for a comprehensive survey offering a systemic understanding. To fill this gap, we propose a unified, multidimensional framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale. Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique functional roles of individual techniques within the broader TTS landscape. From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment. Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions. Our repository is available on https://github.com/testtimescaling/testtimescaling.github.io/</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8728.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8728.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine (Iterative refinement with self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generate-then-reflect iterative inference procedure in which the same LLM alternates between producing an answer, generating feedback/critique on that answer, and then producing a revised answer using that feedback; repeats until a stopping condition or T iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified pre-trained LLM (generic)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey references the method as applied with generic pre-trained LLMs; no specific model size or training details are given in this survey entry.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Loop: y^(0) = M(x); at each step t the model produces feedback f^(t)=M(x,y^(t-1); feedback-prompt), then refines y^(t)=M(x,y^(t-1),f^(t); refine-prompt). Iterate until convergence or a pre-set number of iterations T (T is left unspecified in the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General QA / reasoning and generation tasks (not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Applied as a general-purpose iterative refinement for free-form QA, reasoning, and generation tasks where successive critique and revision can improve outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Qualitative claim in survey: 'For sufficiently powerful models, this self-iteration yields significantly better results.' No numerical performance values are reported in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not numerically reported in this survey; baseline is single-pass generation.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-based self-critique and refinement implemented by prompting the same model to generate feedback and then conditioning a follow-up generation on that feedback (no external verifier required in the basic formulation).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey states that repeating the feedback-refinement loop 'progressively polishes the output' and that 'for sufficiently powerful models' iterative self-refinement yields better results; this is a qualitative summary of prior work rather than a quantitative meta-analysis in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey notes that effectiveness depends on model capability—smaller LLMs often need strong external verifiers to self-correct; potential for reinforcing mistakes if the model's critiques are incorrect; no numeric failure cases provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Positioned vs single-pass generation and other TTS methods (e.g., self-consistency, verifier-based selection). Survey emphasizes Self-Refine requires no extra training and is complementary to verifier- or search-based techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8728.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8728.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (majority-vote over multiple chain-of-thought samples)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generate multiple independent chain-of-thought (CoT) reasoning traces and take the majority-voted answer under the assumption that consensus among diverse chains increases correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified pre-trained LLMs (survey-level discussion)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey aggregates literature using multiple LLM sampling strategies (various models/sizes referenced across works), but does not give a single model specification for self-consistency experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency / Consensus@k</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Sample k independent CoT outputs from a model (or ensemble of models) and aggregate by majority voting (or weighted voting/self-consistency) to select final answer; can be framed as Pass@k / Cons@k style coverage plus voting.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reasoning benchmarks (mathematics, multi-step QA) e.g., AIME, MATH-family tasks (representative domains mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step reasoning tasks where multiple reasoning chains can be generated and aggregated for higher reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Survey reports general empirical trend: consensus voting (self-consistency) improves accuracy and stability; no single numeric result reported in the survey itself (various cited works report task-dependent gains).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not reported numerically in the survey; baseline is single-chain CoT generation (Pass@1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Parallel repeated sampling (decode-time stimulation) combined with an aggregation/voting mechanism; decoding hyperparameters (temperature, prompt variants) are used to promote diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey cites multiple works (e.g., Wang et al., 2023b; Brown et al., 2024) that observe improved accuracy via generating multiple solutions and voting; also cites log-linear relationship between compute and coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Diminishing returns as coverage saturates; majority voting can be misled if many samples share the same incorrect bias; sample inefficiency and increased compute/token cost; potential non-monotonic behavior when scaling calls for some tasks (survey notes non-monotonicity in multi-call systems).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to Best-of-N (selection by scorer) and fusion summarization approaches; self-consistency is simple but can be outperformed by verifier-weighted selection or fusion when candidate quality varies.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8728.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8728.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Best-of-N</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Best-of-N (generate N candidates and select best by scorer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generate N candidate outputs then choose the single highest-scoring candidate according to an external or internal scoring function (verifier, execution test, or judge).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified pre-trained LLM(s)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey discusses Best-of-N as an inference-time sampling + selection technique used across multiple LLMs; no single model is evaluated within this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Best-of-N selection</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Sample N outputs (via repeated sampling or multiple models), evaluate each with a scoring function M(y_i) and select ŷ = argmax_i M(y_i); scoring can be heuristic, tool-assisted, or via a separate verifier LM.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General reasoning and generation tasks (cited uses include reasoning and code generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where some candidate outputs may be correct and selection can pick the best among many samples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Survey cites literature showing Best-of-N increases probability of getting a correct output (1 - (1-p)^N under independent p), but provides no specific numeric results in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Single-sample baseline (Pass@1); numerical baselines are left to cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Sampling diversity (temperature, prompt rephrasing), external verifier or scoring function for selection, optionally followed by distillation or RL to bake improvements into the model.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Theoretical and empirical intuition: increasing N raises coverage probability; survey references empirical findings across works that Best-of-N yields higher-quality outputs at the cost of extra compute.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Sample inefficiency, heavy compute/token cost; selection quality depends critically on verifier accuracy; naive Best-of-N may be dominated by fusion or verifier-weighted schemes when candidates are low-quality.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Contrasted with self-consistency (voting) and fusion (summary/merge) approaches; sometimes combined with selection-based verification (best-of-N with verifier).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8728.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8728.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Evaluation (prompted)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Evaluation / self-critique by prompting the same LM to evaluate its own steps</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt the same LLM to evaluate or score intermediate steps or full candidate outputs (e.g., 'judge this step') and use those evaluations to guide search, beam pruning, or refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-evaluation guided beam search for reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified pre-trained LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey references methods that prompt the same LM to act as a critic/verifier of its own output; specific model instances vary by cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Evaluation (LM-as-critic)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>At step t the LM is prompted with the current reasoning trace or candidate and asked to evaluate correctness/quality; the evaluation is then used to prune/score search branches or to trigger refinement steps.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mathematical and multi-step reasoning tasks (cited uses)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks with intermediate logical steps where step-level evaluation can detect and prune flawed branches.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Survey reports that LM-based process verifiers and self-evaluation can improve search-guided reasoning in many cited works, but no unified numerical summary is provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not numerically reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering: reuse the LLM as a verifier/critic (no external verifier), sometimes combined with external tools for more reliable checks.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey cites Xie et al. (2023) and other works demonstrating that prompting the LM to self-evaluate improves step selection in beam/search frameworks; also notes limitations in verifier reliability for complex problems.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>LM-based verifiers may be unreliable on long/complex chains and can produce false positives/negatives; survey recommends tool-assisted or external verifiers for formal domains; small models need stronger verifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to training external PRMs or using tool-assisted verification; survey notes self-evaluation is easy to integrate but less reliable than trained verifiers or external tool checks in some domains.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8728.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8728.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Reflection Feedback (Li et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Reflection Feedback (feedback utilization formulated as optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that formalizes using reflected feedback across samples as an optimization problem and adaptively propagates information between samples during test time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified LLM (survey cites Li et al., 2025g)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey refers to Li et al. (2025g) as proposing an optimization-based formulation for using self-reflection feedback; specific model details are not given in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Reflection Feedback (optimization formulation)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Aggregate feedback across generated samples, then solve an optimization that adaptively uses that feedback to improve selected outputs or guide subsequent generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General test-time feedback-utilization scenarios (paper-cited tasks unspecified in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Survey-level description: applicable where multiple candidate samples and feedback signals are available and can be propagated.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Not reported in this survey; described conceptually.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Algorithmic: treat feedback as optimization constraints or objectives and propagate feedback across samples; implementation details are in the cited work rather than in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey summarizes the proposed formulation but does not present quantitative results; claims suggest adaptive propagation of feedback can improve aggregation/selection quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey does not report empirical failure modes for this specific method; general caveats apply (verifier reliability, compute cost).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Presented alongside other verification and aggregation techniques; positioned as a more formal/optimization-driven way to use feedback compared to ad hoc heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8728.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8728.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Repetition / Repeated Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Repetition / repeated prompting (iterative prompting to produce multiple samples or refinements)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Test-time strategy of re-prompting the model (in parallel or sequentially) to produce multiple candidate outputs or to mimic a refinement loop; supports both parallel coverage and sequential revision workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs referenced across literature (survey-level discussion)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey describes this as a decoding/prompt-level stimulation strategy that can be used with many LLMs; no single model specified.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Repetition / repeated prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Invoke the model multiple times with the same or slightly varied prompts to produce multiple solutions (parallel) or to iteratively refine a single solution (sequential self-reprompting).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General reasoning and constrained-instruction tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used to encourage diversity (parallel) or to mimic refinement (sequential) in tasks requiring constrained instruction following or complex reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Survey notes improvements in coverage/diversity from repeated sampling; no unified numeric results in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Single-run generation baseline not numerically reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering and decoding-level strategies (inject filler tokens, budget-forcing, adaptive injection phrases) to encourage repeated or elongated generation and enable iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey cites Wang et al. (2023b) and others that use repeated prompting to increase coverage and reliability; also cites self-refine style sequential prompting as effective for iterative improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High compute/token cost; potential for redundant or low-quality repeated outputs; sample inefficiency can be mitigated but not eliminated by filtering/verifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Self-repetition is a lower-cost/low-complexity way to approximate parallel or sequential scaling compared to search-based or RL-based methods, but often requires better aggregation/verifiers to reach top performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8728.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8728.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree-of-Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thoughts (branching chain-of-thought search)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid iterative search approach where at each reasoning step the model proposes multiple candidate 'thoughts', branches into a tree of partial solutions, and uses evaluation/pruning to explore alternative reasoning trajectories before committing to a final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of thoughts: Deliberate problem solving with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified LLM used as thought generator and evaluator (varies by cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey describes ToT as a framework that uses an LLM to generate candidate steps and either an LM-based or heuristic verifier to evaluate/prune branches; concrete model instances vary in cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Tree-of-Thoughts (search + evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>At each node generate multiple candidate intermediate steps; evaluate candidates with a heuristic or verifier (LM or external tool); expand promising branches; iterate until goal or budget exhaustion. This implements iterative improvement by exploring multiple alternative refinements of partial solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Complex reasoning tasks (puzzles, math, planning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks that benefit from exploring alternative solution paths and from pruning using process/outcome verifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Survey reports ToT and follow-ups significantly improve reasoning on complex reasoning benchmarks in cited literature, but does not list consolidated numeric figures in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Single-path CoT baselines (single linear chain) — numerical comparisons are in original cited papers, not reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Hybrid of search (tree expansion) and verification (LM-based or tool-assisted), combining parallel generation at each branching point with iterative pruning/refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Survey cites Yao et al. (2023b) and later works showing ToT improves performance by allowing recovery from wrong turns compared to single-chain CoT; evidence is presented in cited works rather than with pooled numeric summaries here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Compute and memory cost (KV cache per trajectory), dependence on verifier quality (discriminators are bottlenecks), and non-trivial engineering for KV sharing; search can be misled if evaluator is poor.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Presented as a hybrid that combines benefits of parallel (coverage) and sequential (deep refinement) approaches; compared favorably to single-path CoT in cited literature.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback. <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models. <em>(Rating: 2)</em></li>
                <li>Self-evaluation guided beam search for reasoning. <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models. <em>(Rating: 2)</em></li>
                <li>Self-rag: Learning to retrieve, generate, and critique through self-reflection. <em>(Rating: 1)</em></li>
                <li>Self-r eflection Feedback (Li et al., 2025g) [reference in survey] <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8728",
    "paper_id": "paper-277467322",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "Self-Refine",
            "name_full": "Self-Refine (Iterative refinement with self-feedback)",
            "brief_description": "A generate-then-reflect iterative inference procedure in which the same LLM alternates between producing an answer, generating feedback/critique on that answer, and then producing a revised answer using that feedback; repeats until a stopping condition or T iterations.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback.",
            "mention_or_use": "mention",
            "model_name": "unspecified pre-trained LLM (generic)",
            "model_description": "Survey references the method as applied with generic pre-trained LLMs; no specific model size or training details are given in this survey entry.",
            "reflection_method_name": "Self-Refine",
            "reflection_method_description": "Loop: y^(0) = M(x); at each step t the model produces feedback f^(t)=M(x,y^(t-1); feedback-prompt), then refines y^(t)=M(x,y^(t-1),f^(t); refine-prompt). Iterate until convergence or a pre-set number of iterations T (T is left unspecified in the survey).",
            "task_name": "General QA / reasoning and generation tasks (not specified)",
            "task_description": "Applied as a general-purpose iterative refinement for free-form QA, reasoning, and generation tasks where successive critique and revision can improve outputs.",
            "performance_with_reflection": "Qualitative claim in survey: 'For sufficiently powerful models, this self-iteration yields significantly better results.' No numerical performance values are reported in this survey.",
            "performance_without_reflection": "Not numerically reported in this survey; baseline is single-pass generation.",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Prompt-based self-critique and refinement implemented by prompting the same model to generate feedback and then conditioning a follow-up generation on that feedback (no external verifier required in the basic formulation).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey states that repeating the feedback-refinement loop 'progressively polishes the output' and that 'for sufficiently powerful models' iterative self-refinement yields better results; this is a qualitative summary of prior work rather than a quantitative meta-analysis in this survey.",
            "limitations_or_failure_cases": "Survey notes that effectiveness depends on model capability—smaller LLMs often need strong external verifiers to self-correct; potential for reinforcing mistakes if the model's critiques are incorrect; no numeric failure cases provided.",
            "comparison_to_other_methods": "Positioned vs single-pass generation and other TTS methods (e.g., self-consistency, verifier-based selection). Survey emphasizes Self-Refine requires no extra training and is complementary to verifier- or search-based techniques.",
            "ablation_study_results": null,
            "uuid": "e8728.0",
            "source_info": {
                "paper_title": "A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (majority-vote over multiple chain-of-thought samples)",
            "brief_description": "Generate multiple independent chain-of-thought (CoT) reasoning traces and take the majority-voted answer under the assumption that consensus among diverse chains increases correctness.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models.",
            "mention_or_use": "mention",
            "model_name": "unspecified pre-trained LLMs (survey-level discussion)",
            "model_description": "Survey aggregates literature using multiple LLM sampling strategies (various models/sizes referenced across works), but does not give a single model specification for self-consistency experiments.",
            "reflection_method_name": "Self-Consistency / Consensus@k",
            "reflection_method_description": "Sample k independent CoT outputs from a model (or ensemble of models) and aggregate by majority voting (or weighted voting/self-consistency) to select final answer; can be framed as Pass@k / Cons@k style coverage plus voting.",
            "task_name": "Reasoning benchmarks (mathematics, multi-step QA) e.g., AIME, MATH-family tasks (representative domains mentioned)",
            "task_description": "Multi-step reasoning tasks where multiple reasoning chains can be generated and aggregated for higher reliability.",
            "performance_with_reflection": "Survey reports general empirical trend: consensus voting (self-consistency) improves accuracy and stability; no single numeric result reported in the survey itself (various cited works report task-dependent gains).",
            "performance_without_reflection": "Not reported numerically in the survey; baseline is single-chain CoT generation (Pass@1).",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Parallel repeated sampling (decode-time stimulation) combined with an aggregation/voting mechanism; decoding hyperparameters (temperature, prompt variants) are used to promote diversity.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey cites multiple works (e.g., Wang et al., 2023b; Brown et al., 2024) that observe improved accuracy via generating multiple solutions and voting; also cites log-linear relationship between compute and coverage.",
            "limitations_or_failure_cases": "Diminishing returns as coverage saturates; majority voting can be misled if many samples share the same incorrect bias; sample inefficiency and increased compute/token cost; potential non-monotonic behavior when scaling calls for some tasks (survey notes non-monotonicity in multi-call systems).",
            "comparison_to_other_methods": "Compared to Best-of-N (selection by scorer) and fusion summarization approaches; self-consistency is simple but can be outperformed by verifier-weighted selection or fusion when candidate quality varies.",
            "ablation_study_results": null,
            "uuid": "e8728.1",
            "source_info": {
                "paper_title": "A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Best-of-N",
            "name_full": "Best-of-N (generate N candidates and select best by scorer)",
            "brief_description": "Generate N candidate outputs then choose the single highest-scoring candidate according to an external or internal scoring function (verifier, execution test, or judge).",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "unspecified pre-trained LLM(s)",
            "model_description": "Survey discusses Best-of-N as an inference-time sampling + selection technique used across multiple LLMs; no single model is evaluated within this survey.",
            "reflection_method_name": "Best-of-N selection",
            "reflection_method_description": "Sample N outputs (via repeated sampling or multiple models), evaluate each with a scoring function M(y_i) and select ŷ = argmax_i M(y_i); scoring can be heuristic, tool-assisted, or via a separate verifier LM.",
            "task_name": "General reasoning and generation tasks (cited uses include reasoning and code generation)",
            "task_description": "Tasks where some candidate outputs may be correct and selection can pick the best among many samples.",
            "performance_with_reflection": "Survey cites literature showing Best-of-N increases probability of getting a correct output (1 - (1-p)^N under independent p), but provides no specific numeric results in this survey.",
            "performance_without_reflection": "Single-sample baseline (Pass@1); numerical baselines are left to cited works.",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Sampling diversity (temperature, prompt rephrasing), external verifier or scoring function for selection, optionally followed by distillation or RL to bake improvements into the model.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Theoretical and empirical intuition: increasing N raises coverage probability; survey references empirical findings across works that Best-of-N yields higher-quality outputs at the cost of extra compute.",
            "limitations_or_failure_cases": "Sample inefficiency, heavy compute/token cost; selection quality depends critically on verifier accuracy; naive Best-of-N may be dominated by fusion or verifier-weighted schemes when candidates are low-quality.",
            "comparison_to_other_methods": "Contrasted with self-consistency (voting) and fusion (summary/merge) approaches; sometimes combined with selection-based verification (best-of-N with verifier).",
            "ablation_study_results": null,
            "uuid": "e8728.2",
            "source_info": {
                "paper_title": "A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Self-Evaluation (prompted)",
            "name_full": "Self-Evaluation / self-critique by prompting the same LM to evaluate its own steps",
            "brief_description": "Prompt the same LLM to evaluate or score intermediate steps or full candidate outputs (e.g., 'judge this step') and use those evaluations to guide search, beam pruning, or refinement.",
            "citation_title": "Self-evaluation guided beam search for reasoning.",
            "mention_or_use": "mention",
            "model_name": "unspecified pre-trained LLM",
            "model_description": "Survey references methods that prompt the same LM to act as a critic/verifier of its own output; specific model instances vary by cited work.",
            "reflection_method_name": "Self-Evaluation (LM-as-critic)",
            "reflection_method_description": "At step t the LM is prompted with the current reasoning trace or candidate and asked to evaluate correctness/quality; the evaluation is then used to prune/score search branches or to trigger refinement steps.",
            "task_name": "Mathematical and multi-step reasoning tasks (cited uses)",
            "task_description": "Tasks with intermediate logical steps where step-level evaluation can detect and prune flawed branches.",
            "performance_with_reflection": "Survey reports that LM-based process verifiers and self-evaluation can improve search-guided reasoning in many cited works, but no unified numerical summary is provided in the survey.",
            "performance_without_reflection": "Not numerically reported in the survey.",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Prompt engineering: reuse the LLM as a verifier/critic (no external verifier), sometimes combined with external tools for more reliable checks.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey cites Xie et al. (2023) and other works demonstrating that prompting the LM to self-evaluate improves step selection in beam/search frameworks; also notes limitations in verifier reliability for complex problems.",
            "limitations_or_failure_cases": "LM-based verifiers may be unreliable on long/complex chains and can produce false positives/negatives; survey recommends tool-assisted or external verifiers for formal domains; small models need stronger verifiers.",
            "comparison_to_other_methods": "Compared to training external PRMs or using tool-assisted verification; survey notes self-evaluation is easy to integrate but less reliable than trained verifiers or external tool checks in some domains.",
            "ablation_study_results": null,
            "uuid": "e8728.3",
            "source_info": {
                "paper_title": "A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Self-Reflection Feedback (Li et al.)",
            "name_full": "Self-Reflection Feedback (feedback utilization formulated as optimization)",
            "brief_description": "A method that formalizes using reflected feedback across samples as an optimization problem and adaptively propagates information between samples during test time.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "unspecified LLM (survey cites Li et al., 2025g)",
            "model_description": "Survey refers to Li et al. (2025g) as proposing an optimization-based formulation for using self-reflection feedback; specific model details are not given in the survey.",
            "reflection_method_name": "Self-Reflection Feedback (optimization formulation)",
            "reflection_method_description": "Aggregate feedback across generated samples, then solve an optimization that adaptively uses that feedback to improve selected outputs or guide subsequent generation.",
            "task_name": "General test-time feedback-utilization scenarios (paper-cited tasks unspecified in survey)",
            "task_description": "Survey-level description: applicable where multiple candidate samples and feedback signals are available and can be propagated.",
            "performance_with_reflection": "Not reported in this survey; described conceptually.",
            "performance_without_reflection": "Not reported.",
            "has_performance_comparison": null,
            "mechanism_of_reflection": "Algorithmic: treat feedback as optimization constraints or objectives and propagate feedback across samples; implementation details are in the cited work rather than in this survey.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey summarizes the proposed formulation but does not present quantitative results; claims suggest adaptive propagation of feedback can improve aggregation/selection quality.",
            "limitations_or_failure_cases": "Survey does not report empirical failure modes for this specific method; general caveats apply (verifier reliability, compute cost).",
            "comparison_to_other_methods": "Presented alongside other verification and aggregation techniques; positioned as a more formal/optimization-driven way to use feedback compared to ad hoc heuristics.",
            "ablation_study_results": null,
            "uuid": "e8728.4",
            "source_info": {
                "paper_title": "A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Self-Repetition / Repeated Prompting",
            "name_full": "Self-Repetition / repeated prompting (iterative prompting to produce multiple samples or refinements)",
            "brief_description": "Test-time strategy of re-prompting the model (in parallel or sequentially) to produce multiple candidate outputs or to mimic a refinement loop; supports both parallel coverage and sequential revision workflows.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "various LLMs referenced across literature (survey-level discussion)",
            "model_description": "Survey describes this as a decoding/prompt-level stimulation strategy that can be used with many LLMs; no single model specified.",
            "reflection_method_name": "Self-Repetition / repeated prompting",
            "reflection_method_description": "Invoke the model multiple times with the same or slightly varied prompts to produce multiple solutions (parallel) or to iteratively refine a single solution (sequential self-reprompting).",
            "task_name": "General reasoning and constrained-instruction tasks",
            "task_description": "Used to encourage diversity (parallel) or to mimic refinement (sequential) in tasks requiring constrained instruction following or complex reasoning.",
            "performance_with_reflection": "Survey notes improvements in coverage/diversity from repeated sampling; no unified numeric results in the survey text.",
            "performance_without_reflection": "Single-run generation baseline not numerically reported here.",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Prompt engineering and decoding-level strategies (inject filler tokens, budget-forcing, adaptive injection phrases) to encourage repeated or elongated generation and enable iterative refinement.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey cites Wang et al. (2023b) and others that use repeated prompting to increase coverage and reliability; also cites self-refine style sequential prompting as effective for iterative improvement.",
            "limitations_or_failure_cases": "High compute/token cost; potential for redundant or low-quality repeated outputs; sample inefficiency can be mitigated but not eliminated by filtering/verifiers.",
            "comparison_to_other_methods": "Self-repetition is a lower-cost/low-complexity way to approximate parallel or sequential scaling compared to search-based or RL-based methods, but often requires better aggregation/verifiers to reach top performance.",
            "ablation_study_results": null,
            "uuid": "e8728.5",
            "source_info": {
                "paper_title": "A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Tree-of-Thoughts (ToT)",
            "name_full": "Tree-of-Thoughts (branching chain-of-thought search)",
            "brief_description": "A hybrid iterative search approach where at each reasoning step the model proposes multiple candidate 'thoughts', branches into a tree of partial solutions, and uses evaluation/pruning to explore alternative reasoning trajectories before committing to a final answer.",
            "citation_title": "Tree of thoughts: Deliberate problem solving with large language models.",
            "mention_or_use": "mention",
            "model_name": "unspecified LLM used as thought generator and evaluator (varies by cited work)",
            "model_description": "Survey describes ToT as a framework that uses an LLM to generate candidate steps and either an LM-based or heuristic verifier to evaluate/prune branches; concrete model instances vary in cited works.",
            "reflection_method_name": "Tree-of-Thoughts (search + evaluation)",
            "reflection_method_description": "At each node generate multiple candidate intermediate steps; evaluate candidates with a heuristic or verifier (LM or external tool); expand promising branches; iterate until goal or budget exhaustion. This implements iterative improvement by exploring multiple alternative refinements of partial solutions.",
            "task_name": "Complex reasoning tasks (puzzles, math, planning)",
            "task_description": "Tasks that benefit from exploring alternative solution paths and from pruning using process/outcome verifiers.",
            "performance_with_reflection": "Survey reports ToT and follow-ups significantly improve reasoning on complex reasoning benchmarks in cited literature, but does not list consolidated numeric figures in the survey text.",
            "performance_without_reflection": "Single-path CoT baselines (single linear chain) — numerical comparisons are in original cited papers, not reproduced here.",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Hybrid of search (tree expansion) and verification (LM-based or tool-assisted), combining parallel generation at each branching point with iterative pruning/refinement.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Survey cites Yao et al. (2023b) and later works showing ToT improves performance by allowing recovery from wrong turns compared to single-chain CoT; evidence is presented in cited works rather than with pooled numeric summaries here.",
            "limitations_or_failure_cases": "Compute and memory cost (KV cache per trajectory), dependence on verifier quality (discriminators are bottlenecks), and non-trivial engineering for KV sharing; search can be misled if evaluator is poor.",
            "comparison_to_other_methods": "Presented as a hybrid that combines benefits of parallel (coverage) and sequential (deep refinement) approaches; compared favorably to single-path CoT in cited literature.",
            "ablation_study_results": null,
            "uuid": "e8728.6",
            "source_info": {
                "paper_title": "A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback.",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models.",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Self-evaluation guided beam search for reasoning.",
            "rating": 2,
            "sanitized_title": "selfevaluation_guided_beam_search_for_reasoning"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models.",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Self-rag: Learning to retrieve, generate, and critique through self-reflection.",
            "rating": 1,
            "sanitized_title": "selfrag_learning_to_retrieve_generate_and_critique_through_selfreflection"
        },
        {
            "paper_title": "Self-r eflection Feedback (Li et al., 2025g) [reference in survey]",
            "rating": 1,
            "sanitized_title": "selfr_eflection_feedback_li_et_al_2025g_reference_in_survey"
        }
    ],
    "cost": 0.023273,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well
4 May 2025</p>
<p>Qiyuan Zhang 
City University of Hong Kong</p>
<p>Fuyuan Lyu fuyuan.lyu@mail.mcgill.ca 
McGill University &amp; MILA</p>
<p>Zexu Sun 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Lei Wang 
Salesforce AI Research</p>
<p>Weixu Zhang 
McGill University &amp; MILA</p>
<p>Wenyue Hua 
University of California
Santa Barbara</p>
<p>Haolun Wu 
McGill University &amp; MILA</p>
<p>Stanford University</p>
<p>Zhihan Guo 
Chinese University of Hong</p>
<p>Yufei Wang 
Macquarie University</p>
<p>Niklas Muennighoff 
Stanford University</p>
<p>Irwin King 
Chinese University of Hong</p>
<p>Xue Liu 
McGill University &amp; MILA</p>
<p>Chen Ma 
City University of Hong Kong</p>
<p>Kong 
A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well
4 May 2025A8719B221F0999E514C38D675D2F617BarXiv:2503.24235v3[cs.CL]
As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS)-also referred to as "test-time computing"-has emerged as a prominent research focus.Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q&amp;A.However, despite the explosion of recent efforts in this area, there remains an urgent need for a comprehensive survey offering systemic understanding.To fill this gap, we propose a unified, hierarchical framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale.Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique contributions of individual techniques within the broader TTS landscape.From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment.Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions.Our repository is available on https://github.com/testtimescaling/testtimescaling.github.io/.</p>
<p>Introduction</p>
<p>Large language models (LLMs) (Brown et al., 2020;OpenAI, 2024a) have emerged in recent years as a transformative milestone toward artificial general intelligence (AGI) (Goertzel, 2014;Bubeck et al., 2023).These models remarkably learn general intelligence by training-time scaling, where the models ingest more data and parameters (Kaplan et al., 2020;Hoffmann et al., 2022).However, the progress of pretraining scaling has gradually slowed due to its resource-intensive nature and the bounded availability of human data, prompting researchers to shift their focus toward how to fully elicit the intelligence encoded in LLMs at test time to maximize their real-world effectiveness (Wei et al., 2022;Ouyang et al., 2022;Li et al., 2024d)?</p>
<p>Human cognition may suggest a clue.When faced with complex problems, people tend to engage in deeper, more deliberate thinking, often producing better outcomes (Kahneman, 2011(Kahneman, , 2003;;Evans, 1984).Inspired by this principle, recent research (Wei et al., 2022;Wang et al., 2023b) has introduced methods that allocate additional computation during inference to boost task performance.Notably, some studies (Brown et al., 2024;Wu et al., 2024d) observe patterns akin to scaling laws: increasing inference-time compute yields consistent performance improvements.This family of methods, referred to as test-time scaling (TTS), progressively elicits the model's intelligence in the test-time, as depicted in Figure 1.The remarkable successes of reasoning models, such as o1 (OpenAI, 2024b) and R1 (DeepSeek-AI, 2025), have further amplified interest in TTS, highlighting its potential as a key driver of LLM reasoning and utility.However, despite this surge in research activity, the field currently lacks a unified and systematic framework to synthesize insights, compare techniques, or identify consistent trends in TTS .To address this gap, we present a comprehensive survey of TTS, offering a hierarchical and extensible framework to analyze methods, map research efforts, and guide future progress.</p>
<p>To the best of our knowledge, this is the first survey to comprehensively examine TTS across multiple orthogonal dimensions, offering a structured perspective for both theoretical inquiry and practical deployment.Our framework dissects TTS into four key dimensions: what to scale, how to scale, where to scale, and how well to scale.Our work emphasizes a fine-grained, decomposition-based understanding of TTS.While prior efforts have examined TTS from specific lenses-such as input modification and output verification (Snell et al., 2024), or through the lens of System 2 AI and Long Chain-of-Thought (CoT) (Li et al., 2025h;Ji et al., 2025;Chen et al., 2025c)-these works are structured around a timeline, tracing the evolution of techniques over time.We analyze the full pipeline, from scaling formulations and algorithmic mechanisms to task domains and performance dimensions.We provide a structured foundation that allows future research to be seamlessly integrated into our taxonomy, making it easier to understand their contributions.Specifically, what to scale in Sec. 2 is about what to be scaled at the inference stage.How to scale in Sec. 3 depicts how they are implemented.We categorize various techniques, recognizing that a single work may involve multiple techniques; Where to scale in Sec. 4 covers the tasks and datasets where these techniques are applied.Finally, how well to scale inSec.5 refers to evaluating the different attributions of TTS methods.We further provide fine-grained subcategories under each axis and systematically map representative works to highlight their contributions and trade-offs (Sec.6).From this structured analysis, we extract major trends in TTS development and offer hands-on guidance (Sec.7) for real-world deployment.Grounded in our multi-dimensional taxonomy, we also identify persistent challenges and promising research directions (Sec.8): advancing test-time scalability, clarifying the fundamental essence of the effectiveness of different techniques in TTS, broadening generalization to a wider range of downstream tasks, and optimizing TTS methods along additional dimensions such as efficiency.</p>
<p>Our contributions are threefold:</p>
<p>1.A Unified, Multi-Dimensional Taxonomy.We propose a four-axis taxonomy-what to scale, how to scale, where to scale, and how well to scale-that supports structured classification, comparison, and extensibility for TTS methods.</p>
<ol>
<li>
<p>Systematical Literature Organization and Pragmatic Analysis.Using our taxonomy, we survey the TTS landscape, analyze representative methods, and present guidelines for research application and deployment.</p>
</li>
<li>
<p>Challenges, Insights, and Forward Directions.Building on our organized perspective, we uncover critical challenges-ranging from advancing scaling to clarifying essence-and outline promising research directions that could shape future progress.Our unified framework facilitates the mapping of these open questions to concrete dimensions of TTS, enabling more targeted and impactful advancements.</p>
</li>
</ol>
<p>We plan to continuously update our taxonomy to reflect ongoing progress and provide an evolving foundation for organizing future developments in TTS research.</p>
<p>What to Scale</p>
<p>"What to scale" refers to the specific form of TTS that is expanded or adjusted to enhance an LLM's performance during inference.When applying TTS , researchers typically choose a specific "what to scale" based on an empirical hypothesis, aiming to achieve performance gains.For example, some researchers hypothesize that Parallel Scaling ( §2.1) Self-Consistency (Brown et al., 2024;Irvine et al., 2023;Song et al., 2024;Snell et al., 2024;Wang et al., 2023b;Nguyen et al., 2024) (Chen et al., 2024e;Wu et al., 2025b), Multi-Agents (Jiang et al., 2023), PlanSearch (Wang et al., 2024a), CCE (Zhang et al., 2025e) Sequential Scaling ( §2.2) Self-Refine (Madaan et al., 2023;Chen et al., 2024h;Gou et al., 2024;Zhang et al., 2024d), Sequential Revision (Lee et al., 2025), ReAct (Yao et al., 2023c), Budget-aware (Kimi, 2025;Muennighoff et al., 2025;Han et al., 2025), RecurrentBlock (Geiping et al., 2025), STaR (Yuan et al., 2023;Singh et al., 2024), Meta-STaR (Xiang et al., 2025), PlanningToken (Wang et al., 2024g), RaLU (Li et al., 2025c) Hybrid Scaling ( §2.3) MoA (Wang et al., 2025a), Tree of Thoughts (Yao et al., 2023b;Zhang et al., 2024b), Graph of Thoughts (Besta et al., 2024), Tree-Search (Chen et al., 2024j), SoS (Gandhi et al., 2024), REBASE (Wu et al., 2024d), OAIF (Guo et al., 2024), Beam-Search (Guo et al., 2024),M-CTS (Tian et al., 2024;Zhang et al., 2024e;Gao et al., 2024b;Wan et al., 2024;Chen et al., 2024a), Journey Learning (Qin et al., 2024),A-daptiveAlloc (Snell et al., 2024;Ong et al., 2025), METAL (Li et al., 2025a), rStar-Math (Guan et al., 2025),AtomThink (Xiang et al., 2024) Internal Scaling ( §2.4) DeepSeek-R1 (DeepSeek-AI, 2025), OpenAI-o1&amp;o3 (OpenAI, 2024b, 2025), Gemini Flash Thinking (Google, 2024), QwQ (Qwen, 2024), K1.5 (Kimi, 2025), 3SUM (Pfau et al., 2024), OAIF (Guo et al., 2024), LIMO (Ye et al., 2025), T1 (Hou et al., 2025), Distilled-o1 (Huang et al., 2024c), RedStar (Xu et al., 2025a), SKY-T1 (NovaSky, 2025), s1 (Muennighoff et al., 2025), ITT (Hao et al., 2024) How to Scale ( §3)
Controllability ( §5.3)
Control Metric (Muennighoff et al., 2025), Length Deviation (Aggarwal and Welleck, 2025),k-ϵ Controllability (Bhargava et al., 2024), etc. Scalability ( §5.4) Scaling Metric (Muennighoff et al., 2025), Scaling Curves (Accuracy vs. Compute) (Aggarwal and Welleck, 2025;Teng et al., 2025), etc. 2.1 Parallel Scaling longer CoTs improve complex reasoning, leading them to enforce longer outputs from LLMs.Others leverage the self-consistency principle, assuming that generating multiple solutions to a reasoning task increases the likelihood of reaching the correct answer.</p>
<p>Parallel Scaling</p>
<p>LLMs typically generate a single response per query.Parallel scaling improves test-time performance by generating multiple outputs in parallel and then aggregating them into a final answer.Formally, consider a problem set P and a collection of models m ∈ {1, . . ., M }.Each model generates k m candidate responses for a given problem p ∈ P, producing a set of sampled solutions S:
S = {s m,i | m ≤ M, i ≤ k m }, ⇒ (∃ŝ) ŝ = A(s 1,1 , . . . , s M,k M ) is correct. (1)
Here, A is the aggregation function that derives a final response from the set S. The effectiveness of parallel scaling depends on both coverage-the likelihood of generating at least one correct response-and aggregation quality, which determines whether a correct response is successfully identified.This approach is supported by both theory and intuition: cognitive science research (Stanovich and West, 2000) suggests that complex problems often allow multiple valid solution paths, and increasing the number of generated responses improves the chance of finding a correct one (Li et al., 2025d).Empirically, this relationship is often log-linear with respect to compute (Brown et al., 2024).</p>
<p>We categorize parallel scaling into two common forms based on different sources of coverage: (1) repeated sampling from a single model and (2) sampling across multiple models.Furthermore, there are some additional techniques to enhance solution diversity and reliability, such as hyperparameter adjustments (e.g., sampling temperature (Renze, 2024) to control output variability) and input modifications (e.g., prompt rephrasing (Lambert et al., 2025) to elicit diverse responses).</p>
<p>Sequential Scaling</p>
<p>Sequential scaling involves explicitly directing later computations based on intermediate steps.Unlike parallel methods, sequential scaling updates intermediate states iteratively.We denote the partial solution states (subproblem results, or initial drafts) by n 1 , n 2 , . . ., n T , with each new state n t+1 = R(n t , p) incorporating both the previous state and the problem context.Because many problems require deliberation rather than immediate pattern matching, single-pass 'System 1' (Yu et al., 2024c)-style generation often fails on complex reasoning tasks.Iterative methods emulate a 'System 2' approach, breaking down and refining the solution step by step.</p>
<p>Early work like chain-of-thought prompting (Wei et al., 2022) motivated solve the problem step-by-step, n t+1 = AppendStep(n t , new reasoning step), leading to approaches that refine responses (Madaan et al., 2023), n t+1 = Refine(n t ), or break down problems systematically (Zhou et al., 2023a;Zelikman et al., 2022), n t+1 = IntegrateSub n t , solution to next subproblem .Subsequent studies show that iterative revision (Chen et al., 2024h;Gou et al., 2024;Chen et al., 2025d;Snell et al., 2024) triggers self-correction, improving accuracy on challenging tasks In practice, real-world tasks often demand more flexible and potentially non-linear reasoning paths, suggesting that purely sequential approaches, while effective, may be only one part of a broader solution.</p>
<p>Hybrid Scaling</p>
<p>Hybrid scaling exploits the complementary benefits of parallel and sequential scaling.Parallel scaling mitigates the risk of the model missing the correct line of thought by casting a wide net, while sequential scaling allows deep exploration of a line of reasoning once it seems promising.Formally, let F t be the set of candidate solutions at iteration t.Each iteration expands these candidates in parallel with an expansion function E and sequentially filters them with a selection function S:
F t+1 = S s∈Ft E(s) .
(2)</p>
<p>After T iterations, an aggregator A selects the final solution ŝ ∈ F T .From a cognitive standpoint, such a combination mirrors how human problem-solvers generate multiple hypotheses (divergent thinking) and then refine/evaluate them (convergent thinking).Classic search algorithms (e.g., iterative deepening (Chen et al., 2025d) and beam search (Snell et al., 2024)) embody this strategy by balancing exploration and exploitation.</p>
<p>Recent work expands on this idea.Tree-of-Thoughts (ToT) (Yao et al., 2023b) branches at decision points, exploring multiple reasoning paths before pruning to a single sequence.Follow-up methods, such as Graph-of-Thoughts (Besta et al., 2024), Algorithm-of-Thought (Sel et al., 2024), Forest-of-Thought (Bi et al., 2024), Monte Carlo Tree Search (MCTS) (Lin et al., 2025), and multi-agent reasoning (Wang et al., 2025a;Chen et al., 2024f), leverage similar but more complex hybrid patterns.For instance, multiple LLMs can debate or verify each other's answers (Liang et al., 2024;Schaul, 2024), while "journey learning" and "tool-augmented reasoning" (Li et al., 2025b) emphasize capturing full reasoning trajectories.</p>
<p>Internal Scaling</p>
<p>Internal scaling elicits a model to autonomously determine how much computation to allocate for reasoning during testing within the model's internal parameters instead of depending on external human-guided strategies.Formally, we update an initial model M 0 to a new model M 1 via a training procedure, Φ : (M 0 , D) → M 1 , on data D that includes multi-step reasoning tasks (e.g., long CoT examples produced by external scaling (Qin et al., 2024)).Surprisingly, employing outcome-oriented reward modeling (DeepSeek-AI, 2025;OpenAI, 2024b) for RL enables the model to extend its reasoning process autonomously.</p>
<p>At test time, M 1 generates a sequence of internal states z 1 , z 2 , . . ., z T via
z t+1 = f θ (z t ), stop(z t ) = π θ (z t ).(3)
The model's learned policy π θ controls when to halt.This internal feedback loop can lead to emergent behaviors-such as more detailed reasoning chains or self-evaluation steps-without any external prompts or multi-call orchestration.In practice, internal scaling often rivals or surpasses standard techniques, thanks to its ability to focus computational effort on a single, coherent reasoning trajectory.</p>
<p>3 How to Scale</p>
<p>Tuning-based Approaches</p>
<p>To activate a model's ability to devote cost at test time, directly tuning its parameters is an effective strategy.This includes two approaches: 1) Supervised Finetuning (SFT): Training an LLM via next-token prediction on synthetic or distilled long CoTs enables it to imitate and internalize structured reasoning patterns, effectively learning to think through complex problems.By mimicking extended rationales, SFT reduces the reliance on explicit prompting at inference time.2) Reinforcement Learning (RL): By leveraging feedback from a reward model on inference tasks, the policy model is automatically updated.Although no supervised data is introduced, the model autonomously generates long CoT reasoning while ensuring reliable answers.We divide the RL for internal scaling works into two perspectives.The reward model-based methods and the reward model-free methods.</p>
<p>Supervised Finetuning (SFT)</p>
<p>Training an LLM via next-token prediction on synthetic or distilled long CoTs enables it to internalize structured reasoning patterns and effectively "think" through complex problems.By mimicking extended rationales, SFT reduces the reliance on explicit prompting at inference time.This will include three subsections: (1) Imitation, describing techniques like MCTS used to generate CoT-style demonstrations for fine-tuning, (2) Distillation, summarizing how student models are trained using outputs from stronger models (e.g., o1, R1), and (3) Warmup, stabilizing learning and aligning the model's behavior to produce useful step-by-step reasoning.</p>
<p>Imitation A prominent approach to enhancing LLM reasoning via SFT is to generate long CoT demonstrations using test-time "planner" algorithms and then fine-tune the model to imitate those demonstrations.For example, STaR (Zelikman et al., 2022) uses the model itself to generate step-by-step solutions for a given problem and filters for correct outcomes, treating the verified solutions as new demonstrations to fine-tune.More structured search has been applied to generate even higher-quality traces: ReST-MCTS (Zhang et al., 2024a) integrates an MCTS planner (guided by a learned value model) to explore the space of possible reasoning steps; the model is subsequently fine-tuned on these search-generated traces, i.e., it learns to imitate the successful reasoning trajectories discovered by the planner.Warmup SFT warmup (Luong et al., 2024) refers to an initial SFT phase applied to an LLM after its unsupervised pretraining but before other post-training steps like RL.This stage stabilizes subsequent training by providing a well-initialized model that adapts better to preference optimization and avoids instability due to ungrounded behavior (Zeng et al., 2025c).Effective SFT warmup is characterized by several key elements: (i) the use of highquality, task-relevant datasets (Luong et al., 2024); (ii) short duration; (iii) a tailored learning rate schedule (Pareja et al., 2024).Technically, SFT warmup is often integrated with methods like rejection sampling (Pareja et al., 2024)-which uses warm-started models to generate high-quality data for further training.</p>
<p>Reinforcement Learning (RL)</p>
<p>Reward model-free.Recent advancements in RL and preference optimization have significantly enhanced the performance of large language models, particularly in reasoning and problem-solving tasks.A key innovation in this domain is the introduction of RL with verifiable reward by DeepSeek R1 (DeepSeek-AI, 2025), which leverages rule-based reward mechanisms to optimize models efficiently and reliably.This approach has sparked growing interest among researchers working on large models, as it addresses challenges such as sparse rewards and training instability by providing dense feedback for policy optimization.Several methods have been developed to improve exploration and accuracy in reasoning tasks through preference optimization.For instance, cDPO (Lin et al., 2024), CPL (Wang et al., 2024f), Focused-DPO (Zhang et al., 2025b), DAPO (Liu et al., 2024b), and RFTT (Zhang et al., 2025c) prioritize critical or error-prone areas, enhancing internal scaling and reasoning accuracy.Additionally, Selective DPO (Gao et al., 2025b) emphasizes the importance of aligning data difficulty with model capacity by filtering out overly challenging examples, further refining the training process.VC-PPO (Yuan et al., 2025) investigates the failure of PPO for the long CoT task and uses a pre-trained value model to achieve better results.Light-R1 (Wen et al., 2025) proposes a curriculum training framework for increasing data difficulty combined with multi-staged post-training.SimPO (Meng et al., 2024) uses the average log probability of a sequence as the implicit reward and removes the reference model in DPO.</p>
<p>In the realm of mathematical problem-solving, DQO (Ji et al., 2024) and OREO (Wang et al., 2024b) propose novel value function optimization techniques, demonstrating improvements in model performance.DAPO (Yu et al., 2025) leverages dynamic sampling for large-scale RL systems.These advancements are complemented by a range of open-source training frameworks that have equipped researchers and developers with tools to optimize training and enhance inference.Early frameworks like SimpleRL (Zeng et al., 2025b) and DeepScaler (Luo et al., 2025b) quickly replicated the technology stack of DeepSeek R1.Furthermore, SimpleRL-Zoo (Zeng et al., 2025a) presents more experimental details about SimpleRL.Others, such as X-R1 (X-R1Team, 2025) and TinyZero (Pan et al., 2025b), focus on delivering an intuitive and cost-effective user experience.Notably, Open-Reasoner-Zero (Hu et al., 2025b) replicated the DeepSeek R1-zero training scheme using a 32B model, achieving comparable performance.Further advancements in RL for internal scaling have been facilitated by frameworks like OpenR (Wang et al., 2024c), OpenRLHF (Hu et al., 2024), OpenR1 (HuggingFace, 2025), Logic-RL (Xie et al., 2025) and AReaL(AntResearch-RL-Lab, 2025).These frameworks have enhanced the replication of internal scaling and, through open-source sharing, accelerated academic research progress.The above developments not only address key challenges in RL but also pave the way for more efficient and reliable model training and deployment.</p>
<p>Reward model-based.With a Bradley-Terry model (Zheng et al., 2023b) optimized by human preference as the reward model, PPO (Schulman et al., 2017) stands as one of the most influential algorithms with its efficiency and stability and is widely used for internal scaling.Building upon PPO, ReMax (Li et al., 2023b) introduces variance reduction techniques along with REINFORCE (Sutton et al., 1999) and RLOO (Ahmadian et al., 2024) methods.This eliminates the need for additional value models in PPO, reduces over four hyperparameters, lowers GPU memory usage, and speeds up the training process.GRPO (Shao et al., 2024) replaces traditional value models with improved sampling strategies.This significantly accelerates the learning process and achieves performance comparable to GPT-4 in mathematics.REINFORCE++ (Hu et al., 2025a) further simplifies GRPO and enhances its training.DVPO (Huang et al., 2025a) presents a streamlined framework, substituting the reward model with a pre-trained global value model and removing the dependency between the actor and critic.PRIME (Cui et al., 2025) integrates the SFT model as a PRM within a unified RL framework, allowing online updates through policy rollouts and outcome labels via implicit process rewards.SPPD (Yi et al., 2025) utilizes process preference learning with a dynamic value margin for self-training.Recently, several works have focused on other challenges of existing reward model-based methods.UGDA (Sun et al., 2025) leverages the uncertainty and influence of samples during PPO training and iteratively refines the reward model.VinePPO (Kazemnejad et al., 2024) exploits the flexibility of language environments to compute unbiased Monte Carlo-based estimates, avoiding the need for large value networks.LCPO (Aggarwal and Welleck, 2025) focuses on optimizing accuracy and adherence to user-specified length constraints for reasoning tasks.Rest-MCTS* (Zhang et al., 2024a) uses tree-search-based RL to bypass per-step manual annotation typically required for training process rewards.These advancements and refinements in algorithms continue to drive the field of reinforcement learning for internal scaling, offering more effective tools and methods for solving complex problems.</p>
<p>Inference-based Approaches</p>
<p>Unlike training-based approaches, which adjust the model's parameters offline, inference-based approaches dynamically adjust computation during deployment.This paradigm includes four essential components: (i) Stimulation, which encourages the model to generate longer or multiple candidate outputs; (ii) Verification, which filters or scores outputs based on correctness or other criteria; (iii) Search, which systematically explores the sample space; and (iv) Aggregation, which consolidates multiple outputs into the final output.These four components are often used in combination to allocate test-time computation more effectively and boost performance on complex reasoning tasks.In the following sections, we provide detailed discussions of each component.</p>
<p>Stimulation</p>
<p>Stimulation techniques are the first step in encouraging the model to allocate more computation to thinking.It basically stimulates the LLM to generate (i) longer samplers and (ii) more samples instead of generating single and short samples via naive prompting.This includes several key approaches:</p>
<p>Prompt Strategy.Instead of allowing the model to generate an answer directly, one way to stimulate the scaling of LLM during test time is through the prompt.This behavior requires the backbone LLM's ability to follow instructions.For instance, prompts can guide the model toward step-by-step reasoning.Simple modifications such as adding explicit instructions (e.g., "Please think step by step.")can improve the model's ability to break down complex problems into intermediate steps (Lightman et al., 2023).This strategy ensures more deliberate and structured thought generation by shaping the reasoning process at the input level.Other techniques such as (Wei et al., 2022;Ranaldi et al., 2025) also rely on explicitly stating the requirements in the prompt to stimulate samples during the TTS .</p>
<p>Decode Strategy Rather than passively accepting the model's default output behavior, this approach modifies the decoding process to encourage LLM to generate longer, more detailed samples adaptively.Techniques such as injecting filler token (Pfau et al., 2024), adaptively injecting predefined injection phrase (Jin et al., 2020), forcing scaling budget (Muennighoff et al., 2025), enforcing intermediate generation (Li et al., 2025f), or predictive decoding (Ma et al., 2025a) allow the model to modify its distribution progressively.Enforcing extended reasoning at the output level enables the model to think longer and generate more comprehensive solutions without requiring additional external guidance.</p>
<p>Latent Strategy Unlike strategies that rely on token-level instructions or output expansion, latent strategies encourage deeper or recurrent thinking within the hidden representations themselves, effectively scaling up test-time computation through continuous internal states.For example, Hao et al. (2024) propose a paradigm where the model completes reasoning steps entirely in hidden space before producing the final answer; Kong et al. (2025) introduce a latent-thought framework that conditions text generation on an inferred latent variable to guide more thorough or expansive reasoning, while Shen et al. (2025c) show that compressing CoT into continuous embeddings can preserve intermediate reasoning fidelity without lengthy textual traces.Other approaches (Saunshi et al., 2025) harness looped or recurrent inference to repeatedly refine hidden states, effectively unfolding multiple "thinking iterations" in a single forward pass.</p>
<p>Self-Repetition Strategy Apart from generating longer samples, another way to stimulate the LLM is to generate multiple samples instead of individual ones.One commonly adopted strategy is to prompt the LLM repeatedly during the decoding stage, commonly known as self-repetition (Wang et al., 2023b).Another strategy is to prompt the LLM sequentially, in order to mimic refinement process (Madaan et al., 2023) or correlation under constraint (Ferraz et al., 2024).</p>
<p>Mixture-of-Model Strategy Gathering the "wisdom of the crowd" can move beyond repeated sampling from a single model to coordinated sampling across multiple models.These LLMs can play either homogeneous roles (Wang et al., 2025a) or heterogeneous roles (Chen et al., 2024i;He et al., 2025) during the process.By harnessing diverse perspectives, such multi-model strategy not only increases the coverage of possible solutions but also improves the system's overall robustness.</p>
<p>Category</p>
<p>Approach Approach Description</p>
<p>CoT (Wei et al., 2022) Contains a series of intermediate reasoning steps in prompts Step-by-step (Lightman et al., 2023) Stimulate step-by-step thinking via prompt QuaSAR (Ranaldi et al., 2025) Decompose CoT into Quasi-Symbolic Language CoD (Xu et al., 2025b) Generate concrete representations and distill into concise equation Hint-infer (Li et al., 2025b) Inserting artificially designed hints in the prompt Think (Li et al., 2025b) Prompt LLM with "Think before response"</p>
<p>Prompt</p>
<p>Think About World (Jin et al., 2024) Prompt LLM with "Think About the World" to enforce larger inference Filler-token (Pfau et al., 2024) uses arbitrary, irrelevant filler tokens before answering Budget-forcing (Muennighoff et al., 2025) suppress the generation of the end-of-thinking token AFT (Li et al., 2025f) iteratively aggregating proposals and aggregate for future proposals Predictive-Decoding (Ma et al., 2025a) re-weight decoding distribution given evaluation of foresight</p>
<p>Decode</p>
<p>Adaptive Injection (Jin et al., 2025) Injecting a predefined injection phrase under certain condition</p>
<p>Coconut (Hao et al., 2024) Perform chain-of-thought in hidden space without explicit token generation CoDI (Shen et al., 2025c) Compress chain-of-thought into continuous vectors via self-distillation Looped (Recurrent) Transformers (Saunshi et al., 2025) Unroll model depth at inference by repeatedly refining hidden states Heima (Shen et al., 2025b) Encode each reasoning step into a single latent token to reduce output length</p>
<p>Latent</p>
<p>LTV (Kong et al., 2025) Introduce a latent thought variable to guide text generation Self-Repetition (Wang et al., 2023b) prompt LM in parallel Self-Refine (Madaan et al., 2023) Naively prompt LM to iteratively refine answer Self-Repetition DeCRIM (Ferraz et al., 2024) Self-correlation for multi-constrained instruction following</p>
<p>MoA (Wang et al., 2025a) Prompt different models in parallel and iteratively improve RR-MP (He et al., 2025) Propose Reactive and Reflection agents to collaborate BRAIN (Chen et al., 2024i) Propose frontal &amp; parietal lobe model to inspire brain Mixture-of-Model Collab (Chakraborty et al., 2025) Propose decoding strategies to leverage multiple off-the-shelf aligned LLM policies Table 1: Summary of Certain Stimulation Techniques.</p>
<p>Verification</p>
<p>Verifying the correctness and consistency of LLM during the test-time scaling is also crucial.The verification process plays an important role in the test-time scaling, as a solid verification process can be adapted to:</p>
<p>• directly selects the output sample among various ones, under the Parallel Scaling paradigm;</p>
<p>• guides the stimulation process and determines when to stop, under the Sequential Scaling paradigm;</p>
<p>• serves as the criteria in the search process, which we will discuss in Section 3.2.3;</p>
<p>• determines what sample to aggregate and how to aggregate them, e.g., weights, which we will discuss in Section 3.2.4.</p>
<p>Usually, there are two types of verifications, as shown below:</p>
<p>Outcome Verification.Outcome verification plays a crucial role in ensuring the correctness and consistency of generated outputs.Common approaches include using a separate verifier model to score multiple candidate answers (e.g., Cobbe et al. (2021)), employing self-consistency, voting mechanisms (Wang et al., 2023b) and discriminator LM (Chen et al., 2024j) and leveraging tool-assisted (Gou et al., 2024) or heuristic checks (DeepSeek-AI, 2025) in domains such as math and code generation.For specific task problems, such as trip planning, functional scoring (Lee et al., 2025) is also adopted for verifying the proposed plans.Instead of formulating the outcome verification as a classification problem, Zhang et al. (2025d) exploits the generative ability of LLM and proposes to reformulate the outcome verification process as a next-token prediction task.Li et al. (2025g) formulate the feedback utilization as an optimization problem and adaptive propagate information between samples.Apart from single criteria, certain outcome verification approaches verify the quality of the simulated samples from multiple perspectives.Liu et al. (2023b) conducts both (i) passive verification from external tools and (ii) active verification via a rethinking mechanism to justify each sample.Zhang et al. (2024c) follows a similar idea and proposes to verify each sample from three aspects: Assertion, Process, and Result.Lifshitz et al. (2025) further extends the number of verification agents to an arbitrary number and decouples the semantic criteria with verification agents.Parmar et al. (2025) and Saad-Falcon et al. (2024) also propose a verification agent to score each sample considering various factors, respectively.Saad-Falcon et al. (2024) additionally proposes a unit test-based verification approach.We provide a detailed technical categorization in the Appendix A.</p>
<p>Process Verification.Process verification approaches verify the sample outcomes and the process of obtaining such an outcome.They are commonly adopted in tasks with formal, deductive processes, such as reasoning, coding, or mathematics.They are also known as the process reward model (PRM) or state verification.Lightman et al. (2023) processes to train a PRM as a step-level verification on mathematical tasks.Yao et al. (2023b) processes an LM-based state verifier as guidelines for searching the samples under the tree structure.Zhang et al. (2024b) further tunes these preference data into LLM and enables CoT structure during test time.Instead of training an external verifier, Xie et al. (2023) prompts the same LM to evaluate the current step given all previous ones.Hosseini et al. (2024) proposes to train the verifier with both accurate and inaccurate generated data.Although LM-based process verifiers can be easily integrated, they may yield unreliable verification, especially for complex problems with long processes.Ling et al. (2023) decomposes the verification process in a deductive manner.Hence, the verifier only needs to verify a few statements within the long thought chain.Yu et al. (2024a) is based on similar intuition but instead focuses on code-aided mathematical reasoning tasks with the critic model iteratively.Li et al. (2025b) instead relies on the external toolbox, such as code interpreters, to verify the process.</p>
<p>Category Approach</p>
<p>Approach Description</p>
<p>Outcome</p>
<p>Naive ORM (Cobbe et al., 2021) Naively process to train solution-level and token-level verifiers on labeled-dataset OVM (Yu et al., 2024b) Train a value model under outcome supervision for guided decoding Heuristic (DeepSeek-AI, 2025)</p>
<p>Heuristic check for domain-specific problems Functional (Lee et al., 2025) Functional scoring for task-specific problems Bandit (Sui et al., 2025) Train a bandit algorithm to learn how to verify Generative Verifier (Zhang et al., 2025d) Exploit the generative ability of LLM-based verifiers via reformulating the verification Self-Reflection Feedback (Li et al., 2025g) formulate the feedback utilization as an optimization problem and solve during test-time Discriminator (Chen et al., 2024j) SFT a domain-specific LM as a discriminator Unit Test (Saad-Falcon et al., 2024) Verify each sample as unit tests XoT (Liu et al., 2023b) Passive verification from external tools and Activate verification via re-thinking WoT (Zhang et al., 2024c) Multi-Perspective Verification on three aspects: Assertion, Process, and Result Multi-Agent Verifiers (Lifshitz et al., 2025) Multi-Perspective Verification without explicit semantic meanings Naive PRM (Lightman et al., 2023) SFT an LM as a PRM on each reasoning step over mathematical tasks State Verifier (Yao et al., 2023b) SFT an LM as a state verifier and evaluate states either independently or jointly Deductive PRM (Ling et al., 2023) Deductively verify a few statements in the process Self-Evaluation (Xie et al., 2023) Prompting the same LM to evaluate the current step given previous ones Lego-prover (Wang et al., 2023a) Evaluate lemma in Theorem Proving PoT (Chen et al., 2023a) delegate computation steps to an external language interpreter Tool (Li et al., 2025b) Relies on external toolbox for verification Process V-STaR (Hosseini et al., 2024) Verifier trained on both accurate and inaccurate self-generated data</p>
<p>Table 2: Summary of Certain Verification Techniques.</p>
<p>Search</p>
<p>Search is also a frequently used component during the test-time scaling.LLMs pre-trained on vast amounts of online data, can be viewed as a compression of real-world knowledge.However, standard inference tends to underutilize their capacity.Search, being a classic yet working technique in retrieving relevant information from vast databases, can be utilized to fully exploit the capability of LLMs by exploring their potential options in a structured manner.</p>
<p>Existing test-time scaling approaches based on search techniques demonstrate significant performance increases over complex tasks, such as complex mathematics, etc. Yao et al. (2023b) explores the potential of search by decomposing the output samples into multiple thoughts and organizing them in a tree structure.Based on only Naive tree search algorithms, such as depth-first search and breath-first search, it demonstrates superior performance on reasoning tasks.Monte-Carlo Tree Search (Coulom, 2006), being a classical and powerful search algorithm, also shines its light on better exploiting the hidden knowledge of LLMs.Chaffin et al. (2022) adopts MCTS during the decoding stage guided by discriminators for constrained textual generation.Zhang et al. (2023b) further extends the MCTS to enhance the planning ability in code generation via looking ahead.Tian et al. (2024) incorporates the MCTS as a critical component in the self-improving framework for LLM.Wan et al. (2024) tailors the search algorithm to tackle problems requiring long-horizon planning and deep tree structure for searching.Chen et al. (2024j) further identifies that discriminators are the key bottleneck in search-enhanced planning.Gandhi et al. (2024) systematizes the search process in a unified language and proposes to train an LLM with data and feedback from the search process.Wu et al. (2024d) empirically analyzes various search algorithms and designs a reward-balanced search algorithm toward Paretooptimal test-time scaling.Edward Beeching (2024) further extends the beam search by incorporating diversity consideration.</p>
<p>Apart from searching within the tree structure, Besta et al. (2024) models the output as a graph search problem.Xie et al. (2023) proposes a stochastic beam search solution based on self-evaluation for reasoning tasks.Pan et al. (2025a) enhances MCTS with proposed associative memory to dynamically update its knowledge base.Li et al. (2025c) proposes to solve the reasoning process as constructing a control flow graph with each node indicating a logic unit.</p>
<p>Aggregation</p>
<p>Aggregation techniques consolidate multiple solutions into a final decision to enhance the reliability and robustness of model predictions at test time.Based on how the final output is generated, we empirically categorize them into two key classes: (i) Selection, which selects the best-performed sample among all candidates, where the selection criteria may vary across different approaches; and (ii) Fusion, which fuse multiple samples into one through tricks like weighting or generation.</p>
<p>Selection In this category, the aggregation process can be viewed as a selection problem.One well-known example is to select the most consistent answer, commonly known as self-consistency.Wang et al. (2023b) improves accuracy by leveraging statistical redundancy-if different reasoning paths converge to the same conclusion, the answer is more likely to be correct.Self-consistency effectively reduces variance in model outputs and mitigates occasional hallucinations.However, as the final output is voted based on consistency, inaccurate and low-quality samples would inevitably influence the output quality.Therefore, various approaches are proposed to filter the candidates before voting.Chen et al. (2024e) incorporates an LM as a filter, while Wu et al. (2025b) proposes a Length-filtered vote, where prediction uncertainty is adopted as a proxy to filter reliable CoT length.</p>
<p>Best-of-N (Irvine et al., 2023) follows the same process but replaces the self-consistency criteria with scalar scores generated by external verifiers.Song et al. (2024) further demonstrates that best-of-N on small LLMs can yield competitive performance against SOTA propriety models.Munkhbat et al. (2025) attaches a few-conditioning filtering before the best-of-N selection.This aims to alleviate its sample inefficiency and achieves significant length reduction.Motivated by particle filtering, Puri et al. (2025)  Fusion Directly selecting the final output sample among candidates may yield unsatisfactory results, especially when the sample quality of candidates is low.Fusion approaches propose to merge multiple samples into one to solve such a problem.Brown et al. (2024) and Li et al. (2023a) extend the idea from Best-of-N and weigh each sample by its score from external verifiers.Jiang et al. (2023), on the other hand, directly prompts another LLM as a summarizer to merge multiple selected samples.Li et al. (2025j) shares similar intuition by replacing the majority voting in self-consistency (Wang et al., 2024e) with generative self-aggregation.Li et al. (2025c)   4 Where to Scale TTS can substantially enhance LLMs' performance across diverse real-world scenarios.We systematically categorize these scenarios into representative domains, detailing the characteristic challenges, critical evaluation criteria, and representative benchmarks that illustrate the practical value of TTS.Here, we also list a brief summary of various benchmarks in Table 4.</p>
<p>Reasoning-intensive Tasks</p>
<p>Reasoning-intensive tasks require structured, explicit, multi-step reasoning, precision, and rigorous correctness verification.These tasks challenge LLMs' ability to systematically decompose problems, iteratively refine solutions, and verify intermediate reasoning steps.</p>
<p>Agentic Tasks</p>
<p>Mathematical Reasoning Mathematical tasks involve complex computations, logical inference, and iterative verification.Key challenges for TTS methods include generating accurate step-by-step solutions, effectively verifying intermediate steps, and handling intricate reasoning logic.Representative benchmarks include MiniF2F (Zheng et al., 2021), AIME 2024 (Google, 2025), MATH-500 (Zhang et al., 2024a), AMC 2023 (Guan et al., 2025), PutnamBench (Tsoukalas et al., 2024), MUSTARD (Huang et al., 2024a) and OlympiadBench (He et al., 2024a).These datasets span advanced competition-level math problems, emphasizing precise and explicit reasoning skills.</p>
<p>Programming &amp; Code Generation Coding tasks demand syntactic accuracy, executable correctness, and iterative debugging.Challenges for TTS methods lie in generating correct implementations, debugging code iteratively, and efficiently exploring multiple coding solutions.Representative datasets include Codeforces (codeforce, 2025), SWE-bench (Jimenez et al., 2024), and LiveCodeBench (Jain et al., 2025), each providing expert-level coding challenges that require rigorous logical thinking and implementation accuracy.</p>
<p>Game Playing and Strategic Reasoning Strategic reasoning tasks involve adaptive planning, interactive decisionmaking, and complex multi-round reasoning.TTS methods must efficiently perform iterative search, adaptive inference, and dynamic interactions.A representative benchmark is SysBench (Google, 2025), which evaluates models' strategic reasoning in interactive tasks.</p>
<p>Scientific Reasoning Scientific problems typically require multi-domain knowledge integration across physics, chemistry, biology, and other disciplines.TTS methods must demonstrate broad knowledge synthesis, multi-step reasoning, and accurate factual verification.Notable benchmarks include GPQA Diamond (Rein et al., 2024) and MR-Ben (Zeng et al., 2024), focusing on advanced scientific reasoning and integrated domain knowledge.Domain specific benchmarks, such as TP-Bench (Chung et al., 2025), UGPhysics (Xu et al., 2025c), PHYSICS (Feng et al., 2025), have also investigated recently.</p>
<p>Medical Reasoning Medical tasks involve diagnostic decision-making, clinical reasoning, and precise medical knowledge.The key challenge for TTS here is ensuring reliable, accurate reasoning that mimics medical professionals' decision logic.Representative datasets include JAMA Clinical Challenge (Chen et al., 2025a), Medbullets (Chen et al., 2025a), and MedQA (Jin et al., 2020).These benchmarks critically assess reasoning LLMs' capabilities in diagnosis, treatment planning, and medical decision accuracy.</p>
<p>Agentic Tasks</p>
<p>The scaling of artificial intelligence agents can be classified into three distinct categories: scaling through design choice, scaling for emergent behavior analysis, and scaling through environmental interaction.In addition, many benchmarks have been proposed for agentic task training, attempting to provide extensive environment feedback.</p>
<p>Scaling Agents as Design Choice</p>
<p>The first category involves scaling multi-agent systems as a deliberate design choice to enhance performance.A central question in this domain concerns the scaling laws of collaborative agents: specifically, how the progressive addition of collaborative agents impacts overall system performance.(Li et al., 2024a) studies the scaling of multi-agent system with sampling and voting.The experiments are conducted by using various LLMs of different sizes on diverse datasets covering reasoning and generation.Performance can be improved by increasing the ensemble size across various tasks, including GSM8k, MMLU, and HumanEval.(Chen et al., 2024d) have examined the relationship between the number of LLM calls and system performance, revealing non-monotonic behavior patterns.This non-monotonicity stems from varying query difficulties within tasks: while increased LLM calls enhance performance on simpler queries, they may diminish performance on more complex ones.(Qian et al., 2024) studies the scaling of multi-round discussion in multi-agent collaboration on MMLU, HumanEval, SRDD, and SRDD under regular typologies including chain, tree, and graphs and irregular typologies.They found that the performance mostly follow a logistic growth pattern as the process of scaling agents.(Meyerson and Qiu, 2025) asks a more theoretical question on how task decompositions and assign multiple agents with different subtasks can be optimal?It provides an asymptotic analysis with LLM primitives about the efficiency of such decomposed systems, leading future opportunities for scaling them.</p>
<p>Scaling Agent for Emergent Social Ability</p>
<p>The second category focuses on scaling multi-agent systems to study emergent behaviors in large-scale simulations, particularly in social science applications.(Zhang et al., 2025h) studies large-scale multi-agent simulation over 300 agents under information asymmetry, focusing on phenomena including the emergence of information cocoons, the evolution of information gaps, and the accumulation of social capital.(Piao et al., 2025) generates social lives for over 10k agents, simulating their 5 million interactions both among agents and between agents and their environment.The simulation is used as a testbed for to study four key social issues: polarization, the spread of inflammatory messages, the effects of universal basic income policies, and the impact of external shocks such as hurricanes.To support such large scale experiments, (Pan et al.) developed a user-friendly multi-agent infrastructure for future large-scale multi-agent experiments.</p>
<p>Others</p>
<p>Scaling Environment Feedback The third category involves scaling the interaction between agents and their environment to obtain richer feedback.(Hilton et al., 2023) find out that an agent's intrinsic performance scales as a power law relative to model size and environmental interactions.However, constructing extensive environments to scale feedback and interactions for agent training remains challenging.(Chen et al., 2025e) tried to address this issue by training a reward model based on synthetic data to enable the agent to do Monte-Carlo Search during planning.This process typically involves using LLM-based agents to collect action trajectory demonstrations, using another LLM for generating contrasting negative trajectories, and training customized reward models based on the synthetic data collected.(Yu et al., 2024e) tries to combine test-time search and self-learning by Reflective Monte Carlo Tree Search for richer feedback to enhance AI agents' ability to explore decision space on the fly.</p>
<p>Simulated Environment for Agentic Tasks Agentic tasks involve realistic and interactive environments, requiring complex planning, iterative reasoning, and effective tool utilization.TTS methods face challenges such as optimal stepwise planning, adaptive decision-making, tool integration, and iterative refinement.Representative benchmarks include WebShop (Yao et al., 2023a), WebArena (Zhou et al., 2023c), SciWorld (Wang et al., 2022), and TextCraft (Prasad et al., 2024).These datasets provide realistic interactive scenarios, emphasizing iterative decision-making and effective tool usage.Recent advances in scaling LLM-driven autonomous agents center on improved planning, memory, and self-optimization techniques.For example, ARMAP (Chen et al., 2025f) automatically learns a reward model from unlabeled environment interactions to score and guide an agent's actions, circumventing the need for human-labeled feedback and improving multi-step decision-making.</p>
<p>Others</p>
<p>These tasks require broad, general-purpose reasoning capabilities, creativity, and subjective evaluation of outputs.</p>
<p>General To achieve general objectives, many efforts have collected numerous official, public datasets that are challenging for humans but are not exclusive to any particular domain.Representative benchmarks include AGIEval (Zhong et al., 2024), MMLU-Pro (Wang et al., 2024d), and Gaokao (Guan et al., 2025).These benchmarks may cover multiple aspects of language models and aim to test their general performance.</p>
<p>Open-Ended Tasks TTS methods must enhance output diversity, quality, and coherence, balancing creativity and correctness.Representative benchmarks include AlpacaEval2.0(Dubois et al., 2024), ArenaHard (Li et al., 2024c), IF-Eval (Zhou et al., 2023b), and C-Eval (Huang et al., 2023), which collectively evaluate subjective, open-ended, and general-purpose reasoning.</p>
<p>Knowledge-intensive Tasks Knowledge-intensive tasks require LLMs to retrieve and synthesize factual knowledge from external sources, ensuring accuracy and reducing hallucinations.TTS challenges center around effective retrieval-augmented reasoning, iterative verification, and multi-source aggregation.Representative benchmarks include SimpleQA (Wei et al., 2024a), C-SimpleQA (He et al., 2024c), and FRAMES (Krishna et al., 2025), emphasizing factual correctness and retrieval-based reasoning.</p>
<p>Evaluation Tasks Evaluation tasks require LLMs to act as judges, also known as Generative Reward Models (GRMs), to conduct comprehensive and in-depth quality assessments of the candidate responses, thus ensuring reliable evaluation results.Representative benchmarks in this field include RewardBench (Lambert et al., 2024), JudgeBench (Tan et al., 2025), RMBench (Liu et al., 2024c), PPE (Frick et al., 2024), and RMB (Zhou et al., 2025).Recent research (Kim et al., 2025) has demonstrated that TTS effectively enhances the evaluation reasoning capabilities of LLMs.For instance, CCE (Zhang et al., 2025e) scales the evaluation by comparing the candidate responses with other crowd-generated responses, enabling TTS evaluation effects.EvalPlan (Saha et al., 2025) achieves deeper evaluation by first generating a specific evaluation plan tailored to the candidate responses.SPCT (Liu et al., 2025c) goes a step further by employing RL to generate evaluation principles, further activating the TTS potential.Additionally, JudgeLRM (Chen et al., 2025b) has validated that training using the R1 method can effectively enhance the performance of RMs, while MAV (Lifshitz et al., 2025) introduces multiple aspect verifiers.Notably, improving evaluator accuracy in Out-of-Distribution scenarios remains a critical issue, like Reward Hacking (Skalse et al., 2025;Shen et al., 2025a), worthy of deeper exploration.</p>
<p>Multimodal Tasks Multimodal reasoning tasks demand effective cross-modal integration, iterative reasoning between modalities, and robust verification across visual and textual inputs.TTS methods face challenges in modality fusion, iterative multimodal reasoning, and handling ambiguity across modalities.Representative benchmarks include MMMU (Yue et al., 2024), MathVista (Lu et al., 2024), MathVision (Wang et al., 2024d), CMMaTH (Li et al., 2025i), and PGPS9K (Zhang et al., 2023a), each testing multimodal reasoning across visual and textual modalities.</p>
<p>How Well to Scale</p>
<p>In this section, we classify the metrics used in evaluating the test-time scaling methods into four high-level dimensions: Performance, Controllability, Scalability, and Efficiency.Each dimension captures an essential aspect critical to assessing test-time scaling approaches.</p>
<p>Performance</p>
<p>Performance metrics assess the correctness of generated solutions.</p>
<p>Pass@1.Pass@1 is one of the most widely used metrics for evaluating the correctness of a model's first output attempt (DeepSeek-AI, 2025; Li et al., 2025d;Snell et al., 2024;Xie et al., 2025;Kimi, 2025;Yang et al., 2025b,a;Hou et al., 2025).It measures the proportion of problems where the model's first generated solution is correct.A correct solution means the one that exactly matches the ground-truth answer or passes all required validation checks, such as the exact answer match in mathematical benchmarks and private unit tests in coding tasks.Pass@1 is frequently used in tasks such as mathematical reasoning and coding benchmarks.In mathematical reasoning tasks such as AIME 2024 (Google, 2025) and MATH-500 (Zhang et al., 2024a), Pass@1 measures the percentage of exact matches between the model's answer and the ground truth.In coding benchmarks such as LiveCodeBench (Jain et al., 2025) and HumanEval-Mul, Pass@1 evaluates the code correctness against hidden test cases.</p>
<p>Pass@k (Coverage).Pass@k extends Pass@1 by measuring whether at least one of the model's k sampled outputs is correct (Brown et al., 2024;Snell et al., 2024;Li et al., 2025d).Formally, Pass@k can be estimated using the unbiased estimator from Chen et al. ( 2021):
Pass@k = 1 n n i=1 1 − N −Ci k N k
,</p>
<p>where n is the number of problems, N is the total number of samples per problem, and C i is the number of correct samples for the i-th problem.Pass@k is widely adopted in program synthesis and formal theorem-proving tasks, such as CodeContests (Li et al., 2022) and SWE-bench Lite (Jimenez et al., 2024).</p>
<p>Cons@k (Consensus@k).Cons@k measures the majority vote correctness from k independently sampled outputs (DeepSeek-AI, 2025; Zeng et al., 2025d).Given k responses generated by a model for a given problem, the majority-voted prediction is the most frequent answer.The answer is then compared against the ground truth.Cons@k is frequently used alongside pass@1 to assess the benefit of leveraging multiple samples.Larger values of k (e.g., 16, 64) typically improve answer stability and accuracy but at the cost of increased compute.This metric is especially valuable in tasks where single generations may be noisy or uncertain, and ensemble strategies can improve robustness.Cons@k has been widely adopted in mathematical reasoning benchmarks such as AIME 2024 (Google, 2025) and MATH-500 (Zhang et al., 2024a).</p>
<p>Arena-based Evaluation (Pairwise Win Rate).In addition to accuracy-oriented metrics, some studies adopt pairwise comparison metrics, where model outputs are compared against baselines using human or LLM-based judges (DeepSeek-AI, 2025; Hou et al., 2025).For instance, LC-Winrate (Dubois et al., 2024) adjusts win rates to control for response length, while ArenaHard GPT-4 Judge (Li et al., 2024c) uses GPT-4-Turbo to score outputs from open-ended tasks.These pairwise evaluation methods are especially common in generation tasks where qualitative assessments (e.g., fluency, coherence) matter.</p>
<p>Task-Specific Metrics.Certain domains employ specialized metrics.For example, Codeforces Percentile and Elo Rating are used to measure coding capabilities under competitive programming settings (DeepSeek-AI, 2025; Kimi, 2025).Percentile indicates how well a model performs relative to other participants, while Elo Rating reflects relative skill under tournament-based evaluations.</p>
<p>Efficiency</p>
<p>Efficiency should not be treated as a monolithic concept.Instead, it spans multiple dimensions, encompassing both computational cost and the quality of the reasoning process.Different perspectives capture not only how many resources (e.g., tokens, FLOPs, memory) a model consumes, but also how effectively it reasons-whether it produces concise and goal-directed outputs, or falls into patterns of repetition and over-generation.</p>
<p>Key Concern and Multi-Dimensional Perspectives of Efficiency</p>
<p>A central challenge in efficiency is the trade-off between the reasoning length and solution quality.The formal notion of reasoning efficiency captures this trade-off, as introduced by (Qu et al., 2025).Let M represent a language reasoning model evaluated over a distribution of tasks T ∼ p(T ), where each task consists of a dataset D and an associated question or objective.The reasoning efficiency η(M) is defined as the expected ratio between solution quality Q and computational cost C:
η(M) = E T ∼p(T ) Q(M, D) C(M, D) ,(4)
where Q(M, D) quantifies task performance (e.g., accuracy, exact match, or creativity), and C(M, D) measures the computation involved (e.g., number of generated tokens, FLOPs, or latency).This concept highlights how excessively long reasoning chains may not yield proportionate gains in solution quality and often reflect redundancy in reasoning traces.</p>
<p>Recent work has identified multi-dimensional perspectives of inefficiency patterns in reasoning LLMs.These include redundancy, where models show excessive repetition of reasoning steps or thoughts, often restating questions or repeating points without contributing to solution quality (Song et al., 2025;Su et al., 2025;Luo et al., 2025a); underthinking, where models shift reasoning direction too early or fail to elaborate on promising ideas (Wang et al., 2025e); and overthinking, where models revisit or verify earlier steps excessively even for simple problems (Luo et al., 2025a;Chiang and Lee, 2024;Chen et al., 2024g).These behaviors contribute to higher computational cost without proportional gains in correctness, ultimately lowering overall reasoning efficiency.</p>
<p>General Computational Cost Metrics</p>
<p>To quantify inference efficiency, we consider the following standard computational metrics:</p>
<p>Token Cost.Token cost measures the total number of tokens generated during inference, including intermediate reasoning steps and final outputs (Welleck et al., 2024;Brown et al., 2024;Hou et al., 2025;Yang et al., 2025b;Xu et al., 2025d;Wang et al., 2025c;Aytes et al., 2025).This metric is especially important, as verbose reasoning typically leads to higher token consumption.Reducing token cost while maintaining performance is crucial for inference efficiency, particularly when operating under fixed computational budgets or API pricing constraints.In addition, inference efficiency metrics such as latency and throughput are critical in real-world applications, especially for high-throughput systems (Welleck et al., 2024).</p>
<p>FLOPs-based Efficiency Analysis.FLOPs-based compute analysis has been widely adopted to quantify computational cost (Kaplan et al., 2020;Snell et al., 2024;Wu et al., 2024c;Teng et al., 2025).Several recent works (Snell et al., 2024;Wu et al., 2024c) benchmark test-time scaling strategies, such as adaptive revisions and verifier-based search, against model scaling by plotting accuracy versus total inference FLOPs.This FLOPs-based evaluation can be used to determine whether test-time methods outperform larger models under equivalent compute budgets.</p>
<p>KV Cache Size.The KV cache size (Hooper et al., 2025) refers to the total memory footprint required to store the Key-Value cache across all trajectories and time steps during the test-time search process.As each unique generation path requires its own KV cache, methods with low KV sharing across trajectories tend to consume significantly more memory and incur higher latency.By promoting KV cache sharing among trajectories, ETS reduces the total KV cache size, thereby improving throughput.For instance, ETS achieves up to 1.8× KV cache reduction compared to REBASE, leading to 1.4× faster inference on NVIDIA H100 GPUs, without compromising accuracy.</p>
<p>Reasoning Efficiency Metrics</p>
<p>Beyond standard compute metrics, reasoning inefficiency can also be measured through the following metrics.</p>
<p>Underthinking Score.The underthinking score (Wang et al., 2025e) quantifies cases where a model produces correct intermediate thoughts early in the reasoning chain but fails to reach a correct final answer.</p>
<p>Formally, the underthinking score ξ UT is defined as:
ξ UT = 1 N inc Ninc i=1 1 − Ti T i ,(5)
where N inc is the number of incorrect responses in the test set, T i is the total number of tokens in the i-th incorrect response, and Ti is the position (in tokens) of the first correct intermediate thought.If no correct intermediate thought exists, Ti = T i , yielding a score of zero for that instance.A higher ξ UT indicates inefficient reasoning behavior where good ideas are introduced but not effectively pursued.</p>
<p>Outcome Efficiency.The outcome efficiency metric (Chen et al., 2024g) quantifies how economically a model reaches the correct answer in multi-turn reasoning.It reflects whether additional solution rounds meaningfully contribute to accuracy.Empirically, the metric captures the ratio of efficient tokens-those used before the first correct answer appears-to the total number of output tokens.Formally, the outcome efficiency ξ O is defined as:
ξ O = 1 N N i=1 σ i • Ti T i , (6)
where N is the number of test instances, T i denotes the total tokens generated for the i-th instance, and Ti is the number of tokens needed to reach the first correct solution.The correctness indicator σ i is set to 1 if at least one correct solution appears in the response, and 0 otherwise.A higher ξ O suggests that the model typically finds correct answers early in the reasoning process, while a low value indicates inefficient use of computation due to overextended or redundant reasoning chains.</p>
<p>Controllability</p>
<p>Process Efficiency.The process efficiency metric (Chen et al., 2024g) evaluates how productively a model explores diverse reasoning strategies across multiple solution rounds.Rather than focusing solely on accuracy, it assesses the novelty of later solutions by measuring how many output tokens contribute to distinct reasoning approaches.Formally, the process efficiency ξ P is defined as:
ξ P = 1 N N i=1 D i T i , (7)
where N is the number of test instances, D i is the number of tokens from non-redundant (i.e., distinct) solutions in the i-th response, and T i is the total number of tokens.A solution is considered distinct if its reasoning strategy differs significantly from those in earlier turns.A higher ξ P value implies more efficient cognitive exploration, while a lower value indicates repetitive or self-verifying reasoning that fails to expand the model's strategic diversity.</p>
<p>Controllability</p>
<p>Controllability metrics evaluate whether test-time methods can consistently adhere to pre-defined resource constraints such as compute budgets or output length targets.</p>
<p>Control Metric.• Mean Deviation from Target Length quantifies the average relative difference between the generated output length and the target length:
Mean Deviation = E x∼D |n generated − n gold | n gold ,
where n generated is the model's output length and n gold is the target length.</p>
<p>• Root Mean Squared Error (RMSE) of Length Deviation captures the variance in length control:
RMSE = 1 N N i=1 n generated,i − n gold,i n gold,i 2 .
Lower values for both metrics indicate more stable and precise length control across samples.</p>
<p>k-ϵ Controllability.Bhargava et al. (2024) propose k-ϵ controllability as a formal metric to characterize the prompt-based steerability of language models.Unlike metrics focused on compute or length constraints, this metric quantifies whether a model can be guided to produce a target output within a bounded prompt length and allowable deviation.Formally, a model is said to be (k, ϵ)-controllable for a target output y if there exists a prompt p with |p| ≤ k such that the model outputs y with probability at least 1 − ϵ:
Pr[LLM(p) = y] ≥ 1 − ϵ.
By evaluating across different values of k and ϵ, one can map out the controllability landscape of a model.In practice, Bhargava et al. (2024) measures this property on tasks such as next-token prediction in WikiText, finding that over 97% of targets are reachable with a prompt of at most 10 tokens and an error tolerance ϵ ≤ 0.05.This metric provides a theoretical lens for quantifying how easily a model's outputs can be controlled via prompt design.</p>
<p>While not directly tied to resource constraints, k-ϵ controllability offers valuable insight into the model's test-time responsiveness and has been used to compare inherent steerability across model families and sizes.</p>
<p>Scalability</p>
<p>Scalability metrics measure how effectively test-time scaling methods can leverage increased compute (e.g., token budgets, samples, inference steps) to improve performance.</p>
<p>Scaling Metric Muennighoff et al. (2025) propose the Scaling metric, capturing the average slope of performance gains as compute increases:
Scaling = 1 |A| 2 a,b∈A b&gt;a f (b) − f (a) b − a .
This metric quantifies how effectively models improve accuracy or pass rates with additional computation.</p>
<p>Scaling Curves (Accuracy vs. Compute).Scaling curves are used to visualize how metrics such as accuracy, pass rate, or EM improve as token budgets, iteration depth, or the number of samples increase (Aggarwal and Welleck, 2025;Teng et al., 2025;Wu et al., 2024c).These plots help reveal diminishing returns and performance saturation at higher compute budgets.Building on our taxonomy, we decompose the existing literature along multiple dimensions (Table 5).As shown in Figure 4, these works, with different technical innovations, follow a broadly consistent path.From 2022 to 2023, researchers emphasized structured inference to guide LLMs in generating more complex solutions.In 2024, methods like PRM and MCTS enabled the automatic supervision of intricate reasoning trajectories, yielding richly annotated data for fine-tuning and improving TTS performance.Subsequent approaches, such as o1 and R1, demonstrated that pure RL can also elicit comprehensive, logically sound reasoning.</p>
<p>• Crucially, these techniques are complementary rather than mutually exclusive: for instance, R1 necessitates an SFT-based warmup via rejection sampling.Therefore, achieving more powerful scaling requires systematically  integrating these methods.Even within RL frameworks, practitioners should continue to leverage synthesized CoT approaches and incorporate structured inference strategies to tackle increasingly complex scenarios effectively.</p>
<p>• Researchers found that there does not exist one simple scaling solution that works for all problems.Increasingly, researchers tend to focus on optimal-scaling solutions (Wu et al., 2024d;Snell et al., 2024).</p>
<p>A Hand-on Guideline for Test-time Scaling</p>
<p>In this section, we shift from theoretical categorizations to providing a practical, hands-on guideline for TTS.Our goal is to offer clear, actionable instructions and technical pathways to facilitate effective SST deployment.</p>
<p>Hands-on Guidelines: Common Problems</p>
<p>Q: What kind of task does TTS help?A: Almost any task!While traditional reasoning tasks-such as Olympiad-level mathematics, complex coding, and game-based challenges-have been shown to significantly improve with TTS , community observations suggest that TTS can also enhance performance in open-ended tasks, such as comment generation or evaluation.However, due to the long-form nature of outputs and the lack of centralized, objective benchmarks, these tasks are inherently more difficult to evaluate quantitatively, making it harder to draw conclusive claims.Beyond that, more realistic, complex, and long-horizon scenarios, like medical reasoning and law, have also shown promising gains through TTS strategies.</p>
<p>Q: If I want to quickly implement a TTS pipeline, what are the essential paths I should consider?How can beginners use TTS at a minimal cost?</p>
<p>A: Broadly speaking, there are three essential technical pathways for test-time scaling: i) Deliberate reasoning procedure at inference time, ii) imitating complex reasoning trajectories, and iii) RL-based incentivization.If your goal is to get a quick sense of the potential upper bound that a strong TTS can bring to your task at a minimum cost, you can directly utilize a model that has been trained with (iii).If you want to develop a TTS baseline at a minimum cost, you can start with (i).Once (i) yields a result that meets expectations, you can apply (ii) to further verify and generalize the outcome.Q: Are these pipelines mutually exclusive?How should I design a frontier-level TTS strategy?A: These pipelines are by no means mutually exclusive-they can be seamlessly integrated.For instance, R1 inherently necessitates SFT through rejection sampling as a preliminary warmup step.When employing RL, practitioners should continue leveraging synthesized CoT methods and introduce additional structured inference strategies to tackle increasingly complex scenarios effectively.</p>
<p>Q:</p>
<p>Can training-time choices like SFT and RL affect the effectiveness of TTS? A: Yes, training-time strategies often shape the ceiling of what TTS methods can achieve at inference.For example, SFT can provide strong reasoning priors that improve the stability and quality of scaling strategies like self-consistency or verifier-based search.On the other hand, RL-based fine-tuning can explicitly incentivize concise and correct reasoning chains, reducing overthinking or incoherent outputs at test time.Both approaches can also be combined: SFT initializes the policy, while RL further sharpens it-this is the strategy behind systems like DeepSeek-R1.Alternatively, RL-trained models can be distilled back into SFT models to reduce inference-time costs without losing the benefits of RL optimization.</p>
<p>Q: How can we improve the efficiency of TTS in multi-turn setups?A: Improving TTS efficiency involves both structural design and learned behaviors: (1) Model-based: Fine-tune models to generate more concise and purposeful reasoning traces.Reinforcement learning (RL) or reward-guided training can penalize overlong or redundant outputs while preserving correctness.Distillation from overthinking-prone models into lighter, more efficient ones can further reduce reasoning overhead.(2) Output-based: Apply early stopping strategies based on verifier confidence or answer agreement to terminate reasoning once sufficient certainty is achieved.This avoids unnecessary continuation in well-understood problems.(3) Prompt-based: Use prompt-level controls-such as token budgets, step limits, or task difficulty cues-to guide the model toward more targeted and efficient reasoning paths at inference.These strategies together help reduce computational cost while maintaining strong solution quality during TTS.</p>
<p>Q: What are some representative or widely-used TTS methods that can serve as baselines?A: Parallel-Self-Consistency, Best-of-N; Sequential-STaR, Self-Refine, PRM; Hybrid-MCTS, ToT; Internal-Distilled-R1, R1.</p>
<p>Q: Is there an optimal go-to solution so far?A: No free lunch.Optimal computing is often dependent on the hardness and openness of the question.</p>
<p>Q: How should we evaluate the performance of a TTS method?In addition to standard accuracy, what other aspects should we pay attention to?</p>
<p>A: The evaluation is largely task-aware, but metrics like accuracy remain the most critical indicators.In addition, efficiency (the trade-off between performance and cost) is another key concern in practical settings.As TTS becomes a more general-purpose strategy, researchers have also begun evaluating a range of secondary attributes, including robustness, safety, bias, and interpretability, to better understand the broader impacts of TTS .</p>
<p>Q: Is there any difference when tuning other scaling formats into internal scaling, compared with directly using the original scaling format?</p>
<p>A: Yes, one intuitive difference lies in the efficiency aspect.Internal scaling tends to yield higher efficiency as it only prompts the LM once, while other scaling techniques usually require multiple trials.However, internal scaling requires non-neglectable resources for tuning, making it less available for practitioners.</p>
<p>8 Challenges and Opportunities 8.1 More Scaling is the Frontier Pushing AI toward more general intelligence, especially for complex tasks, test-time scaling has emerged as one of the most promising methodologies in the post-pretraining era.Given its transformative impact on reasoningintensive tasks-as seen in models like OpenAI's o1 and DeepSeek-R1-it is increasingly clear that realizing the full promise of test-time scaling remains a central pillar in advancing AGI.However, to push the frontier further, we need new and more effective strategies.There are some several promising research directions: Parallel Scaling.Parallel scaling improves solution reliability by generating multiple responses and selecting the best answer.Despite its effectiveness, parallel scaling remains has diminishing returns when coverage reaches saturation.A key challenge is how to enhance coverage, shifting from brute-force coverage expansion to a more guided, efficient process.1. Structured Self-Refinement: Rather than blindly refining the entire response, models could learn to target specific parts of their reasoning that require adjustment.</p>
<ol>
<li>Verification-Enhanced Iterative Scaling: Introducing real-time validation steps within the sequential reasoning process could prevent models from propagating early mistakes.This could involve running self-verification checks between iterations (e.g., checking consistency with known facts, comparing intermediate results to prior context, or re-computing specific logical steps).By selectively verifying before proceeding, models can ensure high-quality stepwise improvements instead of compounding errors.</li>
</ol>
<p>By addressing these challenges, sequential scaling can evolve beyond simple iterative refinement, becoming a highly adaptive, self-correcting reasoning paradigm that enables models to engage in goal-directed, long-horizon thinking.</p>
<p>Hybrid Scaling.Hybrid scaling blends parallel and sequential methods, making it more adaptive and practical for real-world applications.Current test-time scaling methods are often highly specialized, limiting their generalizability.</p>
<p>To address these limitations, hybrid scaling can be improved in several ways:</p>
<p>1.By addressing these challenges, internal scaling has the potential to maximize efficiency, enhance model adaptability, and push AI systems toward more autonomous, self-regulating reasoning.</p>
<p>Clarifying the Essence of Techniques in Scaling is the Foundation</p>
<p>While what to scale continues to evolve and techniques further developing internally, such as PPO transitioning to GRPO, we observe that the core categories of scaling techniques remain relatively stable.For example, SFT and RL remain two of the most common approaches, though their roles and interactions have shifted over time.This raises an urgent need to deepen our understanding of how these fundamental techniques contribute to test-time scaling.</p>
<p>Here, we raise some potential directions for further investigation:</p>
<p>1.By addressing these challenges, test-time scaling can become a foundational AI capability, enabling models to extend their own reasoning dynamically, adapt to real-world constraints, and generalize across specialized fields.This shift represents a paradigm change, where AI systems don't just memorize knowledge-they actively scale their intelligence at inference to meet the demands of diverse, evolving tasks.</p>
<p>Conclusion</p>
<p>This is the first survey to decompose TTS through a hierarchical taxonomy, offering a structured perspective that aids both conceptual understanding and the identification of individual contributions.Emphasizing practical utility, we introduce a hands-on guideline aligned with each taxonomy dimension, which we plan to expand over time.Based on this framework, we outline key trends, challenges, and opportunities shaping the future of TTS research.</p>
<p>A.1 Verifier Model-Based Scoring</p>
<p>The verifier, which is typically trained using human feedback or supervised data (e.g., as in (Cobbe et al., 2021;Lambert et al., 2024)), scores each candidate based on its expected correctness or quality.Variants include i) pairwise comparison verifiers (Liu et al., 2025b), where candidates are compared against each other to determine a winner, ii) weighted voting systems (Wettig et al., 2024;Li et al., 2024b) that use the verifier's scores to combine outputs, iii) LLM-based verifiers that prompt LLM to perform evaluation instruction, like LLM-as-a-Judge (Zheng et al., 2023a;Zhang et al., 2025e,f), LLM-based Evaluator (Liu et al., 2023c;Xu et al., 2023;Jiang et al., 2024a), Critic-based Model (Gao et al., 2024a;McAleese et al., 2024).</p>
<p>A.2 Self-Consistency and Voting Mechanisms</p>
<p>Self-consistency techniques generate multiple independent reasoning chains and choose the final answer based on majority voting (Wang et al., 2023b).The underlying assumption is that if several chains converge on the same answer, that answer is more likely to be correct.Some approaches (Taubenfeld et al., 2025;Mahmud et al., 2025) also incorporate confidence scores or soft-voting schemes to mitigate noise in individual outputs.In place of multiple samples from one model, one can also have multiple models (Wan et al., 2025;Wu and Ito, 2025;Wang et al., 2025f;Feng et al., 2024;Chen et al., 2024b): if a majority (or consensus) of these "agents" agree on an answer, trust it; if they diverge, it may trigger further scrutiny.This is effectively an ensemble vote.</p>
<p>A.3 Tool-Assisted and Heuristic Verification</p>
<p>In domains like code generation or mathematical problem-solving, outcome verification can be implemented via direct execution or rule-based checks.For example, candidate programs are executed on sample test cases to ensure they produce correct results, while in math tasks, answers can be validated by plugging them back into the original equations.These approaches serve as an external check on the LLM's internal reasoning.</p>
<p>Execution-Based Verification.In programming tasks, the ultimate test of correctness is running the code (Tian et al., 2025;Ni et al., 2024;Yang et al., 2024;Ni et al., 2023).For math problems, a simple heuristic is to verify the answer by plugging it back into the original equation or problem constraints.Similarly, if a puzzle answer must satisfy certain conditions, those can be programmatically checked.</p>
<p>Fact-Checking via Retrieval.In open-domain QA or tasks that risk factual errors, search engines or knowledge bases serve as powerful verifiers (Wei et al., 2024b;Vladika and Matthes, 2024;Asai et al., 2023;Peng et al., 2023).</p>
<p>An LLM may draft an answer, but then the system issues search queries (based on the answer's claims) to find supporting evidence.If the retrieved documents contradict the LLM's answer, the answer is likely incorrect and can be rejected or revised.Some frameworks generate answers in a "closed-book" fashion, then do a post-hoc retrieval to validate facts.This idea overlaps with Retrieval-Augmented Generation (Salemi and Zamani, 2024), but the focus is on post-generation validation -essentially checking if the answer aligns with external truth.</p>
<p>Rule-Based Filters.In some applications, simple heuristic filters (Bai et al., 2022;Sun et al., 2023;Weber et al., 2024) can automatically reject bad outputs.For a dialogue system, one might have a list of forbidden answers (certain unsafe or nonsensical replies) and if the model outputs one, the system can either regenerate or adjust it.These aren't "outcome-based" in terms of correctness, but they verify the output against predefined rules of form and content.</p>
<p>B Representative Methods</p>
<p>B.1 Best-of-N</p>
<p>The "Best-of-N" strategy is a TTS approach in which a model generates N candidate outputs for a given input and then selects the best one according to a chosen evaluation metric (Wu et al., 2024d).Mathematically, given an input x and model f , one draws N independent outputs y 1 , . . ., y N ∼ f (x) (e.g., via different random seeds or sampling strategies) and chooses the result ŷ = arg max N i=1 M (y i ), where M is a quality scoring function.At the cost of additional inference compute, increasing N raises the probability of obtaining a high-quality outcome (for example, if each attempt succeeds with probability p, then a best-of-N run succeeds with probability 1 − (1 − p) N ).This technique leverages extra computation to boost performance (Kang et al., 2025) and has been applied in real-world settings ranging from complex reasoning and code generation with LLMs to enhancing image synthesis quality in diffusion models (Ma et al., 2025b).</p>
<p>B.5 Self-Refine MCTS is well-suited for TTS because its anytime nature allows flexible computation budgets.At test time, running MCTS for longer or with more rollouts leads to deeper search and better decisions.Notably, AlphaGo used MCTS at runtime to refine moves, significantly improving performance without additional training.</p>
<p>Researchers are leveraging MCTS to enhance test-time reasoning in other AI domains.MCTS-Judge improves code correctness evaluation by systematically exploring reasoning paths, raising verification accuracy significantly.Similarly, hybrid approaches integrate MCTS into generative model inference for problem-solving, such as solving Sudoku puzzles through sequential search.</p>
<p>By repeating these steps, MCTS concentrates simulations on the most promising branches.In the limit, MCTS value estimates converge to the optimal values in certain perfect-information games.</p>
<p>B.5 Self-Refine</p>
<p>Self-Refine (Madaan et al., 2023) is an advanced TTS technique that enables an LLM to iteratively improve its own outputs through self-generated feedback.Introduced by Madaan et al. (2023), the Self-Refine framework is inspired by how humans revise a draft: The model first produces an initial answer, then critiques or evaluates that answer, and finally uses the critique to refine the answer.This feedback-refinement loop can be repeated multiple times, progressively polishing the output.Notably, Self-Refine requires no additional training data or fine-tuning -the same pre-trained model acts as the initial answer generator, the feedback provider, and the refiner.For sufficiently powerful models, this self-iteration yields significantly better results, presumably because it is easier for a model to identify and fix errors in a given solution than to produce a perfect solution in one attempt.In essence, Self-Refine leverages test-time compute to let the model "think twice (or more)" about its answer, leading to higher-quality and more reliable outputs.</p>
<p>Formally, consider an input x and a language model M θ with parameters θ, defining a conditional distribution P θ (y | x) over possible outputs y.The Self-Refine procedure generates a sequence of outputs y (0) , y (1) , . . ., y (T )  as follows:</p>
<ol>
<li>Initial Output Generation: The model first produces an initial response:
y (0) = M θ (x).(8)</li>
<li>Feedback Generation: At each refinement step t = 1, 2, . . ., T , the model evaluates the previous output and generates feedback: f (t) = M θ x, y (t−1) ; feedback-prompt .</li>
</ol>
<p>(9)</p>
<p>Refinement</p>
<p>Step: Using the generated feedback, the model updates its output: y (t) = M θ x, y (t−1) , f (t) ; refine-prompt .</p>
<p>(10)</p>
<p>This feedback-refinement loop continues iteratively until a stopping condition is met, such as reaching a predefined number of iterations T or detecting convergence in the output quality.The Self-Refine approach enhances model reliability by progressively improving its responses without requiring additional training.</p>
<p>B.6 Tree-of-Thought</p>
<p>Complex reasoning problems often require exploring different lines of thought before arriving at a correct solution.</p>
<p>CoT prompting was a first step in this direction: CoT guides the model to produce a single sequence of intermediate reasoning steps (a linear chain) leading to the answer.This improves the model's performance on tasks requiring multi-step logic by breaking the problem into a step-by-step narrative.However, CoT still follows a single path -if the model makes a wrong turn in the reasoning chain, it cannot recover because it doesn't revisit earlier decisions.Tree-of-Thought (Yao et al., 2023b), by contrast, generalizes CoT to a branching search.At each reasoning step, the model can generate multiple candidate thoughts instead of one, forming a tree of possibilities.It evaluates these candidates (using heuristics or self-evaluation prompts) and selects the most promising branch(es) to continue expanding (Bi et al., 2024).This test-time exploration allows the model to consider alternative approaches and scale up inference computation as needed -much like how a human might try different reasoning avenues for a hard problem.Researchers have categorized ToT and similar strategies (e.g., graph-of-thought) as "X-of-Thought" (XoT) reasoning methods, which significantly improve LLM reasoning by introducing iterative, structured inference without additional training.</p>
<p>ToT can be modeled as a search process through a state space of partial solutions, where each state encodes the sequence of thoughts (intermediate steps) explored so far.Let S be the set of all possible reasoning states for a given problem.The initial state s 0 contains the problem statement, and a goal state s ∈ S represents a complete solution.</p>
<p>B.7 Reinforcement Learning</p>
<p>Thought Generation (State Transitions): At each step, the language model serves as a thought generator function G. Given the current state (context) s, the model generates a set of next-step thoughts: G(s) → {t 1 , t 2 , . . ., t b } (11)</p>
<p>where each t i represents a candidate next reasoning step.Each thought extends the current reasoning path, yielding a new state:
s i = s ⊕ t i (12)
where ⊕ denotes concatenation of the thought to the sequence.</p>
<p>State Evaluation (Heuristic Function): To guide the search, ToT uses an evaluation function f (s) that estimates the quality of a partial state s:
f : S → R(13)
This function may be implemented by the model itself using a self-evaluation prompt or a scoring heuristic.</p>
<p>Search Algorithm (Tree Expansion): ToT can employ different search strategies, including:</p>
<p>• Breadth-First Search (BFS): Expands all plausible thoughts at each depth, keeping the top b best states based on f (s).</p>
<p>• Depth-First Search (DFS): Follows the most promising thought path deeply, backtracking if necessary.</p>
<p>Each strategy allows ToT to control computational budgets by limiting depth d (number of steps) and branching factor b (number of candidates per step).</p>
<p>Solution Extraction: A state s is considered a valid solution if it satisfies the problem constraints.The search continues until:</p>
<p>1.A goal state is reached.</p>
<ol>
<li>The computational budget (depth or number of states evaluated) is exhausted.This framework formalizes ToT as an organized search over the space of reasoning sequences, allowing models to iteratively refine and explore multiple potential solutions during test-time inference.</li>
</ol>
<p>B.7 Reinforcement Learning</p>
<p>Reinforcement learning can play a pivotal role in unlocking effective TTS for language models.The process of inference itself can be formulated as a sequential decision-making problem: at each step in generating a solution, e.g., each token in a reasoning chain or each attempt at an answer, the model (agent) must decide whether to continue reasoning, which direction to explore, or when to stop and output an answer.By training the model with RL, we can explicitly reward outcomes that lead to correct or high-quality answers, thereby encouraging the model to make better use of the extra inference steps available.This addresses a key challenge in TTS : simply allowing a model to think longer doesn't guarantee better answers unless the model knows how to productively use that extra time (it could otherwise repeat mistakes or terminate too early).RL provides a feedback-driven way to learn such behaviors.In fact, prior approaches to improve reasoning in LLMs often relied solely on imitation learning (learning from observed human or AI reasoning traces), which can limit a model to mimicking given patterns (Hou et al., 2025).By contrast, RL enables self-exploration: the model can try diverse reasoning paths and learn from trial-and-error which strategies yield the highest reward (for example, reaching a correct solution).This means an RL-trained language model can learn dynamic inference policies-such as when to double-check an intermediate result or how to backtrack and correct itself if the reasoning seems to be going astray.Recent research indeed shows that combining chain-of-thought reasoning with reinforcement learning techniques leads to improved inference-time performance.</p>
<p>Figure 1 :
1
Figure 1: Comparison of Scaling Paradigms in Pre-training and Test-time Phases.</p>
<p>Figure 2 :
2
Figure 2: Taxonomy of research in Test-time Scaling that consists of what, how, where, and how well to scale.</p>
<p>Figure 3 :
3
Figure 3: A Visual Map and Comparison: From What to Scale to How to Scale.</p>
<p>proposes to consider filtering upon the samples.Sessa et al. (2024) went one step further in reducing sample inefficiency.It tunes the best-of-N results into the LM via RLHF.With the blooming of the agentic approach, Parmar et al. (2025) proposes a selection agent considering complex factors with both historical and current status.Apart from selecting samples from one single LM, Ong et al. (2025) views the selection of samples generated by weak and strong LLMs as a routing problem and proposes constraints on computation costs.</p>
<p>Figure 4 :
4
Figure 4: From Emergence to the Next Frontier, the Evolutionary Path of Test-Time Scaling.</p>
<p>Li et al. (2025e)e the imitation approach uses a model's own intermediate outputs for improvement, distillation techniques aim to transfer the capabilities of a stronger model (or ensemble of models) into a target model via supervised learning.As reported byMuennighoff et al. (2025);Li et al. (2025e), a 32B model trained on a curated sample set generated by a top-tier reasoner was able to solve competition-level math problems nearly as well as the teacher, indicating successful distillation of reasoning.</p>
<p>also adopts LLM as the synthesizer, given the intermediate consideration in previous steps.
Category ApproachExternal Verifier Approach DescriptionAlso Utilized inMajority Voting (Wang et al., 2023b)✗Select the most common sample(Chen et al., 2024e)SelectionBest-of-N (Irvine et al., 2023) Few-shot BoN (Munkhbat et al., 2025)✓ ✓Select the highest scored sample BoN with few-shot conditioning(Song et al., 2024)Agentic (Parmar et al., 2025)✗agent considering both current and previous statusWeighted BoN (Li et al., 2023a)✓Weight each sample by its score(Brown et al., 2024)FusionSynthesize (Jiang et al., 2023)✗Fuse the selected samples via GenAI(Wang et al., 2025a; Li et al., 2025c)Ensemble Fusion (Saad-Falcon et al., 2024)✗Conduct ensemble before fusion</p>
<p>Table 3 :
3
Summary of Certain Aggregation Techniques.BoN stands for Best-of-N.</p>
<p>Table 4 :
4
Summary of Benchmarks
BenchmarkSizeEvaluation CriteriaExample TaskKey FeaturesTypeReasoning-intensive TasksFrontierMath (Glazer et al., 2024)HundredsExact matchAlgebraic geometryHigh complexityMATH (Cobbe et al., 2021)12.5KExact matchAMC/AIME-styleStructured reasoningMiniF2F (Zheng et al., 2021)1794FormalMixedFormal BenchmarkPutnamBench (Tsoukalas et al., 2024)1709num-solvedCompetition-levelTheorem-provingMUSTARD (Huang et al., 2024a)VariedPass@1MixedAuto-GeneratedNuminaMath (LI et al., 2024)860KExact match, CoTOlympiad-level mathAnnotated reasoningMathOmniMath (Gao et al., 2025a)4.4KAccuracyMath OlympiadsAdvanced reasoningGSM8K (Zhang et al., 2024a)8.5KAccuracyGrade-school mathNatural-language solutionsrStar-Math (Guan et al., 2025)747KPass@1 accuracyCompetition mathIterative refinementReST-MCTS (Zhang et al., 2024a)VariedAccuracyMulti-step reasoningReward-guided searchs1 (Muennighoff et al., 2025)1KAccuracyMath/science tasksControlled computeUSACO (Shi et al., 2024)307Pass@1Olympiad codingCreative algorithmsAlphaCode (Li et al., 2022) LiveCodeBench (Jain et al., 2025)Thousands 511Solve rate Pass@1Competitive coding Real-time codingComplex algorithms Live evaluationCodeSWE-bench (Jimenez et al., 2024)2.3KResolution rateGitHub issuesMulti-file editsGPQA (Rein et al., 2024)448AccuracyGraduate STEMDomain expertisePHYSICS (Feng et al., 2025)1297AccuracyUniversity-levelPhysics ProblemOlympicArena (Huang et al., 2024b)11.1KAccuracyMultidisciplinary tasksMultimodal reasoningOlympiadBench (He et al., 2024a)8.4KAccuracyMath/Physics OlympiadsExpert multimodal tasksTheoremQA (Chen et al., 2023b)800AccuracyTheorem-based STEMTheoretical applicationScienceTP-Bench (Chung et al., 2025)57GradesMixedTheoretical PhysicsUGPhysics (Xu et al., 2025c)5520AccuracyUniversity-levelBilingual Physics ProblemPhysReason (Zhang et al., 2025g)1200Step &amp; AnswerMixedPhysics-based ReasoningPHYBench (Qiu et al., 2025)500Accuracy &amp; EDDMixedmeticulously curatedMedQA (Jin et al., 2020)1.3KAccuracyClinical diagnosticsMedical accuracyJAMA (Chen et al., 2025a)1,524AccuracyChallenging clinical casesExpert-written explanationsMedicalMedbullets (Chen et al., 2025a)308AccuracyUSMLE Step 2/3 clinical questionsExpert-written explanationsOthersAGIEval (Zhong et al., 2024)8KAccuracyCollege examsHuman-centric reasoningMMLU-Pro (Wang et al., 2024h)12KAccuracyMultidisciplinary testsDeep reasoning complexityC-Eval (Huang et al., 2023)13.9KAccuracyChinese examsMultidisciplinary reasoningGaokao (NCEE, 2025)VariedAccuracyChinese college examsBroad knowledgeBasicKaoyan (GSEE, 2025)VariedAccuracyGraduate entry examsSpecialized knowledgeCMMLU (Li et al., 2024)VariedAccuracyMulti-task Chinese evalComprehensive coverageLongBench (Bai et al., 2024)VariedAccuracyBilingual multi-task evalLong-form reasoningIF-Eval (Zhou et al., 2023b)541AccuracyInstruction adherenceObjective evaluationArenaHard (Li et al., 2024c) Chatbot Arena (Zheng et al., 2023a)500 VariedHuman preference Human alignmentOpen-ended creativity Chatbot qualityHuman alignment User-aligned responsesOpen-endedAlpacaEval2.0 (Dubois et al., 2024)805Win rateChatbot responsesDebiased evaluationWebShop (Yao et al., 2023a)1.18MTask successOnline shoppingReal-world interactionWebArena (Zhou et al., 2023c) SciWorld (Wang et al., 2022)Varied 30 tasksTask completion Task-specific scoresWeb navigation tasks Scientific experimentsAdaptive decision-making Interactive simulationAgenticTextCraft (Prasad et al., 2024)VariedSuccess rateTask decompositionIterative planningSimpleQA (Wei et al., 2024a)4.3KAccuracyShort queriesFactual correctnessC-SimpleQA (He et al., 2024c)3KAccuracyChinese queriesCultural relevanceKnowledgeFRAMES (Krishna et al., 2025)824AccuracyMulti-hop queriesSource aggregationRewardBench (Lambert et al., 2024)2,985AccuracyChat,Safety,ReasoningMultiple Domains General RewardJudgeBench (Tan et al., 2025)350Accuracyknowledge, reasoning, math, and codingChallenging TasksRMBench (Liu et al., 2024b)1,327AccuracyVisual math problemssubtle differences and style biasesEvaluationPPE (Frick et al., 2024)16,038AccuracyInstruction, Math, Coding, etc.Real-world preferenceRMB (Zhou et al., 2025)3,197Accuracy49 fine-grained real-world scenariosClosely related to alignment objectivesMMMU (Yue et al., 2024)11.5KAccuracyMultimodal expert tasksMultidisciplinary integrationMathVista (Lu et al., 2024)6.1KAccuracyVisual math reasoningVisual-math integrationMATH-Vision (Wang et al., 2024d)3KAccuracyVisual math problemsMultimodal math reasoningLLAVA-Wild (Liu et al., 2023a)VariedGPT-4 scoreVisual QAComplex visualsMM-Vet (Yu et al., 2024d)VariedGPT-4 evaluationIntegrated multimodalMulti-capability evalMultimodalMMBench (Liu et al., 2024d)3.2KAccuracyDiverse multimodalFine-grained evalCVBench (Tong et al., 2024)VariedAccuracyVision tasksHigh-quality evalMMStar (Chen et al., 2024c)1.5KAccuracyVision-critical QAVisual relianceCHAIR (Rohrbach et al., 2018)VariedHallucination rateImage captioningObject hallucination</p>
<p>Muennighoff et al. (2025)propose Control as a formal metric to quantify adherence to a specified compute budget range.It measures the fraction of test-time compute values that stay within given upper and lower bounds:
Control =1 |A|
(Aggarwal and Welleck, 2025)here A is the set of observed compute values such as thinking tokens, and I(•) is the indicator function.A score of 100% denotes perfect adherence to the compute budget across all tasks.Additionally,Hou et al. (2025)andYang et al. (2025b)report experiments where models are evaluated under fixed token budgets, e.g., 1024, 2048, 4096, to examine how well models meet pre-specified length or token constraints during reasoning.Moreover,Xie et al. (2025)andTeng et al. (2025)impose explicit constraints on maximum output lengths to ensure test-time stability and prevent output truncation.Length Deviation Metrics.Mean Deviation from Target Length and RMSE of Length Deviation are introduced to quantify a model's ability to control output length(Aggarwal and Welleck, 2025):</p>
<p>Table 5 :
5
Commonly-used combinations in existing literature when conducting inference scaling.</p>
<p>•</p>
<p>The boundary between inference-based and tuning-based approaches is blurring.Consequentially, the target of scaling (what to scale) changes between different stages.Certain papers, such as Li et al. (2025f); Munkhbat et al. (2025), tune the inference-based capability into the LLM by synthesizing high-quality data from inference-based approaches as the tuning data.Others, such as Wan et al. (2024), are proposing various techniques that better exploit the LLM's capability during both the training and inference stages.</p>
<p>Sequential Scaling.Sequential scaling faces unique challenges, particularly in maintaining coherence and preventing error accumulation.A key issue is optimizing stepwise reasoning to avoid diminishing returns or reinforcing incorrect steps.Instead of naive iterative refinement, future advancements should focus on more 8.2 Clarifying the Essence of Techniques in Scaling is the Foundation adaptive and structured approaches to ensure each reasoning step meaningfully improves the final outcome.Possible directions include:
Possible future advancements include:1. Smart Coverage Expansion: Instead of naive best-of-N sampling, a model could intelligently generate diversereasoning paths, ensuring each sampled response explores a meaningfully different approach;2. Verifier-Augmented Parallel Scaling: Integrating real-time verification mechanisms could allow parallelsamples to be filtered dynamically.</p>
<p>Generalized Hybrid Scaling Architectures: research should focus on unifying test-time scaling mechanisms into a single framework that dynamically chooses the best strategy for different query types.2.Multi-Agent &amp; Interactive Scaling: Expanding hybrid scaling beyond a single-agent reasoning process could allow multiple model instances to engage in structured debate, argumentation, or negotiation, improving solution reliability.While current hybrid scaling is mostly studied in controlled benchmarks, future work must consider its role in real-world applications.Internal Scaling.Internal scaling allows on-the-fly computation modulation without external intervention.While this paradigm has demonstrated promising results, it also introduces unique challenges.1.Effective Compute Allocation: Ensuring that internal scaling allocates extra reasoning steps only where necessary is critical.If the model overthinks simple tasks or fails to extend reasoning on complex ones, the benefits of dynamic computation are lost.
2. Stability and Consistency: As models extend their own reasoning paths, they risk logical drift, hallucination,or over-complication. Unlike sequential scaling, which can incorporate external verification, internal scalingmust maintain self-consistency without external guidance.3. Interpretability and Controllability: Internal scaling happens implicitly, making it difficult to diagnose failuresor regulate inference costs. Unlike parallel scaling (which provides multiple explicit outputs) or sequentialscaling (which follows structured iterations), internal scaling lacks clear intermediate checkpoints, posingchallenges for debugging and efficiency management.</p>
<p>(Zhang et al., 2025a;Huang et al., 2025b)ow do core techniques (SFT, RL, reward modeling) contribute to test-time scaling?how should SFT and RL be optimally combined?2.Re-evaluating Reward Modeling: whether PRMs actually improve multi-step inference?Does the classic reward model incorporate noise and unnecessary complexity? 3. Mathematical Properties of Test-Time Scaling: How does performance scale with increased inference steps?Is there an optimal stopping criterion?Are there fundamental constraints on how much test-time scaling can improve reasoning performance?8.3 Optimizing Scaling is the Key 4. Chain-of-Thought Reasoning Priorities: which aspects of chain-of-thought are most crucial for effective test-time scaling? 5. Adaptive Test-Time Scaling: How can we make a model automatically adjust its inference process based on the problem at hand?As empirical observations on certain property models (xAI, 2025) show blindly scaling over test-time may lead to over-thinking.As new TTS methods proliferate, systematic evaluation and optimization become critical.We must comprehensively measure how different strategies perform regarding task accuracy and consider efficiency, robustness, bias, safety, interpretability, and more.Optimizing these aspects of TTS is gradually emerging(Zhang et al., 2025a;Huang et al., 2025b)and will become an important part of future developments.8.4 Generalization across Domains is the MainstreamWe anticipate a wave of research extending test-time scaling into a wider range of domains, such as medicine and finance, where complex decision-making and structured reasoning are critical.This expansion is both inevitable and promising, as test-time scaling offers a powerful mechanism to enhance reasoning depth, adapt computation dynamically, and improve accuracy without requiring costly retraining.Beyond these fields, we can expect widespread applications in law, AI evaluation, open-domain QA, and other high-stakes or knowledge-intensive areas.Despite its potential, scaling test-time reasoning across domains presents several key challenges:
6. Thoughtology: How do the reasoning patterns in its language help improve reasoning effectiveness by treatinga finetuned reasoning model as an agent? Recent studies, such as Marjanović et al. (2025); Wu et al. (2024a),have also explored this question.8.3 Optimizing Scaling is the Key1. Balancing Cost and Accuracy: Unlike general NLP tasks, specialized domains often require strict computa-tional efficiency and reliability;2. Ensuring Domain-Specific Interpretability: In fields like medicine and law, outputs must be transparent andjustifiable;3. Integrating External Knowledge &amp; Real-World Constraints: Many domains require retrieval-augmentedgeneration, real-time data analysis, or interactive query refinement;4. Future research must identify generalizable test-time scaling strategies that are robust across diverse reasoningtasks.</p>
<p>Best-of-N . . . . . . . . . . . . . . .40 B.2 Majority Voting . . . . . . . . . . . .41 B.3 Process Reward Model . . . . . . . .41 B.4 MCTS . . . . . . . . . . . . . . . . .41 B.5 Self-Refine . . . . . . . . . . . . . .42 B.6 Tree-of-Thought . . . . . . . . . . . .42 B.7 Reinforcement Learning . . . . . . .43 A Detailed Outcome Verification Methods This appendix expands on the outcome verification techniques employed at test time in LLMs.Unlike training-time methods (e.g., RL fine-tuning), these techniques operate on the fly during inference, often by generating multiple solutions and using a proposer-verifier framework.
Contents1 Introduction15.3 Controllability . . . . . . . . . . . . .162 What to Scale 2.1 Parallel Scaling . . . . . . . . . . . . 2.2 Sequential Scaling . . . . . . . . . .2 4 45.4 Scalability . . . . . . . . . . . . . . . 6 Organization and Trends in Test-time scaling 17 172.3 Hybrid Scaling . . . . . . . . . . . . 2.4 Internal Scaling . . . . . . . . . . . .4 57 A Hand-on Guideline for Test-time Scaling 183 How to Scale 3.1 Tuning-based Approaches . . . . . . .5 58 Challenges and Opportunities 8.1 More Scaling is the Frontier . . . . .19 193.1.1 Supervised Finetuning (SFT) .58.2 Clarifying the Essence of Techniques3.1.2 Reinforcement Learning (RL)6in Scaling is the Foundation . . . . .203.2 Inference-based Approaches . . . . . 3.2.1 Stimulation . . . . . . . . . . 3.2.2 Verification . . . . . . . . . . 3.2.3 Search . . . . . . . . . . . . . 3.2.4 Aggregation . . . . . . . . . .6 7 8 9 108.3 Optimizing Scaling is the Key . . . . 8.4 Generalization across Domains is the Mainstream . . . . . . . . . . . . . . 9 Conclusion21 21 214 Where to Scale 4.1 Reasoning-intensive Tasks . . . . . . 4.2 Agentic Tasks . . . . . . . . . . . . . 4.3 Others . . . . . . . . . . . . . . . . .10 10 11 12A Detailed Outcome Verification Methods A.1 Verifier Model-Based Scoring . . . . A.2 Self-Consistency and Voting Mechanisms 40 40 40 A.3 Tool-Assisted and Heuristic Verification 405 How Well to Scale 5.1 Performance . . . . . . . . . . . . . . 5.2 Efficiency . . . . . . . . . . . . . . . 5.2.1 Key Concern and Multi-13 13 14B Representative Methods B.140Dimensional Perspectives ofEfficiency . . . . . . . . . . .145.2.2 General Computational CostMetrics . . . . . . . . . . . . 5.2.3 Reasoning Efficiency Metrics15 15
Author ContributionsBelow, we list the individual author contributions: Qiyuan Zhang and Fuyuan Lyu are core contributors who coordinate and finalize the full paper.Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, Wenyue Hua and Haolun Wu are significant contributors who are responsible for certain chapters of this paper.Yufei Wang provides the overall structures of the taxonomy and provides close supervision during the process.Niklas Muennighoff, Irwin King, Xue Liu, and Chen Ma provide insightful feedback and high-level suggestions on this survey overall.B.2 Majority VotingMajority voting is a fundamental ensemble strategy for TTS that aggregates multiple independent predictions to make a final decision.In this approach, each model or inference (voter) casts a vote for a predicted outcome, and the output chosen is the one with the highest number of votes (i.e., the mode of the predictions).Formally, given an ensemble of M models h 1 , h 2 , . . ., h M each producing a prediction h m (x) for input x, the majority vote outcome is defined aswhere 1{•} is the indicator function and c ranges over all possible classes or outputs.This test-time inference technique leverages additional computing at inference to improve reliability without retraining models, and it is widely used in real-world applications, such as combining votes of decision trees in a random forest, consolidating crowd-sourced annotations, or enhancing the consistency of answers from LLMs by selecting the most frequent response.B.3 Process Reward ModelA Process Reward Model (PRM)(Uesato et al., 2022;Pfau et al., 2024) is a reward model designed to evaluate an entire reasoning trajectory on a step-by-step basis.Formally, given an input problem x and a sequence of reasoning steps z 1 , z 2 , . . ., z T leading to a final output y, we can represent this full reasoning trace as:and define the PRM as a function that assigns a real-valued score: r : S T → R, mapping a possible reasoning process S T to a reward score(Choudhury, 2025;Ma et al., 2025c).Intuitively, r(S T ) is higher when the reasoning process is logical, valid, and leads to a correct solution, and lower (or negative) when the reasoning is flawed.PRMs are typically trained on human or algorithmic annotations for each step, internalizing a notion of "partial credit" to evaluate correctness and relevance at each stage.PRMs play a crucial role in TTS strategies such as stepwise beam search and self-consistency verification.They have been successfully applied in mathematical reasoning, code generation, automated theorem proving, and decision-making tasks.By leveraging PRMs, models can optimize not only for correctness but also for process coherence, making AI systems more transparent and robust.B.4 MCTSMonte Carlo Tree Search (MCTS) is a simulation-based decision-making algorithm for sequential decision problems, often formalized as a Markov Decision Process (MDP).It incrementally builds a search tree by sampling many possible future trajectories (playouts) and using their outcomes to estimate the value of decisions.Unlike brute-force search, MCTS selectively explores the most promising actions by balancing exploration (trying unexplored or uncertain moves) and exploitation (favoring moves with high estimated reward).Each iteration of MCTS consists of four phases:1. Selection -Recursively select child actions that maximize a heuristic value until reaching a leaf node.A common selection strategy is the Upper Confidence Bound for Trees (UCT):where w a is the total simulation reward, n a is the visit count for action a, N is the total simulations from the parent state, and c &gt; 0 is an exploration constant.2. Expansion -Once a leaf state is reached, new child nodes are created by simulating unexplored actions.3. Simulation (Rollout) -Perform a Monte Carlo simulation by selecting actions to simulate a full episode to the end, providing an estimate of the node's value.4. Backpropagation -Propagate the simulation result back up the tree, updating the statistics of each node along the path.
Pranjal Aggarwal, Sean Welleck, arXivL1: Controlling how long a reasoning model thinks with reinforcement learning. 2025</p>
<p>Back to basics: Revisiting reinforce-style optimization for learning from human feedback in llms. Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, Sara Hooker, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241Aider. AntResearch-RL-Lab. 2025. Areal: Ant reasoning rl</p>
<p>Have LLMs advanced enough? a challenging problem solving benchmark for large language models. Daman Arora, Himanshu Gaurav Singh, Mausam , Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Self-rag: Learning to retrieve, generate, and critique through self-reflection. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Sketch-of-thought: Efficient llm reasoning with adaptive cognitive-inspired sketching. Simon A Aytes, Jinheon Baek, Sung Ju Hwang, arXiv2025</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, arXivConstitutional ai: Harmlessness from ai feedback. Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam Mccandlish, Tom Brown, Jared Kaplan, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston2022</p>
<p>Longbench: A bilingual, multitask benchmark for long context understanding. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li, 2024In arXiv</p>
<p>Bespoke-stratos: The unreasonable effectiveness of reasoning distillation. Bespoke, 2025Accessedthe-unreasonable-effectiveness-of-reasoning-distillation</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Torsten Hoefler, AAAI Conference on Artificial Intelligence. 2024</p>
<p>What's the magic word? a control theory of llm prompting. Aman Bhargava, Cameron Witkowski, Shi-Zhuo, Matt Looi, Thomson, arXiv2024</p>
<p>Zhenni Bi, Kai Han, Chuanjian Liu, Yehui Tang, Yunhe Wang, arXiv:2412.09078Forest-of-thought: Scaling test-time compute for enhancing llm reasoning. 2024arXiv preprint</p>
<p>Large language monkeys: Scaling inference compute with repeated sampling. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V Le, Christopher Ré, Azalia Mirhoseini, arXiv2024</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, arXivIlya Sutskever, and Dario Amodei. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford2020</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, Sparks of artificial general intelligence: Early experiments with gpt-4. 2023In arXiv</p>
<p>Ppl-mcts: Constrained textual generation through discriminator-guided mcts decoding. Antoine Chaffin, Vincent Claveau, Ewa Kijak, NAACL 2022-Conference of the North American Chapter. Human Language Technologies2022</p>
<p>Collab: Controlled decoding using mixture of agents for LLM alignment. Souradip Chakraborty, Sujay Bhatt, Udari Madhushani Sehwag, Soumya Suvra Ghosal, Jiahao Qiu, Mengdi Wang, Dinesh Manocha, Furong Huang, Alec Koppel, Sumitra Ganesh, International Conference on Learning Representations. 2025</p>
<p>Alphamath almost zero: Process supervision without process. Guoxin Chen, Minpeng Liao, Chengxi Li, Kai Fan, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024a</p>
<p>Benchmarking large language models on answering and explaining challenging medical questions. Hanjie Chen, Zhouxiang Fang, Yash Singla, Mark Dredze, arXiv2025a</p>
<p>Justin Chih, -Yao Chen, arXivSwarnadeep Saha, and Mohit Bansal. 2024b. Reconcile: Round-table conference improves reasoning via consensus among diverse llms. </p>
<p>Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, arXiv:2403.20330Are we on the right way for evaluating large vision-language models?. 2024carXiv preprint</p>
<p>Are more llm calls all you need? towards scaling laws of compound inference systems. Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis, Ion Stoica, Matei Zaharia, James Zou, arXiv:2403.024192024darXiv preprint</p>
<p>Are more LLM calls all you need? towards the scaling properties of compound AI systems. Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis, Ion Stoica, Matei Zaharia, James Zou, Conference on Neural Information Processing Systems. 2024e</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De, Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>JudgeLRM: Large reasoning models as a judge. Nuo Chen, Zhiyuan Hu, Qingyun Zou, Jiaying Wu, Qian Wang, Bryan Hooi, Bingsheng He, arXiv2025b</p>
<p>Towards reasoning era: A survey of long chain-of-thought for reasoning large language models. Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, Wanxiang Che, 2025c</p>
<p>RouterDC: Query-based router by dual contrastive learning for assembling large language models. Shuhao Chen, Weisen Jiang, Baijiong Lin, James T Kwok, Yu Zhang, arXiv2024f</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Weizhe Chen, Sven Koenig, Bistra Dilkina, Transactions on Machine Learning Research. arXiv. Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen2025d. 2023aIterative deepening sampling for large language models</p>
<p>TheoremQA: A theorem-driven question answering dataset. Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, Tony Xia, Conference on Empirical Methods in Natural Language Processing. 2023b</p>
<p>Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, arXiv:2412.21187Do not think that much for 2+ 3=? on the overthinking of o1-like llms. 2024garXiv preprint</p>
<p>Teaching large language models to self-debug. Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou, International Conference on Learning Representations. 2024h</p>
<p>Brain-inspired two-stage approach: Enhancing mathematical reasoning by imitating human thought processes. Yezeng Chen, Zui Chen, Yi Zhou, arXiv2024i</p>
<p>Scaling autonomous agents via automatic reward modeling and planning. Zhenfang Chen, Delin Chen, Rui Sun, Wenjun Liu, Chuang Gan, arXiv:2502.121302025earXiv preprint</p>
<p>Scaling autonomous agents via automatic reward modeling and planning. Zhenfang Chen, Delin Chen, Rui Sun, Wenjun Liu, Chuang Gan, arXiv2025f</p>
<p>When is tree search useful for LLM planning? it depends on the discriminator. Ziru Chen, Michael White, Ray Mooney, Ali Payani, Yu Su, Huan Sun, Annual Meeting of the Association for Computational Linguistics. 2024j</p>
<p>Spar: Self-play with tree-search refinement to improve instruction-following in large language models. Jiale Cheng, Xiao Liu, Cunxiang Wang, Xiaotao Gu, Yida Lu, Dan Zhang, Yuxiao Dong, Jie Tang, Hongning Wang, Minlie Huang, arXiv2025</p>
<p>Over-reasoning and redundant calculation of large language models. Cheng- , Han Chiang, Hung-Yi Lee, arXiv:2401.114672024arXiv preprint</p>
<p>On the measure of intelligence. Franc ¸ois Chollet. 2019In arXiv</p>
<p>Process reward models for llm agents: Practical framework and directions. Sanjiban Choudhury, arXiv2025</p>
<p>Theoretical physics benchmark (tpbench) -a dataset and study of AI reasoning capabilities in theoretical physics. J H Daniel, Zhiqi Chung, Yurii Gao, Tianyi Kvasiuk, Moritz Li, Maja Münchmeyer, Frederic Rudolph, Sai Sala, Tadepalli Chaitanya, 10.48550/ARXIV.2502.15815CoRR, abs/2502.158152025</p>
<p>Chinese national high school mathematics olympiad. 2025CMS</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168.codeforce.2025.CodeforcesTraining verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Efficient selectivity and backup operators in monte-carlo tree search. International conference on computers and games. Springer2006</p>
<p>Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, arXiv:2502.01456Process reinforcement through implicit rewards. ess reinforcement through implicit rewards2025arXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Deepseek-Ai , arXiv2025</p>
<p>Length-controlled alpacaeval: A simple way to debias automatic evaluators. Yann Dubois, Balázs Galambosi, Percy Liang, Tatsunori B Hashimoto, arXiv2024</p>
<p>Scaling test-time compute with open models. Sasha Rush, Edward Beeching, Lewis Tunstall, 2024</p>
<p>Heuristic and analytic processes in reasoning. Jonathan Evans, British Journal of Psychology. 7541984</p>
<p>Kaiyue Feng, Yilun Zhao, Yixin Liu, Tianyu Yang, Chen Zhao, John Sous, Arman Cohan, arXiv:2503.21821Physics: Benchmarking foundation models on university-level physics problem solving. 2025arXiv preprint</p>
<p>Diverseagententropy: Quantifying black-box llm uncertainty through diverse perspectives and multi-agent interaction. Yu Feng, Mon Phu, Zheng Htut, Wei Qi, Manuel Xiao, Nikolaos Mager, Kishaloy Pappas, Yang Halder, Yassine Li, Dan Benajiba, Roth, arXiv2024</p>
<p>Llm self-correction with decrim: Decompose, critique, and refine for enhanced following of instructions with multiple constraints. Thomas Palmeira Ferraz, Kartik Mehta, Yu-Hsiang Lin, Haw-Shiuan Chang, Shereen Oraby, Sijia Liu, Vivek Subramanian, Tagyoung Chung, Mohit Bansal, Nanyun Peng, Findings of the Association for Computational Linguistics: EMNLP 2024. 2024</p>
<p>and Ion Stoica. 2024. How to evaluate reward models for rlhf. Evan Frick, Tianle Li, Connor Chen, Wei-Lin Chiang, Anastasios N Angelopoulos, Jiantao Jiao, Banghua Zhu, Joseph E Gonzalez, arXiv</p>
<p>Stream of search (sos): Learning to search in language. Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, Noah D Goodman, arXiv2024</p>
<p>Bofei Gao, Zefan Cai, Runxin Xu, Peiyi Wang, Ce Zheng, Runji Lin, Keming Lu, Dayiheng Liu, Chang Zhou, Wen Xiao, Junjie Hu, Tianyu Liu, Baobao Chang, arXivLlm critics help catch bugs in mathematics: Towards a better mathematical verifier with natural language feedback. 2024a</p>
<p>Omni-MATH: A universal olympiad level mathematic benchmark for large language models. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Chenghao Ma, Shanghaoran Quan, Liang Chen, Qingxiu Dong, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Ge Zhang, Lei Li, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, Baobao Chang, International Conference on Learning Representations. 2025a</p>
<p>Principled data selection for alignment: The hidden risks of difficult examples. Chengqian Gao, Haonan Li, Liu Liu, Zeke Xie, Peilin Zhao, Zhiqiang Xu, arXiv:2502.096502025barXiv preprint</p>
<p>Interpretable contrastive monte carlo tree search reasoning. Zitian Gao, Boye Niu, Xuzheng He, Haotian Xu, Hongzhang Liu, Aiwei Liu, Xuming Hu, Lijie Wen, arXiv2024b</p>
<p>Scaling up test-time compute with latent reasoning: A recurrent depth approach. Jonas Geiping, Sean Mcleish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Tom Goldstein, arXiv2025</p>
<p>. Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily De Oliveira Santos, Olli Järviniemi, Matthew Barnett, Robert Sandler, Matej Vrzala, Jaime Sevilla, Lionel Levine, Grant Barkley, Natalie Stewart, Bogdan Grechuk, Tetiana Grechuk, Shreepranav Varma Enugandla, and Mark Wildon2024Qiuyu Ren, Elizabeth PrattFrontiermath: A benchmark for evaluating advanced mathematical reasoning in ai</p>
<p>Artificial general intelligence: Concept, state of the art, and future prospects. Ben Goertzel, Journal of Artificial General Intelligence. 2014</p>
<p>Gemini 2.0 flash thinking. Google. 2025. Aime problems and solutions. Google, 2024</p>
<p>CRITIC: Large language models can self-correct with tool-interactive critiquing. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Nan Duan, Weizhu Chen, International Conference on Learning Representations. 2024</p>
<p>Chinese graduate school entrance examinations. GSEE. 2025</p>
<p>rstar-math: Small llms can master math reasoning with self-evolved deep thinking. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, Mao Yang, arXiv2025</p>
<p>Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, Johan Ferret, and Mathieu Blondel. 2024. Direct language model alignment from online ai feedback. </p>
<p>Token-budgetaware llm reasoning. Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, Zhenyu Chen, arXiv2025</p>
<p>Training large language models to reason in a continuous latent space. Shibo Hao, Sainbayar Sukhbaatar, Dijia Su, Xian Li, Zhiting Hu, Jason Weston, Yuandong Tian, arXiv:2412.067692024arXiv preprint</p>
<p>OlympiadBench: A challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, Maosong Sun, Annual Meeting of the Association for Computational Linguistics. 2024a</p>
<p>Enhancing llm reasoning with multi-path collaborative reactive and reflection agents. Chengbo He, Bochao Zou, Xin Li, Jiansheng Chen, Junliang Xing, Huimin Ma, arXiv2025</p>
<p>Webvoyager: Building an end-to-end web agent with large multimodal models. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, Dong Yu, arXiv2024b</p>
<p>Yancheng He, Shilong Li, Jiaheng Liu, Yingshui Tan, Weixun Wang, Hui Huang, Xingyuan Bu, Hangyu Guo, Chengwei Hu, Boren Zheng, Zhuoran Lin, Xuepeng Liu, Dekai Sun, Shirong Lin, Zhicheng Zheng, Xiaoyong Zhu, Wenbo Su, Bo Zheng, Chinese simpleqa: A chinese factuality evaluation for large language models. 2024c</p>
<p>Measuring mathematical problem solving with the MATH dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021</p>
<p>Scaling laws for single-agent reinforcement learning. Jacob Hilton, Jie Tang, John Schulman, arXiv:2301.134422023arXiv preprint</p>
<p>Training compute-optimal large language models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Tom Clark, Eric Hennigan, Katie Noland, George Millican, Bogdan Van Den Driessche, Aurelia Damoc, Simon Guy, Karen Osindero, Simonyan, arXivErich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre2022</p>
<p>Advances in reasoning by prompting large language models: A survey. Ruixin Hong, Xinyu Pang, Changshui Zhang, Cybernetics and Intelligence. 2024</p>
<p>Ets: Efficient tree search for inference-time scaling. Coleman Hooper, Sehoon Kim, Suhong Moon, Kerem Dilmen, Monishwaran Maheswaran, Nicholas Lee, Michael W Mahoney, Sophia Shao, Kurt Keutzer, Amir Gholami, arXiv2025</p>
<p>V-star: Training verifiers for self-taught reasoners. Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, Rishabh Agarwal, First Conference on Language Modeling. 2024</p>
<p>Advancing model reasoning through reinforcement learning and inference scaling. Zhenyu Hou, Xin Lv, Rui Lu, Jiajie Zhang, Yujiang Li, Zijun Yao, Juanzi Li, Jie Tang, Yuxiao Dong, 2025</p>
<p>Jian Hu, Jason Klein Liu, Shen Wei, arXiv:2501.03262Reinforce++: A simple and efficient approach for aligning large language models. 2025aarXiv preprint</p>
<p>Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. Jian Hu, Xibin Wu, Zilin Zhu, Weixun Xianyu, Dehao Wang, Yu Zhang, Cao, arXiv:2405.111432024arXiv preprint</p>
<p>Openreasoner-zero: An open source approach to scaling reinforcement learning on the base model. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Heung-Yeung Shum, Xiangyu Zhang, 2025b</p>
<p>Lean and mean: Decoupled value policy optimization with global value guidance. Chenghua Huang, Lu Wang, Fangkai Yang, Pu Zhao, Zhixu Li, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Qi Zhang, arXiv:2502.169442025aarXiv preprint</p>
<p>Efficient test-time scaling via self-calibration. Chengsong Huang, Langlin Huang, Jixuan Leng, Jiacheng Liu, Jiaxin Huang, arXiv2025b</p>
<p>MUSTARD: mastering uniform synthesis of theorem and proof data. Yinya Huang, Xiaohan Lin, Zhengying Liu, Qingxing Cao, Huajian Xin, Haiming Wang, Zhenguo Li, Linqi Song, Xiaodan Liang, The Twelfth International Conference on Learning Representations, ICLR 2024. Vienna, Austria2024a. May 7-11, 2024OpenReview.net</p>
<p>Maosong Sun, and Junxian He. 2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, Conference on Neural Information Processing Systems Datasets and Benchmarks Track. </p>
<p>Olympicarena: Benchmarking multi-discipline cognitive reasoning for superintelligent AI. Zhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li, Haoyang Zou, Ruijie Xu, Run-Ze Fan, Lyumanshan Ye, Ethan Chern, Yixin Ye, Yikai Zhang, Yuqing Yang, Ting Wu, Binjie Wang, Shichao Sun, Yang Xiao, Yiyuan Li, Fan Zhou, Steffi Chern, Yiwei Qin, Yan Ma, Jiadi Su, Yixiu Liu, Yuxiang Zheng, Shaoting Zhang, Dahua Lin, Yu Qiao, Pengfei Liu, Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024b</p>
<p>O1 replication journey -part 2: Surpassing o1-preview through simple distillation. Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, Pengfei Liu, 2024cbig progress or bitter lesson? In arXiv</p>
<p>Open r1: A fully open reproduction of deepseek-r1. Huggingface, 2025</p>
<p>Rewarding chatbots for real-world engagement with millions of users. Robert Irvine, Douglas Boubert, Raina Vyas, Adian Liusie, Ziyi Zhu, Vineet Mudupalli, Aliaksei Korshuk, Zongyi Liu, Fritz Cremer, Valentin Assassi, Christie-Carol Beauchamp, Xiaoding Lu, Thomas Rialan, William Beauchamp, 2023In arXiv</p>
<p>Livecodebench: Holistic and contamination free evaluation of large language models for code. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, International Conference on Learning Representations. 2025Armando Solar-Lezama, Koushik Sen, and Ion Stoica</p>
<p>Enhancing multi-step reasoning abilities of language models through direct q-function optimization. Kaixuan Ji, Guanlin Liu, Ning Dai, Qingping Yang, Renjie Zheng, Zheng Wu, Chen Dun, Quanquan Gu, Lin Yan, arXiv:2410.093022024arXiv preprint</p>
<p>Yixin Ji, Juntao Li, Hai Ye, Kaixin Wu, Jia Xu, Linjian Mo, Min Zhang, arXiv:2501.02497Test-time computing: from system-1 thinking to system-2 thinking. 2025arXiv preprint</p>
<p>Tigerscore: Towards building explainable metric for all text generation tasks. Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, Wenhu Chen, arXiv2024a</p>
<p>LLM-blender: Ensembling large language models with pairwise ranking and generative fusion. Dongfu Jiang, Xiang Ren, Bill Yuchen, Lin , Annual Meeting of the Association for Computational Linguistics. 2023</p>
<p>Followbench: A multi-level fine-grained constraints following benchmark for large language models. Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, Wei Wang, arXiv2024b</p>
<p>SWE-bench: Can language models resolve real-world github issues. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik R Narasimhan, International Conference on Learning Representations. 2024</p>
<p>What disease does this patient have? a large-scale open domain question answering dataset from medical exams. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, Peter Szolovits, arXiv2020</p>
<p>well, keep thinking": Enhancing llm reasoning with adaptive injection decoding. Hyunbin Jin, Je Won Yeom, Seunghyun Bae, Taesup Kim, arXiv2025</p>
<p>The impact of reasoning step length on large language models. Mingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, Mengnan Du, Findings of the Association for Computational Linguistics ACL 2024. 2024</p>
<p>D Kahneman, Thinking, Fast and Slow. Farrar, Straus and Giroux. 2011</p>
<p>Maps of bounded rationality: Psychology for behavioral economics. Daniel Kahneman, The American Economic Review. 9352003</p>
<p>Mindstar: Enhancing math reasoning in pre-trained llms at inference time. Jikun Kang, Xin Zhe Li, Xi Chen, Amirreza Kazemi, Qianyi Sun, Boxing Chen, Dong Li, Xu He, Quan He, Feng Wen, Jianye Hao, arXivJun Yao. 2024</p>
<p>Scalable best-of-n selection for large language models via self-certainty. Zhewei Kang, Xuandong Zhao, Dawn Song, arXiv2025</p>
<p>Scaling laws for neural language models. Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv2020</p>
<p>Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, Nicolas Le Roux, arXiv:2410.01679Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment. 2024arXiv preprint</p>
<p>Scaling evaluation-time compute with reasoning models as process evaluators. Seungone Kim, Ian Wu, Jinu Lee, Xiang Yue, Seongyun Lee, Mingyeong Moon, Kiril Gashteovski, Carolin Lawrence, Julia Hockenmaier, Graham Neubig, Sean Welleck, arXiv2025</p>
<p>Kimi k1.5: Scaling reinforcement learning with llms. Kimi , arXiv2025</p>
<p>Scalable language models with posterior inference of latent thought vectors. Deqian Kong, Minglu Zhao, Dehong Xu, Bo Pang, Shu Wang, Edouardo Honig, Zhangzhang Si, Chuan Li, Jianwen Xie, Sirui Xie, Ying Nian Wu, arXiv2025</p>
<p>Fact, fetch, and reason: A unified evaluation of retrieval-augmented generation. Satyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay, Manaal Faruqui, 2025</p>
<p>Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James, V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. 2025. Tulu 3: Pushing frontiers in open language model post-training. In arXiv</p>
<p>Rewardbench: Evaluating reward models for language modeling. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, Bill Miranda, Khyathi Yuchen Lin, Nouha Chandu, Sachin Dziri, Tom Kumar, Yejin Zick, Noah A Choi, Hannaneh Smith, Hajishirzi, arXiv2024</p>
<p>Dipper: Diversity in prompts for producing large language model ensembles in reasoning tasks. Gregory Kang, Ruey Lau, Wenyang Hu, Diwen Liu, Jizhuo Chen, See-Kiong Ng, Bryan Kian, Hsiang Low, arXiv2024</p>
<p>Shumeet Baluja, Dale Schuurmans, and Xinyun Chen. 2025. Evolving deeper llm thinking. Kuang-Huei Lee, Ian Fischer, Yueh-Hua Wu, Dave Marwood, arXiv</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, Vedant Misra, Conference on Neural Information Processing Systems. 2022</p>
<p>METAL: A multi-agent framework for chart generation with test-time scaling. Bingxuan Li, Yiwei Wang, Jiuxiang Gu, Kai-Wei Chang, Nanyun Peng, arXiv2025a</p>
<p>Chengpeng Li, Mingfeng Xue, Zhenru Zhang, Jiaxi Yang, Beichen Zhang, Xiang Wang, Bowen Yu, Binyuan Hui, Junyang Lin, Dayiheng Liu, arXivSTART: Self-taught reasoner with tools. 2025b</p>
<p>Reasoning-as-logic-units: Scaling test-time reasoning in large language models through logic unit alignment. Cheryl Li, Tianyuan Xu, Yiwen Guo, arXiv2025c</p>
<p>Dacheng Li, Shiyi Cao, Chengkun Cao, Xiuyu Li, Shangyin Tan, Kurt Keutzer, Jiarong Xing, Joseph E Gonzalez, and Ion Stoica. 2025d. S*: Test time scaling for code generation. </p>
<p>Llms can easily learn to reason from demonstrations structure. Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Eric Tang, Sumanth Hegde, Kourosh Hakhamaneshi, G Shishir, Matei Patil, Joseph E Zaharia, Ion Gonzalez, Stoica, 2025enot content, is what matters! In arXiv</p>
<p>Cmmlu: Measuring massive multitask language understanding in chinese. Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, Timothy Baldwin, arXiv2024</p>
<p>. L I Jia, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Costa Shengyi, Kashif Huang, Longhui Rasul, Albert Yu, Ziju Jiang, Zihan Shen, Bin Qin, Li Dong, Yann Zhou, Guillaume Fleureau, Stanislas Lample, Polu, 2024</p>
<p>More agents is all you need. Junyou Li, Qin Zhang, Yangbin Yu, Qiang Fu, Deheng Ye, arXiv:2402.051202024aarXiv preprint</p>
<p>Minzhi Li, Zhengyuan Liu, Shumin Deng, Shafiq Joty, Nancy F Chen, Min-Yen Kan, arXivDna-eval: Enhancing large language model evaluation through decomposition and aggregation. 2024b</p>
<p>From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E Gonzalez, Ion Stoica, arXiv2024c</p>
<p>From drafts to answers: Unlocking llm potential via aggregation fine-tuning. Yafu Li, Zhilin Wang, Tingchen Fu, Ganqu Cui, Sen Yang, Yu Cheng, arXiv2025f</p>
<p>Learning to reason from feedback at test-time. Yanyang Li, Michael Lyu, Liwei Wang, 2025gIn arXiv</p>
<p>Making language models better reasoners with step-aware verifier. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics2023a1</p>
<p>Competition-level code generation with alphacode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien De Masson D'autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Science. 2022Koray Kavukcuoglu, and Oriol Vinyals</p>
<p>Chain of thought empowers transformers to solve inherently serial problems. Zhiyuan Li, Hong Liu, Denny Zhou, Tengyu Ma, The Twelfth International Conference on Learning Representations. 2024d</p>
<p>Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, arXiv:2502.17419From system 1 to system 2: A survey of reasoning large language models. 2025harXiv preprint</p>
<p>CMMaTH: A Chinese multi-modal math skill evaluation benchmark for foundation models. Zhongzhi Li, Ming-Liang Zhang, Pei-Jie Wang, Jian Xu, Rui-Song Zhang, Yin Fei, Zhi-Long Ji, Jin-Feng Bai, Zhen-Ru Pan, Jiaxin Zhang, Cheng-Lin Liu, International Conference on Computational Linguistics. 2025i</p>
<p>Llms can generate a better answer by aggregating their own responses. Zichong Li, Xinyu Feng, Yuheng Cai, Zixuan Zhang, Tianyi Liu, Chen Liang, Weizhu Chen, Haoyu Wang, Tuo Zhao, arXiv2025j</p>
<p>Remax: A simple, effective, and efficient reinforcement learning method for aligning large language models. Ziniu Li, Tian Xu, Yushun Zhang, Zhihang Lin, Yang Yu, Ruoyu Sun, Zhi-Quan Luo, arXiv:2310.105052023barXiv preprint</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, Zhaopeng Tu, Conference on Empirical Methods in Natural Language Processing. 2024</p>
<p>Multi-agent verification: Scaling test-time compute with goal verifiers. Shalev Lifshitz, Sheila A Mcilraith, Yilun Du, Workshop on Reasoning and Planning for Large Language Models. 2025</p>
<p>Let's verify step by step. Vineet Hunter Lightman, Yuri Kosaraju, Harrison Burda, John Edwards ; Leike, Ilya Schulman, Karl Sutskever, Cobbe, The Twelfth International Conference on Learning Representations. Bowen Baker, Teddy LeeJan. 2023</p>
<p>Leveraging constrained monte carlo tree search to generate reliable long chain-of-thought for mathematical reasoning. Qingwen Lin, Boyan Xu, Zijian Li, Zhifeng Hao, Keli Zhang, Ruichu Cai, arXiv2025</p>
<p>Zicheng Lin, Tian Liang, Jiahao Xu, Xing Wang, Ruilin Luo, Chufan Shi, Siheng Li, Yujiu Yang, Zhaopeng Tu, arXiv:2411.19943Critical tokens matter: Token-level contrastive estimation enhence llm's reasoning capability. 2024arXiv preprint</p>
<p>Deductive verification of chain-of-thought reasoning. Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, Hao Su, Advances in Neural Information Processing Systems. 202336</p>
<p>Codemind: A framework to challenge large language models for code reasoning. Changshu Liu, Dylan Shizhuo, Ali Zhang, Reyhaneh Reza Ibrahimzada, Jabbarvand, arXiv2024a</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Advances in neural information processing systems. 2023a36</p>
<p>Improving multi-step reasoning abilities of large language models with direct advantage policy optimization. Jiacai Liu, Chaojie Wang, Chris Yuhao Liu, Liang Zeng, Rui Yan, Yiwen Sun, Yang Liu, Yahui Zhou, arXiv2024b</p>
<p>Can 1b llm surpass 405b llm? rethinking compute-optimal test-time scaling. Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, Bowen Zhou, arXiv:2502.067032025aarXiv preprint</p>
<p>Plan, verify and switch: Integrated reasoning with diverse x-of-thoughts. Tengxiao Liu, Qipeng Guo, Yuqing Yang, Xiangkun Hu, Yue Zhang, Xipeng Qiu, Zheng Zhang, Conference on Empirical Methods in Natural Language Processing. 2023b</p>
<p>G-eval: Nlg evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, arXiv2023c</p>
<p>Rm-bench: Benchmarking reward models of language models with subtlety and style. Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, Juanzi Li, arXiv2024c</p>
<p>Pairjudge rm: Perform best-of-n sampling with knockout tournament. Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, Juanzi Li, arXiv2025b</p>
<p>Mmbench: Is your multi-modal model an all-around player?. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, European Conference on Computer Vision. 2024d</p>
<p>Inference-time scaling for generalist reward modeling. Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, Yu Wu, arXiv2025c</p>
<p>Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao, arXiv2024</p>
<p>Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. 2025a. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning. Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, 2025</p>
<p>Deepscaler: Surpassing o1-preview with a 1.5b model by scaling rl. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, Raluca , Ada Popa, Ion Stoica, DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8 Notion Blog. 2025b</p>
<p>Reft: Reasoning with reinforced fine-tuning. Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, Hang Li, arXiv2024</p>
<p>Non-myopic generation of language models for reasoning and planning. Chang Ma, Haiteng Zhao, Junlei Zhang, Junxian He, Lingpeng Kong, International Conference on Learning Representations. 2025a</p>
<p>Inference-time scaling for diffusion models beyond scaling denoising steps. Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, Saining Xie, arXiv2025b</p>
<p>What are step-level reward models rewarding? counterintuitive findings from mcts-boosted mathematical reasoning. Yiran Ma, Zui Chen, Tianqiao Liu, Mi Tian, Zhuo Liu, Zitao Liu, Weiqi Luo, arXiv2025c</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark, Conference on Neural Information Processing Systems. 2023</p>
<p>Enhancing llm code generation with ensembles: A similarity-based selection approach. Tarek Mahmud, Bin Duan, Corina Pasareanu, Guowei Yang, arXiv2025</p>
<p>Sara Vera Marjanović, Arkil Patel, Vaibhav Adlakha, Milad Aghajohari, Parishad Behnamghader, Mehar Bhatia, Aditi Khandelwal, Austin Kraft, Benno Krojer, Xing Han Lù, Nicholas Meade, Dongchan Shin, Amirhossein Kazemnejad, Gaurav Kamath, Marius Mosbach, Karolina Stańczak, Siva Reddy, arXivDeepseek-r1 thoughtology: Let's ¡think¿ about llm reasoning. 2025</p>
<p>Llm critics help catch llm bugs. Nat Mcaleese, Rai Michael Pokorny, Juan Felipe Ceron, Evgenia Uribe, Maja Nitishinskaya, Trebacz, arXivJan Leike. 2024</p>
<p>Simpo: Simple preference optimization with a reference-free reward. Yu Meng, Mengzhou Xia, Danqi Chen, Advances in Neural Information Processing Systems. 202437</p>
<p>Position: Scaling llm agents requires asymptotic analysis with llm primitives. Elliot Meyerson, Xin Qiu, arXiv:2502.043582025arXiv preprint</p>
<p>Wider or deeper? scaling llm inference-time compute with adaptive branching tree search. Kou Misaki, Yuichi Inoue, Yuki Imajuku, So Kuroki, Taishi Nakamura, Takuya Akiba, arXiv2025</p>
<p>Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang , Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, arXiv</p>
<p>Self-training elicits concise reasoning in large language models. Tergel Munkhbat, Namgyu Ho, Hyun Seo, Yongjin Kim, Yujin Yang, Se-Young Kim, Yun, arXiv2025</p>
<p>China's national college entrance examination. 2025NCEE</p>
<p>When is the consistent prediction likely to be a correct prediction. Alex Nguyen, Dheeraj Mekala, Chengyu Dong, Jingbo Shang, 2024In arXiv</p>
<p>Next: Teaching large language models to reason about code execution. Ansong Ni, Miltiadis Allamanis, Arman Cohan, Yinlin Deng, Kensen Shi, Charles Sutton, Pengcheng Yin, arXiv2024</p>
<p>Lever: Learning to verify language-to-code generation with execution. Ansong Ni, Srini Iyer, Dragomir Radev, Ves Stoyanov, Wen Tau Yih, Sida I Wang, Xi Victoria, Lin , arXiv2023</p>
<p>Sky-t1: Train your own o1 preview model within $450. Novasky, 2025</p>
<p>RouteLLM: Learning to route LLMs from preference data. Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph E Gonzalez, Waleed Kadous, Ion Stoica, International Conference on Learning Representations. 2025</p>
<p>Openai o1 system card. arXiv. OpenAI. 2025Openai o3-mini system card. 2024aOpenAIGpt-4 technical report</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Advances in Neural Information Processing Systems. Jan Leike, and Ryan Lowe. 202235</p>
<p>Coat: Chain-of-associated-thoughts framework for enhancing large language models reasoning. Jianfeng Pan, Senyou Deng, Shaomang Huang, arXiv2025a</p>
<p>. Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, Alane Suhr, 2025b</p>
<p>Very large-scale multi-agent simulation with llm-powered agents. Xuchen Pan, Dawei Gao, Yuexiang Xie, Yushuo Chen, Zhewei Wei, Yaliang Li, Bolin Ding, Ji-Rong Wen, Jingren Zhou, </p>
<p>Unveiling the secret recipe: A guide for supervised fine-tuning small llms. Aldo Pareja, Nikhil Shivakumar Nayak, Hao Wang, Krishnateja Killamsetty, Shivchander Sudalairaj, Wenlong Zhao, Seungwook Han, Abhishek Bhandwaldar, Guangxuan Xu, Kai Xu, Ligong Han, Luke Inglis, Akash Srivastava, arXiv2024</p>
<p>Plangen: A multi-agent framework for generating planning and reasoning trajectories for complex problem solving. Mihir Parmar, Xin Liu, Palash Goyal, Yanfei Chen, Long Le, Swaroop Mishra, Hossein Mobahi, Jindong Gu, Zifeng Wang, Hootan Nakhost, Chitta Baral, Chen-Yu Lee, Tomas Pfister, Hamid Palangi, arXiv2025</p>
<p>Check your facts and try again: Improving large language models with external knowledge and automated feedback. Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, Jianfeng Gao, 2023In arXiv</p>
<p>Let's think dot by dot: Hidden computation in transformer language models. Jacob Pfau, William Merrill, Samuel R Bowman, Conference on Language Modeling. 2024</p>
<p>Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, arXiv:2501.14249Humanity's last exam. 2025arXiv preprint</p>
<p>Agentsociety: Large-scale simulation of llm-driven generative agents advances understanding of human behaviors and society. Jinghua Piao, Yuwei Yan, Jun Zhang, Nian Li, Junbo Yan, Xiaochong Lan, Zhihong Lu, Zhiheng Zheng, Jing , Yi Wang, Di Zhou, arXiv:2502.086912025arXiv preprint</p>
<p>Adapt: As-needed decomposition and planning with language models. Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, Tushar Khot, Findings of the Association for Computational Linguistics: NAACL 2024. 2024</p>
<p>A probabilistic inference approach to inference-time scaling of llms using particle-based monte carlo methods. Isha Puri, Shivchander Sudalairaj, Guangxuan Xu, Kai Xu, Akash Srivastava, arXiv2025</p>
<p>Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, Maosong Sun, arXiv:2406.07155Scaling large-language-model-based multi-agent collaboration. 2024arXiv preprint</p>
<p>Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, Pengfei Liu, arXivO1 replication journey: A strategic progress report -part 1. 2024</p>
<p>Jiahao Qiu, Yifu Lu, Yifan Zeng, Jiacheng Guo, Jiayi Geng, Huazheng Wang, Kaixuan Huang, Yue Wu, Mengdi Wang, arXiv:2410.16033Treebon: Enhancing inference-time alignment with speculative tree-search and best-of-n sampling. 2024arXiv preprint</p>
<p>Phybench: Holistic evaluation of physical perception and reasoning in large language models. Shaoyang Shi Qiu, Zhuo-Yang Guo, Yunbo Song, Zeyu Sun, Jiashen Cai, Tianyu Wei, Yixuan Luo, Haoxu Yin, Yi Zhang, Hu, arXiv:2504.160742025arXiv preprint</p>
<p>Xiaoye Qu, Yafu Li, Zhaochen Su, Weigao Sun, Jianhao Yan, Dongrui Liu, Ganqu Cui, Daizong Liu, Shuxian Liang, Junxian He, arXiv:2503.21614A survey of efficient reasoning for large reasoning models: Language, multimodality, and beyond. 2025arXiv preprint</p>
<p>Qwq: Reflect deeply on the boundaries of the unknown. Qwen, 2024</p>
<p>Improving chain-of-thought reasoning via quasi-symbolic abstractions. Leonardo Ranaldi, Marco Valentino, Alexander Polonsky, Andrè Freitas, arXiv2025</p>
<p>GPQA: A graduate-level google-proof q&amp;a benchmark. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R Bowman, First Conference on Language Modeling. 2024</p>
<p>The effect of sampling temperature on problem solving in large language models. Matthew Renze, Findings of the Association for Computational Linguistics: EMNLP 2024. 2024</p>
<p>Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, Kate Saenko, arXiv:1809.02156Object hallucination in image captioning. 2018arXiv preprint</p>
<p>Archon: An architecture search framework for inference-time techniques. Jon Saad-Falcon, Gamarra Lafuente, Shlok Natarajan, Nahum Maru, Hristo Todorov, Etash Guha, E Kelly Buchanan, Mayee Chen, Neel Guha, Christopher Ré, Azalia Mirhoseini, arXiv2024</p>
<p>Learning to plan &amp; reason for evaluation with thinking-llm-as-a-judge. Swarnadeep Saha, Xian Li, Marjan Ghazvininejad, Jason Weston, Tianlu Wang, arXiv2025</p>
<p>Towards a search engine for machines: Unified ranking for multiple retrieval-augmented large language models. Alireza Salemi, Hamed Zamani, arXiv2024</p>
<p>Reasoning with latent thoughts: On the power of looped transformers. Nikunj Saunshi, Nishanth Dikkala, Zhiyuan Li, Sanjiv Kumar, Sashank J Reddi, arXiv2025</p>
<p>Boundless socratic learning with language games. Tom Schaul, Language Gamification-NeurIPS 2024 Workshop. 2024</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, Proximal policy optimization algorithms. 2017</p>
<p>Algorithm of thoughts: Enhancing exploration of ideas in large language models. Bilgehan Sel, Ahmad Tawaha, Vanshaj Khattar, Ruoxi Jia, Ming Jin, International Conference on Machine Learning. PMLR2024</p>
<p>Bond: Aligning llms with best-of-n distillation. Giuseppe Pier, Robert Sessa, Léonard Dadashi, Johan Hussenot, Nino Ferret, Alexandre Vieillard, Bobak Ramé, Sarah Shariari, Abe Perrin, Geoffrey Friesen, Sertan Cideron, Piotr Girgin, Andrea Stanczyk, Danila Michi, Sabela Sinopalnikov, Amélie Ramos, Aliaksei Héliou, Matt Severyn, Nikola Hoffman, Olivier Momchev, Bachem, arXiv2024</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Li, Wu, arXiv:2402.03300Deepseekmath: Pushing the limits of mathematical reasoning in open language models. 2024arXiv preprint</p>
<p>Exploring data scaling trends and effects in reinforcement learning from human feedback. Wei Shen, Guanlin Liu, Zheng Wu, Ruofei Zhu, Qingping Yang, Chao Xin, Yu Yue, Lin Yan, arXiv2025a</p>
<p>Efficient reasoning with hidden thinking. Xuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu Zhao, Jiuxiang Gu, arXiv2025b</p>
<p>Codi: Compressing chain-of-thought into continuous space via self-distillation. Zhenyi Shen, Hanqi Yan, Linhai Zhang, Zhanghao Hu, Yali Du, Yulan He, arXiv2025c</p>
<p>Can language models solve olympiad programming?. Ben Shi, Michael Tang, Shunyu Karthik R Narasimhan, Yao, Conference on Language Modeling. 2024</p>
<p>Beyond human data: Scaling self-training for problem-solving with language models. Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron T Parisi, Abhishek Kumar, Alexander A Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Gamaleldin Fathy Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura A Culp, Lechao Xiao, Maxwell Bileschi, Transactions on Machine Learning Research. Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yamini Bansal, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, and Noah Fiedel2024</p>
<p>Self-evolved preference optimization for enhancing mathematical reasoning in small language models. Joykirat Singh, Tanmoy Chakraborty, Akshay Nambi, arXiv2025</p>
<p>Defining and characterizing reward hacking. Joar Skalse, H R Nikolaus, Dmitrii Howe, David Krasheninnikov, Krueger, arXiv2025</p>
<p>Scaling llm test-time compute optimally can be more effective than scaling model parameters. Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar, arXiv:2408.033142024arXiv preprint</p>
<p>Prmbench: A fine-grained and challenging benchmark for process-level reward models. Mingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou, Yu Cheng, arXiv:2501.031242025arXiv preprint</p>
<p>The good, the bad, and the greedy: Evaluation of llms should not ignore non-determinism. Yifan Song, Guoyin Wang, Sujian Li, Bill Yuchen, Lin , arXiv2024</p>
<p>Advancing the rationality debate. Keith E Stanovich, Richard F West, Behavioral and Brain Sciences. 2000</p>
<p>Dijia Su, Hanlin Zhu, Yingchen Xu, Jiantao Jiao, Yuandong Tian, Qinqing Zheng, arXiv:2502.03275Token assorted: Mixing latent and text tokens for improved language model reasoning. 2025arXiv preprint</p>
<p>Yuan Sui, Yufei He, Tri Cao, Simeng Han, Bryan Hooi, Meta-reasoner: Dynamic guidance for optimized inference-time reasoning in large language models. 2025</p>
<p>Scieval: a multi-level large language model evaluation benchmark for scientific research. Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, Kai Yu, AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence. 2024</p>
<p>Uncertainty and influence aware reward model refinement for reinforcement learning from human feedback. Zexu Sun, Yiju Guo, Yankai Lin, Xu Chen, Qi Qi, Xing Tang, Ji-Rong Wen, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Principle-driven self-alignment of language models from scratch with minimal human supervision. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, Chuang Gan, Advances in Neural Information Processing Systems. 2023</p>
<p>Policy gradient methods for reinforcement learning with function approximation. David Richard S Sutton, Satinder Mcallester, Yishay Singh, Mansour, Advances in neural information processing systems. 199912</p>
<p>Judgebench: A benchmark for evaluating llm-based judges. Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William Y Tang, Alejandro Cuadron, Chenguang Wang, arXivRaluca Ada Popa, and Ion Stoica. 2025</p>
<p>Confidence improves self-consistency in llms. Amir Taubenfeld, Tom Sheffer, Eran Ofek, Amir Feder, Ariel Goldstein, Zorik Gekhman, Gal Yona, arXiv2025</p>
<p>Fengwei Teng, Zhaoyang Yu, Quan Shi, Jiayi Zhang, Chenglin Wu, Yuyu Luo, arXiv:2502.12018Atom of thoughts for markov llm test-time scaling. 2025arXiv preprint</p>
<p>Toward self-improvement of LLMs via imagination, searching, and criticizing. Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Lei Han, Haitao Mi, Dong Yu, Conference on Neural Information Processing Systems. 2024</p>
<p>Codehalu: Investigating code hallucinations in llms via execution-based verification. Yuchen Tian, Weixiang Yan, Qian Yang, Xuandong Zhao, Qian Chen, Wen Wang, Ziyang Luo, Lei Ma, Dawn Song, arXiv2025</p>
<p>Cambrian-1: A fully open, visioncentric exploration of multimodal llms. Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri, Iyer , Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, Advances in Neural Information Processing Systems. 202437</p>
<p>. George Tsoukalas, Jasper Lee, John Jennings, Jimmy Xin, Michelle Ding, Michael Jennings, Amitayush Thakur, Swarat Chaudhuri, 2024Putnambench: Evaluating neural theorem-provers on the putnam mathematical competition</p>
<p>Solving math word problems with process-and outcome-based feedback. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, Irina Higgins, arXiv2022</p>
<p>Improving health question answering with reliable and time-aware evidence retrieval. Juraj Vladika, Florian Matthes, Findings of the Association for Computational Linguistics: NAACL 2024. 2024</p>
<p>Mamm-refine: A recipe for improving faithfulness in generation with multi-agent collaboration. David Wan, Justin Chih-Yao Chen, Elias Stengel-Eskin, Mohit Bansal, arXiv2025</p>
<p>Alphazero-like tree-search can guide large language model decoding and training. Ziyu Wan, Xidong Feng, Muning Wen, Stephen Marcus Mcaleer, Ying Wen, Weinan Zhang, Jun Wang, Forty-first International Conference on Machine Learning. 2024</p>
<p>Planning in natural language improves llm search for code generation. Evan Wang, Federico Cassano, Catherine Wu, Yunfeng Bai, Will Song, Vaskar Nath, Ziwen Han, Sean Hendryx, Summer Yue, Hugh Zhang, arXiv2024a</p>
<p>Haiming Wang, Huajian Xin, Chuanyang Zheng, Lin Li, Zhengying Liu, Qingxing Cao, Yinya Huang, Jing Xiong, Han Shi, Enze Xie, arXiv:2310.00656Lego-prover: Neural theorem proving with growing libraries. 2023aarXiv preprint</p>
<p>Huaijie Wang, Shibo Hao, Hanze Dong, Shenao Zhang, Yilin Bao, Ziran Yang, Yi Wu, arXiv:2412.16145Offline reinforcement learning for llm multi-step reasoning. 2024barXiv preprint</p>
<p>Openr: An open source framework for advanced reasoning with large language models. Jun Wang, Meng Fang, Ziyu Wan, Muning Wen, Jiachen Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei Chen, Lionel M Ni, arXiv:2410.096712024carXiv preprint</p>
<p>Mixture-of-agents enhances large language model capabilities. Junlin Wang, Wang Jue, Ben Athiwaratkun, Ce Zhang, James Zou, International Conference on Learning Representations. 2025a</p>
<p>Measuring multimodal mathematical reasoning with MATH-vision dataset. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, Hongsheng Li, Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024d</p>
<p>Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, Zhifang Sui, Annual Meeting of the Association for Computational Linguistics. 2024e</p>
<p>Ma-lot: Multi-agent lean-based long chain-of-thought reasoning enhances formal theorem proving. Ruida Wang, Rui Pan, Yuxin Li, Jipeng Zhang, Yizhen Jia, Shizhe Diao, Renjie Pi, Junjie Hu, Tong Zhang, arXiv2025b</p>
<p>Scienceworld: Is your agent smarter than a 5th grader?. Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Cpl: Critical plan step learning boosts llm generalization in reasoning tasks. Tianlong Wang, Junzhe Chen, Xueting Han, Jing Bai, arXiv:2409.086422024farXiv preprint</p>
<p>Make every penny count: Difficulty-adaptive self-consistency for cost-efficient reasoning. Xinglin Wang, Shaoxiong Feng, Yiwei Li, Peiwen Yuan, Yueqi Zhang, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li, arXiv2025c</p>
<p>Guiding language model reasoning with planning tokens. Xinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, William Yang, Wang , Alessandro Sordoni, Conference on Language Modeling. 2024g</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, International Conference on Learning Representations. 2023b</p>
<p>MMLU-pro: A more robust and challenging multi-task language understanding benchmark. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, Wenhu Chen, Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024h</p>
<p>Critique fine-tuning: Learning to critique is more effective than learning to imitate. Yubo Wang, Xiang Yue, Wenhu Chen, arXiv2025d</p>
<p>Thoughts are all over the place: On the underthinking of o1-like llms. Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu, arXiv2025e</p>
<p>Talk structurally, act hierarchically: A collaborative framework for llm multi-agent systems. Zhao Wang, Sota Moriyama, Wei-Yao Wang, Briti Gangopadhyay, Shingo Takamatsu, arXiv2025f</p>
<p>Maurice Weber, Daniel Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Chalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher Ré, arXivIrina Rish, and Ce Zhang. 2024. Redpajama: an open dataset for training large language models. </p>
<p>Measuring short-form factuality in large language models. Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, William Fedus, 2024a</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Jie Huang, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, arXivCosmo Du, and Quoc V. Le. 2024b. Long-form factuality in large language models. </p>
<p>Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, Zaid Harchaoui, arXiv:2406.16838From decoding to meta-generation: Inference-time algorithms for large language models. 2024arXiv preprint</p>
<p>Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, arXiv:2503.10460Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond. 2025arXiv preprint</p>
<p>Qurating: Selecting high-quality data for training language models. Alexander Wettig, Aatmik Gupta, Saumya Malik, Danqi Chen, arXiv2024</p>
<p>Siwei Wu, Zhongyuan Peng, Xinrun Du, Tuney Zheng, Minghao Liu, Jialong Wu, Jiachen Ma, Yizhi Li, Jian Yang, Wangchunshu Zhou, Qunshu Lin, Junbo Zhao, Zhaoxiang Zhang, Wenhao Huang, Ge Zhang, Chenghua Lin, J H Liu, arXivA comparative study on reasoning patterns of openai's o1 model. 2024a</p>
<p>Thinking llms: General instruction following with thought generation. Tianhao Wu, Janice Lan, Weizhe Yuan, Jiantao Jiao, Jason Weston, Sainbayar Sukhbaatar, arXiv2024b</p>
<p>Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, Yiming Yang, arXiv:2408.007242024carXiv preprint</p>
<p>Scaling inference computation: Compute-optimal inference for problem-solving with language models. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, Yiming Yang, Workshop on Mathematical Reasoning and AI at NeurIPS'24. 2024d</p>
<p>Inference scaling laws: An empirical analysis of compute-optimal inference for llm problem-solving. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, Yiming Yang, The Thirteenth International Conference on Learning Representations. 2025a</p>
<p>When more is less: Understanding chain-of-thought length in llms. Yuyang Wu, Yifei Wang, Tianqi Du, Stefanie Jegelka, Yisen Wang, arXiv2025b</p>
<p>The hidden strength of disagreement: Unraveling the consensus-diversity tradeoff in adaptive multi-agent systems. Zengqing Wu, Takayuki Ito, arXiv. X-R1Team. 2025. X-r12025Grok 3 beta -the age of reasoning agents</p>
<p>Atomthink: A slow thinking framework for multimodal mathematical reasoning. Kun Xiang, Zhili Liu, Zihao Jiang, Yunshuang Nie, Runhui Huang, Haoxiang Fan, Hanhui Li, Weiran Huang, Yihan Zeng, Jianhua Han, Lanqing Hong, Hang Xu, Xiaodan Liang, arXiv2024</p>
<p>Violet Xiang, Charlie Snell, Kanishk Gandhi, Alon Albalak, Anikait Singh, Chase Blagden, Duy Phung, Rafael Rafailov, Nathan Lile, Philipp Franken, Nick Haber, and Chelsea Finn. 2025. Towards system 2 reasoning in llms: Learning how to think with meta chain-of-thought. Dakota Mahan, Louis CastricatoJan</p>
<p>Travelplanner: A benchmark for real-world planning with language agents. Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, Yu Su, International Conference on Machine Learning. PMLR2024</p>
<p>Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, Chong Luo, arXiv:2502.14768Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. 2025arXiv preprint</p>
<p>Self-evaluation guided beam search for reasoning. Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, Qizhe Xie, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Redstar: Does scaling long-cot data unlock better slow-reasoning systems. Haotian Xu, Xing Wu, Weinong Wang, Zhongzhi Li, Da Zheng, Boyuan Chen, Yi Hu, Shijia Kang, Jiaming Ji, Yingying Zhang, Zhijiang Guo, Yaodong Yang, Muhan Zhang, Debing Zhang, 2025aIn arXiv</p>
<p>Chain of draft: Thinking faster by writing less. Silei Xu, Wenhao Xie, Lingxiao Zhao, Pengcheng He, arXiv2025b</p>
<p>INSTRUCTSCORE: Towards explainable text generation evaluation with automatic feedback. Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Yang, Wang , Lei Li, Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Ugphysics: A comprehensive benchmark for undergraduate physics reasoning with large language models. Xin Xu, Qiyun Xu, Tong Xiao, Tianhao Chen, Yuchen Yan, Jiaxin Zhang, Shizhe Diao, Can Yang, Yang Wang, 10.48550/ARXIV.2502.00334CoRR, abs/2502.003342025c</p>
<p>Softcot: Soft chain-of-thought for efficient reasoning with llms. Yige Xu, Xu Guo, Zhiwei Zeng, Chunyan Miao, arXiv2025d</p>
<p>Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie, Tianjun Ji, Zhang, G Shishir, Ion Patil, Joseph E Stoica, Gonzalez, Berkeley function calling leaderboard. 2024</p>
<p>Reasonflux: Hierarchical llm reasoning via scaling thought templates. Ling Yang, Zhaochen Yu, Bin Cui, Mengdi Wang, arXiv2025a</p>
<p>Towards thinking-optimal scaling of test-time compute for llm reasoning. Wenkai Yang, Shuming Ma, Yankai Lin, Furu Wei, arXiv:2502.180802025barXiv preprint</p>
<p>Exploring and unleashing the power of large language models in automated code translation. Zhen Yang, Fang Liu, Zhongxing Yu, Jacky Wai Keung, Jia Li, Shuo Liu, Yifan Hong, Xiaoxue Ma, Zhi Jin, Ge Li, arXiv2024</p>
<p>Webshop: Towards scalable real-world web interaction with grounded language agents. Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan, arXiv2023a</p>
<ol>
<li>τ -bench: A benchmark for tool-agentuser interaction in real-world domains. Shunyu Yao, Noah Shinn, Pedram Razavi, Karthik Narasimhan, arXiv</li>
</ol>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik R Narasimhan, Conference on Neural Information Processing Systems. 2023b</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Yuan Karthik R Narasimhan, Cao, International Conference on Learning Representations. 2023c</p>
<p>LIMO: Less is more for reasoning. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, Pengfei Liu, arXiv2025</p>
<p>Evolving alignment via asymmetric self-play. Ziyu Ye, Rishabh Agarwal, Tianqi Liu, Rishabh Joshi, Sarmishta Velury, Quoc V Le, Qijun Tan, Yuan Liu, arXiv2024</p>
<p>Demystifying long chain-of-thought reasoning in llms. Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, Xiang Yue, arXiv2025</p>
<p>Sppd: Self-training with process preference learning using dynamic value margin. Hao Yi, Qingyang Li, Yulan Hu, Fuzheng Zhang, Di Zhang, Yong Liu, arXiv:2502.135162025arXiv preprint</p>
<p>Siam: Self-improving code-assisted mathematical reasoning of large language models. Dian Yu, Baolin Peng, Ye Tian, Linfeng Song, Haitao Mi, Dong Yu, arXiv2024a</p>
<p>Ovm, outcome-supervised value models for planning in mathematical reasoning. Fei Yu, Anningzhe Gao, Benyou Wang, Findings of the Association for Computational Linguistics: NAACL 2024. 2024b</p>
<p>Distilling system 2 into system 1. Ping Yu, Jing Xu, Jason E Weston, Ilia Kulikov, The First Workshop on System-2 Reasoning at Scale, NeurIPS'24. 2024c</p>
<p>Dapo: An open-source llm reinforcement learning system at scale. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, arXiv:2503.144762025arXiv preprint</p>
<p>Mm-vet: Evaluating large multimodal models for integrated capabilities. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, Lijuan Wang, International Conference on Machine Learning. PMLR2024d</p>
<p>Exact: Teaching ai agents to explore with reflective-mcts and exploratory learning. Xiao Yu, Baolin Peng, Vineeth Vajipey, Hao Cheng, Michel Galley, Jianfeng Gao, Zhou Yu, arXiv:2410.020522024earXiv preprint</p>
<p>What's behind ppo's collapse in long-cot? value optimization holds the secret. Yufeng Yuan, Yu Yue, Ruofei Zhu, Tiantian Fan, Lin Yan, arXiv:2503.014912025arXiv preprint</p>
<p>Scaling relationship on learning mathematical reasoning with large language models. Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, Jingren , 2023</p>
<p>Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, arXiv2024</p>
<p>Star: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah Goodman, Advances in Neural Information Processing Systems. 202235</p>
<p>Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, Junxian He, arXiv:2503.18892Simplerlzoo: Investigating and taming zero reinforcement learning for open base models in the wild. 2025aarXiv preprint</p>
<p>Keqing He, Qian Liu, Zejun Ma, and Junxian He. 2025b. 7b model and 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient. Weihao Zeng, Yuzhen Huang, Wei Liu, </p>
<p>itool: Boosting tool use of large language models via iterative reinforced fine-tuning. Yirong Zeng, Xiao Ding, Yuxian Wang, Weiwen Liu, Wu Ning, Yutai Hou, Xu Huang, Bing Qin, Ting Liu, 2025cIn arXiv</p>
<p>Revisiting the test-time scaling of o1-like models: Do they truly possess test-time scaling capabilities. Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Yunhua Zhou, Xipeng Qiu, arXiv:2502.122152025darXiv preprint</p>
<p>MR-ben: A meta-reasoning benchmark for evaluating system-2 thinking in LLMs. Zhongshen Zeng, Yinhong Liu, Yingjia Wan, Jingyao Li, Pengguang Chen, Jianbo Dai, Yuxuan Yao, Rongwu Xu, Zehan Qi, Wanru Zhao, Linling Shen, Jianqiao Lu, Haochen Tan, Yukang Chen, Hao Zhang, Zhan Shi, Bailin Wang, Zhijiang Guo, Jiaya Jia, Conference on Neural Information Processing Systems. 2024</p>
<p>Fine-tuning large vision-language models as decision-making agents via reinforcement learning. Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann Lecun, Yi Ma, Sergey Levine, arXiv2024</p>
<p>ReST-MCTS*: LLM selftraining via process reward guided tree search. Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, Jie Tang, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024a</p>
<p>Lightthinker: Thinking step-by-step compression. Jintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo, Shuofei Qiao, Lun Du, Da Zheng, Huajun Chen, Ningyu Zhang, arXiv2025a</p>
<p>Focused-dpo: Enhancing code generation through focused preference optimization on error-prone points. Kechi Zhang, Ge Li, Jia Li, Yihong Dong, Zhi Jin, arXiv:2502.114752025barXiv preprint</p>
<p>Kongcheng Zhang, Qi Yao, Baisheng Lai, Jiaxing Huang, Wenkai Fang, Dacheng Tao, Mingli Song, Shunyu Liu, arXiv:2502.13389Reasoning with reinforced functional token tuning. 2025carXiv preprint</p>
<p>Generative verifiers: Reward modeling as next-token prediction. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, Rishabh Agarwal, arXiv2025d</p>
<p>A multi-modal neural geometric solver with textual clauses parsed from diagram. Ming-Liang Zhang, Fei Yin, Cheng-Lin Liu, Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence. the Thirty-Second International Joint Conference on Artificial Intelligence2023a</p>
<p>Crowd comparative reasoning: Unlocking comprehensive evaluations for llm-as-a-judge. Qiyuan Zhang, Yufei Wang, Yuxin Jiang, Liangyou Li, Chuhan Wu, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, Fuyuan Lyu, Chen Ma, 2025eIn arXiv</p>
<p>Reviseval: Improving LLM-as-a-judge via response-adapted references. Qiyuan Zhang, Yufei Wang, Y U Tiezheng, Yuxin Jiang, Chuhan Wu, Liangyou Li, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, Fuyuan Lyu, Chen Ma, International Conference on Learning Representations. 2025f</p>
<p>Planning with large language models for code generation. Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B Tenenbaum, Chuang Gan, International Conference on Learning Representations. 2023b</p>
<p>Xinyu Zhang, Yuxuan Dong, Yanrui Wu, Jiaxing Huang, Chengyou Jia, Basura Fernando, Mike Zheng Shou, Lingling Zhang, 10.48550/ARXIV.2502.12054CoRR, abs/2502.12054Physreason: A comprehensive benchmark towards physics-based reasoning. Jun Liu. 2025g</p>
<p>Chain of preference optimization: Improving chain-of-thought reasoning in LLMs. Xuan Zhang, Chao Du, Tianyu Pang, Qian Liu, Wei Gao, Min Lin, Conference on Neural Information Processing Systems. 2024b</p>
<p>Understanding dynamic diffusion process of llm-based agents under information asymmetry. Yiwen Zhang, Yifu Wu, Wenyue Hua, Xiang Lu, Xuming Hu, arXiv:2502.131602025harXiv preprint</p>
<p>Wrong-of-thought: An integrated reasoning framework with multi-perspective verification and wrong information. Yongheng Zhang, Qiguang Chen, Jingxuan Zhou, Peng Wang, Jiasheng Si, Jin Wang, Wenpeng Lu, Libo Qin, Findings of the Association for Computational Linguistics: EMNLP 2024. 2024c</p>
<p>Small language models need strong verifiers to self-correct reasoning. Yunxiang Zhang, Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Moontae Lee, Honglak Lee, Lu Wang, ACL (Findings). 2024d</p>
<p>Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, Jitao Sang, arXivo1-coder: an o1 replication for coding. 2024e</p>
<p>Marco-o1: Towards open reasoning models for open-ended solutions. Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, Kaifu Zhang, arXiv2024</p>
<p>Kunhao Zheng, Jesse Michael Han, Stanislas Polu, arXiv:2109.00110Minif2f: a cross-system benchmark for formal olympiad-level mathematics. 2021arXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, Advances in Neural Information Processing Systems. 2023a36</p>
<p>Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, arXiv:2307.04964Secrets of rlhf in large language models part i: Ppo. 2023barXiv preprint</p>
<p>AGIEval: A human-centric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, Findings of North American Chapter. the Association for Computational Linguistics2024</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, The Eleventh International Conference on Learning Representations. 2023a</p>
<p>Enyu Zhou, Guodong Zheng, Binghai Wang, Zhiheng Xi, Shihan Dou, Rong Bao, Wei Shen, Limao Xiong, Jessica Fan, Yurong Mou, Rui Zheng, Tao Gui, arXivQi Zhang, and Xuanjing Huang. 2025. Rmb: Comprehensively benchmarking reward models in llm alignment. </p>
<p>Instruction-following evaluation for large language models. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, Le Hou, arXiv2023b</p>
<p>Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, arXiv:2307.13854Webarena: A realistic web environment for building autonomous agents. 2023carXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
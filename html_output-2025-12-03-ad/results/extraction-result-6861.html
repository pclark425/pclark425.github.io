<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6861 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6861</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6861</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-267499800</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.04119v1.pdf" target="_blank">A quantitative analysis of knowledge-learning preferences in large language models in molecular science</a></p>
                <p><strong>Paper Abstract:</strong> Deep learning has significantly advanced molecular modelling and design, enabling an efficient understanding and discovery of novel molecules. In particular, large language models introduce a fresh research paradigm to tackle scientific problems from a natural language processing perspective. Large language models significantly enhance our understanding and generation of molecules, often surpassing existing methods with their capabilities to decode and synthesize complex molecular patterns. However, two key issues remain: how to quantify the match between model and data modalities and how to identify the knowledge-learning preferences of models. To address these challenges, we propose a multimodal benchmark, named ChEBI-20-MM, and perform 1,263 experiments to assess the modelâ€™s compatibility with data modalities and knowledge acquisition. Through the modal transition probability matrix, we provide insights into the most suitable modalities for tasks. Furthermore, we introduce a statistically interpretable approach to discover context-specific knowledge mapping by localized feature filtering. Our analysis offers an exploration of the learning mechanism and paves the way for advancing large language models in molecular science. Large language models promise substantial advances in molecular modelling and design. A multimodal benchmark is proposed to analyse performance, and 1,263 experiments are conducted to examine the compatibility of a large language model with data modalities and knowledge acquisition.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6861.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6861.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolT5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolT5 (T5-based molecular model / used as decoder in composite models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A T5-series encoder-decoder model variant used in this study as a decoder component for text-based molecular generation and captioning tasks; reported to outperform many other architectures on text-to-text molecular tasks in the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolT5</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>encoder-decoder (used as decoder in composite models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Fine-tuned in the paper's experiments on the ChEBI-20-MM benchmark (32,998 molecules, text forms including SMILES/SELFIES/IUPAC/captions); pretraining corpora not fully specified in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Fine-tuning of a T5-style decoder for direct text generation (de novo SMILES/SELFIES/IUPAC and caption generation); used within encoder-decoder and composite setups (MolT5 as decoder paired with various encoders).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES and SELFIES primarily for generation; also supports IUPAC and textual captions.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>General de novo molecule generation and molecule captioning (text-based molecule generation / molecular optimization tasks relevant to drug-discovery-style workflows).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Dataset filtered to remove molecules that cannot be converted into SELFIES to ensure validity; generation validity assessed post-hoc (validity metric used).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Cheminformatics tools (RDKit) used for canonicalization/conversion and validity checks; graph encoders (e.g., GIN) used in composite experiments where MolT5 served as decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ChEBI-20-MM (primary fine-tuning/evaluation), also compared across MoleculeNet for property tasks and ChEBI-20 / PubChem324k for captioning datasets in evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU / ROUGE / METEOR for text similarity (generation and captioning); validity %, Levenshtein distance, fingerprint-based similarity (e.g., Morgan/RDKit keys) for molecule-generation comparisons; no bespoke property-optimization RL reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Qualitative: T5-based models (including MolT5 used as decoder) show superior performance in text-to-text molecular tasks and dominate top-5 rankings across multiple tasks; paper reports 1,263 experiments (291 generative/retrieval) but does not provide single-number performance summaries for MolT5 in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>No wet-lab validation reported; limitations include modality conversion information loss (IUPAC/captions), potential hallucinated or invalid strings mitigated by SELFIES filter, need for specialized evaluation metrics beyond text similarity, and high compute/data requirements for large T5 models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A quantitative analysis of knowledge-learning preferences in large language models in molecular science', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6861.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6861.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioT5 / BioT5+</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioT5 (and BioT5+) (T5-based models enriched for biological/chemical text and IUPAC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>T5-derived models tailored for biological/chemical cross-modal tasks; BioT5 is used in multimodal fusion and embedding experiments and BioT5+ (IUPAC-integrated, multi-task tuned variant) is mentioned as achieving strong captioning/cross-modal performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BioT5 (and BioT5+)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>encoder-decoder / T5-family fine-tuned models</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Fine-tuned or evaluated on chemical textual representations (SMILES, SELFIES, IUPAC, captions) from ChEBI-20-MM and other captioning corpora; specific pretraining corpora not detailed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Fine-tuning for text-to-text tasks (caption generation and cross-modal generation); used with SELFIES and IUPAC as inputs for captioning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SELFIES and IUPAC names for captioning; textual SMILES for some experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Molecule captioning (generation of human-readable captions from molecular representations) and cross-modal embedding/fusion for retrieval and property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>SELFIES convertibility ensured in dataset; evaluation constrained to textual similarity metrics for captioning.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit for conversions; used alongside graph encoders in multimodal fusion experiments; contrastive learning and weighted additive fusion strategies applied at embedding layer.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ChEBI-20-MM for captioning experiments; PubChem324k and ChEBI-20 cited as captioning datasets for evaluation comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU, ROUGE family, METEOR and other textual similarity metrics for captioning; multimodal fusion evaluated via ROC_AUC for property tasks when applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Qualitative: BioT5+ highlighted as achieving state-of-the-art results for T5-based caption generation when using IUPAC/SELFIES integration; exact numeric scores not shown in main text (referenced in Supplementary Tables).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Performance sensitive to pretraining/fine-tuning datasets; current evaluation largely based on textual similarity which may not reflect downstream chemical utility; cross-attention fusion can cause cross-modal interference in single-modality settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A quantitative analysis of knowledge-learning preferences in large language models in molecular science', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6861.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6861.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioGPT (generative pretrained transformer for biomedical text)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-family biomedical language model mentioned and used for IUPAC name recognition tasks in the benchmark, where it shows strong performance for mapping captions/SELFIES to IUPAC names.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BioGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only GPT-family LLM (fine-tuned / evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on biomedical corpora (original BioGPT work); in this paper evaluated/fine-tuned on ChEBI-20-MM caption/IUPAC/SELFIES data for name-recognition tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Fine-tuning / decoding for IUPAC name recognition (text generation mapped from SMILES/SELFIES/captions).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SELFIES, SMILES, textual captions and IUPAC strings.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>IUPAC name recognition and mapping from textual or string molecular representations (useful for standardized chemical identification).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Standard dataset preprocessing; no special generation constraints reported beyond training/validation splits and SELFIES convertibility.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Cheminformatics conversions via RDKit for preparing inputs/targets and for canonicalization checks.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ChEBI-20-MM (captioning and IUPAC recognition tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU / ROUGE / METEOR family for text similarity; exact-match also mentioned for image recognition/generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Qualitative: BioGPT cited as excelling in IUPAC recognition tasks using captions and SELFIES (hence appearing prominently among top results), no single numeric value provided in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Converting internal representations to external (IUPAC) can cause information loss; IUPAC nomenclature complexity presents evaluation challenges; reliance on textual similarity metrics may not capture full correctness of chemical identity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A quantitative analysis of knowledge-learning preferences in large language models in molecular science', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6861.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6861.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolXPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolXPT (wrapping molecules with text for generative pre-training)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative pretraining approach that augments molecular sequences with textual contexts to improve generation; cited as a related model demonstrating strong sequence generation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MolXPT: wrapping molecules with text for generative pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolXPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only / generative pretraining (GPT-2 based) as described in citation</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Described elsewhere (citation) as pretraining on molecular sequences paired with text; specifics not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Pretrained generative model for molecular sequence generation (SMILES), referenced as an example of text-wrapped molecular pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES and textual wrappers/contexts (per citation)</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Text-based molecule generation and description; general molecule sequence generation</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Mentioned as showing strong performance in generating molecular sequences and descriptions in related work; no numbers provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Not discussed in this paper beyond being a cited related method.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A quantitative analysis of knowledge-learning preferences in large language models in molecular science', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6861.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6861.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolReGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolReGPT (ChatGPT-based molecular generator, cited)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A ChatGPT-based approach reported in related work to generate molecular sequences and descriptions; cited as an example of GPT-style models applied to molecular generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolReGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>ChatGPT-derived / decoder-only LLM (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper (referenced as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompting / finetuning ChatGPT-like models for SMILES/text generation (as described in citation).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES and textual descriptions (per citation)</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Textual molecule generation and description</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Mentioned in related-work context as demonstrating strong performance; no experimental numbers provided in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A quantitative analysis of knowledge-learning preferences in large language models in molecular science', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6861.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6861.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text-based SMILES/SELFIES generation (benchmarked approach)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-based de novo molecule generation using LLMs with SMILES/SELFIES representations (benchmarked methodology)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's primary generation paradigm: use LLMs (especially T5-family) fine-tuned to output one-dimensional molecular strings (SMILES/SELFIES/IUPAC) for de novo generation and captioning, evaluated by textual-similarity and chemical-validity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-series LLMs (MolT5 and other T5 variants) and decoder-only GPT variants evaluated for SMILES/SELFIES generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>encoder-decoder and decoder-only LLM fine-tuning / text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Fine-tuned on the ChEBI-20-MM dataset (32,998 molecules with SMILES/SELFIES/IUPAC/captions); property-prediction components used MoleculeNet for embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Fine-tuning for direct SMILES/SELFIES text generation (text-to-text translation tasks); also experimented with encoder-decoder and decoder-only architectures and composite models; no reinforcement learning or docking-in-the-loop used in the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES and SELFIES for generated molecules; IUPAC and captions for external/textual targets.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>General molecule generation, molecule captioning, and molecular optimization (text-based); intended downstream uses include drug discovery and molecular property design although no task-specific optimization (e.g., binding affinity) with experimental follow-up reported.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Dataset filtered to SELFIES-convertible molecules to guarantee syntactic validity; post-generation validity checks (validity %); evaluation using fingerprint similarity and Levenshtein distance for structural similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit used for conversions/validity/fingerprints; fingerprint metrics (Morgan/RDKit keys) used for evaluation; graph models (GIN) used for graph-text baselines and multimodal fusion.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ChEBI-20-MM (primary), ChEBI-20 and PubChem324k for captioning comparisons, MoleculeNet for property prediction baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity %, BLEU (BLEU-2/4), ROUGE (1/2/L), METEOR for text; Levenshtein distance; fingerprint-based similarity metrics (e.g., Morgan/RDKit) for chemical similarity; retrieval metrics (R@1/R@5/R@10, mean reciprocal rank) for retrieval tasks; property metrics (ROC_AUC, PR_AUC, F1, MSE/RMSE/MAE for regression).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Paper reports that T5-series models outperform BERT/GPT variants on text-to-text molecular generation tasks; graph modalities often outperform textual ones for property-prediction unless very large T5 models are used; 291 generative/retrieval tasks executed but the main text provides qualitative summaries rather than standalone numeric generation-performance tables (detailed numbers in Supplementary Tables).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Key limitations: lack of wet-lab synthesis/experimental testing, information loss converting between modalities (e.g., image->graph, internal->external), dependence on text-similarity metrics which may not reflect chemical usefulness, potential for cross-modal interference in fusion strategies, and computational/data costs for scaling LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A quantitative analysis of knowledge-learning preferences in large language models in molecular science', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6861.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6861.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChEBI-20-MM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChEBI-20-MM (multimodal benchmark derived from ChEBI-20)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal benchmark dataset created in this paper containing 32,998 molecules with multimodal representations (SMILES, InChI, SELFIES, IUPAC names, captions, images, graphs) designed to evaluate LLM compatibility with chemical modalities across generation, captioning, retrieval and property tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>dataset (used to fine-tune/evaluate models in this study)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Comprised of ChEBI-20 derived molecules; filtered to ensure molecules are convertible to SELFIES; includes textual (SMILES/SELFIES/IUPAC/captions), graph and image modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Used as target/conditioning data for LLM fine-tuning; ensures validity by filtering non-SELFIES-convertible molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES, SELFIES, InChI, IUPAC names, textual captions, 2D graphs (via RDKit), and images.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Benchmarking text-based molecule generation, captioning, retrieval and property prediction tasks for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Filtered to SELFIES-convertible molecules to ensure generation validity; splits included train/validation/test (train:validation:test not numerically specified in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit used to generate 2D graphs and convert between representations during dataset curation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>This is the dataset; used alongside MoleculeNet, ChEBI-20, and PubChem324k for different tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Used to compute textual-similarity metrics (BLEU/ROUGE/METEOR) for generation and retrieval metrics; property tasks evaluated on MoleculeNet metrics mapped into the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Dataset used in 1,263 experiments across text-to-text, image-to-text, graph-to-text, retrieval and property prediction tasks; dataset statistics (text lengths, scaffold distributions, descriptor distributions) visualized in Extended Data Fig. 1 and used to argue benchmark robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Authors note possible omission of some fingerprint modalities and that the dataset's evaluation of generation tasks is primarily similarity-based rather than chemically functional assessments; recommend future tailored evaluation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A quantitative analysis of knowledge-learning preferences in large language models in molecular science', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MolXPT: wrapping molecules with text for generative pre-training <em>(Rating: 2)</em></li>
                <li>BioT5: enriching cross-modal integration in biology with chemical knowledge and natural language associations <em>(Rating: 2)</em></li>
                <li>BioT5+: towards generalized biological understanding with IUPAC integration and multi-task tuning <em>(Rating: 2)</em></li>
                <li>MolFM: a multimodal molecular foundation model <em>(Rating: 2)</em></li>
                <li>Text2Mol: cross-modal molecule retrieval with natural language queries <em>(Rating: 1)</em></li>
                <li>Transformer-based molecular generative model for antiviral drug design <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6861",
    "paper_id": "paper-267499800",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "MolT5",
            "name_full": "MolT5 (T5-based molecular model / used as decoder in composite models)",
            "brief_description": "A T5-series encoder-decoder model variant used in this study as a decoder component for text-based molecular generation and captioning tasks; reported to outperform many other architectures on text-to-text molecular tasks in the benchmark.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MolT5",
            "model_type": "encoder-decoder (used as decoder in composite models)",
            "model_size": null,
            "training_data_description": "Fine-tuned in the paper's experiments on the ChEBI-20-MM benchmark (32,998 molecules, text forms including SMILES/SELFIES/IUPAC/captions); pretraining corpora not fully specified in paper.",
            "generation_method": "Fine-tuning of a T5-style decoder for direct text generation (de novo SMILES/SELFIES/IUPAC and caption generation); used within encoder-decoder and composite setups (MolT5 as decoder paired with various encoders).",
            "chemical_representation": "SMILES and SELFIES primarily for generation; also supports IUPAC and textual captions.",
            "target_application": "General de novo molecule generation and molecule captioning (text-based molecule generation / molecular optimization tasks relevant to drug-discovery-style workflows).",
            "constraints_used": "Dataset filtered to remove molecules that cannot be converted into SELFIES to ensure validity; generation validity assessed post-hoc (validity metric used).",
            "integration_with_external_tools": "Cheminformatics tools (RDKit) used for canonicalization/conversion and validity checks; graph encoders (e.g., GIN) used in composite experiments where MolT5 served as decoder.",
            "dataset_used": "ChEBI-20-MM (primary fine-tuning/evaluation), also compared across MoleculeNet for property tasks and ChEBI-20 / PubChem324k for captioning datasets in evaluation.",
            "evaluation_metrics": "BLEU / ROUGE / METEOR for text similarity (generation and captioning); validity %, Levenshtein distance, fingerprint-based similarity (e.g., Morgan/RDKit keys) for molecule-generation comparisons; no bespoke property-optimization RL reported.",
            "reported_results": "Qualitative: T5-based models (including MolT5 used as decoder) show superior performance in text-to-text molecular tasks and dominate top-5 rankings across multiple tasks; paper reports 1,263 experiments (291 generative/retrieval) but does not provide single-number performance summaries for MolT5 in main text.",
            "experimental_validation": false,
            "challenges_or_limitations": "No wet-lab validation reported; limitations include modality conversion information loss (IUPAC/captions), potential hallucinated or invalid strings mitigated by SELFIES filter, need for specialized evaluation metrics beyond text similarity, and high compute/data requirements for large T5 models.",
            "uuid": "e6861.0",
            "source_info": {
                "paper_title": "A quantitative analysis of knowledge-learning preferences in large language models in molecular science",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "BioT5 / BioT5+",
            "name_full": "BioT5 (and BioT5+) (T5-based models enriched for biological/chemical text and IUPAC)",
            "brief_description": "T5-derived models tailored for biological/chemical cross-modal tasks; BioT5 is used in multimodal fusion and embedding experiments and BioT5+ (IUPAC-integrated, multi-task tuned variant) is mentioned as achieving strong captioning/cross-modal performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BioT5 (and BioT5+)",
            "model_type": "encoder-decoder / T5-family fine-tuned models",
            "model_size": null,
            "training_data_description": "Fine-tuned or evaluated on chemical textual representations (SMILES, SELFIES, IUPAC, captions) from ChEBI-20-MM and other captioning corpora; specific pretraining corpora not detailed in the paper.",
            "generation_method": "Fine-tuning for text-to-text tasks (caption generation and cross-modal generation); used with SELFIES and IUPAC as inputs for captioning tasks.",
            "chemical_representation": "SELFIES and IUPAC names for captioning; textual SMILES for some experiments.",
            "target_application": "Molecule captioning (generation of human-readable captions from molecular representations) and cross-modal embedding/fusion for retrieval and property prediction.",
            "constraints_used": "SELFIES convertibility ensured in dataset; evaluation constrained to textual similarity metrics for captioning.",
            "integration_with_external_tools": "RDKit for conversions; used alongside graph encoders in multimodal fusion experiments; contrastive learning and weighted additive fusion strategies applied at embedding layer.",
            "dataset_used": "ChEBI-20-MM for captioning experiments; PubChem324k and ChEBI-20 cited as captioning datasets for evaluation comparisons.",
            "evaluation_metrics": "BLEU, ROUGE family, METEOR and other textual similarity metrics for captioning; multimodal fusion evaluated via ROC_AUC for property tasks when applicable.",
            "reported_results": "Qualitative: BioT5+ highlighted as achieving state-of-the-art results for T5-based caption generation when using IUPAC/SELFIES integration; exact numeric scores not shown in main text (referenced in Supplementary Tables).",
            "experimental_validation": false,
            "challenges_or_limitations": "Performance sensitive to pretraining/fine-tuning datasets; current evaluation largely based on textual similarity which may not reflect downstream chemical utility; cross-attention fusion can cause cross-modal interference in single-modality settings.",
            "uuid": "e6861.1",
            "source_info": {
                "paper_title": "A quantitative analysis of knowledge-learning preferences in large language models in molecular science",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "BioGPT",
            "name_full": "BioGPT (generative pretrained transformer for biomedical text)",
            "brief_description": "A GPT-family biomedical language model mentioned and used for IUPAC name recognition tasks in the benchmark, where it shows strong performance for mapping captions/SELFIES to IUPAC names.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BioGPT",
            "model_type": "decoder-only GPT-family LLM (fine-tuned / evaluated)",
            "model_size": null,
            "training_data_description": "Pretrained on biomedical corpora (original BioGPT work); in this paper evaluated/fine-tuned on ChEBI-20-MM caption/IUPAC/SELFIES data for name-recognition tasks.",
            "generation_method": "Fine-tuning / decoding for IUPAC name recognition (text generation mapped from SMILES/SELFIES/captions).",
            "chemical_representation": "SELFIES, SMILES, textual captions and IUPAC strings.",
            "target_application": "IUPAC name recognition and mapping from textual or string molecular representations (useful for standardized chemical identification).",
            "constraints_used": "Standard dataset preprocessing; no special generation constraints reported beyond training/validation splits and SELFIES convertibility.",
            "integration_with_external_tools": "Cheminformatics conversions via RDKit for preparing inputs/targets and for canonicalization checks.",
            "dataset_used": "ChEBI-20-MM (captioning and IUPAC recognition tasks).",
            "evaluation_metrics": "BLEU / ROUGE / METEOR family for text similarity; exact-match also mentioned for image recognition/generation tasks.",
            "reported_results": "Qualitative: BioGPT cited as excelling in IUPAC recognition tasks using captions and SELFIES (hence appearing prominently among top results), no single numeric value provided in main text.",
            "experimental_validation": false,
            "challenges_or_limitations": "Converting internal representations to external (IUPAC) can cause information loss; IUPAC nomenclature complexity presents evaluation challenges; reliance on textual similarity metrics may not capture full correctness of chemical identity.",
            "uuid": "e6861.2",
            "source_info": {
                "paper_title": "A quantitative analysis of knowledge-learning preferences in large language models in molecular science",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "MolXPT",
            "name_full": "MolXPT (wrapping molecules with text for generative pre-training)",
            "brief_description": "A generative pretraining approach that augments molecular sequences with textual contexts to improve generation; cited as a related model demonstrating strong sequence generation performance.",
            "citation_title": "MolXPT: wrapping molecules with text for generative pre-training",
            "mention_or_use": "mention",
            "model_name": "MolXPT",
            "model_type": "decoder-only / generative pretraining (GPT-2 based) as described in citation",
            "model_size": null,
            "training_data_description": "Described elsewhere (citation) as pretraining on molecular sequences paired with text; specifics not detailed in this paper.",
            "generation_method": "Pretrained generative model for molecular sequence generation (SMILES), referenced as an example of text-wrapped molecular pretraining.",
            "chemical_representation": "SMILES and textual wrappers/contexts (per citation)",
            "target_application": "Text-based molecule generation and description; general molecule sequence generation",
            "constraints_used": null,
            "integration_with_external_tools": null,
            "dataset_used": null,
            "evaluation_metrics": null,
            "reported_results": "Mentioned as showing strong performance in generating molecular sequences and descriptions in related work; no numbers provided in this paper.",
            "experimental_validation": null,
            "challenges_or_limitations": "Not discussed in this paper beyond being a cited related method.",
            "uuid": "e6861.3",
            "source_info": {
                "paper_title": "A quantitative analysis of knowledge-learning preferences in large language models in molecular science",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "MolReGPT",
            "name_full": "MolReGPT (ChatGPT-based molecular generator, cited)",
            "brief_description": "A ChatGPT-based approach reported in related work to generate molecular sequences and descriptions; cited as an example of GPT-style models applied to molecular generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "MolReGPT",
            "model_type": "ChatGPT-derived / decoder-only LLM (cited)",
            "model_size": null,
            "training_data_description": "Not specified in this paper (referenced as related work).",
            "generation_method": "Prompting / finetuning ChatGPT-like models for SMILES/text generation (as described in citation).",
            "chemical_representation": "SMILES and textual descriptions (per citation)",
            "target_application": "Textual molecule generation and description",
            "constraints_used": null,
            "integration_with_external_tools": null,
            "dataset_used": null,
            "evaluation_metrics": null,
            "reported_results": "Mentioned in related-work context as demonstrating strong performance; no experimental numbers provided in main text.",
            "experimental_validation": null,
            "challenges_or_limitations": "Not detailed in this paper.",
            "uuid": "e6861.4",
            "source_info": {
                "paper_title": "A quantitative analysis of knowledge-learning preferences in large language models in molecular science",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Text-based SMILES/SELFIES generation (benchmarked approach)",
            "name_full": "Text-based de novo molecule generation using LLMs with SMILES/SELFIES representations (benchmarked methodology)",
            "brief_description": "The paper's primary generation paradigm: use LLMs (especially T5-family) fine-tuned to output one-dimensional molecular strings (SMILES/SELFIES/IUPAC) for de novo generation and captioning, evaluated by textual-similarity and chemical-validity metrics.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "T5-series LLMs (MolT5 and other T5 variants) and decoder-only GPT variants evaluated for SMILES/SELFIES generation",
            "model_type": "encoder-decoder and decoder-only LLM fine-tuning / text generation",
            "model_size": null,
            "training_data_description": "Fine-tuned on the ChEBI-20-MM dataset (32,998 molecules with SMILES/SELFIES/IUPAC/captions); property-prediction components used MoleculeNet for embeddings.",
            "generation_method": "Fine-tuning for direct SMILES/SELFIES text generation (text-to-text translation tasks); also experimented with encoder-decoder and decoder-only architectures and composite models; no reinforcement learning or docking-in-the-loop used in the reported experiments.",
            "chemical_representation": "SMILES and SELFIES for generated molecules; IUPAC and captions for external/textual targets.",
            "target_application": "General molecule generation, molecule captioning, and molecular optimization (text-based); intended downstream uses include drug discovery and molecular property design although no task-specific optimization (e.g., binding affinity) with experimental follow-up reported.",
            "constraints_used": "Dataset filtered to SELFIES-convertible molecules to guarantee syntactic validity; post-generation validity checks (validity %); evaluation using fingerprint similarity and Levenshtein distance for structural similarity.",
            "integration_with_external_tools": "RDKit used for conversions/validity/fingerprints; fingerprint metrics (Morgan/RDKit keys) used for evaluation; graph models (GIN) used for graph-text baselines and multimodal fusion.",
            "dataset_used": "ChEBI-20-MM (primary), ChEBI-20 and PubChem324k for captioning comparisons, MoleculeNet for property prediction baselines.",
            "evaluation_metrics": "Validity %, BLEU (BLEU-2/4), ROUGE (1/2/L), METEOR for text; Levenshtein distance; fingerprint-based similarity metrics (e.g., Morgan/RDKit) for chemical similarity; retrieval metrics (R@1/R@5/R@10, mean reciprocal rank) for retrieval tasks; property metrics (ROC_AUC, PR_AUC, F1, MSE/RMSE/MAE for regression).",
            "reported_results": "Paper reports that T5-series models outperform BERT/GPT variants on text-to-text molecular generation tasks; graph modalities often outperform textual ones for property-prediction unless very large T5 models are used; 291 generative/retrieval tasks executed but the main text provides qualitative summaries rather than standalone numeric generation-performance tables (detailed numbers in Supplementary Tables).",
            "experimental_validation": false,
            "challenges_or_limitations": "Key limitations: lack of wet-lab synthesis/experimental testing, information loss converting between modalities (e.g., image-&gt;graph, internal-&gt;external), dependence on text-similarity metrics which may not reflect chemical usefulness, potential for cross-modal interference in fusion strategies, and computational/data costs for scaling LLMs.",
            "uuid": "e6861.5",
            "source_info": {
                "paper_title": "A quantitative analysis of knowledge-learning preferences in large language models in molecular science",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "ChEBI-20-MM",
            "name_full": "ChEBI-20-MM (multimodal benchmark derived from ChEBI-20)",
            "brief_description": "A multimodal benchmark dataset created in this paper containing 32,998 molecules with multimodal representations (SMILES, InChI, SELFIES, IUPAC names, captions, images, graphs) designed to evaluate LLM compatibility with chemical modalities across generation, captioning, retrieval and property tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "dataset (used to fine-tune/evaluate models in this study)",
            "model_type": "N/A",
            "model_size": null,
            "training_data_description": "Comprised of ChEBI-20 derived molecules; filtered to ensure molecules are convertible to SELFIES; includes textual (SMILES/SELFIES/IUPAC/captions), graph and image modalities.",
            "generation_method": "Used as target/conditioning data for LLM fine-tuning; ensures validity by filtering non-SELFIES-convertible molecules.",
            "chemical_representation": "SMILES, SELFIES, InChI, IUPAC names, textual captions, 2D graphs (via RDKit), and images.",
            "target_application": "Benchmarking text-based molecule generation, captioning, retrieval and property prediction tasks for LLMs.",
            "constraints_used": "Filtered to SELFIES-convertible molecules to ensure generation validity; splits included train/validation/test (train:validation:test not numerically specified in main text).",
            "integration_with_external_tools": "RDKit used to generate 2D graphs and convert between representations during dataset curation.",
            "dataset_used": "This is the dataset; used alongside MoleculeNet, ChEBI-20, and PubChem324k for different tasks.",
            "evaluation_metrics": "Used to compute textual-similarity metrics (BLEU/ROUGE/METEOR) for generation and retrieval metrics; property tasks evaluated on MoleculeNet metrics mapped into the benchmark.",
            "reported_results": "Dataset used in 1,263 experiments across text-to-text, image-to-text, graph-to-text, retrieval and property prediction tasks; dataset statistics (text lengths, scaffold distributions, descriptor distributions) visualized in Extended Data Fig. 1 and used to argue benchmark robustness.",
            "experimental_validation": false,
            "challenges_or_limitations": "Authors note possible omission of some fingerprint modalities and that the dataset's evaluation of generation tasks is primarily similarity-based rather than chemically functional assessments; recommend future tailored evaluation metrics.",
            "uuid": "e6861.6",
            "source_info": {
                "paper_title": "A quantitative analysis of knowledge-learning preferences in large language models in molecular science",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MolXPT: wrapping molecules with text for generative pre-training",
            "rating": 2,
            "sanitized_title": "molxpt_wrapping_molecules_with_text_for_generative_pretraining"
        },
        {
            "paper_title": "BioT5: enriching cross-modal integration in biology with chemical knowledge and natural language associations",
            "rating": 2,
            "sanitized_title": "biot5_enriching_crossmodal_integration_in_biology_with_chemical_knowledge_and_natural_language_associations"
        },
        {
            "paper_title": "BioT5+: towards generalized biological understanding with IUPAC integration and multi-task tuning",
            "rating": 2,
            "sanitized_title": "biot5_towards_generalized_biological_understanding_with_iupac_integration_and_multitask_tuning"
        },
        {
            "paper_title": "MolFM: a multimodal molecular foundation model",
            "rating": 2,
            "sanitized_title": "molfm_a_multimodal_molecular_foundation_model"
        },
        {
            "paper_title": "Text2Mol: cross-modal molecule retrieval with natural language queries",
            "rating": 1,
            "sanitized_title": "text2mol_crossmodal_molecule_retrieval_with_natural_language_queries"
        },
        {
            "paper_title": "Transformer-based molecular generative model for antiviral drug design",
            "rating": 1,
            "sanitized_title": "transformerbased_molecular_generative_model_for_antiviral_drug_design"
        }
    ],
    "cost": 0.01596475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>17 January 2025</p>
<p>School of Computer Science and Engineering
Sun Yat-sen University
GuangzhouChina</p>
<p>Peng Cheng Laboratory
ShenzhenChina
17 January 20259BD759F06BA1D1E809F5A6AAA614514C10.1038/s42256-024-00977-6Received: 6 February 2024 Accepted: 18 December 2024
Extended Data Fig. 1 | Visualization of data source suitability and chemical space diversity.The top section analyzes molecular text length distributions, tokenizer-processed lengths and representative scaffolds.The bottom section visualizes molecular property distributions and their interrelationships.</p>
<p>nature machine intelligence https://doi.org/10.1038/s42256-024-00977-6</p>
<p>Analysis</p>
<p>A quantitative analysis of knowledge-learning preferences in large language models in molecular science Pengfei Liu 1,2 , Jun Tao 1 &amp; Zhixiang Ren 2 Deep learning has significantly advanced molecular modelling and design, enabling an efficient understanding and discovery of novel molecules.In particular, large language models introduce a fresh research paradigm to tackle scientific problems from a natural language processing perspective.Large language models significantly enhance our understanding and generation of molecules, often surpassing existing methods with their capabilities to decode and synthesize complex molecular patterns.However, two key issues remain: how to quantify the match between model and data modalities and how to identify the knowledge-learning preferences of models.To address these challenges, we propose a multimodal benchmark, named ChEBI-20-MM, and perform 1,263 experiments to assess the model's compatibility with data modalities and knowledge acquisition.Through the modal transition probability matrix, we provide insights into the most suitable modalities for tasks.Furthermore, we introduce a statistically interpretable approach to discover context-specific knowledge mapping by localized feature filtering.Our analysis offers an exploration of the learning mechanism and paves the way for advancing large language models in molecular science.</p>
<p>Molecular modelling and design are pivotal in the discovery and development of new molecules, serving a wide range of applications ranging from drug discovery to materials science.These fields are fundamentally reliant on the ability to not only discover novel molecules but also to understand their structures and properties comprehensively.Such capabilities are crucial for developing targeted therapies in medicine, creating advanced materials with specific functionalities, and enhancing our fundamental understanding of chemical and biological processes.By unlocking the potential of molecular insights, we pave the way for breakthroughs that can transform technologies and improve human health.Nevertheless, traditional approaches to discover new molecules or refining existing ones are often arduous, costly and fraught with a high risk of failure.By contrast, modern computational approaches significantly improve the creation and modification of molecules.Yet, they require considerable computational resources.</p>
<p>Artificial intelligence (AI) methods have made significant strides in this domain.They offer faster computations and the ability to handle large datasets.In recent years, AI-driven approaches such as generative adversarial networks 1 and diffusion models 2 have been designed for molecule generation.These techniques simulate the creative process of designing new molecular structures, thereby expanding the potential for groundbreaking discoveries in molecular science.The task of understanding molecular structures and properties, particularly through molecular description and embedding, has been significantly advanced by machine-learning techniques 3 and graph models [4][5][6] .These approaches have enhanced our ability to accurately represent and interpret the intricate details of molecules.However, most AI models often face challenges in generalizability and flexibility, requiring extensive data for training to ensure accuracy and require specific adjustments for each molecular task.</p>
<p>Analysis</p>
<p>https://doi.org/10.1038/s42256-024-00977-6instance, although graphs can be directly transformed into images with minimal information loss, the reverse process from images to graphs tends to incur significant data loss.Similarly, converting molecular internal information into captions or IUPAC names typically causes a reduction in informational content.</p>
<p>To address these losses, models are essential for bridging these gaps.We construct a modal transition probability matrix to analyse the efficiency of modal conversion.As depicted in Fig. 1b, the classification is reflected in the evaluation framework of the six tasks (tasks 1-6) and their respective modalities.Our benchmark not only evaluates the performance variances across modalities but can also assess the multimodal synergistic performance.It includes the translation from various types of internal information to external information and generating captions based on IUPAC names, as represented by task 7, which exemplifies typical multimodal tasks.It should be noted that our benchmark encompasses seven modalities.Furthermore, to facilitate the evaluation of embedding ability, we integrate the MoleculeNet dataset 19 and classify molecular properties as external information, treating property prediction as a form of translation.</p>
<p>In summary, this study analyses the LLMs in molecular modelling and design.We categorize six common molecular tasks into three distinct objectives: description, embedding and generation (Fig. 1).Moreover, we establish a unified multimodal benchmark ChEBI-20-MM and conduct experiments to assess the compatibility of data modalities, model architectures and diverse task types, examining their impact on task performance.Additionally, our end-to-end visual method showcases the discovery of modelling insights embedded with chemical knowledge.Overall, our main contributions are as follows:</p>
<p>â€¢ This work quantitatively analyses LLMs in molecule modelling, categorizes existing models and presents a multimodal benchmark (ChEBI-20-MM) for performance evaluation, backed by 1,263 experiments.â€¢ We explore the modal transition probability matrix and identify the best match within typical data modalities and model architectures.â€¢ We propose a statistically interpretable approach to showcase knowledge acquisition through localized feature filtering.</p>
<p>Results</p>
<p>To obtain comprehensive insights into the factors of model performance, it is essential to conduct a systematic and unbiased comparison across various LLMs.We introduce ChEBI-20-MM, a multimodal benchmark developed from the ChEBI-20 dataset that integrates various data, including SMILES, InChI, IUPAC names, SELFIES, captions and images.This benchmark is designed to assess the chemical knowledge of models across tasks such as molecule generation, image and IUPAC recognition, molecular captioning, and retrieval, featuring an in-depth analysis of the compatibility between modalities and models.We explore the models' tendencies for knowledge acquisition through two typical tasks, using a localized feature-filtering method.These processes demonstrate the feasibility of cross-domain knowledge-learning analysis.Case studies further validate the utility of this analysis process.</p>
<p>Performance evaluation of tasks</p>
<p>For the analysis of modality adaptation to different tasks, we select representative metrics indicative of successful task completion, such as the metric for evaluation of translation with explicit ordering (METEOR) score for molecule captioning and the area under the receiver operating characteristic curve (ROC_AUC) for property classification tasks.We fill out the modal transition probability matrix, which provides insights into the most suitable data modalities for different task types (Fig. 2a).</p>
<p>Regarding model analysis for different tasks, Fig. 2b,c demonstrates the frequency of appearances of encoders and decoders in nine text-to-text</p>
<p>Transformers 7 offer an advantage with their robust text encoding and generation capabilities.These models can be fine-tuned with minimal task-specific adjustments, making them more versatile and efficient in molecular modelling and design.As depicted in Fig. 1a, we organize six pivotal molecular tasks into three objectives, each addressing specific scientific needs.In the description category, tasks such as captioning and International Union of Pure and Applied Chemistry (IUPAC) name recognition are pivotal for understanding complex molecular structures.Molecular captioning renders intricate chemical details more comprehensible.IUPAC name recognition is crucial for ensuring precise chemical identity across various platforms, despite the complexities of nomenclature rules and the scarcity of robust automated tools capable of handling such challenges.In the embedding category, we focus on property prediction and retrieval.These tasks are vital for extracting meaningful insights from molecular data and facilitating the prediction of molecular behaviours under various conditions.The generation category encompasses text-based molecule generation and optical recognition, which aim to create or recreate accurate molecular representations.Text-based generation enables the creation of novel molecules through predictive models, whereas optical recognition is essential for digitizing historical chemical data, ensuring that legacy research continues to inform and enhance contemporary scientific endeavours.</p>
<p>Moreover, since the advent of ChatGPT 8 and GPT-4 (ref.9), large language models (LLMs) have emerged as a groundbreaking trend, especially in molecular science.LLMs, with their advanced capabilities in processing and generating human-like text, present a novel paradigm for understanding and designing molecular structures.This paradigm is known as scientific language modelling.Their ability to assimilate and analyse vast amounts of textual data can provide unprecedented insights, overcoming some of the limitations of traditional AI methods.This new capability combines accuracy and novelty for improved outcomes, termed chemical knowledge.Its effectiveness depends on factors like input data, model architecture and training strategies.However, current assessments of this capability remain incomplete.</p>
<p>Existing related work in molecular science, like the molecule generation survey 10 , lacks comprehensive model comparisons and has a limited task scope.The knowledge-driven review 11 categorizes molecular learning but misses detailed method comparisons and dataset discussions.Furthermore, recent benchmarks, such as the one testing ChatGPT 12 , cover eight chemical tasks, each providing unique chemical insights.Mol-Instructions 13 offers a dataset for fine-tuning with various molecule and protein instructions, boosting biomolecular understanding in LLMs.Multimodal data and methodologies are crucial in the molecular domain, as they naturally integrate diverse data types, providing complementary insights that enhance the understanding of molecular behaviours.However, these papers lack multimodal content and do not sufficiently explore the models' chemical knowledge.</p>
<p>In this study, we regard different representations of molecules as distinct modalities, transcending conventional formats like one-dimensional text, two-dimensional graphs or images.We introduce ChEBI-20-MM, a comprehensive multimodal benchmark encompassing 32,998 molecules.These molecules can be characterized by one-dimensional textual chemical descriptors, including simplified molecular input line entry system (SMILES) 14 , international chemical identifier (InChI) 15 and self-referencing embedded strings (SELFIES) 16 , along with their corresponding two-dimensional graphs generated by cheminformatics tools such as RDKit 17 .These representations are classified as the molecules' internal information because they encapsulate the molecular essence.Furthermore, these modalities can be interconverted using cheminformatics tools, although the processes of their conversion are not the focal point of our investigation.Conversely, modalities such as molecular captions, IUPAC 18 names and images, which enhance human comprehension, are categorized as external information.The intrinsic connections between different modalities, such as images, two-dimensional graphs and text, are profound.For Analysis https://doi.org/10.1038/s42256-024-00977-6tasks, as well as eight encoders and two pooling mechanisms in embedding tasks.Detailed experimental results are shown in Supplementary Tables 3-9.We also incorporate typical traditional machine-learning methods, graph models and transformer-based models for the property prediction task analysis (Supplementary Tables 6 and 7).</p>
<p>Modal transition probability matrix.In our study, we construct a modal transition probability matrix based on the outcomes from various text generation tasks, such as any-to-text task, encompassing 13 distinct tasks derived from the ChEBI-20-MM dataset and experimental settings.Additionally, results for molecular property prediction tasks are obtained from MoleculeNet and mapped to the paradigms illustrated in Fig. 1b.This matrix features the inputs along the vertical axis and the outputs along the horizontal axis, with the transition probabilities within the same modality set to 1.For transitions involving molecular internal information, which can be directly converted using specialized tools, we set the transition success rate to 1. Molecular generation tasks are specifically addressed using SMILES representations, and the bilingual evaluation understudy (BLEU) score serves as the metric for conversion effectiveness.This standard is uniformly applied to all entries pertaining to internal information generation within the same row.In cases where images can be converted from graphs using tools, the transition metrics for images are aligned with those for internal information.For captioning and IUPAC naming tasks, the METEOR score is utilized as the conversion metric.However, the molecular property prediction regression tasks cannot be presented as probability.We select classification tasks and use the mean ROC_AUC as the conversion metric.Finally, tasks involving text generation based on molecular properties are not within the scope of our research and are assigned a transition probability of zero.This matrix provides a comprehensive view of modal adaptability across different task configurations, laying a foundational basis for future research.Comparing model architectures.Our research demonstrates the novel performance advantages of text-to-text transfer transformer (T5)-based LLMs in molecular science, surpassing the capabilities of BERT and GPT variants.We evaluate the performance of these models on molecular text-to-text and retrieval tasks.We select the top five results for each task, providing a comparative analysis of the corresponding encoders and decoders.As depicted in Fig. 2b, we present the frequency of various encoders and the proportions of encoder-decoder combinations utilized.T5-based models demonstrate a distinct advantage as encoders, whereas BERT models also show commendable performance.In particular, BioGPT 20 , a variant of the GPT model, excels in IUPAC name recognition tasks using captions and SELFIES, which is why it is prominently featured twice in our analysis.In terms of decoding, T5-based models not only excel as encoder-decoder models but also perform robustly when deployed as decoders.To assess the retrieval capabilities across different modalities, we consider all encoders and select the top five results for each task (Fig. 2c).We compare the embedding capabilities of different models and pooling strategies.Additionally, we demonstrate the suitability of these modalities for cross-modal retrieval, using R@1 as the comparing metric.</p>
<p>Modality and architecture insights.From the results, we present the following modality insights:</p>
<p>â€¢ IUPAC names for generation and captioning: IUPAC names are conducive to text-based molecule generation and molecule captioning.â€¢ SMILES for IUPAC recognition: SMILES representations are particularly well suited for IUPAC recognition compared with other molecular internal information.â€¢ Preferred modalities for captioning: as some captions include the IUPAC name of the molecule, IUPAC is the most appropriate modality for molecule captioning, followed by SMILES.â€¢ Choosing modalities for retrieval: for molecule retrieval tasks targeting captions, IUPAC is the optimal modality, followed by</p>
<p>Analysis</p>
<p>https://doi.org/10.1038/s42256-024-00977-6SMILES.Conversely, when the target is IUPAC names, SMILES becomes the most favourable modality.</p>
<p>â€¢ Graph versus textual modalities in property prediction: in molecular property prediction tasks, among the selected top five rank combinations from 90 groups, graph modalities appear 40 times and SMILES, 25 times, indicating a clear advantage for graph modalities and the relative superiority of SMILES over other textual representations.</p>
<p>The model performance insights are as follows:</p>
<p>â€¢ T5 series excellence: the T5 series models show excellent performance across these tasks.â€¢ Graph models versus T5 series: only the significantly larger T5 series models surpass graph neural networks, affirming the superiority of smaller-scale graph models for molecular embedding.â€¢ Efficacy of average pooling: the average pooling mechanism tends to achieve superior performance more easily.</p>
<p>Case studies on model knowledge-learning preferences</p>
<p>Our analysis aims to discern model preferences for knowledge acquisition.To enhance interpretability, we specifically focus on the mapping processes from IUPAC names to captions and from SELFIES to captions, which bridge natural language tasks and chemical cross-domain tasks.We analyse the text inference processes of LLMs and compile two token-mapping matrices.To mitigate the impact of overwhelmingly frequent mappings on our analysis, we introduce a localized feature-filtering method.This approach allows us to analyse specific high-frequency token mappings that indicate knowledge-learning preferences.By adjusting the threshold T, we ensure that the selected token mappings are statistically significant.Finally, we validate and analyse the effectiveness of our methods through molecular case studies, confirming their utility in real-world applications.</p>
<p>Token-mapping matrix.Initially, we extract typical tokens from the model's tokenizer, removing meaningless symbols and filtering the top 20 high-frequency chemical tokens to construct the mapping matrix A. Figure 3a displays the token-mapping matrices after normalization and rearrangement based on the total counts of rows and columns.The common high-frequency mappings, such as 'oxy' and 'methyl', are prevalent across various tokens, indicating their widespread presence in chemical structures.Traditional filtering methods, although capturing many of these frequent mappings, may inadvertently exclude tokens with specific contextual significance.Our localized feature-filtering method aims to refine this process by prioritizing tokens that provide greater contextual relevance, thereby enhancing the interpretation of our analysis.</p>
<p>Threshold T analysis.To identify particular high-frequency mapping pairs, we varied the threshold T to obtain different quantities of token mappings and their corresponding confidence levels.We carefully balance the number of token mappings against the confidence levels to select an optimal T, ensuring neither an overly small set of mappings nor insufficiently low confidence levels.Taking the IUPAC name to the caption process as an example (Fig. 3), at T = 3.515, the Z test attains a value of 2.758, corresponding to a confidence level of approximately 99.71%.This analysis identifies eleven specific token-mapping pairs.After excluding pairs with identical row and column names, we select seven unique pairs.Similarly, for the SELFIES to caption process, at T = 3.434, the Z test reached a value of 2.476, translating to a confidence level of approximately 99.34%.From this, we identify five unique pairs.By consolidating mappings with the same leading or trailing tokens, we categorize the mappings into the following 12 groups: Case studies of knowledge-learning preferences.Following these mapping patterns, we randomly select the corresponding molecular IUPAC names, SELFIES and their captions (Fig. 3b).The token 'min' in the IUPAC name is directly mapped to the 'amino' group.It reflects the NH2 group, which is common in biomolecules and crucial for biological functions.The token '[N]' in SELFIES denotes a nitrogen atom, key to the amino group's bonding and interactions.Similarly, the suffix 'ine' in captions often signifies nitrogen-based compounds like amines and amino acids such as methionine, highlighting the amino group's presence.In another case study, the token 'ent' often indicates pentane derivatives, such as 'pentanoic' or 'pentanoyl', signalling a five-carbon chain integral to the molecule's structure.For instance, in a dipeptide composed of l-aspartic acid and l-arginine, the presence of 'pentanoic' denotes a five-carbon fatty acid essential for the molecule's stability and functional integrity.Concurrently, the SELFIES token '[=C]' underscores the significance of carbon double bonds, which are vital for maintaining the molecular rigidity and facilitating biochemical interactions.Furthermore, the term 'acid' in the caption specifically refers to the carboxylic acid groups in both amino acids, which are indispensable for forming peptide bonds that define the dipeptide's functionality.For more case studies, please refer to Supplementary Fig. 1.</p>
<p>The model exhibits a robust understanding of molecular structure and functional groups, effectively mapping specific tokens to key chemical components such as carbon chains and double bonds.It demonstrates the model's ability to discern critical elements that define molecular stability and reactivity, essential for predicting molecular behaviour and interactions in biochemical contexts.Through a series of model knowledge-learning insights, we demonstrate that the observed mapping patterns are not merely coincidental but reflective of the model's inherent ability to process and interpret complex chemical information.</p>
<p>Discussion</p>
<p>Exploring the effects of multimodality</p>
<p>To identify how data modalities influence model performance in molecular science tasks, we develop a multimodal benchmark and carry out thousands of experiments.Furthermore, we perform experiment replication and gather experimental data from other research to analyse the effects of factors related to multimodal fusion.Figure 1 illustrates that molecular multimodal fusion tasks include molecular retrieval, molecule captioning and property prediction, with the latter two commonly used as benchmarks as they integrate information from multiple modalities to improve molecular representation learning.The molecule is a dipeptide composed of L-aspartic acid and L-arginine joined by a peptide linkage.It has a role as a metabolite.</p>
<p>[
N][=C][Branch1][C][N][N][C][C][C][C@H1][Branch1][S][N][= C][Branch1][C][O][C@@H1][Branch1][C][N][C][C][=Branch1] [C][=O][O][C][=Branch1][C][=O][O] [=C]-&gt;acid N,N-dihydroxy-L- tetrahomomethionine min-&gt;ine
The molecule is an N,N-dihydroxy-L-polyhomomethioninate that is the conjugate base of N,N-dihydroxy-Ltetrahomomethionine, obtained by deprotonation of the carboxy group; major species at pH 7.3.
[C][S][C][C][C][C][C][C][C@@H1][Branch1][=Branch1][C][=Br anch1][C][=O][O-1][N][Branch1][C][O][O] [N]-&gt;ine[#C] [C@] [=Branch2] [\C] [#Branch2] [S] [O-1] [#Branch1] [P] [=N] [/C] [Branch2] [Ring2]
[N]</p>
<p>[C@@H1]</p>
<p>[=O]</p>
<p>[C@H1]</p>
<p>[=Branch1]</p>
<p>[Ring1]</p>
<p>[=C]</p>
<p>Analysis</p>
<p>https://doi.org/10.1038/s42256-024-00977-6</p>
<p>We use well-established benchmarks like MoleculeNet 19 for property prediction, and ChEBI-20 (ref.21) and PubChem324k 22 for molecule captioning.As shown in Fig. 4, our focus is on molecular property classification and molecule captioning, using SMILES, SELFIES and graph representations processed by a graph isomorphism network 4 .For more information about the results of multimodal fusion, please refer to Supplementary Tables 10 and 11.The modal fusion techniques at the embedding layer include additive blending with weights, concatenation and concatenation followed by self-attention.By contrast, the encoding layer predominantly utilizes contrastive learning and cross-attention methods.The comparative experiments based on pooling methods consistently utilize average pooling.</p>
<p>Our findings indicate that among the four fusion techniques at the embedding layer, only weighted additive blending consistently outperforms the baseline models, regardless of the underlying model and output modality.Moreover, contrastive learning strategies also enhance the model performance and are applicable to both graph and text models.However, the use of cross-attention strategies tends to decrease performance in single-modality setups due to cross-modal interference and improve the outcomes of modal fusion.In the molecule-captioning task, both contrastive learning and cross-attention mechanisms emerge as critical factors, with the choice of pretraining datasets also influencing outcomes.Although our study predominantly focuses on prevalent data modalities, potentially overlooking some fingerprint modalities, our findings provide comparative insights and highlight areas for deeper investigation in future work.</p>
<p>Discovery of scientific insights</p>
<p>To explore the knowledge-learning preferences, we provide an interpretable analysis, revealing the foundational mechanisms in molecular scientific language modelling.This analysis confirms the models' proficiency in encoding chemical knowledge, not only from the natural language processing perspective but also from a cross-domain viewpoint.However, there remains substantial room for exploration.In Fig. 3, we select knowledge patterns with a confidence level exceeding 99%.By lowering the threshold T, although confidence may decrease, it provides an opportunity to uncover more novel insights, such as exploring unique properties of certain molecules or amino acids.As models increase in complexity and the corpus of training data grows, the capacity of these systems to delve into and illuminate scientific phenomena also expands.This enlargement not only enhances the depth and accuracy of the models' predictions but also allows for the exploration of previously unanticipated aspects of molecular behaviour.To more clearly showcase the models' capabilities in scientific understanding, future research should incorporate a broader spectrum of biochemical and physical chemistry knowledge.</p>
<p>Evaluating molecular language modelling</p>
<p>A benchmark must exhibit a balanced distribution, preventing biases towards any specific molecular representation.Analysing the distribution of text lengths and tokenizer outputs helps confirm that a language model's tokenizer captures molecular information effectively.Moreover, the diversity in scaffold distribution is crucial for determining the benchmark's generalization across chemical spaces, ensuring that the language model can learn and predict a wide array of chemical entities.Finally, the distribution and correlation of molecular descriptors reveal the complexity within the chemical space.Overall, the visual analyses (Extended Data Fig. 1) of text length, scaffold count and descriptor statistics provide insights into the dataset's characteristics, affirming the benchmark's robustness for tasks in molecular science.Although we strive to include a broad range of metrics for various tasks, our approach to molecular generation tasks primarily relies on similarity metrics compared with target texts.</p>
<p>In future research, we will focus on a sophisticated exploration of multimodal data fusion to understand how various data modalities enhance model performance.By incorporating scientific perspectives from biochemical and pharmacological fields, we expect to achieve a richer comprehension of the models' capabilities and potentially uncover more scientific insights.We also plan to develop tailored evaluation metrics for various scientific language-modelling tasks, moving beyond simple comparisons with target texts.</p>
<p>Methods</p>
<p>In this section, we present essential background information.We start by defining different molecular representation forms and vital LLMs, highlighting their significance in this field.We then provide a detailed introduction to our benchmark dataset and the setup of our evaluation framework.Finally, we elaborate on our statistically interpretable localized feature-filtering method, which facilitates an end-to-end analysis of model knowledge-learning preferences.</p>
<p>Molecular representation</p>
<p>Molecular structures can be represented in multiple ways for computational analyses, each with advantages and limitations.These include graph structures, images and one-dimensional notations.Figure 1a showcases examples of various data representation forms, whereas Fig. 1b categorizes these representations into molecular internal and external information.Additionally, it illustrates the relationships between these forms of representation and their corresponding tasks.</p>
<p>Molecular internal information.</p>
<p>SMILES offers a widely used one-dimensional notation for describing molecular structures.InChI provides a unique identifier for chemical substances, facilitating the standard encoding of molecular information.Further advancing molecular representations, SELFIES offers machine-learning-oriented approaches, addressing the limitations of SMILES.Moreover, graph structures naturally represent molecules, with atoms as nodes and bonds as edges, effectively capturing both local and global topological traits.</p>
<p>Molecular external information.</p>
<p>Images offer a visual two-dimensional or three-dimensional representation of molecules.The IUPAC names provide a standardized naming system for organic compounds.Additionally, molecular properties serve as a form of representation aiding human comprehension.Furthermore, the molecular captions offer rich contextual information, encompassing textual descriptions of functions, properties and intermolecular relationships.</p>
<p>LLMs in molecular science</p>
<p>Transformers, known for their self-attention mechanisms, enable the model to weigh the importance of the different parts of input data, which is crucial for understanding complex molecular structures of text modality.LLMs are transformer-based models scaled to a large extent.Among the notable foundation models in this domain are GPT 23 and BERT 24 .Especially in the fine-tuning stage, these models exhibit remarkable versatility and efficiency, requiring minimal task-specific adjustments for applications in molecular modelling.We present the classifications and architectures in Fig. 5.The details of the pivotal developments in transformer-based models for molecular modelling and design are shown in Supplementary Fig. 2 and Supplementary Table 2. To organize the models under a unified framework, we categorize them into three groups based on their architectures.</p>
<p>â€¢ Encoder-based models.BERT and its derivatives like RoBERTa 25 and domain-specific variants such as SciBERT 26 and Chem-BERTa 27   IUPAC name recognition.Unlike free-form textual descriptions, IUPAC names possess the advantage of uniformity and specificity, making them highly structured and predictable.Furthermore, the task of IUPAC name recognition involves not only the decoding of these structured names but also understanding the underlying molecular structure they represent.Struct2IUPAC 37 uses a transformer-based architecture, which is complemented by a specialized tokenizer adept at processing both SMILES strings and IUPAC nomenclature.Moreover, TransAntivirus 38 goes a step further by adopting the versatile T5 model architecture.</p>
<p>Molecular property prediction.</p>
<p>Molecular representation is critical in molecular modelling, providing ways to describe and encode molecular structures for computational use.Within this domain, molecular property prediction is essential, assessing attributes like solubility, toxicity and protein-binding affinity.On the basis of BERT, KV-PLM 39 uses a dual tokenizer to better recognize specific atoms and functional groups within SMILES strings.As for multimodal methods, MoMu 33 applies contrastive learning techniques to assimilate modality-specific and complementary information from text and graph data.Post-encoding, embeddings are transformed into fixed-length vectors via a pooling layer, using methods like average pooling and max pooling.Average pooling calculates the mean of all sequence vectors for a general context representation.Max pooling selects each feature's maximum value across the sequence, highlighting key molecular features.Molecular retrieval.Molecular retrieval aims for efficient, accurate identification of molecules in large datasets.Its effectiveness relies on the type of molecular data used, like structural formulas or SMILES strings.In cross-modal retrieval tasks, Text2Mol 40 excels by combining natural language with molecular data into a unified semantic space for effective retrieval.Similarly, MoMu 33 and MolFM 34 use advanced contrastive learning to merge text and molecular graphics, enhancing the feature space for robust retrieval tasks.The pooling layers are similar to those detailed in molecular property prediction.</p>
<p>Molecular image recognition.</p>
<p>In our research, molecular image recognition focuses on converting visual representations of molecular structures into standardized, textual formats like SMILES.This task is particularly crucial for automating the interpretation of chemical literature, where molecular images are frequently used to describe compounds.DECIMER 1.0 (ref.41), Image2SMILES 42 and MICER 43 use transformer-based architectures and image augmentation, handling large datasets effectively.SwinOCSR 44 utilizes Swin transformers 45 for global feature extraction and tackles token imbalance with focal loss.</p>
<p>Molecule generation.Our research focuses on developing one-dimensional textual representations, particularly in the SMILES format.This includes text-based de novo molecule generation and molecular optimization due to their methodological similarities.In molecular generation, models adept at molecule captioning are equally capable of molecule generation.BioT5 (ref.46) further expands this to include protein-related texts, focusing on SELFIES and extending to protein property and drug-target interaction prediction.Molecular optimization aims to refine molecules for improved properties, aligning with generative goals.MoleculeSTM 47 uses contrastive learning on graphs and text, optimizing target attributes like binding affinity and drug-relatedness with precision.(train:validation:test) and remove molecules that cannot be converted into SELFIES to ensure the validity of all molecular data.In total, the refined dataset comprises 32,998 molecules, which are suitable for the text generation tasks depicted in Fig. 1, including text generation based on multimodal fusion.</p>
<p>Data distribution.</p>
<p>We analyse the suitability of data sources for language models and chemical-space coverage.As shown in Extended Data Fig. 1, we use different visualization methods to analyse the distribution of text lengths and the number of tokens generated by each model's tokenizer for various text data types.This approach allows us to evaluate the adaptability of language models to the textual characteristics of our dataset.We also focus on the top ten scaffolds within the dataset, counting the number of molecules for each scaffold.Here semitransparent bars represent the total count, whereas the solid bars indicate the quantity in the training set.By contrast, for the analysis of chemical-space coverage, we choose molecular weight, LogP (a measure of hydrophobicity), the number of aromatic rings and the topological polar surface area as descriptors.We examine the distribution and correlation of these descriptors within the dataset, providing insights into the chemical diversity and complexity present in our data.</p>
<p>Experimental setup.We conduct 1,263 experiments, encompassing 291 generative and retrieval tasks from the ChEBI-20-MM benchmark and 972 property prediction experiments.For nine text-to-text tasks, our models include 11 architectural variations (Fig. 6, items 3-5), combining three decoder-only models, four encoder-decoder models and four composite models with MolT5 as the decoder paired with various encoders.We conduct experiments based on the setup, performing each experiment three times to calculate the mean and standard deviation of the results.</p>
<p>For nine text-to-text translation tasks, our models encompass 11 architectural variations (Fig. 6, items 4-6), which include three decoder-only models, four encoder-decoder models and four composite models featuring MolT5 as the decoder paired with various encoders.For two graph-to-text tasks, we deploy 12 distinct architectures (item 7), and for two image-to-text tasks, we utilize 16 different architectures (item 8).Additionally, for two retrieval and nine property prediction tasks, we implement 11 encoder architectures (items 1-3 and 5) alongside native encoder-decoder configurations serving as molecular encoders.</p>
<p>Metrics</p>
<p>Analysis</p>
<p>https://doi.org/10.1038/s42256-024-00977-6precision, Valid for chemical validity of the generated molecules, Levenshtein distance for structural similarity and fingerprint-based metrics (such as Molecular ACCess System keys, RDKit and Morgan) for capturing chemical feature representation.Molecular retrieval is evaluated using the mean reciprocal rank and Recall at K (R@1, R@5 and R@10).For the property prediction metrics, we utilize widely recognized classification and regression tasks from MoleculeNet.For classification, we measure the ROC_AUC, the area under the precision-recall curve (PR_AUC) and the mean F1 score (F1_score).For regression tasks, we use the mean squared error (MSE_mean), root mean squared error (RMSE_mean) and mean absolute error (MAE_mean) to quantitatively assess the predictive accuracy of the models.</p>
<p>Analysis of model knowledge-learning preferences</p>
<p>The interpretability methods have become more important in LLMs, particularly in molecular science.As research increasingly validates the effectiveness of these LLMs, a significant challenge remains their 'black box' nature.This opacity limits our understanding of what knowledge the models have actually learned.Furthermore, it restricts the design and application of these models.Although existing interpretability methods, such as case-study-based attention and embedding visualizations, offer some insights, they often lack generalizability and provide limited understanding to researchers.Therefore, there is a pressing need for a unified analysis of batch data that can extract and distil insights into the models' knowledge-learning preferences and present them in an interpretable textual format.To explore the knowledge-learning preferences of models in process of molecular scientific language modelling, this section introduces a language-driven, statistically significant interpretable method.This tool constructs a token-mapping matrix between input data and inference outcomes, revealing the model's mechanism.</p>
<p>Particular high-frequency token mappings.We decompose the input data and inference outcomes of LLMs into corresponding tokens and select high-frequency tokens to construct a mapping matrix, denoted as A. Additionally, we apply special processing to these tokens, such as filtering out tokens related to chemical vocabulary in captions and removing irrelevant words.We typically view high-frequency token mappings as indicative of specialized knowledge.Our approach identifies two types of high-frequency token mappings: general pairs, which are widespread and lack specific context, and particular pairs, essential for certain contexts but not always the most frequent.The challenge arises when standard analysis methods prioritize these general high-frequency mappings, consequently overlooking the specific ones that often carry more valuable contextual insights.</p>
<p>Localized feature-filtering method.We utilize a localized feature-filtering method for high-frequency selection to eliminate common high-frequency mapping pairs, thereby more accurately filtering the model's knowledge mappings.Subsequently, a statistical significance method is applied to determine the filtering threshold, identifying high-frequency mapping pairs that reflect the model's chemical knowledge.</p>
<p>Proof point 1: sorting the matrix A in descending order by row and column totals enhances the similarity among adjacent values.Assuming each A ij follows an independent and identical distribution, such as normal distribution, the row sums R i and column sums C j mimic this distribution.Consequently, in the sorted matrix Aâ€², elements next to each other are more likely to come from rows or columns with similar totals.</p>
<p>Considering A â€² kl and its neighbours A â€² kÂ±1,lÂ±1 in Aâ€², which were close in sum in the original matrix A, the probability they are similar is mathematically represented as follows:
P(A â€² kl â‰ˆ A â€² kÂ±1,lÂ±1 |R i â‰ˆ R iÂ±1 , C j â‰ˆ C jÂ±1 ) &gt; P(A ij â‰ˆ A iÂ±1, jÂ±1 ). (1)
This highlights that adjacent elements in Aâ€² are more likely to have close values compared with those in A, due to the sorting based on similar row and column sums.</p>
<p>Proof point 2: to uncover significant patterns within a matrix, we introduce a threshold T, aiming to spotlight elements A ij that significantly surpass the mean plus a certain number of standard deviations of their neighbouring values.This process starts with assuming that elements in matrix A are drawn from a distribution, enabling the calculation of each element's neighbourhood mean Î¼ neighbour and standard deviation Ïƒ neighbour .This step evaluates whether an element exceeds the set threshold, the condition to filter elements that starkly differ from their surrounding context:
A ij &gt; Î¼ neighbour + T Ã— Ïƒ neighbour .
(
)2
To assess the actual significance of these findings, we calculate the actual proportion P actual of elements exceeding this threshold across the matrix and compare it with the expected proportion P expected under a normal distribution.The actual and expected proportions are determined by the fraction of elements satisfying our condition:
P actual = âˆ‘ I ij n Ã— m ,(3)P expected = 1 âˆ’ Î¦ ( Î¼ neighbour + T Ã— Ïƒ neighbour âˆ’ Î¼ Ïƒ ) ,(4)
where I ij is an indicator function that equals 1 when the condition is met and 0 otherwise, and n Ã— m represents the matrix size.Î¦ represents the cumulative distribution function of the standard normal distribution, and Î¼ and Ïƒ are the mean and standard deviation of the entire matrix's elements, respectively.Finally, a statistical test, such as the Z test, evaluates the significance of the observed proportion against the expected, indicating whether the identified patterns are indeed statistically significant beyond random chance: (
)5
This succinct methodology not only clarifies the process of identifying statistically significant patterns but also underlines its effectiveness in distinguishing genuine high-frequency elements from those of a random distribution, offering valuable insights into the structure of the knowledge-learning preferences of models.</p>
<p>Comparison with other interpretability methods.Interpretability in machine learning, especially within molecular science, utilizes various methods to render the decision-making processes of models both accessible and comprehensible.As shown in Supplementary Table 12, researchers frequently use techniques such as text-based attention map visualizations and molecular-graph-based attention heat maps.These methods typically rely on inductive reasoning from specific case studies, using a bottom-up approach to derive insights.Although these methods can provide valuable insights, their dependence on extensive case studies renders the process time-consuming and complex.Furthermore, the insights obtained through these traditional methods often require further verification to confirm their validity, thereby challenging their reliability and general applicability.By contrast, our approach, termed the localized feature-filtering method, initiates the process of insight derivation through statistical significance techniques.We can set thresholds to balance the volume of insights with their confidence levels, using a top-down validation approach to substantiate the effectiveness of our selected insights.This method Analysis https://doi.org/10.1038/s42256-024-00977-6not only ensures the credibility of the insights but also significantly boosts the efficiency of insight extraction.By combining rigorous statistical analysis with a systematic validation framework, our approach enhances the interpretability of molecular models, offering a robust and efficient alternative to traditional, more heuristic methods.</p>
<p>Utility and potential application.LLMs demonstrate significant superiority in molecular science.Although their performance in representation tasks may align with that of traditional approaches and graph models, their unparalleled ability to generate coherent and contextually relevant text sets them apart.Our benchmark and modal transition matrix are designed to rapidly identify optimal molecular modalities, model architectures and training strategies for specialized tasks within molecular science.For instance, the BioT5+ model 48 , which utilizes molecular IUPAC names and SELFIES for T5-based caption generation, achieves state-of-the-art results.It illustrates a new use of IUPAC names, a previously unrecognized strategy in the field, which is also concluded by our benchmark analysis.</p>
<p>The localized feature-filtering method facilitates the rapid extraction of a model's knowledge-learning preferences.This capability is instrumental in guiding model training, for example, by optimizing the tokenizer or refining model parameters to boost the learning efficiency.Furthermore, it allows for the strategic adjustment of threshold T to unearth potential scientific insights.By exploring these insights, researchers can probe new dimensions of molecular behaviour, paving the way for innovative breakthroughs in molecular design and synthesis.Through these applications and methodological advancements, our approach demonstrates its utility, providing a robust framework for enhancing both interpretability and functionality of models in molecular science.</p>
<p>Fig. 1 |
1
Fig. 1 | Paradigm of the analysis.a, Molecular modelling and design tasks, showcasing six task types with their standard modelling methods and data examples.b, Paradigms of tasks, in which we divide common molecular data into two categories: internal and external information.Internal information, integral to molecular representation, can be converted through various tools.External information is more accessible to human understanding.Additionally, this part highlights the research scope of our analysis, detailing the input and output for each task.</p>
<p>Analysishttps://doi.org/10.1038/s42256-024-00977-6</p>
<p>aFig. 2 |
2
Fig. 2 | Results of benchmark.a, Modal transition probability matrix: this matrix presents the performance in text generation and property prediction tasks.The vertical axis represents the input modalities, whereas the horizontal axis denotes the output modalities.b, Encoders and decoders in nine text-to-text tasks: this illustration highlights the frequency of various models appearing in the top five rankings.The T5-based models exhibit a dominant presence.c, Encoders, pooling mechanisms and retrieval performance in embedding tasks.Alongside model rankings, the figure indicates that average pooling is a preferred choice for the pooling layer.</p>
<p>Analysis https://doi.org/10.1038/s42256-024-00977-6(2S)-2-(dihydroxyamino)-8-methylsulfanyloctanoate a Token-mapping matrix and threshold T analysis b Case studies of knowledge-learning preferences Transition of IUPAC name to caption (2S)-2-[[(2S)-2-amino-3-carboxypropanoyl]amino]-5-(diaminomethylideneamino)pentanoic acid L-Aspartyl-L-arginine ent-&gt;acid</p>
<ol>
<li>14 0Fig. 3 |
143
Fig. 3 | Knowledge patterns and insights.a, Token-mapping matrix and threshold T analysis: the two matrices represent the high-frequency tokenmapping patterns generated by the processes from IUPAC names and SELFIES to molecular captions.As the threshold T increases, the selection criteria for identifying specific high-frequency token pairs become more stringent,</li>
</ol>
<p>Fig. 4 |
4
Fig. 4 | Performance of multimodal fusion.a, Multimodal fusion performance for molecular property predictions: this figure displays the AUC-ROC results for various molecular property prediction classification tasks.It compares the performance of the SciBERT and MolT5 models as encoders using SMILES (S) as the input text and BioT5 using SELFIES as the input text, with the graph model (graph isomorphism network (GIN)) utilizing graph data (G).In each subplot, the final results contributed by the vectors obtained after encoding and pooling from the foundation models are shown.add, vector addition; weight_add, adaptive weighted vector addition; concat, concatenated encoding followed by pooling; attention, concatenated encoding processed by the attention mechanism before pooling.Different colours represent different tasks.b, Multimodal fusion performance for molecule captioning.This panel shows the performance of six textual similarity metrics across two common datasets for molecule-captioning tasks.The x axis represents the models and input modalities, whereas the y axis represents the metric values.Each colour corresponds to a different metric.RNN, recurrent neural network.</p>
<p>aFig. 5 |
5
Fig. 5 | An overview of model tasks and architectures.a, Tasks and models: it clarifies the relationship between six downstream tasks and model architectures.b, Encoder-decoder model architectures: it delineates three main frameworks: (1) text-text is primarily focused on the text translation tasks, (2) graph-text is predominantly used in contrastive learning frameworks and serves as an encoder for downstream tasks and (3) image-text is chiefly applied in molecular image recognition tasks.</p>
<p>Fig. 6 |
6
Fig. 6 | Overview of benchmark experiments.Our study encompasses tests across eight primary model architectures, each featuring 3-4 common foundation models or composite models within its category.In total, 1,263 experimental setups were conducted, demonstrating the adaptability of various</p>
<p>Z = P actual âˆ’ P expected âˆš P expected (1âˆ’P expected ) nÃ—m .</p>
<p>.</p>
<p>We use the BLEU (Bilingual Evaluation Understudy) and ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metrics, including BLEU-2, BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L and METEOR scores, for evaluating IUPAC name recognition and molecule captioning.Moreover, for image recognition and molecule generation, we assess models using BLEU for textual similarity, Exact Match for</p>
<p>AcknowledgementsThis work is supported by grants from the National Natural Science Foundation of China (grant nos.62372484 and 62172456), the Major Key Project of PCL PCL2021A13 and Pengcheng Cloud Brain.Data availabilityThe dataset ChEBI-20-MM49can be accessed at https://huggingface. co/datasets/liupf/ChEBI-20-MM.Code availabilityThe open-source project repository is available via Zenodo (https://doi.org/10.5281/zenodo.14293309)50and via GitHub (https://github.com/AI-HPC-Research-Team/SLM4Mol).It is available for non-commercial use.Analysishttps://doi.org/10.1038/s42256-024-00977-6models like MolXPT28(based on GPT-2) and MolReGPT29(based on ChatGPT) showcase strong performance in generating molecular sequences and descriptions.The typical decoding task can be formulated as Y t = Decoder(Y &lt;t , C), where Y t is the output at time t, Y &lt;t represents all preceding outputs and C is the contextual input.Author contributions P.L. was involved in the conceptualization, methodology, data curation, model training and writing (original draft).J.T. was involved in the funding acquisition and writing (review and editing).Z.R. was involved in the conceptualization, formal analysis, supervision, funding acquisition and writing (review and editing).Competing interestsThe authors declare no competing interests.Additional informationExtended data is available for this paper at https://doi.org/10.1038/s42256-024-00977-6.Supplementary informationThe online version contains supplementary material available at https://doi.org/10.1038/s42256-024-00977-6.Correspondence and requests for materials should be addressed to Zhixiang Ren.Peer review information Nature Machine Intelligence thanks LeroyCronin, Karl Grantham and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.Reprints and permissions information is available at www.nature.com/reprints.Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.
Generative adversarial networks: an overview. A Creswell, IEEE Signal Process. Magazine. 352018</p>
<p>GeoDiff: a geometric diffusion model for molecular conformation generation. M Xu, Proc. International Conference on Learning Representations 2181. International Conference on Learning Representations 2181ICLR2022</p>
<p>Support vector machines for drug discovery. K Heikamp, J Bajorath, Expert Opin. Drug Discov. 92014</p>
<p>How powerful are graph neural networks?. K Xu, W Hu, J Leskovec, S Jegelka, Proc. International Conference on Learning Representations 9104-9121. International Conference on Learning Representations 9104-9121ICLR2019</p>
<p>Semi-supervised classification with graph convolutional networks. T N Kipf, M Welling, Proc. International Conference on Learning Representations 2713-2727. International Conference on Learning Representations 2713-27272017</p>
<p>Graph attention networks. P VeliÄkoviÄ‡, Proc. International Conference on Learning Representations 2920-2932. International Conference on Learning Representations 2920-2932ICLR2018</p>
<p>Attention is all you need. A Vaswani, Advances in Neural Information Processing Systems. 2017</p>
<p>ChatGPT: optimizing language models for dialogue. J Schulman, OpenAI Blog. 2022</p>
<p>OpenAI. GPT-4 technical report. 2023Preprint at</p>
<p>Molgensurvey: a systematic survey in machine learning models for molecule design. Y Du, T Fu, J Sun, S Liu, 2022Preprint at</p>
<p>Knowledge-informed molecular learning: a survey on paradigm transfer. Y Fang, Q Zhang, Z Chen, X Fan, H Chen, Proc. International Conference on Knowledge Science, Engineering and Management. International Conference on Knowledge Science, Engineering and ManagementSingaporeSpringer Nature2024</p>
<p>T Guo, Advances in Neural Information Processing Systems. Curran Associates202336</p>
<p>Mol-Instructions: a large-scale biomolecular instruction dataset for large language models. Y Fang, Proc. The Twelfth International Conference on Learning Representations. The Twelfth International Conference on Learning Representations2024</p>
<p>SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. D Weininger, J. Chem. Inf. Comput. Sci. 281988</p>
<p>InChI-the worldwide chemical structure identifier standard. S Heller, A Mcnaught, S Stein, D Tchekhovskoi, I Pletnev, J. Cheminform. 572013</p>
<p>Self-referencing embedded strings (SELFIES): a 100% robust molecular string representation. M Krenn, F HÃ¤se, A Nigam, P Friederich, A Aspuru-Guzik, Mach. Learn. Sci. Technol. 1450242020</p>
<p>RDKit: a software suite for cheminformatics, computational chemistry, and predictive modeling. G Landrum, Greg Landrum. 852812013</p>
<p>Limit of detection. A closer look at the IUPAC definition. G L Long, J D Winefordner, Anal. Chem. 551983</p>
<p>MoleculeNet: a benchmark for molecular machine learning. Z Wu, Chem. Sci. 92018</p>
<p>BioGPT: generative pre-trained transformer for biomedical text generation and mining. R Luo, Brief. Bioinform. 234092022</p>
<p>Translation between molecules and natural language. C Edwards, Proc. 2022 Conference on Empirical Methods in Natural Language Processing. 2022 Conference on Empirical Methods in Natural Language essing2022</p>
<p>MolCA: molecular graph-language modeling with cross-modal projector and uni-modal adapter. Z Liu, Proc. 2023 Conference on Empirical Methods in Natural Language Processing 15623-15638. 2023 Conference on Empirical Methods in Natural Language essing 15623-15638Association for Computational Linguistics2023</p>
<p>Language models are unsupervised multitask learners. A Radford, OpenAI Blog. 2019</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Proc. NAACL-HLT. NAACL-HLT2019</p>
<p>RoBERTa: a robustly optimized BERT pretraining approach. Y Liu, 2019Preprint at</p>
<p>SciBERT: a pretrained language model for scientific text. I Beltagy, K Lo, A Cohan, Proc. 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. 2019 Conference on Empirical Methods in Natural Language essing and the 9th International Joint Conference on Natural Language essingAssociation for Computational Linguistics20193615</p>
<p>ChemBERTa: large-scale self-supervised pretraining for molecular property prediction. S Chithrananda, G Grand, B Ramsundar, 2020Preprint at</p>
<p>MolXPT: wrapping molecules with text for generative pre-training. Z Liu, Proc. 61st Annual Meeting of the Association for Computational Linguistics. Short Papers. 61st Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics20232</p>
<p>Empowering molecule discovery for molecule-caption translation with large language models: a ChatGPT perspective. J Li, IEEE Trans. Knowl. Data Eng. 362024</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, J. Mach. Learn. Res. 212020</p>
<p>. 10.1038/s42256-024-00977-6Analysis. </p>
<p>BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. M Lewis, Proc. 58th Annual Meeting of the Association for Computational Linguistics 7871-7880. 58th Annual Meeting of the Association for Computational Linguistics 7871-7880Association for Computational Linguistics2020</p>
<p>PubChem 2023 update. S Kim, Nucleic Acids Res. 512023</p>
<p>A molecular multimodal foundation model associating molecule graphs with natural language. B Su, 2022Preprint at</p>
<p>MolFM: a multimodal molecular foundation model. Y Luo, K Yang, M Hong, X Liu, Z Nie, 2023Preprint</p>
<p>Git-mol: a multi-modal large language model for molecular science with graph, image, and text. P Liu, Y Ren, J Tao, Z Ren, Comput. Biol. Med. 1711080732024</p>
<p>BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. J Li, D Li, S Savarese, S Hoi, International Conference on Machine Learning 19730-19742. PMLR2023</p>
<p>Transformer-based artificial neural network for the conversion between chemical notations. L Krasnov, I Khokhlov, M Fedorov, S Sosnin, Sci. Rep. 11147982021</p>
<p>Transformer-based molecular generative model for antiviral drug design. J Mao, J. Chem. Inf. Model. 642023</p>
<p>A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals. Z Zeng, Y Yao, Z Liu, M Sun, Nat. Commun. 138622022</p>
<p>Text2Mol: cross-modal molecule retrieval with natural language queries. C Edwards, C Zhai, H Ji, Proc. 2021 Conference on Empirical Methods in Natural Language Processing. 2021 Conference on Empirical Methods in Natural Language essingAssociation for Computational Linguistics2021</p>
<p>DECIMER 1.0: deep learning for chemical image recognition using transformers. K Rajan, A Zielesny, C Steinbeck, J. Cheminform. 13612021</p>
<p>Image2SMILES: transformer-based molecular optical recognition engine. I Khokhlov, L Krasnov, M V Fedorov, S Sosnin, Chem. Methods. 2e2021000692022</p>
<p>MICER: a pre-trained encoder-decoder architecture for molecular image captioning. J Yi, Bioinformatics. 382022</p>
<p>SwinOCSR: end-to-end optical chemical structure recognition using a swin transformer. Z Xu, J Li, Z Yang, S Li, H Li, J. Cheminform. 14412022</p>
<p>Swin transformer: hierarchical vision transformer using shifted windows. Z Liu, Proc. IEEE/CVF International Conference on Computer Vision. IEEE/CVF International Conference on Computer Vision2021</p>
<p>BioT5: enriching cross-modal integration in biology with chemical knowledge and natural language associations. Q Pei, Proc. 2023 Conference on Empirical Methods in Natural Language Processing 1102-1123. 2023 Conference on Empirical Methods in Natural Language essing 1102-1123Association for Computational Linguistics2023</p>
<p>Multi-modal molecule structure-text model for text-based retrieval and editing. S Liu, Nat. Mach. Intell. 52023</p>
<p>BioT5+: towards generalized biological understanding with IUPAC integration and multi-task tuning. Q Pei, Findings of the Association for Computational Linguistics: ACL 2024 1216-1240. Association for Computational Linguistics2024</p>
<p>ChEBI-20-MM Dataset (revision 7d44959. P Liu, 2024</p>
<p>. P Liu, Z Ren, Ai-Hpc, 10.5281/zenodo.14293309Research-Team/SLM4Mol: 1.02024</p>            </div>
        </div>

    </div>
</body>
</html>
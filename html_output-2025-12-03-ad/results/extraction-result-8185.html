<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8185 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8185</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8185</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-273850412</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.03766v3.pdf" target="_blank">Number Cookbook: Number Understanding of Language Models and How to Improve It</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) can solve an increasing number of complex reasoning tasks while making surprising mistakes in basic numerical understanding and processing (such as 9.11>9.9). The latter ability is essential for tackling complex arithmetic and mathematical problems and serves as a foundation for most reasoning tasks, but previous work paid little attention to it or only discussed several restricted tasks (like integer addition). In this paper, we comprehensively investigate the numerical understanding and processing ability (NUPA) of LLMs. Firstly, we introduce a benchmark covering four common numerical representations and 17 distinct numerical tasks in four major categories, resulting in 41 meaningful combinations in total. These tasks are derived from primary and secondary education curricula, encompassing nearly all everyday numerical understanding and processing scenarios, and the rules of these tasks are very simple and clear. Through the benchmark, we find that current LLMs fail frequently in many of the tasks. To study the problem, we train small models with existing and potential techniques for enhancing NUPA (such as tokenizers, PEs, and number formats), comprehensively evaluating their effectiveness using our testbed. We also finetune practical-scale LLMs on our proposed NUPA tasks and find that 1) naive finetuning can improve NUPA a lot on many but not all tasks, and 2) surprisingly, techniques designed to enhance NUPA prove ineffective for finetuning pretrained models. We further explore the impact of chain-of-thought techniques on NUPA. Our work provides a more detailed and comprehensive understanding of NUPA in LLMs. Our benchmark and code are released at https://github.com/GraphPKU/number_cookbook.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8185.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8185.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NUPA Test</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Number Understanding and Processing Ability (NUPA) Test</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark introduced in this paper that systematically evaluates LLMs on core numerical representations (integer, float, fraction, scientific notation) across 17 task types (41 representation-task pairs) with metrics exact-match, digit-match and dlength for fine-grained failure analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>benchmark (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Synthetic dataset generation covering 1-100 digit ranges (split into S/M/L/XL intervals), tasks extracted from primary/secondary curricula; includes training/validation/test splits and task prompts/formatting rules.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>comprehensive: integer/float/fraction/scientific tasks including add, sub, multiply, true/floor/mod division, compare (max/min), digit operations (get digit, length, count, digit-compare, digit-add), conversions (to_float, to_scientific, sig.fig.)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Not a model; formalizes four numeric representations and digit-level alignment as the central representational concern for LLM arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Used as evaluation probe for zero-shot, few-shot, fine-tuned, and rule-following CoT models; metrics include exact-match, digit-match (per-digit accuracy after alignment), and dlength (sum absolute length differences per part).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>N/A (benchmark specification). The paper reports per-model exact-match / digit-match / dlength over four length buckets (S/M/L/XL).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>N/A (benchmark specification) — the benchmark is designed to expose length sensitivity, digit misalignment, representation-specific failures (fractions/scientific), and digit-understanding failures.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Number Cookbook: Number Understanding of Language Models and How to Improve It', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8185.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8185.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (and GPT-4o-mini as reported together)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>State-of-the-art commercial autoregressive models evaluated zero-shot on the NUPA Test; strong on small/in-domain integer tasks but show sharp degradation with length and with non-integer representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o / GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4o family (commercial), autoregressive transformer; exact pretraining details not provided in paper (evaluated via OpenAI API: gpt-4o-2024-08-06 and gpt-4o-mini-2024-07-18).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Zero-shot evaluation across NUPA tasks: integer/float addition, max/min, digit tasks (get_digit, length, digit_add), fraction/scientific conversions, modulus, truediv, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Empirical evidence suggests digit-by-digit or token-level processing with sensitivity to tokenization/alignment and to global length cues; no internal neuron-level mechanism probed in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Zero-shot prompts with strict format enforcement (regex); few-shot experiments also reported in appendices; no internal probes or activation interventions applied to GPT-4o itself.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Representative results reported in paper: integer addition exact-match (S≈1.00, M≈0.80, L≈0.40, XL≈0.15); float addition exact-match (S≈0.90, M≈0.60, L≈0.15, XL<0.05); get_digit in XL range ≈0.20 (vs random baseline 0.10); poor (<20%) on fractions and scientific notation even in S-range.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Length sensitivity (performance drops rapidly with increasing digit length); digit-understanding failures (cannot reliably return i-th digit for long numbers); representation brittleness (fractions & scientific notation fail more than integers/floats); wrong output length (high dlength) or missing digits; arithmetic mistakes even on apparently simple operations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Analyses of digit-match and dlength indicate failures stem from both per-digit prediction errors and incorrect output length; comparison across tasks suggests models rely on shortcuts like sequence-length heuristics and token alignment rather than robust digit-level algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Performs near-perfect on small integer tasks (S-range), demonstrating capacity for arithmetic in constrained regimes; but these successes do not generalize to slightly longer inputs or less common representations (fractions/scientific).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Number Cookbook: Number Understanding of Language Models and How to Improve It', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8185.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8185.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2 family (Qwen2-7B, Qwen2-72B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source LLM family evaluated on NUPA; generally matches trend of strong small-length integer performance but sharp degradation on length and non-integer representations, similar to GPT-4o.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2-7B / Qwen2-72B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Qwen2 family of autoregressive transformers (Alibaba/Qwen Team); evaluated with/without 4-bit quantization; specific pretraining not detailed in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same NUPA suite as other evaluated LLMs (add/sub/mul/div, digit-tasks, conversion, comparisons) in zero-shot and few-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Empirical behavior consistent with digit-token alignment processing and length heuristics; no internal representational probe performed.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Zero-shot prompts; few-shot reported; no activation probing.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative summary: strong on easy integer tasks (S/M ranges); large drop for longer inputs and for fractions/scientific notation (average accuracy below 20% even in short S-range for many such tasks). Table 3 shows Qwen2-72B example scores: some tasks high (e.g., digit-related/maybe copy tasks) but many arithmetic tasks low (see paper figures/tables).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Similar to GPT-4o: length sensitivity, digit-level errors, failures on fractions/scientific, and modulus/significant-figure tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Observed patterns across tasks and metrics (digit-match/dlength) point to tokenization and alignment causes rather than a single arithmetic algorithm learned.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Large Qwen2 models and small Qwen2 models show similar behavior once scale reaches a threshold, suggesting scaling alone doesn't fix representation-specific failures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Number Cookbook: Number Understanding of Language Models and How to Improve It', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8185.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8185.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3.1-8B (baseline and finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Meta LLaMA-3.1 8B evaluated both as base model and after targeted finetuning (LoRA) on NUPA tasks; naive finetuning substantially improves many NUPA tasks but not digit-specific tasks; modifications to tokenizer/PE during post-training harmed performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B (and Llama-3.1-8B-ft (LoRA))</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer (LLaMA-3.1 family). Baseline 8B parameters; finetuning performed with LoRA (rank=128, α=32) on a mixed NUPA training set (~50M samples; 800 LoRA steps).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Trained/fine-tuned on the full NUPA suite (mixed tasks); evaluated on additions, comparisons, truediv, float/fraction/scientific tasks, digit tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Finetuning supplies missing numerical diversity and task-specific procedural knowledge; still limited on digit-identification tasks, implying some capabilities are not recovered by simple SFT/LoRA.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>LoRA finetuning on mixed NUPA tasks; additional intervention attempts included replacing PE or tokenizer or changing number formats post-training (these disrupted behavior). Also finetuned with RF-CoT in separate experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Naive LoRA finetuning: large improvements after ~800 steps (~50M samples). Example from paper (Table 3): Llama-8B-ft Add-Float ≈ 0.88±0.02 exact-match (compared with GPT-4o ≈0.78 and RF-CoT 1.00). On several tasks (max, max-hard, add-float, truediv) Llama-8B-ft matched or surpassed GPT-4o. Digit tasks remained largely unimproved.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Finetuning improves many arithmetic tasks but fails to remediate digit-understanding failure modes (e.g., get_digit long-range) and representation brittleness (fractions/scientific still challenging relative to integers). Alterations to a pretrained model's tokenizer or positional encoding during finetuning produced large performance drops.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical: targeted SFT provides missing task diversity and substantially improves results, showing that some NUPA gaps are due to data rather than pure capacity limits. Conversely, changing low-level model components (PEs/tokenizer) post-hoc causes catastrophic degradation, implying these components are entangled with learned behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Although LoRA finetuning produced large gains on many tasks, it did not solve digit-specific tasks or fully close gaps on fractions/scientific notation; also, tricks that help small-from-scratch models (PE/tokenizer changes) do not transfer when applied to an already pretrained large model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Number Cookbook: Number Understanding of Language Models and How to Improve It', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8185.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8185.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Small-from-scratch models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Small transformer models trained from scratch (0.1B, 0.9B, 3B) for controlled experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Models trained from scratch on single NUPA tasks to probe the effects of tokenization, positional encodings, and data-format interventions under controlled conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>0.1B / 0.9B / 3B decoder-only transformers (LLaMA-3.1-like)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models with 100M, 0.9B and 3B parameters (architectural details in Appendix Table 11), trained from scratch on single tasks (e.g., integer addition 1-8 digits) to measure in-domain and OOD length generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Single-task training: integer addition, float addition, integer multiplication, fraction multiplication, scientific addition in controlled length ranges; then evaluated on longer lengths (OOD).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Findings emphasize token-level processing and digit alignment: the unit of tokenization (one-digit vs k-digit) governs how digits are represented and aligned for autoregressive prediction and consequently affects algorithmic generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Training-from-scratch on controlled datasets with varying tokenizers (1-digit, 2-digit, 3-digit, random tokenizers), positional encodings (RoPE/Alibi/NoPE), and data-format manipulations (reverse formatting, zero-padding, index hints), then measuring exact-match, digit-match and dlength.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>One-digit tokenizer gave best in-domain and OOD performance for sub-billion models; 3-digit tokenizer performed poorly for small models. Scaling to 3B improved ability to learn larger tokenizers but did not make large-tokenizer superior to one-digit. Random tokenizers improved OOD length generalization compared to fixed multi-digit tokenizers but still trailed one-digit.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Multi-digit tokenizers (k≥2) caused worse digit alignment and length generalization for small models; models learned length-dependent shortcuts depending on PE leading to catastrophic drops beyond training-length boundary.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Controlled experiments (Figures 4,9–12) show clear performance gaps by tokenizer type and PE; digit-match and dlength metrics corroborate that one-digit tokenization facilitates per-digit learning and correct output length.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Scaling reduces but does not eliminate the penalty of larger tokenizers; random tokenizers help but do not outperform 1-digit tokenizers in these controlled settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Number Cookbook: Number Understanding of Language Models and How to Improve It', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8185.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8185.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tokenizers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Number tokenization schemes (one-digit, k-digit greedy, random)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Different tokenization strategies substantially affect LLM arithmetic: one-digit tokenizers facilitate digit alignment and length generalization for small/medium models, while k-digit tokenizers (k=2,3) harm sub-billion models; random tokenizers provide regularization improving OOD generalization but not surpassing one-digit.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various trained small models / evaluated LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Tokenizers studied include: one-digit (digit as token), aligned k-digit tokenizers (k=2,3 with greedy left-to-right segmentation), and random tokenizers (variable-length split between 1 and max). Modern LLMs often use k=3 numeric vocabularies.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Impact measured across integer addition, float addition, integer multiplication, and length-generalization experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Tokenization determines atomic units for autoregressive prediction; poor tokenization breaks digit alignment and makes per-digit computation and carry handling difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Experimental comparison: trained models with different tokenizers on identical data and compared in-domain and OOD performance (exact-match, digit-match, dlength).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>One-digit tokenizers showed best overall in-domain and OOD performance for sub-billion models; 3-digit tokenizers performed worst for small models. 3B model narrowed gap (2-digit ≈ 1-digit) but 3-digit still inferior. Random tokenizer improved OOD performance over fixed multi-digit tokenizers.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Multi-digit tokenizers induce misalignment and more missing digits (increased dlength) and worse per-digit accuracy; token vocabulary growth imposes higher model capacity requirements to learn arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Training curves (Figures 4,9–12) show faster/higher convergence for one-digit tokenizers and higher digit-match; theoretical observations: number tokens have different distributional properties than text and combinatorial structure (concatenation forms new valid tokens), which makes large numeric token vocabularies less helpful.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Large models (≥3B) reduce but do not eliminate disadvantages of larger tokenizers; real-world deployed large LLMs nonetheless often use 3-digit tokenizers, suggesting trade-offs with text modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Number Cookbook: Number Understanding of Language Models and How to Improve It', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8185.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8185.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Positional Encodings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoPE vs Alibi vs NoPE (positional encodings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Different positional encodings change length generalization: RoPE encourages learning a length-dependent shortcut (strong in-domain but catastrophic boundary drops), while Alibi and NoPE act as regularizers that improve length generalization on numerical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>100M / small models (controlled experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PE variants tested: RoPE (rotary position embedding), Alibi (linear attention bias), NoPE (no positional encoding relying on causal mask). Models trained on 1–8 digit ranges then evaluated to 1–20.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Integer addition, float addition, fraction multiplication, scientific addition (controlled training on short lengths, OOD test to longer lengths).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>RoPE provides explicit position signals enabling models to shortcut by encoding absolute length/position cues; Alibi/NoPE suppress strong absolute position signals, encouraging stepwise digit processing.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Train/test experiments comparing exact-match/digit-match/dlength for same models with different PE choices; boundary tests (D8→D9) to expose length-overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Alibi and NoPE demonstrated superior length generalization compared to RoPE across tasks. RoPE produced near-100% in-domain at training boundary but collapsed just beyond (example: int-add exact-match drops from ~100% at 8 digits to ~0% at 9); RoPE sometimes better on tiny models/in-domain.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>RoPE-induced length overfitting: models rely on training-range length heuristics producing catastrophic failures just beyond training lengths (large dlength and low digit-match).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical boundary experiments (Table 12) showing dramatic drops when stepping out of training-length with RoPE; small-model experiments (Table 13) where RoPE helps in-domain when capacity is low but hurts generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>RoPE can be beneficial when model capacity/data are limited (it helps in-domain learning), indicating the inductive bias trade-off between fast in-domain learning and robust length generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Number Cookbook: Number Understanding of Language Models and How to Improve It', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8185.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8185.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Data-format interventions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reverse formatting, zero padding, index hints</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Input-output formatting manipulations that improve digit alignment: reverse formatting and zero padding both help integer and float addition and length generalization; index hints were ineffective in the authors' finetuning settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>0.1B / small models (controlled experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Data formatting techniques applied to training data: reverse digit order (least-significant first), left zero-padding inputs to equal length, and explicit per-digit index tokens (index hints).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Primarily integer and float addition (also extended to other tasks in NUPA), evaluated for length-generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Formatting aligns digits to the model's autoregressive generation order and standardizes operand lengths, reducing the need for long-lookahead carry reasoning and improving digit pairing.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>A/B experiments comparing vanilla formatting vs reverse / zero-padding / combinations; measured exact-match and length generalization (Table 16 and figures).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reverse formatting and zero-padding each outperform vanilla formats for integer and float addition; combination gives similar benefits (functionality overlaps). Index hints did not improve performance on the tested models and digit ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Reverse formatting's theoretical advantage for carry-handling is limited in practice because worst-case carry dependencies are rare; index hints may help only in regimes (very small models, long training ranges) not studied here.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical gains in Table 16 and Figures 14–18 show higher exact-match on addition tasks with reverse/pad formats; theoretical argument about carry cases explains limited additional benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Index hints produced contradictory results compared to prior work (likely due to model size and training-range differences); reverse formatting/padding help but do not solve digit understanding or representation diversity issues (fractions/scientific).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Number Cookbook: Number Understanding of Language Models and How to Improve It', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8185.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8185.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RF-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rule-Following Chain-of-Thought (RF-CoT / RF- CoT / Rule-Following CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chain-of-thought variant where models are finetuned to follow explicit code/pseudocode procedures (rules) for arithmetic; it substantially improves correctness on many tasks but requires large context windows and is much slower at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B (finetuned with RF-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-3.1-8B finetuned to generate rule-following intermediate steps (python-like pseudo-code) before the final answer; context window set at 2000 tokens during experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Selected NUPA tasks that can be decomposed into recurrences/basic unit ops (e.g., fraction addition, multiplication, multi-digit addition).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Makes the solving algorithm explicit in the generated trace so the model follows a deterministic multi-step procedural algorithm (digit-by-digit or chunked addition) rather than attempting to produce answer in one token sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Finetuning on RF-CoT data (prompt + expected stepwise response), then inference where prompt includes rule and model generates steps + answer. Measured exact-match and also maximal digit-length affordance within 2k context.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>RF-CoT often achieved near-perfect exact-match on tasks within the context limit (examples in Table 3: RF-CoT Add-Float 1.00±.00; other tasks often high). Llama-8B-ft (non-RF-CoT) had Add-Float ≈0.88±.02; GPT-4o ≈0.78 (per Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Severe context-window limitation: many tasks exceed 2000-token limit for realistic digit lengths (fraction-add limited to ~3-digit within 2k). Inference is much slower: RF-CoT ≈5.625s/sample vs direct answer ≈0.336–0.371s/sample (~17x slower).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical gains when RF-CoT is used and sufficient context is available; but dramatic inference-time and context-size cost quantified in Tables 2 and 3 and Appendix A.5.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Scalability: RF-CoT cannot handle large digit lengths within practical context windows and is computationally expensive, making it impractical as a general solution for routine NUPA operations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Number Cookbook: Number Understanding of Language Models and How to Improve It', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8185.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8185.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Finetuning + structural changes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Post-training interventions on pretrained LLMs (changing PE, tokenizer, representation during/after finetuning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Attempts to adapt pretrained LLMs by changing low-level components (positional encoding, tokenizer, number formats) during finetuning produced detrimental effects, suggesting these components are entangled with learned behavior and not easily replaced post-hoc.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B (experiments reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained LLaMA-3.1-8B checkpoint modified by altering PE (e.g., RoPE→NoPE/Alibi), replacing tokenizer (e.g., to one-digit), or changing number formats, then finetuned or evaluated; LoRA finetuning without structural changes used as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Mixed NUPA tasks as in the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Findings suggest pretrained models encode entangled dependencies between their positional/tokenization schemes and numeracy behaviors; arbitrarily swapping these at finetuning disrupts learned strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Empirical interventions: finetune with new PE/tokenizer/representation (Table 18), compare to naive LoRA finetuning and original baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>None of the structural modification attempts outperformed naive finetuning or the original model; many modifications caused substantial performance drops (see Table 18 in Appendix A.4.5).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Catastrophic degradation after changing low-level model components post-pretraining; suggests interventions that help from-scratch training may break behavior of pretrained models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical negative results: direct attempts to change PE/tokenizer in a pretrained checkpoint yielded worse NUPA performance compared to baseline or simple LoRA finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>These negative results do not preclude alternative integration strategies (e.g., re-pretraining or more careful adapter techniques), but show the straightforward approach of swapping components during finetuning is harmful.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Number Cookbook: Number Understanding of Language Models and How to Improve It', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Positional description matters for transformers arithmetic <em>(Rating: 2)</em></li>
                <li>Tokenization counts: the impact of tokenization on arithmetic in frontier llms <em>(Rating: 2)</em></li>
                <li>Train short, test long: Attention with linear biases enables input length extrapolation <em>(Rating: 2)</em></li>
                <li>Teaching arithmetic to small transformers <em>(Rating: 2)</em></li>
                <li>Transformers can achieve length generalization but not robustly <em>(Rating: 2)</em></li>
                <li>Injecting numerical reasoning skills into language models <em>(Rating: 1)</em></li>
                <li>Subword regularization: Improving neural network translation models with multiple sub-word candidates <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8185",
    "paper_id": "paper-273850412",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "NUPA Test",
            "name_full": "Number Understanding and Processing Ability (NUPA) Test",
            "brief_description": "A benchmark introduced in this paper that systematically evaluates LLMs on core numerical representations (integer, float, fraction, scientific notation) across 17 task types (41 representation-task pairs) with metrics exact-match, digit-match and dlength for fine-grained failure analysis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "benchmark (dataset)",
            "model_description": "Synthetic dataset generation covering 1-100 digit ranges (split into S/M/L/XL intervals), tasks extracted from primary/secondary curricula; includes training/validation/test splits and task prompts/formatting rules.",
            "arithmetic_task_type": "comprehensive: integer/float/fraction/scientific tasks including add, sub, multiply, true/floor/mod division, compare (max/min), digit operations (get digit, length, count, digit-compare, digit-add), conversions (to_float, to_scientific, sig.fig.)",
            "mechanism_or_representation": "Not a model; formalizes four numeric representations and digit-level alignment as the central representational concern for LLM arithmetic.",
            "probing_or_intervention_method": "Used as evaluation probe for zero-shot, few-shot, fine-tuned, and rule-following CoT models; metrics include exact-match, digit-match (per-digit accuracy after alignment), and dlength (sum absolute length differences per part).",
            "performance_metrics": "N/A (benchmark specification). The paper reports per-model exact-match / digit-match / dlength over four length buckets (S/M/L/XL).",
            "error_types_or_failure_modes": "N/A (benchmark specification) — the benchmark is designed to expose length sensitivity, digit misalignment, representation-specific failures (fractions/scientific), and digit-understanding failures.",
            "evidence_for_mechanism": "N/A",
            "counterexamples_or_challenges": "N/A",
            "uuid": "e8185.0",
            "source_info": {
                "paper_title": "Number Cookbook: Number Understanding of Language Models and How to Improve It",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (and GPT-4o-mini as reported together)",
            "brief_description": "State-of-the-art commercial autoregressive models evaluated zero-shot on the NUPA Test; strong on small/in-domain integer tasks but show sharp degradation with length and with non-integer representations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o / GPT-4o-mini",
            "model_description": "OpenAI GPT-4o family (commercial), autoregressive transformer; exact pretraining details not provided in paper (evaluated via OpenAI API: gpt-4o-2024-08-06 and gpt-4o-mini-2024-07-18).",
            "arithmetic_task_type": "Zero-shot evaluation across NUPA tasks: integer/float addition, max/min, digit tasks (get_digit, length, digit_add), fraction/scientific conversions, modulus, truediv, etc.",
            "mechanism_or_representation": "Empirical evidence suggests digit-by-digit or token-level processing with sensitivity to tokenization/alignment and to global length cues; no internal neuron-level mechanism probed in this work.",
            "probing_or_intervention_method": "Zero-shot prompts with strict format enforcement (regex); few-shot experiments also reported in appendices; no internal probes or activation interventions applied to GPT-4o itself.",
            "performance_metrics": "Representative results reported in paper: integer addition exact-match (S≈1.00, M≈0.80, L≈0.40, XL≈0.15); float addition exact-match (S≈0.90, M≈0.60, L≈0.15, XL&lt;0.05); get_digit in XL range ≈0.20 (vs random baseline 0.10); poor (&lt;20%) on fractions and scientific notation even in S-range.",
            "error_types_or_failure_modes": "Length sensitivity (performance drops rapidly with increasing digit length); digit-understanding failures (cannot reliably return i-th digit for long numbers); representation brittleness (fractions & scientific notation fail more than integers/floats); wrong output length (high dlength) or missing digits; arithmetic mistakes even on apparently simple operations.",
            "evidence_for_mechanism": "Analyses of digit-match and dlength indicate failures stem from both per-digit prediction errors and incorrect output length; comparison across tasks suggests models rely on shortcuts like sequence-length heuristics and token alignment rather than robust digit-level algorithms.",
            "counterexamples_or_challenges": "Performs near-perfect on small integer tasks (S-range), demonstrating capacity for arithmetic in constrained regimes; but these successes do not generalize to slightly longer inputs or less common representations (fractions/scientific).",
            "uuid": "e8185.1",
            "source_info": {
                "paper_title": "Number Cookbook: Number Understanding of Language Models and How to Improve It",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Qwen2",
            "name_full": "Qwen2 family (Qwen2-7B, Qwen2-72B)",
            "brief_description": "Open-source LLM family evaluated on NUPA; generally matches trend of strong small-length integer performance but sharp degradation on length and non-integer representations, similar to GPT-4o.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen2-7B / Qwen2-72B",
            "model_description": "Qwen2 family of autoregressive transformers (Alibaba/Qwen Team); evaluated with/without 4-bit quantization; specific pretraining not detailed in paper.",
            "arithmetic_task_type": "Same NUPA suite as other evaluated LLMs (add/sub/mul/div, digit-tasks, conversion, comparisons) in zero-shot and few-shot settings.",
            "mechanism_or_representation": "Empirical behavior consistent with digit-token alignment processing and length heuristics; no internal representational probe performed.",
            "probing_or_intervention_method": "Zero-shot prompts; few-shot reported; no activation probing.",
            "performance_metrics": "Qualitative summary: strong on easy integer tasks (S/M ranges); large drop for longer inputs and for fractions/scientific notation (average accuracy below 20% even in short S-range for many such tasks). Table 3 shows Qwen2-72B example scores: some tasks high (e.g., digit-related/maybe copy tasks) but many arithmetic tasks low (see paper figures/tables).",
            "error_types_or_failure_modes": "Similar to GPT-4o: length sensitivity, digit-level errors, failures on fractions/scientific, and modulus/significant-figure tasks.",
            "evidence_for_mechanism": "Observed patterns across tasks and metrics (digit-match/dlength) point to tokenization and alignment causes rather than a single arithmetic algorithm learned.",
            "counterexamples_or_challenges": "Large Qwen2 models and small Qwen2 models show similar behavior once scale reaches a threshold, suggesting scaling alone doesn't fix representation-specific failures.",
            "uuid": "e8185.2",
            "source_info": {
                "paper_title": "Number Cookbook: Number Understanding of Language Models and How to Improve It",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Llama-3.1-8B",
            "name_full": "Llama-3.1-8B (baseline and finetuned)",
            "brief_description": "Meta LLaMA-3.1 8B evaluated both as base model and after targeted finetuning (LoRA) on NUPA tasks; naive finetuning substantially improves many NUPA tasks but not digit-specific tasks; modifications to tokenizer/PE during post-training harmed performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B (and Llama-3.1-8B-ft (LoRA))",
            "model_description": "Decoder-only transformer (LLaMA-3.1 family). Baseline 8B parameters; finetuning performed with LoRA (rank=128, α=32) on a mixed NUPA training set (~50M samples; 800 LoRA steps).",
            "arithmetic_task_type": "Trained/fine-tuned on the full NUPA suite (mixed tasks); evaluated on additions, comparisons, truediv, float/fraction/scientific tasks, digit tasks.",
            "mechanism_or_representation": "Finetuning supplies missing numerical diversity and task-specific procedural knowledge; still limited on digit-identification tasks, implying some capabilities are not recovered by simple SFT/LoRA.",
            "probing_or_intervention_method": "LoRA finetuning on mixed NUPA tasks; additional intervention attempts included replacing PE or tokenizer or changing number formats post-training (these disrupted behavior). Also finetuned with RF-CoT in separate experiments.",
            "performance_metrics": "Naive LoRA finetuning: large improvements after ~800 steps (~50M samples). Example from paper (Table 3): Llama-8B-ft Add-Float ≈ 0.88±0.02 exact-match (compared with GPT-4o ≈0.78 and RF-CoT 1.00). On several tasks (max, max-hard, add-float, truediv) Llama-8B-ft matched or surpassed GPT-4o. Digit tasks remained largely unimproved.",
            "error_types_or_failure_modes": "Finetuning improves many arithmetic tasks but fails to remediate digit-understanding failure modes (e.g., get_digit long-range) and representation brittleness (fractions/scientific still challenging relative to integers). Alterations to a pretrained model's tokenizer or positional encoding during finetuning produced large performance drops.",
            "evidence_for_mechanism": "Empirical: targeted SFT provides missing task diversity and substantially improves results, showing that some NUPA gaps are due to data rather than pure capacity limits. Conversely, changing low-level model components (PEs/tokenizer) post-hoc causes catastrophic degradation, implying these components are entangled with learned behaviors.",
            "counterexamples_or_challenges": "Although LoRA finetuning produced large gains on many tasks, it did not solve digit-specific tasks or fully close gaps on fractions/scientific notation; also, tricks that help small-from-scratch models (PE/tokenizer changes) do not transfer when applied to an already pretrained large model.",
            "uuid": "e8185.3",
            "source_info": {
                "paper_title": "Number Cookbook: Number Understanding of Language Models and How to Improve It",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Small-from-scratch models",
            "name_full": "Small transformer models trained from scratch (0.1B, 0.9B, 3B) for controlled experiments",
            "brief_description": "Models trained from scratch on single NUPA tasks to probe the effects of tokenization, positional encodings, and data-format interventions under controlled conditions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "0.1B / 0.9B / 3B decoder-only transformers (LLaMA-3.1-like)",
            "model_description": "Models with 100M, 0.9B and 3B parameters (architectural details in Appendix Table 11), trained from scratch on single tasks (e.g., integer addition 1-8 digits) to measure in-domain and OOD length generalization.",
            "arithmetic_task_type": "Single-task training: integer addition, float addition, integer multiplication, fraction multiplication, scientific addition in controlled length ranges; then evaluated on longer lengths (OOD).",
            "mechanism_or_representation": "Findings emphasize token-level processing and digit alignment: the unit of tokenization (one-digit vs k-digit) governs how digits are represented and aligned for autoregressive prediction and consequently affects algorithmic generalization.",
            "probing_or_intervention_method": "Training-from-scratch on controlled datasets with varying tokenizers (1-digit, 2-digit, 3-digit, random tokenizers), positional encodings (RoPE/Alibi/NoPE), and data-format manipulations (reverse formatting, zero-padding, index hints), then measuring exact-match, digit-match and dlength.",
            "performance_metrics": "One-digit tokenizer gave best in-domain and OOD performance for sub-billion models; 3-digit tokenizer performed poorly for small models. Scaling to 3B improved ability to learn larger tokenizers but did not make large-tokenizer superior to one-digit. Random tokenizers improved OOD length generalization compared to fixed multi-digit tokenizers but still trailed one-digit.",
            "error_types_or_failure_modes": "Multi-digit tokenizers (k≥2) caused worse digit alignment and length generalization for small models; models learned length-dependent shortcuts depending on PE leading to catastrophic drops beyond training-length boundary.",
            "evidence_for_mechanism": "Controlled experiments (Figures 4,9–12) show clear performance gaps by tokenizer type and PE; digit-match and dlength metrics corroborate that one-digit tokenization facilitates per-digit learning and correct output length.",
            "counterexamples_or_challenges": "Scaling reduces but does not eliminate the penalty of larger tokenizers; random tokenizers help but do not outperform 1-digit tokenizers in these controlled settings.",
            "uuid": "e8185.4",
            "source_info": {
                "paper_title": "Number Cookbook: Number Understanding of Language Models and How to Improve It",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Tokenizers",
            "name_full": "Number tokenization schemes (one-digit, k-digit greedy, random)",
            "brief_description": "Different tokenization strategies substantially affect LLM arithmetic: one-digit tokenizers facilitate digit alignment and length generalization for small/medium models, while k-digit tokenizers (k=2,3) harm sub-billion models; random tokenizers provide regularization improving OOD generalization but not surpassing one-digit.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "various trained small models / evaluated LLMs",
            "model_description": "Tokenizers studied include: one-digit (digit as token), aligned k-digit tokenizers (k=2,3 with greedy left-to-right segmentation), and random tokenizers (variable-length split between 1 and max). Modern LLMs often use k=3 numeric vocabularies.",
            "arithmetic_task_type": "Impact measured across integer addition, float addition, integer multiplication, and length-generalization experiments.",
            "mechanism_or_representation": "Tokenization determines atomic units for autoregressive prediction; poor tokenization breaks digit alignment and makes per-digit computation and carry handling difficult.",
            "probing_or_intervention_method": "Experimental comparison: trained models with different tokenizers on identical data and compared in-domain and OOD performance (exact-match, digit-match, dlength).",
            "performance_metrics": "One-digit tokenizers showed best overall in-domain and OOD performance for sub-billion models; 3-digit tokenizers performed worst for small models. 3B model narrowed gap (2-digit ≈ 1-digit) but 3-digit still inferior. Random tokenizer improved OOD performance over fixed multi-digit tokenizers.",
            "error_types_or_failure_modes": "Multi-digit tokenizers induce misalignment and more missing digits (increased dlength) and worse per-digit accuracy; token vocabulary growth imposes higher model capacity requirements to learn arithmetic.",
            "evidence_for_mechanism": "Training curves (Figures 4,9–12) show faster/higher convergence for one-digit tokenizers and higher digit-match; theoretical observations: number tokens have different distributional properties than text and combinatorial structure (concatenation forms new valid tokens), which makes large numeric token vocabularies less helpful.",
            "counterexamples_or_challenges": "Large models (≥3B) reduce but do not eliminate disadvantages of larger tokenizers; real-world deployed large LLMs nonetheless often use 3-digit tokenizers, suggesting trade-offs with text modeling.",
            "uuid": "e8185.5",
            "source_info": {
                "paper_title": "Number Cookbook: Number Understanding of Language Models and How to Improve It",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Positional Encodings",
            "name_full": "RoPE vs Alibi vs NoPE (positional encodings)",
            "brief_description": "Different positional encodings change length generalization: RoPE encourages learning a length-dependent shortcut (strong in-domain but catastrophic boundary drops), while Alibi and NoPE act as regularizers that improve length generalization on numerical tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "100M / small models (controlled experiments)",
            "model_description": "PE variants tested: RoPE (rotary position embedding), Alibi (linear attention bias), NoPE (no positional encoding relying on causal mask). Models trained on 1–8 digit ranges then evaluated to 1–20.",
            "arithmetic_task_type": "Integer addition, float addition, fraction multiplication, scientific addition (controlled training on short lengths, OOD test to longer lengths).",
            "mechanism_or_representation": "RoPE provides explicit position signals enabling models to shortcut by encoding absolute length/position cues; Alibi/NoPE suppress strong absolute position signals, encouraging stepwise digit processing.",
            "probing_or_intervention_method": "Train/test experiments comparing exact-match/digit-match/dlength for same models with different PE choices; boundary tests (D8→D9) to expose length-overfitting.",
            "performance_metrics": "Alibi and NoPE demonstrated superior length generalization compared to RoPE across tasks. RoPE produced near-100% in-domain at training boundary but collapsed just beyond (example: int-add exact-match drops from ~100% at 8 digits to ~0% at 9); RoPE sometimes better on tiny models/in-domain.",
            "error_types_or_failure_modes": "RoPE-induced length overfitting: models rely on training-range length heuristics producing catastrophic failures just beyond training lengths (large dlength and low digit-match).",
            "evidence_for_mechanism": "Empirical boundary experiments (Table 12) showing dramatic drops when stepping out of training-length with RoPE; small-model experiments (Table 13) where RoPE helps in-domain when capacity is low but hurts generalization.",
            "counterexamples_or_challenges": "RoPE can be beneficial when model capacity/data are limited (it helps in-domain learning), indicating the inductive bias trade-off between fast in-domain learning and robust length generalization.",
            "uuid": "e8185.6",
            "source_info": {
                "paper_title": "Number Cookbook: Number Understanding of Language Models and How to Improve It",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Data-format interventions",
            "name_full": "Reverse formatting, zero padding, index hints",
            "brief_description": "Input-output formatting manipulations that improve digit alignment: reverse formatting and zero padding both help integer and float addition and length generalization; index hints were ineffective in the authors' finetuning settings.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "0.1B / small models (controlled experiments)",
            "model_description": "Data formatting techniques applied to training data: reverse digit order (least-significant first), left zero-padding inputs to equal length, and explicit per-digit index tokens (index hints).",
            "arithmetic_task_type": "Primarily integer and float addition (also extended to other tasks in NUPA), evaluated for length-generalization.",
            "mechanism_or_representation": "Formatting aligns digits to the model's autoregressive generation order and standardizes operand lengths, reducing the need for long-lookahead carry reasoning and improving digit pairing.",
            "probing_or_intervention_method": "A/B experiments comparing vanilla formatting vs reverse / zero-padding / combinations; measured exact-match and length generalization (Table 16 and figures).",
            "performance_metrics": "Reverse formatting and zero-padding each outperform vanilla formats for integer and float addition; combination gives similar benefits (functionality overlaps). Index hints did not improve performance on the tested models and digit ranges.",
            "error_types_or_failure_modes": "Reverse formatting's theoretical advantage for carry-handling is limited in practice because worst-case carry dependencies are rare; index hints may help only in regimes (very small models, long training ranges) not studied here.",
            "evidence_for_mechanism": "Empirical gains in Table 16 and Figures 14–18 show higher exact-match on addition tasks with reverse/pad formats; theoretical argument about carry cases explains limited additional benefit.",
            "counterexamples_or_challenges": "Index hints produced contradictory results compared to prior work (likely due to model size and training-range differences); reverse formatting/padding help but do not solve digit understanding or representation diversity issues (fractions/scientific).",
            "uuid": "e8185.7",
            "source_info": {
                "paper_title": "Number Cookbook: Number Understanding of Language Models and How to Improve It",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "RF-CoT",
            "name_full": "Rule-Following Chain-of-Thought (RF-CoT / RF- CoT / Rule-Following CoT)",
            "brief_description": "A chain-of-thought variant where models are finetuned to follow explicit code/pseudocode procedures (rules) for arithmetic; it substantially improves correctness on many tasks but requires large context windows and is much slower at inference.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B (finetuned with RF-CoT)",
            "model_description": "LLaMA-3.1-8B finetuned to generate rule-following intermediate steps (python-like pseudo-code) before the final answer; context window set at 2000 tokens during experiments.",
            "arithmetic_task_type": "Selected NUPA tasks that can be decomposed into recurrences/basic unit ops (e.g., fraction addition, multiplication, multi-digit addition).",
            "mechanism_or_representation": "Makes the solving algorithm explicit in the generated trace so the model follows a deterministic multi-step procedural algorithm (digit-by-digit or chunked addition) rather than attempting to produce answer in one token sequence.",
            "probing_or_intervention_method": "Finetuning on RF-CoT data (prompt + expected stepwise response), then inference where prompt includes rule and model generates steps + answer. Measured exact-match and also maximal digit-length affordance within 2k context.",
            "performance_metrics": "RF-CoT often achieved near-perfect exact-match on tasks within the context limit (examples in Table 3: RF-CoT Add-Float 1.00±.00; other tasks often high). Llama-8B-ft (non-RF-CoT) had Add-Float ≈0.88±.02; GPT-4o ≈0.78 (per Table 3).",
            "error_types_or_failure_modes": "Severe context-window limitation: many tasks exceed 2000-token limit for realistic digit lengths (fraction-add limited to ~3-digit within 2k). Inference is much slower: RF-CoT ≈5.625s/sample vs direct answer ≈0.336–0.371s/sample (~17x slower).",
            "evidence_for_mechanism": "Empirical gains when RF-CoT is used and sufficient context is available; but dramatic inference-time and context-size cost quantified in Tables 2 and 3 and Appendix A.5.",
            "counterexamples_or_challenges": "Scalability: RF-CoT cannot handle large digit lengths within practical context windows and is computationally expensive, making it impractical as a general solution for routine NUPA operations.",
            "uuid": "e8185.8",
            "source_info": {
                "paper_title": "Number Cookbook: Number Understanding of Language Models and How to Improve It",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Finetuning + structural changes",
            "name_full": "Post-training interventions on pretrained LLMs (changing PE, tokenizer, representation during/after finetuning)",
            "brief_description": "Attempts to adapt pretrained LLMs by changing low-level components (positional encoding, tokenizer, number formats) during finetuning produced detrimental effects, suggesting these components are entangled with learned behavior and not easily replaced post-hoc.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B (experiments reported)",
            "model_description": "Pretrained LLaMA-3.1-8B checkpoint modified by altering PE (e.g., RoPE→NoPE/Alibi), replacing tokenizer (e.g., to one-digit), or changing number formats, then finetuned or evaluated; LoRA finetuning without structural changes used as baseline.",
            "arithmetic_task_type": "Mixed NUPA tasks as in the benchmark.",
            "mechanism_or_representation": "Findings suggest pretrained models encode entangled dependencies between their positional/tokenization schemes and numeracy behaviors; arbitrarily swapping these at finetuning disrupts learned strategies.",
            "probing_or_intervention_method": "Empirical interventions: finetune with new PE/tokenizer/representation (Table 18), compare to naive LoRA finetuning and original baseline.",
            "performance_metrics": "None of the structural modification attempts outperformed naive finetuning or the original model; many modifications caused substantial performance drops (see Table 18 in Appendix A.4.5).",
            "error_types_or_failure_modes": "Catastrophic degradation after changing low-level model components post-pretraining; suggests interventions that help from-scratch training may break behavior of pretrained models.",
            "evidence_for_mechanism": "Empirical negative results: direct attempts to change PE/tokenizer in a pretrained checkpoint yielded worse NUPA performance compared to baseline or simple LoRA finetuning.",
            "counterexamples_or_challenges": "These negative results do not preclude alternative integration strategies (e.g., re-pretraining or more careful adapter techniques), but show the straightforward approach of swapping components during finetuning is harmful.",
            "uuid": "e8185.9",
            "source_info": {
                "paper_title": "Number Cookbook: Number Understanding of Language Models and How to Improve It",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Positional description matters for transformers arithmetic",
            "rating": 2,
            "sanitized_title": "positional_description_matters_for_transformers_arithmetic"
        },
        {
            "paper_title": "Tokenization counts: the impact of tokenization on arithmetic in frontier llms",
            "rating": 2,
            "sanitized_title": "tokenization_counts_the_impact_of_tokenization_on_arithmetic_in_frontier_llms"
        },
        {
            "paper_title": "Train short, test long: Attention with linear biases enables input length extrapolation",
            "rating": 2,
            "sanitized_title": "train_short_test_long_attention_with_linear_biases_enables_input_length_extrapolation"
        },
        {
            "paper_title": "Teaching arithmetic to small transformers",
            "rating": 2,
            "sanitized_title": "teaching_arithmetic_to_small_transformers"
        },
        {
            "paper_title": "Transformers can achieve length generalization but not robustly",
            "rating": 2,
            "sanitized_title": "transformers_can_achieve_length_generalization_but_not_robustly"
        },
        {
            "paper_title": "Injecting numerical reasoning skills into language models",
            "rating": 1,
            "sanitized_title": "injecting_numerical_reasoning_skills_into_language_models"
        },
        {
            "paper_title": "Subword regularization: Improving neural network translation models with multiple sub-word candidates",
            "rating": 1,
            "sanitized_title": "subword_regularization_improving_neural_network_translation_models_with_multiple_subword_candidates"
        }
    ],
    "cost": 0.0247475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>NUMBER COOKBOOK: NUMBER UNDERSTANDING OF LANGUAGE MODELS AND HOW TO IMPROVE IT
5 Mar 2025</p>
<p>Haotong Yang haotongyang@pku.edu.cn 
Yi Hu 
Shijia Kang kangshijia@stu.pku.edu.cn 
Zhouchen Lin zlin@pku.edu.cn 
Muhan Zhang </p>
<p>School of Intelligence Science and Technology
Peking University</p>
<p>Institution for Artificial Intelligence
Peking University</p>
<p>State Key Lab of General Artificial Intelligence
Peking University</p>
<p>Pazhou Laboratory (Huangpu)
Guangzhou, GuangdongChina</p>
<p>NUMBER COOKBOOK: NUMBER UNDERSTANDING OF LANGUAGE MODELS AND HOW TO IMPROVE IT
5 Mar 20257A2B489FD857588DD5C334997477493CarXiv:2411.03766v3[cs.CL]
Large language models (LLMs) can solve an increasing number of complex reasoning tasks while making surprising mistakes in basic numerical understanding and processing (such as 9.11 &gt; 9.9).The latter ability is essential for tackling complex arithmetic and mathematical problems and serves as a foundation for most reasoning tasks, but previous work paid little attention to it or only discussed several restricted tasks (like integer addition).In this paper, we comprehensively investigate the numerical understanding and processing ability (NUPA) of LLMs.Firstly, we introduce a benchmark covering four common numerical representations and 17 distinct numerical tasks in four major categories, resulting in 41 meaningful combinations in total.These tasks are derived from primary and secondary education curricula, encompassing nearly all everyday numerical understanding and processing scenarios, and the rules of these tasks are very simple and clear.Through the benchmark, we find that current LLMs fail frequently in many of the tasks.To study the problem, we train small models with existing and potential techniques for enhancing NUPA (such as tokenizers, positional encoding, and number formats), comprehensively evaluating their effectiveness using our testbed.We also finetune practical-scale LLMs on our proposed NUPA tasks and find that 1) naive finetuning can significantly improve NUPA on many but not all tasks, and 2) surprisingly, techniques designed to enhance NUPA prove ineffective for finetuning pretrained models.We further explore the impact of chain-of-thought techniques on NUPA.Our work provides a more detailed and comprehensive understanding of NUPA in LLMs.Our benchmark and codes are released at https://github.com/GraphPKU/number_cookbook.</p>
<p>INTRODUCTION</p>
<p>The mathematical and reasoning abilities of large language models (LLMs) are currently quite impressive (OpenAI, 2023;Meta, 2024a;OpenAI, 2024a;Yang et al., 2024a), capable of solving problems at the level of a high-school student or even more difficult ones like GAOKAO (a nationwide examination of high school students applying to universities in China) (Zhang et al., 2024b), Olympiad-level problems (He et al., 2024), and college mathematics (Tang et al., 2024).However, upon closer examination of the models' outputs, we find that although the models demonstrate remarkable proficiency in problem-solving approaches, they often struggle with basic numerical understanding and processing -like a careless student who claims, "I know how to do it, but I didn't get it right."Some of these errors are quite surprising, such as believing that 9.11 &gt; 9.9 or making mistakes in simple addition 8/7 + 3/5.These errors are a major cause of hallucinations when dealing with math, reasoning, and data analysis tasks, as the model presents seemingly correct problem-solving approaches, but ultimately produces incorrect results (Huang et al., 2024;Li et al., 2024b;Jiang et al., 2024).Therefore, investigating and improving the fundamental "numerical understanding and processing abilities" (NUPA) of models is crucial.</p>
<p>Table 1: Task overview of NUPA Test.The four rows represent four numerical representations, and the 17 columns correspond to different tasks.✓: 41 tasks included in our test.✗: Not included, too complex.⃝: Not directly included but can be easily adapted from an included task.−: Not applicable.The detailed explanation for these non-included tasks is provided in Appendix A.1.5
✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ − ✓ ✓ Float ✓ ✓ ✓ ✗ − − ✓ ✓ ✓ ✓ ✓ ✓ ✓ ⃝ − ✓ ✓ Fraction ✓ ✓ ✓ ✓ − − ✓ ✓ − − − − − ⃝ ✓ ⃝ ⃝ Scientific ✓ ✓ ✓ ✗ − − ✓ ✓ − − − − − ⃝ ✓ − ⃝
However, in current research, reasoning ability and NUPA are often tested together, both on classic datasets such as GSM8k (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), MMLU (Hendrycks et al., 2021a), and in more challenging tests mentioned above.For example, a problem in GSM8k is: "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May.How many clips did Natalia sell altogether in April and May?" Solving this problem requires two aspects: on the one hand, mathematical reasoning including understanding the text, extracting relevant information, formulating mathematical equations (or finding other solution methods), solving the equations or executing an algorithm, and obtaining the result; on the other hand, it also requires understanding and processing the numbers provided in the problem or produced as intermediate results at each step, like 48/2 = 24 and 48 + 24 = 72.While these two abilities are both essential to correctly solving the problems, tests on such datasets do not distinguish between them.</p>
<p>A more severe issue is that the numerical content is often deliberately simplified in these datasets.</p>
<p>In various exam questions (like in the American Invitational Mathematics Examination (Li et al., 2024a)), to focus on assessing students' understanding of mathematical concepts -such as how to set up the correct equations and apply the right theorems -the numbers in both the questions and answers are often specially chosen to be integers.However, this is not the case in real-world scenarios (Chen et al., 2021).</p>
<p>Despite the importance of NUPA, there is still a lack of accurate, detailed, and comprehensive formalization, measurement, and analysis of this fundamental capability.In this paper, we take the preliminary step towards formalizing the NUPA of LLMs.We categorize the numerical concepts and operations from primary and secondary education into four representations: integers, floatingpoint numbers (finite decimals), fractions, and scientific notation, along with four ability categories comprising 17 tasks.Pairing these representations results in 41 meaningful tasks, forming our NUPA benchmark (Table 1).These representations and tasks cover the most common scenarios involving number understanding and processing, which are typically not challenging for humans, as we read, use, or process such numbers nearly every day.</p>
<p>On this benchmark, we rigorously test several state-of-the-art LLMs containing GPT-4o (OpenAI, 2024a), Llama-3.1 (Meta, 2024a) and Qwen2 (Qwen Team, 2024).We ask the models to directly output the answers without calling external tools.Although the latest LLMs perform well on some of the easiest tasks, their performance declines significantly as tasks become slightly more complex (such as multiplication, modulus operations, or digit-based calculations), or as the representation of numbers extends beyond basic integers.See Figure 2 of Section 2.4.The overall unsatisfactory performance highlights a pronounced mismatch between the claimed strong mathematical reasoning abilities and the poor practical, everyday numerical understanding and processing abilities of today's LLMs.</p>
<p>aforementioned techniques (PEs, data formats and tokenizers) at this stage may have adverse effects.</p>
<p>We test various settings and finetuning configurations, but none are able to achieve performance equal to or better than the original model.Our results suggest that these modifications can significantly disrupt the models' established behavior or conflict with its pre-existing knowledge, leading to a decrease in performance.</p>
<p>Finally, we discuss the potential of using chain-of-thought (CoT) techniques (Wei et al., 2022) for numerical processing.Although CoT methods can break down complex problems into simpler subtasks and significantly increase the likelihood of obtaining correct answers, their drawbacks -such as consuming a large context window and requiring extended processing time -become particularly apparent in numerical tasks.We test a general CoT method known as RFFT (Hu et al., 2024), and find that for more complex tasks (such as multiplication and fraction addition), chain-of-thought methods face scalability challenges, making them difficult to be applied in practical scenarios.It is noteworthy that in this paper, we do not discuss tool use methods (Schick et al., 2023;Lu et al., 2023a) for NUPA as 1) we want to study the self-contained NUPA of LLMs, 2) calling external tools whenever encountering numbers increases the inference latency (Xu et al., 2024), and 3) we believe NUPA without tools is a necessary ability of AGI.</p>
<p>In summary, we propose a more comprehensive benchmark on the basic numerical understanding and processing abilities (NUPA) of LLMs, evaluate several SOTA LLMs' performance on it, and further study three categories of approaches to improve NUPA: pretraining, finetuning and CoT.Our results reveal that the current research is insufficient to fully address the NUPA problem, despite it being a fundamental capability for solving many more complex tasks.We hope that by introducing a systematic classification and more comprehensive evaluation of NUPA, we can bring greater attention from the community to this important but overlooked fundamental capability.</p>
<p>NUPA TEST: A BENCHMARK FOR NUMBER UNDERSTANDING AND PROCESSING ABILITY</p>
<p>In this section, we will introduce our NUPA benchmark from the following four aspects: number representations, tasks, metrics, and result analysis of current LLMs.We will explain the rationale behind the inclusion (or exclusion) of specific representations and tasks in our benchmark, highlighting their distinctive features.</p>
<p>NUMBER REPRESENTATION</p>
<p>As discussed above, we believe that the educational curricula on the (Chinese) primary and secondary school levels serve as a valuable reference for determining the essential NUPAs that LLMs should master.We identify four number formats in these curricula that are both common and sufficient to cover most practical scenarios.</p>
<p>• Integer: The most common number and the foundation of other number representations.</p>
<p>• Floating-Point Number (Float): Floats, or finite decimals, are a useful subset of fractions.</p>
<p>Calculations with floats like addition and comparison, work similarly to integers, making them common in daily life.• Fraction: We consider fractions with integer numerators and denominators.In practical situations involving distribution, fractions become unavoidable, especially when the inaccuracy introduced by converting fractions to floats is unacceptable.• Scientific Notation: Scientific notation is characterized by separating a number's precise value from its order of magnitude.It is widely used in fields like physics, economics, and computer science because it efficiently handles a wide range of numbers and clearly conveys significant figures and precision.For LLMs, mastering scientific notation can significantly enhance their ability to handle practical tasks, such as interpreting financial reports or reading scientific texts.</p>
<p>Details of these four representations in our benchmark can be found in Appendix A.1.1.There are possible representations of numbers that are not included in these four formats, like complex numbers, infinite decimal representation (repeating and non-repeating), radical expression (like √ 2), ...These representations either occur infrequently in practical conversations (e.g., complex numbers) or present significant challenges for language models to process without the aid of external tools (e.g., radicals).For these reasons, we have opted not to include them in our benchmark at this stage.</p>
<p>Published as a conference paper at ICLR 2025</p>
<p>TASKS IN FOUR ABILITY CATEGORIES</p>
<p>Another aspect of NUPA is defining the tasks that models need to handle.The tasks should have clear calculation rules.Furthermore, most practical numerical processing tasks should either fall within these tasks or can be easily transformed into some of them.Extracted from the primary and secondary education curricula, we propose 17 tasks in four ability categories and students who have completed the stage of education are expected to solve them.The complete task list is shown in Table 1 and we provide a more detailed discussion in Appendix A.1.2 and an example for each task in Appendix A.1.3.Below we discuss the rationales for including some tasks in detail.</p>
<p>• Elementary arithmetic: addition, subtraction, multiplication, and division.The most fundamental mathematical operations.For division, we consider three types of related operators: True division, floor division and modulus.</p>
<p>• Comparison: max and min.Understanding numbers on the concept of "order".</p>
<p>• Digit understanding: When we care about a language model's understanding, processing (and generation) of numbers, digit is a crucial concept, as numbers are not read and processed by the language model as a whole, but rather as a sequence of digits.We specially designed some digit-related tasks to test whether LLMs truly handle digits, including:</p>
<p>-Get digit: Given a number and an integer i, return the i-th digit.This task is important when certain digits have special meanings in a number (such as a phone number or SSN).-Length: Return the total length (i.e., the number of digits) of a number.</p>
<p>-Count: Count the times that a particular digit occurs in an integer.</p>
<p>-Digit compare: Compare and return the larger (smaller) digits one by one.</p>
<p>-Digit add: Perform the normal addition digit by digit but ignore any carrying.For example, digit_add(12345, 34567) = 46802.It can test a model's understanding of digit alignment and its mastery of single-digit addition.</p>
<p>• Conversion between representations: Converting a number to two representations: to float and to scientific notation, as they are frequently used to present final results.These two tasks test whether models can understand the relationship between various numerical formats.In particular, since many tasks present answers as approximate values, we designed a "significant digit" (sig.fig.</p>
<p>) task to evaluate a model's ability to round long numbers to fixed-length significant digits.</p>
<p>The combination of representations and tasks ultimately results in a total of 41 meaningful pairs.Without confusion, we refer to each combination as a task.The tasks receive either one or two numbers as inputs and return a number as result, and the input numbers and results share the same representation for most tasks unless otherwise stated (refer to Appendix A.1.4).The remaining combinations are excluded due to being excessively complex, uncommon, inapplicable, or redundant with other tasks.For further details, see the discussion in Appendix A.1.5.</p>
<p>The difficulty of each task depends not only on the nature of the task itself but also on the length of the numbers to be processed -longer tasks involve longer inputs and outputs as well as more steps of internal operations.Therefore, we test on different problem lengths.For tasks that are inherently more difficult, we limit the size of the problem to 1-20 digits, and for easier tasks to 1-100 digits.</p>
<p>(For which tasks are considered difficult or easy, please refer to the Appendix A.1.6.)</p>
<p>We generated 1,000 questions for each task and each length.Unlike some previous works that set the lengths of two numbers to be the same, in our tests, the length L of a question is determined by the longer of the two numbers, while the length of the shorter number follows a uniform distribution between L/2 and L. We implemented additional handling to ensure that generated problems do not result in overly simple, complex, or meaningless results.Some tasks are further split into a hard and an easy version.More details about generating the benchmark are provided in Appendix A.1.7.Measuring the performance of NUPA benchmarks on these tasks is not trivial."Exact match" accuracy is the golden standard of the performance where the answer is considered as correct when it exactly matches the groundtruth.However, a smoother and more detailed metric is useful to understand the behavior and capabilities of a model.Therefore, we also report    the "digit match" and "dlength" (difference of length) metrics, as metrics of digit accuracy and length accuracy respectively.We first split numbers into parts (e.g., integer and decimal parts of a float, numerator and denominator of a fraction) and align the generated answer with the groundtruth digit by digit.Integer parts are aligned from the least significant digit; and the decimal parts of float are aligned from the most significant digit.For "digit match", we measure the correctness of each digit, with missing digits considered as errors, and report the overall accuracy.For "dlength", we report the sum of absolute difference in length between each part of the prediction and the groundtruth.Figure 1 illustrates these three metrics.</p>
<p>METRICS
S M L X L S M L X L S M L X L S M L X L S M L X L S M L X L S M L X L S M L X L S M L X L S M L X L Digit Max FloatGPT-4o GPT-4o-mini Llama-3.1-8B-ft Llama-3.1-8B Llama-3.1-70B Mixtral-8x7B Qwen2-72B Qwen2-7B Llama-2-7b-hf S M L X L S M L X L S M L X L S M L X L S M L X L S M L X L S M L X L S M L X L S M L X L S M L X LGPT-4o GPT-4o-mini Llama-3.1-8B-ft Llama-3.1-8B Llama-3.1-70B Mixtral-8x7B Qwen2-72B Qwen2-7B Llama-2-7b-hf S M L X L S M L X L S M L X L S M L X L S M L X L S M L X L S M L X L S M L X L S M L X L S M L X LS M L X L S M L X L S M L X L S M L X L S M L X L S M L X L S M L X L S M L X L S M L X L S M L X L
For each task, we divide the digits into four intervals (S, M, L, XL).For tasks with lengths 1-20, the four intervals correspond to 1-4 5-8, 9-14, 15-20 digits respectively.For tasks with lengths 1-100, they correspond to 1-10, 11-20, 21-60, 61-100 digits respectively.We average the results in each interval for each task and metric.More details of our metrics are given in Appendix A.1.8</p>
<p>PERFORMANCE OF CURRENT LLMS</p>
<p>We test some commonly used LLMs on our benchmark, including three Llama models: Llama-2-7b, Llama-3.1-8b and Llama-3.1-70b(Meta, 2024a), one of the most popular open-source model families from Meta; Mixtral-8×7B (MistralAI, 2024), a strong MoE model; and Qwen2-7B and Qwen2-72B (Qwen Team, 2024) which are also open-source models that are believed to have strong math abilities.Finally, we also test state-of-the-art commercial models GPT-4o-2024-08-06 and GPT-4o-mini-2024-07-18 (OpenAI, 2024a).We use prompts to control models to directly output result numbers without relying on external tools or CoT.The prompts used for each model and task are included in Appendix A.2.We select the results of some typical tasks in each category in Figure 2, while the complete results1 and discussion on all metrics are shown in Appendix A.3.</p>
<p>Here, we mainly focus on the zero-shot performance while we discuss few-shot performance in Appendix A.3.1.We have several observations regarding the results:</p>
<p>The best model performs well on typical tasks, but its performance declines on more specialized tasks.We find that GPT-4o, GPT-4o-mini and Qwen2 handle typical tasks, such as integer addition, float addition, integer max, and integer length, with high accuracy in the S and M ranges.This aligns with their strong performance on various mathematical datasets.However, their accuracy drops sharply when working with less common representations, like fractions and scientific notation, with average accuracy falling below 20%, even for the shortest S-range (1-4 digits).Similarly, for tasks such as significant figures, modulus operations, and digit-based calculations, their performance was unsatisfactory.This highlights the current limitations of LLMs in understanding numerical diversity and complexity.Despite their good performance on a narrow set of numerical tasks, they struggle with many others, failing to produce accurate results in these areas.</p>
<p>Length remains a significant challenge for NUPA of LLMs.We observe a noticeable decline in accuracy for even simple tasks like integer addition as the problem length increases.For instance, GPT-4o's accuracy drops from nearly 100% in the S range and 80% in the M range to around 40% in the L range and just 15% in the XL range.In the more complex task float addition, the accuracy decreases from 90% (S) and 60% (M) to merely 15% (L) and less than 5% (XL).This trend is consistent across other models and tasks.For example, Qwen2's performance in the integer-length task declines from almost 100% in the S range to 50% in the M range, and falls below 5% in the L and XL ranges.</p>
<p>Length impedes learning both individual digits and overall length.To understand why models struggle with longer input numbers, we examine digit match and dlength performance in Figure 6 and Figure 7 in Appendix A.3.These metrics reveal that length affects both the accuracy of individual digits (digit match) and the answer's overall length (dlength), with variations across tasks.For example, GPT-4o and Llama-3.1 display consistently low dlength in the add-integer task, with digit match decreasing sharply as length increases, suggesting that length primarily impacts per-digit accuracy on this task.Conversely, in the max-float task, dlength increases significantly with length (about 30-60 in the XL range), while digit match remains at 60% in the XL range.Note that since missing digits are treated as errors, this 0.6 digit match is likely due to these missing digits.This suggests that the main challenge here lies in generating answers of the correct length, rather than individual digit accuracy.In other tasks like fraction, both length and digit accuracy issues arise, as reflected in rising dlength and declining digit match."Digit" is more challenging than expected.We were surprised to find that LLMs struggle to fully grasp "digits".For instance, in the "get digit" task, where the model is asked to return the i-th digit of a long integer, performance drops significantly as the length of the number increases.This suggests that current LLMs lack a consistent ability to simply find a digit.Note that the performance is good in the shorter S-range, which indicates that the models can at least comprehend the task instruction.In the XL-range, GPT-4o achieves only 20% accuracy, barely above the random guessing 10% baseline (since the correct answer is always a digit between 0 and 9).This fundamental limitation may explain why current LLMs struggle with numerical understanding and processing, especially as task complexity and input length increase.If a model cannot reliably identify a specific digit in a given number, it casts doubt on its ability to generalize to more complex arithmetic tasks, such as addition.</p>
<p>We also have some interesting observations: (1) LLMs find the "max-hard" task easier than "max" with integer inputs.The difference between the tasks is that in the max task, the two numbers often differ in length, whereas in max-hard, they are always the same length and share some left-most digits, requiring more digits to be compared.While max-hard intuitively seems more difficult, models actually perform better on it.This is likely because they struggle to effectively use sequence length information, as reflected in their weaker performance on the "length" tasks in the longer ranges.It suggests that models might process tasks in different ways from humans.They could have to compare two numbers digit by digit.In this situation, the "harder" subtasks are actually easier because the numbers are already aligned.</p>
<p>(2) GPT-4o and GPT-4o-mini show nearly identical performance across most tasks, similar to the comparison between Qwen2-72B and Qwen2-7B.This suggests that once a model reaches a certain size, NUPA performance relies more on factors like architecture, training strategies, data diversity, and post-training refinements, rather than simply on increasing model size.</p>
<p>HOW DO TOKENIZERS, PES AND DATA FORMATS AFFECT NUPA?</p>
<p>We have observed that the NUPA Test poses significant challenges even for the most advanced LLMs.</p>
<p>In this section, we aim to investigate the factors that can influence the NUPA of LLMs during their pretraining phase, including tokenization strategies, PEs, and different data formats.We utilize the architecture of decoder-only transformers and alter the size to create models with 0.1B, 0.9B and 3B parameters.These models are trained from scratch, incorporating a wide range of techniques that could potentially impact NUPA.In this section, each model is trained on a single task .The details of the training process and models are included in Appendix A.4.1.LLMs interpret numbers as segmented tokens rather than whole numbers.With the development of language models, various tokenization strategies have emerged, including mixed tokenizers, one-digit tokenizers, and k-digit tokenizers (k ≥ 2), as shown in Figure 3.In the BPE tokenizer used by GPT-2, the numbers are not specially treated, which resulted in irregular number cutting and is harmful to digit alignment.The cutting  of numbers in modern tokenizers has become more aligned.These tokenizers greedily segment a number from left to right into k-digit tokens until a remainder shorter than k digits is left, which is then segmented into a single token.Llama-2 uses a one-digit tokenizer, but all of the latest LLMs use a tokenizer with k = 3, which comes with an extended vocabulary for numbers.Additionally, Singh &amp; Strouse (2024) discovers that just alternating the greedy direction from "left-to-right" to "right-to-left" (for integers) can improve performance of Llama-3 and GPT-4.</p>
<p>ONE-DIGIT TOKENIZERS ARE GOOD ENOUGH</p>
<p>There is a growing tendency to expand the vocabulary size as the number of parameters in LLMs rapidly increases.Recent work has shown that a larger vocabulary is more suitable for larger LLMs (Tao et al., 2025) because longer tokens can encapsulate more complex and precise meanings for text tokens.However, numbers behave differently:</p>
<p>• The long-tail phenomenon (Raunak et al., 2020), common in text tokens, is not as pronounced for the number tokens.The distribution of number tokens is closer to a uniform distribution.• Two smaller number tokens can always be combined into a valid new one (e.g., 3 and 7 form 37), which is not true for text tokens (e.g., "hello" and "hi" cannot form "hellohi").So the number of possible number tokens grows exponentially as k increases, much faster than text tokens.• The next token prediction of number tokens is harder than predicting the next text token because number prediction often involves calculation and operations, whereas word mapping tends to be more intuitive.</p>
<p>We trained 0.9B models on 1-to 8-digit length samples including integer addition, float addition, and integer multiplication, using aligned k-digit tokenizers where k = 1, 2, 3 (d in Figure 3).Figure 4 shows the in-domain performance of these models in the first three columns and their out-of-domain (OOD) performance in the last two columns, evaluated using the exact match metric.</p>
<p>From the figure, the one-digit tokenizer shows the best in-domain performance in these three tasks, while three-digit tokenizer exhibits poor performance.In out-of-domain tests, the one-digit tokenizer also exceeds the others by large margins.Tokenizers with an increasing number of digits significantly hinder sub-billion models' NUPA.We also performed experiments on models of 3 different sizes including 0.1B, 0.9B, and 3B in Appendix A.4.2 and got similar results.Even as the model size increases, the performance of 2-or 3-digit tokenizer improves but remains either similar or worse than that of the one-digit tokenizers.For these experiments, we also report the digit match and dlength results in Appendix A.4.2 Figure 10 and 11, where one-digit tokenizer performs better both on digit learning (larger digit match) and length learning (less dlength).On the contrary, larger vocabularies significantly increase the model size requirements.In conclusion, we find no evidence to support the idea that increasing the vocabulary size improves NUPA performance.</p>
<p>Recently, Sathe et al. (2024) found that the "random tokenizer" (Kudo, 2018;Provilkov et al., 2020) which splits words like "Hello world" into variable tokens such as "He/llo/ world" or "Hell/o/ world" enhances reasoning by introducing variability in generation path.We also test it in number domain and find the random tokenizers consistently outperform their standard counterparts in length generalization, but still fall short of the performance achieved by the one-digit tokenizer.See the details in Appendix A.4.2.</p>
<p>Published as a conference paper at ICLR 2025</p>
<p>SPECIALIZED PES ACT AS LENGTH REGULARIZERS</p>
<p>Previous work has suggested that PE could be the key factor (Zhou et al., 2024b) of length generalization.To further investigate whether the influence is specific on a certain task, we train 100M models with different PEs: RoPE (Su et al., 2024), NoPE (Kazemnejad et al., 2023) and Alibi (Press et al., 2022) on four tasks: integer addition, float addition, fraction multiplication (easy) and scientific notation addition respectively.Models are trained on 1-8 lengths (S and M range), then test them on full range (S to XL, 1-20).RoPE, widely used in Llama and its derivatives, is the most classic relative PE.Then Alibi, another relative PE, is proposed to address RoPE's length overfitting issues.NoPE (transformers without PE, relying solely on the causal mask to encode the position information) offers a surprisingly easy way to achieve length generalization.Therefore, we compare these three typical PEs to evaluate the performance on NUPA.</p>
<p>Our results, presented in Figure 13 in Appendix A.4.3, align with conclusions from previous works.Alibi and NoPE demonstrate superior length generalization across various representations and tasks, indicating that the influence of PEs is relatively consistent across these common representations, tasks within the number domain.</p>
<p>Moreover, we aim to characterize further the mechanism underlying these differences.Specifically, we found that RoPE leads the model to learn a length-related shortcut, while Alibi and NoPE act as a form of regularization by avoiding this, thereby preventing length overfitting.For more details, please refer to the appendix A.4.3.</p>
<p>DATA FORMATS HELP DIGIT ALIGNMENT</p>
<p>A series of works have proposed specific data formats including reverse formatting, zero padding and index hints.Reverse formatting (Lee et al., 2024;Shen et al., 2023) presents numbers in reverse order from the least significant digit to the most significant one to align with the models' autoregressive mechanism, simplifying the learning process for addition.Zero padding (Lee et al., 2024;Shen et al., 2023;Zhou et al., 2024b;Cho et al., 2024) adds leading zeros to numbers to standardize the lengths of operands, helping models align operands.Index Hints (Zhou et al., 2024a) explicitly incorporate positional information by prefixing each digit with its corresponding position index in both input and output sequences.</p>
<p>While previous work mainly focuses on integer addition or multiplication, we extend the techniques to various tasks in the NUPA Test of different number domains.To compare the effects of reverse formatting and zero padding, we demonstrate in Table 16 how the combination of reverse formatting and zero padding impacts length generalization.Reverse formatting, zero padding, and their combination all outperform vanilla formats in integer and float addition, while their performance is comparable to each other, suggesting that their functionality largely overlaps.Zero padding helps ensure proper alignment, while reverse formatting also plays a crucial role in maintaining alignment.The previously believed "helping calculation" function of reverse formatting is minor.As for index hint, we find it doesn't work for our models.We discuss the details of these experiment results and the reasons in Appendix A.4.4.</p>
<p>DOES FINETUNING IMPROVE NUPA PERFORMANCE OF LLMS?</p>
<p>The existing techniques aimed at enhancing NUPA have rarely been applied to practical LLMs, mostly being tested on toy models and isolated tasks.This raises the question of whether it is possible to enhance the NUPA capabilities of large models through post-training finetuning.To explore this, we generate training sets (10 5 samples for each digit and each task) and validation sets for our NUPA tasks, ensuring no overlap with the original test set.We then use them to finetune a pretrained model.Specifically, we finetune a Meta-Llama-3.1-8Bmodel with LoRA (Hu et al., 2022) (rank 128, α=32) on a mixed training set comprising all of our NUPA tasks.Remarkably, we find only 800 steps training (about 50M training samples, ≪ 1 epoch) leads to significant improvement, as shown in Figure 2 with the finetuned model labeled as "Llama-8B-ft".Though Llama-3.1-8B is not a strong baseline, this finetuned version achieves much better performance.For example, in max, max-hard, add-float and truediv tasks, this model even surpassed or matched GPT-4o, confirming our hypothesis: for many NUPA tasks, the model's base capacity may not be the main limiting factor, but rather the lack of numerical diversity and task variety in the training data.</p>
<p>However, we also found that such finetuning does not provide much improvement on certain tasks, such as understanding digits.Furthermore, when we tried to incorporate the various tricks, such as modifying the model's original PEs, tokenizers, or number formats, into an already trained model, these methods proved ineffective.When we altered the PE or adjusted the tokenization and representation of the model, the changes significantly disrupted the model's original behavior, causing a substantial performance drop.This suggests that enhancing a model's NUPA capabilities through post-training may require more revolutionary innovations beyond the current tricks.The detailed results of these attempts are presented in Table 18 in Appendix A.4.5.</p>
<p>IS COT SUITABLE AND VALID FOR NUPA?</p>
<p>CoT has been proven to be effective in enhancing the capacity of LLMs both theoretically (Feng et al., 2023;Yang et al., 2024b) and experimentally (Wei et al., 2022;OpenAI, 2024b).Thus, we are also interested in whether CoT is the ultimate solution for improving NUPA.Due to the task and representation diversity in our benchmark, it is hard to cover all issues with a single form of CoT.So we adapt a special CoT form called Rule-Following CoT (Hu et al., 2024) (RF-CoT), where LLMs are trained to follow a provided code or pseudo-code that outlines the procedure to solve the task.RF-CoT is capable of handling any problem with a solving procedure that can be broken down into recurrences and basic unit operations, making it well-suited for our benchmark tasks.The detailed introduction with an example of RF-CoT can be found in Appendix A.5.1.To evaluate the performance of this CoT method, we finetuned the LLaMA 3.1-8B model on a subset of the NUPA tasks with RF-CoT.During both training and testing, we set a context window of 2000 tokens, with any data exceeding this limit being ignored.Table 3 shows the performance on selected tasks.Accuracy and standard error for RF-CoT and finetuned Llama-3.1-8B are averaged over three runs.For GPT-4o and Qwen2, which are not finetuned, we report single-run accuracy without standard error.Within the context length limit, the rule-following finetuned LLaMA 3.1-8B significantly outperformed GPT-4o and Qwen2-72B as well as the one finetuned without RF-CoT in most situations.</p>
<p>However, it requires a significantly longer context window and causes much slower inference speed compared to directly generating the answer.As shown in Table 3, with the 2000-token limit, CoT can only handle fraction addition involving numbers up to three digits.We provide the maximal digit length within the 2k context window limitation for each task in Appendix A.5.2 to show the context window limitation for complex tasks.As for inference time, Table 2 demonstrates the average inference time for generating each sample using "RF-CoT" and "direct answer" during the NUPA Test where both experiments are operated on an A800 GPU.In the table, the "direct answer" with batch size 256 uses a similar amount of CUDA memory as RF-CoT with batch size 128.The RF-CoT method is approximately 17 times slower than directly generating the answer, causing an unsustainable burden for such a basic operation that is frequently encountered in solving real-world problems, especially considering that number calculations may only account for a small part of a complex, real-world reasoning problem (such as analyzing a financial report).</p>
<p>RELATED WORK</p>
<p>We have discussed some related work in the corresponding section.This section highlights some other studies related to NUPA in language models.Numerical understanding in natural language comprehension Earlier studies explored numerical reasoning within language comprehension contexts.For example, Dua et al. (2019) introduced a reading comprehension dataset requiring discrete reasoning, such as sorting and addition.Similarly, Ravichander et al. (2019) proposed a benchmark for evaluating quantitative understanding in textual entailment.However, these datasets blend numerical reasoning with broader language understanding tasks, making it challenging to isolate numerical processing abilities.</p>
<p>Probing numerical understanding in LMs Several works have probed numerical comprehension in encoder models.Wallace et al. (2019) trained probing models to assess numerical understanding embedded in model representations, while Johnson et al. (2020) extend this conclusion to multilanguage settings.Naik et al. (2019) used contrastive tests to evaluate models' understanding of number magnitudes.Geva et al. ( 2020) demonstrated that finetuning on numerical reasoning data enhances the understanding.Unlike these studies, which focus on embeddings, our work emphasizes generating correct answers in autoregressive models.Recent efforts on such models include Razeghi et al. (2022), who studied few-shot learning correlations between term frequency and performance, and Zhang et al. (2024a), who identified key components in LLMs for basic arithmetic tasks.These works focus on some most classic tasks and our benchmark expands on these by incorporating diverse numerical representations, tasks, and digit ranges, offering a more comprehensive analysis.</p>
<p>Numerical dataset in specific domains Datasets like those proposed by Spithourakis &amp; Riedel (2018) and Lin et al. ( 2020) test numerical commonsense reasoning, while others focus on specific contexts, such as financial reasoning (Chen et al., 2021;2022) or tabular data (Akhtar et al., 2023).These works highlight numerical reasoning within specific domains rather than general numerical processing tasks.In contrast, our benchmark targets core numerical understanding, emphasizing tasks decoupled from domain-specific constraints.</p>
<p>Mathematical Reasoning Datasets Despite its close relationship with NUPA, mathematical reasoning is a broader field involving diverse skills such as task comprehension, equation solving, tool usage, and more (Lu et al., 2023b).While correct numerical processing is a critical component of mathematical reasoning, it is not the entirety of it (Stolfo et al., 2023).Datasets like MathQA (Amini et al., 2019), GSM8k (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), and SVAMP (Patel et al., 2021) focus on math word problems requiring multi-step reasoning and problem-solving.Few works isolate numerical processing from mathematical reasoning.Saxton et al. ( 2019) introduced a dataset for numerical tasks, such as adding floating-point numbers, but lacked task categorization by difficulty or length.Moreover, mixing numerical and algebraic tasks complicated analyses of pure numerical processing.Our benchmark addresses this gap, offering fine-grained categorization and evaluation of numerical understanding tasks.</p>
<p>CONCLUSION</p>
<p>We investigate NUPA of LLMs and introduce a comprehensive benchmark, the NUPA Test, to reveal that numerical problems remain challenging for modern LLMs.Our comprehensive test, which includes a variety of numerical representations and tasks, has exposed the surprising vulnerability of LLMs in this fundamental area.To explore ways to improve NUPA, we extend and evaluate previous pretraining techniques on the NUPA benchmark.While direct finetuning on the NUPA tasks does improve the performance, utilizing those tricks specifically designed for NUPA in the finetuning tends to harm NUPA, suggesting that these methods are not easily transferable to practical LLMs.We also explore the potential of chain-of-thought techniques to enhance NUPA and discuss their limitations.</p>
<p>LIMITATION</p>
<p>As a benchmark that specifically focuses on number understanding and processing abilities, we acknowledge that the range of tasks could still be incomplete and biased toward certain aspects.We will continue updating our benchmark, including but not limited to adding new tasks and refining existing ones to ensure appropriate difficulty.Additionally, the number of models we have tested so far is limited, and we plan to include more promising pretrained models in future evaluations.</p>
<p>On the other hand, although we have identified the limitations of LLMs' NUPA, the existing solutions each have their own drawbacks.We have yet to find a path that fully addresses the problem.Solving this issue may require research across multiple fields, such as enhancing the diversity of pretraining corpora, developing new techniques, or enabling more efficient reasoning paradigms that make more complex CoT approaches feasible.We hope our work can contribute to and be complemented by advancements in these areas.</p>
<p>REPRODUCIBILITY STATEMENT</p>
<p>We have made every effort to ensure that the results presented in this paper are fully reproducible.Detailed descriptions of the number formats, construction and metrics of our NUPA dataset are provided in Section 2 and A.1.</p>
<p>A APPENDIX</p>
<p>A.1 NUPA TEST</p>
<p>A.1.1 REPRESENTATIONS</p>
<p>We present the four representations as follows:</p>
<p>• Integer: we use no comma or point as a digit group separator like 1234567.The integer has only one part as itself.In this paper, we have not considered negative numbers for the time being.</p>
<p>• Float: A float has two parts: integer and decimal.We use a decimal point to split these two parts and also do not use any digit group separator.An example is 1234.567891.Trailing zeros in the decimal part are usually omitted.</p>
<p>• Fraction: A fraction has two parts: numerator and denominator and we use a "/" to separate the numerator and denominator parts.Unless otherwise specified, all fractions mentioned in this paper are in their simplest form (that is the numerator and denominator are coprime), but they may be greater than 1.An example is 12/7.Only in the "truediv" task between two fractions, because the "/" is also the division operator, we enclose fractions in a pair of parentheses like (12/7) / (2/3) = 18/7 to make it clear.• Scientific Notation: A scientific notation has two parts: significand and exponent.In our benchmark, the significand is always a float larger than 1 and less than 10 and the exponent should be a positive integer (and we also set an upper bound of 99).We use a "e" to separate these two parts.An example is 1.5e8.</p>
<p>A.1.2 DETAILED INTRODUCTION AND DISCUSSION ABOUT TASKS</p>
<p>In addition to the brief introduction of the 17 tasks in our benchmark, here we provide a detailed discussion on why these tasks are significant and the specific abilities they aim to evaluate.</p>
<p>• Elementary arithmetic: addition, subtraction, multiplication, and division.They are the most fundamental mathematical operations and the first branch of mathematics taught in schools.However, some operations can be complicated when different number representations are involved.For example, fraction addition is more complicated than multiplication because it needs to be reduced to a common denominator first.</p>
<p>-True division, floor division and modulus: The division is somewhat unique because it is not closed for integers and floats.Here, we consider three common division-related calculations.True division: To maintain precision, we represent the division of two integers as a simplified fraction.Combined with the "significant digits" task we will mention later, this can approximate the result of dividing two integers as a float.Integer division and modulus: Represent approximate multiple relationships, frequently used in practical applications, such as dividing individuals into batches.• Comparison: max and min.Another important aspect of understanding numbers lies in the concept of "order".To truly comprehend a number, we must know how large it is and whether it is greater or smaller than another one.Moreover, comparison serves as the foundation for other significant operations.For instance, when adding negative and positive numbers, we determine the sign first and then subtract with their absolute values -this involves identifying which of the two numbers has a greater absolute value.• Digit understanding: The concept of a digit is fundamental.Unlike the "value" of a number, a digit is tied to its specific representation.When we care about a language model's understanding, processing (and generation) of numbers, digit is a crucial concept, as numbers are not read and processed by the language model as a whole, but rather as a sequence of digits.We are curious whether LLMs truly understand the concept of digits.Therefore, we specially designed some digit-related tasks, including:</p>
<p>-Get digit: Given a number and an integer i, return the i-th digit.This task is important when certain digits have special meanings in a number (such as a phone number or SSN).-Length: Return the total length (i.e., the number of digits) of a number.</p>
<p>-Count: Count the times that a particular digit occurs in an integer.</p>
<p>-Digit compare: Compare and return the larger (smaller) digits one by one.</p>
<p>-Digit add: Perform the normal addition digit by digit but ignore any carrying.For example, digit_add(12345, 34567) = 46802.It can test a model's understanding of digit alignment and its mastery of single-digit addition.Through these tasks, we can assess whether models correctly understand the concepts of digits, length, positions, and the alignment of the digits between two numbers.</p>
<p>• Conversion between representations: we design tasks for converting a number to two representations: to float and to scientific notation, as they are frequently used to present final results.These two tasks also create transformations between different representations to test whether models can understand the relationship between various numerical formats.In particular, since many tasks present answers as approximate values, we designed a "significant digit" (sig.fig.</p>
<p>) task to evaluate a model's ability to round long numbers to fixed-length significant digits.</p>
<p>A.1.3 EXAMPLES FOR EACH TASK</p>
<p>We provide each tasks with an example.To test the models, we also add some model specific system messages like "You are a helpful assistant to process numbers.Please directly answer the question after the =".The context before "=" is the question and the context after "=" is the groundtruth and is removed when testing.</p>
<p>• Add-Integer: Add two numbers: 744 + 543 = 1287</p>
<p>• Add-Float: Add two numbers: 93.81 + 9.976 = 103.786</p>
<p>• Add-Fraction: Add two numbers: 3/8 + 2/5 = 31/40</p>
<p>• Add-Scientific: Add two numbers: 9.92e16 + 9.731e18 = 9.8302e18 • Max-Fraction: Get the maximal number: 3/5 and 3/8 = 3/5</p>
<p>• Max-Scientific: Get the maximal number: 8.15e64 and 1.063e73 = 1.063e73</p>
<p>• Digit_max-Integer: Compare two numbers digit by digit and return the larger digit at each position, treating any missing digits as 0. 50194 and 14283 = 54294</p>
<p>• Digit_max-Float: Compare two numbers digit by digit and return the larger digit at each position, treating any missing digits as 0. 35.905 and 8.4 = 38.905</p>
<p>• Digit_add-Integer: The task is to add two given numbers digit by digit and return the result modulo 10 (ignoring carry), treating any missing digits as 0. 50404 digit add 97871 = 47275</p>
<p>• Digit_add-Float: The task is to add two given numbers digit by digit and return the result modulo 10 (ignoring carry), treating any missing digits as 0. 44.418 digit add 65.669 = 9.077</p>
<p>• Get_digit-Integer: Get the digit at the given position (from left to right, starting from 0). 50404 at position 4 = 4</p>
<p>• Get_digit-Float: Get the digit at the given position (from left to right, starting from 0). 44.418 at position 3 = 1</p>
<p>• Length-Integer: The total number of digits of 50404 = 5</p>
<p>• Length-Float: The total number of digits of 262.534 = 6</p>
<p>• Count-Integer: Count the number of the given digit in the given number: 27422 count the occurrence time of digit 2 = 3</p>
<p>• To_float-Fraction: Convert the number to float: 9/5 = 1.8</p>
<p>• To_float-Scientific: Convert the number to float: 8.538e2 = 853.8</p>
<p>• To_scientific-Integer: Convert the number to scientific notation: 50400 = 5.04e4</p>
<p>• To_scientific-Float: Convert the number to scientific notation: 262.534 = 2.62534e2 Each task in the 41 ones receives one or two input numbers and expects one number as the result.We name the representation by the first input numbers.For simplicity, the second input number shares the same representation as the first one for most tasks.Calculations between different representations can be performed by first converting them to the same representation.Two types of tasks are the exception.Tasks "length", "to float" and "to scientific" do not have the second input.The second inputs in tasks "get digit", "count", "sig.fig." are always a short Integer, representing a position, length, or a digit number from 0 to 9. To distinguish them from potentially long integers to be processed, we call the former int and the latter integer.</p>
<p>We summarize the second number representation and result representation in each task in Table 4 and Table 5 where I means integer, i means (shorter) int, Fl means float, Fr means fraction, S means scientific notation and N means no such a number.
I i N i − N i Float Fl Fl Fl ✗ − − Fl Fl Fl Fl Fl i N ⃝ − N i Fraction Fr Fr Fr Fr − − Fr Fr − − − − − ⃝ N ⃝ ⃝ Scientific S S S ✗ − − S S − − − − − ⃝ N − ⃝− − − − − ⃝ Fl ⃝ ⃝ Scientific S S S ✗ − − S S − − − − − ⃝ Fl − ⃝ A.1.5 NON-INCLUDED TASKS
We exclude some compositions between number representations and tasks because of the following three reasons:</p>
<p>• ✗ too complex.We exclude the truediv between float and scientific.Division between float numbers is difficult to define accurately in our scenario.It is very common to divide two floating point numbers into an infinite decimal, which means that even very short decimals can still result in a very long and unpredictable result after division.And in this task we do not want to discuss the case of rounding the result.(This is another task of ours.)For the same reason, we also exclude division in scientific notation.</p>
<p>• ⃝: can be easily transferred to from an included task.</p>
<p>-Converting fractions to scientific notation can be done by first converting to a float.(Fraction-to_scientific = Fraction-to_float + Float-to_scientific). Fraction-SignificantFigure is similar.-Scientific notation retains significant digits and is virtually identical to floating point numbers.</p>
<p>count is a special task where we just consider a number as "a set of digits" so count in a float, fraction and scientific notation is as the same as in a integer.</p>
<p>• −: not applicable.</p>
<p>-In fraction and scientific notation, the digit concept is not well-defined so the tasks about digit (digit-compare, digit-add, get-digit and length) are not applicable.-Floordiv and mod is only defined on integer.</p>
<p>-Integer and float do not need to be further converted to float.Similarly, scientific has no need to converted to scientific.</p>
<p>A.1.6EASY/HARD SPLIT OF NUPA TASKS</p>
<p>We divide the tasks into easy and hard as shown in Table 6, where the hard tasks marked as H with maximal test digit as 20 and the easy tasks marked as E with maximal test digit as 100.We define the length of a number as the number of digits in the longest part of a number.The "integer" part and "decimal" part of a float (as well as the significand of a scientific notation), the "numerator" and "denominator" of a fraction, the "exponent" of a scientific notation are considered as different "parts".In order to generate a pair of numbers with the larger length L, we first generate a L-length number and then generate a l-length number where l follows a uniform distribution from L/2 to L. If the operation is commutative, we swap the two numbers with probability 0.5.</p>
<p>After we select two random numbers, we have some preprocessing to generate the final questions:</p>
<p>• For "Multiply", the difficulty also affected by the shorter number severely, so we split the task into two sub-tasks as "multiply-hard" and "multiply-easy".For the hard subset, we require that the shorter number must be longer than half of the longer one.For an easy subset, we require that the length of the shorter number is less than 3, so that the complexity is O(n) instead of O(n 2 ).And because the addition of fractions also involves multiplication, we also add an add-easy for this task in the same way.</p>
<p>• For "max" and "min" tasks, we additionally provide a harder version.For Integers and floats, we make sure that two compared numbers share the same length.At the same time, they should have more digits as the same like 12949 and 12961 to avoid models that can solve the problem by only counting the length or comparing the first digit.For scientific notation, we ensure 70% pairs of compared numbers with the same exponential part so that models cannot directly get the answer without comparing the significand part.For fractions, we ensure the numbers are both less than one, avoiding the model can just compare them with 1 to get more than 50% accuracy.</p>
<p>• For "to_float-Fraction", we require that the fraction can be converted into a finite decimal, that is the denominator contains only factors 2 and 5.</p>
<p>• For "add/sub-Scientific", we require the exponential part of each number to have a difference less than 5 to make sure that the generated answer will not be too long.</p>
<p>The pre-processing could introduce additional duplicated data, so we implement a post-filtering step to remove duplicates and ensure data integrity.</p>
<p>A.1.8 METRICS</p>
<p>For digit match, we should first align the numbers.For the integers and integer parts in floats, the numerator and denominator of fractions, and the exponential part of the scientific notation, we use the right alignment.For the decimal part in floats (as well as the in the significand part in scientific notation), we use the left alignment.</p>
<p>For dlength, we first measure the difference of each part of a number and then add the absolute values up.</p>
<p>Besides the average metrics in each range, we also present the following metrics: well-learned digits and performance-preserving digits to demonstrate the model's upper and lower performance limits on length.These represent the maximum number of digits that can maintain over 90% and 10% accuracy, respectively.(For digit match, the thresholds are set to 90% and 50%, and for dlength, where smaller is better, the thresholds are 0.1 and 1).</p>
<p>We ensure that there is no duplicated sample in dataset, so for some range, the test samples could be less than 1000.We also omit 1 digit or some 2 digit test in our testbed to make sure that unit rules can be included in a training set.</p>
<p>A.2 PROMPTS AND OTHER DETAILS TO TEST BASELINE MODELS</p>
<p>For all models in our test, we first provide a "format prompt" describing the expected return format (and avoiding models generating complex CoT), and a "task prompt" describing the task.We use some easy problems to ensure powerful models (gpt-4o-mini and Llama-3.1-8B) can correctly understand the tasks and expected return format by the prompts.The expected return representation of each task is referred to in Appendix A.1.4.</p>
<p>The format prompt based on the expected return type of the task is as follows:</p>
<p>• Integer: Directly return the answer as an integer without any comma separator, like 123 .</p>
<p>• float: Directly return the answer as a float without any comma separator, like 10.4 .</p>
<p>• Fraction: Directly return the answer as an <strong>irreducible</strong> fraction without any comma separator, like 7/13 .• Scientific Notation: Directly return the answer as a scientific notation without any comma separator, like 1.23e4 .The float part should be in the range [1, 10).</p>
<p>The task prompts are listed as follows where <a> and <b> are numbers.</p>
<p>• Notice that all prompts are ended with an "=" so that we can easily separate the input question and the generation of models.When we use the texts in supervised finetuning (SFT), the context before the "=" is not involved in the loss calculation.</p>
<p>For GPT-4o and GPT-4o-mini, we also add a system message as follows and use the aforementioned question as user message:</p>
<p>You are a capable math assistant.Return your solution without any process in the format: The answer is [YOUR ANSWER].The final answer must strictly match the format <regex>.</p>
<p>where the <regex> is a regular expression based on the expected return format:</p>
<p>• Integer: r"\d+"</p>
<p>• Float: r"\d+.\d+"</p>
<p>• Fraction: r"\d+/\d+"</p>
<p>• Scientific Notation: r"\d+.\d+e\d+"</p>
<p>We use the models expect GPT from huggingface and use the default tokenizer, model and generation configuration provided by the models.We test GPT-4o and GPT-4o-mini by the OpenAI API, where GPT-4o means gpt-4o-2024-0806 and GPT-4o-mini means GPT-4o-mini-2024-07-18.For Qwen2-72B and Llama-3.1-70B,we additionally use 4-bit quantization but we also test several samples without quantization and ensure this quantization does not affect generation quality.</p>
<p>We retrieve the first match of the corresponding regular expression after the "=" as the answer.If there is no retrieve, we use an empty answer to calculate the metrics, where exact match and digit match is both zero and the dlength is the total length of the groundtruth number.</p>
<p>A.3 FULL TEST RESULTS OF LLMS</p>
<p>We show the full NUPA Test results in Figures 5 (exact match), 6 (digit match), 7 (dlength) and Table 7, 8, 9 (well-learned digits and performance-preserving digits for each metrics).</p>
<p>With the detailed metrics, we can more clearly understand the behavior of some models on some tasks.For example, we find that the "exact match" and "digit match" of some models like Qwen-2 and GPT-4o on the "integer-max" task are similar, suggesting that when the models know which one is correct, they can always copy the answer from question correctly.So the wrong answer comes from incorrect comparison.Another example is the Llama-2 performance on max-hard.Because the length of two input numbers and the groundtruth answer in the max-hard task are all the same, most models show less dlength on this task suggesting they know that "the answer should have the same length of inputs", but we find Llama-2 shows dlength approximately equal to the average length in the range, suggesting that Llama-2 cannot generate a valid answer on this task.These are just a few examples to illustrate how more detailed metrics can help us gain a deeper understanding of model behavior.There are many possible conclusions, but there are too many to list here.</p>
<p>A.3.1 FEW-SHOT LEARNING</p>
<p>To ensure the output format of models is as precise as possible, we employ 5-shot learning.For each task, we select one sample from 5 different lengths respectively and test the few-shot performance.Table 10 summarizes the exact match score performance across three selected tasks and Figure 8 shows more tasks.Notably, providing an explicit output format results in general performance improvements across tasks and input lengths.In most tasks, the models can usually produce accurately formatted set.Models are trained for one epoch using a cosine decay learning rate scheduler, and the best checkpoint on validation data is reported.</p>
<p>Our experiments were conducted on a cluster equipped with Nvidia A800 GPUs (80GB memory).</p>
<p>Training a 100M model takes 5-8 hours, a 1B model approximately 1 day, and a 3B model around 2 days on a single A800 GPU.Finetuning a pretrained model typically takes about 1 day.(c) 3B model int add Figure 9: Accuracy of models of 0.1B, 0.9B and 3B parameters trained with 1-3 digit tokenizer on the task of integer addition.X-axis is the number of seen training samples.</p>
<p>A.4.2 TOKENIZATION</p>
<p>We experiment on models of 3 different size, including 0.1B, 0.9B and 3B.For the 0.1B and 0.9B models, we train them on integer addition of 1-8 digits; for the 3B model, we train it on the same task of 1-40 digits.</p>
<p>Figure 9 illustrates the in-domain performance of these three models in the first three columns and their out-of-domain (OOD) performance in the last two columns.Here we use the exact match metric.In our experiments of the 0.1B and 0.9B models, the one-digit and the two-digit tokenizer demonstrate comparable performance in the in-domain test, while the one-digit tokenizer exceeds the others to a large extent in length generalization.In contrast, the three-digit tokenizer exhibits poor performance in both in-domain and out-of-domain evaluations.Tokenizers with an increasing number of digits significantly hinder subbillion models' NUPA.In the experiments of the 3B model, the two-digit tokenizer matches the one-digit tokenizer in both in-domain and OOD performance.In addition, the three-digit tokenizer shows the potential in length generalization for the first time, yet its performance remains inferior to that of the smaller tokenizers.This indicates that scaling up the model size indeed alleviate the challenges in developing NUPA caused by larger tokenizers.Nevertheless, larger tokenizers do not present any distinct benefits in either in-domain or out-of-domain generalization in both small and large models.</p>
<p>We report the results according to different metrics from Figure 4 including digit match and dlength in Figure 10   Random tokenizer Introduced as "sub-word regularization" by Kudo (2018); Provilkov et al. (2020), the random tokenizer splits words like "Hello world" into variable tokens such as "He/llo/ world" or "Hell/o/ world".Though not widely used in LLMs, Sathe et al. (2024) found that it enhances reasoning by introducing variability in generation path.Inspired by this, we apply this to the numbers, segmenting numbers into tokens with lengths randomly chosen between 1 and a predefined maximum, instead of using greedy left-to-right segmentation.(c) 0.9B model int multiply Figure 11: Accuracy of 0.9B models trained with 1-3 digit tokenizer on three tasks of integer addition, float addition and integer multiplication according to dlength.Here we report log 2 (dlength + 1).Xaxis is the number of seen training samples.Figure 12 shows the performance of 1-to 3-digit tokenizers alongside 2-to 3-digit random tokenizers, where n-digit random tokenizer means the one with maximal length n.In terms of in-domain generalization, the three-digit random tokenizer outperforms the three-digit standard tokenizer, while the two-digit random tokenizer shows a slight decline compared to its standard counterpart.We believe this is because the 0.9B model is capable of learning the two-digit tokenizer well, and the added perturbation from random tokenization acts as a form of regularization, introducing noise that slightly affects performance.The random tokenizers consistently outperform their standard counterparts in OOD generalization, indicating the regularization benefits in that aspect.In the case of the three-digit tokenizer, which is more challenging for a 0.9B model to learn, random tokenization generates smaller tokens, making the learning process easier and leading to improved in-domain performance.However, they still fall short of the performance achieved by the one-digit tokenizer.We show exact match, digit match and dlength of 100M models trained with various including RoPE, NoPE and Alibi in Figure 13.We find NoPE and Alibi achieve better length generalization than RoPE, which is consistent with previous work like Zhou et al. (2024b).</p>
<p>To explain the mechanism of PEs, it is necessary to describe what the "generalization" is about.In most tasks, there is an intrinsic "length-agnostic" calculating rule, independent of the length of input numbers.For example, the addition rules: "align numbers by their least significant digits, add them digit by digit and carry over if the sum exceeds 9" is length-agnostic because it applies universally, regardless of the input length.However, during training on data with restricted length range (like 1 to 8), models may also learn length-related rules that fit the training data, such as combining normal addition rules with constraints like "the output length must range from 1 to 8".Because these two rules are indistinguishable, prior knowledge should be added into the model as an inductive bias to help the model learn the "length-agnostic" rules expected in most practical settings (Abbe et al., 2024;Chen et al., 2024).Int-add 1.00±0.000.00±0.001.00±0.000.45±0.020.00±0.001.07±0.02Float-add 1.00±0.000.00±0.001.00±0.000.59±0.020.00±0.001.06±0.01Frac-mul 0.70±0.010.01±0.000.85±0.010.22±0.020.18±0.021.45±0.08Sci-add 1.00±0.000.23±0.081.00±0.000.92±0.010.00±0.000.66±0.11</p>
<p>According to our experiments, we find that (1) RoPE encourages the model to rely on the length of the input.The first evidence is that RoPE causes the model's predictive performance to plummet dramatically just beyond the training boundary.We report the RoPE's performance at the boundary of training length in Table 12 where D8 (digit 8) is the longest length in the training range while D9 (digit 9) is the shortest length out of the training range.In "int-add" task, the exact match drops from nearly 100% to 0% when moving from 8 to 9 digits, while "dlength" rises from 0 to 1.07 (Table 12).This indicates that the model has a significant probability of generating shorter results, avoiding the generation of more than 8-digit answers.At the same time, RoPE not only constrains the model's output length but also affects the digit pairing.The performance of 100% for inputs of 8 digits indicates that the model performs calculations for each position unless it can successfully align the corresponding digits.However, when the model encounters 9-digit inputs, digit match drops significantly to 50%, suggesting a considerable probability of failing to align the digits.Similar results on the other three tasks suggest that it is a task-agnostic behavior.The only exception is the digit match of scientific notation addition.We discuss the results later.(2) On the other hand, length learning provided by RoPE appears to be a shortcut.In cases where the model is extremely small or has been trained very little, we see the advantages of this "shortcut".In Table 13, we train a 2-layer transformer (1.3M parameters) on integer addition using three different PEs on 1-to 8-digit integer addition or the 0.1B model with only 1M samples, we find RoPE shows the best in-domain performance.Experiments on the other three tasks are shown in Table 14 and Table 15, where the RoPE always surpasses others.</p>
<p>As a possible explanation about why Alibi and NoPE achieve better length generalization, our experiments suggest that for length generalization in number tasks, the required inductive bias is to interpret the input as a sequence of digits while deliberately ignoring its overall length.RoPE, as a positional encoding that enables the model to quickly learn position-related information, may lead the model to adopt a length-dependent shortcut (Table 13), causing it to favor length-related rules.In contrast, both Alibi and NoPE diminish this reliance on position and length, encouraging the model to treat each unit's operation as a step-by-step process, thereby achieving better length generalization.</p>
<p>Discuss about scientific addition The results in Table 12 reveal a clear trend where performance drops from 8-digit to 9-digit numbers, with one exception: the digit match score in the scientific notation addition task, which remains relatively high at 0.93 even for 9-digit numbers.We believe it is mainly because of the alignment mechanism between two scientific notations which differs from other representations.In other representations, numbers are aligned by position -integers from the most-left digit and the floats by the decimal point.However, in scientific notation, alignment depends on the difference in exponent values, which reduces RoPE's reliance on position and mitigates length overfitting.Despite this, the effect of RoPE limiting output length remains apparent, as evidenced by the significant increase in the dlength score.</p>
<p>A.4.4 DATA FORMATS We provide the experiments in Table 16 and the evaluation curves of compositions of reverse formatting, zero padding and index hints in Figure 14, Figure 15, Figure 16, Figure 17 and Figure 18.We experiment on 0.1B models trained on 1-to 8-digit training samples.Here we all use the exact match metric.</p>
<p>Previous work (Zhou et al., 2024b) believes that reverse formatting can help the calculation of each digit by aligning the calculation order to the right-to-left order that humans are accustomed to and solve the carrying problem.That is, from left-to-right, we cannot determine the result of the current digit unless the next digit results and whether there is a carrying have been known.However, a more detailed analysis can explain why the order is not as important as previously believed:</p>
<p>Regarding addition, the cases where reverse formatting can make a difference through the effects of assisting carry-over calculations are quite rare.Most of the time, knowing the result of the next digit allows us to determine the answer for the current digit.When the next digit addition is not less than 10 (without considering further carrying from the following digit), there must be a carrying from that digit into the current one, no matter what the result of the later digits is.And when the next digit addition is not more than 8, there will never be a carrying.The only exception is the next digit addition is 9.In this situation, we must refer to the next two digits to determine the current digit results.Therefore, we point out that, although in the worst-case scenario, performing non-reversed addition requires O(n)-length looking forward for each digit (44445 + 55556 = 100001), and reversing could solve this problem, such cases are extremely rare.In most instances, the task can be accomplished with a very limited local view.</p>
<p>About the experiments of index hint, we show in Table 17.Our conclusion on index hints seems to contradict the findings of Zhou et al. (2024b), where models with index hints appeared to achieve better results.We believe this discrepancy may be related to model size and digit range.In their work, a much smaller model (only 25M parameters) was used, but the training range covered 1-40 digits.This reduced the model's ability to learn the patterns independently without external hints, resulting in a different learning outcome where the model began to rely on index hints.As a piece of evidence, when Zhou et al. (2024b) train 1-10 digits, the performance without index hint is OK.(But they did not provide the complete results of 1-10 digit training in their work.)The effectiveness of index hints may involve complex interactions, which could be an interesting direction for future research.</p>
<p>A.4.5 NUPA FINETUNING WITH PE, TOKENIZER AND REPRESENTATION MODIFICATION</p>
<p>We show parts of results of our attempt to finetune a Llama-3.1-8Bmodel with PE, tokenizer and data format modification in Table 18.All the checkpoint we select by the lowest valid loss.No one can outperform the naive finetuning or the original Llama.</p>
<p>A.5 RULE-FOLLOWING CHAIN-OF-THOUGHT A.5.1 DATA FORMAT OF RULE-FOLLOWING COT Rule-following CoT consists of a rule prompt and a response to follow the rule.To generate the CoT samples for a task, first the computational rules are converted to Python code.We utilize a code LLM to write the code in this experiment.Then, a data generation script which can generate the rule-following intermediate steps for any given input is manually written.With the script, given a pair of numbers and a task, both the rules (code prompt) and the response can be generated automatically.</p>
<p>In the finetuning stage, both the prompt and the response are provided to the model but only the loss on the response part is calculated.During inference, the code prompt is still provided and the models are expected to generate the rule-following steps as well as the final answer.</p>
<p>Below is an example of a complete Rule-Following CoT data format, where the model is required to solve integer addition task with a right-to-left recursion and three digit addition as the unit task.</p>
<p>Prompt:</p>
<p>Follow the given rule to solve the question.19 and we also report the maximal length within 2k tokens context windows limitation.</p>
<p>Figure 1 :
1
Figure 1: An example of metrics.</p>
<p>Figure 3 :
3
Figure 3: Different tokenization of a long number.(a) GPT2: mixed digit tokenizer, (b) Llama-2: onedigit tokenizer.(c) GPT-3.5, GPT-4 and Llama-3: three-digit tokenizer.(d) Aligned three-digit tokenizer.</p>
<p>Figure 4 :
4
Figure4: Accuracy of 0.9B models trained with 1-3 digit tokenizer on three tasks of integer addition, float addition and integer multiplication.Shadow shows the standard error.Dn means n digits.X-axis is the number of seen training samples.</p>
<p>•</p>
<p>Sub-Integer: Subtract two numbers: 744 − 543 = 201 • Sub-Float: Subtract two numbers: 93.81 − 9.976 = 83.834• Sub-Fraction: Subtract two numbers: 2/5 − 3/8 = 1/40 • Sub-Scientific: Subtract two numbers: 9.731e38 − 9.92e36 = 9.6318e38 • Multiply-Integer: Multiply two numbers: 968 × 8 = 7744 • Multiply-Float: Multiply two numbers: 8.4 × 9.555 = 80.262 • Multiply-Fraction: Multiply two numbers: 8/7 × 5/2 = 20/7 • Multiply-Scientific: Multiply two numbers: 9.92e16 × 9.731e38 = 9.653152e55 • Truediv-Integer: Divide two numbers and return the result as a fraction.744 / 543 = 248/181 • Truediv-Fraction: Divide two numbers and return the result as a fraction.(3/8) / (2/5) = 15/16 • Floordiv-Integer: Divide two numbers and return the result as an integer.845 // 152 = 5 • Mod-Integer: Divide two numbers and return the remainder.845 % 152 = 85 • Max-Integer: Get the maximal number: 50404 and 97871 = 97871 • Max-Float: Get the maximal number: 44.418 and 65.669 = 65.669</p>
<p>•</p>
<p>Sig.Fig-Integer: Convert the number to scientific notation: 50194 and keep significant figures as 3 = 5.02e4 • Sig.Fig-Float: Convert the number to scientific notation: 65.669 and keep significant figures as 2 = 6.6e1A.1.4EXPECTED REPRESENTATION IN EACH TASK</p>
<p>Add: Add two numbers: <a> + <b> = • Sub: Subtract two numbers: <a> -<b> = • Multiply: Multiply two numbers: <a> * <b> = • Truediv: Divide two numbers and return the result as a fraction.<a> / <b> = • Floordiv: Divide two numbers and return the result as an integer.<a> // <b> = • Mod: Divide two numbers and return the remainder.<a> % <b> • Max: Get the maximal number: <a> and <b> = • Min: Get the minimal number: <a> and <b> = • Digit max: Compare two numbers digit by digit and return the larger digit at each position, treating any missing digits as 0. <a> and <b> = • Digit min: Compare two numbers digit by digit and return the smaller digit at each position, treating any missing digits as 0. <a> and <b> = • Digit add: The task is to add two given numbers digit by digit and return the result modulo 10 (ignoring carry), treating any missing digits as 0. <a> digit add <b> = • Get digit: Get the digit at the given position (from left to right, starting from 0). <a> at position <b> = • Length: The total number of digits of <a> = • Count: Count the number of the given digit in the given number: <a> count the occurrence time of digit <b> = • To_float: Convert the number to float: <a> = • To_scient: Convert the number to scientific notation: <a> = • Sig_fig: Convert the number to scientific notation: <a> and keep significant figures as <b>.</p>
<p>Figure 10 :
10
Figure10: Accuracy of 0.9B models trained with 1-3 digit tokenizer on three tasks of integer addition, float addition and integer multiplication according to digit match.X-axis is the number of seen training samples.</p>
<p>Figure 12 :
12
Figure12: Accuracy of 0.9B models trained with 1-to 3-digit tokenizers and 2-to 3-digit random tokenizers on integer addition.Shadow shows the standard error.Dn means n digits.X-axis is the number of seen training samples.</p>
<p>Figure 13 :
13
Figure13: Exact match, digit match and dlength of 100M models trained with various PE, including RoPE, NoPE and Alibi.From top to bottom, the tasks are integer addition, float addition, fraction multiplication and scientific notation.</p>
<p>while num1 or num2: digit1 = int(num1[-3:]) if num1 else 0 digit2 = int(num2[-3:]) if num2 else 0 total = digit1 + digit2 + carry result = str(total%1000) + result carry = total//1000 num1 = num1[:-3] if num1 else num1 num2 = num2[:-3] if num2 else num2 if carry: result =str(carry) + result result = result.lstrip('0')or '0' return result Q: Return the answer as an integer without any delimiter, like 123.Add two numbers: 28 DIGIT LENGTHS WITHIN CONTEXT WINDOW The selective tasks used to train the RFFT are shown in Table</p>
<p>Table 3 :
3
Performance of RF CoT."-" means exceeding context window limitation (2k tokens).
Exact MatchAdd FloatMultiply FractionMax ScientificMod Integer# Digit567234383940678RF CoT 1.00±.00 1.00±.00-0.93±.01 0.88±.03-1.00±.00 1.00±.00 1.00±.00 0.67±.05 0.43±.07-GPT-4o0.780.660.490.530.200.000.370.460.360.010.000.00Qwen2-72B 0.620.500.700.050.000.000.960.980.950.030.000.00Llama-8B-ft 0.88±.02 0.79±.04 0.74±.04 0.50±.02 0.20±.03 0.01±.00 0.98±.01 0.97±.01 0.98±.01 0.08±.02 0.05±.04 0.05±.04</p>
<p>Table 2 :
2
Average inference time.
batchsize sec / sampleRF CoT1285.625Direct1280.371Direct2560.336</p>
<p>5, and examples for each task in A.4.4.To further facilitate reproducibility, we have included the complete dataset and the source code, enabling the generation of the entire dataset and the training and assessment of models, within the supplementary materials and the github page https://github.com/GraphPKU/number_cookbook.Researchers wishing to generate NUPA benchmark or replicate our experiments can refer to these resources for all necessary information.ETHICS STATEMENTIn conducting this research, we have adhered to the highest ethical standards to ensure the integrity and fairness of our work.For source code releases, we have ensured compliance with applicable legal standards.During the construction of the dataset, all data was entirely generated randomly, without including any personal identity information or other private data of individuals.Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy.The impact of positional encoding on length generalization in transformers.Advances in Neural Information ProcessingSystems, 36:24892-24928, 2023.
Taku Kudo. Subword regularization: Improving neural network translation models with multiple sub-word candidates. In Proceedings of the 56th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pp. 66-75, Melbourne, Australia, July 2018. Association forComputational Linguistics. doi: 10.18653/v1/P18-1007.</p>
<p>Table 4 :
4
The second input number representation
Elementary arithmeticComparisonDigit UnderstandingConversionAdd Sub Multiply Truediv Floordiv Mod Max MinDigit MaxDigit MinDigit AddGet DigitLength CountTo FloatTo ScientificSig. Fig.IntegerIIIIIIIIII</p>
<p>Table 5 :
5
Result number representation
Elementary arithmeticComparisonDigit UnderstandingConversionAdd Sub Multiply Truediv Floordiv Mod Max MinDigit MaxDigit MinDigit AddGet DigitLength CountTo FloatTo ScientificSig. Fig.IntegerIIIFrIIIIIIIiii−SSFloatFl FlFl✗−− FlFlFl Fl Flii⃝−SSFraction Fr FrFrFr−− FrFr</p>
<p>Table 6 :
6
Tasks can be divided into Easy and Hard.
Elementary arithmeticComparisonDigit UnderstandingConversionAdd Sub Multiply Truediv Floordiv Mod Max MinDigit MaxDigit MinDigit AddGet DigitLength CountTo FloatTo ScientificSig. Fig.IntegerH HHHHH EEEEEEEEEEFloatH HHEEEEEEEEEFraction H HHHHHHScientific H HHEEEA.1.7 PREPROCESS AND QUESTION GENERATION FOR NUPA TASKS</p>
<p>Table 11 :
11
Detailed model settings for experiments.
parameter size num hidden layers hidden size intermediate size num attention heads num KV heads100M810243584820.9B16204871681643.0B24307210752246Accuracy0.0 0.2 0.4 0.6 0.8 1.00.1M 9.3M 18.5M 27.8M Exact Match D6 1-digit tokenizer 3-digit tokenizer 2-digit tokenizer0.0 0.2 0.4 0.6 0.8 1.00.1M 9.3M 18.5M 27.8M Exact Match D70.0 0.2 0.4 0.6 0.8 1.00.1M 9.3M 18.5M 27.8M Exact Match D80.0 0.2 0.4 0.6 0.8 1.00.1M 9.3M 18.5M 27.8M Exact Match D90.00 0.05 0.10 0.15 0.20 0.250.1M 9.3M 18.5M 27.8M Exact Match D10(a) 0.1B model int addAccuracy0.0 0.2 0.4 0.6 0.8 1.00.0M 0.6M 1.2M 1.8M 2.4M Exact Match D6 1-digit tokenizer 2-digit tokenizer 3-digit tokenizer0.0 0.2 0.4 0.6 0.8 1.00.0M 0.6M 1.2M 1.8M 2.4M Exact Match D70.0 0.2 0.4 0.6 0.8 1.00.0M 0.6M 1.2M 1.8M 2.4M Exact Match D80.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.80.0M 0.6M 1.2M 1.8M 2.4M Exact Match D9 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.2000.0M 0.6M 1.2M 1.8M 2.4M Exact Match D10(b) 0.9B model int addAccuracy0.0 0.2 0.4 0.6 0.8 1.00.1M 1.4M 2.7M 4.0M 5.2M Exact Match D10 1-digit tokenizer 2-digit tokenizer 3-digit tokenizer0.0 0.2 0.4 0.6 0.80.1M 1.4M 2.7M 4.0M 5.2M Exact Match D250.0 0.1 0.2 0.3 0.4 0.5 0.60.1M 1.4M 2.7M 4.0M 5.2M Exact Match D400.00 0.05 0.10 0.15 0.20 0.25 0.300.1M 1.4M 2.7M 4.0M 5.2M Exact Match D450.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035 0.0400.1M 1.4M 2.7M 4.0M 5.2M Exact Match D50</p>
<p>and Figure 11.
Accuracy0.20 0.40 0.60 0.80 1.000.1M 0.5M 0.9M 1.3M 1.7M 2.0M Digit Match D6 1-digit tokenizer 2-digit tokenizer 3-digit tokenizer0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.000.1M 0.5M 0.9M 1.3M 1.7M 2.0M Digit Match D70.30 0.40 0.50 0.60 0.70 0.80 0.90 1.000.1M 0.5M 0.9M 1.3M 1.7M 2.0M Digit Match D80.20 0.40 0.60 0.800.1M 0.5M 0.9M 1.3M 1.7M 2.0M Digit Match D90.10 0.20 0.30 0.40 0.50 0.60 0.700.1M 0.5M 0.9M 1.3M 1.7M 2.0M Digit Match D10(a) 0.9B model int addAccuracy0.20 0.40 0.60 0.80 1.000.1M 3.3M 6.5M 9.7M Digit Match D6 1-digit tokenizer 2-digit tokenizer 3-digit tokenizer0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.000.1M 3.3M 6.5M 9.7M Digit Match D70.30 0.40 0.50 0.60 0.70 0.80 0.90 1.000.1M 3.3M 6.5M 9.7M Digit Match D80.20 0.40 0.60 0.80 1.000.1M 3.3M 6.5M 9.7M Digit Match D90.20 0.40 0.60 0.800.1M 3.3M 6.5M 9.7M Digit Match D10(b) 0.9B model float addAccuracy0.20 0.30 0.40 0.50 0.60 0.700.1M 1.4M 2.7M 4.0M Digit Match D6 1-digit tokenizer 2-digit tokenizer 3-digit tokenizer0.20 0.30 0.40 0.50 0.600.1M 1.4M 2.7M 4.0M Digit Match D70.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.550.1M 1.4M 2.7M 4.0M Digit Match D80.15 0.20 0.25 0.30 0.35 0.40 0.45 0.500.1M 1.4M 2.7M 4.0M Digit Match D90.15 0.20 0.25 0.30 0.35 0.400.1M 1.4M 2.7M 4.0M Digit Match D10(c) 0.9B model int multiply</p>
<p>Table 12 :
12
RoPE performance with standard error from three repeated experiments.Dn means n digits where D8 is the longest in-domain length and D9 is the shortest out-of-domain length.
Exact MatchDigit MatchDlengthD8D9D8D9D8D9</p>
<p>Table 13
13: 8-digit digit-match accuracywith small model or small dataset.1.3M Model 1M SamplesRoPE0.0910.97NoPE0.0610.78Alibi0.0560.23</p>
<p>Table 16 :
16
Exact match of 0.1B models trained on integer addition and float addition respectively with various compositions of reverse formatting and zero padding.97±.05 1.00±.000.98±.021.00±.010.11±.010.24±.000.12±.000.24±.000.12±.000.24±.001.00±.001.00±.000.99±.011.00±.00d10 0.69±.110.91±.050.16±.110.50±.340.07±.030.21±.010.10±.020.23±.000.07±.020.17±.040.97±.030.87±.160.17±.040.76±.19
Integer AdditionFloat Additionrevrev +padnopadrev totalrev total + padrev eachrev each + padrev decrev dec + padrev intrev int + padnopadd9 0.</p>
<p>Table 19 :
19
Maximum length of each task that 2k context window can afford with RF-CoT Add Sub Multiply Floordiv Mod Max DigitMax GetDigit Length
Integer2020122061001710034Float654--50-100-Fraction323--20---Scientific333--100---
An interactive performance report is shown in https://huggingface.co/spaces/kangshijia/NUPA-Performance.
ACKNOWLEDGEMENTSThis work was supported by National Key R&amp;D Program of China (2022ZD0160300) and the NSF China (No. 62276004).Published as a conference paper at ICLR 2025 outputs even in the zero-shot setting, with limited additional benefit observed from few-shot examples, while the few-shot examples can indeed provide some performance improvement.We find that the conclusions mentioned in main paper have still holds.For example, the performance also significantly decreases as the length increases or the tasks and representations become unfamiliar (like Add-Fraction, Add-Scientific or floordiv).And the performance of digit-related tasks are still unsatisfying.Llama-2-7b-hf-5-shot 0.61 0.12 0.00 0.00 0.68 0.55 0.49 0.43 0.04 0.01 0.01 0.00 Llama-2-7b-hf 0.74 0.11 0.00 0.00 0.44 0.47 0.28 0.15 0.04 0.01 0.00 0.00 Llama-3.1-8B-5-shot0.94 0.41 0.10 0.01 0.88 0.81 0.63 0.54 0.23 0.02 0.01 0.00 Llama-3.1-8B0.95 0.38 0.06 0.02 0.70 0.57 0.41 0.36 0.19 0.01 0.01 0.01 Qwen2-7B-5-shot 0.83 0.82 0.37 0.04 1.00 0.98 0.81 0.64 0.28 0.08 0.03 0.01 Qwen2-7B 0.93 0.70 0.23 0.03 0.68 0.72 0.55 0.43 0.22 0.05 0.01 0.02 Exact Match We train several models to test the effectiveness of tokenizers, PEs and data formats.Unless otherwise mentioned, our model architecture uses the Llama-3.1 architecture (Decoder-only Transformers with causal masking, autoregressive generation, and RoPE as the default PEs).We modify the layer numbers, hidden size and the number of heads to change the parameter size of models.See Table11.We keep all hyperparameters, except model size, consistent with the original Llama setup in the implementation from Huggingface.We use the default sampling generation strategy with default hyperparameters, where the temperature is set as 0.6 and top_p is 0.9.About the meaning of these settings please refer to Llama technique report(Meta, 2024a)and model cards (Meta, 2024b).Digit Max FloatTruedivTo train these models, we use the AdamW optimizer(Loshchilov &amp; Hutter, 2019)with a learning rate of 5e-5, weight decay of 0.01, and batch sizes of 256, 64, and 32 for 0.1B, 0.9B, and 3B models, respectively.Other optimizer settings follow the default values in the Transformers library.We sample 1e7 samples for each length (where feasible) and concatenate them into a single trainingMax Hard IntegerMax Hard FloatMax Hard ScientificNotationMultiply Hard IntegerMultiply Hard FloatMultiply Hard FractionMultiply Hard ScientificNotation GPT-4o-miniFigure5: Exact match of models tested on NUPA Test.GPT-4o-miniMultiply Easy IntegerMultiply Easy FloatMultiply Easy FractionMultiply Easy ScientificNotationDigit Max IntegerDigit Max FloatDigit Add IntegerDigit Add FloatTruediv Fraction Floordiv IntegerMod IntegerMod Easy Integer To Float FractionTo Float ScientificNotationFigure6: Digit match of models tested on NUPA Test.Max Hard IntegerMax Hard FloatMax Hard ScientificNotationMultiply Hard IntegerMultiply Hard FloatMultiply Hard FractionMultiply Hard ScientificNotationMultiply Easy IntegerMultiply Easy FloatMultiply Easy FractionMultiply Easy ScientificNotationDigit Max IntegerDigit Max FloatDigit Add IntegerDigit Add FloatTruediv Fraction Floordiv IntegerMod IntegerMod Easy Integer To Float FractionTo Float ScientificNotation To Scient IntegerFigure7: Dlength of models tested on NUPA Test.Note that we use log 2 (dlength + 1) as the ylabel in the figure.Published as a conference paper at ICLR 2025 GPT-4o-mini 0 / 5 0 / 0 1 / 2 0 / 4 0 / 6 0 / 0 1 / 2 0 / 4 GPT-4o 0 / 5 0 / 0 1 / 2 0 / 4 0 / 6 0 / 0 1 / 3 0 / 4 Qwen2-72B 0 / 5 0 / 0 0 / 0 0 / 0 0 / 6 0 / 0 0 / 0 0 / 0 Qwen2-7B 0 / 3 0 / 0 0 / 0 0 / 0 0 / 4 0 / 0 0 / 0 0 / 0 Llama-3.1-8B-ft0 / 4 0 / 0 0 / 3 0 / 4 0 / 6 0 / 3 1 / 3 0 / 5 Llama-3.1-70B0 / 5 0 / 3 0 / 2 0 / 3 0 / 6 0 / 3 0 / 2 0 / 3 Llama-3.1-8B0 / 4 0 / 0 0 / 0 0 / 0 0 / 5 0 / 0 0 / 1 0 / 0 Mixtral-8x7B 0 / 4 0 / 0 0 / 1 0 / 0 0 / 5 0 / 0 0 / 2 0 / 0 Llama-2-7b-hf 0 / 0 0 / 0 0 / 0 0 / 0 0 / 0 0 / 0 0 / 0 0 / 0
Generalization on the unseen, logic reasoning and degree curriculum. Emmanuel Abbe, Samy Bengio, Aryo Lotfi, Kevin Rizk, Journal of Machine Learning Research. 253312024</p>
<p>Exploring the numerical reasoning capabilities of language models: A comprehensive analysis on tabular data. Mubashara Akhtar, Abhilash Shankarampeta, Vivek Gupta, Arpit Patil, Oana Cocarascu, Elena Simperl, 10.18653/v1/2023.findings-emnlp.1028Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>MathQA: Towards interpretable math word problem solving with operation-based formalisms. Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, Hannaneh Hajishirzi, 10.18653/v1/N19-1245Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational LinguisticsJune 20191</p>
<p>Low-dimension-to-high-dimension generalization and its implications for length generalization. Yang Chen, Yitao Liang, Zhouchen Lin ; Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan R Routledge, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: EMNLP 2021. the 2021 Conference on Empirical Methods in Natural Language Processing: EMNLP 20212024. 2021Finqa: A dataset of numerical reasoning over financial data</p>
<p>Convfinqa: Exploring the chain of numerical reasoning in conversational finance question answering. Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, William Yang, Wang , Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: EMNLP 2022. the 2022 Conference on Empirical Methods in Natural Language Processing: EMNLP 20222022</p>
<p>Position coupling: Leveraging task structure for improved length generalization of transformers. Hanseul Cho, Jaeyoung Cha, Pranjal Awasthi, Srinadh Bhojanapalli, Anupam Gupta, Chulhee Yun, First Workshop on Long-Context Foundation Models@ ICML 2024. 2024</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner, 10.18653/v1/N19-1246Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational LinguisticsJune 20191</p>
<p>Towards revealing the mystery behind chain of thought: A theoretical perspective. Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, Liwei Wang, Advances in Neural Information Processing Systems. Curran Associates, Inc202336</p>
<p>Injecting numerical reasoning skills into language models. Mor Geva, Ankit Gupta, Jonathan Berant, 10.18653/v1/2020.acl-main.89Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020</p>
<p>Transformer language models without positional encodings still learn positional information. Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, Omer Levy, 10.18653/v1/2022.findings-emnlp.99Findings of the Association for Computational Linguistics: EMNLP 2022. Yoav Goldberg, Zornitsa Kozareva, Yue Zhang, Abu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022</p>
<p>Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, Maosong Sun, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, International Conference on Learning Representations. 2021a</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021</p>
<p>Low-rank adaptation of large language models. J Edward, Phillip Hu, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 2022</p>
<p>Case-based or rule-based: how do transformers do the math?. Yi Hu, Xiaojuan Tang, Haotong Yang, Muhan Zhang, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine Learning2024</p>
<p>A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, Ting Liu, 10.1145/3703155ACM Trans. Inf. Syst. 1046-8188November 2024</p>
<p>Llms can find mathematical reasoning mistakes by pedagogical chain-of-thought. Zhuoxuan Jiang, Haoyuan Peng, Shanshan Feng, Fan Li, Dongsheng Li, Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence. the Thirty-Third International Joint Conference on Artificial Intelligence2024</p>
<p>Probing for multilingual numerical understanding in transformer-based language models. Devin Johnson, Denise Mak, Andrew Barker, Lexi Loessberg-Zahl, Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP2020</p>
<p>Teaching arithmetic to small transformers. Nayoung Lee, Kartik Sreenivasan, Jason D Lee, Kangwook Lee, Dimitris Papailiopoulos, International Conference on Learning Representations. 2024</p>
<p>Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Q Jiang, Ziju Shen, 2024a</p>
<p>Evaluating mathematical reasoning of large language models: A focus on error identification and correction. Xiaoyuan Li, Wenjie Wang, Moxin Li, Junrong Guo, Yang Zhang, Fuli Feng, Findings of the Association for Computational Linguistics ACL 2024. 2024b</p>
<p>Birds have four legs?! numersense: Probing numerical commonsense knowledge of pre-trained language models. Seyeon Bill Yuchen Lin, Rahul Lee, Xiang Khanna, Ren, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: EMNLP 2020. the 2020 Conference on Empirical Methods in Natural Language Processing: EMNLP 20202020</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, International Conference on Learning Representations. 2019</p>
<p>Chameleon: Plug-and-play compositional reasoning with large language models. Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao, Advances in Neural Information Processing Systems. 2023a36</p>
<p>A survey of deep learning for mathematical reasoning. Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, Kai-Wei Chang, 10.18653/v1/2023.acl-long.817Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 2023b1</p>
<p>Model cards and prompt formats of llama 3.1, 2024b. Meta. 2024aThe llama 3 herd of models</p>
<p>Mistralai, Mixtral of experts. 2024</p>
<p>Exploring numeracy in word embeddings. Aakanksha Naik, Abhilasha Ravichander, Carolyn Rose, Eduard Hovy, 10.18653/v1/P19-1329Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsJuly 2019</p>
<p>Gpt-4o system card. Gpt-o1 system card, 2024b. 2023. 2024aOpenAIGpt-4 technical report</p>
<p>Are NLP models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, doi: 10.18653Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterAssociation for Computational LinguisticsJune 2021</p>
<p>Train short, test long: Attention with linear biases enables input length extrapolation. Ofir Press, Noah Smith, Mike Lewis, International Conference on Learning Representations. 2022</p>
<p>BPE-dropout: Simple and effective subword regularization. Ivan Provilkov, Dmitrii Emelianenko, Elena Voita, 10.18653/v1/2020.acl-main.170Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020</p>
<p>Alibaba Group Qwen Team. Qwen2 technical report. 2024</p>
<p>On long-tailed phenomena in neural machine translation. Siddharth Vikas Raunak, Vivek Dalmia, Florian Gupta, Metze, 10.18653/v1/2020.findings-emnlp.276Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational LinguisticsNovember 2020</p>
<p>EQUATE: A benchmark evaluation framework for quantitative reasoning in natural language inference. Abhilasha Ravichander, Aakanksha Naik, Carolyn Rose, Eduard Hovy, 10.18653/v1/K19-1033Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL). the 23rd Conference on Computational Natural Language Learning (CoNLL)Hong Kong, ChinaAssociation for Computational LinguisticsNovember 2019</p>
<p>Impact of pretraining term frequencies on few-shot numerical reasoning. Yasaman Razeghi, Robert L Logan, I V , Matt Gardner, Sameer Singh, 10.18653/v1/2022.findings-emnlp.59Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022</p>
<p>Improving self consistency in LLMs through probabilistic tokenization. Ashutosh Sathe, Divyanshu Aggarwal, Sunayana Sitaram, ICML 2024 Workshop on LLMs and Cognition. 2024</p>
<p>Analysing mathematical reasoning abilities of neural models. David Saxton, Edward Grefenstette, Felix Hill, Pushmeet Kohli, International Conference on Learning Representations. 2019</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, Advances in Neural Information Processing Systems. 202336</p>
<p>Positional description matters for transformers arithmetic. Ruoqi Shen, Sébastien Bubeck, Ronen Eldan, Yin Tat Lee, Yuanzhi Li, Yi Zhang, 2023</p>
<p>Tokenization counts: the impact of tokenization on arithmetic in frontier llms. K Aaditya, D J Singh, Strouse, arXiv:2402.149032024arXiv preprint</p>
<p>Numeracy for language models: Evaluating and improving their ability to predict numbers. Georgios Spithourakis, Sebastian Riedel, 10.18653/v1/P18-1196Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational LinguisticsJuly 20181</p>
<p>A causal framework to quantify the robustness of mathematical reasoning with language models. Alessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bernhard Schoelkopf, Mrinmaya Sachan, 10.18653/v1/2023.acl-long.32Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>Roformer: Enhanced transformer with rotary position embedding. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, Yunfeng Liu, 10.1016/j.neucom.2023.127063Neurocomput. 0925- 2312568February 2024</p>
<p>Mathscale: scaling instruction tuning for mathematical reasoning. Zhengyang Tang, Xingxing Zhang, Benyou Wang, Furu Wei, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine Learning2024</p>
<p>Scaling laws with vocabulary: Larger models deserve larger vocabularies. Chaofan Tao, Qian Liu, Longxu Dou, Niklas Muennighoff, Zhongwei Wan, Ping Luo, Min Lin, Ngai Wong, Advances in Neural Information Processing Systems. 202537</p>
<p>Do NLP models know numbers? probing numeracy in embeddings. Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, Matt Gardner, 10.18653/v1/D19-1534Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsNovember 2019</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Conveyor: Efficient tool-aware llm serving with tool partial execution. Yechen Xu, Xinhao Kong, Tingjun Chen, Danyang Zhuo, arXiv:2406.000592024arXiv preprint</p>
<p>An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. 2024a</p>
<p>Parrot mind: Towards explaining the complex task reasoning of pretrained large language models with template-content structure. Haotong Yang, Fanxu Meng, Zhouchen Lin, Muhan Zhang, 2024b</p>
<p>Interpreting and improving large language models in arithmetic calculation. Wei Zhang, Chaoqun Wan, Yonggang Zhang, Yiu-Ming Cheung, Xinmei Tian, Xu Shen, Jieping Ye, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine Learning2024a</p>
<p>Evaluating the performance of large language models on gaokao benchmark. Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, Xipeng Qiu, 2024b</p>
<p>What algorithms can transformers learn? a study in length generalization. Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Joshua M Susskind, Samy Bengio, Preetum Nakkiran, International Conference on Learning Representations. 2024a</p>
<p>Transformers can achieve length generalization but not robustly. Yongchao Zhou, Uri Alon, Xinyun Chen, Xuezhi Wang, Rishabh Agarwal, Denny Zhou, ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models. 2024b</p>            </div>
        </div>

    </div>
</body>
</html>
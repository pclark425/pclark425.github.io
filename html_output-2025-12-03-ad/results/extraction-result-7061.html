<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7061 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7061</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7061</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-806b5882c983bd156a8c10bcd34fe285d8a0593b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/806b5882c983bd156a8c10bcd34fe285d8a0593b" target="_blank">GLoRE: Evaluating Logical Reasoning of Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> GLoRE is a General Logical Reasoning Evaluation platform that not only consolidates diverse datasets but also standardizes them into a unified format suitable for evaluating large language models across zero-shot and few-shot scenarios.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown significant general language understanding abilities. However, there has been a scarcity of attempts to assess the logical reasoning capacities of these LLMs, an essential facet of natural language understanding. To encourage further investigation in this area, we introduce GLoRE, a General Logical Reasoning Evaluation platform that not only consolidates diverse datasets but also standardizes them into a unified format suitable for evaluating large language models across zero-shot and few-shot scenarios. Our experimental results show that compared to the performance of humans and supervised fine-tuning models, the logical reasoning capabilities of large reasoning models, such as OpenAI's o1 mini, DeepSeek R1 and QwQ-32B, have seen remarkable improvements, with QwQ-32B achieving the highest benchmark performance to date. GLoRE is designed as a living project that continuously integrates new datasets and models, facilitating robust and comparative assessments of model performance in both commercial and Huggingface communities.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7061.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7061.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa-base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa (base)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 125M-parameter Transformer encoder model used as a supervised fine-tuned baseline across GLoRE datasets; fine-tuned for five epochs per dataset in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Roberta: A robustly optimized bert pretraining approach.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only Transformer pretrained with masked language modeling; used here as a supervised fine-tuned baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Fine-tuned on each GLoRE dataset's training split (task-specific supervised fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Supervised fine-tuning on logical reasoning datasets (no explicit algorithmic or symbolic augmentation).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GLoRE (multiple constituent datasets such as LogiQA, ReClor, ProofWriter)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A consolidated logical reasoning benchmark composed of MRC (LogiQA, ReClor, AR-LSAT, LogiQA22), NLI (ConTRoL, HELP, TaxiNLI, NaN-NLI) and True-or-False tasks (FraCaS, RuleTaker, ProofWriter).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-choice reading comprehension, NLI (entailment/contradiction/neutral), True-or-False entailment</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (classification)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Average 47.76% (per Table 2); examples: LogiQA 48.76%, LogiQA22 33.22%, NaN-NLI 90.02%, ProofWriter 55.92%</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Serves as the supervised baseline; most zero-shot LLMs (vanilla LLaMA/Falcon) underperform it on many tasks but GPT-4 and reasoning-enhanced models outperform it.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fine-tuned RoBERTa achieves reasonable task-specific performance (stronger on some NLI/TF tasks) but lags far behind human averages and top reasoning-enhanced LLMs on GLoRE.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Limited by supervised fine-tuning on each dataset; poor generalization across distributions and lower performance on MRC logical tasks compared to reasoning-enhanced LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7061.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7061.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-30B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (instruction-tuned variant used: LLaMA-30B-supercot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open foundation transformer language model (30B parameters variant referenced) evaluated zero-shot and few-shot on GLoRE; shows weak zero-shot logical reasoning without further tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llama: Open and efficient foundation language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-30B-supercot</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Transformer decoder language model; instruction-tuned variant (supercot).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>30B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretraining on large web corpora; instruction tuning (supercot) as used by authors (no dataset-specific logical fine-tuning reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Zero-shot / in-context learning; no explicit symbolic or external reasoning module.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GLoRE</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>As above: unified logical reasoning benchmark covering MRC, NLI, and TF tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>MRC, NLI, TF</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Zero-shot average 32.34%; examples: LogiQA 19.31%, ProofWriter 53.78%, NaN-NLI 47.29%</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Underperforms RoBERTa-base on many tasks; few-shot yields modest gains (0-shot 32.34% -> 5-shot 39.62%).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Vanilla LLaMA variants struggle at zero-shot logical MRC and benefit moderately from few-shot examples but remain far below reasoning-enhanced models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Poor zero-shot MRC performance (sometimes below random-chance for multi-way tasks), sensitivity to dataset distribution, no specialized reasoning training reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7061.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7061.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-40B-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon-40B (instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 40B-parameter open language model instruction-tuned for instruction following; evaluated on GLoRE and shows similar reasoning capability to LLaMA in zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Falcon-40B: an open large language model with state-of-the-art performance</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-40B-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large decoder-only Transformer (40B) with instruction tuning for improved conversational/instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>40B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretraining on large web corpora; instruction tuning (dataset specifics not reported in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Zero-shot and in-context learning (no explicit symbolic enhancements).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GLoRE</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Unified logical reasoning evaluation across multiple datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>MRC, NLI, TF</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Zero-shot average 32.28%; examples: ProofWriter 53.33%, RuleTaker 56.11%, many MRC scores ~20-30%</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Comparable to LLaMA and below RoBERTa-base on several tasks; few-shot improves performance modestly (0-shot 32.28% -> 5-shot 35.72%).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Instruction-tuned Falcon matches LLaMA in baseline reasoning performance but does not close the gap to reasoning-enhanced models without further training signals.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Struggles with MRC logical questions in zero-shot; sensitive to data distributions and lacks specialized reasoning training in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7061.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7061.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixtral-8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixtral (mixture-of-experts) 8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mixture-of-experts (MoE) model configuration evaluated on GLoRE that outperforms same‑class decoder LLMs like LLaMA and Falcon, showing effectiveness of MoE for some reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mixtral of experts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixtral-8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mixture-of-experts Transformer architecture (8 experts × 7B experts configuration referenced) aimed at efficiency and capacity scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Approx. 56B total capacity (8x7B experts) as reported (referred to as Mixtral-8x7B)</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer + Mixture-of-Experts (MoE)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretraining corpora and instruction-style tuning details not specified in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Zero-shot / in-context learning; benefits attributed to MoE architecture rather than explicit logical training.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GLoRE</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Comprehensive logical reasoning evaluation across multiple datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>MRC, NLI, TF</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Zero-shot average 42.52%; examples: LogiQA 45.29%, ReClor 48.92%, ProofWriter 44.80%</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Outperforms LLaMA and Falcon, indicating MoE scaling benefits; still below top reasoning-enhanced models like QwQ-32B.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mixture-of-experts design yields better zero-shot reasoning performance compared to similarly situated decoder models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Although improved relative to LLaMA/Falcon, still sensitive to task distribution and lacks explicit reasoning-specific training described here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7061.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7061.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (instruct-tuned GPT family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's instruction-tuned conversational model (GPT-3.5 family) evaluated zero-shot and few-shot on GLoRE; shows intermediate performance and benefits from in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned conversational Transformer model from OpenAI (GPT-3.5 family reference in literature; exact internals not detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (decoder, instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Large-scale pretraining and instruction tuning (details not enumerated in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Zero-shot and in-context learning; often evaluated with few-shot demonstrations; chain-of-thought discussed but not specifically used to claim causal improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GLoRE</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Unified benchmark including MRC/NLI/TF datasets testing strict logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>MRC, NLI, TF</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Zero-shot average 52.10%; few-shot: 1-shot 55.85%, 2-shot 57.43%, 5-shot 60.32%; examples: ConTRoL 58.45%, ProofWriter 53.95%</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Outperforms RoBERTa-base on many MRC and NLI tasks but is outperformed by GPT-4 and top reasoning-enhanced models (o1 mini, DeepSeek R1, QwQ-32B) on average.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ChatGPT benefits from in-context examples (ICL) and shows reasonable logical reasoning ability, but still trails specialized reasoning-enhanced LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Performance varies across datasets (sensitive to distribution); relies on superficial patterns rather than consistent human-like logical generalization per authors' analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7061.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7061.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's advanced multi-modal large language model that achieves strong zero-shot and few-shot performance on many GLoRE tasks but shows sensitivity to dataset distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large Transformer-based model from OpenAI (detailed architecture and size not specified in this paper); excels in zero-shot logical reasoning on several datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (decoder/multi-modal design; instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretraining on large web and proprietary corpora; instruction tuning and safety/alignments as per OpenAI (details not enumerated here).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Zero-shot and in-context learning; can be prompted with few-shot examples to improve accuracy; chain-of-thought discussed in related literature but not claimed as causal here.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GLoRE</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Comprehensive logical reasoning benchmark covering multiple datasets and task types.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>MRC, NLI, TF</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Zero-shot average 66.34%; few-shot: 1-shot 70.31%, 2-shot 71.44%, 5-shot 75.83%; dataset examples: ReClor 87.20%, LogiQA 72.25%, LogiQA22 58.49%, FraCaS 75.35%, ProofWriter 59.66%</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Substantially outperforms RoBERTa-base and most open-source vanilla LLMs; however QwQ-32B and some reasoning-enhanced models outperform GPT-4 on average (e.g., QwQ-32B avg 78.95%).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4 attains near-human performance on some datasets (e.g., ReClor) and benefits from few-shot prompting; nevertheless shows inconsistent performance across datasets, indicating distributional sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Significant sensitivity to dataset distribution (e.g., large drop between LogiQA and LogiQA22); inconsistent NLI/TF results on some specialized tasks (e.g., HELP 46.01%).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7061.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7061.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o1 mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI o1 mini</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial instruction-following model from OpenAI evaluated on GLoRE; shows strong performance and competitive results vs GPT-4 on multiple tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Openai o1 system card</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o1 mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI commercial instruction-tuned model variant; designed for instruction-following and reasoning workloads (technical specifics in system card).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Instruction tuning and likely RLHF per OpenAI system card (details not fully enumerated in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Instruction tuning and training optimizations for reasoning (no external symbolic tools reported).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GLoRE</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Aggregated logical reasoning benchmark spanning MRC, NLI, and TF datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>MRC, NLI, TF</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Zero-shot average 72.44%; examples: LogiQA 69.35%, LogiQA22 59.84%, ConTRoL 75.12%, HELP 63.69%, ProofWriter 75.33%</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Outperforms GPT-4 on several NLI/TF metrics in this evaluation (and is competitive on MRC); overall better than many vanilla LLMs and RoBERTa baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>o1 mini is a strong commercial reasoning-capable model that narrows the gap to leading open-source reasoning models on GLoRE.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Still sensitive to dataset distribution; not uniformly superior across all NLI datasets (some tasks show lower performance).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7061.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7061.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek R1</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning-enhanced model that uses reinforcement learning incentives to improve reasoning capability; demonstrates strong performance on GLoRE and balanced results across many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek R1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source reasoning-enhanced LLM incorporating reinforcement-learning-based techniques to incentivize reasoning behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer with reinforcement learning (training/finetuning augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not exhaustively listed; trained/finetuned with reinforcement learning incentives targeting reasoning benchmarks (authors reference use of RL frameworks).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Reinforcement learning incentives to encourage reasoning (rewarding reasoning-congruent outputs); evaluated zero-shot on GLoRE.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GLoRE</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Comprehensive logical reasoning benchmark (MRC, NLI, TF datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>MRC, NLI, TF</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Zero-shot average 75.14%; examples: LogiQA 76.22%, LogiQA22 71.63%, AR-LSAT 90.01%, ProofWriter 80.51%</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Substantially outperforms vanilla LLaMA/Falcon and RoBERTa baseline; close to QwQ-32B on average but below QwQ-32B's top performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Reinforcement-learning-based training yields substantial gains in logical reasoning robustness and average performance across GLoRE.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Still shows sensitivity to dataset-specific reasoning phenomena; uneven performance on some NLI subsets (e.g., HELP-type monotonicity tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7061.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7061.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QwQ-32B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QwQ-32B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 32B-parameter reasoning-enhanced open model employing reinforcement learning or specialized training methodology, achieving state-of-the-art results on GLoRE in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Qwq-32b: Embracing the power of reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QwQ-32B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 32B Transformer-based LLM with specialized reinforcement-learning training/finetuning aiming to improve logical reasoning and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer with reinforcement learning / reasoning-enhanced training</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Trained/finetuned with reinforcement-learning-based objectives and likely on reasoning-oriented datasets (paper hypothesizes RL framework contributes to gains); specific datasets not enumerated beyond evaluation sets.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Reinforcement learning incentives / reasoning-targeted fine-tuning (no external symbolic engine reported).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GLoRE</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Unified logical reasoning benchmark covering MRC, NLI, and TF tasks with 12 constituent datasets and ~72,848 instances.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>MRC, NLI, TF (multi-step symbolic and textual entailment challenges)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Zero-shot average 78.95%; dataset highlights: LogiQA 85.70%, ReClor 93.76%, AR-LSAT 92.35%, LogiQA22 86.30%, ProofWriter 82.40%</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Outperforms GPT-4 (avg +12.61 percentage points: 78.95% vs. 66.34%) and RoBERTa baseline; sets new SOTA on multiple MRC and TF datasets in this evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Reinforcement learning and specialized training led to substantial improvements in multi-choice MRC and true-or-false proof tasks, yielding state-of-the-art average performance on GLoRE.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Less generalizable on certain fine-grained NLI phenomena (e.g., HELP monotonicity: 61.53% vs. o1 mini 63.69%); uneven performance across NLI tasks suggests brittleness on monotonicity/negation reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7061.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7061.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>In-context learning (ICL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique where few labeled examples are included in the prompt to the LLM; used in this paper to study few-shot improvements on logical reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A survey on in-context learning (2023)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>In-context learning (ICL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting paradigm rather than a model: models are given 1-5 example Q/A pairs (demonstrations) in the prompt to induce task behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Prompting / demonstration-based (not an architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>N/A (uses examples drawn from the same datasets for prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Few-shot demonstrations appended to prompts (1-shot, 2-shot, 5-shot experiments performed).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GLoRE (few-shot evaluation reported in Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Few-shot evaluation across the aggregated GLoRE benchmark to measure adaptation with demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>MRC, NLI, TF (as tested via few-shot prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported few-shot gains: GPT-4 0-shot 66.34% -> 5-shot 75.83% (+9.49 pp); ChatGPT 52.10% -> 60.32% (+8.22 pp); LLaMA modest gains 32.34% -> 39.62%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>In-context learning improves accuracy for large models but authors argue improvements reflect statistical adaptation (superficial pattern learning) rather than deeper causal improvements in reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ICL yields meaningful accuracy gains, especially for stronger models (GPT-4), but improvements may be due to pattern adaptation rather than robust logical generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Authors caution that ICL improvements do not necessarily indicate true reasoning capability and that performance remains sensitive to dataset distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7061.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7061.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting strategy that elicits step-by-step intermediate reasoning traces from LLMs; discussed in paper as correlated with outputs but not causally driving true logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How likely do llms with cot mimic human reasoning?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A prompting technique that requests intermediate reasoning steps (natural language) from the model to improve multi-step reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Prompting technique</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>N/A (prompting method applied at inference time; some models are trained/tuned with chain-of-thought data elsewhere but not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Generate intermediate natural-language reasoning steps (chain-of-thought); can be combined with self-consistency or other sampling methods (discussed in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GLoRE (discussed as a relevant technique but not used as the primary experimental variable here)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>GLoRE can be used with reasoning prompts, though the paper emphasizes dataset sensitivity and that CoT correlation does not imply causal improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step logical reasoning / MRC / proof tasks when invoked</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (where applied in literature); not isolated in this paper for causal impact</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Not directly quantified in this paper; authors reference external work showing correlations but caution against interpreting CoT as causal for reasoning gains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Cited literature suggests CoT can correlate with improved outputs, but this paper does not present an isolated ablation showing causal improvement on GLoRE.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT-style prompts may change outputs and correlate with better answers, but improvements are argued to often reflect pattern exploitation rather than genuine mechanistic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Authors cite work showing CoT's effects are correlational; therefore CoT may not yield robust, generalizable logical reasoning across distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning <em>(Rating: 2)</em></li>
                <li>Qwq-32b: Embracing the power of reinforcement learning <em>(Rating: 2)</em></li>
                <li>Transformers as soft reasoners over language <em>(Rating: 2)</em></li>
                <li>Proofwriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>Logiqa: A challenge dataset for machine reading comprehension with logical reasoning <em>(Rating: 2)</em></li>
                <li>Reclor: A reading comprehension dataset requiring logical reasoning <em>(Rating: 2)</em></li>
                <li>How likely do llms with cot mimic human reasoning? <em>(Rating: 2)</em></li>
                <li>A survey on in-context learning <em>(Rating: 1)</em></li>
                <li>Roberta: A robustly optimized bert pretraining approach. <em>(Rating: 1)</em></li>
                <li>Mixtral of experts <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7061",
    "paper_id": "paper-806b5882c983bd156a8c10bcd34fe285d8a0593b",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "RoBERTa-base",
            "name_full": "RoBERTa (base)",
            "brief_description": "A 125M-parameter Transformer encoder model used as a supervised fine-tuned baseline across GLoRE datasets; fine-tuned for five epochs per dataset in this paper.",
            "citation_title": "Roberta: A robustly optimized bert pretraining approach.",
            "mention_or_use": "use",
            "model_name": "RoBERTa-base",
            "model_description": "Encoder-only Transformer pretrained with masked language modeling; used here as a supervised fine-tuned baseline.",
            "model_size": "125M",
            "architecture_type": "Transformer (encoder)",
            "training_data": "Fine-tuned on each GLoRE dataset's training split (task-specific supervised fine-tuning).",
            "reasoning_method": "Supervised fine-tuning on logical reasoning datasets (no explicit algorithmic or symbolic augmentation).",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GLoRE (multiple constituent datasets such as LogiQA, ReClor, ProofWriter)",
            "benchmark_description": "A consolidated logical reasoning benchmark composed of MRC (LogiQA, ReClor, AR-LSAT, LogiQA22), NLI (ConTRoL, HELP, TaxiNLI, NaN-NLI) and True-or-False tasks (FraCaS, RuleTaker, ProofWriter).",
            "task_type": "Multi-choice reading comprehension, NLI (entailment/contradiction/neutral), True-or-False entailment",
            "performance_metric": "Accuracy (classification)",
            "performance_value": "Average 47.76% (per Table 2); examples: LogiQA 48.76%, LogiQA22 33.22%, NaN-NLI 90.02%, ProofWriter 55.92%",
            "comparison_with_baseline": "Serves as the supervised baseline; most zero-shot LLMs (vanilla LLaMA/Falcon) underperform it on many tasks but GPT-4 and reasoning-enhanced models outperform it.",
            "key_findings": "Fine-tuned RoBERTa achieves reasonable task-specific performance (stronger on some NLI/TF tasks) but lags far behind human averages and top reasoning-enhanced LLMs on GLoRE.",
            "limitations": "Limited by supervised fine-tuning on each dataset; poor generalization across distributions and lower performance on MRC logical tasks compared to reasoning-enhanced LLMs.",
            "uuid": "e7061.0",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLaMA-30B",
            "name_full": "LLaMA (instruction-tuned variant used: LLaMA-30B-supercot)",
            "brief_description": "An open foundation transformer language model (30B parameters variant referenced) evaluated zero-shot and few-shot on GLoRE; shows weak zero-shot logical reasoning without further tuning.",
            "citation_title": "Llama: Open and efficient foundation language models",
            "mention_or_use": "use",
            "model_name": "LLaMA-30B-supercot",
            "model_description": "Open-source Transformer decoder language model; instruction-tuned variant (supercot).",
            "model_size": "30B",
            "architecture_type": "Transformer (decoder)",
            "training_data": "Pretraining on large web corpora; instruction tuning (supercot) as used by authors (no dataset-specific logical fine-tuning reported here).",
            "reasoning_method": "Zero-shot / in-context learning; no explicit symbolic or external reasoning module.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GLoRE",
            "benchmark_description": "As above: unified logical reasoning benchmark covering MRC, NLI, and TF tasks.",
            "task_type": "MRC, NLI, TF",
            "performance_metric": "Accuracy",
            "performance_value": "Zero-shot average 32.34%; examples: LogiQA 19.31%, ProofWriter 53.78%, NaN-NLI 47.29%",
            "comparison_with_baseline": "Underperforms RoBERTa-base on many tasks; few-shot yields modest gains (0-shot 32.34% -&gt; 5-shot 39.62%).",
            "key_findings": "Vanilla LLaMA variants struggle at zero-shot logical MRC and benefit moderately from few-shot examples but remain far below reasoning-enhanced models.",
            "limitations": "Poor zero-shot MRC performance (sometimes below random-chance for multi-way tasks), sensitivity to dataset distribution, no specialized reasoning training reported.",
            "uuid": "e7061.1",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Falcon-40B-instruct",
            "name_full": "Falcon-40B (instruction-tuned)",
            "brief_description": "A 40B-parameter open language model instruction-tuned for instruction following; evaluated on GLoRE and shows similar reasoning capability to LLaMA in zero-shot.",
            "citation_title": "Falcon-40B: an open large language model with state-of-the-art performance",
            "mention_or_use": "use",
            "model_name": "Falcon-40B-instruct",
            "model_description": "Large decoder-only Transformer (40B) with instruction tuning for improved conversational/instruction following.",
            "model_size": "40B",
            "architecture_type": "Transformer (decoder)",
            "training_data": "Pretraining on large web corpora; instruction tuning (dataset specifics not reported in this paper).",
            "reasoning_method": "Zero-shot and in-context learning (no explicit symbolic enhancements).",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GLoRE",
            "benchmark_description": "Unified logical reasoning evaluation across multiple datasets.",
            "task_type": "MRC, NLI, TF",
            "performance_metric": "Accuracy",
            "performance_value": "Zero-shot average 32.28%; examples: ProofWriter 53.33%, RuleTaker 56.11%, many MRC scores ~20-30%",
            "comparison_with_baseline": "Comparable to LLaMA and below RoBERTa-base on several tasks; few-shot improves performance modestly (0-shot 32.28% -&gt; 5-shot 35.72%).",
            "key_findings": "Instruction-tuned Falcon matches LLaMA in baseline reasoning performance but does not close the gap to reasoning-enhanced models without further training signals.",
            "limitations": "Struggles with MRC logical questions in zero-shot; sensitive to data distributions and lacks specialized reasoning training in reported experiments.",
            "uuid": "e7061.2",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Mixtral-8x7B",
            "name_full": "Mixtral (mixture-of-experts) 8x7B",
            "brief_description": "A mixture-of-experts (MoE) model configuration evaluated on GLoRE that outperforms same‑class decoder LLMs like LLaMA and Falcon, showing effectiveness of MoE for some reasoning tasks.",
            "citation_title": "Mixtral of experts",
            "mention_or_use": "use",
            "model_name": "Mixtral-8x7B",
            "model_description": "Mixture-of-experts Transformer architecture (8 experts × 7B experts configuration referenced) aimed at efficiency and capacity scaling.",
            "model_size": "Approx. 56B total capacity (8x7B experts) as reported (referred to as Mixtral-8x7B)",
            "architecture_type": "Transformer + Mixture-of-Experts (MoE)",
            "training_data": "Pretraining corpora and instruction-style tuning details not specified in paper.",
            "reasoning_method": "Zero-shot / in-context learning; benefits attributed to MoE architecture rather than explicit logical training.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GLoRE",
            "benchmark_description": "Comprehensive logical reasoning evaluation across multiple datasets.",
            "task_type": "MRC, NLI, TF",
            "performance_metric": "Accuracy",
            "performance_value": "Zero-shot average 42.52%; examples: LogiQA 45.29%, ReClor 48.92%, ProofWriter 44.80%",
            "comparison_with_baseline": "Outperforms LLaMA and Falcon, indicating MoE scaling benefits; still below top reasoning-enhanced models like QwQ-32B.",
            "key_findings": "Mixture-of-experts design yields better zero-shot reasoning performance compared to similarly situated decoder models.",
            "limitations": "Although improved relative to LLaMA/Falcon, still sensitive to task distribution and lacks explicit reasoning-specific training described here.",
            "uuid": "e7061.3",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (instruct-tuned GPT family)",
            "brief_description": "OpenAI's instruction-tuned conversational model (GPT-3.5 family) evaluated zero-shot and few-shot on GLoRE; shows intermediate performance and benefits from in-context examples.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_description": "Instruction-tuned conversational Transformer model from OpenAI (GPT-3.5 family reference in literature; exact internals not detailed in this paper).",
            "model_size": null,
            "architecture_type": "Transformer (decoder, instruction-tuned)",
            "training_data": "Large-scale pretraining and instruction tuning (details not enumerated in this paper).",
            "reasoning_method": "Zero-shot and in-context learning; often evaluated with few-shot demonstrations; chain-of-thought discussed but not specifically used to claim causal improvement.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GLoRE",
            "benchmark_description": "Unified benchmark including MRC/NLI/TF datasets testing strict logical reasoning.",
            "task_type": "MRC, NLI, TF",
            "performance_metric": "Accuracy",
            "performance_value": "Zero-shot average 52.10%; few-shot: 1-shot 55.85%, 2-shot 57.43%, 5-shot 60.32%; examples: ConTRoL 58.45%, ProofWriter 53.95%",
            "comparison_with_baseline": "Outperforms RoBERTa-base on many MRC and NLI tasks but is outperformed by GPT-4 and top reasoning-enhanced models (o1 mini, DeepSeek R1, QwQ-32B) on average.",
            "key_findings": "ChatGPT benefits from in-context examples (ICL) and shows reasonable logical reasoning ability, but still trails specialized reasoning-enhanced LLMs.",
            "limitations": "Performance varies across datasets (sensitive to distribution); relies on superficial patterns rather than consistent human-like logical generalization per authors' analysis.",
            "uuid": "e7061.4",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "OpenAI's advanced multi-modal large language model that achieves strong zero-shot and few-shot performance on many GLoRE tasks but shows sensitivity to dataset distribution.",
            "citation_title": "Gpt-4 technical report",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large Transformer-based model from OpenAI (detailed architecture and size not specified in this paper); excels in zero-shot logical reasoning on several datasets.",
            "model_size": null,
            "architecture_type": "Transformer (decoder/multi-modal design; instruction-tuned)",
            "training_data": "Pretraining on large web and proprietary corpora; instruction tuning and safety/alignments as per OpenAI (details not enumerated here).",
            "reasoning_method": "Zero-shot and in-context learning; can be prompted with few-shot examples to improve accuracy; chain-of-thought discussed in related literature but not claimed as causal here.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GLoRE",
            "benchmark_description": "Comprehensive logical reasoning benchmark covering multiple datasets and task types.",
            "task_type": "MRC, NLI, TF",
            "performance_metric": "Accuracy",
            "performance_value": "Zero-shot average 66.34%; few-shot: 1-shot 70.31%, 2-shot 71.44%, 5-shot 75.83%; dataset examples: ReClor 87.20%, LogiQA 72.25%, LogiQA22 58.49%, FraCaS 75.35%, ProofWriter 59.66%",
            "comparison_with_baseline": "Substantially outperforms RoBERTa-base and most open-source vanilla LLMs; however QwQ-32B and some reasoning-enhanced models outperform GPT-4 on average (e.g., QwQ-32B avg 78.95%).",
            "key_findings": "GPT-4 attains near-human performance on some datasets (e.g., ReClor) and benefits from few-shot prompting; nevertheless shows inconsistent performance across datasets, indicating distributional sensitivity.",
            "limitations": "Significant sensitivity to dataset distribution (e.g., large drop between LogiQA and LogiQA22); inconsistent NLI/TF results on some specialized tasks (e.g., HELP 46.01%).",
            "uuid": "e7061.5",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "o1 mini",
            "name_full": "OpenAI o1 mini",
            "brief_description": "A commercial instruction-following model from OpenAI evaluated on GLoRE; shows strong performance and competitive results vs GPT-4 on multiple tasks.",
            "citation_title": "Openai o1 system card",
            "mention_or_use": "use",
            "model_name": "o1 mini",
            "model_description": "OpenAI commercial instruction-tuned model variant; designed for instruction-following and reasoning workloads (technical specifics in system card).",
            "model_size": null,
            "architecture_type": "Transformer (instruction-tuned)",
            "training_data": "Instruction tuning and likely RLHF per OpenAI system card (details not fully enumerated in this paper).",
            "reasoning_method": "Instruction tuning and training optimizations for reasoning (no external symbolic tools reported).",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GLoRE",
            "benchmark_description": "Aggregated logical reasoning benchmark spanning MRC, NLI, and TF datasets.",
            "task_type": "MRC, NLI, TF",
            "performance_metric": "Accuracy",
            "performance_value": "Zero-shot average 72.44%; examples: LogiQA 69.35%, LogiQA22 59.84%, ConTRoL 75.12%, HELP 63.69%, ProofWriter 75.33%",
            "comparison_with_baseline": "Outperforms GPT-4 on several NLI/TF metrics in this evaluation (and is competitive on MRC); overall better than many vanilla LLMs and RoBERTa baseline.",
            "key_findings": "o1 mini is a strong commercial reasoning-capable model that narrows the gap to leading open-source reasoning models on GLoRE.",
            "limitations": "Still sensitive to dataset distribution; not uniformly superior across all NLI datasets (some tasks show lower performance).",
            "uuid": "e7061.6",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "DeepSeek R1",
            "name_full": "DeepSeek R1",
            "brief_description": "A reasoning-enhanced model that uses reinforcement learning incentives to improve reasoning capability; demonstrates strong performance on GLoRE and balanced results across many tasks.",
            "citation_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
            "mention_or_use": "use",
            "model_name": "DeepSeek R1",
            "model_description": "Open-source reasoning-enhanced LLM incorporating reinforcement-learning-based techniques to incentivize reasoning behavior.",
            "model_size": null,
            "architecture_type": "Transformer with reinforcement learning (training/finetuning augmentation)",
            "training_data": "Not exhaustively listed; trained/finetuned with reinforcement learning incentives targeting reasoning benchmarks (authors reference use of RL frameworks).",
            "reasoning_method": "Reinforcement learning incentives to encourage reasoning (rewarding reasoning-congruent outputs); evaluated zero-shot on GLoRE.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GLoRE",
            "benchmark_description": "Comprehensive logical reasoning benchmark (MRC, NLI, TF datasets).",
            "task_type": "MRC, NLI, TF",
            "performance_metric": "Accuracy",
            "performance_value": "Zero-shot average 75.14%; examples: LogiQA 76.22%, LogiQA22 71.63%, AR-LSAT 90.01%, ProofWriter 80.51%",
            "comparison_with_baseline": "Substantially outperforms vanilla LLaMA/Falcon and RoBERTa baseline; close to QwQ-32B on average but below QwQ-32B's top performance.",
            "key_findings": "Reinforcement-learning-based training yields substantial gains in logical reasoning robustness and average performance across GLoRE.",
            "limitations": "Still shows sensitivity to dataset-specific reasoning phenomena; uneven performance on some NLI subsets (e.g., HELP-type monotonicity tasks).",
            "uuid": "e7061.7",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "QwQ-32B",
            "name_full": "QwQ-32B",
            "brief_description": "A 32B-parameter reasoning-enhanced open model employing reinforcement learning or specialized training methodology, achieving state-of-the-art results on GLoRE in this work.",
            "citation_title": "Qwq-32b: Embracing the power of reinforcement learning",
            "mention_or_use": "use",
            "model_name": "QwQ-32B",
            "model_description": "Open-source 32B Transformer-based LLM with specialized reinforcement-learning training/finetuning aiming to improve logical reasoning and generalization.",
            "model_size": "32B",
            "architecture_type": "Transformer with reinforcement learning / reasoning-enhanced training",
            "training_data": "Trained/finetuned with reinforcement-learning-based objectives and likely on reasoning-oriented datasets (paper hypothesizes RL framework contributes to gains); specific datasets not enumerated beyond evaluation sets.",
            "reasoning_method": "Reinforcement learning incentives / reasoning-targeted fine-tuning (no external symbolic engine reported).",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "GLoRE",
            "benchmark_description": "Unified logical reasoning benchmark covering MRC, NLI, and TF tasks with 12 constituent datasets and ~72,848 instances.",
            "task_type": "MRC, NLI, TF (multi-step symbolic and textual entailment challenges)",
            "performance_metric": "Accuracy",
            "performance_value": "Zero-shot average 78.95%; dataset highlights: LogiQA 85.70%, ReClor 93.76%, AR-LSAT 92.35%, LogiQA22 86.30%, ProofWriter 82.40%",
            "comparison_with_baseline": "Outperforms GPT-4 (avg +12.61 percentage points: 78.95% vs. 66.34%) and RoBERTa baseline; sets new SOTA on multiple MRC and TF datasets in this evaluation.",
            "key_findings": "Reinforcement learning and specialized training led to substantial improvements in multi-choice MRC and true-or-false proof tasks, yielding state-of-the-art average performance on GLoRE.",
            "limitations": "Less generalizable on certain fine-grained NLI phenomena (e.g., HELP monotonicity: 61.53% vs. o1 mini 63.69%); uneven performance across NLI tasks suggests brittleness on monotonicity/negation reasoning.",
            "uuid": "e7061.8",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "In-context learning (ICL)",
            "name_full": "In-context learning",
            "brief_description": "A prompting technique where few labeled examples are included in the prompt to the LLM; used in this paper to study few-shot improvements on logical reasoning tasks.",
            "citation_title": "A survey on in-context learning (2023)",
            "mention_or_use": "use",
            "model_name": "In-context learning (ICL)",
            "model_description": "Prompting paradigm rather than a model: models are given 1-5 example Q/A pairs (demonstrations) in the prompt to induce task behavior.",
            "model_size": null,
            "architecture_type": "Prompting / demonstration-based (not an architecture)",
            "training_data": "N/A (uses examples drawn from the same datasets for prompts).",
            "reasoning_method": "Few-shot demonstrations appended to prompts (1-shot, 2-shot, 5-shot experiments performed).",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "GLoRE (few-shot evaluation reported in Table 3)",
            "benchmark_description": "Few-shot evaluation across the aggregated GLoRE benchmark to measure adaptation with demonstrations.",
            "task_type": "MRC, NLI, TF (as tested via few-shot prompting)",
            "performance_metric": "Accuracy",
            "performance_value": "Reported few-shot gains: GPT-4 0-shot 66.34% -&gt; 5-shot 75.83% (+9.49 pp); ChatGPT 52.10% -&gt; 60.32% (+8.22 pp); LLaMA modest gains 32.34% -&gt; 39.62%.",
            "comparison_with_baseline": "In-context learning improves accuracy for large models but authors argue improvements reflect statistical adaptation (superficial pattern learning) rather than deeper causal improvements in reasoning.",
            "key_findings": "ICL yields meaningful accuracy gains, especially for stronger models (GPT-4), but improvements may be due to pattern adaptation rather than robust logical generalization.",
            "limitations": "Authors caution that ICL improvements do not necessarily indicate true reasoning capability and that performance remains sensitive to dataset distribution.",
            "uuid": "e7061.9",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Chain-of-thought (CoT)",
            "name_full": "Chain-of-thought prompting",
            "brief_description": "A prompting strategy that elicits step-by-step intermediate reasoning traces from LLMs; discussed in paper as correlated with outputs but not causally driving true logical reasoning.",
            "citation_title": "How likely do llms with cot mimic human reasoning?",
            "mention_or_use": "mention",
            "model_name": "Chain-of-thought prompting",
            "model_description": "A prompting technique that requests intermediate reasoning steps (natural language) from the model to improve multi-step reasoning performance.",
            "model_size": null,
            "architecture_type": "Prompting technique",
            "training_data": "N/A (prompting method applied at inference time; some models are trained/tuned with chain-of-thought data elsewhere but not detailed here).",
            "reasoning_method": "Generate intermediate natural-language reasoning steps (chain-of-thought); can be combined with self-consistency or other sampling methods (discussed in related work).",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "GLoRE (discussed as a relevant technique but not used as the primary experimental variable here)",
            "benchmark_description": "GLoRE can be used with reasoning prompts, though the paper emphasizes dataset sensitivity and that CoT correlation does not imply causal improvement.",
            "task_type": "Multi-step logical reasoning / MRC / proof tasks when invoked",
            "performance_metric": "Accuracy (where applied in literature); not isolated in this paper for causal impact",
            "performance_value": "Not directly quantified in this paper; authors reference external work showing correlations but caution against interpreting CoT as causal for reasoning gains.",
            "comparison_with_baseline": "Cited literature suggests CoT can correlate with improved outputs, but this paper does not present an isolated ablation showing causal improvement on GLoRE.",
            "key_findings": "CoT-style prompts may change outputs and correlate with better answers, but improvements are argued to often reflect pattern exploitation rather than genuine mechanistic reasoning.",
            "limitations": "Authors cite work showing CoT's effects are correlational; therefore CoT may not yield robust, generalizable logical reasoning across distributions.",
            "uuid": "e7061.10",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Qwq-32b: Embracing the power of reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Transformers as soft reasoners over language",
            "rating": 2
        },
        {
            "paper_title": "Proofwriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2
        },
        {
            "paper_title": "Logiqa: A challenge dataset for machine reading comprehension with logical reasoning",
            "rating": 2
        },
        {
            "paper_title": "Reclor: A reading comprehension dataset requiring logical reasoning",
            "rating": 2
        },
        {
            "paper_title": "How likely do llms with cot mimic human reasoning?",
            "rating": 2
        },
        {
            "paper_title": "A survey on in-context learning",
            "rating": 1
        },
        {
            "paper_title": "Roberta: A robustly optimized bert pretraining approach.",
            "rating": 1
        },
        {
            "paper_title": "Mixtral of experts",
            "rating": 1
        }
    ],
    "cost": 0.0187795,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>GLoRE: Evaluating Logical Reasoning of Large Language Models</h1>
<p>Hanmeng Liu ${ }^{1}$, Zhiyang Teng ${ }^{3}$ Ruoxi Ning ${ }^{2}$, Yiran Ding ${ }^{2}$, Xiulai Li ${ }^{1}$, Xiaozhang Liu ${ }^{1}$, and Yue Zhang ${ }^{2}$<br>${ }^{1}$ Hainan University, Haikou, Hainan<br>${ }^{2}$ Westlake University, Hangzhou, Zhejiang, China<br>${ }^{3}$ ByteDance SG</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have shown significant general language understanding abilities. However, there has been a scarcity of attempts to assess the logical reasoning capacities of these LLMs, an essential facet of natural language understanding. To encourage further investigation in this area, we introduce GLoRE, a General Logical Reasoning Evaluation platform that not only consolidates diverse datasets but also standardizes them into a unified format suitable for evaluating large language models across zero-shot and few-shot scenarios. Our experimental results show that compared to the performance of humans and supervised fine-tuning models, the logical reasoning capabilities of large reasoning models, such as OpenAI's o1 mini, DeepSeek R1 and QwQ-32B, have seen remarkable improvements, with QwQ-32B achieving the highest benchmark performance to date. GLoRE is designed as a living project that continuously integrates new datasets and models, facilitating robust and comparative assessments of model performance in both commercial and Huggingface communities. It garnered over 300 citations upon its release.</p>
<p>Keywords: Large Language Model $\cdot$ Large Reasoning Model $\cdot$ Logical reasoning $\cdot$ Natural Language Inference.</p>
<h2>1 Introduction</h2>
<p>Large Language Models (LLMs) [50, 67], especially reasoning language models [18, 51] demonstrate advanced capabilities in complex reasoning tasks and show significant adaptability and versatility across various applications, from simple everyday tasks to specialized domains such as coding, mathematics, law, medicine, and finance [11, 22, 34, 37, 76]. Quantitative evaluation of LLM reasoning has thus become a very important task. To this end, existing work has considered mathematical reasoning [15, 26], algorithmic problem solving [9, 58], and knowledge-driven reasoning $[25,73]$.</p>
<p>Logical reasoning is a cornerstone of human intelligence and has been a central focus in artificial intelligence research since its inception [16,29,33]. However, evaluating verbal reasoning turned out to be too difficult in the 1950s due to</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Instruction and question format for logical reading comprehension tasks.
insufficient natural language understanding (NLU) technologies, and thus AI researchers focused on formal logical reasoning instead [29, 48, 49]. Since the 2010s, NLU has witnessed huge advances, where reading comprehension [8, 21] and natural language inference $[7,74]$ tasks were solved with high accuracies, which made verbal reasoning evaluation feasible [43, 80]. Figure 1 illustrates an example of logical reasoning in reading comprehension. To address such questions, LLMs must engage in multi-step, algorithmic, symbolic reasoning. This makes logical reasoning an ideal testbed for evaluating LLMs' ability to process complex natural language information accurately, robustly, and logically.</p>
<p>To this end, we introduce the General Logical Reasoning Evaluation (GLoRE) benchmark, designed to assess instruction-tuned LLMs on various logical reasoning tasks. GLoRE evaluates the strengths and limitations of LLMs in this domain, similar to how GLUE [71] and Super-GLUE [70] benchmark natural language understanding. GLoRE includes three types of logical reasoning tasks: Multi-choice Reading Comprehension [35], Natural Language Inference (NLI) [17], and True-or-False (Yes-or-No) Questions [13]. These tasks encompass a wide range of logical reasoning phenomena, with high-quality datasets that remain challenging for pre-trained language models [13, 27, 32]. In total, GLoRE covers 12 datasets with 72,848 instances. Since its release in 2023, GLoRE has been used for evaluating language models, receiving over 300 citations on ArXiv.</p>
<p>Using GLoRE, we report the logical reasoning capabilities of commercial models such as GPT-4 and OpenAI o1 [51], as well as popular open-source models such as LLaMA [67], Falcon [1], Mistral [30], DeepSeek R1 [18], and QwQ-32B [66]. We test their instruction-following and problem-solving abilities in logical reasoning tasks. Results show that while commercial LLMs like GPT-4 still excel in zero-shot settings and approach human performance on specific datasets like ReClor, open-source models like QwQ-32B now rival or even surpass commercial counterparts in key tasks, achieving state-of-the-art results on multiple benchmarks. This underscores rapid advancements in open-source LLMs, narrowing the performance gap with commercial models. However, performance varies significantly across datasets, indicating sensitivity to data distribution. This sensitivity is further confirmed by observations that in-context</p>
<p>learning and supervised fine-tuning primarily enhance LLM performance on specific test distributions, demonstrating their strong learning ability. While LLMs show promise in logical reasoning, their robustness to data distribution variations remains a challenge, highlighting the need for further improvement.</p>
<h1>2 Related Work</h1>
<p>Logical reasoning with natural language. Tapping into logical reasoning capabilities represents a holistic endeavour in natural language understanding (NLU). A variety of methods have been explored to realize this objective, including symbolic systems [45, 47, 55], fine-tuning of language models [28, 41, 71, 78], and hybrid approaches combining neural and symbolic elements [36, 59, 60].</p>
<p>The recent introduction of evaluation datasets, notably LogiQA [43] and Reclor [80], has reinvigorated the focus on logical reasoning in NLP research. Logical reasoning is now leveraged in numerous probing tasks over large Pretrained Language Models (PLMs) and applied to downstream tasks such as question-answering and dialogue systems [6, 63]. Despite these advancements, the aspiration to emulate human-like logical reasoning capabilities within NLU systems remains a significant challenge for traditional models [27, 43]. In this study, our goal is not only to quantitatively evaluate the capability of Large Language Models (LLMs) in addressing the previously mentioned challenge but also to underscore the significance of our work in providing a validated platform for enhancing various reasoning methods with our data.</p>
<p>LLM reasoning evaluation. Despite progress in evaluating LLMs for specific reasoning tasks like arithmetic [57] and commonsense [4], a yawning gap exists in comprehensively assessing their logical reasoning. While LLMs excel at specific tasks like arithmetic reasoning [57], they face challenges in complex areas like multi-step reasoning [23] and abstract scenarios [24]. ChatGPT exhibits strengths in chat-specific reasoning and some commonsense domains [4, 53], but struggles with tasks requiring longer chains of inference [4]. Other LLMs like FLAN-T5 [12], LLaMA [67], and PaLM [2] show potential in general deductive reasoning [61], while InstructGPT and Codex excel in specialized domains like medical reasoning [38]. Despite these advances, limitations in data bias [52], and complex reasoning tasks necessitate further research and optimization to fully unlock the reasoning potential of LLMs [77].</p>
<p>Big-Bench Hard (BBH) [64] isolates 23 most challenging tasks from BIGBench [3]. These tasks comprise general language understanding, arithmetic and algorithmic reasoning, and logical deduction. However, in comparison to our benchmark, the data size of the logical reasoning section in BBH is very small. HumanEval [10] serves as a hand-written evaluation set for coding. The programming problems included are designed to assess language comprehension, reasoning, algorithms, and simple mathematics. While similar to logical reasoning in that code generation necessitates complex reasoning skills, GLoRE differs in presenting logical reasoning problems via natural language prompts.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: right;">Size</th>
<th style="text-align: left;">Target</th>
<th style="text-align: left;">Dataset</th>
<th style="text-align: right;">Size</th>
<th style="text-align: left;">Target</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LogiQA 2.0 test</td>
<td style="text-align: right;">1,572</td>
<td style="text-align: left;">4-way multi-choice</td>
<td style="text-align: left;">ConTRoL</td>
<td style="text-align: right;">805</td>
<td style="text-align: left;">E, C, N</td>
</tr>
<tr>
<td style="text-align: left;">LogiQA 2.0 zh test</td>
<td style="text-align: right;">1,594</td>
<td style="text-align: left;">4-way multi-choice</td>
<td style="text-align: left;">HELP</td>
<td style="text-align: right;">35,891</td>
<td style="text-align: left;">E, C, N</td>
</tr>
<tr>
<td style="text-align: left;">ReClor dev</td>
<td style="text-align: right;">500</td>
<td style="text-align: left;">4-way multi-choice</td>
<td style="text-align: left;">TaxiNLI test</td>
<td style="text-align: right;">10,071</td>
<td style="text-align: left;">E, C, N</td>
</tr>
<tr>
<td style="text-align: left;">AR-LSAT test</td>
<td style="text-align: right;">230</td>
<td style="text-align: left;">5-way multi-choice</td>
<td style="text-align: left;">NaN-NLI</td>
<td style="text-align: right;">259</td>
<td style="text-align: left;">E, C, N</td>
</tr>
<tr>
<td style="text-align: left;">LogiQA22</td>
<td style="text-align: right;">1,354</td>
<td style="text-align: left;">4-way multi-choice</td>
<td style="text-align: left;">FraCas</td>
<td style="text-align: right;">346</td>
<td style="text-align: left;">Yes, No, Neutral</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">RuleTaker dev</td>
<td style="text-align: right;">10,068</td>
<td style="text-align: left;">Yes, No</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">ProofWriter dev</td>
<td style="text-align: right;">10,158</td>
<td style="text-align: left;">Yes, No</td>
</tr>
</tbody>
</table>
<p>Table 1. Data statistics. ("E": entailment; "C": contradiction; "N": neutral.)</p>
<p>ARB [62] is a benchmark for advanced reasoning over multiple fields like mathematics, physics, biology, chemistry, and law. Similar to GLoRE, it introduces a challenging subset of math and physics problems that require advanced symbolic reasoning. However, the benchmark constraints its problem on the above subjects with domain knowledge, not general logical reasoning questions, which is the focus of GLoRE.</p>
<h1>3 The GLoRE Dataset</h1>
<p>As mentioned in the introduction, GLoRE contains three NLU tasks: Multichoice Reading Comprehension, NLI, and Yes-or-No. First, Multi-choice reading comprehension [35] is essential in verbal reasoning tests, which cover abundant high-quality logical reasoning problems in the wild. Second, Unlike multi-choice reading comprehension, NLI [17] is more general and centric on entailment relations in a simpler task format, which is a fundamental task for evaluating reasoning abilities [19,54]. Third, the Yes-or-No reasoning task [13] is a combination of question-answering and textual entailment, which can serve as a playground for testing models' reasoning abilities [14, 65]. The data statistics are shown in Table 1.</p>
<h3>3.1 Multi-choice Reading Comprehension (MRC)</h3>
<p>Within the standard multi-choice reading comprehension (MRC) task setting, a system is presented with a passage and a question, and the objective is to choose the most suitable answer from a set of candidate responses. Particularly, GLoRE contains five such datasets:
LogiQA [43] is a logical MRC dataset derived from the Chinese Civil Service Examination, translated into English, and made available in both Chinese and English versions. We adopt LogiQA 2.0 [40] and use both the English (LogiQA 2.0) and Chinese (LogiQA 2.0 zh ) test sets for our evaluation.</p>
<p>ReClor [80] comprises question-answering examples from the LSAT exams designed to assess human logical reasoning abilities. We use the development set for our testing as the test set does not provide gold labels.
AR-LSAT [72] is a dataset of analytical reasoning questions from the Law School Admission Test. Each question contains five options rather than four.</p>
<p>LogiQA22 is collected and processed according to the LogiQA 2.0 format after ChatGPT was released. It incorporates the newly released Chinese Civil Servant Exams from 2022, which are not included in the original LogiQA dataset.</p>
<h1>3.2 Natural Language Inference (NLI)</h1>
<p>NLI is the task of determining the logical relationship between a hypothesis and a premise. The typical scheme involves text classification, where the model selects one of three labels: entailment, contradiction, and neutral.
ConTRoL [39] is an NLI dataset that offers an in-depth examination of contextual reasoning within the NLI framework. Approximately $36.2 \%$ of premisehypothesis pairs fall under the category of logical reasoning in this dataset. We choose the logical reasoning portion for our evaluation.
HELP [79] is an NLI dataset emphasizing monotonicity reasoning, a crucial concept in Natural Logic [46]. We use the training set for our evaluation.
TaxiNLI [31] is an NLI dataset that has been re-annotated based on MNLI [75], with categories include logical categories such as connectives, mathematical reasoning, and deduction.
NaN-NLI [68] is a test suite designed to probe the capabilities of NLP models in capturing sub-clausal negation. The successful handling of sub-clausal negation can be seen as a strong indicator of a model's logical reasoning capacity.</p>
<h3>3.3 True-or-False (Yes-or-No) Questions (TF)</h3>
<p>FraCaS test suite [56] presents complex entailment problems involving multipremised contexts as a three-way classification task. The ability to determine entailment relationships in this context is closely tied to logical reasoning.
RuleTaker [14] dataset is a synthetic creation designed to examine the reasoning ability of transformer models [69] over natural language rules. This task explicitly targets logical reasoning by asking models to reason over a set of rules and facts to generate true-or-false responses as output.
ProofWriter [65] dataset generates sets of facts and rules, each followed by questions, which can be proven true or false using proofs of various depths.</p>
<h2>4 Experiments</h2>
<p>We employ GLoRE to assess the logical reasoning capabilities across different categories of language models, including traditional pre-trained models and reasoning-enhanced LLMs, both open-source and proprietary. We conduct a comprehensive comparative analysis of their performance against human benchmarks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">MRC</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">NLI</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">TF</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Dataset</td>
<td style="text-align: center;">LQ</td>
<td style="text-align: center;">LQ zh</td>
<td style="text-align: center;">RC</td>
<td style="text-align: center;">AL</td>
<td style="text-align: center;">LQ22</td>
<td style="text-align: center;">CT</td>
<td style="text-align: center;">HL</td>
<td style="text-align: center;">TN</td>
<td style="text-align: center;">NN</td>
<td style="text-align: center;">FC</td>
<td style="text-align: center;">RT</td>
<td style="text-align: center;">PW</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Human avg.</td>
<td style="text-align: center;">86.00</td>
<td style="text-align: center;">88.00</td>
<td style="text-align: center;">63.00</td>
<td style="text-align: center;">56.00</td>
<td style="text-align: center;">83.00</td>
<td style="text-align: center;">87.00</td>
<td style="text-align: center;">81.00</td>
<td style="text-align: center;">97.00</td>
<td style="text-align: center;">94.00</td>
<td style="text-align: center;">92.00</td>
<td style="text-align: center;">84.00</td>
<td style="text-align: center;">82.00</td>
<td style="text-align: center;">82.75</td>
</tr>
<tr>
<td style="text-align: center;">Human Ceiling</td>
<td style="text-align: center;">95.00</td>
<td style="text-align: center;">96.00</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">91.00</td>
<td style="text-align: center;">99.00</td>
<td style="text-align: center;">94.00</td>
<td style="text-align: center;">95.00</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">97.00</td>
<td style="text-align: center;">95.00</td>
<td style="text-align: center;">93.00</td>
<td style="text-align: center;">96.25</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">48.76</td>
<td style="text-align: center;">35.64</td>
<td style="text-align: center;">55.01</td>
<td style="text-align: center;">30.90</td>
<td style="text-align: center;">33.22</td>
<td style="text-align: center;">48.76</td>
<td style="text-align: center;">39.47</td>
<td style="text-align: center;">49.91</td>
<td style="text-align: center;">90.02</td>
<td style="text-align: center;">32.01</td>
<td style="text-align: center;">53.50</td>
<td style="text-align: center;">55.92</td>
<td style="text-align: center;">47.76</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">19.31</td>
<td style="text-align: center;">26.35</td>
<td style="text-align: center;">17.81</td>
<td style="text-align: center;">17.98</td>
<td style="text-align: center;">18.41</td>
<td style="text-align: center;">24.10</td>
<td style="text-align: center;">32.26</td>
<td style="text-align: center;">41.91</td>
<td style="text-align: center;">47.29</td>
<td style="text-align: center;">40.00</td>
<td style="text-align: center;">48.89</td>
<td style="text-align: center;">53.78</td>
<td style="text-align: center;">32.34</td>
</tr>
<tr>
<td style="text-align: center;">Falcon</td>
<td style="text-align: center;">23.21</td>
<td style="text-align: center;">19.77</td>
<td style="text-align: center;">26.77</td>
<td style="text-align: center;">12.70</td>
<td style="text-align: center;">17.33</td>
<td style="text-align: center;">16.13</td>
<td style="text-align: center;">28.49</td>
<td style="text-align: center;">44.66</td>
<td style="text-align: center;">53.31</td>
<td style="text-align: center;">35.57</td>
<td style="text-align: center;">56.11</td>
<td style="text-align: center;">53.33</td>
<td style="text-align: center;">32.28</td>
</tr>
<tr>
<td style="text-align: center;">Mixtral-8x7B</td>
<td style="text-align: center;">45.29</td>
<td style="text-align: center;">36.81</td>
<td style="text-align: center;">48.92</td>
<td style="text-align: center;">41.40</td>
<td style="text-align: center;">38.97</td>
<td style="text-align: center;">50.84</td>
<td style="text-align: center;">33.27</td>
<td style="text-align: center;">40.86</td>
<td style="text-align: center;">50.13</td>
<td style="text-align: center;">32.08</td>
<td style="text-align: center;">46.84</td>
<td style="text-align: center;">44.80</td>
<td style="text-align: center;">42.52</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">52.37</td>
<td style="text-align: center;">53.18</td>
<td style="text-align: center;">57.38</td>
<td style="text-align: center;">51.49</td>
<td style="text-align: center;">38.44</td>
<td style="text-align: center;">58.45</td>
<td style="text-align: center;">42.13</td>
<td style="text-align: center;">57.30</td>
<td style="text-align: center;">56.59</td>
<td style="text-align: center;">49.13</td>
<td style="text-align: center;">54.74</td>
<td style="text-align: center;">53.95</td>
<td style="text-align: center;">52.10</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">72.25</td>
<td style="text-align: center;">70.56</td>
<td style="text-align: center;">87.20</td>
<td style="text-align: center;">73.12</td>
<td style="text-align: center;">58.49</td>
<td style="text-align: center;">56.40</td>
<td style="text-align: center;">46.01</td>
<td style="text-align: center;">60.08</td>
<td style="text-align: center;">76.74</td>
<td style="text-align: center;">75.35</td>
<td style="text-align: center;">60.19</td>
<td style="text-align: center;">59.66</td>
<td style="text-align: center;">66.34</td>
</tr>
<tr>
<td style="text-align: center;">o1 mini</td>
<td style="text-align: center;">69.35</td>
<td style="text-align: center;">74.18</td>
<td style="text-align: center;">80.57</td>
<td style="text-align: center;">79.23</td>
<td style="text-align: center;">59.84</td>
<td style="text-align: center;">75.12</td>
<td style="text-align: center;">63.69</td>
<td style="text-align: center;">81.41</td>
<td style="text-align: center;">71.55</td>
<td style="text-align: center;">63.07</td>
<td style="text-align: center;">75.90</td>
<td style="text-align: center;">75.33</td>
<td style="text-align: center;">72.44</td>
</tr>
<tr>
<td style="text-align: center;">DeepSeek R1</td>
<td style="text-align: center;">76.22</td>
<td style="text-align: center;">81.49</td>
<td style="text-align: center;">77.88</td>
<td style="text-align: center;">90.01</td>
<td style="text-align: center;">71.63</td>
<td style="text-align: center;">78.37</td>
<td style="text-align: center;">62.05</td>
<td style="text-align: center;">75.74</td>
<td style="text-align: center;">72.58</td>
<td style="text-align: center;">59.96</td>
<td style="text-align: center;">75.29</td>
<td style="text-align: center;">80.51</td>
<td style="text-align: center;">75.14</td>
</tr>
<tr>
<td style="text-align: center;">QwQ-32B</td>
<td style="text-align: center;">85.70</td>
<td style="text-align: center;">73.33</td>
<td style="text-align: center;">93.76</td>
<td style="text-align: center;">92.35</td>
<td style="text-align: center;">86.30</td>
<td style="text-align: center;">73.87</td>
<td style="text-align: center;">61.53</td>
<td style="text-align: center;">81.96</td>
<td style="text-align: center;">75.59</td>
<td style="text-align: center;">62.45</td>
<td style="text-align: center;">78.10</td>
<td style="text-align: center;">82.40</td>
<td style="text-align: center;">78.95</td>
</tr>
</tbody>
</table>
<p>Table 2. LLMs' performance on the GLoRE benchmark. $L Q$ : LogiQA 2.0, $R C$ : ReClor, $A L$ : AR-LSAT, $C T$ : ConTRoL, $H L$ : HELP, $T N$ : TaxiNLI, $N N$ : NaN-NLI, $F C$ : FraCas, $R T$ : RuleTaker, $P W$ : ProofWriter. All results are in $\%$, the best ones are in bold, and the second best ones are in underline.</p>
<h1>4.1 Experimental Settings</h1>
<p>We adopted RoBERTa-base [44] as a baseline, fine-tuning it on the training set over five epochs for each dataset. The community models selected for comparison include Falcon-40b-instruct [1], LLaMA-30b-supercot [67] Mixtral8x7b, DeepSeek R1 [18] and QwQ-32B [66]. For OpenAI models, we choose ChatGPT, GPT-4 and o1 mini [51].</p>
<p>Our evaluation metrics consisted of classification accuracy scores. Additionally, we utilized reported accuracies for datasets where human performance data was available and recorded both the average and peak performance of human participants to establish a human baseline. For the LogiQA22 dataset, we engaged five co-authors as test subjects and computed their accuracy based on 150 test examples.</p>
<h3>4.2 Main Results</h3>
<p>Zero-shot results. Table 2 summarizes the zero-shot evaluation results. The first block shows human performance. The second block presents RoBERTabase's supervised fine-tuning results. With 125M parameters, RoBERTa-base achieves $48.76 \%$ and $33.22 \%$ accuracy on LogiQA 2.0 and LogiQA22, respectively, lagging behind human performance. It performs better on NLI and TF tasks than MRC tasks, likely due to output ambiguities. On NaN-NLI, RoBERTa achieves $90.02 \%$ accuracy, matching human performance, possibly due to learning superficial patterns from rule-based negation data. On ProofWriter, RoBERTabase scores $55.92 \%$, indicating potential for specific logical reasoning tasks.</p>
<p>The third block shows zero-shot results for LLaMA, Falcon, and Mixtral. LLaMA and Falcon perform similarly ( $32.34 \%$ vs. $32.28 \%$ ), suggesting comparable reasoning capabilities despite LLaMA-30B's smaller size. Both underperform RoBERTa-base on most tasks, except Falcon on RT. On MRC tasks, their</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">$\mathbf{0}$-shot</th>
<th style="text-align: center;">$\mathbf{1}$-shot</th>
<th style="text-align: center;">$\mathbf{2}$-shot</th>
<th style="text-align: center;">$\mathbf{5}$-shot</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LLaMA</td>
<td style="text-align: center;">32.34</td>
<td style="text-align: center;">32.89</td>
<td style="text-align: center;">35.03</td>
<td style="text-align: center;">39.62</td>
</tr>
<tr>
<td style="text-align: left;">Falcon</td>
<td style="text-align: center;">32.28</td>
<td style="text-align: center;">33.14</td>
<td style="text-align: center;">33.76</td>
<td style="text-align: center;">35.72</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">52.10</td>
<td style="text-align: center;">55.85</td>
<td style="text-align: center;">57.43</td>
<td style="text-align: center;">60.32</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">66.34</td>
<td style="text-align: center;">70.31</td>
<td style="text-align: center;">71.44</td>
<td style="text-align: center;">75.83</td>
</tr>
</tbody>
</table>
<p>Table 3. Average accuracies on GLoRE few-shot evaluation.
accuracy is around $20 \%$, worse than random guessing in 4 -way classification, indicating challenges in logical reasoning without in-context demonstrations. Performance gaps between LogiQA and LogiQA22 are smaller for these models, suggesting stable performance across data distributions without in-domain tuning. Mixtral-8x7b outperforms LLaMA and Falcon, demonstrating the effectiveness of mixture-of-expert models.</p>
<p>The fourth block provides zero-shot results for ChatGPT and GPT-4. Both models, especially GPT-4, surpass RoBERTa-base on several MRC benchmarks. However, GPT-4's accuracy drops significantly on LogiQA22 ( $58.49 \%$ vs. $72.25 \%$ on LogiQA 2.0), indicating sensitivity to data distribution. In NLI and TF tasks, ChatGPT and GPT-4 outperform RoBERTa, with ChatGPT achieving $58.45 \%$ accuracy on ConTRoL, surpassing GPT-4. GPT-4's NLI performance varies across datasets, further highlighting its sensitivity to data distribution. TF task results show similar inconsistencies, suggesting model rationales differ from human reasoning.</p>
<p>The final block shows results for o1 mini, DeepSeek R1, and QwQ-32B, which achieve notable improvements over prior models. QwQ-32B attains the highest average accuracy $(78.95 \%)$, surpassing GPT-4 $(66.34 \%)$ and DeepSeek R1 $(75.14 \%)$. It achieves state-of-the-art results on MRC tasks like ReClor ( $93.76 \%$ ) and AR-LSAT ( $92.35 \%$ ), indicating the need for more challenging benchmarks for logical reasoning. Its robustness is evident in LogiQA22 ( $86.30 \%$ ), outperforming GPT-4 by 27.81 percentage points. However, QwQ-32B shows uneven performance on NLI datasets, such as HELP ( $61.53 \%$, lagging behind o1 mini's $63.69 \%)$, suggesting its reasoning capabilities are less generalizable in tasks requiring fine-grained entailment analysis.</p>
<p>While GPT-4 retains an advantage on FraCas ( $75.35 \%$ ), QwQ-32B surpasses GPT-4 on ReClor ( $93.76 \%$ vs. $87.20 \%$ ), redefining state-of-the-art performance for MRC tasks. QwQ-32B and DeepSeek R1 showcase balanced performance across most tasks, with QwQ-32B achieving unprecedented TF results (e.g., $82.40 \%$ on ProofWriter, outperforming both GPT-4's $59.66 \%$ and DeepSeek R1's $80.51 \%)$. Though still below the human average overall, these models mark substantial progress - QwQ-32B's $78.95 \%$ average accuracy (vs. DeepSeek R1's $75.14 \%$ and GPT-4's $66.34 \%$ ) highlights significant architectural or training innovations for logical inference.</p>
<p>Few-shot results. LLMs excel at in-context learning [20], where performance improves with context examples and demonstration methods [42]. For this study, we randomly sampled instances ( 1 for 1 -shot, 2 for 2 -shot, and 5 for 5 -shot) from each dataset and appended them to the prompt. We used the same</p>
<p>model configuration as in the zero-shot scenario. Table 3 highlights the impact of in-context learning (ICL), as seen in GPT-4's $9 \%$ accuracy gain with 5 -shot learning. However, this improvement stems from statistical adaptation rather than true reasoning, as models rely on superficial patterns rather than humanlike logical inference. This aligns with findings that chain-of-thought prompts correlate with outputs but do not causally drive reasoning [5]. While reasoningenhanced models narrow the gap with human performance, their sensitivity to data distribution highlights the need for further research into more robust reasoning mechanisms. GLoRE's evolving framework will continue to track these advancements.</p>
<h1>4.3 Analysis</h1>
<p>Large language models vs. reasoning-enhanced models. The reasoningenhanced models like QwQ-32B, DeepSeek R1, and OpenAI's o1 mini demonstrate significant improvements over traditional LLMs. QwQ-32B, in particular, achieves the highest average performance ( $78.95 \%$ ), indicating that its reinforcement learning framework or specialized training methodology enables better generalization across data distributions. While QwQ-32B dominates MRC and TF tasks, its relatively lower performance on NLI datasets like HELP ( $61.53 \%$ ) suggests that even state-of-the-art models struggle with tasks requiring monotonicity or negation reasoning, highlighting the need for broader evaluation beyond task-specific robustness.
Data leakage concerns. While GLoRE includes diverse datasets, potential data leakage risks arise from overlapping sources. GPT-4's lower accuracy on LogiQA22 ( $58.49 \%$ ) compared to LogiQA $2.0(72.25 \%)$ suggests limited exposure to newer data, reducing leakage concerns but highlighting distributional sensitivity. The benchmark's dynamic updates and inclusion of newly annotated datasets help mitigate leakage by testing models on unseen distributions.
Sensitivity to data distribution. The above experiments show that the performance of LLMs is sensitive to the data distribution. Even though the underlying reasoning principles are the same, LLM performance varies significantly across datasets. This suggests that LLMs might not reason using the correct rationale but rely on superficial features. As shown in Table 2, although GPT-4 achieves near-human performance on datasets like ReClor ( $87.20 \%$ ) and NaNNLI $(75.74 \%)$, it lags significantly on others (e.g., HELP at $46.01 \%$ ). This inconsistency mirrors the behavior of reasoning-enhanced models like DeepSeek R1, revealing a critical divergence from human reasoning: once humans master a reasoning pattern, their performance generalizes robustly, whereas LLMs remain sensitive to data-specific features.</p>
<h2>5 Conclusion</h2>
<p>We constructed GLoRE, a dynamic and comprehensive benchmark tailored for assessing the logical reasoning capabilities of advanced language models, including GPT-4 and various strong open-source LLMs across multiple reasoning tasks.</p>
<p>Our findings indicate that QwQ-32B, a reasoning-enhanced model, sets a new state-of-the-art on the GLoRE benchmark, significantly narrowing the gap to human performance. This underscores the potential of targeted architectural and training innovations for logical reasoning. GLoRE will be continually maintained to track advancements in this rapidly evolving domain.</p>
<h1>References</h1>
<ol>
<li>Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli, A., Cojocaru, R., Debbah, M., Goffinet, E., Heslow, D., Launay, J., Malartic, Q., Noune, B., Pannier, B., Penedo, G.: Falcon-40B: an open large language model with state-of-the-art performance (2023)</li>
<li>Anil, R., Dai, A.M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., others.: Palm 2 technical report (2023)</li>
<li>bench authors, B.: Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. TMLR (2023)</li>
<li>Bang, Y., Cahyawijaya, S., Lee, N., Dai, W., Su, D., Wilie, B., Lovenia, H., Ji, Z., Yu, T., Chung, W., et al.: A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023 (2023)</li>
<li>Bao, G., Zhang, H., Wang, C., Yang, L., Zhang, Y.: How likely do llms with cot mimic human reasoning? arXiv preprint arXiv:2402.16048 (2024)</li>
<li>Beygi, S., Fazel-Zarandi, M., Cervone, A., Krishnan, P., Jonnalagadda, S.R.: Logical reasoning for task oriented dialogue systems (2022)</li>
<li>Bowman, S.R., Angeli, G., Potts, C., Manning, C.D.: A large annotated corpus for learning natural language inference. In: Proc. of EMNLP. pp. 632-642 (2015)</li>
<li>Chen, D., Bolton, J., Manning, C.D.: A thorough examination of the CNN/daily mail reading comprehension task. In: ACL (2016)</li>
<li>Chen, M., Tworek, J., Jun, H., Yuan, Q., Others: Evaluating large language models trained on code (2021)</li>
<li>Chen, M., Tworek, J., Jun, H., Yuan, Q., Others: Evaluating large language models trained on code (2021)</li>
<li>Choi, J.H., Hickman, K.E., Monahan, A., Schwarcz, D.: Chatgpt goes to law school. Available at SSRN (2023)</li>
<li>Chung, H.W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al.: Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022)</li>
<li>Clark, C., Lee, K., Chang, M.W., Kwiatkowski, T., Collins, M., Toutanova, K.: Boolq: Exploring the surprising difficulty of natural yes/no questions (2019)</li>
<li>Clark, P., Tafjord, O., Richardson, K.: Transformers as soft reasoners over language. In: Proc. of IJCAI (2020)</li>
<li>Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al.: Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 (2021)</li>
<li>Cresswell, M.J.: Logics and languages (1st ed.). Routledge (1973)</li>
<li>Dagan, I., Glickman, O., Magnini, B.: The pascal recognising textual entailment challenge. In: MLCW (2005)</li>
<li>
<p>DeepSeek-AI: Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning (2025), https://arxiv.org/abs/2501.12948</p>
</li>
<li>
<p>Demszky, D., Guu, K., Liang, P.: Transforming question answering datasets into natural language inference datasets (2018)</p>
</li>
<li>Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu, J., Li, L., Sui, Z.: A survey on in-context learning (2023)</li>
<li>Dua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S., Gardner, M.: DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In: Proc. of AACL (2019)</li>
<li>Frieder, S., Pinchetti, L., Griffiths, R.R., Salvatori, T., Lukasiewicz, T., Petersen, P.C., Chevalier, A., Berner, J.: Mathematical capabilities of chatgpt (2023)</li>
<li>Fu, Y., Ou, L., Chen, M., Wan, Y., Peng, H., Khot, T.: Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance. arXiv preprint arXiv:2305.17306 (2023)</li>
<li>Gendron, G., Bao, Q., Witbrock, M., Dobbie, G.: Large language models are not abstract reasoners. arXiv preprint arXiv:2305.19555 (2023)</li>
<li>Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., Steinhardt, J.: Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR) (2021)</li>
<li>Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., Steinhardt, J.: Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874 (2021)</li>
<li>Huang, J., Chang, K.C.C.: Towards reasoning in large language models: A survey (2023)</li>
<li>Huang, Y., Fang, M., Cao, Y., Wang, L., Liang, X.: Dagn: Discourse-aware graph network for logical reasoning. arXiv preprint arXiv:2103.14349 (2021)</li>
<li>Iwańska, L.: Logical reasoning in natural language: It is all about knowledge. Minds and Machines pp. 475-510 (1993)</li>
<li>Jiang, A.Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Others.: Mixtral of experts (2024)</li>
<li>Joshi, P., Aditya, S., Sathe, A., Choudhury, M.: Taxinli: Taking a ride up the NLU hill. CoRR (2020)</li>
<li>Koreeda, Y., Manning, C.: ContractNLI: A dataset for document-level natural language inference for contracts. In: Proc. of EMNLP Findings (2021)</li>
<li>Kowalski, R.: Logic for problem solving. Ediciones Díaz de Santos (1979)</li>
<li>Kung, T.H., Cheatham, M., Medenilla, A., Sillos, C., De Leon, L., Elepaño, C., Madriaga, M., Aggabao, R., Diaz-Candido, G., Maningo, J., et al.: Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models. PLoS digital health p. e0000198 (2023)</li>
<li>Lai, G., Xie, Q., Liu, H., Yang, Y., Hovy, E.: RACE: Large-scale Reading Comprehension dataset from Examinations. In: EMNLP (2017)</li>
<li>Li, T., Srikumar, V.: Augmenting neural networks with first-order logic. In: Proc. of ACL. pp. 292-302 (2019)</li>
<li>Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Others.: Competitionlevel code generation with alphacode. Science pp. 1092-1097 (2022)</li>
<li>Liévin, V., Hother, C.E., Winther, O.: Can large language models reason about medical questions? arXiv preprint arXiv:2207.08143 (2022)</li>
<li>Liu, H., Cui, L., Liu, J., Zhang, Y.: Natural language inference in context - investigating contextual reasoning over long texts. CoRR (2020)</li>
<li>
<p>Liu, H., Liu, J., Cui, L., Teng, Z., Duan, N., Zhou, M., Zhang, Y.: Logiqa 2.0 - an improved dataset for logical reasoning in natural language understanding. IEEE/ACM Transactions on Audio, Speech, and Language Processing pp. $2947-2962(2023)$</p>
</li>
<li>
<p>Liu, H., Teng, Z., Cui, L., Zhang, C., Zhou, Q., Zhang, Y.: Logicot: Logical chain-of-thought instruction tuning. In: Proc. of EMNLP Findings. pp. 2908-2921 (2023)</p>
</li>
<li>Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., Chen, W.: What makes good in-context examples for gpt-3? (2021)</li>
<li>Liu, J., Cui, L., Liu, H., Huang, D., Wang, Y., Zhang, Y.: Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. CoRR (2020)</li>
<li>Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized bert pretraining approach. arXiv (2019)</li>
<li>MacCartney, B., Manning, C.D.: Natural logic for textual inference. In: Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing. pp. $193-200(2007)$</li>
<li>MacCartney, B., Manning, C.D.: Natural logic for textual inference. In: Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing (2007)</li>
<li>Mccarthy, J.: Programs with common sense (2002)</li>
<li>McCarthy, J., Hayes, P.J.: Some philosophical problems from the standpoint of artificial intelligence. In: Machine Intelligence 4 (1969)</li>
<li>Newell, A., Simon, H.: The logic theory machine-a complex information processing system. IRE Transactions on Information Theory (1956)</li>
<li>OpenAI: Gpt-4 technical report (2023)</li>
<li>OpenAI: Openai o1 system card (2024), https://arxiv.org/abs/2412.16720</li>
<li>Orrù, G., Piarulli, A., Conversano, C., Gemignani, A.: Human-like problem-solving abilities in large language models using chatgpt. Frontiers in Artificial Intelligence p. $1199350(2023)$</li>
<li>Ott, S., Hebenstreit, K., Liévin, V., Hother, C.E., Moradi, M., Mayrhauser, M., Praas, R., Winther, O., Samwald, M.: Thoughtsource: A central hub for large language model reasoning data. arXiv preprint arXiv:2301.11596 (2023)</li>
<li>Poliak, A., Haldar, A., Rudinger, R., Hu, J.E., Pavlick, E., White, A.S., Van Durme, B.: Collecting diverse natural language inference problems for sentence representation evaluation. In: Proc. of EMNLP (2018)</li>
<li>Poole, D., Goebel, R., Aleliunas, R.: Theorist: A Logical Reasoning System for Defaults and Diagnosis, pp. 331-352 (1987)</li>
<li>Pulman, S.G.: Using the framework (1996)</li>
<li>Qin, C., Zhang, A., Zhang, Z., Chen, J., Yasunaga, M., Yang, D.: Is chatgpt a general-purpose natural language processing task solver? (2023)</li>
<li>Quan, S., Yang, J., Yu, B., Zheng, B., Liu, D., Yang, A., Ren, X., Gao, B., Miao, Y., Feng, Y., et al.: Codeelo: Benchmarking competition-level code generation of llms with human-comparable elo ratings. arXiv preprint arXiv:2501.01257 (2025)</li>
<li>Saha, S., Ghosh, S., Srivastava, S., Bansal, M.: Prover: Proof generation for interpretable reasoning over rules (2020)</li>
<li>Sanyal, S., Singh, H., Ren, X.: Fairr: Faithful and robust deductive reasoning over natural language (2022)</li>
<li>Saparov, A., Pang, R.Y., Padmakumar, V., Joshi, N., Kazemi, S.M., Kim, N., He, H.: Testing the general deductive reasoning capacity of large language models using ood examples. arXiv preprint arXiv:2305.15269 (2023)</li>
<li>Sawada, T., Paleka, D., Havrilla, A., Tadepalli, P., Vidas, P., Kranias, A., Nay, J.J., Gupta, K., Komatsuzaki, A.: Arb: Advanced reasoning benchmark for large language models (2023)</li>
<li>
<p>Shi, J., Ding, X., Du, L., Liu, T., Qin, B.: Neural natural logic inference for interpretable question answering. In: Proc. of EMNLP (2021)</p>
</li>
<li>
<p>Suzgun, M., Scales, N., Schärli, N., Gehrmann, S., Tay, Y., Chung, H.W., Chowdhery, A., Le, Q.V., Chi, E.H., Zhou, D., Wei, J.: Challenging big-bench tasks and whether chain-of-thought can solve them (2022)</p>
</li>
<li>Tafjord, O., Mishra, B.D., Clark, P.: Proofwriter: Generating implications, proofs, and abductive statements over natural language (2021)</li>
<li>Team, Q.: Qwq-32b: Embracing the power of reinforcement learning (2025)</li>
<li>Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., Lample, G.: Llama: Open and efficient foundation language models (2023)</li>
<li>Truong, T.H., Otmakhova, Y., Baldwin, T., Cohn, T., Lau, J.H., Verspoor, K.: Not another negation benchmark: The NaN-NLI test suite for sub-clausal negation. In: Proc. of AACL. pp. 883-894 (2022)</li>
<li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. CoRR (2017)</li>
<li>Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S.R.: SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems (2019)</li>
<li>Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S.: GLUE: A multitask benchmark and analysis platform for natural language understanding. In: Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP (2018)</li>
<li>Wang, S., Liu, Z., Zhong, W., Zhou, M., Wei, Z., Chen, Z., Duan, N.: From lsat: The progress and challenges of complex reasoning. IEEE/ACM Transactions on Audio, Speech, and Language Processing (2022)</li>
<li>Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., et al.: Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. In: Proc. of NeurIPS (2024)</li>
<li>Williams, A., Nangia, N., Bowman, S.: A broad-coverage challenge corpus for sentence understanding through inference. In: Proc. of AACL (2018)</li>
<li>Williams, A., Nangia, N., Bowman, S.: A broad-coverage challenge corpus for sentence understanding through inference. In: Proc. of NAACL (2018)</li>
<li>Wu, S., Irsoy, O., Lu, S., Dabravolski, V., Dredze, M., Gehrmann, S., Kambadur, P., Rosenberg, D., Mann, G.: Bloomberggpt: A large language model for finance. arXiv preprint arXiv:2303.17564 (2023)</li>
<li>Wu, Z., Qiu, L., Ross, A., Akyürek, E., Chen, B., Wang, B., Kim, N., Andreas, J., Kim, Y.: Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. arXiv preprint arXiv:2307.02477 (2023)</li>
<li>Xu, F., Liu, J., Lin, Q., Pan, Y., Zhang, L.: Logiformer. In: Proc. of SIGIR (2022)</li>
<li>Yanaka, H., Mineshima, K., Bekki, D., Inui, K., Sekine, S., , Bos, J.: Help: A dataset for identifying shortcomings of neural models in monotonicity reasoning. In: Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM2019) (2019)</li>
<li>Yu, W., Jiang, Z., Dong, Y., Feng, J.: Reclor: A reading comprehension dataset requiring logical reasoning. In: Proc. of ICLR (2020)</li>
</ol>            </div>
        </div>

    </div>
</body>
</html>
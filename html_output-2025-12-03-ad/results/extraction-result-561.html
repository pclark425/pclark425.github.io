<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-561 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-561</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-561</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-16.html">extraction-schema-16</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <p><strong>Paper ID:</strong> paper-9b6e4cbf1f8d6fbf09017769ae65ff90234e0aa0</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9b6e4cbf1f8d6fbf09017769ae65ff90234e0aa0" target="_blank">Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomization</a></p>
                <p><strong>Paper Venue:</strong> 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</p>
                <p><strong>Paper TL;DR:</strong> This work presents a system for training deep neural networks for object detection using synthetic images that relies upon the technique of domain randomization, in which the parameters of the simulator are randomized in non-realistic ways to force the neural network to learn the essential features of the object of interest.</p>
                <p><strong>Paper Abstract:</strong> We present a system for training deep neural networks for object detection using synthetic images. To handle the variability in real-world data, the system relies upon the technique of domain randomization, in which the parameters of the simulator-such as lighting, pose, object textures, etc.-are randomized in non-realistic ways to force the neural network to learn the essential features of the object of interest. We explore the importance of these parameters, showing that it is possible to produce a network with compelling performance using only non-artistically-generated synthetic data. With additional fine-tuning on real data, the network yields better performance than using real data alone. This result opens up the possibility of using inexpensive synthetic data for training neural networks while avoiding the need to collect large amounts of hand-annotated real-world data or to generate high-fidelity synthetic worlds-both of which remain bottlenecks for many applications. The approach is evaluated on bounding box detection of cars on the KITTI dataset.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e561.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e561.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DR (this work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain Randomization (applied to object detection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedure that generates large amounts of synthetic training images by randomizing nonessential scene parameters (textures, lighting, camera pose, distractors) so networks learn invariant, task-relevant features; here adapted to train object detectors for real-world car detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Domain Randomization (synthetic data generation for sim-to-real transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Randomized procedural rendering pipeline: place random numbers/types of 3D object models and 'flying distractors' into a 3D scene, apply randomly sampled textures (from Flickr8K), random lighting (1–12 point lights + planar ambient), random camera viewpoints (azimuth, elevation, pan/tilt/roll ranges), random backgrounds (photographs), random object/distractor scales/colors, and render annotated images (bounding boxes) using an Unreal Engine plugin at 1200×400 @30Hz; produced large datasets (e.g., 100K images) to train deep neural detectors so that the variation forces learning of essential invariants and bridges the reality gap.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / synthetic data generation / sim-to-real transfer technique</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>simulation / computer graphics / robotics (sim-to-real pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>computer vision — object detection on real-world driving imagery (KITTI)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Adapted generic DR ideas to object detection by (1) using 36 car 3D models and up to 14 cars per image, (2) introducing a new component 'flying distractors' (geometric shapes, pedestrians, trees, etc.) to emulate occlusion and distractors, (3) composing renders over Flickr8K real background photos, (4) randomizing camera pose ranges suitable for car viewpoints, (5) batching with standard detector augmentations (brightness/contrast/Gaussian noise/flips/crops), and (6) integrating into UE4 via a custom plugin to output annotations; these changes targeted the statistics and nuisances of automotive detection.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - models trained only on DR achieved strong real-world performance: e.g., Faster R-CNN trained on DR reached AP@0.5 = 78.1 on a KITTI subset (compare VKITTI 79.7); after fine-tuning on all 6000 real KITTI images, DR→real achieved AP@0.5 = 98.5, outperforming VKITTI by 1.6% and training on real data alone by 2.1%. Different detectors varied, with DR outperforming VKITTI on R-FCN and SSD but slightly below VKITTI for Faster R-CNN when trained purely on synthetic.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>mismatch in data distribution for some high-recall cases (precision drop at high recall), lack of scene-context structure (e.g., parked-car patterns) not modeled by DR, simplified/cartoonish appearance may omit particular real-world variations; requires sufficient variety and scale of synthetic data and compute for training.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>ease of generating arbitrarily large labeled datasets, large randomized variation forcing invariance, availability of 3D models and Flickr8K textures, UE4 rendering pipeline and plugin, pretrained feature extractors (ImageNet/COCO) that accelerated learning, standard data augmentation strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>3D object models, simulator/rendering engine (Unreal Engine), background image corpus, compute for rendering and DNN training (GPUs), and optionally pretrained weights; design of randomization ranges appropriate to target task (camera ranges, lighting).</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>moderately high — authors argue DR can be extended to other vision tasks (e.g., heavily textured objects, segmentation) and robotics; success depends on selecting randomizations relevant to the target task and providing sufficient variation, so applicable beyond cars but needs task-specific adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and instrumental/technical skills (practical know-how for synthetic dataset generation and sim-to-real training)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomization', 'publication_date_yy_mm': '2018-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e561.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e561.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DR (original robotics refs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain Randomization (original sim-to-real robotics applications)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The domain randomization concept originally applied to simulation-to-real transfer in robotics (e.g., 3D pose estimation for a robotic arm and simulated quadcopter flight) by creating highly varied synthetic inputs so real-world inputs appear as just another variation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Domain randomization for transferring deep neural networks from simulation to the real world</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Domain Randomization (original robotics/robot control use)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Generating synthetic images with large, non-photorealistic random perturbations (textures, lighting, distractors) during simulator rendering such that policies or estimators trained entirely in simulation generalize to real-world robotics tasks (e.g., 3D coordinate estimation for an arm, control of a quadcopter).</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / sim-to-real transfer technique</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>robotics / simulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>robotics in the real world (e.g., robot arm control, quadcopter flight)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application of synthetic training with randomization to real-world robots (sim-to-real)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Original works randomized scene attributes broadly; specific task-dependent elements included generating images of target objects in the manipulated contexts (e.g., cubes on a table for arm tasks), and selecting appropriate randomization ranges for camera and environment; this paper references those methods and adopts the general principle.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful in cited works (e.g., Tobin et al. and Sadeghi & Levine reported transfer of pose estimation and flight control policies from simulation to real hardware without real images), cited here as motivation for applying DR to object detection.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>task-specific mismatches if simulator does not include necessary dynamics or sensory modalities; need to choose useful randomizations; not all real-world nuances are captured simply by randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>shared visual priors between sim and real, ability to render many variants cheaply, and modular simulator control enabling wide randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>simulator capable of producing varied imagery, task-specific modeling in simulation, hardware for testing on real robots for validation.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>demonstrated across multiple robotics tasks in cited works; motivates extension to other domains (this paper extends to detection).</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>theoretical principles and explicit procedural steps (how to randomize simulation parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomization', 'publication_date_yy_mm': '2018-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e561.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e561.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VKITTI transfer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Virtual KITTI (photorealistic synthetic data transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of a high-fidelity synthetic dataset (Virtual KITTI) rendered to closely mimic a real-world driving dataset (KITTI) to train detectors for real-world tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Virtual worlds as proxy for multi-object tracking analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Photorealistic virtual-world dataset generation and transfer</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Construct photorealistic synthetic scenes in a game engine (Unity) reproducing the real KITTI dataset distribution and render annotated images for training detectors; the aim is to closely mimic appearances and contexts of the target real dataset to enable direct transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / synthetic data generation for domain transfer</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>computer graphics / game-engine simulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>computer vision — object detection and tracking on real driving data</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application (photorealistic proxy dataset intended to emulate real data closely)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>VKITTI is intentionally rendered to replicate KITTI scenes and distributions; in this work it was used without further modification as a baseline synthetic dataset for comparison with DR.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful — VKITTI produced competitive or superior results for some detectors (e.g., Faster R-CNN trained on VKITTI achieved AP@0.5 = 79.7 vs DR 78.1 on a KITTI subset), but DR outperformed VKITTI on other architectures (R-FCN, SSD) and after fine-tuning DR achieved higher final performance.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>requires costly, high-fidelity rendering and expert modeling to replicate target dataset; close correlation to one test set may limit generalization to other distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>high visual fidelity and close matching of dataset statistics to the target test set improved transfer in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>game-engine expertise, artist effort to model scenes, and significant rendering resources.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>useful when one can closely model the target domain; less scalable than DR for producing broad variation cheaply; may not generalize across different target distributions without re-rendering.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>instrumental/technical skills and explicit procedural steps (how to generate photorealistic proxy datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomization', 'publication_date_yy_mm': '2018-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e561.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e561.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pretraining & fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pretraining on large labeled datasets (ImageNet/COCO) followed by fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard transfer-learning strategy where feature extractor weights are initialized from networks trained on large classification/detection corpora (ImageNet, COCO) and then fine-tuned on synthetic and/or real target data to improve downstream performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ImageNet: A large-scale hierarchical image database</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Pretraining and fine-tuning (transfer learning for DNNs)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Initialize network feature-extractor weights with models pretrained on large datasets (e.g., ImageNet for classification, COCO for detection), then continue training on synthetic DR or VKITTI data and optionally fine-tune on varying amounts of real KITTI images at reduced learning rates, allowing gradients to flow end-to-end to adapt features to the target domain.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / machine learning transfer learning strategy</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>general image recognition (ImageNet/COCO research / large-scale supervised learning)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>task-specific object detection on KITTI (autonomous-driving imagery)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application of pretrained weights followed by fine-tuning (adapted for target dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Used ImageNet- and COCO-pretrained feature extractors (Inception-ResNet V2, ResNet101), trained on synthetic DR or VKITTI datasets, then fine-tuned on real images with learning rate reduced by factor of 10 and full backpropagation (no freezing unless specifically tested).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful and important: pretraining accelerated convergence and improved final accuracy; DR trained from COCO initialization achieved AP@0.5 = 83.7 (COCO alone = 56.1, COCO+VKITTI = 79.7), showing synthetic+pretraining synergy; fine-tuning on real images further boosted performance (DR + full fine-tuning on 6000 real images → AP@0.5 = 98.5).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>pretrained datasets may not match target domain (COCO alone gave poor KITTI performance); choice of whether to freeze early layers affects performance (freezing could hurt if synthetic data differs), and significant computational resources required.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>rich, general visual features learned from large datasets; compatibility of architectures and ability to fine-tune end-to-end; synthetic data providing labeled examples complementary to pretrained features.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>availability of pretrained weights, compute for fine-tuning, and appropriate learning rate schedules; careful decision on freezing vs full training.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>high — pretraining followed by fine-tuning is a broadly applicable strategy across vision tasks and domains, but effectiveness depends on similarity between pretraining and target domains and availability of fine-tuning data.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and theoretical principles (transfer learning strategies and hyperparameter settings)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomization', 'publication_date_yy_mm': '2018-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e561.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e561.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Freezing early layers (Hinterstoisser)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Freezing early pretrained feature layers when training on synthetic data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transfer strategy proposed to retain low-level features learned from real images (ImageNet) by freezing early convolutional layers during training on synthetic data, intended to prevent overfitting to synthetic appearance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On pre-trained image features and synthetic images for deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Freezing pretrained early layers during synthetic-data training</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Keep early convolutional layers' weights fixed (as initialized from ImageNet) while training on synthetic images so that low-level features remain tuned to real-image statistics, and only later layers adapt to synthetic-target task.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / transfer-training protocol</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>transfer learning practices in computer vision (pretraining studies)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>training detectors using synthetic images for real-world tasks</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application of a transfer-training protocol</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>This paper implemented the freezing protocol as described in the cited work and compared it to full end-to-end training on DR and VKITTI datasets to measure effects.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>unsuccessful in this work — freezing decreased performance significantly: Faster R-CNN freezing → AP@0.5 = 66.4 vs full training = 78.1; R-FCN freezing → 69.4 vs full = 71.5. The authors conclude freezing early layers hindered adaptation to their highly varied synthetic data.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>freezing prevented beneficial adaptation of low-level filters to the distribution induced by heavy randomization; when synthetic data vary widely, fixed early features may be suboptimal.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>the hypothesis behind freezing is that it preserves real-image priors, but in practice this requires synthetic appearance to be sufficiently close to real data for frozen features to be helpful.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>access to pretrained weights and careful empirical validation of whether freezing helps for particular synthetic generation procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>context-dependent — freezing may help when synthetic data are close to real data or when limited variability would otherwise corrupt low-level features, but can be detrimental when synthetic data are highly varied (as with DR).</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and interpretive framework (protocol for training and rationale)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomization', 'publication_date_yy_mm': '2018-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e561.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e561.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cut-Paste (Dwibedi)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cut, Paste and Learn (pasting segmented real-object images onto backgrounds)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data synthesis procedure that composes real segmented object crops onto arbitrary background images to produce labeled training examples for instance detection, reducing the need for full scene rendering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cut, paste and learn: Surprisingly easy synthesis for instance detection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Pasting segmented real images onto backgrounds for synthetic training data</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Collect segmented object crops from real images and composite them onto varied background images (with appropriate augmentation) to create synthetic examples with automatic bounding-box labels for training detectors, avoiding expensive 3D modeling or rendering.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / data augmentation / synthetic data generation</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>image compositing / computer vision data augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>object detection training for real-world images</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>analogical transfer (using real-image fragments to synthesize new training examples)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Referenced as an alternative to fully synthetic DR; not implemented in this paper. The authors note segmentation accuracy of objects is a challenge for this approach.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>not evaluated in this paper (mentioned as related work); cited method showed promising results in original work.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>requires accurate segmentation of objects from real images, and compositing realism/context consistency can be challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>uses real object appearance directly which may preserve realistic textures and shading without requiring 3D assets.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>datasets of segmented object crops, background images, and compositing pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>applicable to many detection tasks where segmented object crops are available; usefulness depends on segmentation quality and contextual matching.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and instrumental skills (compositing and dataset construction)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomization', 'publication_date_yy_mm': '2018-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>CAD^2RL: Real single-image flight without a single real image <em>(Rating: 2)</em></li>
                <li>Driving in the matrix: Can virtual worlds replace human-generated annotations for real world tasks? <em>(Rating: 2)</em></li>
                <li>Cut, paste and learn: Surprisingly easy synthesis for instance detection <em>(Rating: 2)</em></li>
                <li>On pre-trained image features and synthetic images for deep learning <em>(Rating: 2)</em></li>
                <li>Learning deep object detectors from 3D models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-561",
    "paper_id": "paper-9b6e4cbf1f8d6fbf09017769ae65ff90234e0aa0",
    "extraction_schema_id": "extraction-schema-16",
    "extracted_data": [
        {
            "name_short": "DR (this work)",
            "name_full": "Domain Randomization (applied to object detection)",
            "brief_description": "A procedure that generates large amounts of synthetic training images by randomizing nonessential scene parameters (textures, lighting, camera pose, distractors) so networks learn invariant, task-relevant features; here adapted to train object detectors for real-world car detection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "procedure_name": "Domain Randomization (synthetic data generation for sim-to-real transfer)",
            "procedure_description": "Randomized procedural rendering pipeline: place random numbers/types of 3D object models and 'flying distractors' into a 3D scene, apply randomly sampled textures (from Flickr8K), random lighting (1–12 point lights + planar ambient), random camera viewpoints (azimuth, elevation, pan/tilt/roll ranges), random backgrounds (photographs), random object/distractor scales/colors, and render annotated images (bounding boxes) using an Unreal Engine plugin at 1200×400 @30Hz; produced large datasets (e.g., 100K images) to train deep neural detectors so that the variation forces learning of essential invariants and bridges the reality gap.",
            "procedure_type": "computational method / synthetic data generation / sim-to-real transfer technique",
            "source_domain": "simulation / computer graphics / robotics (sim-to-real pipelines)",
            "target_domain": "computer vision — object detection on real-world driving imagery (KITTI)",
            "transfer_type": "adapted/modified for new context",
            "modifications_made": "Adapted generic DR ideas to object detection by (1) using 36 car 3D models and up to 14 cars per image, (2) introducing a new component 'flying distractors' (geometric shapes, pedestrians, trees, etc.) to emulate occlusion and distractors, (3) composing renders over Flickr8K real background photos, (4) randomizing camera pose ranges suitable for car viewpoints, (5) batching with standard detector augmentations (brightness/contrast/Gaussian noise/flips/crops), and (6) integrating into UE4 via a custom plugin to output annotations; these changes targeted the statistics and nuisances of automotive detection.",
            "transfer_success": "successful - models trained only on DR achieved strong real-world performance: e.g., Faster R-CNN trained on DR reached AP@0.5 = 78.1 on a KITTI subset (compare VKITTI 79.7); after fine-tuning on all 6000 real KITTI images, DR→real achieved AP@0.5 = 98.5, outperforming VKITTI by 1.6% and training on real data alone by 2.1%. Different detectors varied, with DR outperforming VKITTI on R-FCN and SSD but slightly below VKITTI for Faster R-CNN when trained purely on synthetic.",
            "barriers_encountered": "mismatch in data distribution for some high-recall cases (precision drop at high recall), lack of scene-context structure (e.g., parked-car patterns) not modeled by DR, simplified/cartoonish appearance may omit particular real-world variations; requires sufficient variety and scale of synthetic data and compute for training.",
            "facilitating_factors": "ease of generating arbitrarily large labeled datasets, large randomized variation forcing invariance, availability of 3D models and Flickr8K textures, UE4 rendering pipeline and plugin, pretrained feature extractors (ImageNet/COCO) that accelerated learning, standard data augmentation strategies.",
            "contextual_requirements": "3D object models, simulator/rendering engine (Unreal Engine), background image corpus, compute for rendering and DNN training (GPUs), and optionally pretrained weights; design of randomization ranges appropriate to target task (camera ranges, lighting).",
            "generalizability": "moderately high — authors argue DR can be extended to other vision tasks (e.g., heavily textured objects, segmentation) and robotics; success depends on selecting randomizations relevant to the target task and providing sufficient variation, so applicable beyond cars but needs task-specific adaptation.",
            "knowledge_type": "explicit procedural steps and instrumental/technical skills (practical know-how for synthetic dataset generation and sim-to-real training)",
            "uuid": "e561.0",
            "source_info": {
                "paper_title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomization",
                "publication_date_yy_mm": "2018-04"
            }
        },
        {
            "name_short": "DR (original robotics refs)",
            "name_full": "Domain Randomization (original sim-to-real robotics applications)",
            "brief_description": "The domain randomization concept originally applied to simulation-to-real transfer in robotics (e.g., 3D pose estimation for a robotic arm and simulated quadcopter flight) by creating highly varied synthetic inputs so real-world inputs appear as just another variation.",
            "citation_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "mention_or_use": "mention",
            "procedure_name": "Domain Randomization (original robotics/robot control use)",
            "procedure_description": "Generating synthetic images with large, non-photorealistic random perturbations (textures, lighting, distractors) during simulator rendering such that policies or estimators trained entirely in simulation generalize to real-world robotics tasks (e.g., 3D coordinate estimation for an arm, control of a quadcopter).",
            "procedure_type": "computational method / sim-to-real transfer technique",
            "source_domain": "robotics / simulation",
            "target_domain": "robotics in the real world (e.g., robot arm control, quadcopter flight)",
            "transfer_type": "direct application of synthetic training with randomization to real-world robots (sim-to-real)",
            "modifications_made": "Original works randomized scene attributes broadly; specific task-dependent elements included generating images of target objects in the manipulated contexts (e.g., cubes on a table for arm tasks), and selecting appropriate randomization ranges for camera and environment; this paper references those methods and adopts the general principle.",
            "transfer_success": "successful in cited works (e.g., Tobin et al. and Sadeghi & Levine reported transfer of pose estimation and flight control policies from simulation to real hardware without real images), cited here as motivation for applying DR to object detection.",
            "barriers_encountered": "task-specific mismatches if simulator does not include necessary dynamics or sensory modalities; need to choose useful randomizations; not all real-world nuances are captured simply by randomization.",
            "facilitating_factors": "shared visual priors between sim and real, ability to render many variants cheaply, and modular simulator control enabling wide randomization.",
            "contextual_requirements": "simulator capable of producing varied imagery, task-specific modeling in simulation, hardware for testing on real robots for validation.",
            "generalizability": "demonstrated across multiple robotics tasks in cited works; motivates extension to other domains (this paper extends to detection).",
            "knowledge_type": "theoretical principles and explicit procedural steps (how to randomize simulation parameters)",
            "uuid": "e561.1",
            "source_info": {
                "paper_title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomization",
                "publication_date_yy_mm": "2018-04"
            }
        },
        {
            "name_short": "VKITTI transfer",
            "name_full": "Virtual KITTI (photorealistic synthetic data transfer)",
            "brief_description": "Use of a high-fidelity synthetic dataset (Virtual KITTI) rendered to closely mimic a real-world driving dataset (KITTI) to train detectors for real-world tasks.",
            "citation_title": "Virtual worlds as proxy for multi-object tracking analysis",
            "mention_or_use": "use",
            "procedure_name": "Photorealistic virtual-world dataset generation and transfer",
            "procedure_description": "Construct photorealistic synthetic scenes in a game engine (Unity) reproducing the real KITTI dataset distribution and render annotated images for training detectors; the aim is to closely mimic appearances and contexts of the target real dataset to enable direct transfer.",
            "procedure_type": "computational method / synthetic data generation for domain transfer",
            "source_domain": "computer graphics / game-engine simulation",
            "target_domain": "computer vision — object detection and tracking on real driving data",
            "transfer_type": "direct application (photorealistic proxy dataset intended to emulate real data closely)",
            "modifications_made": "VKITTI is intentionally rendered to replicate KITTI scenes and distributions; in this work it was used without further modification as a baseline synthetic dataset for comparison with DR.",
            "transfer_success": "partially successful — VKITTI produced competitive or superior results for some detectors (e.g., Faster R-CNN trained on VKITTI achieved AP@0.5 = 79.7 vs DR 78.1 on a KITTI subset), but DR outperformed VKITTI on other architectures (R-FCN, SSD) and after fine-tuning DR achieved higher final performance.",
            "barriers_encountered": "requires costly, high-fidelity rendering and expert modeling to replicate target dataset; close correlation to one test set may limit generalization to other distributions.",
            "facilitating_factors": "high visual fidelity and close matching of dataset statistics to the target test set improved transfer in some settings.",
            "contextual_requirements": "game-engine expertise, artist effort to model scenes, and significant rendering resources.",
            "generalizability": "useful when one can closely model the target domain; less scalable than DR for producing broad variation cheaply; may not generalize across different target distributions without re-rendering.",
            "knowledge_type": "instrumental/technical skills and explicit procedural steps (how to generate photorealistic proxy datasets)",
            "uuid": "e561.2",
            "source_info": {
                "paper_title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomization",
                "publication_date_yy_mm": "2018-04"
            }
        },
        {
            "name_short": "Pretraining & fine-tuning",
            "name_full": "Pretraining on large labeled datasets (ImageNet/COCO) followed by fine-tuning",
            "brief_description": "Standard transfer-learning strategy where feature extractor weights are initialized from networks trained on large classification/detection corpora (ImageNet, COCO) and then fine-tuned on synthetic and/or real target data to improve downstream performance.",
            "citation_title": "ImageNet: A large-scale hierarchical image database",
            "mention_or_use": "use",
            "procedure_name": "Pretraining and fine-tuning (transfer learning for DNNs)",
            "procedure_description": "Initialize network feature-extractor weights with models pretrained on large datasets (e.g., ImageNet for classification, COCO for detection), then continue training on synthetic DR or VKITTI data and optionally fine-tune on varying amounts of real KITTI images at reduced learning rates, allowing gradients to flow end-to-end to adapt features to the target domain.",
            "procedure_type": "computational method / machine learning transfer learning strategy",
            "source_domain": "general image recognition (ImageNet/COCO research / large-scale supervised learning)",
            "target_domain": "task-specific object detection on KITTI (autonomous-driving imagery)",
            "transfer_type": "direct application of pretrained weights followed by fine-tuning (adapted for target dataset)",
            "modifications_made": "Used ImageNet- and COCO-pretrained feature extractors (Inception-ResNet V2, ResNet101), trained on synthetic DR or VKITTI datasets, then fine-tuned on real images with learning rate reduced by factor of 10 and full backpropagation (no freezing unless specifically tested).",
            "transfer_success": "successful and important: pretraining accelerated convergence and improved final accuracy; DR trained from COCO initialization achieved AP@0.5 = 83.7 (COCO alone = 56.1, COCO+VKITTI = 79.7), showing synthetic+pretraining synergy; fine-tuning on real images further boosted performance (DR + full fine-tuning on 6000 real images → AP@0.5 = 98.5).",
            "barriers_encountered": "pretrained datasets may not match target domain (COCO alone gave poor KITTI performance); choice of whether to freeze early layers affects performance (freezing could hurt if synthetic data differs), and significant computational resources required.",
            "facilitating_factors": "rich, general visual features learned from large datasets; compatibility of architectures and ability to fine-tune end-to-end; synthetic data providing labeled examples complementary to pretrained features.",
            "contextual_requirements": "availability of pretrained weights, compute for fine-tuning, and appropriate learning rate schedules; careful decision on freezing vs full training.",
            "generalizability": "high — pretraining followed by fine-tuning is a broadly applicable strategy across vision tasks and domains, but effectiveness depends on similarity between pretraining and target domains and availability of fine-tuning data.",
            "knowledge_type": "explicit procedural steps and theoretical principles (transfer learning strategies and hyperparameter settings)",
            "uuid": "e561.3",
            "source_info": {
                "paper_title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomization",
                "publication_date_yy_mm": "2018-04"
            }
        },
        {
            "name_short": "Freezing early layers (Hinterstoisser)",
            "name_full": "Freezing early pretrained feature layers when training on synthetic data",
            "brief_description": "A transfer strategy proposed to retain low-level features learned from real images (ImageNet) by freezing early convolutional layers during training on synthetic data, intended to prevent overfitting to synthetic appearance.",
            "citation_title": "On pre-trained image features and synthetic images for deep learning",
            "mention_or_use": "use",
            "procedure_name": "Freezing pretrained early layers during synthetic-data training",
            "procedure_description": "Keep early convolutional layers' weights fixed (as initialized from ImageNet) while training on synthetic images so that low-level features remain tuned to real-image statistics, and only later layers adapt to synthetic-target task.",
            "procedure_type": "computational method / transfer-training protocol",
            "source_domain": "transfer learning practices in computer vision (pretraining studies)",
            "target_domain": "training detectors using synthetic images for real-world tasks",
            "transfer_type": "direct application of a transfer-training protocol",
            "modifications_made": "This paper implemented the freezing protocol as described in the cited work and compared it to full end-to-end training on DR and VKITTI datasets to measure effects.",
            "transfer_success": "unsuccessful in this work — freezing decreased performance significantly: Faster R-CNN freezing → AP@0.5 = 66.4 vs full training = 78.1; R-FCN freezing → 69.4 vs full = 71.5. The authors conclude freezing early layers hindered adaptation to their highly varied synthetic data.",
            "barriers_encountered": "freezing prevented beneficial adaptation of low-level filters to the distribution induced by heavy randomization; when synthetic data vary widely, fixed early features may be suboptimal.",
            "facilitating_factors": "the hypothesis behind freezing is that it preserves real-image priors, but in practice this requires synthetic appearance to be sufficiently close to real data for frozen features to be helpful.",
            "contextual_requirements": "access to pretrained weights and careful empirical validation of whether freezing helps for particular synthetic generation procedures.",
            "generalizability": "context-dependent — freezing may help when synthetic data are close to real data or when limited variability would otherwise corrupt low-level features, but can be detrimental when synthetic data are highly varied (as with DR).",
            "knowledge_type": "explicit procedural steps and interpretive framework (protocol for training and rationale)",
            "uuid": "e561.4",
            "source_info": {
                "paper_title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomization",
                "publication_date_yy_mm": "2018-04"
            }
        },
        {
            "name_short": "Cut-Paste (Dwibedi)",
            "name_full": "Cut, Paste and Learn (pasting segmented real-object images onto backgrounds)",
            "brief_description": "A data synthesis procedure that composes real segmented object crops onto arbitrary background images to produce labeled training examples for instance detection, reducing the need for full scene rendering.",
            "citation_title": "Cut, paste and learn: Surprisingly easy synthesis for instance detection",
            "mention_or_use": "mention",
            "procedure_name": "Pasting segmented real images onto backgrounds for synthetic training data",
            "procedure_description": "Collect segmented object crops from real images and composite them onto varied background images (with appropriate augmentation) to create synthetic examples with automatic bounding-box labels for training detectors, avoiding expensive 3D modeling or rendering.",
            "procedure_type": "computational method / data augmentation / synthetic data generation",
            "source_domain": "image compositing / computer vision data augmentation",
            "target_domain": "object detection training for real-world images",
            "transfer_type": "analogical transfer (using real-image fragments to synthesize new training examples)",
            "modifications_made": "Referenced as an alternative to fully synthetic DR; not implemented in this paper. The authors note segmentation accuracy of objects is a challenge for this approach.",
            "transfer_success": "not evaluated in this paper (mentioned as related work); cited method showed promising results in original work.",
            "barriers_encountered": "requires accurate segmentation of objects from real images, and compositing realism/context consistency can be challenging.",
            "facilitating_factors": "uses real object appearance directly which may preserve realistic textures and shading without requiring 3D assets.",
            "contextual_requirements": "datasets of segmented object crops, background images, and compositing pipeline.",
            "generalizability": "applicable to many detection tasks where segmented object crops are available; usefulness depends on segmentation quality and contextual matching.",
            "knowledge_type": "explicit procedural steps and instrumental skills (compositing and dataset construction)",
            "uuid": "e561.5",
            "source_info": {
                "paper_title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomization",
                "publication_date_yy_mm": "2018-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2
        },
        {
            "paper_title": "CAD^2RL: Real single-image flight without a single real image",
            "rating": 2
        },
        {
            "paper_title": "Driving in the matrix: Can virtual worlds replace human-generated annotations for real world tasks?",
            "rating": 2
        },
        {
            "paper_title": "Cut, paste and learn: Surprisingly easy synthesis for instance detection",
            "rating": 2
        },
        {
            "paper_title": "On pre-trained image features and synthetic images for deep learning",
            "rating": 2
        },
        {
            "paper_title": "Learning deep object detectors from 3D models",
            "rating": 1
        }
    ],
    "cost": 0.01470575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomization</h1>
<p>Jonathan Tremblay<em> Aayush Prakash</em> David Acuna<em> ${ }^{</em> \dagger}$ Mark Brophy* Varun Jampani<br>Cem Anil ${ }^{\dagger}$ Thang To Eric Cameracci Shaad Boochoon Stan Birchfield<br>NVIDIA<br>${ }^{\dagger}$ also University of Toronto<br>{jtremblay, aayushp, dacunamarrer, markb, vjampani, thangt, ecameracci, sboochoon, sbirchfield}@nvidia.com<br>cem.anil@mail.utoronto.com</p>
<h4>Abstract</h4>
<p>We present a system for training deep neural networks for object detection using synthetic images. To handle the variability in real-world data, the system relies upon the technique of domain randomization, in which the parameters of the simulator-such as lighting, pose, object textures, etc.-are randomized in non-realistic ways to force the neural network to learn the essential features of the object of interest. We explore the importance of these parameters, showing that it is possible to produce a network with compelling performance using only non-artisticallygenerated synthetic data. With additional fine-tuning on real data, the network yields better performance than using real data alone. This result opens up the possibility of using inexpensive synthetic data for training neural networks while avoiding the need to collect large amounts of handannotated real-world data or to generate high-fidelity synthetic worlds-both of which remain bottlenecks for many applications. The approach is evaluated on bounding box detection of cars on the KITTI dataset.</p>
<h2>1. Introduction</h2>
<p>Training and testing a deep neural network is a timeconsuming and expensive task which typically involves collecting and manually annotating a large amount of data for supervised learning. This requirement is problematic when the task demands either expert knowledge, labels that are difficult to specify manually, or images that are difficult to capture in large quantities with sufficient variety. For example, 3D poses or pixelwise segmentation can take a substan-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tial amount of time for a human to manually label a single image.</p>
<p>A promising approach to overcome this limitation is to use a graphic simulator to generate automatically labeled data. Several such simulated datasets have been created in recent years $[1,9,4,18,24,36,19,27,26,7,21,33]$. For the most part, these datasets are expensive to generate, requiring artists to carefully model specific environments in detail. These datasets have been used successfully for training networks for geometric problems such as optical flow, scene flow, stereo disparity estimation, and camera pose estimation.</p>
<p>Even with this unprecedented access to high-fidelity ground truth data, it is not obvious how to effectively use such data to train neural networks to operate on real images. In particular, the expense required to generate photorealistic quality negates the primary selling point of synthetic data, namely, that arbitrarily large amounts of labeled data are available essentially for free. To solve this problem, domain randomization [32] is a recently proposed inexpensive approach that intentionally abandons photorealism by randomly perturbing the environment in non-photorealistic ways (e.g., by adding random textures) to force the network to learn to focus on the essential features of the image. This approach has been shown successful in tasks such as detecting the 3D coordinates of homogeneously colored cubes on a table [32] or determining the control commands of an indoor quadcopter [28], as well as for optical flow [4] and scene flow [18].</p>
<p>In this paper we extend domain randomization (DR) to the task of detection of real-world objects. In particular, we are interested in answering the following questions: 1) Can DR on synthetic data achieve compelling results on real-</p>
<p>world data? 2) How much does augmenting DR with real data during training improve accuracy? 3) How do the parameters of DR affect results? 4) How does DR compare to higher quality/more expensive synthetic datasets? In answering these questions, this work contributes the following:</p>
<ul>
<li>Extension of DR to non-trivial tasks such as detection of real objects in front of complex backgrounds;</li>
<li>Introduction of a new DR component, namely, flying distractors, which improves detection / estimation accuracy;</li>
<li>Investigation of the parameters of DR to evaluate their importance for these tasks;</li>
</ul>
<p>As shown in the experimental results, we achieve competitive results on real-world tasks when trained using only synthetic DR data. For example, our DR-based car detector achieves better results on the KITTI dataset than the same architecture trained on virtual KITTI [7], even though the latter dataset is highly correlated with the test set. Furthermore, augmenting synthetic DR data by fine-tuning on real data yields better results than training on real KITTI data alone.</p>
<h2>2. Previous Work</h2>
<p>The use of synthetic data for training and testing deep neural networks has gained in popularity in recent years, as evidenced by the availability of a large number of such datasets: Flying Chairs [4], FlyingThings3D [18], MPI Sintel [1], UnrealStereo [24, 36], SceneNet [9], SceneNet RGB-D [19], SYNTHIA [27], GTA V [26], Sim4CV [21], and Virtual KITTI [7], among others. These datasets were generated for the purpose of geometric problems such as optical flow, scene flow, stereo disparity estimation, and camera pose estimation.</p>
<p>Although some of these datasets also contains labels for object detection and semantic segmentation, few networks trained only on synthetic data for these tasks have appeared. Hinterstoisser et al. [11] used synthetic data generated by adding Gaussian noise to the object of interest and Gaussian blurring the object edges before composing over a background image. The resulting synthetic data are used to train the later layers of a neural network while freezing the early layers pretrained on real data (e.g., ImageNet). In contrast, we found this approach of freezing the weights to be harmful rather than helpful, as shown later.</p>
<p>The work of Johnson-Roberson et al. [15] used photorealistic synthetic data to train a car detector that was tested on the KITTI dataset. This work is closely related to ours, with the primary difference being our use of domain randomization rather than photorealistic images. Our experimental results reveal a similar conclusion, namely, that synthetic data
can rival, and in some cases beat, real data for training neural networks. Moreover, we show clear benefit from finetuning on real data after training on synthetic data. Other non-deep learning work that uses intensity edges from synthetic 3D models to detect isolated cars can be found in [29].</p>
<p>As an alternative to high-fidelity synthetic images, domain randomization (DR) was introduced by Tobin et al. [32], who propose to close the reality gap by generating synthetic data with sufficient variation that the network views real-world data as just another variation. Using DR, they trained a neural network to estimate the 3D world position of various shape-based objects with respect to a robotic arm fixed to a table. This introduction of DR was inspired by the earlier work of Sadeghi and Levine [28], who trained a quadcopter to fly indoors using only synthetic images. The Flying Chairs [4] and FlyingThings3D [18] datasets for optical flow and scene flow algorithms can be seen as versions of domain randomization.</p>
<p>The use of DR has also been explored to train robotics control policies. The network of James et al. [13] was used to cause a robot to pick a red cube and place it in a basket, and the network of Zhang et al. [35] was used to position a robot near a blue cube. Other work explores learning robotic policies from a high-fidelity rendering engine [14], generating high-fidelity synthetic data via a procedural approach [33], or training object classifiers from 3D CAD models [22]. In contrast to this body of research, our goal is to use synthetic data to train networks that detect complex, real-world objects.</p>
<p>A similar approach to domain randomization is to paste real images (rather than synthetic images) of objects on background images, as proposed by Dwibedi et al. [5]. One challenge with this approach is the accurate segmentation of objects from the background in a time-efficient manner.</p>
<h2>3. Domain Randomization</h2>
<p>Our approach to using domain randomization (DR) to generate synthetic data for training a neural network is illustrated in Fig. 1. We begin with 3D models of objects of interest (such as cars). A random number of these objects are placed in a 3D scene at random positions and orientations. To better enable the network to learn to ignore objects in the scene that are not of interest, a random number of geometric shapes are added to the scene. We call these flying distractors. Random textures are then applied to both the objects of interest and the flying distractors. A random number of lights of different types are inserted at random locations, and the scene is rendered from a random camera viewpoint, after which the result is composed over a random background image. The resulting images, with automatically generated ground truth labels (e.g., bounding boxes), are used for training the neural network.</p>
<p>More specifically, images were generated by randomly</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Domain randomization for object detection. Synthetic objects (in this case cars, top-center) are rendered on top of a random background (left) along with random flying distractors (geometric shapes next to the background images) in a scene with random lighting from random viewpoints. Before rendering, random texture is applied to the objects of interest as well as to the flying distractors. The resulting images, along with automatically-generated ground truth (right), are used for training a deep neural network.</p>
<p>varying the following aspects of the scene:</p>
<ul>
<li>number and types of objects, selected from a set of 36 downloaded 3D models of generic sedan and hatchback cars;</li>
<li>number, types, colors, and scales of distractors, selected from a set of 3D models (cones, pyramids, spheres, cylinders, partial toroids, arrows, pedestrians, trees, etc.);</li>
<li>texture on the object of interest, and background photograph, both taken from the Flickr 8K [12] dataset;</li>
<li>location of the virtual camera with respect to the scene (azimuth from 0° to 360°, elevation from 5° to 30°);</li>
<li>angle of the camera with respect to the scene (pan, tilt, and roll from -30° to 30°);</li>
<li>number and locations of point lights (from 1 to 12), in addition to a planar light for ambient illumination;</li>
<li>visibility of the ground plane.</li>
</ul>
<p>Note that all of these variations, while empowering the network to achieve more complex behavior, are nevertheless extremely easy to implement, requiring very little additional work beyond previous approaches to DR. Our pipeline uses an internally created plug-in to the Unreal Engine (UE4) that is capable of outputting 1200 × 400 images with annotations at 30 Hz.</p>
<p>A comparison of the synthetic images generated by our version of DR with the high-fidelity Virtual KITTI (VKITTI) dataset [7] is shown in Fig. 2. Although our crude (and almost cartoonish) images are not as aesthetically pleasing, this apparent limitation is arguably an asset: Not only are our images orders of magnitude faster to create (with less expertise required) but they include variations that force the deep neural network to focus on the important structure of the problem at hand rather than details that may or may not be present in real images at test time.</p>
<h2>4. Evaluation</h2>
<p>To quantify the performance of domain randomization (DR), in this section we compare the results of training an object detection deep neural network (DNN) using images generated by our DR-based approach with results of the same network trained using synthetic images from the Virtual KITTI (VKITTI) dataset [7]. The real-world KITTI dataset [8] was used for testing. Statistical distributions of these three datasets are shown in Fig. 3. Note that our DR-based approach makes it much easier to generate a dataset with a large amount of variety, compared with existing approaches.</p>
<h3>4.1. Object detection</h3>
<p>We trained three state-of-the-art neural networks using open-source implementations. In each case we used the feature extractor recommended by the respective authors. These three network architectures are briefly described as follows.</p>
<p>Faster R-CNN [25] detects objects in two stages. The first stage is a region proposal network (RPN) that generates</p>
<p><sup>1</sup>https://github.com/tensorflow/models/tree/master/research/slim</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Sample images from Virtual KITTI (first row), and our DR approach (second row). Note that our images are easier to create (because of their lack of fidelity) and yet contain more variety to force the deep neural network to focus on the structure of the objects of interest.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Statistics of the KITTI (top), virtual KITTI (middle), and our DR dataset (bottom). Shown are the distributions of the number of cars per image (left) and car centroid location (right). Note that with DR it is much easier to generate data with a wide distribution.</p>
<p>candidate regions of interest using extracted features along with the likelihood of finding an object in each of the proposed regions. In the second stage, features are cropped from the image using the proposed regions and fed to the remainder of the feature extractor, which predicts a probability density function over object class along with a refined class-specific bounding box for each proposal. The architecture is trained in an end-to-end fashion using a multi-task loss. For training, we used momentum [23] with a value of 0.9, and a learning rate of 0.0003. Inception-Resnet V2 [30] pretrained on ImageNet [3] was used as the feature extractor.</p>
<p><strong>R-FCN</strong> [2] is similar to Faster R-CNN. However, instead of cropping features from the same layer where region proposals are predicted, crops are taken from the last layer of features prior to prediction. The main idea behind this approach is to minimize the amount of per-region computation that must be done. Inference time is faster than Faster R-CNN but with comparable accuracy. For training we used RmsProp [31] with an initial learning rate of 0.0001, a decay step of 10000, a decay factor of 0.95, momentum and decay values of 0.9, and epsilon of 1.0. As with Faster R-CNN, we used Inception-Resnet V2 pretrained on ImageNet as the feature extractor.</p>
<p><strong>SSD</strong> [17] uses a single feed-forward convolutional network to directly predict classes and anchor offsets without requiring a second stage per-proposal classification operation. The predictions are then followed by a non-maximum suppression step to produce the final detections. This architecture uses the same training strategies as Faster R-CNN. We used Resnet101 [10] pretrained on ImageNet as the feature extractor.</p>
<p>For our DR dataset, we generated 100K images containing no more than 14 cars each. As described in the previous section, each car instance was randomly picked from a set of 36 models, and a random texture from 8K choices was applied. For comparison, we used the VKITTI dataset composed of 2.5K images generated by the Unity 3D game en</p>
<table>
<thead>
<tr>
<th>Architecture</th>
<th>VKITTI [7]</th>
<th>DR (ours)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Faster R-CNN [25]</td>
<td>$\mathbf{7 9 . 7}$</td>
<td>78.1</td>
</tr>
<tr>
<td>R-FCN [2]</td>
<td>64.6</td>
<td>$\mathbf{7 1 . 5}$</td>
</tr>
<tr>
<td>SSD [17]</td>
<td>36.1</td>
<td>$\mathbf{4 6 . 3}$</td>
</tr>
</tbody>
</table>
<p>Table 1. Comparison of three different state-of-the-art object detectors trained on Virtual KITTI versus our DR-generated dataset. Shown are AP@0.5 numbers from a subset of the real-world KITTI dataset [8].
gine [7]. (Although our approach uses more images, these images come essentially for free since they are generated automatically.) Note that this VKITTI dataset was specifically rendered with the intention of recreating as closely as possible the original real-world KITTI dataset (used for testing).</p>
<p>During training, we applied the following data augmentations: random brightness, random contrast, and random Gaussian noise. We also included more classic augmentations to our training process, such as random flips, random resizing, box jitter, and random crop. For all architectures, training was stopped when performance on the test set saturated to avoid overfitting, and only the best results are reported. Every architecture was trained on a batch size of 4 on an NVIDIA DGX Station. (We have also trained on a Titan X with a smaller batch size with similar results.)</p>
<p>For testing, we used 500 images taken at random from the KITTI dataset [8]. Detection performance was evaluated using average precision (AP) [6], with detections judged to be true/false positives by measuring bounding box overlap with intersection over union (IoU) at least 0.5. In these experiments we only consider objects for evaluation that have a bounding box with height greater than 40 pixels and truncation lower than 0.15 , as in [8]. ${ }^{2}$</p>
<p>Table 1 compares the performance of the three architectures when trained on VKITTI versus our DR dataset. The highest scoring method, Faster R-CNN, performs better with VKITTI than with our data. In contrast, the other methods achieve higher AP with our DR dataset than with VKITTI, despite the fact that the latter is closely correlated with the test set, whereas the former are randomly generated.</p>
<p>Fig. 4 shows sample results from the best detector (Faster R-CNN) on the KITTI test set, after being trained on either our DR or VKITTI datasets. Notice that even though our network has never seen a real image (beyond pretraining of the early layers on ImageNet), it is able to successfully detect most of the cars. This surprising result illustrates the power of such a simple technique as DR for bridging the reality gap.</p>
<p>Precision-recall curves for the three architectures trained</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>on both DR and VKITTI are shown in Fig. 5. From these plots observe that DR actually consistently achieves higher precision than VKITTI for most values of recall for all architectures. This helps to explain the apparent inconsistency in Table 1.</p>
<p>On the other hand, for high values of recall, DR consistently achieves lower precision than VKITTI. This is likely due to a mismatch between the distribution of our DR-based data and the real KITTI data. We hypothesize that our simplified DR procedure prevents some variations observed in the test set from being generated. For example, image context is ignored by our procedure, so that the structure inherent in parked cars is not taken into account.</p>
<p>In an additional experiment, we explored the benefits of fine-tuning [34] on real images after first training on synthetic images. For fine-tuning, the learning rate was decreased by a factor of ten while keeping the rest of the hyperparameters unchanged, the gradient was allowed to fully flow from end-to-end, and the Faster R-CNN network was trained until convergence. Results of VKITTI versus DR as a function of the amount of real data used is shown in Fig. 6. (For comparison, the figure also shows results after training only on real images at the original learning rate of 0.0003 .) Note that DR surpasses VKITTI as the number of real images increases, likely due to the fact that the advantage of the latter becomes less important as real images that resemble the synthetic images are added. With fine-tuning on all 6000 real images, our DR-based approach achieves an AP score of 98.5 , which is better than VKITTI by $1.6 \%$ and better than using only real data by $2.1 \%$.</p>
<h3>4.2. Ablation study</h3>
<p>To study the effects of the individual DR parameters, this section presents an ablation study by systematically omitting them one at a time. For this study, we used Faster RCNN [25] with Resnet V1 [10] pretrained on ImageNet as a feature extractor [3]. For training we used 50 K images, momentum [23] with a value of 0.9 , and a learning rate of 0.0003 . We used the same performance criterion as in the earlier detection evaluation, namely, AP@0.5 on the same KITTI test set.</p>
<p>Fig. 7 shows the results of omitting individual random components of the DR approach, showing the effect of these on detection performance, compared with the baseline ('full randomization'), which achieved an AP of 73.7. These components are described in detail below.</p>
<p>Lights variation. When the lights were randomized but the brightness and contrast were turned off ('no light augmentation'), the AP dropped barely, to 73.6. However, the AP dropped to 67.6 when the detector was trained on a fixed light ('fixed light'), thus showing the importance of using random lights.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Bounding box car detection on real KITTI images using Faster-RCNN trained only on synthetic data, either on the VKITTI dataset (middle) or our DR dataset (right). Note that our approach achieves results comparable to VKITTI, despite the fact that our training images do not resemble the test images.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Precision-recall curves for all trained models on DR (solid) and VKITTI (dashed).</p>
<p><strong>Texture.</strong> When no random textures were applied to the object of interest ('no texture'), the AP fell to 69.0. When half of the available textures were used ('4K textures'), that is, 4K rather than 8K, the AP fell to 71.5.</p>
<p><strong>Data augmentation.</strong> This includes randomized contrast, brightness, crop, flip, resizing, and additive Gaussian noise. The network achieved an AP of 72.0 when augmentation was turned off ('no augmentation').</p>
<p><strong>Flying distractors.</strong> These force the network to learn to ignore nearby patterns and to deal with partial occlusion of the object of interest. Without them ('no distractors'), the performance decreased by 1.1%.</p>
<h3>4.3. Training strategies</h3>
<p>We also studied the importance of the pretrained weights, the strategy of freezing these weights during training, and the impact of dataset size upon performance.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Results (AP@0.5) on real-world KITTI of Faster R-CNN fine-tuned on real images after training on synthetic images (VKITTI or DR), as a function of the number of real images used. For comparison, results after training only on real images are shown.</p>
<p><strong>Pretraining.</strong> For this experiment we used Faster R-CNN with Inception ResNet V2 as the feature extractor. To compare with pretraining on ImageNet (described earlier), this time we initialized the network weights using COCO [16]. Since the COCO weights were obtained by training on a dataset that already contains cars, we expected them to be able to perform with some reasonable amount of accuracy. We then trained the network, starting from this initialization, on both the VKITTI and our DR datasets.</p>
<p>The results are shown in Table 2. First, note that the</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Impact upon AP by omitting individual randomized components from the DR data generation procedure.</p>
<table>
<thead>
<tr>
<th>COCO</th>
<th>COCO+VKITTI</th>
<th>COCO+DR (Ours)</th>
</tr>
</thead>
<tbody>
<tr>
<td>56.1</td>
<td>79.7</td>
<td>83.7</td>
</tr>
</tbody>
</table>
<p>Table 2. AP performance on KITTI using COCO initialized weights. Shown are the results from no additional training, training on VKITTI, and training on DR. Note that our DR approach yields the highest performance.</p>
<p>COCO weights alone achieve an AP of only 56.1, showing that the real COCO images are not very useful for training a network to operate on the KITTI dataset. This is, in fact, a significant problem with networks today, namely, that they often fail to transfer from one dataset to another. Synthetic data, and in particular DR, has the potential to overcome this problem by enabling the network to learn features that are invariant to the specific dataset. Our DR approach achieves an AP of 83.7 when used to train the network pretrained on COCO, compared with an AP of 79.7 achieved by training on VKITTI. Thus, DR improves upon the performance of COCO and COCO+VKITTI by 49% and 5%, respectively.</p>
<p>Freezing weights. Hinterstoisser <em>et al.</em> [11] have recently proposed to freeze the weights of the early network layers (<em>i.e.</em>, the feature extraction weights pretrained on ImageNet) when learning from synthetic data. To test this idea, we trained Faster R-CNN and R-FCN using the same hyperparameters as in Section 4.1, except that we froze the weights initialized from ImageNet rather than allowing them to update during training. As shown in Table 3, we found that freezing the weights in this manner actually decreased rather than increased performance, contrary to the results of [11]. This effect was significant, degrading results by as much as 13.5%. We suspect that the large variety of our data enables full training to adapt these weights in an advantageous manner, and therefore freezing the weights hinders performance by preventing this adaptation.</p>
<table>
<thead>
<tr>
<th>architecture</th>
<th>freezing [11]</th>
<th>full (Ours)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Faster R-CNN [25]</td>
<td>66.4</td>
<td>78.1</td>
</tr>
<tr>
<td>R-FCN [2]</td>
<td>69.4</td>
<td>71.5</td>
</tr>
</tbody>
</table>
<p>Table 3. Comparing freezing early layers <em>vs.</em> full learning for different detection architectures.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Performance of Faster R-CNN as a function of the number of training images used, for both pretrained weights using ImageNet (red) and randomly initialized weights (yellow).</p>
<p>Dataset size. For this experiment we used the Faster R-CNN architecture with either randomly initialized weights or the Inception ResNet V2 weights. Using these two models, we explored the influence of dataset size upon prediction performance. We used the same data generation procedure explained earlier. The results, shown in Fig. 8, surprisingly reveal that performance saturates after only about 10K of training images with pretrained weights or after about 50K without pretraining. Conflicting somewhat with the findings of [20], we discovered that pretraining helps significantly even up to 1M images. This result can be explained by the fact that our training images are not photorealistic.</p>
<h1>5 Conclusion</h1>
<p>We have demonstrated that domain randomization (DR) is an effective technique to bridge the reality gap. Using synthetic DR data alone, we have trained a neural network to accomplish complex tasks like object detection with performance comparable to more labor-intensive (and therefore more expensive) datasets. By randomly perturbing the synthetic images during training, DR intentionally abandons photorealism to force the network to learn to focus on the relevant features. With fine-tuning on real images, we have shown that DR both outperforms more photorealistic datasets and improves upon results obtained using real data alone. Thus, using DR for training deep neural networks is a promising approach to leveraging the power of synthetic data. Future directions that should be explored</p>
<p>include using more object models, adding scene structure (e.g., parked cars), applying the technique to heavily textured objects (e.g., road signs), and further investigating the mixing of synthetic and real data to leverage the benefits of both.</p>
<h2>ACKNOWLEDGMENT</h2>
<p>We would like to thank Jan Kautz, Gavriel State, Kelly Guo, Omer Shapira, Sean Taylor, Hai Loc Lu, Bryan Dudash, and Willy Lau for the valuable insight they provided to this work.</p>
<h2>References</h2>
<p>[1] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A naturalistic open source movie for optical flow evaluation. In European Conference on Computer Vision (ECCV), pages 611-625, Oct. 2012. 1, 2
[2] J. Dai, Y. Li, K. He, and J. Sun. R-FCN: Object detection via region-based fully convolutional networks. In arXiv:1605.06409, 2016. 4, 5, 7
[3] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei. ImageNet: A large-scale hierarchical image database. In CVPR, 2009. 4, 5
[4] A. Dosovitskiy, P. Fischer, E. Ilg, P. Häusser, C. Hazırbaş, V. Golkov, P. Smagt, D. Cremers, and T. Brox. FlowNet: Learning optical flow with convolutional networks. In IEEE International Conference on Computer Vision (ICCV), 2015. 1,2
[5] D. Dwibedi, I. Misra, and M. Hebert. Cut, paste and learn: Surprisingly easy synthesis for instance detection. In ICCV, 2017. 2
[6] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The Pascal visual object classes challenge: A retrospective. International Journal of Computer Vision (IJCV), 111(1):98-136, Jan. 2015. 5
[7] A. Gaidon, Q. Wang, Y. Cabon, and E. Vig. Virtual worlds as proxy for multi-object tracking analysis. In CVPR, 2016. $1,2,3,5$
[8] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for autonomous driving? The KITTI vision benchmark suite. In CVPR, 2012. 3, 5
[9] A. Handa, V. Pătrăucean, V. Badrinarayanan, S. Stent, and R. Cipolla. SceneNet: Understanding real world indoor scenes with synthetic data. In arXiv:1511.07041, 2015. 1, 2
[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016. 4, 5
[11] S. Hinterstoisser, V. Lepetit, P. Wohlhart, and K. Konolige. On pre-trained image features and synthetic images for deep learning. In arXiv:1710.10710, 2017. 2, 7
[12] M. Hodosh, P. Young, and J. Hockenmaier. Framing image description as a ranking task: Data, models and evaluation metrics. Journal of Artificial Intelligence Research, 47:853899, 2013. 3
[13] S. James, A. J. Davison, and E. Johns. Transferring end-toend visuomotor control from simulation to real world for a multi-stage task. In arXiv:1707.02267, 2017. 2
[14] S. James and E. Johns. 3D simulation for robot arm control with deep Q-learning. In NIPS Workshop: Deep Learning for Action and Interaction, 2016. 2
[15] M. Johnson-Roberson, C. Barto, R. Mehta, S. N. Sridhar, K. Rosaen, and R. Vasudevan. Driving in the matrix: Can virtual worlds replace human-generated annotations for real world tasks? In ICRA, 2017. 2
[16] T. Lin, M. Maire, S. J. Belongie, L. D. Bourdev, R. B. Girshick, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft COCO: Common objects in context. In CVPR, 2014. 6
[17] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg. SSD: Single shot multibox detector. In $E C C V, 2016.4,5$
[18] N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy, and T. Brox. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In arXiv:1512.02134, Dec. 2015. 1, 2
[19] J. McCormac, A. Handa, and S. Leutenegger. SceneNet RGB-D: 5M photorealistic images of synthetic indoor trajectories with ground truth. In arXiv:1612.05079, 2016. 1, 2
[20] J. McCormac, A. Handa, S. Leutenegger, and A. J. Davison. Scenenet RGB-D: Can 5M synthetic images beat generic ImageNet pre-training on indoor segmentation? In ICCV, 2017. 7
[21] M. Mueller, V. Casser, J. Lahoud, N. Smith, and B. Ghanem. Sim4CV: A photo-realistic simulator for computer vision applications. In arXiv:1708.05869, 2017. 1, 2
[22] X. Peng, B. Sun, K. Ali, and K. Saenko. Learning deep object detectors from 3D models. In ICCV, 2015. 2
[23] N. Qian. On the momentum term in gradient descent learning algorithms. Neural Networks, 12(1):145-151, Jan. 1999. 4, 5
[24] W. Qiu and A. Yuille. UnrealCV: Connecting computer vision to Unreal Engine. In arXiv:1609.01326, 2016. 1, 2
[25] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In NIPS, pages 91-99, 2015. 3, 5, 7
[26] S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for data: Ground truth from computer games. In ECCV, 2016. 1,2
[27] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. Lopez. The SYNTHIA dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In CVPR, June 2016. 1, 2
[28] F. Sadeghi and S. Levine. CAD ${ }^{2}$ RL: Real single-image flight without a single real image. In Robotics: Science and Systems (RSS), 2017. 1, 2
[29] M. Stark, M. Goesele, and B. Schiele. Back to the future: Learning shape models from 3D CAD data. In BMVC, 2010. 2
[30] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the Inception architecture for computer vision. In arXiv:1512.00567, 2015. 4
[31] T. Tieleman and G. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magni-</p>
<p>tude. COURSERA: Neural networks for machine learning, 4(2):26-31, 2012. 4
[32] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2017. 1, 2
[33] A. Tsirikoglou, J. Kronander, M. Wrenninge, and J. Unger. Procedural modeling and physically based rendering for synthetic data generation in automotive applications. In arXiv:1710.06270, 2017. 1, 2
[34] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How transferable are features in deep neural networks? In NIPS, 2014. 5
[35] F. Zhang, J. Leitner, M. Milford, and P. Corke. Sim-to-real transfer of visuo-motor policies for reaching in clutter: Domain randomization and adaptation with modular networks. In arXiv:1709.05746, 2017. 2
[36] Y. Zhang, W. Qiu, Q. Chen, X. Hu, and A. Yuille. UnrealStereo: A synthetic dataset for analyzing stereo vision. In arXiv:1612.04647, 2016. 1, 2</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ These restrictions are the same as the "easy difficulty" on the KITTI Object Detection Evaluation website, http://www.cvlibs.net/ datasets/kitti/eval_object.php.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7078 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7078</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7078</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-d7c561960db46635d24678b171daf472fa72d9ff</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d7c561960db46635d24678b171daf472fa72d9ff" target="_blank">CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning</a></p>
                <p><strong>Paper Venue:</strong> Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> Chain of Mathematically Annotated Thought (CoMAT) is presented, which enhances reasoning through two stages: Symbolic Conversion and Reasoning Execution, offering a transparent reasoning process for complex mathematical tasks.</p>
                <p><strong>Paper Abstract:</strong> Mathematical reasoning remains a significant challenge for large language models (LLMs), despite progress in prompting techniques such as Chain-of-Thought (CoT). We present Chain of Mathematically Annotated Thought (CoMAT), which enhances reasoning through two stages: Symbolic Conversion (converting natural language queries into symbolic form) and Reasoning Execution (deriving answers from symbolic representations). CoMAT operates entirely with a single LLM and without external solvers. Across four LLMs, CoMAT outperforms traditional CoT on six out of seven benchmarks, achieving gains of 4.48% on MMLU-Redux (MATH) and 4.58% on GaoKao MCQ. In addition to improved performance, CoMAT ensures faithfulness and verifiability, offering a transparent reasoning process for complex mathematical tasks</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7078.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7078.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoMAT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain of Mathematically Annotated Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage, LLM-only symbolic prompting framework that first converts natural language mathematical queries into a standardized symbolic representation and then executes stepwise symbolic reasoning within the same LLM to produce verifiable answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CoMAT (applied with GPT-4o, Gemini-1.5-Pro, Qwen2-7b, Qwen2-72b)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a single pretrained model but a prompt/pipeline applied to existing transformer LLMs: symbolic-conversion (identification & definition, structural logic translation, explicit factual representation, question formalisation) followed by reasoning-execution using stepwise LLM decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (symbolic prompting / in-LLM symbolic reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not reported (CoMAT is a prompting/method technique applied to pretrained LLMs; no new pretraining datasets specified).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Symbolic Chain-of-Thought inside an LLM: two-stage pipeline (Symbolic Conversion -> Reasoning Execution) that enforces symbolic/formal steps and then prompts 'Let's think step-by-step' grounded on the symbolic form.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AQUA, GSM8K, MMLU-Redux, OlympiadBench (EN/CN), GaoKao (MCQ), MGSM (low-resource languages)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A mix of math word problems, multiple-choice mathematics exams (including GaoKao), MMLU-Redux subject-level math questions, and Olympiad-level problems (string- or multi-choice answers); includes low-resource-language MGSM subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Mathematical reasoning (math word problems), multiple-choice QA, string-valued answer generation, olympiad-level problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy / exact match (dataset-specific; exact matching for string answers and full-answer matching for MCQ)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Across four LLMs CoMAT outperforms traditional CoT on 6/7 benchmarks; reported example gains include +4.48% on MMLU-Redux (MATH) and +4.58% on GaoKao MCQ (abstract); Gemini-1.5-Pro: AQUA +3.54% exact match, Olympiad-EN +8.18% absolute; Qwen2-7b: Olympiad-EN 5.19% -> 20.92% (≈+15.73% abs); Qwen2-72b: Olympiad-EN 27.74% -> 32.17% (+4.43% abs); GPT-4o GaoKao 63.27% (CoT) -> 71.43% (CoMAT) (+8.16% abs).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Compared against Standard Prompting, CoT, and Faithful CoT where available; CoMAT often improves substantially on hard benchmarks (OlympiadBench, GaoKao) and gives modest gains or sometimes slight declines on easier tasks (e.g., AQUA declines for some models), e.g., Qwen2-7b average EN+MWP improves from 28.81% -> 55.66% in reported rows, Qwen2-72b average increases modestly.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Symbolic, in-LLM decomposition into a standardized symbolic form plus symbolic stepwise execution increases faithfulness, verifiability and empirical accuracy on challenging mathematical reasoning benchmarks, particularly for complex/multilingual/olympiad problems, without external solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Higher token and computational cost due to additional symbolic representations; evaluated only on the four symbolic steps described (other symbolic languages not tested); focused on mathematical reasoning—unknown generality to other logical domains; on some simpler datasets or already-strong models, CoMAT can slightly reduce accuracy (extra complexity can hurt).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7078.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7078.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Faithful CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Faithful Chain-of-Thought Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that combines natural-language decomposition with symbolic programs that are executed by deterministic external solvers to ensure faithful reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Faithful chain-of-thought reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Faithful CoT (baseline method; often used with GPT-4 for verifiers in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Approach mixes NL reasoning steps with symbolic language/programs (e.g., Python/LEAN) which are executed by external deterministic solvers to verify or derive final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer + external symbolic program / deterministic solver</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper (original Faithful CoT work describes dataset/benchmarks for MWP verification).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Symbolic Chain-of-Thought: generate symbolic programs or expressions and run them with external solvers for faithful execution; uses verifiers and deterministic execution of SL programs.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Uses external solvers/verifiers (e.g., program execution engines or theorem provers) to execute symbolic programs generated by the LLM and ensure faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, AQUA (and other MWP datasets where available)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Math word problem datasets and multiple-choice math benchmarks; Faithful CoT is primarily applied where external program execution is feasible.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Mathematical reasoning / math word problems with program-executable subproblems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy / exact match</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported in this paper: Faithful CoT shows a small gain on GSM8K (93.70% -> 95.0% in Table 1). Faithful CoT achieves 73.6% on AQUA in the table (comparison row).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Faithful CoT slightly outperforms CoT on some MWP tasks (small gain on GSM8K), but fails to execute or produces invalid outputs on many GaoKao and OlympiadBench examples (thus excluded from those comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Faithful CoT can yield incremental improvements on standard MWP benchmarks by using executable symbolic steps, but its reliance on external execution limits applicability on complex/multilingual/olympiad tasks where code/program translation fails.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Depends on external solvers and code-generation correctness; external execution can fail if LLM produces syntactically invalid programs; not robust on multilingual and Olympiad-level problems per authors' reproduction—many examples fail to be executed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7078.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7078.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbolic CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbolic Chain-of-Thought Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting paradigm that mixes natural language steps with symbolic language/programs (e.g., Python, LEAN) so that some reasoning is represented symbolically and can be verified by a deterministic solver.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Faithful chain-of-thought reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Symbolic CoT (method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Hybrid NL+SL chain-of-thought where subproblems are handed to symbolic/programmatic fragments that can be executed by external tools for faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer + symbolic program execution</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified (method-level description; uses standard math/problem datasets in literature)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Combine NL decomposition with symbolic language/programs that deterministic solvers can run to derive final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Executes symbolic fragments (programs) using deterministic solvers or theorem provers to guarantee faithful execution.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Math word problems and logical reasoning benchmarks used in prior works (e.g., GSM8K, MATH-related datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Benchmarks where symbolic program execution is feasible; focuses on faithfulness of reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Mathematical reasoning, symbolic verification, program-executable subproblems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy / ability to execute proofs/programs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Not reported numerically in this paper beyond literature citations; prior work (Lyu et al. 2023) reports faithfulness gains on some MWP tasks but the current paper notes Symbolic CoT approaches require external solvers and can fail on difficult or multilingual datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Positioned as more faithful than plain CoT when external execution is reliable, but less broadly applicable than in-LLM symbolic approaches like CoMAT due to external-tool fragility.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Symbolic CoT can improve faithfulness through executable steps, but reliance on program execution limits applicability; motivates CoMAT's in-LLM symbolic strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires reliable code/program generation by LLMs and external solvers; brittle on complex tasks and languages where code translation or execution fails.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7078.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7078.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logic‑LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that converts natural language into symbolic syntax which is fed to external symbolic solvers to boost faithfulness of logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Logic-LM (framework mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline using LLMs to translate NL into symbolic representations and submitting them to external solvers to produce faithful logical reasoning outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer + external symbolic solver</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper (see original Logic-LM paper for details).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>NL -> symbolic translation by LLMs; execute with symbolic solvers; self-consistency and non-linear reasoning techniques may be used.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Relies on external symbolic solvers to execute the converted symbolic expressions/programs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Logical reasoning / theorem-proving style benchmarks (as used in referenced Logic-LM work)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Benchmarks requiring conversion to symbolic form and execution by solvers; improves faithfulness via solver execution.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Logical reasoning with symbolic execution</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Not specified in this paper (refer to original Logic-LM results).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Referenced as representative of approaches that use external tools and thus contrast with CoMAT's LLM-only approach.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Illustrates the class of approaches that gain faithfulness via external solver integration but inherit tool-dependence limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Assumes reliable NL->symbolic translation and solver availability; can fail when code/symbols are invalid or when tasks don't translate well to solver-friendly forms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7078.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7078.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAT‑LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SAT-LM: Satisfiability-aided language models using declarative prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that integrates SAT/SMT-style solvers into LLM reasoning by declaratively framing constraints and using solvers to check or derive consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Satlm: Satisfiability-aided language models using declarative prompting</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SAT-LM (framework)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Combines transformer LLM outputs framed as declarative constraints with satisfiability solvers to improve reasoning correctness and check consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer + SAT/SMT solver integration</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not reported in this paper (see SAT-LM publication).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Declarative prompting to produce constraints that are solved/verified by SMT/SAT solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Uses SAT/SMT solvers to perform or verify constraint satisfaction derived from LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Logical-constraint and reasoning tasks (refer to original SAT-LM work)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Tasks where logical constraints can be expressed and checked by satisfiability engines.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Constraint solving, logical consistency checking, satisfiability-aided reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Cited as alternative that uses external reasoning tooling, contrasting with CoMAT's internal symbolic execution.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Demonstrates that solver integration can improve faithfulness but also introduces dependencies on solver success and correct translation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on accurate constraint synthesis by the LLM and solver capability; brittle when translation to constraints is erroneous.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7078.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7078.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LeanReasoner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LeanReasoner: Boosting complex logical reasoning with Lean</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that uses the Lean theorem prover combined with LLMs to tackle complex formal logical reasoning by generating formal statements that are verified by Lean.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Leanreasoner: Boosting complex logical reasoning with lean</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LeanReasoner (framework)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses LLMs to produce formalized (Lean) statements or proofs which are then checked or completed using the Lean theorem prover to improve correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer + formal theorem prover (Lean)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified here (see original LeanReasoner paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Generate formal statements/proofs and use Lean prover for verification/completion.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Integration with the Lean theorem prover to validate or derive formal proofs produced/assisted by the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Formal theorem proving / complex logical reasoning benchmarks (refer to LeanReasoner publication)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Benchmarks for formal proof generation and verification where theorem provers like Lean can be applied.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Formal theorem proving, proof verification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Mentioned as part of recent work integrating provers with LLMs; contrasted with CoMAT which avoids external provers.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Shows value of external theorem provers for formal reasoning but shares the drawback of dependence on correct translation to formal syntax.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires precise formalization by LLMs and domain expertise; translation errors harm solver utility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7078.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7078.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (open/variant used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source, high-capability LLM variant used in the paper's experiments as one of the target models evaluated with CoT and CoMAT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source large language model (variant of GPT-4 family) used as an experimental testbed in this paper; no internal architecture/training details reported here beyond citation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper (see GPT-4 technical report cited).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Evaluated with Standard Prompting, Chain-of-Thought (CoT), Faithful CoT (for some comparisons), and CoMAT prompting pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AQUA, GSM8K, MMLU-Redux, OlympiadBench, GaoKao, MGSM low-resource languages</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same mixed set of mathematical reasoning and QA benchmarks used across this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Mathematical reasoning / MWP / MCQ / olympiad problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy / exact match</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported table entries: non-CoT baselines and prompting variants show e.g., GPT-4o +CoT: AQUA 84.25%, GSM8K 94.46%, MMLU-Redux 88.10%, OlympBench 41.84%; +CoMAT: AQUA 83.46%, GSM8K 93.70%, MMLU-Redux 88.30%, OlympBench 40.42%; GaoKao: CoT 63.27% -> CoMAT 71.43%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>CoMAT improved GPT-4o on GaoKao significantly (+8.16% abs) but showed small declines on some other datasets (AQUA slight drop vs CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4o benefits from symbolic prompting on some hard benchmarks, with notable improvement on GaoKao; on already-strong benchmarks CoMAT may yield marginal or mixed effects.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Produces more invalid outputs in some contexts compared to GPT-4 (reported by authors), and CoMAT's token overhead causes higher compute/API usage when applied.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7078.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7078.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-1.5-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-1.5-Pro (Gemini family model variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source Gemini family LLM used in the paper; evaluated under Standard, CoT, and CoMAT prompting to measure gains on mathematical benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gemini: a family of highly capable multimodal models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-1.5-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Gemini family multimodal transformer model (specific internal size/details not reported in this paper); used as a closed-source experimental model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer / multimodal family</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper (see Gemini technical report cited).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Evaluated with Standard Prompting, Chain-of-Thought (CoT), and CoMAT prompting pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AQUA, GSM8K, MMLU-Redux, OlympiadBench (EN/CN), GaoKao</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Mathematical reasoning and exam-style MCQ and string-answer datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Mathematical reasoning, multiple-choice and free-response math problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy / exact match</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table-reported examples: Gemini +CoT: AQUA 75.20%, GSM8K 90.51%, MMLU-Redux 79.55%, OlympBench 21.28%; +CoMAT: AQUA 78.74%, GSM8K 90.43%, MMLU-Redux 79.71%, OlympBench 29.46% (Olymp-EN +8.18% abs improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>CoMAT produced notable gains on Olympiad-EN (+8.18% abs) and improved overall average performance relative to CoT in certain cross-dataset averages.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Gemini-1.5-Pro shows strong gains on challenging benchmarks when applying CoMAT, especially on olympiad-level tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Performance varies across datasets; CoMAT increases token usage and computation; some small declines possible on easier tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7078.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7078.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2-7b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2-7b</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source Qwen2 model family member (7B parameters) evaluated in the paper under standard prompting, CoT, and CoMAT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Qwen2 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2-7b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source transformer LLM in the Qwen2 family; authors evaluated this 7B-sized model under different prompting regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not detailed in this paper (see Qwen2 technical report); used as pre-trained model.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Evaluated using Standard Prompting, Chain-of-Thought prompting, and CoMAT symbolic prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AQUA, GSM8K, MMLU-Redux, OlympiadBench (EN/CN), GaoKao</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A mixture of math word problems, multiple-choice math tests and olympiad-level problems used to probe reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Mathematical reasoning and exam-style QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy / exact match</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table entries: non-CoT baseline averages low; with CoT/Qwen2-7b reported e.g., non-CoT averages: AQUA 34.25%, GSM8K 19.29%, MMLU-Redux 56.52%, OlympBench 5.19% (avg 28.81%). With CoMAT: AQUA 42.13%, GSM8K 79.80%, MMLU-Redux 79.80%, OlympBench 20.92% (substantial absolute gains on Olympiad-EN: 5.19% -> 20.92%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>CoMAT yields large relative and absolute improvements for Qwen2-7b on hard math/olympiad tasks (e.g., Olympiad-EN +≈15.7% abs), and improves average EN+MWP performance substantially compared to baseline prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Smaller open-source models can benefit markedly from structured symbolic prompting, dramatically improving previously poor performance on hard benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>On some datasets CoMAT still does not reach top-tier accuracy; CoMAT increases token usage and computational cost; CoT sometimes outperforms CoMAT on certain metrics/datasets depending on model and task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7078.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7078.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2-72b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2-72b</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger open-source Qwen2 family model (72B parameters) evaluated with standard prompting, CoT, and CoMAT in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Qwen2 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2-72b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source transformer LLM in the Qwen2 family; large-capacity model evaluated under different prompting regimes in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper (see Qwen2 technical report).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Evaluated with Standard Prompting, Chain-of-Thought prompting, and CoMAT symbolic prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AQUA, GSM8K, MMLU-Redux, OlympiadBench, GaoKao</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Mathematical reasoning and multiple-choice/math exam datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Mathematical reasoning / MWP / exam QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy / exact match</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported table examples: non-CoT AQUA 51.97%, GSM8K 37.43%, MMLU-Redux 66.53%, OlympBench 10.39%; +CoT AQUA 79.13% -> +CoMAT AQUA 72.44% (decline on AQUA for this model), +CoMAT improves Olympiad-EN 27.74% -> 32.17% (+4.43% abs); average CN/EN improvements modest.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>CoMAT improved performance on hard benchmarks like OlympiadBench and GaoKao for Qwen2-72b but had mixed effects on simpler tasks (AQUA decline observed), showing that larger models can have dataset-dependent reactions to symbolic prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large open-source models can benefit from CoMAT on challenging tasks but may show small regressions on tasks where CoT already performs strongly.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Inconsistent effects across tasks; extra token and compute cost; not universally beneficial on all benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Faithful chain-of-thought reasoning <em>(Rating: 2)</em></li>
                <li>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 2)</em></li>
                <li>Satlm: Satisfiability-aided language models using declarative prompting <em>(Rating: 2)</em></li>
                <li>Leanreasoner: Boosting complex logical reasoning with lean <em>(Rating: 2)</em></li>
                <li>Faithful logical reasoning via symbolic chain-of-thought <em>(Rating: 2)</em></li>
                <li>Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Gpt-4 technical report <em>(Rating: 1)</em></li>
                <li>Qwen2 technical report <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7078",
    "paper_id": "paper-d7c561960db46635d24678b171daf472fa72d9ff",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "CoMAT",
            "name_full": "Chain of Mathematically Annotated Thought",
            "brief_description": "A two-stage, LLM-only symbolic prompting framework that first converts natural language mathematical queries into a standardized symbolic representation and then executes stepwise symbolic reasoning within the same LLM to produce verifiable answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CoMAT (applied with GPT-4o, Gemini-1.5-Pro, Qwen2-7b, Qwen2-72b)",
            "model_description": "Not a single pretrained model but a prompt/pipeline applied to existing transformer LLMs: symbolic-conversion (identification & definition, structural logic translation, explicit factual representation, question formalisation) followed by reasoning-execution using stepwise LLM decoding.",
            "model_size": null,
            "architecture_type": "transformer (symbolic prompting / in-LLM symbolic reasoning)",
            "training_data": "Not reported (CoMAT is a prompting/method technique applied to pretrained LLMs; no new pretraining datasets specified).",
            "reasoning_method": "Symbolic Chain-of-Thought inside an LLM: two-stage pipeline (Symbolic Conversion -&gt; Reasoning Execution) that enforces symbolic/formal steps and then prompts 'Let's think step-by-step' grounded on the symbolic form.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "AQUA, GSM8K, MMLU-Redux, OlympiadBench (EN/CN), GaoKao (MCQ), MGSM (low-resource languages)",
            "benchmark_description": "A mix of math word problems, multiple-choice mathematics exams (including GaoKao), MMLU-Redux subject-level math questions, and Olympiad-level problems (string- or multi-choice answers); includes low-resource-language MGSM subsets.",
            "task_type": "Mathematical reasoning (math word problems), multiple-choice QA, string-valued answer generation, olympiad-level problem solving",
            "performance_metric": "accuracy / exact match (dataset-specific; exact matching for string answers and full-answer matching for MCQ)",
            "performance_value": "Across four LLMs CoMAT outperforms traditional CoT on 6/7 benchmarks; reported example gains include +4.48% on MMLU-Redux (MATH) and +4.58% on GaoKao MCQ (abstract); Gemini-1.5-Pro: AQUA +3.54% exact match, Olympiad-EN +8.18% absolute; Qwen2-7b: Olympiad-EN 5.19% -&gt; 20.92% (≈+15.73% abs); Qwen2-72b: Olympiad-EN 27.74% -&gt; 32.17% (+4.43% abs); GPT-4o GaoKao 63.27% (CoT) -&gt; 71.43% (CoMAT) (+8.16% abs).",
            "comparison_with_baseline": "Compared against Standard Prompting, CoT, and Faithful CoT where available; CoMAT often improves substantially on hard benchmarks (OlympiadBench, GaoKao) and gives modest gains or sometimes slight declines on easier tasks (e.g., AQUA declines for some models), e.g., Qwen2-7b average EN+MWP improves from 28.81% -&gt; 55.66% in reported rows, Qwen2-72b average increases modestly.",
            "key_findings": "Symbolic, in-LLM decomposition into a standardized symbolic form plus symbolic stepwise execution increases faithfulness, verifiability and empirical accuracy on challenging mathematical reasoning benchmarks, particularly for complex/multilingual/olympiad problems, without external solvers.",
            "limitations": "Higher token and computational cost due to additional symbolic representations; evaluated only on the four symbolic steps described (other symbolic languages not tested); focused on mathematical reasoning—unknown generality to other logical domains; on some simpler datasets or already-strong models, CoMAT can slightly reduce accuracy (extra complexity can hurt).",
            "uuid": "e7078.0",
            "source_info": {
                "paper_title": "CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Faithful CoT",
            "name_full": "Faithful Chain-of-Thought Reasoning",
            "brief_description": "A method that combines natural-language decomposition with symbolic programs that are executed by deterministic external solvers to ensure faithful reasoning traces.",
            "citation_title": "Faithful chain-of-thought reasoning",
            "mention_or_use": "use",
            "model_name": "Faithful CoT (baseline method; often used with GPT-4 for verifiers in this paper)",
            "model_description": "Approach mixes NL reasoning steps with symbolic language/programs (e.g., Python/LEAN) which are executed by external deterministic solvers to verify or derive final answers.",
            "model_size": null,
            "architecture_type": "transformer + external symbolic program / deterministic solver",
            "training_data": "Not specified in this paper (original Faithful CoT work describes dataset/benchmarks for MWP verification).",
            "reasoning_method": "Symbolic Chain-of-Thought: generate symbolic programs or expressions and run them with external solvers for faithful execution; uses verifiers and deterministic execution of SL programs.",
            "external_tool_used": true,
            "external_tool_description": "Uses external solvers/verifiers (e.g., program execution engines or theorem provers) to execute symbolic programs generated by the LLM and ensure faithfulness.",
            "benchmark_name": "GSM8K, AQUA (and other MWP datasets where available)",
            "benchmark_description": "Math word problem datasets and multiple-choice math benchmarks; Faithful CoT is primarily applied where external program execution is feasible.",
            "task_type": "Mathematical reasoning / math word problems with program-executable subproblems",
            "performance_metric": "accuracy / exact match",
            "performance_value": "Reported in this paper: Faithful CoT shows a small gain on GSM8K (93.70% -&gt; 95.0% in Table 1). Faithful CoT achieves 73.6% on AQUA in the table (comparison row).",
            "comparison_with_baseline": "Faithful CoT slightly outperforms CoT on some MWP tasks (small gain on GSM8K), but fails to execute or produces invalid outputs on many GaoKao and OlympiadBench examples (thus excluded from those comparisons).",
            "key_findings": "Faithful CoT can yield incremental improvements on standard MWP benchmarks by using executable symbolic steps, but its reliance on external execution limits applicability on complex/multilingual/olympiad tasks where code/program translation fails.",
            "limitations": "Depends on external solvers and code-generation correctness; external execution can fail if LLM produces syntactically invalid programs; not robust on multilingual and Olympiad-level problems per authors' reproduction—many examples fail to be executed.",
            "uuid": "e7078.1",
            "source_info": {
                "paper_title": "CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Symbolic CoT",
            "name_full": "Symbolic Chain-of-Thought Prompting",
            "brief_description": "A prompting paradigm that mixes natural language steps with symbolic language/programs (e.g., Python, LEAN) so that some reasoning is represented symbolically and can be verified by a deterministic solver.",
            "citation_title": "Faithful chain-of-thought reasoning",
            "mention_or_use": "mention",
            "model_name": "Symbolic CoT (method)",
            "model_description": "Hybrid NL+SL chain-of-thought where subproblems are handed to symbolic/programmatic fragments that can be executed by external tools for faithfulness.",
            "model_size": null,
            "architecture_type": "transformer + symbolic program execution",
            "training_data": "Not specified (method-level description; uses standard math/problem datasets in literature)",
            "reasoning_method": "Combine NL decomposition with symbolic language/programs that deterministic solvers can run to derive final answers.",
            "external_tool_used": true,
            "external_tool_description": "Executes symbolic fragments (programs) using deterministic solvers or theorem provers to guarantee faithful execution.",
            "benchmark_name": "Math word problems and logical reasoning benchmarks used in prior works (e.g., GSM8K, MATH-related datasets)",
            "benchmark_description": "Benchmarks where symbolic program execution is feasible; focuses on faithfulness of reasoning chains.",
            "task_type": "Mathematical reasoning, symbolic verification, program-executable subproblems",
            "performance_metric": "accuracy / ability to execute proofs/programs",
            "performance_value": "Not reported numerically in this paper beyond literature citations; prior work (Lyu et al. 2023) reports faithfulness gains on some MWP tasks but the current paper notes Symbolic CoT approaches require external solvers and can fail on difficult or multilingual datasets.",
            "comparison_with_baseline": "Positioned as more faithful than plain CoT when external execution is reliable, but less broadly applicable than in-LLM symbolic approaches like CoMAT due to external-tool fragility.",
            "key_findings": "Symbolic CoT can improve faithfulness through executable steps, but reliance on program execution limits applicability; motivates CoMAT's in-LLM symbolic strategy.",
            "limitations": "Requires reliable code/program generation by LLMs and external solvers; brittle on complex tasks and languages where code translation or execution fails.",
            "uuid": "e7078.2",
            "source_info": {
                "paper_title": "CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Logic‑LM",
            "name_full": "Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "brief_description": "A framework that converts natural language into symbolic syntax which is fed to external symbolic solvers to boost faithfulness of logical reasoning.",
            "citation_title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "mention_or_use": "mention",
            "model_name": "Logic-LM (framework mentioned in related work)",
            "model_description": "Pipeline using LLMs to translate NL into symbolic representations and submitting them to external solvers to produce faithful logical reasoning outcomes.",
            "model_size": null,
            "architecture_type": "transformer + external symbolic solver",
            "training_data": "Not specified in this paper (see original Logic-LM paper for details).",
            "reasoning_method": "NL -&gt; symbolic translation by LLMs; execute with symbolic solvers; self-consistency and non-linear reasoning techniques may be used.",
            "external_tool_used": true,
            "external_tool_description": "Relies on external symbolic solvers to execute the converted symbolic expressions/programs.",
            "benchmark_name": "Logical reasoning / theorem-proving style benchmarks (as used in referenced Logic-LM work)",
            "benchmark_description": "Benchmarks requiring conversion to symbolic form and execution by solvers; improves faithfulness via solver execution.",
            "task_type": "Logical reasoning with symbolic execution",
            "performance_metric": "Not specified in this paper (refer to original Logic-LM results).",
            "performance_value": "",
            "comparison_with_baseline": "Referenced as representative of approaches that use external tools and thus contrast with CoMAT's LLM-only approach.",
            "key_findings": "Illustrates the class of approaches that gain faithfulness via external solver integration but inherit tool-dependence limitations.",
            "limitations": "Assumes reliable NL-&gt;symbolic translation and solver availability; can fail when code/symbols are invalid or when tasks don't translate well to solver-friendly forms.",
            "uuid": "e7078.3",
            "source_info": {
                "paper_title": "CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "SAT‑LM",
            "name_full": "SAT-LM: Satisfiability-aided language models using declarative prompting",
            "brief_description": "A method that integrates SAT/SMT-style solvers into LLM reasoning by declaratively framing constraints and using solvers to check or derive consistency.",
            "citation_title": "Satlm: Satisfiability-aided language models using declarative prompting",
            "mention_or_use": "mention",
            "model_name": "SAT-LM (framework)",
            "model_description": "Combines transformer LLM outputs framed as declarative constraints with satisfiability solvers to improve reasoning correctness and check consistency.",
            "model_size": null,
            "architecture_type": "transformer + SAT/SMT solver integration",
            "training_data": "Not reported in this paper (see SAT-LM publication).",
            "reasoning_method": "Declarative prompting to produce constraints that are solved/verified by SMT/SAT solvers.",
            "external_tool_used": true,
            "external_tool_description": "Uses SAT/SMT solvers to perform or verify constraint satisfaction derived from LLM outputs.",
            "benchmark_name": "Logical-constraint and reasoning tasks (refer to original SAT-LM work)",
            "benchmark_description": "Tasks where logical constraints can be expressed and checked by satisfiability engines.",
            "task_type": "Constraint solving, logical consistency checking, satisfiability-aided reasoning",
            "performance_metric": "Not specified in this paper.",
            "performance_value": "",
            "comparison_with_baseline": "Cited as alternative that uses external reasoning tooling, contrasting with CoMAT's internal symbolic execution.",
            "key_findings": "Demonstrates that solver integration can improve faithfulness but also introduces dependencies on solver success and correct translation.",
            "limitations": "Relies on accurate constraint synthesis by the LLM and solver capability; brittle when translation to constraints is erroneous.",
            "uuid": "e7078.4",
            "source_info": {
                "paper_title": "CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LeanReasoner",
            "name_full": "LeanReasoner: Boosting complex logical reasoning with Lean",
            "brief_description": "An approach that uses the Lean theorem prover combined with LLMs to tackle complex formal logical reasoning by generating formal statements that are verified by Lean.",
            "citation_title": "Leanreasoner: Boosting complex logical reasoning with lean",
            "mention_or_use": "mention",
            "model_name": "LeanReasoner (framework)",
            "model_description": "Uses LLMs to produce formalized (Lean) statements or proofs which are then checked or completed using the Lean theorem prover to improve correctness.",
            "model_size": null,
            "architecture_type": "transformer + formal theorem prover (Lean)",
            "training_data": "Not specified here (see original LeanReasoner paper).",
            "reasoning_method": "Generate formal statements/proofs and use Lean prover for verification/completion.",
            "external_tool_used": true,
            "external_tool_description": "Integration with the Lean theorem prover to validate or derive formal proofs produced/assisted by the LLM.",
            "benchmark_name": "Formal theorem proving / complex logical reasoning benchmarks (refer to LeanReasoner publication)",
            "benchmark_description": "Benchmarks for formal proof generation and verification where theorem provers like Lean can be applied.",
            "task_type": "Formal theorem proving, proof verification",
            "performance_metric": "Not provided in this paper.",
            "performance_value": "",
            "comparison_with_baseline": "Mentioned as part of recent work integrating provers with LLMs; contrasted with CoMAT which avoids external provers.",
            "key_findings": "Shows value of external theorem provers for formal reasoning but shares the drawback of dependence on correct translation to formal syntax.",
            "limitations": "Requires precise formalization by LLMs and domain expertise; translation errors harm solver utility.",
            "uuid": "e7078.5",
            "source_info": {
                "paper_title": "CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (open/variant used in experiments)",
            "brief_description": "A closed-source, high-capability LLM variant used in the paper's experiments as one of the target models evaluated with CoT and CoMAT prompting.",
            "citation_title": "Gpt-4 technical report",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Closed-source large language model (variant of GPT-4 family) used as an experimental testbed in this paper; no internal architecture/training details reported here beyond citation.",
            "model_size": null,
            "architecture_type": "transformer",
            "training_data": "Not specified in this paper (see GPT-4 technical report cited).",
            "reasoning_method": "Evaluated with Standard Prompting, Chain-of-Thought (CoT), Faithful CoT (for some comparisons), and CoMAT prompting pipelines.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "AQUA, GSM8K, MMLU-Redux, OlympiadBench, GaoKao, MGSM low-resource languages",
            "benchmark_description": "Same mixed set of mathematical reasoning and QA benchmarks used across this paper.",
            "task_type": "Mathematical reasoning / MWP / MCQ / olympiad problems",
            "performance_metric": "accuracy / exact match",
            "performance_value": "Reported table entries: non-CoT baselines and prompting variants show e.g., GPT-4o +CoT: AQUA 84.25%, GSM8K 94.46%, MMLU-Redux 88.10%, OlympBench 41.84%; +CoMAT: AQUA 83.46%, GSM8K 93.70%, MMLU-Redux 88.30%, OlympBench 40.42%; GaoKao: CoT 63.27% -&gt; CoMAT 71.43%.",
            "comparison_with_baseline": "CoMAT improved GPT-4o on GaoKao significantly (+8.16% abs) but showed small declines on some other datasets (AQUA slight drop vs CoT).",
            "key_findings": "GPT-4o benefits from symbolic prompting on some hard benchmarks, with notable improvement on GaoKao; on already-strong benchmarks CoMAT may yield marginal or mixed effects.",
            "limitations": "Produces more invalid outputs in some contexts compared to GPT-4 (reported by authors), and CoMAT's token overhead causes higher compute/API usage when applied.",
            "uuid": "e7078.6",
            "source_info": {
                "paper_title": "CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Gemini-1.5-Pro",
            "name_full": "Gemini-1.5-Pro (Gemini family model variant)",
            "brief_description": "A closed-source Gemini family LLM used in the paper; evaluated under Standard, CoT, and CoMAT prompting to measure gains on mathematical benchmarks.",
            "citation_title": "Gemini: a family of highly capable multimodal models",
            "mention_or_use": "use",
            "model_name": "Gemini-1.5-Pro",
            "model_description": "Gemini family multimodal transformer model (specific internal size/details not reported in this paper); used as a closed-source experimental model.",
            "model_size": null,
            "architecture_type": "transformer / multimodal family",
            "training_data": "Not specified in this paper (see Gemini technical report cited).",
            "reasoning_method": "Evaluated with Standard Prompting, Chain-of-Thought (CoT), and CoMAT prompting pipelines.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "AQUA, GSM8K, MMLU-Redux, OlympiadBench (EN/CN), GaoKao",
            "benchmark_description": "Mathematical reasoning and exam-style MCQ and string-answer datasets.",
            "task_type": "Mathematical reasoning, multiple-choice and free-response math problems",
            "performance_metric": "accuracy / exact match",
            "performance_value": "Table-reported examples: Gemini +CoT: AQUA 75.20%, GSM8K 90.51%, MMLU-Redux 79.55%, OlympBench 21.28%; +CoMAT: AQUA 78.74%, GSM8K 90.43%, MMLU-Redux 79.71%, OlympBench 29.46% (Olymp-EN +8.18% abs improvement).",
            "comparison_with_baseline": "CoMAT produced notable gains on Olympiad-EN (+8.18% abs) and improved overall average performance relative to CoT in certain cross-dataset averages.",
            "key_findings": "Gemini-1.5-Pro shows strong gains on challenging benchmarks when applying CoMAT, especially on olympiad-level tasks.",
            "limitations": "Performance varies across datasets; CoMAT increases token usage and computation; some small declines possible on easier tasks.",
            "uuid": "e7078.7",
            "source_info": {
                "paper_title": "CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Qwen2-7b",
            "name_full": "Qwen2-7b",
            "brief_description": "An open-source Qwen2 model family member (7B parameters) evaluated in the paper under standard prompting, CoT, and CoMAT.",
            "citation_title": "Qwen2 technical report",
            "mention_or_use": "use",
            "model_name": "Qwen2-7b",
            "model_description": "Open-source transformer LLM in the Qwen2 family; authors evaluated this 7B-sized model under different prompting regimes.",
            "model_size": "7B",
            "architecture_type": "transformer",
            "training_data": "Not detailed in this paper (see Qwen2 technical report); used as pre-trained model.",
            "reasoning_method": "Evaluated using Standard Prompting, Chain-of-Thought prompting, and CoMAT symbolic prompting.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "AQUA, GSM8K, MMLU-Redux, OlympiadBench (EN/CN), GaoKao",
            "benchmark_description": "A mixture of math word problems, multiple-choice math tests and olympiad-level problems used to probe reasoning.",
            "task_type": "Mathematical reasoning and exam-style QA",
            "performance_metric": "accuracy / exact match",
            "performance_value": "Table entries: non-CoT baseline averages low; with CoT/Qwen2-7b reported e.g., non-CoT averages: AQUA 34.25%, GSM8K 19.29%, MMLU-Redux 56.52%, OlympBench 5.19% (avg 28.81%). With CoMAT: AQUA 42.13%, GSM8K 79.80%, MMLU-Redux 79.80%, OlympBench 20.92% (substantial absolute gains on Olympiad-EN: 5.19% -&gt; 20.92%).",
            "comparison_with_baseline": "CoMAT yields large relative and absolute improvements for Qwen2-7b on hard math/olympiad tasks (e.g., Olympiad-EN +≈15.7% abs), and improves average EN+MWP performance substantially compared to baseline prompting.",
            "key_findings": "Smaller open-source models can benefit markedly from structured symbolic prompting, dramatically improving previously poor performance on hard benchmarks.",
            "limitations": "On some datasets CoMAT still does not reach top-tier accuracy; CoMAT increases token usage and computational cost; CoT sometimes outperforms CoMAT on certain metrics/datasets depending on model and task.",
            "uuid": "e7078.8",
            "source_info": {
                "paper_title": "CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Qwen2-72b",
            "name_full": "Qwen2-72b",
            "brief_description": "A larger open-source Qwen2 family model (72B parameters) evaluated with standard prompting, CoT, and CoMAT in this paper.",
            "citation_title": "Qwen2 technical report",
            "mention_or_use": "use",
            "model_name": "Qwen2-72b",
            "model_description": "Open-source transformer LLM in the Qwen2 family; large-capacity model evaluated under different prompting regimes in the experiments.",
            "model_size": "72B",
            "architecture_type": "transformer",
            "training_data": "Not specified in this paper (see Qwen2 technical report).",
            "reasoning_method": "Evaluated with Standard Prompting, Chain-of-Thought prompting, and CoMAT symbolic prompting.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "AQUA, GSM8K, MMLU-Redux, OlympiadBench, GaoKao",
            "benchmark_description": "Mathematical reasoning and multiple-choice/math exam datasets.",
            "task_type": "Mathematical reasoning / MWP / exam QA",
            "performance_metric": "accuracy / exact match",
            "performance_value": "Reported table examples: non-CoT AQUA 51.97%, GSM8K 37.43%, MMLU-Redux 66.53%, OlympBench 10.39%; +CoT AQUA 79.13% -&gt; +CoMAT AQUA 72.44% (decline on AQUA for this model), +CoMAT improves Olympiad-EN 27.74% -&gt; 32.17% (+4.43% abs); average CN/EN improvements modest.",
            "comparison_with_baseline": "CoMAT improved performance on hard benchmarks like OlympiadBench and GaoKao for Qwen2-72b but had mixed effects on simpler tasks (AQUA decline observed), showing that larger models can have dataset-dependent reactions to symbolic prompting.",
            "key_findings": "Large open-source models can benefit from CoMAT on challenging tasks but may show small regressions on tasks where CoT already performs strongly.",
            "limitations": "Inconsistent effects across tasks; extra token and compute cost; not universally beneficial on all benchmarks.",
            "uuid": "e7078.9",
            "source_info": {
                "paper_title": "CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Faithful chain-of-thought reasoning",
            "rating": 2
        },
        {
            "paper_title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 2
        },
        {
            "paper_title": "Satlm: Satisfiability-aided language models using declarative prompting",
            "rating": 2
        },
        {
            "paper_title": "Leanreasoner: Boosting complex logical reasoning with lean",
            "rating": 2
        },
        {
            "paper_title": "Faithful logical reasoning via symbolic chain-of-thought",
            "rating": 2
        },
        {
            "paper_title": "Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models",
            "rating": 1
        },
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 1
        },
        {
            "paper_title": "Qwen2 technical report",
            "rating": 1
        }
    ],
    "cost": 0.0211755,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning</h1>
<p>Joshua Ong Jun Leang Aryo Pradipta Gema Shay B. Cohen<br>School of Informatics, The University of Edinburgh<br>{jong2, aryo.gema, scohen}@ed.ac.uk</p>
<h4>Abstract</h4>
<p>Mathematical reasoning remains a significant challenge for large language models (LLMs), despite progress in prompting techniques such as Chain-of-Thought (CoT). We present Chain of Mathematically Annotated Thought (CoMAT), which enhances reasoning through two stages: Symbolic Conversion (converting natural language queries into symbolic form) and Reasoning Execution (deriving answers from symbolic representations). CoMAT operates entirely with a single LLM and without external solvers. Across four LLMs, CoMAT outperforms traditional CoT on six out of seven benchmarks, achieving gains of $4.48 \%$ on MMLURedux (MATH) and 4.58\% on GaoKao MCQ. In addition to improved performance, CoMAT ensures faithfulness and verifiability, offering a transparent reasoning process for complex mathematical tasks ${ }^{1}$.</p>
<h2>1 Introduction</h2>
<p>Complex mathematical reasoning remains a significant challenge for large language models (LLMs; Luo et al., 2024; Li et al., 2023; Meadows and Freitas, 2023). Techniques like Chain-of-Thought (CoT) prompting (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2022) have improved LLM performance by encouraging the generation of intermediate reasoning steps. However, CoT explanations are not always faithful to the actual reasoning process of the model (Bentham et al., 2024; Turpin et al., 2024; Yee et al., 2024); with final answers that may not logically follow from the reasoning chain, suggesting that LLMs can fabricate reasoning paths (Lyu et al., 2023).</p>
<p>Recent efforts to improve reasoning faithfulness have relied on integrating external solvers (Lyu et al., 2023; He-Yueya et al., 2023; Jiang et al., 2024a). However, these approaches introduce new</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An overview of our CoMAT framework. CoMAT divides complex reasoning tasks into two stages: Symbolic Conversion, where queries are translated into structured symbolic reasoning chains (Figure 2a), and Reasoning Execution, where step-by-step calculations are performed to derive the final answer (Figure 2b).
challenges: LLMs often generate erroneous or syntactically invalid code when translating natural language into formal statements, causing solvers to fail (Wen et al., 2024).</p>
<p>To address these issues, we propose Chain of Mathematically Annotated Thought (CoMAT), a novel approach that leverages symbolic reasoning entirely within LLMs. By eliminating external solvers, CoMAT avoids issues related to code generation failures, offering a more robust solution for a broad range of mathematical tasks. Notably, CoMAT performs well where there are difficulties with code-based methods, such as multilingual datasets and Olympiad-level problems. As shown in Appendix A, code-based methods often struggle with Olympiad-level mathematics, while CoMAT demonstrates greater adaptability. To our knowledge, CoMAT is the first method to apply symbolic reasoning across diverse mathematical reasoning tasks, including multilingual datasets, Olympiadlevel problems, and common benchmarks.</p>
<p>Empirically, CoMAT consistently outperforms</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />
(a) An overview of the Symbolic Conversion stage of CoMAT, which includes Indentification and Definition, Structural Logic Translation, Explicit Factual Representation, and Question Formalisation.
<img alt="img-2.jpeg" src="img-2.jpeg" />
(b) An overview of the Reasoning Execution stage of CoMAT, which performs 'step by step' reasoning based on the symbolic representation to provide the final answer.</p>
<p>Figure 2: An overview of CoMAT divided into two main stages: Symbolic Conversion and Reasoning Execution
existing baselines. When evaluated on datasets ranging from standard mathematical problems to complex Olympiad-level challenges in both English and Mandarin, CoMAT showed significant improvements. When applied to Gemini-1.5-Pro, we achieved an overall exact match boost of $3.54 \%$ on AQUA and an $8.18 \%$ increase in accuracy on the Olympiad Bench (English) compared to CoT. Similarly, using Qwen2-72B as a baseline, we observed a $2.55 \%$ improvement averaged across mathematical subjects of the MMLU-Redux benchmark. These results highlight CoMAT's effectiveness in enhancing accuracy across diverse and challenging tasks.</p>
<p>To summarise, our key contributions are:</p>
<ul>
<li>We introduce CoMAT, a novel approach that leverages symbolic reasoning entirely within LLMs, eliminating the need for external tools or verifiers. This ensures both accuracy and verifiability through a transparent and structured reasoning process.</li>
<li>Our symbolic prompts are standardised to tackle an array of mathematical reasoning tasks, achieving state-of-the-art performance across diverse
datasets of varying complexity and languages.</li>
<li>We perform a comprehensive ablation study demonstrating the generalisability of CoMAT across multiple LLMs, datasets, prompt designs, and languages.</li>
</ul>
<h2>2 CoMAT: Chain of Mathematically Annotated Thought</h2>
<p>Traditional CoT methods in mathematical reasoning rely heavily on natural language explanations, which can introduce ambiguity and inconsistencies ( Lu et al., 2024). These methods may lead models to fabricate reasoning paths or misinterpret variable relationships, compromising accuracy in complex tasks. CoMAT addresses these limitations by adopting a structured, symbolic approach that enforces mathematical consistency and reduces ambiguity (Tam et al., 2024). By using well-defined symbolic logic, CoMAT ensures that each reasoning step adheres to sound mathematical principles, enhancing the model's ability to solve problems accurately (Zhong et al., 2024).</p>
<p>CoMAT extends traditional CoT by incorporating symbolic transformations as the core compo-</p>
<p>nent of the reasoning process. While CoT typically follows a $(Q, R, A)$ structure - where $Q$ is the question, $R$ is the reasoning, and $A$ is the final answer - CoMAT introduces a more structured process: $(Q, S, R, A)$, where $S=\left(s_{1}, s_{2}, s_{3}, s_{4}\right)$ represents the four steps in the symbolic reasoning pipeline, designed to break down complex problems into formal, interpretable sequences of logical operations. This structured decomposition enhances transparency and allows for systematic verification of each step. Examples of manual annotation for verifiability can be found in Appendix B.</p>
<h3>2.1 CoMAT's Two-Stage Process</h3>
<p>CoMAT consists of two main stages: Symbolic Conversion and Reasoning Execution. Each stage is critical to ensuring the accuracy and faithfulness of the reasoning process.</p>
<p>Symbolic Conversion. In this stage, the LLM transforms a natural language query $Q$ into a symbolic representation, $S=\left(s_{1}, s_{2}, s_{3}, s_{4}\right)$. This involves identification and definition, structural logic translation, explicit factual representation, and question formalisation, all carried out by the LLM. This stage acts as the foundation for accurate reasoning by converting ambiguous natural language into well-structured mathematical logic.</p>
<p>Reasoning Execution. Once the problem is translated into its symbolic form $(S)$, the model applies logical reasoning to derive the solution. The logical reasoning is followed by stepwise reasoning, similar to Kojima et al. (2022) -i.e., prompting the model with the phrase "Let's think step-by-step". By grounding the reasoning in the symbolic structure, CoMAT ensures that each step aligns with mathematical logic, reducing the risk of errors or illogical steps. The final answer, $A$, is then generated based on this rigorous reasoning process.</p>
<h3>2.2 Case Study</h3>
<p>To demonstrate how CoMAT works in practice, consider the following math problem related to ticket sales, based on Figure 2:</p>
<p>Taylor Swift is planning a concert tour. The venue can hold 50,000 fans. VIP tickets cost $\$ 250$ each, and regular tickets cost $\$ 100$ each. If the total revenue from ticket sales is $\$ 6,500,000$ and all tickets are sold, how many VIP tickets were sold?</p>
<ol>
<li>
<p>Identification and Definition. CoMAT first identifies and defines the relevant variables and constants to ensure precision in the symbolic reasoning process. For instance:</p>
</li>
<li>
<p>Variables: $v$ (number of VIP tickets), $r$ (number of regular tickets)</p>
</li>
<li>
<p>Constants: $T$ (total capacity of 50,000), $P_{v}$ (price of VIP tickets, 250), $P_{r}$ (price of regular tickets, 100), $R$ (total revenue, $6,500,000$ )</p>
</li>
<li>
<p>Structural Logic Translation: Next, CoMAT extracts the key variables and translates the problem into formal rules that define their relationships, ensuring the reasoning process is grounded in well-defined constraints:</p>
</li>
<li>
<p>$v+r=T$</p>
</li>
<li>$P_{v} \cdot v+P_{r} \cdot r=R$</li>
<li>
<p>$v \geq 0, r \geq 0$ (non-negative constraints)</p>
</li>
<li>
<p>Explicit Factual Representation: CoMAT then integrates all relevant facts into the logical structure to avoid omitting key information:</p>
</li>
<li>
<p>$T=50,000$</p>
</li>
<li>$P_{v}=250$</li>
<li>$P_{r}=100$</li>
<li>
<p>$R=6,500,000$</p>
</li>
<li>
<p>Question Formalisation: CoMAT formalises the question into symbolic expression to ensure that the reasoning process remains objective and free of bias from answer options. However, the model may inherently choose to parse the question in natural language rather than a symbolic representation. In this case:</p>
</li>
</ol>
<p>In our example, we are tasked with finding $v$, the number of VIP tickets:
Find $v:(v+r=T) \wedge\left(P_{v} \cdot v+P_{r} \cdot r=R\right)$
5. Reasoning Execution: The problem is then solved step-by-step using the symbolic representation, as demonstrated:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Step 1: Express $r$ in terms of $v$ using $v+$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$r=T$</td>
</tr>
<tr>
<td style="text-align: left;">$r=T-v=50,000-v$</td>
</tr>
</tbody>
</table>
<p>Step 2: Substitute into the revenue equation $P_{v} \cdot v+P_{r} \cdot r=R$ :</p>
<p>$$
250 v+100(50,000-v)=6,500,000
$$</p>
<p>Step 3: Simplify:</p>
<p>$$
250 v+5,000,000-100 v=6,500,000
$$</p>
<p>Step 4: Solve for $v$ :</p>
<p>$$
v=\frac{1,500,000}{150}=10,000
$$</p>
<ol>
<li>Derivation of Final Answer: The final answer is then derived solely based on the logical reasoning applied. In this case:</li>
</ol>
<p>The number of VIP tickets sold is 10,000 .
7. (optional) Answer Matching: In multiplechoice QA tasks, CoMAT match the final answer to the most similar one from the provided options, without considering the order or labelling of the options. This final step ensures that the symbolic reasoning remains unbiased by the format of the answer choices.</p>
<p>This step-by-step symbolic reasoning ensures that each operation is transparent and verifiable, reducing the potential for errors often introduced in purely natural language reasoning. CoMAT's structured methodology facilitates easier error tracing, as shown in Figure 3. In cases where the final answer is incorrect, each step can be individually examined to identify mistakes - an advantage over traditional CoT, where explanations may not align with the actual reasoning steps, making the reasoning process less precise or harder to verify (Turpin et al., 2024; Li et al., 2024).</p>
<h2>3 Experimental Setup</h2>
<p>Datasets. We selected a total of seven datasets, including four in English: AQUA (Ling et al., 2017) and GSM8K (Cobbe et al., 2021), which were extracted from Math Word Problem (MWP) datasets,</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: An example question from the MMLU Redux Elementary Mathematics dataset, comparing CoT and CoMAT. CoT follows a generic "step-by-step" approach without further guidance. In contrast, CoMAT enhances interpretability and verifiability by clearly pinpointing the error, which in this case arises from Step 5. Traditional CoT, by comparison, lacks the ability to identify specific errors directly.</p>
<p>MMLU-Redux (Gema et al. 2024); covering various mathematics subjects, including abstract algebra, college-level mathematics, high school mathematics, and elementary mathematics), Olympiad Bench (English; OlympBench-EN; He et al. 2024), focused on text-based olympiad-mathematics problems, and another two Mandarin datasets: GaoKao (MCQ; Zhang et al. 2023), Olympiad Bench (Chinese; OlympBench-CN; He et al. 2024). These datasets include multiple-choice questions (AQUA, MMLU-Redux, GaoKao MCQ) and string-valued answers (GSM8K, Olympiad Bench). The inclusion of complex datasets like GaoKao and Olympiad allows us to assess model performance on academically challenging, non-standard, questions, such as those in Olympiad-level exams and specialised academic tasks.</p>
<p>Evaluation Metrics. For multiple-choice datasets, we used exact match metrics, requiring the predicted answer to fully match the correct one, not just the option code (e.g., $A, B$ ). This ensures evaluation based on complete responses, addressing clarity concerns in datasets like MMLU-Redux.</p>
<p>For string-valued answers, we also used exact match in GSM8K. On the Olympiad Bench, we used GPT-4o-mini as a benchmark to evaluate how well the model's answers aligned with the ground truth. Further details about GPT-4o-mini evaluation can be found in Appendix C.</p>
<p>Language Models. We conducted our experiments using GPT-4o (Achiam et al., 2023), Gemini (Team et al., 2023), and Qwen2 (Yang et al., 2024). This variety allows us to examine performance across different model architectures, sizes, and language proficiencies. For Faithful CoT, we used GPT-4, following the baselines from Lyu et al. (2023), as GPT-4o tends to produce more invalid outputs compared to GPT-4.</p>
<p>Baselines. We compared our method against two baselines using the same decoding strategies: greedy decoding, where the most probable next token is selected (temperature $=0.0$ ). The baselines include: (1) Standard Prompting, using a similar prompt to that in Holistic Evaluation of Language Models (Lee et al., 2024); (2) CoT prompting, famous "Let's think step by step" (Kojima et al., 2022). For common benchmarks, we included Faithful CoT for additional comparison. In cases where results were not available for specific tasks, we reproduced the baseline results to ensure consistency and fairness in comparison.</p>
<h2>4 Results</h2>
<p>Table 1 compares CoMAT, CoT (Kojima et al., 2022), Faithful CoT (Lyu et al., 2023) and baseline models across various benchmarks. CoMAT consistently outperforms traditional CoT and Faithful CoT in most datasets, particularly on tasks requiring advanced mathematical reasoning.</p>
<p>For open-source models, Qwen2-7b shows significant improvements with CoMAT. Under CoT, it often struggle to provide reasoning on OlympiadBench dataset. With CoMAT, its performance increases on English OlympiadBench (5.19\% $\rightarrow$ 20.92\%), and Chinese OlympiadBench (8.09\% $\rightarrow 13.24 \%$ ). Qwen2-72b also benefits from CoMAT, especially on OlympiadBench and GaoKao, with English OlympiadBench accuracy increasing (27.74\% $\rightarrow$ 32.17\%), highlighting CoMAT's ability to improve reasoning in models that initially underperform.</p>
<p>Among closed-source models, Gemini shows notable gains with CoMAT, with an $8.18 \%$ improve-
ment on English OlympiadBench. Similarly, GPT4o shows substantial gains on Mandarin datasets, with performance on GaoKao rising from $63.27 \%$ (CoT) to $71.43 \%$ (CoMAT).</p>
<p>However, there are some cases where CoMAT does not outperform CoT. For instance, Qwen272b and GPT-4o show declines on AQUA ( $79.13 \%$ $\rightarrow 72.44 \%$ and $84.25 \% \rightarrow 83.46 \%$, respectively). These decreases suggest that on simpler tasks where models already perform well, the added complexity of symbolic reasoning may not yield significant benefits.</p>
<p>When comparing CoMAT with Faithful CoT on MWP datasets, Faithful CoT shows a minor gain on GSM8K ( $93.70 \% \rightarrow 95.0 \%$ ). However, CoMAT demonstrates a significant $9.86 \%$ improvement on AQUA, highlighting its ability to outperform Faithful CoT without relying on external solvers. CoMAT also surpasses Faithful CoT on MMLU-Redux. We excluded Faithful CoT from comparisons on GaoKao and OlympiadBench due to its inability to execute most questions using external solvers, as outlined in Appendix A. CoMAT effectively mitigates these limitations, proving its capability to handle complex tasks without reliance on external solvers.</p>
<p>Generally, we observe the most significant gains on the challenging datasets (GaoKao and OlympiadBench), which require advanced reasoning. For instance, CoMAT improves average performance on the English OlympiadBench ( $23.68 \% \rightarrow$ $30.74 \%$ ), and GaoKao ( $56.13 \% \rightarrow 60.70 \%$ ). This suggests that CoMAT is particularly effective at enhancing reasoning capabilities for complex tasks. On simpler datasets like AQUA, where the base models already perform well, the average gains are smaller but still present, such as a $4.48 \%$ increase at MMLU-Redux. The average results for each model are illustrated in Figure 4.</p>
<p>Overall, integrating symbolic reasoning into the Chain-of-Thought process significantly enhances language models' ability to tackle complex mathematical reasoning. The improvements are more substantial on advanced tasks, aligning with CoMAT's design to handle intricate reasoning steps through structured symbolic representations.</p>
<h3>4.1 Step Contribution Analysis</h3>
<p>We conducted an ablation study to assess the impact of each step in the CoMAT prompt on model performance. We tested 16 variations by removing Steps 1, 2, 3, or 4 individually, as well as combi-</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>MWP</th>
<th></th>
<th>English</th>
<th></th>
<th></th>
<th>Chinese</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>AQUA</td>
<td>GSM8K</td>
<td>MMLU-Redux</td>
<td>OlympBench</td>
<td>Average (EN+MWP)</td>
<td>GaoKao</td>
<td>OlympBench</td>
<td>Average (CN)</td>
</tr>
<tr>
<td>Open Source Models</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Qwen2-7b</td>
<td>34.25%</td>
<td>19.29%</td>
<td>56.52%</td>
<td>5.19%</td>
<td>28.81%</td>
<td>48.98%</td>
<td>8.09%</td>
<td>28.54%</td>
</tr>
<tr>
<td>+ CoT</td>
<td>33.07%</td>
<td>81.78%</td>
<td>64.80%</td>
<td>3.86%</td>
<td>45.88%</td>
<td>40.82%</td>
<td>4.17%</td>
<td>22.50%</td>
</tr>
<tr>
<td>+ CoMAT (Ours)</td>
<td>42.13%</td>
<td>79.80%</td>
<td>79.80%</td>
<td>20.92%</td>
<td>55.66%</td>
<td>44.90%</td>
<td>13.24%</td>
<td>29.07%</td>
</tr>
<tr>
<td>Qwen2-72b</td>
<td>51.97%</td>
<td>37.43%</td>
<td>66.53%</td>
<td>10.39%</td>
<td>41.58%</td>
<td>53.06%</td>
<td>11.27%</td>
<td>32.17%</td>
</tr>
<tr>
<td>+ CoT</td>
<td>79.13%</td>
<td>82.76%</td>
<td>79.17%</td>
<td>27.74%</td>
<td>67.20%</td>
<td>55.10%</td>
<td>19.66%</td>
<td>37.38%</td>
</tr>
<tr>
<td>+ CoMAT (Ours)</td>
<td>72.44%</td>
<td>83.90%</td>
<td>81.72%</td>
<td>32.17%</td>
<td>67.56%</td>
<td>59.18%</td>
<td>18.87%</td>
<td>39.03%</td>
</tr>
<tr>
<td>Closed Source Models</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Gemini</td>
<td>48.03%</td>
<td>45.71%</td>
<td>68.00%</td>
<td>5.50%</td>
<td>41.92%</td>
<td>43.00%</td>
<td>14.95%</td>
<td>28.98%</td>
</tr>
<tr>
<td>+ CoT</td>
<td>75.20%</td>
<td>90.51%</td>
<td>79.55%</td>
<td>21.28%</td>
<td>66.64%</td>
<td>65.31%</td>
<td>13.27%</td>
<td>39.29%</td>
</tr>
<tr>
<td>+ CoMAT (Ours)</td>
<td>78.74%</td>
<td>90.43%</td>
<td>79.71%</td>
<td>29.46%</td>
<td>69.58%</td>
<td>67.30%</td>
<td>15.95%</td>
<td>41.63%</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>44.49%</td>
<td>56.72%</td>
<td>59.70%</td>
<td>9.94%</td>
<td>42.71%</td>
<td>36.73%</td>
<td>8.82%</td>
<td>22.78%</td>
</tr>
<tr>
<td>+ CoT</td>
<td>84.25%</td>
<td>94.46%</td>
<td>88.10%</td>
<td>41.84%</td>
<td>77.17%</td>
<td>63.27%</td>
<td>23.53%</td>
<td>43.40%</td>
</tr>
<tr>
<td>+ CoMAT (Ours)</td>
<td>83.46%</td>
<td>93.70%</td>
<td>88.30%</td>
<td>40.42%</td>
<td>76.48%</td>
<td>71.43%</td>
<td>26.47%</td>
<td>48.95%</td>
</tr>
<tr>
<td>Additional Benchmarks</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Faithful CoT</td>
<td>73.6%</td>
<td>95.0%</td>
<td>76.88%</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>Table 1: Performance comparison of models across multiple benchmarks, separated by language (EN, CN). While methods like Faithful CoT focus on MWP tasks, we compare CoMAT with Faithful CoT <em>Lyu et al. (2023)</em> common benchmarks. However, as Faithful CoT fails to solve most questions in <em>OlympiadBench</em> and <em>GaoKao</em>, generating invalid answers, we exclude it from those datasets and instead use CoT <em>Kojima et al. (2022)</em> as main comparison.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: Average performance across all datasets for each model.</p>
<p>nations (<em>i.e.,</em> removing steps 1 and 2 or removing steps 3 and 4).</p>
<p>Figure 5 shows the change in accuracy compared to the full CoMAT prompt. We found that each individual step plays a crucial role in maintaining the accuracy of CoMAT. Removing Step 1 leads to the most significant drop in accuracy (6.91%). This suggests that Step 1 is fundamental to CoMAT, potentially serving as the foundation upon which other steps build; its absence may disrupt critical initial processes or data preparation necessary for accurate performance. Omitting Step 2 or Step 3 results in smaller declines (1.38% and 2.63% respectively), indicating these steps are important but</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: Performance change $(\Delta)$ for each configuration with missing steps. Detailed results for all complete variants are provided in Appendix D.1.</p>
<p>less critical than Step 1. Interestingly, removing both Steps 1 and 2 results in a smaller performance drop (2.43%) than removing Step 1 alone, suggesting overlapping functionalities where the model compensates using remaining steps. Overall, the full CoMAT prompt achieves the highest average accuracy of 77.45%, reaffirming the importance of retaining all steps for optimal performance.</p>
<p>We further quantified each step's contribution using Shapley-value <em>Shapley (1953)</em> analysis (Figure 6). The analysis shows that all steps positively impact performance, with Steps 1 and 2 having the greatest influence, aligning with the expectation</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 6: (a) and (b) represent the Shapley values in bar and pie chart form, respectively.</p>
<p>that initial steps lay the foundation for subsequent reasoning. For a detailed breakdown of the Shapley analysis calculations, refer to Appendix E.</p>
<p>These results highlight that every step in CoMAT's pipeline is essential, not only for achieving high accuracy but also for enhancing interpretability and verifiability. Each step builds upon the previous ones, ensuring that the reasoning process remains transparent and coherent. This reinforces CoMAT's strength in balancing empirical performance with faithful, interpretable reasoning, making it a robust and reliable framework across diverse mathematical reasoning tasks.</p>
<h3>4.2 Multilingual Analysis</h3>
<p>To assess the impact of low-resource languages on CoMAT's performance, we evaluate the models on MGSM dataset (Shi et al., 2022), focusing on Swahili (SW), Bengali (BN), Thai (TH), and Telugu (TE). While CoMAT has shown strong performance in high-resource languages like English and Mandarin, our analysis reveals mixed results in low-resource language settings relative to CoT. This suggests that while CoMAT is effective, further optimisation may be needed to fully adapt CoMAT to low-resource languages.</p>
<p>As shown in Table 2, Gemini-1.5-Pro demonstrated notable improvements in low-resource contexts, with an average performance increase of 5.60% after applying CoMAT. Significant gains were observed in Bengali (BN) and Swahili (SW), where accuracy increased to 82.40% and 83.20%, respectively. These improvements indicate that CoMAT can enhance reasoning in lower-resource settings when applied to certain models.</p>
<p>However, the performance of GPT-4o model slightly declines, from 89.50% to 88.30%. This suggests that while CoMAT remains effective overall, certain models might struggle to generalise as effectively in low-resource contexts. In languages like Thai (TH) and Telugu (TE), the model's performance remained relatively stable but did not show the same level of improvement observed in higher-resource languages.</p>
<p>These mixed results indicate that while CoMAT shows promise for improving reasoning in low-resource languages, particularly with models like Gemini, further optimization may be required to fully optimise its performance across models and language contexts with limited training data.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>BN</th>
<th>SW</th>
<th>TE</th>
<th>TH</th>
<th>Average</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-4o Models</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>non-CoT</td>
<td>52.80</td>
<td>52.40</td>
<td>51.60</td>
<td>58.00</td>
<td>53.70%</td>
</tr>
<tr>
<td>+CoT</td>
<td>92.00</td>
<td>88.80</td>
<td>86.00</td>
<td>91.20</td>
<td>89.50%</td>
</tr>
<tr>
<td>+CoMAT</td>
<td>89.60</td>
<td>88.00</td>
<td>84.80</td>
<td>90.80</td>
<td>88.30%</td>
</tr>
<tr>
<td>Gemini Models</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>non-CoT</td>
<td>40.00</td>
<td>46.00</td>
<td>41.60</td>
<td>42.00</td>
<td>42.40%</td>
</tr>
<tr>
<td>+CoT</td>
<td>79.20</td>
<td>70.80</td>
<td>62.40</td>
<td>84.00</td>
<td>74.10%</td>
</tr>
<tr>
<td>+CoMAT</td>
<td>82.40</td>
<td>83.20</td>
<td>74.00</td>
<td>79.20</td>
<td>79.70%</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance comparison of GPT-4o and Gemini models across low-resource MGSM benchmarks (BN, SW, TE, TH).</p>
<p>Table 2 illustrates the effect of this option swapping. On the AQUA dataset, both CoT and CoMAT models experienced accuracy drops, highlighting their sensitivity to answer structure changes. However, CoMAT was more resilient, with only a 0.17% decrease and a low standard deviation of 0.91%. In contrast, GPT-4o with CoT saw a significant drop of 14.96% and a higher standard deviation of 3.50%, indicating greater inconsistency when options were shuffled. This highlights CoMAT's stability and robustness under altered conditions.</p>
<p>On the MMLU-Redux dataset, GPT-4o baseline showed a slight improvement, with a 1.34% standard deviation. CoT and CoMAT both had minor accuracy decreases, with CoMAT achieving 85.05% (standard deviation of 1.65%), slightly outperforming CoT at 84.43% (1.87% standard deviation). Detailed results can be referred to in Appendix D.2.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 7: Average accuracy for AQUA and MMLU before and after option changing.</p>
<p>pendix D.1.</p>
<p>These results demonstrate CoMAT's consistency when confronted with complexities of option swapping. Other methods exhibit greater variability and performance sensitivity.</p>
<h2>5 Related Work</h2>
<p><strong>Logical Reasoning.</strong> Logical reasoning tasks require models to handle complex logical structures (Cummins et al., 1991). Traditional methods include rule-based (Robinson, 1965) and neural-based approaches (Amayuelas et al., 2022; Gerasimova et al., 2023) for interpreting symbolic representations. Recent advancements integrate LLMs into symbolic reasoning, introducing frameworks like Logic-LM (Pan et al., 2023), SAT-LM (Ye et al., 2024) and LeanReasoner (Jiang et al., 2024a). These frameworks use LLMs to convert natural language into symbolic syntax processed by external reasoning tools, enhancing performance through self-consistency and non-linear reasoning (Wang et al., 2023; Zhang et al., 2022). However, they often rely on external tools, assuming LLMs cannot parse symbolic expressions as reliably as rule-based reasoners.</p>
<p><strong>Symbolic Chain-of-Thought Prompting.</strong> Symbolic CoT prompting (Lyu et al., 2023) combines natural language (NL) and symbolic language (SL) in the reasoning chain. NL decomposes complex queries into subproblems, and SL programs (e.g., Python, LEAN) handle each subproblem. A deterministic solver executes the SL to derive the final answer, ensuring faithfulness while NL aids interpretability. Recent efforts aim to reduce reliance on SL programs by leveraging LLMs (Xu et al., 2024), but these methods focus primarily on logical reasoning rather than complex mathematical tasks and still require verifiers to ensure accuracy.</p>
<p><strong>Mathematical Reasoning.</strong> Mathematical reasoning with LLMs has been explored widely (Lewkowycz et al., 2022; Luo et al., 2024; Ahn et al., 2024; Imani et al., 2023; Chen et al., 2024; Meadows and Freitas, 2022; Mirzadeh et al., 2024), with CoT methods yielding significant performance gains (Jiang et al., 2024c; Chu et al., 2023; Ranaldi and Freitas, 2024). Deep problem understanding (Zhong et al., 2024), structured formats (Tam et al., 2024), and building supervision models for reasoning (Lightman et al., 2023; Jiang et al., 2024b) also enhance accuracy. Other studies focus on premise selection and symbolic frameworks for systematic evaluation (Meadows et al., 2023; Ferreira and Freitas, 2020).</p>
<h2>6 Conclusion</h2>
<p>We propose CoMAT, a simple yet effective framework that decomposes complex mathematical reasoning into two stages — Symbolic Conversion and Reasoning Execution. CoMAT operates entirely within LLMs, eliminating reliance on external solvers and ensuring a transparent and accurate reasoning process. By avoiding external solvers, CoMAT mitigates issues related to code generation failures, providing a more robust solution for a broad range of mathematical tasks. Our analysis highlights four key steps in the CoMAT process and demonstrates its effectiveness across various datasets with different levels of complexity and linguistic diversity, including English, Mandarin, and low-resource languages. CoMAT consistently outperforms traditional CoT, achieving state-of-the-art performance and enhancing consistency when answer options are shuffled, demonstrating robustness and reliability. Despite its simplicity, CoMAT offers a scalable and effective solution for complex mathematical reasoning, providing greater faithfulness and verifiability across a wide range of tasks.</p>
<h2>Limitations</h2>
<p>While CoMAT demonstrates strong performance, there are several potential limitations. Firstly, our evaluation was limited to the current set of symbolic steps, where some steps have been previously used in other reasoning frameworks. For instance, a step similar to our step 1 is used by Lyu et al.</p>
<p>(2023) and Xu et al. (2024). Although our approach proved effective, further research is needed to assess CoMAT's performance using additional symbolic languages to ensure a more comprehensive evaluation. Secondly, while CoMAT enhances verifiability and faithfulness, it introduces a higher computational overhead compared to CoT due to the structured nature of its formalisations. This process involves generating additional symbolic representations, leading to a larger token count, which increases both computational costs and API usage. Consequently, scaling CoMAT could require more substantial computational resources than traditional CoT methods. Lastly, CoMAT has been focused within the mathematical reasoning. Further research could explore extending symbolic reasoning to other domains, beyond mathematical reasoning, to evaluate its effectiveness in broader tasks.</p>
<h2>References</h2>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.</p>
<p>Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. 2024. Large language models for mathematical reasoning: Progresses and challenges. arXiv preprint arXiv:2402.00157.</p>
<p>Alfonso Amayuelas, Shuai Zhang, Xi Susie Rao, and Ce Zhang. 2022. Neural methods for logical reasoning over knowledge graphs. In International Conference on Learning Representations.</p>
<p>Oliver Bentham, Nathan Stringham, and Ana Marasović. 2024. Chain-of-thought unfaithfulness as disguised accuracy. arXiv preprint arXiv:2402.14897.</p>
<p>Changyu Chen, Xiting Wang, Ting-En Lin, Ang Lv, Yuchuan Wu, Xin Gao, Ji-Rong Wen, Rui Yan, and Yongbin Li. 2024. Masked thought: Simply masking partial reasoning steps can improve mathematical reasoning learning of language models. arXiv preprint arXiv:2403.02178.</p>
<p>Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, and Ting Liu. 2023. A survey of chain of thought reasoning: Advances, frontiers and future. arXiv preprint arXiv:2309.15402.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Denise D Cummins, Todd Lubart, Olaf Alksnis, and Robert Rist. 1991. Conditional reasoning and causation. Memory \&amp; cognition, 19:274-282.</p>
<p>Deborah Ferreira and André Freitas. 2020. Premise selection in natural language mathematical texts. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 73657374 .</p>
<p>Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, et al. 2024. Are we done with MMLU? arXiv preprint arXiv:2406.04127.</p>
<p>Olga Gerasimova, Nikita Severin, and Ilya Makarov. 2023. Comparative analysis of logic reasoning and graph neural networks for ontology-mediated query answering with a covering axiom. IEEE Access.</p>
<p>Vipul Gupta, David Pantoja, Candace Ross, Adina Williams, and Megan Ung. 2024. Changing answer order can decrease mmlu accuracy. arXiv preprint arXiv:2406.19470.</p>
<p>Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. 2024. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008.</p>
<p>Joy He-Yueya, Gabriel Poesia, Rose E Wang, and Noah D Goodman. 2023. Solving math word problems by combining language models with symbolic solvers. arXiv preprint arXiv:2304.09102.</p>
<p>Shima Imani, Liang Du, and Harsh Shrivastava. 2023. Mathprompter: Mathematical reasoning using large language models. arXiv preprint arXiv:2303.05398.</p>
<p>Dongwei Jiang, Marcio Fonseca, and Shay B Cohen. 2024a. Leanreasoner: Boosting complex logical reasoning with lean. arXiv preprint arXiv:2403.13312.</p>
<p>Dongwei Jiang, Guoxuan Wang, Yining Lu, Andrew Wang, Jingyu Zhang, Chuyu Liu, Benjamin Van Durme, and Daniel Khashabi. 2024b. Rationalyst: Pre-training process-supervision for improving reasoning. Preprint, arXiv:2410.01044.</p>
<p>Zhuoxuan Jiang, Haoyuan Peng, Shanshan Feng, Fan Li, and Dongsheng Li. 2024c. LLMs can find mathematical reasoning mistakes by pedagogical chain-ofthought. arXiv preprint arXiv:2405.06705.</p>
<p>Alon Keinan, Ben Sandbank, Claus C Hilgetag, Isaac Meilijson, and Eytan Ruppin. 2004. Fair attribution of functional contribution in artificial and biological networks. Neural computation, 16(9):1887-1915.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in</p>
<p>neural information processing systems, 35:2219922213.</p>
<p>Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park, Agrim Gupta, Yunzhi Zhang, Deepak Narayanan, Hannah Teufel, Marco Bellagente, et al. 2024. Holistic evaluation of text-toimage models. Advances in Neural Information Processing Systems, 36.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857.</p>
<p>Jiachun Li, Pengfei Cao, Yubo Chen, Kang Liu, and Jun Zhao. 2024. Towards faithful chain-of-thought: Large language models are bridging reasoners. arXiv preprint arXiv:2405.18915.</p>
<p>Weixian Waylon Li, Yftah Ziser, Maximin Coavoux, and Shay B Cohen. 2023. BERT is not the count: Learning to match mathematical statements with proofs. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 3581-3593.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Let's verify step by step. arXiv preprint arXiv:2305.20050.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146.</p>
<p>Zhenyi Lu, Jie Tian, Wei Wei, Xiaoye Qu, Yu Cheng, Dangyang Chen, et al. 2024. Mitigating boundary ambiguity and inherent bias for text classification in the era of large language models. arXiv preprint arXiv:2406.07001.</p>
<p>Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, et al. 2024. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592.</p>
<p>Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. 2023. Faithful chain-ofthought reasoning. arXiv preprint arXiv:2301.13379.</p>
<p>Jordan Meadows and Andre Freitas. 2022. A survey in mathematical language processing. arXiv preprint arXiv:2205.15231.</p>
<p>Jordan Meadows and André Freitas. 2023. Introduction to Mathematical Language Processing: Informal Proofs, Word Problems, and Supporting Tasks. Transactions of the Association for Computational Linguistics, 11:1162-1184.</p>
<p>Jordan Meadows, Marco Valentino, Damien Teney, and Andre Freitas. 2023. A symbolic framework for systematic evaluation of mathematical reasoning with transformers.</p>
<p>Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. 2024. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. arXiv preprint arXiv:2410.05229.</p>
<p>Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang. 2023. Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. arXiv preprint arXiv:2305.12295.</p>
<p>Leonardo Ranaldi and Andre Freitas. 2024. Aligning large and small language models via chain-of-thought reasoning. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1812-1827.</p>
<p>John Alan Robinson. 1965. A machine-oriented logic based on the resolution principle. Journal of the ACM (JACM), 12(1):23-41.</p>
<p>Lloyd S Shapley. 1953. A value for n-person games. Contribution to the Theory of Games, 2.</p>
<p>Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, et al. 2022. Language models are multilingual chain-of-thought reasoners. arXiv preprint arXiv:2210.03057.</p>
<p>Zhi Rui Tam, Cheng-Kuang Wu, Yi-Lin Tsai, ChiehYen Lin, Hung-yi Lee, and Yun-Nung Chen. 2024. Let me speak freely? a study on the impact of format restrictions on performance of large language models. arXiv preprint arXiv:2408.02442.</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805.</p>
<p>Miles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. 2024. Language models don't always say what they think: unfaithful explanations in chain-ofthought prompting. Advances in Neural Information Processing Systems, 36.</p>
<p>Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Plan-and-solve prompting: Improving zero-shot chain-ofthought reasoning by large language models. arXiv preprint arXiv:2305.04091.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837.</p>
<p>Hao Wen, Yueheng Zhu, Chao Liu, Xiaoxue Ren, Weiwei Du, and Meng Yan. 2024. Fixing code generation errors for large language models. arXiv preprint arXiv:2409.00676.</p>
<p>Jundong Xu, Hao Fei, Liangming Pan, Qian Liu, MongLi Lee, and Wynne Hsu. 2024. Faithful logical reasoning via symbolic chain-of-thought. arXiv preprint arXiv:2405.18357.</p>
<p>An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671.</p>
<p>Xi Ye, Qiaochu Chen, Isil Dillig, and Greg Durrett. 2024. Satlm: Satisfiability-aided language models using declarative prompting. Advances in Neural Information Processing Systems, 36.</p>
<p>Evelyn Yee, Alice Li, Chenyu Tang, Yeon Ho Jung, Ramamohan Paturi, and Leon Bergen. 2024. Dissociation of faithful and unfaithful reasoning in LLMs. arXiv preprint arXiv:2405.15092.</p>
<p>Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. 2023. Evaluating the performance of large language models on gaokao benchmark. arXiv preprint arXiv:2305.12474.</p>
<p>Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493.</p>
<p>Qihuang Zhong, Kang Wang, Ziyang Xu, Juhua Liu, Liang Ding, Bo Du, and Dacheng Tao. 2024. Achieving&gt; 97\% on gsm8k: Deeply understanding the problems makes LLMs perfect reasoners. arXiv preprint arXiv:2404.14963.</p>
<h1>A Faithful CoT vs CoMAT</h1>
<h2>Verification of solutions - CoT (Faithful-CoT)</h2>
<h2>Question:</h2>
<p>Xenia and Sergey play the following game. Xenia thinks of a positive integer $N$ not exceeding 5000. Then she fixes 20 distinct positive integers $a_{1}, a_{2}, \ldots, a_{20}$ such that, for each $k=1,2, \ldots, 20$, the numbers $N$ and $a_{k}$ are congruent modulo $k$. By a move, Sergey tells Xenia a set $S$ of positive integers not exceeding 20, and she tells him back the set $\left{a_{k}: k \in S\right}$ without spelling out which number corresponds to which index. How many moves does Sergey need to determine for sure the number Xenia thought of?
Answer: 2</p>
<h2>Faithful-CoT:</h2>
<ol>
<li>What is the maximum number Xenia can think of?</li>
</ol>
<p>Xenia thinks of a positive integer $N$ not exceeding 5000.</p>
<ul>
<li>max_number $=5000$</li>
<li>How many distinct positive integers does Xenia fix?</li>
</ul>
<p>She fixes 20 distinct positive integers $a_{1}, a_{2}, \ldots, a_{20}$.</p>
<ul>
<li>distinct_integers $=20$</li>
<li>What is the condition for the numbers $N$ and $a_{k}$ ?</li>
</ul>
<p>The numbers $N$ and $a_{k}$ are congruent modulo $k$ for each $k=1,2, \ldots, 20$.</p>
<ul>
<li>
<p>Condition: N \% k == a_k \% k</p>
</li>
<li>
<p>What is a move in the game?</p>
</li>
<li>
<p>Sergey tells Xenia a set $S$ of positive integers not exceeding 20, and she tells him back the set $\left{a_{k}: k \in S\right}$ without spelling out which number corresponds to which index. Move:Sergey asks set S; Xenia gives $a_{k}: k \in S$</p>
</li>
<li>
<p>How many moves does Sergey need to determine for sure the number Xenia thought of?</p>
</li>
<li>
<p>This requires finding the least common multiple of the numbers from 1 to 20 and figuring out how many times this number fits into the maximum number Xenia can think of. Sergey can then determine the number Xenia thought of by asking for the set $\left{a_{k}: k \in S\right}$ for different sets $S$ until he has enough information to determine $N$.
lcm_1_to_20 = np.lcm.reduce (range (1, 21))
possible_numbers = max_number // lcm_1_to_20
moves_needed = math.ceil(math.log2(possible_numbers))</p>
</li>
<li>Final Answer: How many moves does Sergey need to determine for sure the number Xenia thought of?
answer $=$ moves_needed</li>
</ul>
<h1>Verification of solutions - CoT (CoMAT)</h1>
<h2>Question:</h2>
<p>Xenia and Sergey play the following game. Xenia thinks of a positive integer $N$ not exceeding 5000. Then she fixes 20 distinct positive integers $a_{1}, a_{2}, \ldots, a_{20}$ such that, for each $k=1,2, \ldots, 20$, the numbers $N$ and $a_{k}$ are congruent modulo $k$. By a move, Sergey tells Xenia a set $S$ of positive integers not exceeding 20, and she tells him back the set $\left{a_{k}: k \in S\right}$ without spelling out which number corresponds to which index. How many moves does Sergey need to determine for sure the number Xenia thought of?
Answer: 2</p>
<h2>CoMAT:</h2>
<ol>
<li>
<p>Define predicates, functions, and variables:</p>
</li>
<li>
<p>Let $N$ be the positive integer Xenia thinks of, where $1 \leq N \leq 5000$.</p>
</li>
<li>Let $a_{1}, a_{2}, \ldots, a_{20}$ be 20 distinct positive integers such that $N \equiv a_{k}(\bmod k)$ for each $k=1,2, \ldots, 20$.</li>
<li>Let $S$ be a set of positive integers not exceeding 20 that Sergey chooses.</li>
<li>
<p>Let $A(S)$ be the set of integers Xenia returns, which is $\left{a_{k}: k \in S\right}$.</p>
</li>
<li>
<p>Parse the problem into logical rules:</p>
</li>
<li>
<p>For each $k, N \equiv a_{k}(\bmod k)$.</p>
</li>
<li>Sergey can choose a set $S \subseteq{1,2, \ldots, 20}$.</li>
<li>
<p>Xenia returns the set $\left{a_{k}: k \in S\right}$ without indicating which $a_{k}$ corresponds to which $k$.</p>
</li>
<li>
<p>Write all the facts explicitly mentioned:</p>
</li>
<li>
<p>$N$ is a positive integer such that $1 \leq N \leq 5000$.</p>
</li>
<li>
<p>$a_{1}, a_{2}, \ldots, a_{20}$ are distinct and satisfy $N \equiv a_{k}(\bmod k)$.</p>
</li>
<li>
<p>Parse the question into symbolic form:</p>
</li>
<li>
<p>Determine the minimum number of moves Sergey needs to make to uniquely identify $N$.</p>
</li>
<li>
<p>Solve step by step:</p>
</li>
<li>
<p>The key to solving this problem is to understand that Sergey needs to gather enough information to determine $N$ uniquely.</p>
</li>
<li>Since $N \equiv a_{k}(\bmod k)$ for each $k$, the set of congruences forms a system of simultaneous congruences.</li>
<li>By the Chinese Remainder Theorem (CRT), if the moduli are pairwise coprime, the system of congruences has a unique solution modulo the product of the moduli.</li>
<li>The numbers 1 through 20 are not all pairwise coprime, but we can consider subsets of them that are.</li>
<li>
<p>Sergey needs to choose sets $S$ such that the product of the moduli in each set covers all possible residues for $N$ modulo 1 through 20.</p>
</li>
<li>
<p>Derived answer:</p>
</li>
<li>
<p>Sergey needs to make 2 moves to determine $N$ for sure. In the first move, he can choose a set that gives him enough information to narrow down the possibilities significantly, and in the second move, he can pinpoint the exact $N$.</p>
</li>
</ol>
<h2>Final Answer: 2</h2>
<h1>B Manual Annotation for CoMAT Verifiability</h1>
<p>In this section, we present the manual annotation process used to evaluate the verifiability of reasoning steps, carried out by a group of six annotators with diverse STEM backgrounds, including bachelor's and master's degrees in mathematics and engineering, and PhDs in computer science. We prompted the annotators to rate a given generated reasoning chain in terms of its verifiability. The annotators are encouraged not to focus solely on the correctness of the answer but rather the ease of identifying the errors. The verifiability of each solution was rated on a scale from 1 (lowest) to 10 (highest), and we report the average score.</p>
<p>To account for the inherent complexities of manual verification, we randomly sampled five examples from widely-used datasets, such as MMLU-Redux and GSM8K. These examples were selected to represent a range of outcomes: cases where both CoMAT and CoT models provided correct and incorrect answers, where both failed, and where one model answered correctly while the other did not. This sampling strategy allowed us to systematically assess the verifiability of each model's reasoning under various conditions, providing a robust evaluation framework.</p>
<h2>Example 1</h2>
<h2>Verification of solutions - CoT</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Question</th>
<th style="text-align: left;">John drives for 3 hours at a speed of 60 mph and then turns around because he realizes he forgot <br> something very important at home. He tries to get home in 4 hours but spends the first 2 hours in <br> standstill traffic. He spends the next half-hour driving at a speed of 30 mph , before being able to <br> drive the remaining time of the 4 hours going at 80 mph . How far is he from home at the end of <br> those 4 hours?</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h2>CoT</h2>
<ol>
<li>Calculate the distance John traveled during the initial 3 hours at $\mathbf{6 0} \mathbf{m p h}$ :</li>
</ol>
<p>$$
\text { Distance }=60 \mathrm{mph} \times 3 \text { hours }=180 \text { miles }
$$</p>
<ol>
<li>Calculate the distance John traveled during the half-hour at 30 mph after the standstill traffic:</li>
</ol>
<p>$$
\text { Distance }=30 \mathrm{mph} \times 0.5 \text { hours }=15 \text { miles }
$$</p>
<ol>
<li>Calculate the time left for John to travel at 80 mph after the standstill traffic and the half-hour at 30 mph :</li>
</ol>
<p>$$
\text { Time left }=4 \text { hours }-2 \text { hours }-0.5 \text { hours }=1.5 \text { hours }
$$</p>
<ol>
<li>Calculate the distance John traveled during the remaining time at 80 mph :</li>
</ol>
<p>$$
\text { Distance }=80 \mathrm{mph} \times 1.5 \text { hours }=120 \text { miles }
$$</p>
<ol>
<li>Add up all the distances calculated in steps 1, 2, and 4 to find the total distance John traveled:</li>
</ol>
<p>$$
\text { Total distance traveled }=180 \text { miles }+15 \text { miles }+120 \text { miles }=315 \text { miles }
$$</p>
<ol>
<li>Since John turned around to go back home, subtract the total distance traveled from the distance he was from home initially (which is equal to the distance he traveled in the first 3 hours):</li>
</ol>
<p>$$
\text { Distance from home }=180 \text { miles }-315 \text { miles }=-135 \text { miles }
$$</p>
<p>Since distance cannot be negative, this indicates that John has passed his home and is 135 miles beyond it.</p>
<p>Final Answer: 135 miles</p>
<table>
<thead>
<tr>
<th style="text-align: left;">CoT Results</th>
<th style="text-align: left;">False</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CoT Verification</td>
<td style="text-align: left;">6.33</td>
</tr>
</tbody>
</table>
<h1>Verification of solutions - CoMAT</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">John drives for 3 hours at a speed of 60 mph and then turns around because he realizes he forgot something very important at home. He tries to get home in 4 hours but spends the first 2 hours in standstill traffic. He spends the next half-hour driving at a speed of 30 mph , before being able to drive the remaining time of the 4 hours going at 80 mph . How far is he from home at the end of those 4 hours?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CoMAT</td>
<td style="text-align: center;">Define predicates and functions: <br> - $d(t, s)$ : Function that calculates distance traveled over time $t$ at speed $s$ <br> - $t_{1}$ : Constant representing the first part of the return trip ( 2 hours in standstill traffic) <br> - $t_{2}$ : Constant representing the second part of the return trip ( 0.5 hours at 30 mph ) <br> - $t_{3}$ : Constant representing the third part of the return trip (remaining time after $t_{1}$ and $t_{2}$ at 80 mph ) <br> - $s_{1}$ : Constant representing the initial speed ( 60 mph ) <br> - $s_{2}$ : Constant representing the speed during the second part of the return trip ( 30 mph ) <br> - $s_{3}$ : Constant representing the speed during the third part of the return trip ( 80 mph ) <br> Parse problem into logical rules: <br> - $\forall t, s: d(t, s)=t \times s$ <br> - $t_{1}=2$ <br> - $t_{2}=0.5$ <br> - $t_{3}=4-t_{1}-t_{2}$ <br> - $s_{1}=60$ <br> - $s_{2}=30$ <br> - $s_{3}=80$ <br> Solve step by step: <br> 1. Calculate the distance traveled in the initial 3 hours: <br> $d_{1}=d(3,60)=3 \times 60=180$ miles <br> 2. Calculate the distance traveled during the second part of the return trip: <br> $d_{2}=d(0.5,30)=0.5 \times 30=15$ miles <br> 3. Calculate the remaining time for the third part of the return trip: <br> $t_{3}=4-2-0.5=1.5$ hours <br> 4. Calculate the distance traveled during the third part of the return trip: <br> $d_{3}=d(1.5,80)=1.5 \times 80=120$ miles <br> 5. Calculate the total distance traveled during the return trip: <br> $d_{\text {total return }}=d_{2}+d_{3}=15+120=135$ miles <br> 6. Calculate the distance from home at the end of the 4 hours: <br> Distance from home $=d_{1}-d_{\text {total return }}=180-135=45$ miles</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Final Answer: 45 miles</td>
</tr>
<tr>
<td style="text-align: center;">CoMAT Results</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">CoMAT Verification</td>
<td style="text-align: center;">9.08</td>
</tr>
</tbody>
</table>
<h1>Example 2</h1>
<h2>Verification of solutions - CoT</h2>
<p>Question
Eliza's rate per hour for the first 40 hours she works each week is $\$ 10$. She also receives an overtime pay of 1.2 times her regular hourly rate. If Eliza worked for 45 hours this week, how much are her earnings for this week?</p>
<h2>CoT</h2>
<ol>
<li>
<p>Calculate Eliza's regular earnings for the first 40 hours:</p>
</li>
<li>
<p>Regular hourly rate $=\$ 10$</p>
</li>
<li>Regular hours worked $=40$</li>
<li>
<p>Regular earnings $=40$ hours $\times \$ 10 /$ hour $=\$ 400$</p>
</li>
<li>
<p>Calculate Eliza's overtime earnings for the additional 5 hours:</p>
</li>
<li>
<p>Overtime hourly rate $=1.2 \times \$ 10=\$ 12$</p>
</li>
<li>Overtime hours worked $=45$ hours - 40 hours $=5$ hours</li>
<li>
<p>Overtime earnings $=5$ hours $\times \$ 12 /$ hour $=\$ 60$</p>
</li>
<li>
<p>Calculate Eliza's total earnings for the week:</p>
</li>
<li>
<p>Total earnings $=$ Regular earnings + Overtime earnings $=\$ 400+\$ 60=\$ 460$</p>
</li>
</ol>
<p>Final Answer: $\$ 460$
CoT Results True
CoT Verification 7.17</p>
<h1>Verification of solutions - CoMAT</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Question</th>
<th style="text-align: left;">Eliza's rate per hour for the first 40 hours she works each week is $\$ 10$. She also receives an <br> overtime pay of 1.2 times her regular hourly rate. If Eliza worked for 45 hours this week, how <br> much are her earnings for this week?</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h2>CoMAT Define predicates and functions:</h2>
<ul>
<li>Earnings $(x)$ : Function that calculates total earnings for $x$ hours worked</li>
<li>RegularPayRate: Constant representing Eliza's regular hourly rate (\$10)</li>
<li>OvertimePayRate: Constant representing Eliza's overtime hourly rate (1.2 $\times$ RegularPayRate)</li>
<li>HoursWorked: Variable representing the total hours worked (45)</li>
<li>RegularHours: Constant representing the number of regular hours (40)</li>
<li>OvertimeHours: Variable representing the number of overtime hours (HoursWorked Regular Hours)</li>
</ul>
<h2>Parse problem into logical rules:</h2>
<ul>
<li>$\forall x \quad: \quad$ Earnings $(x)=($ RegularPayRate $\times$ RegularHours $)+$ (OvertimePayRate $\times$ OvertimeHours)</li>
<li>OvertimePayRate $=1.2 \times$ RegularPayRate</li>
</ul>
<h2>Facts:</h2>
<ul>
<li>RegularPayRate $=\$ 10$</li>
<li>HoursWorked $=45$</li>
<li>RegularHours $=40$</li>
</ul>
<h2>Parse the question:</h2>
<ul>
<li>Find Earnings(HoursWorked)</li>
</ul>
<h2>Solve step by step:</h2>
<ol>
<li>Calculate OvertimePayRate:</li>
</ol>
<p>$$
\text { OvertimePayRate }=1.2 \times \text { RegularPayRate }=1.2 \times \$ 10=\$ 12
$$</p>
<ol>
<li>Calculate OvertimeHours:</li>
</ol>
<p>$$
\text { OvertimeHours }=\text { HoursWorked }- \text { RegularHours }=45-40=5
$$</p>
<ol>
<li>Calculate Earnings(HoursWorked):</li>
</ol>
<p>$$
\begin{aligned}
\text { Earnings }(\text { HoursWorked }) &amp; =(\text { RegularPayRate } \times \text { RegularHours }) \
&amp; +(\text { OvertimePayRate } \times \text { OvertimeHours }) \
&amp; =(\$ 10 \times 40)+(\$ 12 \times 5) \
&amp; =\$ 400+\$ 60 \
&amp; =\$ 460
\end{aligned}
$$</p>
<p>Final Answer: $\$ 460$
CoMAT Results True
CoMAT Verification 8.92</p>
<h1>Example 3</h1>
<h2>Verification of solutions - CoT</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Question</th>
<th style="text-align: left;">Ram uses a lot of pens. He discovered that he can save money by mixing the ink from five empty <br> pens to make one full pen. If he buys 25 pens and then uses them to make new pens when the ink <br> runs low, how many total pens does he get to have?</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h2>CoT</h2>
<ol>
<li>Initial Pens: Ram starts with 25 pens.</li>
<li>Create New Pens: For every 5 empty pens, Ram can create 1 new pen.</li>
<li>
<p>Calculate additional pens:</p>
</li>
<li>
<p>After using the initial 25 pens, Ram will have 25 empty pens.</p>
</li>
<li>
<p>With 25 empty pens, he can create $\frac{25}{5}=5$ new pens.</p>
</li>
<li>
<p>Total Pens: Add the new pens to the initial pens:</p>
</li>
<li>
<p>Total pens $=$ Initial pens + New pens $=25+5=30$</p>
</li>
</ol>
<p>Final Answer: 30
CoT Results False
CoT Verification 4.5</p>
<h1>Verification of solutions - CoMAT</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Question</th>
<th style="text-align: left;">Ram uses a lot of pens. He discovered that he can save money by mixing the ink from five <br> empty pens to make one full pen. If he buys 25 pens and then uses them to make new pens <br> when the ink runs low, how many total pens does he get to have?</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h2>CoMAT Define predicates and functions:</h2>
<ul>
<li>Let $P$ be the number of pens Ram initially buys.</li>
<li>Let $E$ be the number of empty pens needed to make one full pen.</li>
<li>Let $T$ be the total number of pens Ram gets to have.</li>
</ul>
<h2>Parse problem into logical rules:</h2>
<ul>
<li>$P=25$ (initial pens bought)</li>
<li>$E=5$ (empty pens needed to make one full pen)</li>
</ul>
<h2>Facts:</h2>
<ul>
<li>Ram starts with 25 pens.</li>
<li>For every 5 empty pens, he can make 1 new pen.</li>
</ul>
<h2>Solve step by step:</h2>
<ol>
<li>Start with 25 pens $(P=25)$.</li>
<li>Use 25 pens, resulting in 25 empty pens.</li>
<li>From 25 empty pens, Ram can make $\frac{25}{5}=5$ new pens.</li>
<li>Use the 5 new pens, resulting in 5 empty pens.</li>
<li>From 5 empty pens, Ram can make $\frac{5}{5}=1$ new pen.</li>
<li>Use this 1 new pen, resulting in 1 empty pen.</li>
<li>From 1 empty pen, Ram cannot make any more new pens.</li>
</ol>
<p>Total Pens: $25+5+1=31$
Final Answer: 31
CoMAT Results True
CoMAT Verification 8.75</p>
<h1>Example 4</h1>
<h2>Verification of solutions - CoT</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Solve $\frac{18.5 \mathrm{~d} d}{m \mathrm{pd}}=\frac{3.60 \mathrm{~d} d}{7.5 \mathrm{pd}}$. Round to the nearest hundredth if necessary.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">To solve the equation $\frac{18.5}{m}=\frac{3.60}{7.5}$, follow these steps:</td>
</tr>
</tbody>
</table>
<ol>
<li><strong>Calculate the right side of the equation:</strong></li>
</ol>
<h1>Verification of solutions - CoMAT</h1>
<p>Question
Solve $\frac{18.5 \mathrm{~dol}}{\text { os gal }}=\frac{3.60 \mathrm{~dol}}{7.5 \mathrm{gal}}$. Round to the nearest hundredth if necessary.
CoMAT</p>
<ol>
<li>
<p>Define predicates and functions:</p>
</li>
<li>
<p>Let $x$ be the unknown value in dollars per gallon (dol/gal).</p>
</li>
<li>Let $a=18.5 \mathrm{dol} / \mathrm{m}$ gal.</li>
<li>Let $b=3.60 \mathrm{dol}$.</li>
<li>
<p>Let $c=7.5$ gal.</p>
</li>
<li>
<p>Parse problem into logical rules:</p>
</li>
<li>
<p>We are given the equation:</p>
</li>
</ol>
<p>$$
\frac{a}{1}=\frac{b}{c}
$$</p>
<ul>
<li>This implies:</li>
</ul>
<p>$$
a=\frac{b}{c}
$$</p>
<ol>
<li>
<p>Facts:</p>
</li>
<li>
<p>$a=18.5$</p>
</li>
<li>$b=3.60$</li>
<li>
<p>$c=7.5$</p>
</li>
<li>
<p>Parse the question:</p>
</li>
<li>
<p>Solve for $a$ in the equation $a=\frac{b}{c}$.</p>
</li>
<li>
<p>Solve step by step:
(a) Substitute the known values into the equation:</p>
</li>
</ol>
<p>$$
18.5=\frac{3.60}{7.5}
$$</p>
<p>(b) Calculate the right-hand side:</p>
<p>$$
\frac{3.60}{7.5}=0.48
$$</p>
<p>(c) Therefore, the equation becomes:</p>
<p>$$
18.5=0.48
$$</p>
<p>(d) This indicates a misunderstanding in the setup. Let's re-evaluate:</p>
<ul>
<li>The problem is asking to solve for the equivalent rate in dollars per gallon.</li>
<li>
<p>We need to find the equivalent of $18.5 \mathrm{dol} / \mathrm{m}$ gal in dol/gal.</p>
</li>
<li>
<p>Derived answer:</p>
</li>
<li>
<p>Convert $18.5 \mathrm{dol} / \mathrm{m}$ gal to dol/gal: $\frac{18.5 \mathrm{~dol}}{1000 \mathrm{gal}}=0.0185 \mathrm{dol} / \mathrm{gal}$</p>
</li>
<li>Compare this with the given rate $\frac{3.60}{7.5}: \frac{3.60}{7.5}=0.48 \mathrm{dol} / \mathrm{gal}$</li>
<li>
<p>The problem seems to be asking for a comparison or conversion, but the setup was incorrect.</p>
</li>
<li>
<p>Match to provided options:</p>
</li>
<li>
<p>The derived calculation does not directly match any of the options, indicating a potential misunderstanding of the problem's intent.</p>
</li>
<li>Assume the task was to find a related value or conversion.</li>
</ul>
<p>Final Answer: A (assuming a misinterpretation or conversion context).
CoMAT Results False
CoMAT Verification 6.17</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Code is available at https://github.com/ joshuaongg21/CoMAT&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
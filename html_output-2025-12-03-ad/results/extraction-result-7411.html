<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7411 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7411</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7411</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-274131315</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.10541v1.pdf" target="_blank">Does Prompt Formatting Have Any Impact on LLM Performance?</a></p>
                <p><strong>Paper Abstract:</strong> In the realm of Large Language Models (LLMs), prompt optimization is crucial for model performance. Although previous research has explored aspects like rephrasing prompt contexts, using various prompting techniques (like in-context learning and chain-of-thought), and ordering few-shot examples, our understanding of LLM sensitivity to prompt templates remains limited. Therefore, this paper examines the impact of different prompt templates on LLM performance. We formatted the same contexts into various human-readable templates, including plain text, Markdown, JSON, and YAML, and evaluated their impact across tasks like natural language reasoning, code generation, and translation using OpenAI's GPT models. Experiments show that GPT-3.5-turbo's performance varies by up to 40\% in a code translation task depending on the prompt template, while larger models like GPT-4 are more robust to these variations. Our analysis highlights the need to reconsider the use of fixed prompt templates, as different formats can significantly affect model performance.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7411.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7411.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MMLU_FormatEffect_GPT-35-16k</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MMLU multiple-choice prompt format effect on gpt-35-turbo-16k-0613</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper reports that presenting MMLU multiple-choice questions in different human-readable templates changes accuracy substantially for gpt-35-turbo-16k-0613; specifically JSON prompts produced much higher accuracy than Markdown in their example (a 42% increase reported).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-35-turbo-16k-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3.5 series conversational transformer (proprietary instruction‑tuned model), accessed via Azure OpenAI.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU (Massive Multitask Language Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice general knowledge and reasoning questions across 57 subjects; here evaluated as a classification/multiple-choice task.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Multiple-choice question presented in different prompt templates (JSON vs Markdown)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Few-shot setting using dev examples as in-paper; the same content injected into different templates (persona, instructions, examples, output-format instructions, user ask); comparison between JSON and Markdown templates; temperature set to 0 in some MMLU experiments to remove sampling noise.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>JSON achieved 42% higher accuracy compared to Markdown (absolute increase reported: +42% accuracy over Markdown)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Markdown (baseline format in the example)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+42% absolute (JSON vs Markdown)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Few-shot examples (dev set), temperature = 0 for MMLU consistency experiments; identical semantic content across templates, only format differs.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>The paper reports matched-pair t-tests across templates with p-values mostly < 0.01 generally; specific p-value for this comparison not listed but reported effect is statistically assessed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Prompt Formatting Have Any Impact on LLM Performance?', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7411.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7411.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FIND_FormatEffect_GPT-35</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FIND (NL->Code) prompt format effect on gpt-35-turbo variants</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>On the FIND benchmark (reverse-engineering Python functions from IO examples), both gpt-35-turbo-0613 and gpt-35-turbo-16k-0613 showed dramatic gains when the prompt template was changed from Markdown to plain text — reported as ~200% improvement in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-35-turbo-0613, gpt-35-turbo-16k-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3.5 series instruction‑tuned transformer models with different context window sizes (4k vs 16k), accessed via Azure OpenAI.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>FIND (Function Interpretation and Description)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>NL-to-code generation task where the model is given input/output examples of a Python function and must generate the underlying function; evaluated on 'strings' category with 5 provided examples per function.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot function description presented in different prompt templates (Markdown vs plain text)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>5 ICL examples provided; templates differ only in structure/syntax (Markdown vs plain text); evaluation uses the string indicator metric (number of test cases passed).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>string indicator (number of test cases passed)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Plain text prompt yielded ~200% improvement relative to Markdown for the mentioned gpt-3.5 variants (reported as a dramatic 200% improvement)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Markdown (baseline format)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>~+200% relative (plain text vs Markdown)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Few-shot with 5 examples; sample of 500 functions in the 'strings' category; settings follow Schwettmann et al. (2023) as cited.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Paper reports matched-pair t-tests across templates with p-values mostly < 0.01; this benchmark's large effect is reported as statistically significant in aggregate analyses (specific p-value for this pair not quoted).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Prompt Formatting Have Any Impact on LLM Performance?', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7411.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7411.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HumanEval_FormatEffect_GPT-4-32k</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HumanEval (NL->Code) prompt format effect on gpt-4-32k-0613</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>On HumanEval (Python programming problems), the gpt-4-32k-0613 model's pass@1 metric was reported to increase dramatically (over 300% boost) when prompts were changed from JSON to plain text; authors observed JSON often caused the model to produce chain-of-thought text without completing code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4-32k-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4 series instruction‑tuned transformer variant with a 32k context window; proprietary model accessible via Azure OpenAI.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HumanEval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Code generation benchmark of Python programming problems with unit tests; metric is pass@1 (whether a single generated sample passes unit tests).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Problem description and few-shot/contextual information presented in different prompt templates (JSON vs plain text)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Prompts include persona, instructions, examples, output-format instructions, and user ask in different syntactic templates (JSON vs plain text); the JSON template in many runs triggered chain-of-thought/plain text generation without producing code continuation for this model/config.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>pass@1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported >300% improvement (relative) when switching from JSON to plain text for gpt-4-32k-0613 on HumanEval</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>JSON (baseline format)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>>+300% relative (plain text vs JSON)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>HumanEval full set (164 problems), decoding defaults as used via Azure OpenAI; authors analyzed model outputs and hypothesized JSON caused chain-of-thought output interfering with code generation for this model/version.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Aggregate matched-pair t-tests reported in paper indicate format-driven differences are statistically significant (p-values mostly < 0.01); specific p-value for this comparison not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Prompt Formatting Have Any Impact on LLM Performance?', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7411.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7411.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Consistency_MMLU_FMT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Consistency of outputs between prompt templates on MMLU for GPT-3.5 vs GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper measures pairwise answer consistency (fraction of identical outputs) between prompt templates; GPT-3.5-turbo models showed low consistency (<0.5) with only ~16% identical responses between Markdown and JSON, whereas GPT-4 models exceeded 0.5 consistency on MMLU.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-35-turbo series, gpt-4 series</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3.5 and GPT-4 family instruction‑tuned transformer models accessed via Azure OpenAI; GPT-4 is a larger, more capable model with unknown exact architecture details.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU (consistency experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice questions from MMLU; consistency computed as fraction of test samples where model outputs are identical across two prompt templates.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Multiple-choice presented under different templates (Markdown, JSON, plain text, YAML)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Temperature set to 0 for MMLU consistency experiments to remove sampling variance; consistency C(Pa,Pb) computed over the test set; template pairs compared (e.g., Markdown vs JSON).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>consistency (proportion of identical answers between two templates)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GPT-3.5: ~0.16 consistency between Markdown and JSON (16% identical); GPT-4: consistency >0.5 for comparable template pairs</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>GPT-4 shows substantially higher consistency (absolute increase >0.34 over GPT-3.5 for the Markdown–JSON pair as reported)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Temperature = 0; MMLU test set (14,079 questions); same prompt content across templates, only formatting differs.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Prompt Formatting Have Any Impact on LLM Performance?', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7411.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7411.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transferability_IoU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Top-template transferability between GPT models measured by Intersection-over-Union (IoU)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper computes IoU between sets of top-performing prompt templates across model pairs and finds low transferability across different GPT model families (IoU often < 0.2), while same-subseries model pairs (e.g., gpt-35-turbo-0613 vs gpt-35-turbo-16k-0613) show high IoU (>0.7). It also reports differing format preferences: GPT-3.5 tends to prefer JSON while GPT-4 favors Markdown.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-35-turbo series, gpt-4 series</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT series models; paper compares top templates identified per model via matched-pair statistical testing.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Cross-model top-template comparison (multiple benchmarks aggregated)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate which prompt templates produce top (statistically indistinguishable) performance per model, then compute IoU between models' top-template sets to quantify transferability.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt templates (plain text, Markdown, YAML, JSON) across multiple tasks/benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / cross-model transferability</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Top-performing templates determined by matched-pair t-tests; IoU = |P_m1 ∩ P_m2| / |P_m1 ∪ P_m2|; thresholds like 0.5/0.7 used to judge overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Intersection-over-Union (IoU) of top template sets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Cross-family IoU often < 0.2; within-subseries IoU > 0.7 reported (example: gpt-35-turbo-0613 vs gpt-35-turbo-16k-0613).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Top templates selected via matched-pair t-tests comparing formats per model and benchmark; analysis across multiple benchmarks (MMLU, FIND, HumanEval, CODEXGLUE, HumanEval-X, NER Finance).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Top templates determined using matched-pair t-tests; the paper reports p-values mostly < 0.01 for format-driven differences, which underlie the IoU selection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Prompt Formatting Have Any Impact on LLM Performance?', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7411.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7411.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CMD_Robustness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Coefficient of Mean Deviation (CMD) measuring robustness to template variation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper introduces/uses CMD (coefficient of mean deviation) across prompt templates as a robustness measure: lower CMD means the model's scalar performance metrics vary less with prompt format. GPT-4-1106-preview showed CMD consistently < 0.036, GPT-4-32k-0613 CMD ≤ 0.043, while GPT-3.5 series CMD ranged from 0.035 to 0.176, indicating greater sensitivity in GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4-1106-preview, gpt-4-32k-0613, gpt-35-turbo series</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4 and GPT-3.5 variants; CMD computed over scalar performance metrics across prompt templates and benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Aggregate robustness across multiple benchmarks (CMD)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Quantify variation in scalar performance metrics (accuracy, pass@1, BLEU, etc.) across prompt templates using CMD as a summary statistic per model and benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Comparison across prompt templates (plain text, Markdown, YAML, JSON) aggregated over tasks</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / robustness metric</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>CMD is computed for each model across benchmarks; lower values indicate more robust performance across templates; per-benchmark CMDs reported (figure/table references in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Coefficient of Mean Deviation (CMD) of scalar task metrics across formats</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GPT-4-1106-preview: CMD consistently < 0.036; GPT-4-32k-0613: CMD up to ~0.043; GPT-3.5 series: CMD range 0.035–0.176.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Higher CMD in GPT-3.5 indicates up to ~5x greater dispersion (0.176 vs ~0.036) relative to the most robust GPT-4 variant reported.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>CMD computed across formats and benchmarks reported in Figure 6 and discussed in text; analyses include inspection of failure modes (e.g., JSON causing chain-of-thought outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Prompt Formatting Have Any Impact on LLM Performance?', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7411.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7411.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Abstract_CodeTranslation_40pc</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reported up-to-40% GPT-3.5-turbo performance variation on code translation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The abstract states that gpt-3.5-turbo's performance varies by up to 40% in a code translation task depending on prompt template, while larger models like GPT-4 are more robust to such variations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-35-turbo (general statement)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3.5 series instruction‑tuned transformer models; statement summarizes aggregated experimental observations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code translation (CODEXGLUE / HumanEval-X family)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate code between programming languages (e.g., Java ↔ C# or Java→Python) evaluated using BLEU or similar code-to-code comparators.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Code translation prompts presented in different templates (JSON, YAML, Markdown, plain text)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Same code context placed into different human-readable templates; evaluation uses BLEU for CODEXGLUE and HumanEval-X; formats compared across models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU (for CODEXGLUE/HumanEval-X) and other task-specific metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Abstract reports up to 40% variation for gpt-3.5-turbo in a code translation task depending on template</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Up to ±40% relative change attributed to prompt template (as reported in abstract)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Experiments across CODEXGLUE and HumanEval-X; exact configuration (which prompt pair produced 40%) is summarized in abstract and main text; full tables in appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Prompt Formatting Have Any Impact on LLM Performance?', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting <em>(Rating: 2)</em></li>
                <li>Mind your format: Towards consistent evaluation of in-context learning improvements <em>(Rating: 2)</em></li>
                <li>You don't need a personality test to know these models are unreliable: Assessing the reliability of large language models on psychometric instruments <em>(Rating: 1)</em></li>
                <li>Table meets llm: Can large language models understand structured table data? a benchmark and empirical study <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7411",
    "paper_id": "paper-274131315",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "MMLU_FormatEffect_GPT-35-16k",
            "name_full": "MMLU multiple-choice prompt format effect on gpt-35-turbo-16k-0613",
            "brief_description": "The paper reports that presenting MMLU multiple-choice questions in different human-readable templates changes accuracy substantially for gpt-35-turbo-16k-0613; specifically JSON prompts produced much higher accuracy than Markdown in their example (a 42% increase reported).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-35-turbo-16k-0613",
            "model_description": "OpenAI GPT-3.5 series conversational transformer (proprietary instruction‑tuned model), accessed via Azure OpenAI.",
            "model_size": null,
            "task_name": "MMLU (Massive Multitask Language Understanding)",
            "task_description": "Multiple-choice general knowledge and reasoning questions across 57 subjects; here evaluated as a classification/multiple-choice task.",
            "problem_format": "Multiple-choice question presented in different prompt templates (JSON vs Markdown)",
            "format_category": "prompt style",
            "format_details": "Few-shot setting using dev examples as in-paper; the same content injected into different templates (persona, instructions, examples, output-format instructions, user ask); comparison between JSON and Markdown templates; temperature set to 0 in some MMLU experiments to remove sampling noise.",
            "performance_metric": "accuracy",
            "performance_value": "JSON achieved 42% higher accuracy compared to Markdown (absolute increase reported: +42% accuracy over Markdown)",
            "baseline_performance": "Markdown (baseline format in the example)",
            "performance_change": "+42% absolute (JSON vs Markdown)",
            "experimental_setting": "Few-shot examples (dev set), temperature = 0 for MMLU consistency experiments; identical semantic content across templates, only format differs.",
            "statistical_significance": "The paper reports matched-pair t-tests across templates with p-values mostly &lt; 0.01 generally; specific p-value for this comparison not listed but reported effect is statistically assessed in the paper.",
            "uuid": "e7411.0",
            "source_info": {
                "paper_title": "Does Prompt Formatting Have Any Impact on LLM Performance?",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "FIND_FormatEffect_GPT-35",
            "name_full": "FIND (NL-&gt;Code) prompt format effect on gpt-35-turbo variants",
            "brief_description": "On the FIND benchmark (reverse-engineering Python functions from IO examples), both gpt-35-turbo-0613 and gpt-35-turbo-16k-0613 showed dramatic gains when the prompt template was changed from Markdown to plain text — reported as ~200% improvement in the paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-35-turbo-0613, gpt-35-turbo-16k-0613",
            "model_description": "OpenAI GPT-3.5 series instruction‑tuned transformer models with different context window sizes (4k vs 16k), accessed via Azure OpenAI.",
            "model_size": null,
            "task_name": "FIND (Function Interpretation and Description)",
            "task_description": "NL-to-code generation task where the model is given input/output examples of a Python function and must generate the underlying function; evaluated on 'strings' category with 5 provided examples per function.",
            "problem_format": "Few-shot function description presented in different prompt templates (Markdown vs plain text)",
            "format_category": "prompt style",
            "format_details": "5 ICL examples provided; templates differ only in structure/syntax (Markdown vs plain text); evaluation uses the string indicator metric (number of test cases passed).",
            "performance_metric": "string indicator (number of test cases passed)",
            "performance_value": "Plain text prompt yielded ~200% improvement relative to Markdown for the mentioned gpt-3.5 variants (reported as a dramatic 200% improvement)",
            "baseline_performance": "Markdown (baseline format)",
            "performance_change": "~+200% relative (plain text vs Markdown)",
            "experimental_setting": "Few-shot with 5 examples; sample of 500 functions in the 'strings' category; settings follow Schwettmann et al. (2023) as cited.",
            "statistical_significance": "Paper reports matched-pair t-tests across templates with p-values mostly &lt; 0.01; this benchmark's large effect is reported as statistically significant in aggregate analyses (specific p-value for this pair not quoted).",
            "uuid": "e7411.1",
            "source_info": {
                "paper_title": "Does Prompt Formatting Have Any Impact on LLM Performance?",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "HumanEval_FormatEffect_GPT-4-32k",
            "name_full": "HumanEval (NL-&gt;Code) prompt format effect on gpt-4-32k-0613",
            "brief_description": "On HumanEval (Python programming problems), the gpt-4-32k-0613 model's pass@1 metric was reported to increase dramatically (over 300% boost) when prompts were changed from JSON to plain text; authors observed JSON often caused the model to produce chain-of-thought text without completing code generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-4-32k-0613",
            "model_description": "OpenAI GPT-4 series instruction‑tuned transformer variant with a 32k context window; proprietary model accessible via Azure OpenAI.",
            "model_size": null,
            "task_name": "HumanEval",
            "task_description": "Code generation benchmark of Python programming problems with unit tests; metric is pass@1 (whether a single generated sample passes unit tests).",
            "problem_format": "Problem description and few-shot/contextual information presented in different prompt templates (JSON vs plain text)",
            "format_category": "prompt style",
            "format_details": "Prompts include persona, instructions, examples, output-format instructions, and user ask in different syntactic templates (JSON vs plain text); the JSON template in many runs triggered chain-of-thought/plain text generation without producing code continuation for this model/config.",
            "performance_metric": "pass@1",
            "performance_value": "Reported &gt;300% improvement (relative) when switching from JSON to plain text for gpt-4-32k-0613 on HumanEval",
            "baseline_performance": "JSON (baseline format)",
            "performance_change": "&gt;+300% relative (plain text vs JSON)",
            "experimental_setting": "HumanEval full set (164 problems), decoding defaults as used via Azure OpenAI; authors analyzed model outputs and hypothesized JSON caused chain-of-thought output interfering with code generation for this model/version.",
            "statistical_significance": "Aggregate matched-pair t-tests reported in paper indicate format-driven differences are statistically significant (p-values mostly &lt; 0.01); specific p-value for this comparison not provided.",
            "uuid": "e7411.2",
            "source_info": {
                "paper_title": "Does Prompt Formatting Have Any Impact on LLM Performance?",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Consistency_MMLU_FMT",
            "name_full": "Consistency of outputs between prompt templates on MMLU for GPT-3.5 vs GPT-4",
            "brief_description": "The paper measures pairwise answer consistency (fraction of identical outputs) between prompt templates; GPT-3.5-turbo models showed low consistency (&lt;0.5) with only ~16% identical responses between Markdown and JSON, whereas GPT-4 models exceeded 0.5 consistency on MMLU.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-35-turbo series, gpt-4 series",
            "model_description": "OpenAI GPT-3.5 and GPT-4 family instruction‑tuned transformer models accessed via Azure OpenAI; GPT-4 is a larger, more capable model with unknown exact architecture details.",
            "model_size": null,
            "task_name": "MMLU (consistency experiments)",
            "task_description": "Multiple-choice questions from MMLU; consistency computed as fraction of test samples where model outputs are identical across two prompt templates.",
            "problem_format": "Multiple-choice presented under different templates (Markdown, JSON, plain text, YAML)",
            "format_category": "prompt style",
            "format_details": "Temperature set to 0 for MMLU consistency experiments to remove sampling variance; consistency C(Pa,Pb) computed over the test set; template pairs compared (e.g., Markdown vs JSON).",
            "performance_metric": "consistency (proportion of identical answers between two templates)",
            "performance_value": "GPT-3.5: ~0.16 consistency between Markdown and JSON (16% identical); GPT-4: consistency &gt;0.5 for comparable template pairs",
            "baseline_performance": null,
            "performance_change": "GPT-4 shows substantially higher consistency (absolute increase &gt;0.34 over GPT-3.5 for the Markdown–JSON pair as reported)",
            "experimental_setting": "Temperature = 0; MMLU test set (14,079 questions); same prompt content across templates, only formatting differs.",
            "statistical_significance": null,
            "uuid": "e7411.3",
            "source_info": {
                "paper_title": "Does Prompt Formatting Have Any Impact on LLM Performance?",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Transferability_IoU",
            "name_full": "Top-template transferability between GPT models measured by Intersection-over-Union (IoU)",
            "brief_description": "The paper computes IoU between sets of top-performing prompt templates across model pairs and finds low transferability across different GPT model families (IoU often &lt; 0.2), while same-subseries model pairs (e.g., gpt-35-turbo-0613 vs gpt-35-turbo-16k-0613) show high IoU (&gt;0.7). It also reports differing format preferences: GPT-3.5 tends to prefer JSON while GPT-4 favors Markdown.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-35-turbo series, gpt-4 series",
            "model_description": "OpenAI GPT series models; paper compares top templates identified per model via matched-pair statistical testing.",
            "model_size": null,
            "task_name": "Cross-model top-template comparison (multiple benchmarks aggregated)",
            "task_description": "Evaluate which prompt templates produce top (statistically indistinguishable) performance per model, then compute IoU between models' top-template sets to quantify transferability.",
            "problem_format": "Prompt templates (plain text, Markdown, YAML, JSON) across multiple tasks/benchmarks",
            "format_category": "prompt style / cross-model transferability",
            "format_details": "Top-performing templates determined by matched-pair t-tests; IoU = |P_m1 ∩ P_m2| / |P_m1 ∪ P_m2|; thresholds like 0.5/0.7 used to judge overlap.",
            "performance_metric": "Intersection-over-Union (IoU) of top template sets",
            "performance_value": "Cross-family IoU often &lt; 0.2; within-subseries IoU &gt; 0.7 reported (example: gpt-35-turbo-0613 vs gpt-35-turbo-16k-0613).",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Top templates selected via matched-pair t-tests comparing formats per model and benchmark; analysis across multiple benchmarks (MMLU, FIND, HumanEval, CODEXGLUE, HumanEval-X, NER Finance).",
            "statistical_significance": "Top templates determined using matched-pair t-tests; the paper reports p-values mostly &lt; 0.01 for format-driven differences, which underlie the IoU selection.",
            "uuid": "e7411.4",
            "source_info": {
                "paper_title": "Does Prompt Formatting Have Any Impact on LLM Performance?",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "CMD_Robustness",
            "name_full": "Coefficient of Mean Deviation (CMD) measuring robustness to template variation",
            "brief_description": "The paper introduces/uses CMD (coefficient of mean deviation) across prompt templates as a robustness measure: lower CMD means the model's scalar performance metrics vary less with prompt format. GPT-4-1106-preview showed CMD consistently &lt; 0.036, GPT-4-32k-0613 CMD ≤ 0.043, while GPT-3.5 series CMD ranged from 0.035 to 0.176, indicating greater sensitivity in GPT-3.5.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-4-1106-preview, gpt-4-32k-0613, gpt-35-turbo series",
            "model_description": "OpenAI GPT-4 and GPT-3.5 variants; CMD computed over scalar performance metrics across prompt templates and benchmarks.",
            "model_size": null,
            "task_name": "Aggregate robustness across multiple benchmarks (CMD)",
            "task_description": "Quantify variation in scalar performance metrics (accuracy, pass@1, BLEU, etc.) across prompt templates using CMD as a summary statistic per model and benchmark.",
            "problem_format": "Comparison across prompt templates (plain text, Markdown, YAML, JSON) aggregated over tasks",
            "format_category": "prompt style / robustness metric",
            "format_details": "CMD is computed for each model across benchmarks; lower values indicate more robust performance across templates; per-benchmark CMDs reported (figure/table references in paper).",
            "performance_metric": "Coefficient of Mean Deviation (CMD) of scalar task metrics across formats",
            "performance_value": "GPT-4-1106-preview: CMD consistently &lt; 0.036; GPT-4-32k-0613: CMD up to ~0.043; GPT-3.5 series: CMD range 0.035–0.176.",
            "baseline_performance": null,
            "performance_change": "Higher CMD in GPT-3.5 indicates up to ~5x greater dispersion (0.176 vs ~0.036) relative to the most robust GPT-4 variant reported.",
            "experimental_setting": "CMD computed across formats and benchmarks reported in Figure 6 and discussed in text; analyses include inspection of failure modes (e.g., JSON causing chain-of-thought outputs).",
            "statistical_significance": null,
            "uuid": "e7411.5",
            "source_info": {
                "paper_title": "Does Prompt Formatting Have Any Impact on LLM Performance?",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Abstract_CodeTranslation_40pc",
            "name_full": "Reported up-to-40% GPT-3.5-turbo performance variation on code translation",
            "brief_description": "The abstract states that gpt-3.5-turbo's performance varies by up to 40% in a code translation task depending on prompt template, while larger models like GPT-4 are more robust to such variations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-35-turbo (general statement)",
            "model_description": "OpenAI GPT-3.5 series instruction‑tuned transformer models; statement summarizes aggregated experimental observations.",
            "model_size": null,
            "task_name": "Code translation (CODEXGLUE / HumanEval-X family)",
            "task_description": "Translate code between programming languages (e.g., Java ↔ C# or Java→Python) evaluated using BLEU or similar code-to-code comparators.",
            "problem_format": "Code translation prompts presented in different templates (JSON, YAML, Markdown, plain text)",
            "format_category": "prompt style",
            "format_details": "Same code context placed into different human-readable templates; evaluation uses BLEU for CODEXGLUE and HumanEval-X; formats compared across models.",
            "performance_metric": "BLEU (for CODEXGLUE/HumanEval-X) and other task-specific metrics",
            "performance_value": "Abstract reports up to 40% variation for gpt-3.5-turbo in a code translation task depending on template",
            "baseline_performance": null,
            "performance_change": "Up to ±40% relative change attributed to prompt template (as reported in abstract)",
            "experimental_setting": "Experiments across CODEXGLUE and HumanEval-X; exact configuration (which prompt pair produced 40%) is summarized in abstract and main text; full tables in appendix.",
            "statistical_significance": null,
            "uuid": "e7411.6",
            "source_info": {
                "paper_title": "Does Prompt Formatting Have Any Impact on LLM Performance?",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting",
            "rating": 2,
            "sanitized_title": "quantifying_language_models_sensitivity_to_spurious_features_in_prompt_design_or_how_i_learned_to_start_worrying_about_prompt_formatting"
        },
        {
            "paper_title": "Mind your format: Towards consistent evaluation of in-context learning improvements",
            "rating": 2,
            "sanitized_title": "mind_your_format_towards_consistent_evaluation_of_incontext_learning_improvements"
        },
        {
            "paper_title": "You don't need a personality test to know these models are unreliable: Assessing the reliability of large language models on psychometric instruments",
            "rating": 1,
            "sanitized_title": "you_dont_need_a_personality_test_to_know_these_models_are_unreliable_assessing_the_reliability_of_large_language_models_on_psychometric_instruments"
        },
        {
            "paper_title": "Table meets llm: Can large language models understand structured table data? a benchmark and empirical study",
            "rating": 1,
            "sanitized_title": "table_meets_llm_can_large_language_models_understand_structured_table_data_a_benchmark_and_empirical_study"
        }
    ],
    "cost": 0.012923999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Does Prompt Formatting Have Any Impact on LLM Performance?
15 Nov 2024</p>
<p>Jia He hejia@microsoft.com 
Mukund Rungta rungtamukund@microsoft.com 
David Koleczek dkoleczek@microsoft.com 
Arshdeep Sekhon 
Franklin X Wang fxwang@mit.edu 
Sadid Hasan 
Microsoft 
Josh Achiam 
Steven Adler 
Sandhini Agarwal 
Lama Ahmad 
Ilge Akkaya 
Florencia Leoni Aleman 
Diogo Almeida 
Janko Altenschmidt 
Sam Altman 
Tom B Brown 
Benjamin Mann 
Nick Ryder 
Melanie Subbiah 
Jared Kaplan 
Prafulla Dhariwal 
Arvind Neelakantan 
Pranav Shyam 
Girish Sastry 
Amanda Askell 
Ariel Herbert-Voss 
Gretchen Krueger 
Tom Henighan 
Rewon Child 
Aditya Ramesh 
Daniel M Ziegler 
Jeffrey Wu 
Clemens Winter 
Christopher Hesse 
Mark Chen 
Eric Sigler 
Mateusz Litwin 
Scott Gray 
Benjamin Chess 
Jack Clark 
Christopher Berner 
Sam Mc- Candlish 
Alec Radford 
Ilya Sutskever 
Dario 2020 Amodei 
Nicholas Carlini 
Daniel Paleka 
Krishnamurthy Dj 
Thomas Steinke 
Jonathan Hayase 
A Feder Cooper 
Katherine Lee 
Matthew Jagielski 
Milad Nasr 
Arthur Conmy 
Jerry Tworek 
Heewoo Jun 
Qiming Yuan 
Henrique Ponde 
Oliveira Pinto 
Jared Ka- Plan 
Harri Edwards 
Yuri Burda 
Nicholas Joseph 
Greg Brockman 
Alex Ray 
Raul Puri 
Michael Petrov 
Heidy Khlaaf 
Girish Sas- Try 
Pamela Mishkin 
Brooke Chan 
Mikhail Pavlov 
Alethea Power 
Lukasz Kaiser 
Mohammad Bavarian 
Philippe Tillet 
Felipe Petroski Such 
Dave Cum- Mings 
Matthias Plappert 
Fotios Chantzis 
Eliza- Beth Barnes 
William Hebgen Guss 
Alex Nichol 
Alex Paino 
Nikolas Tezak 
Jie Tang 
Igor Babuschkin 
Suchir Balaji 
Shantanu Jain 
William Saunders 
Andrew N Carr 
Jan Leike 
Vedant Misra 
Evan Morikawa 
Matthew Knight 
Miles Brundage 
Mira Murati 
Katie Mayer 
Peter Welinder 
Bob Mcgrew 
Sam Mccandlish 
Wojciech 2021 Zaremba 
Evaluating 
Aakanksha Chowdhery 
Sharan Narang 
Jacob Devlin 
Maarten Bosma 
Gaurav Mishra 
Adam Roberts 
HyungPaul Barham 
Won Chung 
Charles Sutton 
Sebastian Gehrmann 
Parker Schuh 
Kensen Shi 
Sasha Tsvyashchenko 
Joshua Maynez 
Abhishek Rao 
Parker Barnes 
Yi Tay 
Noam Shazeer 
Vin- Odkumar Prabhakaran 
Emily Reif 
Nan Du 
Ben Hutchinson 
Reiner Pope 
James Bradbury 
Jacob Austin 
Michael Isard 
Guy Gur-Ari 
Pengcheng Yin 
Toju Duke 
Anselm Levskaya 
Sanjay Ghemawat 
Sunipa Dev 
Henryk Michalewski 
Xavier Garcia 
Kevin Robinson 
Liam Fedus 
Denny Zhou 
Daphne Ippolito 
David Luan 
Hyeontaek Lim 
Barret Zoph 
Alexander Spiridonov 
Ryan Sepassi 
David Dohan 
Shivani Agrawal 
Mark Omernick 
An- Drew M Dai 
Thanumalayan Sankaranarayana 
Marie Pellat 
Aitor Lewkowycz 
Erica Moreira 
Oleksandr Polozov 
Zongwei Zhou 
Xuezhi Wang 
Brennan Saeta 
Mark Diaz 
Orhan Firat 
Michele Catasta 
Jason Wei 
Kathy Meier-Hellstern 
Douglas Eck 
Jeff Dean 
Slav Petrov 
Does Prompt Formatting Have Any Impact on LLM Performance?
15 Nov 2024CDCC3C46F3A364588D91B5E29C7DAB1AarXiv:2411.10541v1[cs.CL]
In the realm of Large Language Models (LLMs), prompt optimization is crucial for model performance.Although previous research has explored aspects like rephrasing prompt contexts, using various prompting techniques (like in-context learning and chain-ofthought), and ordering few-shot examples, our understanding of LLM sensitivity to prompt templates remains limited.Therefore, this paper examines the impact of different prompt templates on LLM performance.We formatted the same contexts into various human-readable templates, including plain text, Markdown, JSON, and YAML, and evaluated their impact across tasks like natural language reasoning, code generation, and translation using Ope-nAI's GPT models.Experiments show that GPT-3.5-turbo'sperformance varies by up to 40% in a code translation task depending on the prompt template, while larger models like GPT-4 are more robust to these variations.Our analysis highlights the need to reconsider the use of fixed prompt templates, as different formats can significantly affect model performance.</p>
<p>Introduction</p>
<p>The emergence of LLMs marks a significant advancement in AI, revolutionizing natural language processing, understanding, and generation ((Brown et al., 2020;Ouyang et al., 2022;Chowdhery et al., 2022;Achiam et al., 2023)).Prompt engineering has become crucial, focusing on crafting inputs that guide LLMs to produce desired outputs, leveraging a nuanced understanding of how these models interpret and respond to prompts ( (Sahoo et al., 2024)).Effective prompt design generally includes clear instructions, Retrieval-Augmented Generation (RAG) or other prompting approaches for enhancing in-context learning (ICL), and appropriate formatting.</p>
<ul>
<li>Equal Contribution Figure 1: An example to demonstrate how prompt formatting impacts GPT-35-turbo-16k-0613 model's performance based on our experiments on multiple choice questions related to international law from the MMLU benchmark ( (Hendrycks et al., 2020)).Texts inside "&lt;&gt;" are replaced by actual contexts.Accuracy goes up by 42% for JSON compared to Markdown.</li>
</ul>
<p>Often overlooked, prompt format can significantly impact model performance, contrary to the assumption that it remains stable across different templates.There exist limited research and anecdotal evidence ((Aghajanyan, June 2023;Sclar et al., 2023;Voronov et al., 2024)), which suggest that prompt format choices may lead to substantial performance variations, raising concerns about current evaluation standards that ignore this factor.For example, one study showed that LLMs are sensitive to minor fine-grained prompt modifications, such as separators or capitalization changes ( (Sclar et al., 2023)).Also, existing evaluation approaches typically use fixed templates, potentially leading to misleading conclusions ( (Voronov et al., 2024)).</p>
<p>Inspired by these findings, our study investigates whether broader changes in prompt format affect model efficacy.We evaluate the impact of prompt templates on OpenAI's four GPT models across six benchmarks, using plain text, Markdown, YAML, and JSON formats, as illustrated in Figure 1.This comprehensive approach contrasts with prior research that primarily examined minor template alterations.Our research focuses on the GPT model series for two main reasons: the lack of comparative analyses of behavioral patterns across different GPT model iterations, especially the latest GPT-4turbo, and the need to identify effective interaction methods and optimal input formats for these models, which do not disclose their training methodologies or data.</p>
<p>Our study is designed to investigate the following key questions:</p>
<p>• Sensitivity: To what extent does the performance of GPT models vary with different prompt formats?</p>
<p>• Consistency: Are GPT models capable of producing uniform responses to identical queries when presented with varying prompt structures?</p>
<p>• Transferability: Is there an optimal prompt format that is universally effective across diverse GPT models, thereby ensuring peak performance?</p>
<p>In addition to our primary questions, we explore the correlation between prompt format efficacy and task-specific competencies, as well as the impact of model size on performance.OpenAI's GPT models including GPT-35-turbo and GPT-4 (Achiam et al., 2023) show unpredictable sensitivity to prompt format changes, with significant performance discrepancies across all models and benchmarks.Notably, there is no universally optimal format, even within the same generational lineage.However, GPT-4turbo demonstrates greater resilience to prompt format changes compared to its predecessors and contemporaries.In summary, our key contributions are as follows:</p>
<p>• This study is the first to compare the impact of different prompt formats on GPT models' performance across various tasks, examining plain text, Markdown, YAML, and JSON.</p>
<p>• Our research provides an extensive analysis of prompt formatting effects on GPT models across a wide range of tasks, including multiple-choice questions, code generation, and translation.</p>
<p>• We present an evaluation of the GPT model iterations via Azure OpenAI, revealing that GPT-4-turbo is less susceptible to prompt structure variations compared to earlier models.</p>
<p>2 Experimental Setup</p>
<p>Datasets</p>
<p>Our experiments span various tasks and datasets, categorized into three main groups:</p>
<p>• Natural Language to Natural Language (NL2NL): Includes Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2020) and NER Finance from OpenAI Evals (OpenAI, 2023).</p>
<p>• Natural Language to Code (NL2Code): Includes HumanEval (Chen et al., 2021) and FIND (Schwettmann et al., 2023).</p>
<p>• Code to Code (Code2Code): Includes CODEXGLUE (Lu et al., 2021) and HumanEval-X (Zheng et al., 2023).</p>
<p>We initially assess model performance using task-specific scalar scoring functions, followed by metrics from Sections 3 to 5 to address our research questions.Detailed dataset descriptions and metrics are in Appendix B.</p>
<p>Prompt Design</p>
<p>We use various input formats: plain text, markdown, YAML, and JSON.Prompts include five components: persona, task instructions, examples, output format instructions, and user ask.We ensure the content of each placeholder stays the same across different prompt formats.The only differences are in structure and syntax.To avoid confounding variables, we design the prompts so that the context and meaning remain consistent, regardless of the format.Examples are in Appendix C.</p>
<p>Models</p>
<p>Experiments were conducted on OpenAI's GPT-3.5 and GPT-4 models via Azure (Microsoft, 2024).For GPT-3.5, we used "gpt-35-turbo-0613" and "gpt-35-turbo-16k-0613" to compare context window sizes (4k vs. 16k).For GPT-4, we used "gpt-4-32k-0613" and "gpt-4-1106-preview" to test the newer, faster variant with a 128k context window.</p>
<p>Sensitivity</p>
<p>Metrics Definition</p>
<p>Sensitivity.To evaluate how much the choice of prompt template impacts a model's performance on a task T, we look at a variety of templates {p 1 , p 2 , . . ., p n } and measure their performance 3.2 Does prompt format impact the performance of language models and how significant is the performance variation when switching prompt formats?</p>
<p>We begin by analyzing if model performance is sensitive to any changes in the prompt format at all.To assess this, we conducted a one-sided matched pair t-test, comparing the best and worst performing formats for each model across various benchmarks.The resulting p-values, which are shown in Table 1, are mostly below 0.01.This suggests that the differences in model performance due to format changes are statistically significant.</p>
<p>Figure 4 visualizes how the models fare across all benchmarks, highlighting a considerable range in performance.For instance, in the FIND dataset, both GPT-35-turbo-0613 and GPT-35-turbo-16k-0613 show a dramatic 200% improvement when prompts are switched from Markdown to plain text.Similarly, for the HumanEval benchmark, the GPT-4 model with a 32k-0613 configuration exhibits an impressive performance boost of over 300% when the prompt format is changed from JSON to plain text.This suggests, LLM performance may not be robust to the choice of prompt format.</p>
<p>Consistency</p>
<p>Metrics Definition</p>
<p>Following the sensitivity measurement, we quantify the extent of answer variation due to prompt changes using the consistency metric from (Shu et al., 2023).This metric calculates the proportion of test samples that yield identical responses for two prompt templates.The consistency C(P a , P b ) for templates P a and P b is defined as:
C(P a , P b ) = 1 N N i=1 1 (A Pa (x i ) = A P b (x i ))
where N is the test set size and A represents the model's answer.A higher score indicates greater answer consistency between prompts.</p>
<p>4.2 Are larger models more consistent in generated outputs between templates?</p>
<p>Our study assessed the consistency of model outputs using the MMLU and FIND datasets, as shown in Figures 2 and 8.For MMLU, we set the temperature to zero to eliminate response variability.The GPT-3.5-turbo series displayed low consistency, with scores below 0.5, and only 16% identical responses between Markdown and JSON formats.In contrast, GPT-4's consistency scores surpassed 0.5, indicating better reliability across different prompts.For the FIND dataset, following the settings from (Schwettmann et al., 2023), GPT-4 again outperformed the GPT-3.5-turboseries in consistency.These findings suggest that larger models like GPT-4 are more consistent, but there is still a need for model improvements to achieve reliable performance across various formats.In summary, the consistency of model responses varies with size, with larger models like GPT-4 providing more uniform outputs across different prompts.</p>
<p>Transferability</p>
<p>Metrics Definition</p>
<p>Intersection-over-Union. To assess the transferability of prompt templates between models, we calculate the Intersection-over-Union (IoU) for the sets of top-performing templates between model pairs.Top-performing templates are those with statistically indistinguishable performance, determined by a matched pairs t-test.The IoU is defined as:
IoU = |P m1 ∩ P m2 | |P m1 ∪ P m2 |
where P m1 and P m2 represent the sets of top templates for models m1 and m2, respectively.An IoU threshold of 0.5 is common, but a higher threshold like 0.7 indicates greater overlap.</p>
<p>5.2 Do models from same family exhibit similar trend across prompt formats?</p>
<p>Our research into Large Language Models (LLMs), GPT-based models in particular, reveals that prompt formatting preferences vary by model.As demonstrated in Figure 5, GPT-3.5-turboprefers JSON, whereas GPT-4 favors Markdown.When examining prompt transferability using Intersection- over-Union (IoU) metrics (Figure 3 and Appendix B), we found low compatibility between different model series, with IoU often below 0.2.However, models from the same sub-series, like GPT-35-turbo-16k-0613 and GPT-35-turbo-0613, show high IoU over 0.7.These insights highlight that even with common architectures and training goals, GPT-models react differently to identical prompts.Optimal performance requires model-specific prompt engineering, as no single format works universally across various GPT models, even within the same family.This underscores the necessity for tailored prompt engineering due to the nontransferability of prompt formats across different GPT models.</p>
<p>Conclusion</p>
<p>Our study reveals that the way prompts are formatted significantly impacts GPT-based models' performance, with no single format excelling universally.This finding questions current evaluation methods that often ignore prompt structure, potentially misjudging a model's true abilities.We advocate for diverse prompt formats in future LLM testing to accurately gauge and enhance their performance.</p>
<p>Regarding explainability, we observe that model size affects model's responses to prompt variations.For instance, GPT-4's performance is less influenced by prompt changes compared to GPT-3.5, suggesting that larger models may process prompts more consistently.This discovery prompts further research into LLM interpretability, aiming to refine AI adaptability and human-AI interaction.</p>
<p>Limitations</p>
<p>This study was focused on GPT-based models, however, we plan to examine the impact of prompt formats on other models, such as LLaMA (Touvron et al., 2023), Gemini (Team et al., 2023), PaLM (Chowdhery et al., 2022), or smaller models like Phi (Li et al., 2023) in the future.This would provide a more holistic understanding of the influence that prompt formatting exerts across different LLM families.</p>
<p>Moreover, there is an opportunity to enhance the breadth of template exploration in subsequent studies.Our research did not include formats like HTML or XML, which are prevalent in the training datasets of many models.Incorporating these formats could yield a more exhaustive examination of prompt format effects.</p>
<p>Lastly, our experimental design maintained all other prompt design elements constant, isolating prompt format as the sole variable.It would be intriguing for future work to investigate how the sensitivity of models to prompt format might shift when other prompt engineering techniques are modified.This includes varying the number of fewshot examples provided or refining the precision of prompt instructions.Such research could offer valuable insights into the interplay between prompt structure and model responsiveness, potentially informing more effective prompt engineering practices.Yao et al., 2023)) have been introduced.A thorough examination of these methodologies can be found in the survey by (Sahoo et al., 2024).</p>
<p>In recent developments, a novel prompt programming framework ((Microsoft)) has been introduced, which offers greater control and efficiency in generating structured outputs.Our study diverges from this approach by examining the effects of more prevalent and established prompt formats on LLMs, as opposed to investigating formats that are newly proposed and not widely adopted yet.Furthermore, it is important to note that third-party tools are predominantly designed for integration with opensource models, which may not seamlessly extend to proprietary models such as GPT.Another similar vein of research is dedicated to the structural design of prompts, aiming to optimize task performance without altering the inherent semantic content.This includes investigations into the sequential arrangement of context ( (Liu et al., 2023;Zhao et al., 2021;Lu et al., 2022)) and the design of prompt formats ( (Sclar et al., 2023;Voronov et al., 2024;Shu et al., 2023)).Our work contributes to this growing body of literature by examining the impact of prompt formatting on the performance of LLMs.</p>
<p>Prompt Format The sensitivity of LLMs to prompt construction is a well-documented phenomenon, yet research on the impact of prompt formats on model performance remains sparse.Pioneering studies ( (Sclar et al., 2023;Voronov et al., 2024;Shu et al., 2023)) have conducted rigorous investigations, revealing that widely used opensource LLMs exhibit extreme sensitivity to variations in prompt format.These studies, however, primarily focus on subtle, local changes to the format-such as the number of colons following a question, the insertion of newlines, or the selection of input/output verbalizers.Besides, their experimental designs are confined to classification tasks, limiting the generalizability of findings across diverse tasks.</p>
<p>Our research diverges from these existing studies by examining the effects of global prompt format modifications on model performance, offering insights that are applicable to a broad spectrum of LLM-based tasks that necessitate prompt engineering.The closest related work to ours is by (Sui et al., 2024), which however only provides a cursory exploration of format influence and is restricted to tabular data.To the best of our knowledge, our study is the first effort to systematically investigate the impact of global prompt format variations -an inescapable aspect of prompt engineering design decisions.</p>
<p>B Datasets</p>
<p>We evaluate six distinct benchmarks and classify them according to the nature of the task involved.</p>
<p>NL2NL</p>
<p>• Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2020) covers 57 subjects including 20 STEM subjects, 13 humanities subjects, 12 social sciences subjects and 12 other subjects.Each subject contains at least 100 multiple choice questions, which tests both world knowledge and problem solving ability.We use the dev set which contains 5 questions per subjects as few-shot examples, and use test set containing 14,079 questions with different levels of difficulty to evaluation model performance.We use accuracy score to measure model performance.</p>
<p>• NER Finance: OpenAI Evals (OpenAI, 2023) is a framework containing a registry of evaluations to test LLMs where NER Finance is one of those.This dataset contains samples between one sentence to one paragraph long from financial documents.The task is to extract the all of the entities in the document, with the evaluation being if the LLM outputs each entity, in order.We randomly sample 500 examples from this dataset.</p>
<p>NL2Code</p>
<p>• HumanEval (Chen et al., 2021) is a benchmark dataset consisting of a collection of Python programming problems, each accompanied by a function signature, a docstring outlining the problem to be solved, and a set of unit tests that the correct implementation must pass.We use the evaluation metric pass@1, which checks if the the generated code passes the unit given unit tests in one attempt.We use all 164 samples in this dataset.</p>
<p>• FIND (Schwettmann et al., 2023): The Function Interpretation and Description (FIND) benchmark dataset is a natural language-tocode generation task.The LLM is given 5 example inputs and outputs to an unknown Python function and is tasked with reverse engineering the original Python code.We evaluate the benchmark by comparing the output of test cases on a ground truth function with the output from LLM generated functions.We use the "strings" category of functions for the task consisting of 500 functions.We provide the LLM with 5 pairs of input and output for each function.To select these examples, we randomly sample from a dataset provided by (Schwettmann et al., 2023) that contains example test strings for each function.To evaluate the generated function code, we use the string indicator metric introduced by (Schwettmann et al., 2023) that measures the number of test cases passed by the function.</p>
<p>Code2Code</p>
<p>• CODEXGLUE (Lu et al., 2021) stands for General Language Understanding Evaluation benchmark for CODE.It was originally introduced to address the lack of diversified benchmarks in code intelligence by providing a diverse set of tasks, including code translation.We use the parallel code for Java and C-Sharp and vice versa.We use the test set containing 1000 parallel code in Java and C-Sharp to experiment the capabilities of the LLMs in translating code from one programming language to another.The performance of the LLMs is evaluated using the BLEU (Papineni et al., 2002) score, which compares the generated code to the reference code.</p>
<p>• HumanEval-X (Zheng et al., 2023) dataset is a benchmark designed to evaluate the multilingual capabilities of code generative models.It contains 820 high-quality, human-crafted data samples, each accompanied by test cases.</p>
<p>The dataset supports a variety of programming languages, including Python, C++, Java, JavaScript, and Go.In this we experiment with one of the above dimension of codetranslation focusing on Java to Python.To accomplish this task, we combine the "declaration" and "canonical-solution" together to finally get the overall function in the respective language."Declaration" contains the function declaration for the respective language and "canonical solution" has the human-crafted example solution for the language.Similar to CODEXGLUE, we use the BLEU (Papineni et al., 2002) score for measuring the performance.</p>
<p>C Prompt Templates</p>
<p>In this section we provide examples of the four prompt templates we used for the NER Finance task.Prompts for all other tasks followed identical formatting.Variables that are injected into the prompt are denoted by blue text wrapped in braces.</p>
<p>For example a user ask being injected is denoted as {USER ASK}.</p>
<p>D Additional Research Questions</p>
<p>D.1 How does the format in which information is structured and presented influence the ability to solve problems that require different skill sets?</p>
<p>We analyze whether model's sensitivity to prompt format changes is related to the skills required to solve the task using the MMLU benchmark which comprises 57 subjects, categorized into four domains: humanities, social science, STEM, and others.Each domain encompasses various disciplines, necessitating distinct skill sets and knowledge for accurate question answering.</p>
<p>Figure 5 breaks down the performance on MMLU dataset by domain.We observe the per-formance spread exists across different tasks, and it's not signified nor eliminated by specific skills required.This suggests that the model's sensitivity to prompt formatting is a general characteristic, rather than being contingent on the specific skills or reasoning abilities required by different tasks.Performance is influenced by how information is presented to it, regardless of the complexity or na-</p>
<p>Markdown</p>
<p>System: ## Persona -You are a annotator working for large financial data company are tasked with extracting named entities from financial documents who follows strict guidelines for quality and formatting.## Instructions -You will be given a sentence from a financial document.-List the named entities in the order they appear in the sentence.</p>
<p>-If an entity appears multiple times, list it multiples times.</p>
<p>-Provide your chain of thought first and then respond with your final answer.Description: You are a annotator working for large financial data company are tasked with extracting named entities from financial documents who follows strict guidelines for quality and formatting.</p>
<p>Instructions:</p>
<p>-You will be given a sentence from a financial document.</p>
<p>-List the named entities in the order they appear in the sentence.</p>
<p>-If an entity appears multiple times, list it multiples times.</p>
<p>-Provide your chain of thought first and then respond with your final answer.</p>
<p>Output_Format:</p>
<p>Entities should be stated in the format NAME -TYPE where TYPE can be PERSON, ORGANIZATION, or LOCATION.State your final answer as a comma-separated list of entities enclosed in square brackets.</p>
<p>Examples:</p>
<ul>
<li>trained on more data than GPT-3.5, and is clearly the overall more capable model ((Achiam et al., 2023;Bubeck et al., 2023;Carlini et al., 2024)).In this section, we aim to ascertain whether an expansion in general capability translates to enhanced stability in response to changes in templates.The CMDs for all the models across benchmarks are presented in Figure 6.</li>
</ul>
<p>A lower value of CMD indicates more robustness to template variation.The results indicate that the GPT-4-1106-preview model exhibits superior robustness to format changes, maintaining a performance dispersion consistently below 0.036 across all benchmarks.In contrast, the GPT-4-32k-0613 model demonstrates less robustness relative to the GPT-4-1106-preview, yet it outperforms the GPT-3.5 series, with CMDs not exceeding 0.043.The GPT-3.5 series displays a broader range of CMDs, from 0.035 to 0.176, signifying a higher degree of performance variability under different prompt formats.GPT-4's observed improvements may be attributed to its enhanced ability to process data in diverse formats.Moreover, it is possible that the robustness of the model is not adversely impacted by format variations at the level of the last hidden layer of prompt embedding.Notably, the GPT-4-1106preview model achieves greater robustness compared to the GPT-4-32k-0613, corroborating existing evidence that suggests the former has a heightened proficiency in comprehending and generating content in specific formats as instructed (OpenAI, November 2023).Further examining GPT-4-32k-0613's performance, we notice the CMD on Hu-manEval benchmark is extremely high, this is due the extremely low score using JSON format, see Table 4 for results.Analyzing the model outputs, we find the poor performance is because most of the time the model would generate chain of thought in plain text, but did not continue with actually generating the code.The other models did not exhibit this behavior for the JSON template.We hypothesize that this may be related to the OpenAI's claim about fixing laziness in task completion in the 0125 version of GPT-4-turbo (OpenAI, 2024).In summary, larger models are more robust to template variation.</p>
<p>E Complete Results</p>
<p>E.1 Additional results on model performance under all templates across benchamrks.</p>
<p>E.2 IoU scores on all benchmarks.</p>
<p>E.3 Dotplot on all benchmark datasets</p>
<p>Figure 2: Consistency comparison for MMLU dataset: GPT-3.5 models show consistency scores below 0.5 across format pairs, whereas GPT-4 consistently exceeds 0.5, indicating greater reliability.</p>
<p>Figure 3 :
3
Figure 3: Intersection over Union (IoU) scores for top templates on the NER Finance benchmark across models.Higher IoU is observed within same-version model pairs, whereas cross-version pairs exhibit lower IoU.</p>
<h1></h1>
<h1>Output Format -Entities should be stated in the format NAME -TYPE where TYPE can be PERSON, ORGANIZATION, or LOCATION.-State your final answer as a comma-separated list of entities enclosed in square brackets.Example: [Bank -ORGANIZATION, Borrower -PERSON].-Ifthere are no entities found, state your final answer as 'No entities found'.</h1>
<p>Figure 6 :
6
Figure 6: Coefficient of mean deviation (CMD) of scalar metrics for all the prompt templates.Figure shows the CMDs across models and datasets.GPT-3.5 series exhibit larger CMD scores across benchmarks than GPT-4 series, indicating higher sensitivity to the choice of format.</p>
<p>Figure 7 :
7
Figure 7: Heatmap of IoU values for other benchmarks.</p>
<p>Figure 8 :
8
Figure 8: Performance of Consistency for FIND dataset across models.</p>
<p>Figure 9 :
9
Figure 9: Dotplot of model performance across prompt formats on all benchmarks.</p>
<p>Zhuosheng Zhang, Aston Zhang,Mu Li, and Alex  Smola.2022.Automatic chain of thought prompting in large language models.Preprint, arXiv:2210.03493.Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh.2021.Calibrate before use: Improving few-shot performance of language models.
Preprint, arXiv:2102.09690.Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, ShanWang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang,Yang Li, Teng Su, Zhilin Yang, and Jie Tang. 2023.Codegeex: A pre-trained model for code genera-tion with multilingual evaluations on humaneval-x.Preprint, arXiv:2303.17568.A Related WorkPrompt Engineering The field of prompt engi-neering has garnered significant interest in recentyears, in parts due to the emergent capabilities ofthe most capable LLMs, while also trying to bet-ter control their still unpredictable outcomes. Aprominent strand of research within this domainconcentrates on innovative prompting methodolo-gies. These include few-shot prompting ((Brownet al., 2020)), which enables models to adapt tonew tasks without extensive retraining, and Chain-of-Thought prompting ((Wei et al., 2023)), both ofwhich are designed to enhance the reasoning capa-bilities of LLMs. Additionally, Automatic Chain-of-Thought (Auto-CoT) ((Zhang et al., 2022)) andSelf-Consistency ((Wang et al., 2023)) approacheshave been developed to further refine these reason-ing processes. To mitigate hallucinations in LLMoutputs, techniques such as Retrieval AugmentedGeneration (RAG) ((Lewis et al., 2021)) and Re-Act ((</p>
<p>Table 2 :
2
Prompt templates considered in this paper.Placeholders are denoted with {variable name} and get replaced with task specific context.You are a annotator working for large financial data company and are tasked with extracting named entities from financial documents who follows strict guidelines for quality and formatting.The following sentence is from a financial document.List the named entities in the order they appear in the sentence.If an entity appears multiple times, list it multiples times.Entities should be stated in the format NAME -TYPE where TYPE can be PERSON, ORGANIZATION, or LOCATION.State your final answer as a comma-separated list of entities enclosed in square brackets.Example: [Bank -ORGANIZATION, Borrower -PERSON].If there are no entities found, state your final answer as 'No entities found'.Provide your chain of thought first and then respond with your final answer.Here is an example: {ICL EXAMPLE INPUT} {ICL EXAMPLE SOLUTION}
Prompt FormatPrompt TemplatePlain text{persona} {instructions} {examples} {output format instructions} {user ask}## Persona{persona}## Instructions{instructions}Markdown## Examples {examples}## Output Format{Output format instructions}## User Question{user ask}Persona-{persona}Instructions-{instructions}YAMLExamples -{examples}Output format-{output format instructions}User question-{user ask}{"Persona": "{persona}","Instructions": "{instructions}","Examples": "{examples}",JSON"Output format": "{output format instructions}"}{"User ask": "{user ask}"}PlaintextSystem:User:{INPUT}
JSONSystem: { "Persona": "You are a annotator working for large financial data company are tasked with extracting named entities from financial documents who follows strict guidelines for quality and formatting.","Instructions": [ "You will be given a sentence from a financial document.","List the named entities in the order they appear in the sentence.sensitivity to prompt format.While the architectural details and exact size of GPT-4 are not published, it is assumed that GPT-4 contains significantly more parameters, was
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Retrieval-augmented generation for knowledgeintensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen Tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela, arXiv:2005.114012021Preprint</p>
<p>Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, Yin Tat, Lee , arXiv:2309.05463Textbooks are all you need ii: phi-1.5 technical report. 2023arXiv preprint</p>
<p>Lost in the middle: How language models use long contexts. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang, arXiv:2307.031722023Preprint</p>
<p>Codexglue: A machine learning benchmark dataset for code understanding and generation. Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, arXiv:2102.046642021arXiv preprint</p>
<p>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp, arXiv:2104.087862022Preprint</p>
<p>Azure openai service models. Microsoft</p>
<p>Evals. 2023OpenAI</p>
<p>New embedding models and api updates. 2024OpenAI</p>
<p>Improved instruction following and json mode. Openai, November 2023</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, arXiv:2203.02155Jan Leike, and Ryan Lowe. 2022Preprint</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>A systematic survey of prompt engineering in large language models: Techniques and applications. Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, Aman Chadha, arXiv:2402.079272024Preprint</p>
<p>Sarah Schwettmann, Tamar Rott Shaham, Joanna Materzynska, Neil Chowdhury, Shuang Li, Jacob Andreas, David Bau, Antonio Torralba, arXiv:2309.03886Find: A function description benchmark for evaluating interpretability methods. 2023Preprint</p>
<p>Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. Melanie Sclar, Yejin Choi, Yulia Tsvetkov, Alane Suhr, arXiv:2310.113242023arXiv preprint</p>
<p>You don't need a personality test to know these models are unreliable: Assessing the reliability of large language models on psychometric instruments. Bangzhao Shu, Lechen Zhang, Minje Choi, Lavinia Dunagan, Dallas Card, David Jurgens, arXiv:2311.097182023arXiv preprint</p>
<p>Table meets llm: Can large language models understand structured table data? a benchmark and empirical study. Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, Dongmei Zhang, The 17th ACM International Conference on Web Search and Data Mining. 2024WSDM '24</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.11805Gemini: a family of highly capable multimodal models. 2023arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Mind your format: Towards consistent evaluation of in-context learning improvements. Anton Voronov, Lena Wolf, Max Ryabinin, arXiv:2401.067662024arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712023Preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.119032023Preprint</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.036292023Preprint</p>            </div>
        </div>

    </div>
</body>
</html>
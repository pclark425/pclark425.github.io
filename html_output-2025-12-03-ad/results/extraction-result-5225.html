<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5225 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5225</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5225</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-267616993</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.05952v1.pdf" target="_blank">Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques</a></p>
                <p><strong>Paper Abstract:</strong> The integration of Large Language Models (LLMs) with Graph Representation Learning (GRL) marks a significant evolution in analyzing complex data structures. This collaboration harnesses the sophisticated linguistic capabilities of LLMs to improve the contextual understanding and adaptability of graph models, thereby broadening the scope and potential of GRL. Despite a growing body of research dedicated to integrating LLMs into the graph domain, a comprehensive review that deeply analyzes the core components and operations within these models is notably lacking. Our survey fills this gap by proposing a novel taxonomy that breaks down these models into primary components and operation techniques from a novel technical perspective. We further dissect recent literature into two primary components including knowledge extractors and organizers, and two operation techniques including integration and training stratigies, shedding light on effective model design and training strategies. Additionally, we identify and explore potential future research avenues in this nascent yet underexplored field, proposing paths for continued progress.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5225.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5225.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphText</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphText (syntax tree-based graph-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A syntax tree-based method that converts graph structure into text sequences so that LLMs can perform graph reasoning in text space by operating on the generated textual representation of graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GraphText: Graph reasoning in text space.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>syntax-tree linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Converts graph structure into a sequence of tokens using a syntax-tree-based linearization: graph nodes/edges and substructures are serialized into a tree-like textual format that preserves hierarchical relations and connectivity as textual constructs (syntax nodes, parent-child relations) rather than raw adjacency lists.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>text-attributed graphs / general graphs (as used for graph reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Aims to increase interpretability by explicitly encoding hierarchical/structural relations in tree form; more compact than naive node-by-node textual dumps but still lossy compared to full graph; trades off completeness for being processable by LLMs; designed to expose structure to language models in a structured textual manner.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node-level tasks and graph reasoning tasks (node classification and graph reasoning evaluated in the survey references)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Survey notes GraphText as an approach alongside other graph-to-text methods; compared qualitatively to naive textual descriptions and to special-token approaches (e.g., GraphTokens in GraphGPT). GraphText emphasizes syntactic/tree structure encoding while other approaches may use ego-graph templates or special tokens; no numeric head-to-head results provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Input length constraints limit how much of the graph can be represented; serialization is lossy (cannot fully capture arbitrary graphs or long-range structural patterns); potential loss of multi-hop/global topology information; evaluation and direct quantitative comparisons are limited in the cited literature.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5225.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5225.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphGPT (Graph instruction tuning with GraphTokens)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses GraphTokens and graph instruction tuning to communicate graph structural information to LLMs, reducing token sequence length required to represent structure and enabling instruction-tuned LLMs to handle graph tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graphgpt: Graph instruction tuning for large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GraphTokens + instruction-tuned textual encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes graph structure by introducing special GraphTokens (compact tokens representing structural elements or substructures) and uses instruction tuning to teach the LLM to interpret those tokens; instead of linearizing entire adjacency, selected subgraph/context is summarized into templates augmented by GraphTokens.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>subgraphs / text-attributed graphs / node-centric ego-graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Compactness: GraphTokens are designed to reduce sequence length; interpretability depends on token design and instruction tuning; aims to balance expressivity and input budget by abstracting repeated or complex substructures into tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node-level tasks (node classification) and other downstream graph learning tasks after Graph Instruction Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Survey highlights GraphTokens as reducing token length relative to verbose natural-language narrations of graph structure; compared qualitatively to template/ego-graph textual encodings (e.g., InstructGLM) and to syntax-tree methods (GraphText). The survey reports GraphTokens significantly reduce token sequence length required to pass structure to an LLM, but does not provide numeric performance comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Designing GraphTokens and instruction-tuning regimes is non-trivial; abstraction into tokens can omit fine-grained structural details; still constrained by LLM input size and LLMs' ability to learn token semantics; potential generalization limits across varying graph topologies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5225.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5225.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT4Graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT4Graph (Graph Modeling Language / Graph Markup Language representations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An empirical evaluation/benchmark that represents graphs for LLMs via Graph Modeling Language or Graph Markup Language textual encodings to probe whether general-purpose LLMs can understand graph-structured data when fed structured textual descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph Modeling Language / Graph Markup Language textualization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes graphs into formalized textual formats resembling a modeling or markup language (explicit node/edge declarations, typed relations, attributes) so that LLMs receive a structured, machine-oriented text describing the graph; aims to be precise and unambiguous compared to freeform natural language descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general graphs / structured graphs used for benchmarking LLM understanding of graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>High faithfulness in describing explicit local structure (node and edge declarations) and attributes; relatively verbose (can be long for large graphs); interpretable by humans and LLMs as a quasi-programmatic description; expressivity good for exact local relations but scalability limited by token budget.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Empirical evaluation and benchmarking of LLM ability to solve graph tasks posed in natural language after being given graph-structured textual descriptions (graph understanding probes, graph reasoning tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Survey cites GPT4Graph's approach as a structured textual encoding compared to narrative templates or token abstractions; it's presented as a baseline to empirically probe LLM capabilities. The survey does not provide direct numeric comparisons, but indicates GPT4Graph evaluates LLMs on graph tasks with these textual encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Verbosity leads to token budget limitations, restricting coverage of large graphs; LLMs still struggle with long-range/complex relational reasoning even when given precise textual encodings; potential mismatch between formal markup text and LLM pretraining distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5225.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5225.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructGLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructGLM (ego-graph templates / self-supervised link-prediction pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that creates template-based textual descriptions of each node's local ego-graph (up to 3-hop context) and also explores self-supervised link-prediction tasks to inject structural knowledge into LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Natural language is all a graph needs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>ego-graph template textualization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Creates human-readable templates summarizing a target node's local neighborhood (multi-hop up to a predefined radius) and integrates that summary into the LLM input; also uses self-supervised link prediction as an auxiliary training objective to teach LLMs structural notions.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>text-attributed graphs / local ego-graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Focuses on local structural context (bounded-hop) to stay within input limits; compactness controlled by template design and sampling strategy; interpretable summaries but inherently lossy for global graph properties; amenable to instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification and other node-centric graph learning tasks; pretraining evaluated via improvement on downstream graph tasks after instruction tuning or auxiliary self-supervised objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Survey contrasts ego-graph templates with full-graph textualizations and with token-based abstractions; ego-graph templates trade global completeness for tractable input size and were reported (in cited work) to significantly improve LLM performance on node tasks relative to vanilla prompts, though the survey does not list numeric results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Limited to bounded-hop local context (loses long-range/global structure); dependent on node sampling strategy and template quality; scaling to large graphs and retaining multi-hop dependencies remains challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5225.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5225.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neighbor-summary prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neighbor-summary prompt (LLM-based contextual neighbor summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that extracts contextual information for a target node by instructing an LLM to summarize the textual features of neighboring nodes and connectivity into a prompt to assist downstream node-level tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the potential of large language models (llms) in learning on graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>neighbor-summary textual prompt</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Constructs prompts by aggregating/ summarizing textual attributes of a node's neighbors (optionally with connectivity descriptions) into a cohesive textual context which is then provided to the LLM as the basis for prediction or reasoning about the target node.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>text-attributed graphs / node neighborhoods (ego-graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Compact and focused on local context; leverages LLM semantic understanding to summarize and infer; interpretable summaries; lossy with respect to exact structural topology beyond immediate neighbors; adjustable by neighborhood radius.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node-level tasks such as node classification and zero-shot/ few-shot label inference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared qualitatively in the survey to other input-level methods (ego-graph templates, syntax-tree linearizations, token abstractions). The neighbor-summary prompt emphasizes semantic neighbor content rather than explicit structural encoding; survey cites it as effective for enhancing LLM's node-level predictions but provides no numeric comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Does not fully encode multi-hop/ global graph structure; quality depends on textual richness of node attributes; may discard structural signals if prompt focuses only on textual neighbors; vulnerable to LLM hallucination or bias in summary generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5225.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5225.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Survey-level input-level conversion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Input-level integration / natural-language linearization (survey summary)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general category of methods that convert graph structure into natural-language narratives or textual templates so that LLMs (Transformers) can directly consume graph information as text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>natural-language linearization / narrative serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Generic approach: transform graph elements (nodes, edges, attributes, subgraphs) into natural language sentences or templates (narratives, descriptions, markup-like declarations) that describe connectivity and attributes; may use sampling to keep within token limits (e.g., select subgraphs, ego-nets).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general graphs, text-attributed graphs, knowledge graphs, ego-graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>High compatibility with LLMs and human interpretable; expressive for semantic content but potentially lossy for exact graph topology; varies in compactness depending on serialization strategy; scalable only with aggressive summarization or token abstractions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification, graph reasoning, few/zero-shot label inference, link prediction (when represented as text), and general graph understanding probes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Survey positions this as the common strategy used by LLM-centric organizers, contrasted with GNN-based encodings and hybrid hidden-level or alignment-based integrations; narrative linearizations are easier to feed to LLMs but often underperform compared to structured GNN encodings on tasks requiring precise topology unless augmented or pre-trained for graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Severe input-length constraints; loss of exact structural fidelity; LLMs' limited ability with long-range dependencies and precise multi-hop reasoning; potential biases in textualization and mismatch with LLM pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>GraphText: Graph reasoning in text space. <em>(Rating: 2)</em></li>
                <li>Graphgpt: Graph instruction tuning for large language models. <em>(Rating: 2)</em></li>
                <li>Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. <em>(Rating: 2)</em></li>
                <li>Natural language is all a graph needs. <em>(Rating: 2)</em></li>
                <li>Exploring the potential of large language models (llms) in learning on graphs. <em>(Rating: 2)</em></li>
                <li>Graph Modeling Language / Graph Markup Language (as used in GPT4Graph evaluations). <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5225",
    "paper_id": "paper-267616993",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "GraphText",
            "name_full": "GraphText (syntax tree-based graph-to-text)",
            "brief_description": "A syntax tree-based method that converts graph structure into text sequences so that LLMs can perform graph reasoning in text space by operating on the generated textual representation of graphs.",
            "citation_title": "GraphText: Graph reasoning in text space.",
            "mention_or_use": "mention",
            "representation_name": "syntax-tree linearization",
            "representation_description": "Converts graph structure into a sequence of tokens using a syntax-tree-based linearization: graph nodes/edges and substructures are serialized into a tree-like textual format that preserves hierarchical relations and connectivity as textual constructs (syntax nodes, parent-child relations) rather than raw adjacency lists.",
            "graph_type": "text-attributed graphs / general graphs (as used for graph reasoning)",
            "representation_properties": "Aims to increase interpretability by explicitly encoding hierarchical/structural relations in tree form; more compact than naive node-by-node textual dumps but still lossy compared to full graph; trades off completeness for being processable by LLMs; designed to expose structure to language models in a structured textual manner.",
            "evaluation_task": "Node-level tasks and graph reasoning tasks (node classification and graph reasoning evaluated in the survey references)",
            "performance_metrics": null,
            "comparison_to_other_representations": "Survey notes GraphText as an approach alongside other graph-to-text methods; compared qualitatively to naive textual descriptions and to special-token approaches (e.g., GraphTokens in GraphGPT). GraphText emphasizes syntactic/tree structure encoding while other approaches may use ego-graph templates or special tokens; no numeric head-to-head results provided in the survey.",
            "limitations_or_challenges": "Input length constraints limit how much of the graph can be represented; serialization is lossy (cannot fully capture arbitrary graphs or long-range structural patterns); potential loss of multi-hop/global topology information; evaluation and direct quantitative comparisons are limited in the cited literature.",
            "uuid": "e5225.0",
            "source_info": {
                "paper_title": "Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GraphGPT",
            "name_full": "GraphGPT (Graph instruction tuning with GraphTokens)",
            "brief_description": "A method that uses GraphTokens and graph instruction tuning to communicate graph structural information to LLMs, reducing token sequence length required to represent structure and enabling instruction-tuned LLMs to handle graph tasks.",
            "citation_title": "Graphgpt: Graph instruction tuning for large language models.",
            "mention_or_use": "mention",
            "representation_name": "GraphTokens + instruction-tuned textual encoding",
            "representation_description": "Encodes graph structure by introducing special GraphTokens (compact tokens representing structural elements or substructures) and uses instruction tuning to teach the LLM to interpret those tokens; instead of linearizing entire adjacency, selected subgraph/context is summarized into templates augmented by GraphTokens.",
            "graph_type": "subgraphs / text-attributed graphs / node-centric ego-graphs",
            "representation_properties": "Compactness: GraphTokens are designed to reduce sequence length; interpretability depends on token design and instruction tuning; aims to balance expressivity and input budget by abstracting repeated or complex substructures into tokens.",
            "evaluation_task": "Node-level tasks (node classification) and other downstream graph learning tasks after Graph Instruction Tuning",
            "performance_metrics": null,
            "comparison_to_other_representations": "Survey highlights GraphTokens as reducing token length relative to verbose natural-language narrations of graph structure; compared qualitatively to template/ego-graph textual encodings (e.g., InstructGLM) and to syntax-tree methods (GraphText). The survey reports GraphTokens significantly reduce token sequence length required to pass structure to an LLM, but does not provide numeric performance comparisons.",
            "limitations_or_challenges": "Designing GraphTokens and instruction-tuning regimes is non-trivial; abstraction into tokens can omit fine-grained structural details; still constrained by LLM input size and LLMs' ability to learn token semantics; potential generalization limits across varying graph topologies.",
            "uuid": "e5225.1",
            "source_info": {
                "paper_title": "Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT4Graph",
            "name_full": "GPT4Graph (Graph Modeling Language / Graph Markup Language representations)",
            "brief_description": "An empirical evaluation/benchmark that represents graphs for LLMs via Graph Modeling Language or Graph Markup Language textual encodings to probe whether general-purpose LLMs can understand graph-structured data when fed structured textual descriptions.",
            "citation_title": "Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking.",
            "mention_or_use": "mention",
            "representation_name": "Graph Modeling Language / Graph Markup Language textualization",
            "representation_description": "Encodes graphs into formalized textual formats resembling a modeling or markup language (explicit node/edge declarations, typed relations, attributes) so that LLMs receive a structured, machine-oriented text describing the graph; aims to be precise and unambiguous compared to freeform natural language descriptions.",
            "graph_type": "general graphs / structured graphs used for benchmarking LLM understanding of graphs",
            "representation_properties": "High faithfulness in describing explicit local structure (node and edge declarations) and attributes; relatively verbose (can be long for large graphs); interpretable by humans and LLMs as a quasi-programmatic description; expressivity good for exact local relations but scalability limited by token budget.",
            "evaluation_task": "Empirical evaluation and benchmarking of LLM ability to solve graph tasks posed in natural language after being given graph-structured textual descriptions (graph understanding probes, graph reasoning tasks).",
            "performance_metrics": null,
            "comparison_to_other_representations": "Survey cites GPT4Graph's approach as a structured textual encoding compared to narrative templates or token abstractions; it's presented as a baseline to empirically probe LLM capabilities. The survey does not provide direct numeric comparisons, but indicates GPT4Graph evaluates LLMs on graph tasks with these textual encodings.",
            "limitations_or_challenges": "Verbosity leads to token budget limitations, restricting coverage of large graphs; LLMs still struggle with long-range/complex relational reasoning even when given precise textual encodings; potential mismatch between formal markup text and LLM pretraining distributions.",
            "uuid": "e5225.2",
            "source_info": {
                "paper_title": "Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "InstructGLM",
            "name_full": "InstructGLM (ego-graph templates / self-supervised link-prediction pretraining)",
            "brief_description": "A system that creates template-based textual descriptions of each node's local ego-graph (up to 3-hop context) and also explores self-supervised link-prediction tasks to inject structural knowledge into LLMs.",
            "citation_title": "Natural language is all a graph needs.",
            "mention_or_use": "mention",
            "representation_name": "ego-graph template textualization",
            "representation_description": "Creates human-readable templates summarizing a target node's local neighborhood (multi-hop up to a predefined radius) and integrates that summary into the LLM input; also uses self-supervised link prediction as an auxiliary training objective to teach LLMs structural notions.",
            "graph_type": "text-attributed graphs / local ego-graphs",
            "representation_properties": "Focuses on local structural context (bounded-hop) to stay within input limits; compactness controlled by template design and sampling strategy; interpretable summaries but inherently lossy for global graph properties; amenable to instruction tuning.",
            "evaluation_task": "Node classification and other node-centric graph learning tasks; pretraining evaluated via improvement on downstream graph tasks after instruction tuning or auxiliary self-supervised objectives.",
            "performance_metrics": null,
            "comparison_to_other_representations": "Survey contrasts ego-graph templates with full-graph textualizations and with token-based abstractions; ego-graph templates trade global completeness for tractable input size and were reported (in cited work) to significantly improve LLM performance on node tasks relative to vanilla prompts, though the survey does not list numeric results.",
            "limitations_or_challenges": "Limited to bounded-hop local context (loses long-range/global structure); dependent on node sampling strategy and template quality; scaling to large graphs and retaining multi-hop dependencies remains challenging.",
            "uuid": "e5225.3",
            "source_info": {
                "paper_title": "Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Neighbor-summary prompting",
            "name_full": "Neighbor-summary prompt (LLM-based contextual neighbor summarization)",
            "brief_description": "A prompting technique that extracts contextual information for a target node by instructing an LLM to summarize the textual features of neighboring nodes and connectivity into a prompt to assist downstream node-level tasks.",
            "citation_title": "Exploring the potential of large language models (llms) in learning on graphs.",
            "mention_or_use": "mention",
            "representation_name": "neighbor-summary textual prompt",
            "representation_description": "Constructs prompts by aggregating/ summarizing textual attributes of a node's neighbors (optionally with connectivity descriptions) into a cohesive textual context which is then provided to the LLM as the basis for prediction or reasoning about the target node.",
            "graph_type": "text-attributed graphs / node neighborhoods (ego-graphs)",
            "representation_properties": "Compact and focused on local context; leverages LLM semantic understanding to summarize and infer; interpretable summaries; lossy with respect to exact structural topology beyond immediate neighbors; adjustable by neighborhood radius.",
            "evaluation_task": "Node-level tasks such as node classification and zero-shot/ few-shot label inference.",
            "performance_metrics": null,
            "comparison_to_other_representations": "Compared qualitatively in the survey to other input-level methods (ego-graph templates, syntax-tree linearizations, token abstractions). The neighbor-summary prompt emphasizes semantic neighbor content rather than explicit structural encoding; survey cites it as effective for enhancing LLM's node-level predictions but provides no numeric comparisons.",
            "limitations_or_challenges": "Does not fully encode multi-hop/ global graph structure; quality depends on textual richness of node attributes; may discard structural signals if prompt focuses only on textual neighbors; vulnerable to LLM hallucination or bias in summary generation.",
            "uuid": "e5225.4",
            "source_info": {
                "paper_title": "Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Survey-level input-level conversion",
            "name_full": "Input-level integration / natural-language linearization (survey summary)",
            "brief_description": "A general category of methods that convert graph structure into natural-language narratives or textual templates so that LLMs (Transformers) can directly consume graph information as text.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "representation_name": "natural-language linearization / narrative serialization",
            "representation_description": "Generic approach: transform graph elements (nodes, edges, attributes, subgraphs) into natural language sentences or templates (narratives, descriptions, markup-like declarations) that describe connectivity and attributes; may use sampling to keep within token limits (e.g., select subgraphs, ego-nets).",
            "graph_type": "general graphs, text-attributed graphs, knowledge graphs, ego-graphs",
            "representation_properties": "High compatibility with LLMs and human interpretable; expressive for semantic content but potentially lossy for exact graph topology; varies in compactness depending on serialization strategy; scalable only with aggressive summarization or token abstractions.",
            "evaluation_task": "Node classification, graph reasoning, few/zero-shot label inference, link prediction (when represented as text), and general graph understanding probes.",
            "performance_metrics": null,
            "comparison_to_other_representations": "Survey positions this as the common strategy used by LLM-centric organizers, contrasted with GNN-based encodings and hybrid hidden-level or alignment-based integrations; narrative linearizations are easier to feed to LLMs but often underperform compared to structured GNN encodings on tasks requiring precise topology unless augmented or pre-trained for graph structure.",
            "limitations_or_challenges": "Severe input-length constraints; loss of exact structural fidelity; LLMs' limited ability with long-range dependencies and precise multi-hop reasoning; potential biases in textualization and mismatch with LLM pretraining.",
            "uuid": "e5225.5",
            "source_info": {
                "paper_title": "Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "GraphText: Graph reasoning in text space.",
            "rating": 2,
            "sanitized_title": "graphtext_graph_reasoning_in_text_space"
        },
        {
            "paper_title": "Graphgpt: Graph instruction tuning for large language models.",
            "rating": 2,
            "sanitized_title": "graphgpt_graph_instruction_tuning_for_large_language_models"
        },
        {
            "paper_title": "Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking.",
            "rating": 2,
            "sanitized_title": "gpt4graph_can_large_language_models_understand_graph_structured_data_an_empirical_evaluation_and_benchmarking"
        },
        {
            "paper_title": "Natural language is all a graph needs.",
            "rating": 2,
            "sanitized_title": "natural_language_is_all_a_graph_needs"
        },
        {
            "paper_title": "Exploring the potential of large language models (llms) in learning on graphs.",
            "rating": 2,
            "sanitized_title": "exploring_the_potential_of_large_language_models_llms_in_learning_on_graphs"
        },
        {
            "paper_title": "Graph Modeling Language / Graph Markup Language (as used in GPT4Graph evaluations).",
            "rating": 1,
            "sanitized_title": "graph_modeling_language_graph_markup_language_as_used_in_gpt4graph_evaluations"
        }
    ],
    "cost": 0.012628249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques</p>
<p>Qiheng Mao maoqiheng@zju.edu.cn 
Zhejiang University</p>
<p>Zemin Liu zeminliu@nus.edu.sg 
National University of Singapore</p>
<p>Chenghao Liu chenghao.liu@salesforce.com 
Salesforce Research Asia</p>
<p>Zhuo Li lizhuo@zju.edu.cn 
State Street Technology (Zhejiang) Ltd</p>
<p>Jianling Sun 
Zhejiang University</p>
<p>Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques
428A65FC795C7FEB757B7F9C22B25922
The integration of Large Language Models (LLMs) with Graph Representation Learning (GRL) marks a significant evolution in analyzing complex data structures.This collaboration harnesses the sophisticated linguistic capabilities of LLMs to improve the contextual understanding and adaptability of graph models, thereby broadening the scope and potential of GRL.Despite a growing body of research dedicated to integrating LLMs into the graph domain, a comprehensive review that deeply analyzes the core components and operations within these models is notably lacking.Our survey fills this gap by proposing a novel taxonomy that breaks down these models into primary components and operation techniques from a novel technical perspective.We further dissect recent literature into two primary components including knowledge extractors and organizers, and two operation techniques including integration and training stratigies, shedding light on effective model design and training strategies.Additionally, we identify and explore potential future research avenues in this nascent yet underexplored field, proposing paths for continued progress.</p>
<p>Introduction</p>
<p>The prevalence of graph data has spurred the need for advanced network analysis.A crucial component of this analysis is Graph Representation Learning (GRL).This field primarily focuses on graph embedding [Cai et al., 2018;Perozzi et al., 2014;Grover and Leskovec, 2016], Graph Neural Networks (GNNs) [Wu et al., 2020;Kipf and Welling, 2017;Velickovic et al., 2018], and graph Transformers [Min et al., 2022;Rong et al., 2020], which aim to represent graph elements (e.g., nodes) into low-dimensional vectors.The advances in GRL have profound implications across various domains, reflecting its growing importance in extracting meaningful insights from complex data.</p>
<p>With the advancements in machine learning techniques, the rapidly evolving field of artificial intelligence is opening * Corresponding author new avenues for research, particularly with the rise of Large Language Models (LLMs) [Zhao et al., 2023b].Known for their impressive performance in Natural Language Processing (NLP), LLMs show potential for wide-ranging applications in various areas beyond NLP [Li et al., 2023a;Luo et al., 2022].Their skill in identifying complex patterns in large datasets leads to an important question: Can the powerful capabilities of LLMs be harnessed to significantly enhance graph representation learning?</p>
<p>Advancing GRL with LLMs.The flexibility of LLMs, shown in their advanced ability to understand and generate human language, makes them a key tool for analyzing complex data structures, including graphs.By combining the advanced analysis power of LLMs with graph data, we have a promising chance to bring the text knowledge and wideranging application skills of LLMs into graph representation learning.This combination not only gives graph models a better grasp of context and meaning but also improves their ability to adapt to different situations.This expansion increases the potential and usefulness of GRL in understanding the complexity and connectedness of data in various fields [Chen et al., 2023a;Tang et al., 2023;He et al., 2023].</p>
<p>As a result, the impressive capabilities of Large Language Models (LLMs) have led to a growing number of research efforts focused on integrating LLMs into the graph domain [He et al., 2023;Chen et al., 2023a,b;Ye et al., 2023].This research is advancing a variety of applications in graph-related tasks, including node classification [Chen et al., 2023a;Huang et al., 2023a], graph reasoning [Wang et al., 2023;Guo et al., 2023], molecular graph analysis [Su et al., 2022;Liu et al., 2022], etc.Despite extensive exploration in this area, there is a noticeable lack of systematic reviews summarizing the progress of graph representation learning with LLMs, particularly from a technical standpoint of how to design a graph foundation learning model with the assistance of LLMs.Addressing this gap is crucial as it can deepen our understanding of the current developments in graph learning models enhanced by LLMs and aid in creating more effective future graph foundation models.</p>
<p>Our survey.To bridge this gap and enhance understanding of the recent literature on GRL with LLMs from a technical perspective, this paper systematically reviews and summarizes the technical advancements in GRL with LLMs.Our goal arXiv:2402.05952v1[cs.LG] 4 Feb 2024</p>
<p>Techniques of GRL with LLMs</p>
<p>Primary Components</p>
<p>Operation Techniques</p>
<p>Attribute Extractor</p>
<p>Knowledge Extractor</p>
<p>Structure Extractor</p>
<p>Label Extractor</p>
<p>GNN-centric Organizer</p>
<p>Knowledge Organizer</p>
<p>LLM-centric Organizer</p>
<p>GNN+LLM Organizer</p>
<p>Input-level Integration</p>
<p>Integration Strategies</p>
<p>Hidden-level Integration</p>
<p>Alignment-based Integration Model Pre-training</p>
<p>Training Strategies</p>
<p>Prompt Tuning Instruction Tuning is to enable readers to thoroughly analyze this technique and understand how to design GRL models assisted by LLMs.This, in turn, will contribute to advancing the field of graph representation learning.</p>
<p>Operation Techniques</p>
<p>Input-level Integration</p>
<p>Integration Strategies</p>
<p>Hidden</p>
<p>To structure this survey, we begin by decomposing the techniques of the existing literature on GRL with LLMs into two primary components: knowledge extractors and knowledge organizers, based on their respective roles within the overall model.Specifically, with a focus on graph representation learning, knowledge extractors (Section 3) are concerned with extracting knowledge from graph data (e.g., graph encoders), while knowledge organizers (Section 4) are responsible for arranging and storing this knowledge (e.g., multi-layer transformers).</p>
<p>In addition to these major components in a GRL model enhanced with LLMs, we also provide a comprehensive overview of the existing operation techniques used for model management.This includes integration strategies for combining graph learning models (e.g., GNNs) with LLMs and training strategies for effectively training the unified model.With knowledge extractors and organizers as the foundation, the operation techniques primarily address how to manage and integrate modules by integration strategies (Section 5) to ensure their seamless combination, and how to achieve effective and efficient training by training strategies (Section 6).</p>
<p>The overall taxonomy is depicted in Figure 1, which is divided into two main branches to highlight the primary components and operation techniques.Building on this framework, we delve into each branch in detail, summarizing the existing literature to showcase the advancements in both major components and operation techniques.Specifically, we compare the related literature from the perspective of their configurations in constructing their entire model, providing readers with a deeper understanding of the various structures, and further illustrate the details in Table 1.Additionally, we highlight potential future research directions that merit exploration to further advance GRL with LLMs.</p>
<p>Relationships with existing surveys.The recent focus on LLMs applied to graph data has prompted several research surveys to review the current literature [Jin et al., 2023a;Liu et al., 2023b;Li et al., 2023b;Zhang et al., 2023].While sharing similar investigation directions, these surveys lack a focus on general graph representation learning domain.They predominantly classify the literature based on LLMs' roles (e.g., as predictor, aligner, etc.) and specific application scenarios, overlooking a holistic overview and detailed analysis of graph foundation models' structural framework.Our work sets itself apart by treating GRL with LLMs as an integrated model and meticulously breaking it down from a technical standpoint into several essential components and operations.To achieve this, we introduce a novel taxonomy to structure the literature and delineate the advancements from each perspective.This approach offers a comprehensive and detailed analysis of these models, equipping readers with the knowledge to design their own graph foundation models.</p>
<p>Contributions.To summarize, our contributions are threefold.(1) We provide a comprehensive analysis of research efforts that integrate LLMs into GRL, viewing them as unified models.This includes a detailed breakdown of these models into essential components and operations from a novel technical perspective.(2) We propose a novel taxonomy that categorizes existing models into two main components: knowledge extractors and organizers, according to their functions within the model.This taxonomy also covers operation techniques, including integration and training strategies, to offer readers a deeper understanding of graph foundation models.</p>
<p>(3) Utilizing our taxonomy, we identify and discuss potential future research directions in this emerging yet underexplored field, suggesting avenues for further development.</p>
<p>Organization.The structure of this survey is outlined as follows.Section 2 provides an introduction to the background relevant to this survey.Sections 3 and 4 detail the primary components identified in the existing literature, focusing on knowledge extractors and knowledge organizers, respectively.Sections 5 and 6 discuss the operation techniques highlighted in the literature, specifically addressing integration strategies and training strategies.Section 7 explores potential future research directions in line with our organizational framework.The survey concludes with Section 8.</p>
<p>Background</p>
<p>Graph representation learning and GNNs.Graph representation learning seeks to transform elements of a graph into low-dimensional vector representations, ensuring the preservation of the graph's structure.Currently, the primary models being extensively investigated for advancement in this field by LLMs are GNNs1 .The fundamental process in GNNs involves recursive neighborhood aggregation, a method that compiles information from neighboring nodes to refine and update the representation of a specific node.Formally, let  g (;  g ) denote a GNN architecture parameterized by  g .In the l-th layer, the representation of node v, i.e., h l v  R d l , can be calculated by where N v is the neighbors set of node v and AGGR(;  l g ) is the aggregation function parameterized by  l g in layer l.
h l v = AGGR(h l1 v , {h l1 i : i  N v };  l g ),(1)
Large language models.LLMs are a category of natural language processing models known for their enormous size, often comprising billions of parameters and being trained on extensive datasets [Zhao et al., 2023b].These models represent a significant advancement over earlier, smaller Pretrained Language Models (PLMs) [Gao et al., 2021] in both size and capabilities, covering a wide range of tasks including understanding Zhu et al. [2023] and generating [Madani et al., 2023] for natural language processing.Central to LLMs is the use of the Transformer architecture [Vaswani et al., 2017], which allows for efficient processing of large datasets.This architecture has facilitated the development of both non-autoregressive models like BERT, which focus on understanding through tasks like masked language modeling [Zaken et al., 2022], and autoregressive models like GPT-3, which excel in generation tasks through next-token prediction [Yang et al., 2019].</p>
<p>Knowledge Extractor</p>
<p>The concept of a knowledge extractor involves extracting and encoding essential information from graph data w.r.t.graph learning models or LLMs, ensuring that the resulting representations faithfully reflect the graph's interconnected nature.In graph representation learning, graph data typically encompasses information from three dimensions: graph attributes, graph structures, and label information.Consequently, in this section, we will explore various types of knowledge extractors, examining them through the lenses of graph attributes, structures, and labels.An illustration of the knowledge extractor can be found in Figure 2.</p>
<p>Attribute Extractor</p>
<p>Structure Extractor</p>
<p>Knowledge extraction from graph structures plays a pivotal role in GRL.The structure extractor with LLMs in this arena manifests in two key ways: noisy graph structure refinement based on LLMs and graph structure utilization with LLMs.These approaches introduce innovative solutions for extracting graph structures, enhancing both the utilization and encoding processes of the structure knowledge.</p>
<p>Graph Structure Refinement.Graph Structure Learning (GSL) represents an advancement over traditional GRL, which primarily focuses on concurrently optimizing both the graph structure and node representations, to enhance the overall effectiveness of the representations by improving the quality of the graph structure.In the context of Text-Attributed Graphs (TAGs), where nodes are associated with rich textual information, LLMs offer a unique opportunity to enhance the graph structure from a semantic standpoint.</p>
<p>Based on the observation that TAGs often contain numerous irrational or unreliable edges, Sun et al. [2023] enables the LLM to assess the semantic-level similarity between pairs of nodes in the TAGs through meticulously crafted prompts with textual attributes.ENG [Yu et al., 2023] leverages LLMs for generating effective nodes and corresponding graph structures in TAGs, enhancing GNN models' performance in fewshot scenarios.Both of the aforementioned methods leverage the powerful textual semantic representation capabilities of LLMs to uncover implicit, effective graph structural relationships, thereby enhancing the efficacy of structural information in graph representation learning and boosting GSL.</p>
<p>Graph Structure Utilization.Integrating LLMs into GRL expands the utilization of existing graph structures.This is accomplished by transforming graph structures or contexts into natural language descriptions [Huang et al., 2023a;Guo et al., 2023].Utilizing LLMs' extensive capabilities in representing language inputs allows for effective information extraction, reducing dependency on graph-structured inputs.</p>
<p>Although the input to LLMs does not depend on graph structures, enhancing their performance can be achieved by articulating certain graph structural information through natural language descriptions [Chen et al., 2023a].InstructGLM [Ye et al., 2023] creates templates to describe each node's local ego-graph structure (up to 3-hop connections), while GraphGPT [Tang et al., 2023] introduces special GraphTokens, refined through Graph Instruction Tuning, to convey graph structural information, significantly reducing the length of token sequences required to describe structural information.In addition to converting the original graph information into natural language, GraphText [Zhao et al., 2023a] introduces a syntax tree-based approach to convert graph structure into text sequences.Although these techniques can furnish LLMs with partial graph-related information, the constraint on input length precludes a comprehensive representation of the full graph information.The most effective method for feeding graph information into LLMs remains a subject of ongoing investigation.</p>
<p>Label Extractor</p>
<p>A key obstacle in GRL, especially in real-world applications, is the scarcity of high-quality annotated data.Label information is crucial for these models to efficiently extract knowledge from graph data.The zero-shot capabilities of LLMs, derived from their extensive parameters and vast training, allow for accurate label inference using the textual features of the nodes, providing a practical solution for ongoing data labeling in GRL.</p>
<p>LLM-GNN [Chen et al., 2023b] utilizes GPT-3.5 [Brown et al., 2020] with a hybrid prompt for zero-shot input, annotating selected nodes and using confidence scores to discard low-quality labels.This strategy allows GNNs to perform well in node classification tasks without real labels.And the effectiveness of using LLMs for label annotation has also been validated in [Zhao et al., 2022;Chen et al., 2023a].However, current annotation methods primarily rely on LLMs' ability to understand textual features, without considering the structural relationships between nodes.If structural information could be incorporated into the LLM input, the quality of the labels is likely to be further improved.</p>
<p>Knowledge Organizer</p>
<p>Beyond the quest for more efficient knowledge extraction, the manner in which extracted knowledge is stored and organized is equally crucial.The advent of LLMs has expanded the design space for knowledge organization systems.Current methodologies combining LLMs with GRL have yielded three distinct types of knowledge organizers based on the integration of GNNs and LLM architectures.In the following, we will delve into an in-depth analysis and discussion of the characteristics and technologies underlying these three types of knowledge organizers: GNN-centric, LLM-centric, and GNN+LLM.The illustration of three knowledge organizers is shown in Figure 3.</p>
<p>GNN-centric Organizer</p>
<p>The focus of GNN-centric Organizer is on using structured encoding ability of GNNs for the final organization and refinement of knowledge.In this scenario, LLMs usually act as auxiliary modules to GNNs.They can serve as initializers for node features in GNN inputs [He et al., 2023;Liu et al., 2023a], optimizers for the graph structures used by GNNs [Sun et al., 2023;Yu et al., 2023], or sources of labels for GNN input data [Chen et al., 2023b,a], providing a more comprehensive and sufficient knowledge base for GNN encoding.</p>
<p>In TAPE [He et al., 2023], LLMs are used to generate additional explanatory and predictive features as inputs for the GNN, with the GNN model ultimately producing the final node representations.In OFA [Liu et al., 2023a], LLMs' versatility is harnessed to encode data from different domains and tasks, enabling GNNs to undergo unified training across various domains and tasks.This breaks the limitation of single GNN models being confined to singular application scenarios, paving the way for the development of large-scale foundational models for graphs.</p>
<p>The improvements and possibilities that LLMs bring to GRL have not been fully explored.Utilizing LLMs as plugand-play auxiliary modules to enhance GNN models remains a promising and significant direction with a bright future.</p>
<p>LLM-centric Organizer</p>
<p>Recent advancements in GRL have witnessed a growing trend of utilizing LLMs as the core and sole backbone.This paradigm shift is attributed to several key advantages that LLMs offer in seamlessly integrating textual information with graph data and unifying diverse graph learning tasks under a natural language-based framework.Applying LLMs as knowledge organizer to graph modalities involves unique challenges, primarily due to the complexity of transforming graph data into a sequential text format.Chen et al. [2023a] developed a method to extract contextual information about a target node using an LLM by con-structing the neighbor summary prompt.The output text from this process is then used as a prompt to assist the LLM in tasks related to GRL.Structural information is depicted either through the textual features of adjacent nodes or through linguistic descriptions of connectivity relationships.Additionally, the Graph Modeling Language and Graph Markup Language are used to describe graph structure in GPT4Graph [Guo et al., 2023].Furthermore, a method based on tree structures for converting graphs to texts has been proposed in GraphText [Zhao et al., 2023a].</p>
<p>Nonetheless, employing LLM-central architecture in GRL presents significant challenges.A primary limitation is the intricacy involved in translating complex graph structures into a format amenable to LLM processing.Moreover, inherent limitations of LLMs, such as their difficulty in managing long-range dependencies and the potential biases embedded within their training data, become pronounced in the context of graph learning.These limitations can affect the model's ability to accurately interpret and utilize the graph-structured information, posing hurdles to effective application in graphbased tasks.</p>
<p>GNN+LLM Organizer</p>
<p>The GNN+LLM Organizer marks a significant advancement in overcoming the limitations of each individual organizer [Brannon et al., 2023;Zhao et al., 2022;Wen and Fang, 2023].GNN-based models, while adept at structural analysis, fall short in processing textual data and interpreting natural language instructions.Conversely, LLM-based models, despite their linguistic prowess, struggle with precise mathematical calculations and multi-hop logical reasoning.This complementary nature of GNNs and LLMs forms the basis for a hybrid model that leverages the strengths of both: the language understanding capabilities of LLMs and the structural analysis proficiency of GNNs.</p>
<p>How to fully leverage the strengths and compensate for the weaknesses of both modalities, effectively integrating knowledge from these two distinct modalities, remains a key challenge for such methods.This will be analyzed in more detail in the next section, focusing on specific knowledge integration strategies between graph and language domains.</p>
<p>Integration Strategies</p>
<p>Incorporating LLMs into model designs enables the generation of text modality representations for graph data, which effectively complement the structural representations derived from GNNs.For a holistic representation, it is crucial to merge semantic and structural knowledge.Centering on the forms of integration between modal knowledge and representations, we have categorized the current integration strategies into three classes: input-level integration at the input stage, hidden-level integration at the latent processing stage, and indirect integration through alignment, as depicted in Figure 4.</p>
<p>Input-level integration</p>
<p>Input-level integration typically employs a unified model structure to amalgamate graph structure and textual information at the input stage.This integration is formatted to align with the model's input demands, enabling a seamless integration of data from both modalities.Models built upon the Transformer architecture of LLMs typically convert graph structures into natural language narratives, seamlessly blending them with textual data.Conversely, models that pivot on GNNs tend to assimilate textual information by forming virtual nodes that maintain specified structural relations, thereby enabling the GNNs to adeptly manage the synthesis and encoding of structure.Chen et al. [2023a] considers first aggregating the textual information of nodes within a neighborhood and then integrating this summarized description into the textual input using a prompt format.InstructGLM [Ye et al., 2023] integrates diverse hop-level contextual node information with a node sampling strategy into the input of the LLM model.Additionally, OFA [Liu et al., 2023a], using GNNs as the knowledge organizer, incorporates textual information into the graph data in the form of prompting virtual nodes.It initializes these virtual nodes with additional task description text, providing not only extra semantic information but also enabling cross-domain multi-task model training integration.</p>
<p>Hidden-level Integration</p>
<p>Hidden-level integration refers to merging the textual semantic representations encoded by LLMs with the graph information representations encoded by GNNs to create a comprehensive representation that fully expresses both semantic and structural information of nodes.TAPE [He et al., 2023] employs original textual features, explanatory textual features, and predictive textual features as inputs for GNNs.It uses a straightforward ensemble approach to fuse the predictive representations of three distinct semantic features encoded by three independent GNNs, which capture complementary information from diverse input sources.Additionally, cascading [Chandra et al., 2020] and concatenation [Edwards et al., 2021] are commonly used hidden-level integration strategies in previous works to enhance the overall model's ability to capture and integrate diverse aspects of graph data.</p>
<p>Given that current applications of LLM models tend to be quite direct, more sophisticated and effective methods for the integration of textual and graph representations have yet to be explored.How to comprehensively utilize the representations from both modalities to generate higher-order representations with greater expressive capabilities remains an important question to address.</p>
<p>Alignment-based Integration</p>
<p>Beyond merely integrating original input data and hidden layers, another approach, i.e., alignment, considers the features of GNNs and LLMs as distinct manifestations of the same entity's knowledge in graph and textual modalities, respectively.By aligning representations across these two modalities, knowledge can be transferred between them, facilitating an indirect form of integration.The objective of alignmentbased integration lies in maintaining the distinct functionalities of each modality while synchronizing their embedding spaces at a particular stage.The intent is to forge a unified, holistic representation that encapsulates the collective advantages of textual and structural information.</p>
<p>Alignment-based knowledge integration includes three main categories: contrastive alignment, iterative alignment and distillation alignment.Contrastive alignment [Brannon et al., 2023] involves treating the graphical representation and textual representation of the same node as positive examples to conduct contrastive learning for knowledge integration.And G2P2 [Wen and Fang, 2023] introduces contrastive learning at multiple levels during pre-training, including node-text, text-summary, and node-summary, which reinforces the alignment between textual and graph representations.For iterative alignment, iterative interaction between the modalities is allowed in the training process for knowledge transferring.For example, GLEM [Zhao et al., 2022] introduces a novel pseudo-likelihood variational framework to the iterative training process, where the E-step involves optimizing the LLM, and the M-step focuses on training the GNN.Additionally, GRAD [Mavromatis et al., 2023] implements a distillation alignment approach for aligning dual modalities.Specifically, it employs a GNN as a teacher model to generate soft labels for an LLM, thereby facilitating the transfer of aggregated information.However, efficiency issues arising from the training of LLMs have limited the application of alignment fusion in real-world scenarios.</p>
<p>Training Strategies</p>
<p>The knowledge of LLMs is derived from training on massive natural language corpora, yet there exists a gap between this knowledge and that required in the graph learning domain, where capturing structural relationships on graphs is crucial.To bridge this gap, training strategies are designed to enhance LLMs' adaptability to graph data from a model training perspective, which can be categorized into three types: model pre-training, prompt-based training and instruction tuning.</p>
<p>Model Pre-training</p>
<p>The core idea of graph pertaining [Hu et al., 2019] is to train a model on a substantial dataset to capture general patterns or knowledge, which can then be tailored for specific downstream tasks.In the realm of GRL, it focuses on extract-ing inherent structural patterns within graph data, paralleling the way language models learn the syntax and semantics of languages.Graph Pre-training methodologies are diverse, ranging from contrastive [You et al., 2020] to predictive/generative [Hu et al., 2020] approaches, each leveraging the structural and semantic richness of graph data.When incorporating LLMs into GRL, pre-training can also include textual knowledge from language models, and textual pretraining tasks like network-contextualized masked language modeling [Jin et al., 2023b] and context graph prediction [Zou et al., 2023].Most current methods integrating LLMs treat them as plug-and-play modules, with relatively few studies focusing on injecting graph structure knowledge into LLMs during pre-training.InstructGLM [Ye et al., 2023] pioneered the use of self-supervised link prediction task as the auxiliary training task for LLMs to comprehend graph structural knowledge.The significant performance indicates that considering a pre-training paradigm that blends GNNs and LLMs integration is both necessary and feasible.</p>
<p>Prompt-based Training</p>
<p>Prompt-based training comprises Prompting and Prompt Tuning [Liu et al., 2023c].The former guides language models to produce specific outputs, while the latter focuses on aligning downstream tasks with pre-training tasks.The introduction of LLMs has led to the widespread use of prompting to enhance models' understanding of graph structural information.Neighborhood or connection information is added as prompting to improve LLMs' adaptability to GRL [Yu et al., 2023;Chen et al., 2023b], or to activate their few/zero-shot capabilities [Guo et al., 2023;Huang et al., 2023a].</p>
<p>For graph tasks, the concept of graph prompts has been explored extensively, aiming at the integration of diverse graph tasks.For example, GPPT [Sun et al., 2022] reconceptualizes graph tasks as edge prediction problems, while Graph-Prompt [Liu et al., 2023d] extends this framework by unifying tasks as subgraph similarity calculations.While OFA [Liu et al., 2023a] employs both prompting and graph prompt tuning.Prompting are used for feature dimension alignment and initializations of nodes across different datasets, and graph prompt tuning is used to unify different tasks, enabling a single model to be trained and evaluated across various datasets and tasks.In the field of GRL with LLMs, where large-scale data is often scarce, prompt-based training remains a vital technique.</p>
<p>Instruction Tuning</p>
<p>The core methodology of Instruction Tuning involves integrating the pre-trained models' input data with task-specific instructional prompts.Instruction Tuning is executed within a multi-prompt training framework, where the model is exposed to various task instructions, aiding in its understanding and response to diverse task requirements.</p>
<p>In the realm of GRL, where annotated data is relatively limited and downstream tasks often span multiple domains and objectives, Instruction Tuning is particularly valuable for enhancing model performance in few-shot and zero-shot scenarios.This efficient fine-tuning approach can effectively tap into the inherent knowledge related to GRL within LLMs, thereby enhancing their comprehension abilities for graphrelated tasks.InstructGLM [Ye et al., 2023] directs the LLM to generate responses for various graph learning tasks within a unified language modeling framework.GraphGPT [Tang et al., 2023] proposes a two-stage instruction tuning approach for graph learning.Initially, it uses self-supervised tasks to teach the LLM graph structure knowledge.Then, it applies task-specific tuning to improve the LLM's performance on downstream tasks, aligning it with graph domain knowledge and specific task requirements.As research on GRL with LLMs progresses, instruction tuning will play an increasingly crucial role in fine-tuning LLMs.</p>
<p>Future Directions</p>
<p>In this section, we explore potential avenues for future research of advancing GRL w.r.t.our organization framework.</p>
<p>Generalization of knowledge extractor.Current progress indicates using LLMs for text-attributed graphs has been promising, but challenges arise with graph data lacking rich text.Adapting LLMs to interpret non-textual graph data is crucial for progress in GRL, especially considering the ubiquity of non-textual graphs in real-world scenarios.</p>
<p>Effectiveness of knowledge organizer.The development of LLMs has led to the coexistence of three distinct architectures in the realm of Graph Foundation Models: GNNs, Graph Transformers, and LLMs.However, there is no consensus on how to design an effective knowledge organizer for GRL, and a unified theoretical framework to analyze the strengths and weaknesses of these various architectures is lacking.</p>
<p>Transferability of integration strategies.Transferability in graph learning is challenging due to the unique characteristics of each graph, such as size, connectivity, and topology, which makes it difficult to apply knowledge across different graphs.However, integrating with the exceptional generalization capabilities of LLMs offers potential solutions to the challenges of transferability.Advancing transferability requires not just subtle integration strategies but also a deeper understanding of knowledge transfer in graph domain.</p>
<p>Adaptability of training strategies.Furthermore, there is a scarcity of research on the pre-training and fine-tuning techniques of LLMs on graph data.Effective training strategies need to consider LLMs' methods and structures and how to integrate graph information, posing a significant challenge in adapting LLMs to graph learning efficiently.</p>
<p>Conclusions</p>
<p>In this survey, we systematically reviewed and summarized the technical advancements in GRL with LLMs.We provided a comprehensive analysis of the essential components and operations of these models from a technical perspective.We then proposed a novel taxonomy that categorizes literature into two main components: knowledge extractors and organizers and covers operation techniques of integration and training strategies.We further identified and discussed potential future research directions to suggest avenues for further development.</p>
<p>Figure 1 :
1
Figure 1: The techniques of GRL with LLMs.</p>
<p>Figure 2 :
2
Figure 2: The illustration of graph knowledge extractors on attribute, structure and label information with LLMs.</p>
<p>Figure 3 :
3
Figure 3: The illustration of different knowledge organizers: GNNcentric, LLM-centric, and GNN+LLM organizers.</p>
<p>Figure 4 :
4
Figure 4: The illustration of different knowledge integration strategies.</p>
<p>Table 1 :
1
The technique summarization of existing most-related literature on graph representation learning with LLMs.
ApproachesAttribute ExtractorKnowledge Extractor Structure ExtractorLabel ExtractorKnowledge OrganizerIntegration StrategiesTraining StrategiesTasksTAPE [He et al., 2023]Feature, TextoriginalGNNhidden-levelPromptingNodeChen et al. [Chen et al., 2023a]Feature, Textno graph, subgraph, originalGNN/LLMinput-levelPromptingNodeConGraT [Brannon et al., 2023]FeatureoriginalGNN+LLM alignment-basedPretrainingNode, LinkGraphGPT [Tang et al., 2023]Featuresubgraph,originalLLMalignment-based Pretraining, Prompting, Instruction TuningNodeG-Prompt [Huang et al., 2023b]FeaturesubgraphLLMinput-levelPrompt TuningNodeENG [Yu et al., 2023]Featurestructure refinementGNNinput-levelPromptingNodeSun et al. [Sun et al., 2023]Featurestructure refinementGNN-PromptingNodeGraphText [Zhao et al., 2023a]Featurestructure refinementLLMinput-levelPromptingNodeGLEM [Zhao et al., 2022]FeatureoriginalGNN+LLM alignment-based-NodeLLM-GNN [Chen et al., 2023b]FeatureoriginalGNN--NodeGRAD [Mavromatis et al., 2023]FeatureoriginalLLMalignment-based-NodeG2P2 [Wen and Fang, 2023]FeatureoriginalGNN+LLM alignment-basedPrompt TuningNodePatton [Jin et al., 2023b]FeatureoriginalGNN+LLMhidden-levelPretrainingNode, LinkGALM [Xie et al., 2023]FeatureoriginalGNN-PretrainingNode, LinkSimTeG [Duan et al., 2023]FeatureoriginalGNN-PretrainingNode, LinkOFA [Liu et al., 2023a]FeatureoriginalGNNinput-levelPrompt TuningNode, Link, GraphInstructGLM [Ye et al., 2023]-subgraphLLMinput-levelPretraining, Prompting, Instruction TuningNodeGPT4Graph [Guo et al., 2023]-no graph, subgraphLLMinput-level-Node, GraphHu et al. [Hu et al., 2023]-no graph, subgraphLLMinput-levelPromptingNode, Link, GraphHuang et al. [Huang et al., 2023a]-no graphLLMinput-levelPromptingNode</p>
<p>[Duan et al., 2023;Xie et al., 2023]oding text semantic information into initial node features for GNNs.Additionally,Chen et al. [2023a]employs Knowledge-Enhanced Attention to prompt LLMs to generate relevant knowledge entities and textual descriptions, thereby enhancing the richness of textual information.These two works demonstrate that when graph data contains textual attributes, LLMs can effectively extract the textual information using their extensive knowledge repository and generative capabilities.Feature-level Extractor.The Feature-level Extractor leverages LLMs to encode the textual representations of nodes in GRL, moving beyond the traditional word embedding methods like Word2Vec[Mikolov et al., 2013].By utilizing the advanced capabilities of LLMs for more nuanced and context-aware feature encoding, the effectiveness of graph representation can be significantly enhanced.Due to its direct effectiveness, the feature-level extractor has been adopted by multiple studies[Duan et al., 2023;Xie et al., 2023].Using the feature-level extractor, OFA[Liu et al., 2023a] converts all nodes, edges and task information into human-readable texts and uniformly encodes them across various domains.It proposes a novel GRL framework capable of utilizing a singular graph model for solving diverse tasks from crossdomain datasets with the help of LLMs.
Attribute Extractor focus on extracting and enhancing pat-terns from node and edge attributes, usually leveraging LLMsto interpret and enrich the textual or numerical data associ-ated with graph components. This involves using LLMs togenerate more complete textual statements and encode morecomprehensive semantic features, corresponding to the Text-level Extractor and Feature-level Extractor, respectively.Text-level Extractor. Textual-level Extractor aims to lever-age the generative prowess of LLMs to augment the incom-plete textual attributes. This method encompasses generatingextensive descriptive or explanatory text, thus amplifying the
[He et al., 2023] of the original graph data from a textual standpoint.The resultant feature set is imbued with richer semantic depth.For example, TAPE[He et al., 2023]uses prompt to manipulate LLM to generate additional explana-tory texts,</p>
<p>Note that, since GNNs are the main models extensively investigated for enhancement in this field with the aid of LLMs, in this survey, "graph learning models" specifically refer to GNNs.</p>
<p>Congrat: Selfsupervised contrastive pretraining for joint graph and text embeddings. William Brannon, Suyash Fulay, Hang Jiang, Wonjune Kang, Brandon Roy, Jad Kabbara, Deb Roy, arXiv:2305.143212023arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, NeurIPS. 332020</p>
<p>A comprehensive survey of graph embedding: Problems, techniques, and applications. TKDE. Hongyun Cai, Vincent W Zheng, Kevin Chen, -Chuan Chang, 201830</p>
<p>Graph-based modeling of online communities for fake news detection. Shantanu Chandra, Pushkar Mishra, Helen Yannakoudakis, Madhav Nimishakavi, Marzieh Saeidi, Ekaterina Shutova, arXiv:2008.062742020arXiv preprint</p>
<p>Exploring the potential of large language models (llms) in learning on graphs. Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, arXiv:2307.033932023arXiv preprint</p>
<p>Label-free node classification on graphs with large language models (llms). Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang Zhang, Hui Liu, Jiliang Tang, arXiv:2310.046682023arXiv preprint</p>
<p>Qian Keyu Duan, Tat-Seng Liu, Shuicheng Chua, Wei Tsang Yan, Qizhe Ooi, Junxian Xie, He, arXiv:2308.02565Simteg: A frustratingly simple approach improves textual graph learning. 2023arXiv preprint</p>
<p>Text2mol: Cross-modal molecule retrieval with natural language queries. Carl Edwards, Chengxiang Zhai, Heng Ji, EMNLP. 2021</p>
<p>Making pretrained language models better few-shot learners. Tianyu Gao, Adam Fisch, Danqi Chen, ACL-IJCNLP. 2021</p>
<p>node2vec: Scalable feature learning for networks. Aditya Grover, Jure Leskovec, SIGKDD. 2016</p>
<p>Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. Jiayan Guo, Lun Du, Hengyu Liu, arXiv:2305.150662023arXiv preprint</p>
<p>Explanations as features: Llm-based features for text-attributed graphs. Xiaoxin He, Xavier Bresson, Thomas Laurent, Bryan Hooi, arXiv:2305.195232023arXiv preprint</p>
<p>Strategies for pre-training graph neural networks. Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, Jure Leskovec, 2019In ICLR</p>
<p>Gpt-gnn: Generative pre-training of graph neural networks. Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, Yizhou Sun, KDD. 2020</p>
<p>Beyond text: A deep dive into large language models' ability on understanding graph data. Yuntong Hu, Zheng Zhang, Liang Zhao, arXiv:2310.049442023arXiv preprint</p>
<p>Can llms effectively leverage graph structural information: when and why. Jin Huang, Xingjian Zhang, Qiaozhu Mei, Jiaqi Ma, arXiv:2309.165952023arXiv preprint</p>
<p>Promptbased node feature extractor for few-shot learning on textattributed graphs. Xuanwen Huang, Kaiqiao Han, Dezheng Bao, Quanjin Tao, Zhisheng Zhang, Yang Yang, Qi Zhu, arXiv:2309.028482023arXiv preprint</p>
<p>Large language models on graphs: A comprehensive survey. Gang Bowen Jin, Chi Liu, Meng Han, Heng Jiang, Jiawei Ji, Han, arXiv:2312.027832023arXiv preprint</p>
<p>Wentao Bowen Jin, Yu Zhang, Yu Zhang, Xinyang Meng, Qi Zhang, Jiawei Zhu, Han, Patton, arXiv:2305.12268Language model pretraining on text-rich networks. 2023arXiv preprint</p>
<p>Semi-supervised classification with graph convolutional networks. Thomas N Kipf, Max Welling, ICLR. 2017</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, arXiv:2301.125972023arXiv preprint</p>
<p>Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, Jeffrey Xu, Yu , arXiv:2311.12399A survey of graph meets large language model: Progress and future directions. 2023arXiv preprint</p>
<p>Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei Xiao, Anima Anandkumar, arXiv:2212.10789Multi-modal molecule structure-text model for text-based retrieval and editing. 2022arXiv preprint</p>
<p>One for all: Towards training one graph model for all classification tasks. Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen, Muhan Zhang, arXiv:2310.001492023arXiv preprint</p>
<p>Towards graph foundation models: A survey and beyond. Jiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Mengmei Zhang, Ting Bai, Yuan Fang, Lichao Sun, Philip S Yu, arXiv:2310.118292023arXiv preprint</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ACM Computing Surveys. 5592023</p>
<p>Graphprompt: Unifying pre-training and downstream tasks for graph neural networks. Zemin Liu, Xingtong Yu, Yuan Fang, Xinming Zhang, WWW. 2023</p>
<p>Biogpt: generative pretrained transformer for biomedical text generation and mining. Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, Tie-Yan Liu, Briefings in Bioinformatics. 2364092022</p>
<p>Large language models generate functional protein sequences across diverse families. Ali Madani, Ben Krause, Eric R Greene, Subu Subramanian, Benjamin P Mohr, James M Holton, Jose Luis OlmosJr, Caiming Xiong, Zachary Z Sun, Richard Socher, Nature Biotechnology. 2023</p>
<p>Costas Mavromatis, N Vassilis, Shen Ioannidis, Da Wang, Soji Zheng, Jun Adeshina, Han Ma, Zhao, arXiv:2304.10668Christos Faloutsos, and George Karypis. Train your own gnn teacher: Graph-aware distillation on textual graphs. 2023arXiv preprint</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean, arXiv:1301.3781Efficient estimation of word representations in vector space. 2013arXiv preprint</p>
<p>Transformer for graphs: An overview from architecture perspective. Erxue Min, Runfa Chen, Yatao Bian, Tingyang Xu, Kangfei Zhao, Wenbing Huang, Peilin Zhao, Junzhou Huang, Sophia Ananiadou, Yu Rong, arXiv:2202.084552022arXiv preprint</p>
<p>Deepwalk: Online learning of social representations. Bryan Perozzi, Rami Al-Rfou, Steven Skiena, SIGKDD. 2014</p>
<p>Self-supervised graph transformer on large-scale molecular data. Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, Junzhou Huang, NeurIPS. 332020</p>
<p>Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Hao Sun, Zhiwu Lu, Ji-Rong Wen, arXiv:2209.05481A molecular multimodal foundation model associating molecule graphs with natural language. 2022arXiv preprint</p>
<p>Gppt: Graph pre-training and prompt tuning to generalize graph neural networks. Mingchen Sun, Kaixiong Zhou, Xin He, Ying Wang, Xin Wang, SIGKDD. 2022</p>
<p>Large language models as topological structure enhancers for text-attributed graphs. Shengyin Sun, Yuxiang Ren, Chen Ma, Xuecang Zhang, arXiv:2311.143242023arXiv preprint</p>
<p>Graphgpt: Graph instruction tuning for large language models. Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, Chao Huang, arXiv:2310.130232023arXiv preprint</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser, Illia Polosukhin, NeurIPS. 302017</p>
<p>Graph attention networks. Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, stat. 105042018</p>
<p>Can language models solve graph problems in natural language?. Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, Yulia Tsvetkov, arXiv:2305.10037arXiv:2307.10230Zhihao Wen and Yuan Fang. Prompt tuning on graphaugmented low-resource text classification. 2023. 2023arXiv preprint</p>
<p>A comprehensive survey on graph neural networks. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, Philip Yu, TNNLS. 3212020</p>
<p>Graph-aware language model pre-training on a large graph corpus can help multiple graph applications. Han Xie, Da Zheng, Jun Ma, Houyu Zhang, Xiang Vassilis N Ioannidis, Qing Song, Sheng Ping, Carl Wang, Yi Yang, Xu, arXiv:2306.025922023arXiv preprint</p>
<p>Xlnet: Generalized autoregressive pretraining for language understanding. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, Quoc V Le, NeurIPS. 322019</p>
<p>Natural language is all a graph needs. Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, Yongfeng Zhang, arXiv:2308.071342023arXiv preprint</p>
<p>Graph contrastive learning with augmentations. Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, Yang Shen, NeurIPS. 2020</p>
<p>Empower text-attributed graphs learning with large language models (llms). Jianxiang Yu, Yuxiang Ren, Chenghua Gong, Jiaqi Tan, Xiang Li, Xuecang Zhang, arXiv:2310.098722023arXiv preprint</p>
<p>Bitfit: Simple parameter-efficient fine-tuning for transformerbased masked language-models. Elad Ben Zaken, Yoav Goldberg, Shauli Ravfogel, ACL. 2022</p>
<p>Large graph models: A perspective. Ziwei Zhang, Haoyang Li, Zeyang Zhang, Yijian Qin, Xin Wang, Wenwu Zhu, arXiv:2308.145222023arXiv preprint</p>
<p>Learning on large-scale textattributed graphs via variational inference. Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, Jian Tang, arXiv:2210.147092022arXiv preprint</p>
<p>Graphtext: Graph reasoning in text space. Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein, Zhaocheng Zhu, Jian Tang, arXiv:2310.010892023arXiv preprint</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.10592Minigpt-4: Enhancing vision-language understanding with advanced large language models. 2023arXiv preprint</p>
<p>Pretraining language models with text-attributed heterogeneous graphs. Tao Zou, Le Yu, Yifei Huang, Leilei Sun, Bowen Du, arXiv:2310.125802023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
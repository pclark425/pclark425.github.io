<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2673 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2673</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2673</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-68.html">extraction-schema-68</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <p><strong>Paper ID:</strong> paper-99832586d55f540f603637e458a292406a0ed75d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/99832586d55f540f603637e458a292406a0ed75d" target="_blank">ReAct: Synergizing Reasoning and Acting in Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> The use of LLMs are explored to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources to gather additional information.</p>
                <p><strong>Paper Abstract:</strong> While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2673.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2673.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (Reasoning + Acting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting paradigm that interleaves natural-language reasoning traces ('thoughts') with environment-directed actions (search/lookup/finish) in a single augmented action space, enabling LLMs to both plan and fetch external evidence to ground multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ReAct augments an LLM's action space with a language/thought channel and explicit domain actions, producing interleaved thought → action → observation steps. Implemented with few-shot prompts to PaLM-540B (and tested with GPT-3), ReAct uses human-authored example trajectories (thoughts + actions + observations) in-context; it interfaces with simple external APIs (Wikipedia search/lookup) to retrieve evidence, and can be fine-tuned (bootstrapped dataset of 3,000 ReAct trajectories) for smaller models. Key components: (1) augmented action space A ∪ L (actions + language thoughts), (2) few-shot example trajectories in prompt, (3) external API for retrieval (e.g., Wikipedia), and (4) optional finetuning on model-generated correct trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based; retrieval-augmented when interacting with external APIs</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Knowledge-intensive question answering, fact verification, and interactive decision-making (general knowledge / NLP tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates hypothesis-like intermediate assertions as free-form natural-language 'thoughts' interleaved with actions; hypotheses arise as decompositions of the goal, candidate facts to check, commonsense inferences, or plans for actions (e.g., what to search or which subgoal to pursue).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility is assessed by grounding thoughts via explicit external retrieval (search/lookup) and incorporating retrieved observations into subsequent thoughts; heuristics (e.g., backoff to CoT-SC) are used when ReAct fails to produce an answer.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Indirect: downstream task metrics (HotpotQA exact match, FEVER accuracy) and human-labeled breakdowns of success/failure modes (true positives, false positives, hallucination, reasoning error, search result error).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Empirical evaluation on benchmarks (HotpotQA, FEVER, ALFWorld, WebShop), human annotation of trajectory success/failure modes, and ablations (Act-only, CoT baselines). When fine-tuned, validated by held-out benchmark performance.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Prompts included in Appendix C, additional GPT-3 experiments in Appendix A.1, and code link provided (https://react-lm.github.io/). Finetuning details (datasets, steps, batch size) reported in Appendix B.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Grounding thoughts with external retrieval (Wikipedia API) to replace or check internal model assertions; human-in-the-loop thought editing demonstrated for correction.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Human annotation of trajectories to identify hallucinated thoughts/facts; detection also implicit via failed or noninformative search (tracked as error modes).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td>From a manual sample on HotpotQA: among sampled trajectories, ReAct had 6% false positive rate in 'success' class and 0% 'hallucination' failure rate vs CoT (14% and 56% respectively); noninformative search caused 23% of ReAct error cases (human-labelled analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Indirect: uses combination heuristics with CoT-SC (backoff strategies) to handle low-confidence situations; no explicit probabilistic uncertainty output described.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>HotpotQA (multi-hop QA), FEVER (fact verification), ALFWorld (text-based interactive tasks), WebShop (web interaction shopping)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>HotpotQA EM: ReAct 27.4 (PaLM-540B prompting); FEVER Acc: ReAct 60.9; ReAct→CoT-SC and CoT-SC→ReAct combinations improve to 35.1 EM (HotpotQA) and 64.6 Acc (FEVER) respectively. ALFWorld best ReAct trial success rate 71% (avg 57%), vs Act best 45% and BUTLER 37%. WebShop score 66.6 and success rate 40.0 vs Act 62.3/30.1 and IL 59.9/29.1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>ReAct consistently outperforms Act-only prompts and imitation / IL baselines on decision-making tasks; compared to CoT, ReAct reduces hallucination and improves FEVER accuracy but slightly underperforms on some HotpotQA prompting runs (CoT 29.4 EM vs ReAct 27.4 EM). Fine-tuned ReAct (PaLM-8B/62B) outperforms larger prompting-only baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Dependence on quality of external search results (noninformative search causes errors); higher reasoning error rate (e.g., repetitive loops) versus CoT; limited by in-context example length for complex tasks; current action API is simplified (weaker than state-of-the-art retrievers); limited automatic uncertainty quantification and no explicit novelty assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2673.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2673.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique where LLMs are induced to produce intermediate multi-step natural-language reasoning traces (rationales) before producing final answers to improve multi-step problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Chain-of-Thought prompting (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>CoT uses few-shot prompting that includes worked-out reasoning chains so the LLM produces internal multi-step rationales before answering. Implemented in this paper as a reasoning-only baseline: thoughts are present but actions/observations are removed. Typically decoded greedily or with sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based prompting (reasoning-only)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General multi-step reasoning tasks and knowledge-intensive QA (here used on HotpotQA and FEVER)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates intermediate natural-language rationales (which can be interpreted as hypotheses or subclaims) via prompted chain-of-thought examples.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Downstream task performance (HotpotQA EM, FEVER accuracy) and human-labeled trajectory success/failure breakdowns (e.g., hallucination rates, reasoning errors).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Benchmark evaluation (HotpotQA, FEVER) and human analysis of success/failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Prompts and in-context examples provided in appendix; CoT experimental settings reported.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Human annotation revealed high hallucination rates; CoT does not have built-in grounding to prevent hallucination in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td>Human-labelled sample on HotpotQA: CoT had a 14% false positive rate among successes and 56% of failure cases were due to hallucination (per Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>HotpotQA, FEVER (as used in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>HotpotQA EM (PaLM-540B CoT): 29.4; FEVER Acc: 56.3. CoT exhibits higher false-positive/hallucination rates than ReAct in manual analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>CoT outperforms Standard prompting on some reasoning tasks but suffers from hallucination and error propagation; combining with self-consistency (CoT-SC) improves results.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Prone to hallucinated facts and error propagation across multi-step rationales; not grounded to external knowledge sources by default.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2673.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2673.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought with Self-Consistency (CoT-SC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble/self-consistency technique that samples multiple chain-of-thought reasoning trajectories from an LLM and aggregates answers (e.g., by majority vote) to improve robustness and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CoT-SC (Self-Consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>CoT-SC samples multiple CoT rationales (21 samples in this paper, decoding temperature 0.7) and selects the final answer by majority vote among sampled answers. Used both standalone and in combination with ReAct as a fallback/backoff mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based ensemble / sampling method</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General reasoning and QA (used on HotpotQA and FEVER here)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates many alternative chain-of-thought hypotheses via stochastic decoding; hypotheses are represented as sampled reasoning traces and final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility is approximated by frequency (majority) across sampled reasoning traces — higher agreement indicates higher confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Improved answer accuracy via majority voting; trade-off measured by number of samples and accuracy (empirical improvement shown with up to 21 samples).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Aggregates multiple stochastic outputs and uses majority answer as validation; used as backoff in hybrid strategies (CoT-SC→ReAct or ReAct→CoT-SC) when confidence is low or ReAct fails.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Sampling counts, temperature, and prompting details provided in paper and appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Reduces hallucination by averaging over multiple stochastic chains, thereby preferring commonly produced answers.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Low agreement across samples triggers fallback to ReAct; disagreement acts as a signal of low confidence/possible hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Uses ensemble agreement (majority frequency among N samples) as a proxy for confidence/uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>HotpotQA, FEVER</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>CoT-SC improves CoT performance (e.g., empirically boosts HotpotQA/FEVER results vs single CoT runs); combining CoT-SC and ReAct yields best prompting performance in some settings (e.g., ReAct→CoT-SC and CoT-SC→ReAct combinations reached up to 35.1 EM on HotpotQA and 64.6 Acc on FEVER).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>CoT-SC outperforms single CoT via self-consistency sampling; in hybrid pipelines, it complements ReAct to balance internal reasoning and external grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Computationally expensive due to many samples; majority voting is a simple heuristic and may not capture nuanced confidence or correct minority answers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2673.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2673.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Act</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Act-only prompting (Act)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting baseline that removes internal thoughts and only prompts LLMs to generate domain-specific actions to interact with an environment (e.g., search/lookup/finish), resembling an acting-only policy without explicit reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Act-only prompting</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Act prompts are constructed from the same human-authored trajectories as ReAct but with thoughts removed. The LLM emits only environment actions (search/lookup/choose/buy/etc.) and receives observations; no free-form internal reasoning is output in the trajectory.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based action generation</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Interactive decision-making and web-augmented QA (ALFWorld, WebShop, HotpotQA/FEVER with API)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Does not explicitly generate hypotheses; decisions are made implicitly via action tokens produced by the model.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Evaluated empirically on benchmarks and compared against ReAct and imitation learning baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Prompts and prompt construction described in appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Manual trajectory inspection; act-only models may produce hallucinating actions (e.g., assuming items present where they are not).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>ALFWorld, WebShop, HotpotQA, FEVER (as baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>HotpotQA: Act 25.7 EM (PaLM-540B prompting); FEVER: Act 58.9 Acc. ALFWorld average success 45% (best of 6 = 45). WebShop score 62.3 and SR 30.1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Act performs worse than ReAct in most decision-making tasks (e.g., ALFWorld, WebShop), showing the value of sparse internal reasoning for long-horizon planning.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Lacks explicit internal reasoning traces, leading to failures to decompose goals, track subgoals, or avoid hallucinating actions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2673.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2673.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct-IM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct-IM (Inner Monologue-style ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation of ReAct where thoughts are restricted to dense external-feedback style observations (an Inner Monologue-like pattern) limiting internal goal decomposition and reliance on pretrained commonsense knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ReAct-IM (IM-style ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ReAct-IM re-annotates ReAct trajectories with dense external-feedback thoughts that focus on decomposing current goals and the current subgoal, but lack higher-level reasoning such as when subgoals are complete, next-subgoal selection, or induction of where items likely are. Used to probe importance of internal reasoning versus reaction to observations.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based prompting / ablation</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Interactive decision-making (ALFWorld)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Limited generation of short observational thoughts; does not produce broader hypotheses about environment structure.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Empirical evaluation on ALFWorld comparing performance with full ReAct; qualitative analysis of failures where ReAct-IM lacks high-level decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Appendix B.2 and prompt examples in Appendix C.4.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>ALFWorld (ablation study)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>ReAct overall best-of-6 = 71% success on ALFWorld; ReAct-IM best-of-6 = 53% (paper reports ReAct 71 vs ReAct-IM 53 overall), showing large drop when limiting internal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>ReAct significantly outperforms ReAct-IM, indicating the value of flexible internal reasoning beyond dense external feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Demonstrates that restricting thoughts to external-feedback prevents use of commonsense internal priors and goal decomposition, degrading performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2673.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2673.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WebGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WebGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that uses language models to interact with web browsers (search, follow links, read pages) combined with human feedback to produce grounded answers to open-domain questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>WebGPT: Browser-assisted question-answering with human feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>WebGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>WebGPT uses an LM to navigate web pages, follow search results, and synthesize answers grounded in retrieved webpages; trained with imitation and reinforcement learning with human feedback to improve factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based; retrieval-augmented; RLHF</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Open-domain question answering / knowledge-grounded QA</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates candidate answers and supporting evidence by actively retrieving and reading web pages; implicit hypothesis formation via reading and synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Uses retrieved web evidence and human feedback (reward models) to encourage factual/supported answers.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Human feedback and RL-based training to align outputs with supported evidence; evaluated on web-grounded QA tasks in its own work.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Grounding answers in retrieved web pages and using human feedback to penalize unsupported claims.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires expensive human feedback/RL training; does not explicitly model internal chain-of-thought in the way ReAct does (as noted in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2673.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2673.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A paradigm that augments generative language models with an explicit retrieval component that fetches relevant documents to condition generation, improving factuality and grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledge-intensive nlp tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>RAG uses an external retriever (lexical or neural) to fetch context documents which are provided to a generator; the generator conditions on retrieved passages to produce answers, improving access to up-to-date and factual information.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>retrieval-augmented; hybrid (retriever + generator)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Knowledge-intensive NLP tasks / open-domain QA</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates answers/hypotheses conditioned on retrieved documents; retrieval guides candidate claims.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility grounded in retrieved documents; relevance scoring (retriever scores) used implicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Evaluated on retrieval+generation benchmarks; this paper references RAG as related work and contrasts ReAct's simple API-based retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Grounding generated content on retrieved passages to reduce hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper notes ReAct's retrieval API is simpler and weaker than state-of-the-art retrievers (RAG), motivating explicit reasoning-guided retrieval despite weaker retriever capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2673.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2673.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STaR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>STaR (Bootstrapping Reasoning with Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that bootstraps model reasoning by finetuning on correct rationales generated by the model itself to improve multi-step reasoning accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Star: Bootstrapping reasoning with reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>STaR</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>STaR generates rationales with an LM, filters or selects correct ones, and finetunes models on these model-generated correct rationales to improve reasoning performance. Cited in related work as a complementary approach to ReAct.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM finetuning on model-generated rationales (self-training)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General multi-step reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Uses LLM-generated rationales as training data to produce better future rationales/hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Quality control via filtering/selecting correct rationales before finetuning (implementation details in STaR paper).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Finetuning and benchmark evaluation (original STaR paper); cited as related.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>By finetuning on correct rationales, reduces propensity to produce incorrect/hallucinated reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires a pipeline to produce and filter correct rationales; cited as complementary to ReAct's prompting approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2673.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2673.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Selection-Inference</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Selection-Inference</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-step reasoning architecture that divides reasoning into a selection phase (retrieve/choose relevant evidence) and an inference phase (compose final answer), improving interpretability and modularity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Selection-inference: Exploiting large language models for interpretable logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Selection-Inference</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Splits reasoning into (1) selection: retrieve or select relevant evidence/premises, and (2) inference: produce the final answer given the selected evidence. Cited as an approach that decomposes reasoning and evidence retrieval, related to ReAct's interleaving of thoughts and actions.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>modular LLM architecture (retrieval/selection + inference)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Logical reasoning and interpretable QA</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Selection step yields candidate premises/evidence; inference step composes hypotheses/answers.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility grounded in the selected evidence; selection quality determines downstream plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>By explicitly selecting supporting evidence, reduces unsupported hallucinated claims.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Mentioned as a different decomposition strategy; details are in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2673.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2673.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inner Monologue (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inner Monologue (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that injects dense environment feedback as an 'inner monologue' to guide an agent's actions, limited to observations about state and outstanding subgoals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Inner monologue: Embodied reasoning through planning with language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Inner Monologue (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>IM augments action generation with injected textual feedback summarizing environment observations and remaining tasks; it was previously applied to embodied reasoning and planning. In this paper IM is discussed and an IM-style ablation (ReAct-IM) is evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based planning with injected observation feedback</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Embodied reasoning / planning</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates plans and reacts to injected observational monologue; does not induce broad internal commonsense hypotheses in the IM formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Evaluated in prior work on planning/embodied tasks; here used as comparative motivation and to design ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Paper reports ReAct substantially outperforms IM-style prompting in ALFWorld (71% vs 53% overall), arguing that IM-like dense external feedback lacks high-level decomposition and commonsense reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>IM-style dense feedback may fail to identify subgoal completion and next subgoals, and lacks induction of commonsense priors for where items likely are.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>WebGPT: Browser-assisted question-answering with human feedback <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Inner monologue: Embodied reasoning through planning with language models <em>(Rating: 2)</em></li>
                <li>Star: Bootstrapping reasoning with reasoning <em>(Rating: 1)</em></li>
                <li>Selection-inference: Exploiting large language models for interpretable logical reasoning <em>(Rating: 1)</em></li>
                <li>Do as i can, not as i say: Grounding language in robotic affordances <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2673",
    "paper_id": "paper-99832586d55f540f603637e458a292406a0ed75d",
    "extraction_schema_id": "extraction-schema-68",
    "extracted_data": [
        {
            "name_short": "ReAct",
            "name_full": "ReAct (Reasoning + Acting)",
            "brief_description": "A prompting paradigm that interleaves natural-language reasoning traces ('thoughts') with environment-directed actions (search/lookup/finish) in a single augmented action space, enabling LLMs to both plan and fetch external evidence to ground multi-step reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ReAct",
            "system_description": "ReAct augments an LLM's action space with a language/thought channel and explicit domain actions, producing interleaved thought → action → observation steps. Implemented with few-shot prompts to PaLM-540B (and tested with GPT-3), ReAct uses human-authored example trajectories (thoughts + actions + observations) in-context; it interfaces with simple external APIs (Wikipedia search/lookup) to retrieve evidence, and can be fine-tuned (bootstrapped dataset of 3,000 ReAct trajectories) for smaller models. Key components: (1) augmented action space A ∪ L (actions + language thoughts), (2) few-shot example trajectories in prompt, (3) external API for retrieval (e.g., Wikipedia), and (4) optional finetuning on model-generated correct trajectories.",
            "system_type": "LLM-based; retrieval-augmented when interacting with external APIs",
            "scientific_domain": "Knowledge-intensive question answering, fact verification, and interactive decision-making (general knowledge / NLP tasks)",
            "hypothesis_generation_method": "Generates hypothesis-like intermediate assertions as free-form natural-language 'thoughts' interleaved with actions; hypotheses arise as decompositions of the goal, candidate facts to check, commonsense inferences, or plans for actions (e.g., what to search or which subgoal to pursue).",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility is assessed by grounding thoughts via explicit external retrieval (search/lookup) and incorporating retrieved observations into subsequent thoughts; heuristics (e.g., backoff to CoT-SC) are used when ReAct fails to produce an answer.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Indirect: downstream task metrics (HotpotQA exact match, FEVER accuracy) and human-labeled breakdowns of success/failure modes (true positives, false positives, hallucination, reasoning error, search result error).",
            "pre_experiment_evaluation": false,
            "validation_mechanism": "Empirical evaluation on benchmarks (HotpotQA, FEVER, ALFWorld, WebShop), human annotation of trajectory success/failure modes, and ablations (Act-only, CoT baselines). When fine-tuned, validated by held-out benchmark performance.",
            "reproducibility_measures": "Prompts included in Appendix C, additional GPT-3 experiments in Appendix A.1, and code link provided (https://react-lm.github.io/). Finetuning details (datasets, steps, batch size) reported in Appendix B.",
            "hallucination_prevention_method": "Grounding thoughts with external retrieval (Wikipedia API) to replace or check internal model assertions; human-in-the-loop thought editing demonstrated for correction.",
            "hallucination_detection_method": "Human annotation of trajectories to identify hallucinated thoughts/facts; detection also implicit via failed or noninformative search (tracked as error modes).",
            "hallucination_rate": "From a manual sample on HotpotQA: among sampled trajectories, ReAct had 6% false positive rate in 'success' class and 0% 'hallucination' failure rate vs CoT (14% and 56% respectively); noninformative search caused 23% of ReAct error cases (human-labelled analysis).",
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Indirect: uses combination heuristics with CoT-SC (backoff strategies) to handle low-confidence situations; no explicit probabilistic uncertainty output described.",
            "benchmark_dataset": "HotpotQA (multi-hop QA), FEVER (fact verification), ALFWorld (text-based interactive tasks), WebShop (web interaction shopping)",
            "performance_metrics": "HotpotQA EM: ReAct 27.4 (PaLM-540B prompting); FEVER Acc: ReAct 60.9; ReAct→CoT-SC and CoT-SC→ReAct combinations improve to 35.1 EM (HotpotQA) and 64.6 Acc (FEVER) respectively. ALFWorld best ReAct trial success rate 71% (avg 57%), vs Act best 45% and BUTLER 37%. WebShop score 66.6 and success rate 40.0 vs Act 62.3/30.1 and IL 59.9/29.1.",
            "comparison_with_baseline": "ReAct consistently outperforms Act-only prompts and imitation / IL baselines on decision-making tasks; compared to CoT, ReAct reduces hallucination and improves FEVER accuracy but slightly underperforms on some HotpotQA prompting runs (CoT 29.4 EM vs ReAct 27.4 EM). Fine-tuned ReAct (PaLM-8B/62B) outperforms larger prompting-only baselines.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Dependence on quality of external search results (noninformative search causes errors); higher reasoning error rate (e.g., repetitive loops) versus CoT; limited by in-context example length for complex tasks; current action API is simplified (weaker than state-of-the-art retrievers); limited automatic uncertainty quantification and no explicit novelty assessment.",
            "uuid": "e2673.0",
            "source_info": {
                "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting (CoT)",
            "brief_description": "A prompting technique where LLMs are induced to produce intermediate multi-step natural-language reasoning traces (rationales) before producing final answers to improve multi-step problem solving.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "system_name": "Chain-of-Thought prompting (CoT)",
            "system_description": "CoT uses few-shot prompting that includes worked-out reasoning chains so the LLM produces internal multi-step rationales before answering. Implemented in this paper as a reasoning-only baseline: thoughts are present but actions/observations are removed. Typically decoded greedily or with sampling.",
            "system_type": "LLM-based prompting (reasoning-only)",
            "scientific_domain": "General multi-step reasoning tasks and knowledge-intensive QA (here used on HotpotQA and FEVER)",
            "hypothesis_generation_method": "Generates intermediate natural-language rationales (which can be interpreted as hypotheses or subclaims) via prompted chain-of-thought examples.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Downstream task performance (HotpotQA EM, FEVER accuracy) and human-labeled trajectory success/failure breakdowns (e.g., hallucination rates, reasoning errors).",
            "pre_experiment_evaluation": false,
            "validation_mechanism": "Benchmark evaluation (HotpotQA, FEVER) and human analysis of success/failure modes.",
            "reproducibility_measures": "Prompts and in-context examples provided in appendix; CoT experimental settings reported.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "Human annotation revealed high hallucination rates; CoT does not have built-in grounding to prevent hallucination in this paper.",
            "hallucination_rate": "Human-labelled sample on HotpotQA: CoT had a 14% false positive rate among successes and 56% of failure cases were due to hallucination (per Table 2).",
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "HotpotQA, FEVER (as used in this paper)",
            "performance_metrics": "HotpotQA EM (PaLM-540B CoT): 29.4; FEVER Acc: 56.3. CoT exhibits higher false-positive/hallucination rates than ReAct in manual analysis.",
            "comparison_with_baseline": "CoT outperforms Standard prompting on some reasoning tasks but suffers from hallucination and error propagation; combining with self-consistency (CoT-SC) improves results.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Prone to hallucinated facts and error propagation across multi-step rationales; not grounded to external knowledge sources by default.",
            "uuid": "e2673.1",
            "source_info": {
                "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "CoT-SC",
            "name_full": "Chain-of-Thought with Self-Consistency (CoT-SC)",
            "brief_description": "An ensemble/self-consistency technique that samples multiple chain-of-thought reasoning trajectories from an LLM and aggregates answers (e.g., by majority vote) to improve robustness and accuracy.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "use",
            "system_name": "CoT-SC (Self-Consistency)",
            "system_description": "CoT-SC samples multiple CoT rationales (21 samples in this paper, decoding temperature 0.7) and selects the final answer by majority vote among sampled answers. Used both standalone and in combination with ReAct as a fallback/backoff mechanism.",
            "system_type": "LLM-based ensemble / sampling method",
            "scientific_domain": "General reasoning and QA (used on HotpotQA and FEVER here)",
            "hypothesis_generation_method": "Generates many alternative chain-of-thought hypotheses via stochastic decoding; hypotheses are represented as sampled reasoning traces and final answers.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility is approximated by frequency (majority) across sampled reasoning traces — higher agreement indicates higher confidence.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Improved answer accuracy via majority voting; trade-off measured by number of samples and accuracy (empirical improvement shown with up to 21 samples).",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Aggregates multiple stochastic outputs and uses majority answer as validation; used as backoff in hybrid strategies (CoT-SC→ReAct or ReAct→CoT-SC) when confidence is low or ReAct fails.",
            "reproducibility_measures": "Sampling counts, temperature, and prompting details provided in paper and appendices.",
            "hallucination_prevention_method": "Reduces hallucination by averaging over multiple stochastic chains, thereby preferring commonly produced answers.",
            "hallucination_detection_method": "Low agreement across samples triggers fallback to ReAct; disagreement acts as a signal of low confidence/possible hallucination.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Uses ensemble agreement (majority frequency among N samples) as a proxy for confidence/uncertainty.",
            "benchmark_dataset": "HotpotQA, FEVER",
            "performance_metrics": "CoT-SC improves CoT performance (e.g., empirically boosts HotpotQA/FEVER results vs single CoT runs); combining CoT-SC and ReAct yields best prompting performance in some settings (e.g., ReAct→CoT-SC and CoT-SC→ReAct combinations reached up to 35.1 EM on HotpotQA and 64.6 Acc on FEVER).",
            "comparison_with_baseline": "CoT-SC outperforms single CoT via self-consistency sampling; in hybrid pipelines, it complements ReAct to balance internal reasoning and external grounding.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Computationally expensive due to many samples; majority voting is a simple heuristic and may not capture nuanced confidence or correct minority answers.",
            "uuid": "e2673.2",
            "source_info": {
                "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Act",
            "name_full": "Act-only prompting (Act)",
            "brief_description": "A prompting baseline that removes internal thoughts and only prompts LLMs to generate domain-specific actions to interact with an environment (e.g., search/lookup/finish), resembling an acting-only policy without explicit reasoning traces.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Act-only prompting",
            "system_description": "Act prompts are constructed from the same human-authored trajectories as ReAct but with thoughts removed. The LLM emits only environment actions (search/lookup/choose/buy/etc.) and receives observations; no free-form internal reasoning is output in the trajectory.",
            "system_type": "LLM-based action generation",
            "scientific_domain": "Interactive decision-making and web-augmented QA (ALFWorld, WebShop, HotpotQA/FEVER with API)",
            "hypothesis_generation_method": "Does not explicitly generate hypotheses; decisions are made implicitly via action tokens produced by the model.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": false,
            "validation_mechanism": "Evaluated empirically on benchmarks and compared against ReAct and imitation learning baselines.",
            "reproducibility_measures": "Prompts and prompt construction described in appendices.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "Manual trajectory inspection; act-only models may produce hallucinating actions (e.g., assuming items present where they are not).",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "ALFWorld, WebShop, HotpotQA, FEVER (as baselines)",
            "performance_metrics": "HotpotQA: Act 25.7 EM (PaLM-540B prompting); FEVER: Act 58.9 Acc. ALFWorld average success 45% (best of 6 = 45). WebShop score 62.3 and SR 30.1.",
            "comparison_with_baseline": "Act performs worse than ReAct in most decision-making tasks (e.g., ALFWorld, WebShop), showing the value of sparse internal reasoning for long-horizon planning.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Lacks explicit internal reasoning traces, leading to failures to decompose goals, track subgoals, or avoid hallucinating actions.",
            "uuid": "e2673.3",
            "source_info": {
                "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "ReAct-IM",
            "name_full": "ReAct-IM (Inner Monologue-style ablation)",
            "brief_description": "An ablation of ReAct where thoughts are restricted to dense external-feedback style observations (an Inner Monologue-like pattern) limiting internal goal decomposition and reliance on pretrained commonsense knowledge.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ReAct-IM (IM-style ablation)",
            "system_description": "ReAct-IM re-annotates ReAct trajectories with dense external-feedback thoughts that focus on decomposing current goals and the current subgoal, but lack higher-level reasoning such as when subgoals are complete, next-subgoal selection, or induction of where items likely are. Used to probe importance of internal reasoning versus reaction to observations.",
            "system_type": "LLM-based prompting / ablation",
            "scientific_domain": "Interactive decision-making (ALFWorld)",
            "hypothesis_generation_method": "Limited generation of short observational thoughts; does not produce broader hypotheses about environment structure.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": false,
            "validation_mechanism": "Empirical evaluation on ALFWorld comparing performance with full ReAct; qualitative analysis of failures where ReAct-IM lacks high-level decomposition.",
            "reproducibility_measures": "Appendix B.2 and prompt examples in Appendix C.4.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "ALFWorld (ablation study)",
            "performance_metrics": "ReAct overall best-of-6 = 71% success on ALFWorld; ReAct-IM best-of-6 = 53% (paper reports ReAct 71 vs ReAct-IM 53 overall), showing large drop when limiting internal reasoning.",
            "comparison_with_baseline": "ReAct significantly outperforms ReAct-IM, indicating the value of flexible internal reasoning beyond dense external feedback.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Demonstrates that restricting thoughts to external-feedback prevents use of commonsense internal priors and goal decomposition, degrading performance.",
            "uuid": "e2673.4",
            "source_info": {
                "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "WebGPT",
            "name_full": "WebGPT",
            "brief_description": "A system that uses language models to interact with web browsers (search, follow links, read pages) combined with human feedback to produce grounded answers to open-domain questions.",
            "citation_title": "WebGPT: Browser-assisted question-answering with human feedback",
            "mention_or_use": "mention",
            "system_name": "WebGPT",
            "system_description": "WebGPT uses an LM to navigate web pages, follow search results, and synthesize answers grounded in retrieved webpages; trained with imitation and reinforcement learning with human feedback to improve factuality.",
            "system_type": "LLM-based; retrieval-augmented; RLHF",
            "scientific_domain": "Open-domain question answering / knowledge-grounded QA",
            "hypothesis_generation_method": "Generates candidate answers and supporting evidence by actively retrieving and reading web pages; implicit hypothesis formation via reading and synthesis.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Uses retrieved web evidence and human feedback (reward models) to encourage factual/supported answers.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": "Human feedback and RL-based training to align outputs with supported evidence; evaluated on web-grounded QA tasks in its own work.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": "Grounding answers in retrieved web pages and using human feedback to penalize unsupported claims.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Requires expensive human feedback/RL training; does not explicitly model internal chain-of-thought in the way ReAct does (as noted in paper).",
            "uuid": "e2673.5",
            "source_info": {
                "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "A paradigm that augments generative language models with an explicit retrieval component that fetches relevant documents to condition generation, improving factuality and grounding.",
            "citation_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "mention_or_use": "mention",
            "system_name": "Retrieval-Augmented Generation (RAG)",
            "system_description": "RAG uses an external retriever (lexical or neural) to fetch context documents which are provided to a generator; the generator conditions on retrieved passages to produce answers, improving access to up-to-date and factual information.",
            "system_type": "retrieval-augmented; hybrid (retriever + generator)",
            "scientific_domain": "Knowledge-intensive NLP tasks / open-domain QA",
            "hypothesis_generation_method": "Generates answers/hypotheses conditioned on retrieved documents; retrieval guides candidate claims.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility grounded in retrieved documents; relevance scoring (retriever scores) used implicitly.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": "Evaluated on retrieval+generation benchmarks; this paper references RAG as related work and contrasts ReAct's simple API-based retrieval.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": "Grounding generated content on retrieved passages to reduce hallucination.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Paper notes ReAct's retrieval API is simpler and weaker than state-of-the-art retrievers (RAG), motivating explicit reasoning-guided retrieval despite weaker retriever capabilities.",
            "uuid": "e2673.6",
            "source_info": {
                "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "STaR",
            "name_full": "STaR (Bootstrapping Reasoning with Reasoning)",
            "brief_description": "A method that bootstraps model reasoning by finetuning on correct rationales generated by the model itself to improve multi-step reasoning accuracy.",
            "citation_title": "Star: Bootstrapping reasoning with reasoning",
            "mention_or_use": "mention",
            "system_name": "STaR",
            "system_description": "STaR generates rationales with an LM, filters or selects correct ones, and finetunes models on these model-generated correct rationales to improve reasoning performance. Cited in related work as a complementary approach to ReAct.",
            "system_type": "LLM finetuning on model-generated rationales (self-training)",
            "scientific_domain": "General multi-step reasoning tasks",
            "hypothesis_generation_method": "Uses LLM-generated rationales as training data to produce better future rationales/hypotheses.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Quality control via filtering/selecting correct rationales before finetuning (implementation details in STaR paper).",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": "Finetuning and benchmark evaluation (original STaR paper); cited as related.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": "By finetuning on correct rationales, reduces propensity to produce incorrect/hallucinated reasoning chains.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Requires a pipeline to produce and filter correct rationales; cited as complementary to ReAct's prompting approach.",
            "uuid": "e2673.7",
            "source_info": {
                "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Selection-Inference",
            "name_full": "Selection-Inference",
            "brief_description": "A two-step reasoning architecture that divides reasoning into a selection phase (retrieve/choose relevant evidence) and an inference phase (compose final answer), improving interpretability and modularity.",
            "citation_title": "Selection-inference: Exploiting large language models for interpretable logical reasoning",
            "mention_or_use": "mention",
            "system_name": "Selection-Inference",
            "system_description": "Splits reasoning into (1) selection: retrieve or select relevant evidence/premises, and (2) inference: produce the final answer given the selected evidence. Cited as an approach that decomposes reasoning and evidence retrieval, related to ReAct's interleaving of thoughts and actions.",
            "system_type": "modular LLM architecture (retrieval/selection + inference)",
            "scientific_domain": "Logical reasoning and interpretable QA",
            "hypothesis_generation_method": "Selection step yields candidate premises/evidence; inference step composes hypotheses/answers.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility grounded in the selected evidence; selection quality determines downstream plausibility.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": null,
            "reproducibility_measures": null,
            "hallucination_prevention_method": "By explicitly selecting supporting evidence, reduces unsupported hallucinated claims.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Mentioned as a different decomposition strategy; details are in the cited work.",
            "uuid": "e2673.8",
            "source_info": {
                "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Inner Monologue (IM)",
            "name_full": "Inner Monologue (IM)",
            "brief_description": "A method that injects dense environment feedback as an 'inner monologue' to guide an agent's actions, limited to observations about state and outstanding subgoals.",
            "citation_title": "Inner monologue: Embodied reasoning through planning with language models",
            "mention_or_use": "mention",
            "system_name": "Inner Monologue (IM)",
            "system_description": "IM augments action generation with injected textual feedback summarizing environment observations and remaining tasks; it was previously applied to embodied reasoning and planning. In this paper IM is discussed and an IM-style ablation (ReAct-IM) is evaluated.",
            "system_type": "LLM-based planning with injected observation feedback",
            "scientific_domain": "Embodied reasoning / planning",
            "hypothesis_generation_method": "Generates plans and reacts to injected observational monologue; does not induce broad internal commonsense hypotheses in the IM formulation.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": "Evaluated in prior work on planning/embodied tasks; here used as comparative motivation and to design ablations.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": "Paper reports ReAct substantially outperforms IM-style prompting in ALFWorld (71% vs 53% overall), arguing that IM-like dense external feedback lacks high-level decomposition and commonsense reasoning.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "IM-style dense feedback may fail to identify subgoal completion and next subgoals, and lacks induction of commonsense priors for where items likely are.",
            "uuid": "e2673.9",
            "source_info": {
                "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2
        },
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models",
            "rating": 2
        },
        {
            "paper_title": "Star: Bootstrapping reasoning with reasoning",
            "rating": 1
        },
        {
            "paper_title": "Selection-inference: Exploiting large language models for interpretable logical reasoning",
            "rating": 1
        },
        {
            "paper_title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "rating": 1
        }
    ],
    "cost": 0.023743,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ReAct: Synergizing Reasoning and Acting in Language MODELS</h1>
<p>Shunyu Yao ${ }^{8,1}$, Jeffrey Zhao ${ }^{2}$, Dian Yu ${ }^{2}$, Nan Du ${ }^{2}$, Izhak Shafran ${ }^{2}$, Karthik Narasimhan ${ }^{1}$, Yuan Cao ${ }^{2}$<br>${ }^{1}$ Department of Computer Science, Princeton University<br>${ }^{2}$ Google Research, Brain team<br>${ }^{1}$ {shunyuy, karthikn}@princeton.edu<br>${ }^{2}$ {jeffreyzhao, dianyu, dunan, izhak, yuancao}@google.com</p>
<h4>Abstract</h4>
<p>While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of $34 \%$ and $10 \%$ respectively, while being prompted with only one or two in-context examples.</p>
<h2>1 INTRODUCTION</h2>
<p>A unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with verbal reasoning (or inner speech, Alderson-Day \&amp; Fernyhough, 2015), which has been theorized to play an important role in human cognition for enabling self-regulation or strategization (Vygotsky, 1987; Luria, 1965; Fernyhough, 2010) and maintaining a working memory (Baddeley, 1992). Consider the example of cooking up a dish in the kitchen. Between any two specific actions, we may reason in language in order to track progress ("now that everything is cut, I should heat up the pot of water"), to handle exceptions or adjust the plan according to the situation ("I don't have salt, so let me use soy sauce and pepper instead"), and to realize when external information is needed ("how do I prepare dough? Let me search on the Internet"). We may also act (open a cookbook to read the recipe, open the fridge, check ingredients) to support the reasoning and to answer questions ("What dish can I make right now?"). This tight synergy between "acting" and "reasoning" allows humans to learn new tasks quickly and perform robust decision making or reasoning, even under previously unseen circumstances or facing information uncertainties.</p>
<p>Recent results have hinted at the possibility of combining verbal reasoning with interactive decision making in autonomous systems. On one hand, properly prompted large language models (LLMs) have demonstrated emergent capabilities to carry out several steps of reasoning traces to derive</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (1) Comparison of 4 prompting methods, (a) Standard, (b) Chain-of-thought (CoT, Reason Only), (c) Act-only, and (d) ReAct (Reason+Act), solving a HotpotQA (Yang et al., 2018) question. (2) Comparison of (a) Act-only and (b) ReAct prompting to solve an AlfWorld (Shridhar et al., 2020b) game. In both domains, we omit in-context examples in the prompt, and only show task solving trajectories generated by the model (Act, Thought) and the environment (Obs).
answers from questions in arithmetic, commonsense, and symbolic reasoning tasks (Wei et al., 2022). However, this "chain-of-thought" reasoning is a static black box, in that the model uses its own internal representations to generate thoughts and is not grounded in the external world, which limits its ability to reason reactively or update its knowledge. This can lead to issues like fact hallucination and error propagation over the reasoning process (Figure 1 (1b)). On the other hand, recent work has explored the use of pre-trained language models for planning and acting in interactive environments (Ahn et al., 2022; Nakano et al., 2021; Yao et al., 2020; Huang et al., 2022a), with a focus on predicting actions via language priors. These approaches usually convert multi-modal observations into text, use a language model to generate domain-specific actions or plans, and then use a controller to choose or execute them. However, they do not employ language models to reason abstractly about high-level goals or maintain a working memory to support acting, barring Huang et al. (2022b) who perform a limited form of verbal reasoning to reiterate spatial facts about the current state. Beyond such simple embodied tasks to interact with a few blocks, there have not been studies on how reasoning and acting can be combined in a synergistic manner for general task solving, and if such a combination can bring systematic benefits compared to reasoning or acting alone.</p>
<p>In this work, we present ReAct, a general paradigm to combine reasoning and acting with language models for solving diverse language reasoning and decision making tasks (Figure 1). ReAct prompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an interleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and adjust high-level plans for acting (reason to act), while also interact with the external environments (e.g. Wikipedia) to incorporate additional information into reasoning (act to reason).</p>
<p>We conduct empirical evaluations of ReAct and state-of-the-art baselines on four diverse benchmarks: question answering (HotPotQA, Yang et al., 2018), fact verification (Fever, Thorne et al., 2018), text-based game (ALFWorld, Shridhar et al., 2020b), and webpage navigation (WebShop, Yao et al., 2022). For HotPotQA and Fever, with access to a Wikipedia API that the model can interact with, ReAct outperforms vanilla action generation models while being competitive with chain-ofthought reasoning (CoT) (Wei et al., 2022). The best approach overall is a combination of ReAct and CoT that allows for the use of both internal knowledge and externally obtained information during reasoning. On ALFWorld and WebShop, two or even one-shot ReAct prompting is able to outperform imitation or reinforcement learning methods trained with $10^{3} \sim 10^{5}$ task instances, with an absolute improvement of $34 \%$ and $10 \%$ in success rates respectively. We also demonstrate the importance of sparse, versatile reasoning in decision making by showing consistent advantages over controlled baselines with actions only. Besides general applicability and performance boost, the combination of reasoning and acting also contributes to model interpretability, trustworthiness, and diagnosability across all domains, as humans can readily distinguish information from model's internal knowledge versus external environments, as well as inspect reasoning traces to understand the decision basis of model actions.</p>
<p>To summarize, our key contributions are the following: (1) we introduce ReAct, a novel promptbased paradigm to synergize reasoning and acting in language models for general task solving; (2) we perform extensive experiments across diverse benchmarks to showcase the advantage of ReAct in a few-shot learning setup over prior approaches that perform either reasoning or action generation in isolation; (3) we present systematic ablations and analysis to understand the importance of acting in reasoning tasks, and reasoning in interactive tasks; (4) we analyze the limitations of ReAct under the prompting setup (i.e. limited support of reasoning and acting behaviors), and perform initial finetuning experiments showing the potential of ReAct to improve with additional training data. Scaling up ReAct to train and operate on more tasks and combining it with complementary paradigms like reinforcement learning could further unlock the potential of large language models.</p>
<h1>2 ReAct: Synergizing ReAsoning + Acting</h1>
<p>Consider a general setup of an agent interacting with an environment for task solving. At time step $t$, an agent receives an observation $o_{t} \in \mathcal{O}$ from the environment and takes an action $a_{t} \in \mathcal{A}$ following some policy $\pi\left(a_{t} \mid c_{t}\right)$, where $c_{t}=\left(o_{1}, a_{1}, \cdots, o_{t-1}, a_{t-1}, o_{t}\right)$ is the context to the agent. Learning a policy is challenging when the mapping $c_{t} \mapsto a_{t}$ is highly implicit and requires extensive computation. For example, the agent shown in Figure 1(1c) is unable to generate the correct final action (Act 4) to finish the QA task as it requires complex reasoning over the trajectory context (Question, Act 1-3, Obs 1-3). Similarly, the agent shown in Figure 1(2a) fails to comprehend from the context that sinkbasin 1 does not contain peppershaker 1, thus keep producing hallucinating actions.</p>
<p>The idea of ReAct is simple: we augment the agent's action space to $\hat{\mathcal{A}}=\mathcal{A} \cup \mathcal{L}$, where $\mathcal{L}$ is the space of language. An action $\hat{a}<em t="t">{t} \in \mathcal{L}$ in the language space, which we will refer to as a thought or a reasoning trace, does not affect the external environment, thus leading to no observation feedback. Instead, a thought $\hat{a}</em>\right)$ to support future reasoning or acting. As shown in Figure 1, there could be various types of useful thoughts, e.g. decomposing task goals and create action plans (2b, Act 1; 1d, Thought 1), injecting commonsense knowledge relevant to task solving (2b, Act 1), extracting important parts from observations (1d, Thought2, 4), track progress and transit action plans (2b, Act 8), handle exceptions and adjust action plans (1d, Thought 3), and so on.}$ aims to compose useful information by reasoning over the current context $c_{t}$, and update the context $c_{t+1}=\left(c_{t}, \hat{a}_{t</p>
<p>However, as the language space $\mathcal{L}$ is unlimited, learning in this augmented action space is difficult and requires strong language priors. In this paper, we mainly focus on the setup where a frozen large language model, PaLM-540B (Chowdhery et al., 2022) ${ }^{1}$, is prompted with few-shot in-context examples to generate both domain-specific actions and free-form language thoughts for task solving (Figure 1 (1d), (2b)). Each in-context example is a human trajectory of actions, thoughts, and environment observations to solve a task instance (see Appendix C). For the tasks where reasoning is of primary importance (Figure 1(1)), we alternate the generation of thoughts and actions so that the task-solving trajectory consists of multiple thought-action-observation steps. In contrast, for decision making tasks that potentially involve a large number of actions (Figure 1(2)), thoughts only need to</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>appear sparsely in the most relevant positions of a trajectory, so we let the language model decide the asynchronous occurrence of thoughts and actions for itself.</p>
<p>Since decision making and reasoning capabilities are integrated into a large language model, ReAct enjoys several unique features: A) Intuitive and easy to design: Designing ReAct prompts is straightforward as human annotators just type down their thoughts in language on top of their actions taken. No ad-hoc format choice, thought design, or example selection is used in this paper. We detail prompt design for each task in Sections 3 and 4. B) General and flexible: Due to the flexible thought space and thought-action occurrence format, ReAct works for diverse tasks with distinct action spaces and reasoning needs, including but not limited to QA, fact verification, text game, and web navigation. C) Performant and robust: ReAct shows strong generalization to new task instances while learning solely from one to six in-context examples, consistently outperforming baselines with only reasoning or acting across different domains. We also show in Section 3 additional benefits when finetuning is enabled, and in Section 4 how ReAct performance is robust to prompt selections. D) Human aligned and controllable: ReAct promises an interpretable sequential decision making and reasoning process where humans can easily inspect reasoning and factual correctness. Moreover, humans can also control or correct the agent behavior on the go by thought editing, as shown in Figure 5 in Section 4.</p>
<h1>3 Knowledge-Intensive Reasoning Tasks</h1>
<p>We begin with knowledge-intensive reasoning tasks like multi-hop question answering and fact verification. As shown in Figure 1(1d), by interacting with a Wikipedia API, ReAct is able to retrieve information to support reasoning, while also use reasoning to target what to retrieve next, demonstrating a synergy of reasoning and acting.</p>
<h3>3.1 SETUP</h3>
<p>Domains We consider two datasets challenging knowledge retrieval and reasoning: (1) HotPotQA (Yang et al., 2018), a multi-hop question answering benchmark that requires reasoning over two or more Wikipedia passages, and (2) FEVER (Thorne et al., 2018), a fact verification benchmark where each claim is annotated SUPPORTS, REFUTES, or NOT ENOUGH INFO, based on if there exists a Wikipedia passage to verify the claim. In this work, we operate in a question-only setup for both tasks, where models only receive the question/claim as input without access to support paragraphs, and have to rely on their internal knowledge or retrieve knowledge via interacting with an external environment to support reasoning.</p>
<p>Action Space We design a simple Wikipedia web API with three types of actions to support interactive information retrieval: (1) search[entity], which returns the first 5 sentences from the corresponding entity wiki page if it exists, or else suggests top-5 similar entities from the Wikipedia search engine, (2) lookup[string], which would return the next sentence in the page containing string, simulating Ctrl+F functionality on the browser. (3) finish[answer], which would finish the current task with answer. We note that this action space mostly can only retrieve a small part of a passage based on exact passage name, which is significantly weaker than state-of-theart lexical or neural retrievers. The purpose is to simulate how humans would interact with Wikipedia, and force models to retrieve via explicit reasoning in language.</p>
<h3>3.2 MEthods</h3>
<p>ReAct Prompting For HotpotQA and Fever, we randomly select 6 and 3 cases $^{2}$ from the training set and manually compose ReAct-format trajectories to use as few-shot exemplars in the prompts. Similar to Figure 1(d), each trajectory consists of multiple thought-action-observation steps (i.e. dense thought), where free-form thoughts are used for various purposes. Specifically, we use a combination of thoughts that decompose questions ("I need to search x, find y, then find z"), extract information from Wikipedia observations ("x was started in 1844", "The paragraph does not tell x"), perform commonsense ("x is not y, so z must instead be...") or arithmetic reasoning (" $1844&lt;1989$ "), guide</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Prompt Method^{a}</th>
<th>HotpotQA</th>
<th>Fever</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>(EM)</td>
<td>(Acc)</td>
</tr>
<tr>
<td>Standard</td>
<td>28.7</td>
<td>57.1</td>
</tr>
<tr>
<td>CoT (Wei et al., 2022)</td>
<td>29.4</td>
<td>56.3</td>
</tr>
<tr>
<td>CoT-SC (Wang et al., 2022a)</td>
<td>33.4</td>
<td>60.4</td>
</tr>
<tr>
<td>Act</td>
<td>25.7</td>
<td>58.9</td>
</tr>
<tr>
<td>ReAct</td>
<td>27.4</td>
<td>60.9</td>
</tr>
<tr>
<td>CoT-SC → ReAct</td>
<td>34.2</td>
<td>64.6</td>
</tr>
<tr>
<td>ReAct → CoT-SC</td>
<td>35.1</td>
<td>62.0</td>
</tr>
<tr>
<td>Supervised SoTA^{b}</td>
<td>67.5</td>
<td>89.5</td>
</tr>
</tbody>
</table>
<p>Table 1: PaLM-540B prompting results on HotpotQA and Fever.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: PaLM-540B prompting results with respect to number of CoT-SC samples used.</p>
<p>^{a}HotpotQA EM is 27.1, 28.9, 33.8 for Standard, CoT, CoT-SC in Wang et al. (2022b).</p>
<p>^{b}(Zhu et al., 2021; Lewis et al., 2020)</p>
<p>search reformulation ("maybe I can search/look up x instead"), and synthesize the final answer ("…so the answer is x"). See Appendix C for more details.</p>
<p><strong>Baselines</strong> We systematically ablate ReAct trajectories to build prompts for multiple baselines (with formats as Figure 1(1a-1c)): (a) <strong>Standard prompting</strong> (Standard), which removes all thoughts, actions, observations in ReAct trajectories. (b) <strong>Chain-of-thought prompting</strong> (CoT) (Wei et al., 2022), which removes actions and observations and serve as a reasoning-only baseline. We also build a self-consistency baseline (CoT-SC) (Wang et al., 2022a;b) by sampling 21 CoT trajectories with decoding temperature 0.7 during inference and adopting the majority answer, which is found to consistently boost performance over CoT. (c) <strong>Acting-only prompt</strong> (Act), which removes thoughts in ReAct trajectories, loosely resembling how WebGPT (Nakano et al., 2021) interacts with the Internet to answer questions, though it operates on a different task and action space, and uses imitation and reinforcement learning instead of prompting.</p>
<p><strong>Combining Internal and External Knowledge</strong> As will be detail in Section 3.3, we observe that the problem solving process demonstrated by ReAct is more factual and grounded, whereas CoT is more accurate in formulating reasoning structure but can easily suffer from hallucinated facts or thoughts. We therefore propose to incorporate ReAct and CoT-SC, and let the model decide when to switch to the other method based on the following heuristics: A) <strong>ReAct → CoT-SC</strong>: when ReAct fails to return an answer within given steps, back off to CoT-SC. We set 7 and 5 steps for HotpotQA and FEVER respectively as we find more steps will not improve ReAct performance^{3}. B) <strong>CoT-SC → ReAct</strong>: when the majority answer among n CoT-SC samples occurs less than n/2 times (i.e. internal knowledge might not support the task confidently), back off to ReAct.</p>
<p><strong>Finetuning</strong> Due to the challenge of manually annotating reasoning traces and actions at scale, we consider a bootstraping approach similar to Zelikman et al. (2022), using 3,000 trajectories with correct answers generated by ReAct (also for other baselines) to finetune smaller language models (PaLM-8/62B) to decode trajectories (all thoughts, actions, observations) conditioned on input questions/claims. More details are in Appendix B.1.</p>
<h3>3.3 RESULTS AND OBSERVATIONS</h3>
<p><strong>ReAct outperforms Act consistently</strong> Table 1 shows HotpotQA and Fever results using PaLM-540B as the base model with different prompting methods. We note that ReAct is better than Act on both tasks, demonstrating the value of reasoning to guide acting, especially for synthesizing the final answer, as shown in Figure 1 (1c-d). Fine-tuning results 3 also confirm the benefit of reasoning traces for more informed acting.</p>
<p>^{3}Of all trajectories with correct final answers, those with 7 steps on HotpotQA and 5 steps on FEVER only take up 0.84% and 1.33% respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Definition</th>
<th style="text-align: center;">ReAct</th>
<th style="text-align: center;">CoT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Success</td>
<td style="text-align: center;">True positive</td>
<td style="text-align: center;">Correct reasoning trace and facts</td>
<td style="text-align: center;">$94 \%$</td>
<td style="text-align: center;">$86 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">False positive</td>
<td style="text-align: center;">Hallucinated reasoning trace or facts</td>
<td style="text-align: center;">$6 \%$</td>
<td style="text-align: center;">$14 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Failure</td>
<td style="text-align: center;">Reasoning error</td>
<td style="text-align: center;">Wrong reasoning trace (including failing to recover from repetitive steps)</td>
<td style="text-align: center;">$47 \%$</td>
<td style="text-align: center;">$16 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Search result error</td>
<td style="text-align: center;">Search return empty or does not contain useful information</td>
<td style="text-align: center;">$23 \%$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Hallucination</td>
<td style="text-align: center;">Hallucinated reasoning trace or facts</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$56 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Label ambiguity</td>
<td style="text-align: center;">Right prediction but did not match the label precisely</td>
<td style="text-align: center;">$29 \%$</td>
<td style="text-align: center;">$28 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2: Types of success and failure modes of ReAct and CoT on HotpotQA, as well as their percentages in randomly selected examples studied by human.</p>
<p>ReAct vs. CoT On the other hand, ReAct outperforms CoT on Fever ( 60.9 vs. 56.3 ) and slightly lags behind CoT on HotpotQA ( 27.4 vs. 29.4). Fever claims for SUPPORTS/REFUTES might only differ by a slight amount (see Appendix D.1), so acting to retrieve accurate and up-to-date knowledge is vital. To better understand the behavioral difference between ReAct and CoT on HotpotQA, we randomly sampled 50 trajectories with correct and incorrect answers (judged by EM) from ReAct and CoT respectively (thus 200 examples in total), and manually labeled their success and failure modes in Table 2. Some key observations are as follows:
A) Hallucination is a serious problem for CoT, resulting in much higher false positive rate than ReAct ( $14 \%$ vs. $6 \%$ ) in success mode, and make up its major failure mode ( $56 \%$ ). In contrast, the problem solving trajectory of ReAct is more grounded, fact-driven, and trustworthy, thanks to the access of an external knowledge base.
B) While interleaving reasoning, action and observation steps improves ReAct's groundedness and trustworthiness, such a structural constraint also reduces its flexibility in formulating reasoning steps, leading to more reasoning error rate than CoT. we note that there is one frequent error pattern specific to ReAct, in which the model repetitively generates the previous thoughts and actions, and we categorize it as part of "reasoning error" as the model fails to reason about what the proper next action to take and jump out of the loop ${ }^{4}$.
C) For ReAct, successfully retrieving informative knowledge via search is critical. Noninformative search, which counts for $23 \%$ of the error cases, derails the model reasoning and gives it a hard time to recover and reformulate thoughts. This is perhaps an expected trade-off between factuality and flexibility, which motivates our proposed strategies of combining two methods.</p>
<p>We provide examples for each success and failure modes in Appendix E.1. We also find some HotpotQA questions may contain outdated answer labels, see Figure 4 for example.</p>
<p>ReAct + CoT-SC perform best for prompting LLMs Also shown in Table 1, the best prompting method on HotpotQA and Fever are ReAct $\rightarrow$ CoT-SC and CoT-SC $\rightarrow$ ReAct respectively. Furthermore, Figure 2 shows how different methods perform with respect to the number of CoT-SC samples used. While two ReAct + CoT-SC methods are advantageous at one task each, they both significantly and consistently outperform CoT-SC across different number of samples, reaching CoT-SC performance with 21 samples using merely 3-5 samples. These results indicate the value of properly combining model internal knowledge and external knowledge for reasoning tasks.</p>
<p>ReAct performs best for fine-tuning Figure 3 shows the scaling effect of prompting/finetuning four methods (Standard, CoT, Act, ReAct) on HotpotQA. With PaLM-8/62B, prompting ReAct performs worst among four methods due to the difficulty to learn both reasoning and acting from in-context examples. However, when finetuned with just 3,000 examples, ReAct becomes the best method among the four, with PaLM-8B finetuned ReAct outperforming all PaLM-62B prompting methods, and PaLM-62B finetuned ReAct outperforming all 540B prompting methods. In contrast, finetuning Standard or CoT is significantly worse than finetuning ReAct or Act for both PaLM8/62B, as the former essentially teaches models to memorize (potentially hallucincated) knowledge facts, and the latter teaches models how to (reason and) act to access information from Wikipedia, a more generalizable skill for knowledge reasoning. As all prompting methods are still significantly far from domain-specific state-of-the-art approaches (Table 1), we believe finetuning with more human-written data might be a better way to unleash the power of ReAct.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Scaling results for prompting and finetuning on HotPotQA with ReAct (ours) and baselines.</p>
<h1>4 DECISION MAKING TASKS</h1>
<p>We also test ReAct on two language-based interactive decision-making tasks, ALFWorld and WebShop, both of which feature complex environments that require agents to act over long horizons with sparse rewards, warranting the need for reasoning to act and explore effectively.</p>
<p>ALFWorld ALFWorld (Shridhar et al., 2020b) (Figure 1(2)) is a synthetic text-based game designed to align with the embodied ALFRED benchmark (Shridhar et al., 2020a). It includes 6 types of tasks in which an agent needs to achieve a high-level goal (e.g. examine paper under desklamp) by navigating and interacting with a simulated household via text actions (e.g. go to coffeetable 1, take paper 2, use desklamp 1). A task instance can have more than 50 locations and take an expert policy more than 50 steps to solve, thus challenging an agent to plan and track subgoals, as well as explore systematically (e.g. check all desks one by one for desklamp). In particular, one challenge built into ALFWorld is the need to determine likely locations for common household items (e.g. desklamps will likely be on desks, shelfs, or dressers), making this environment a good fit for LLMs to exploit their pretrained commonsense knowledge. To prompt ReAct, we randomly annotate three trajectories from the training set for each task type, where each trajectory includes sparse thoughts that (1) decompose the goal, (2) track subgoal completion, (3) determine the next subgoal, and (4) reason via commonsense where to find an object and what to do with it. We show prompts used for ALFWorld in Appendix C.4. Following Shridhar et al. (2020b), we evaluate on 134 unseen evaluation games in a task-specific setup. For robustness, we construct 6 prompts for each task type through each permutation of 2 annotated trajectories from the 3 we annotate. Act prompts are constructed using the same trajectories, but without thoughts - since task instances are randomly chosen from the training set, it favors neither ReAct nor Act and provides a fair and controlled comparison to test the importance of sparse thoughts. For baselines, we use BUTLER (Shridhar et al., 2020b), an imitation learning agent trained on $10^{5}$ expert trajectories for each task type ${ }^{5}$.</p>
<p>WebShop Can ReAct also interact with noisy real-world language environments for practical applications? We investigate WebShop (Yao et al., 2022), a recently proposed online shopping website environment with 1.18M real-world products and 12k human instructions. Unlike ALFWorld, Webshop contains a high variety of structured and unstructured texts (e.g. product titles, descriptions, and options crawled from Amazon), and requires an agent to purchase a product based on a user instruction (e.g. "I am looking for a nightstand with drawers. It should have a nickel finish, and priced lower than $\$ 140$ ") through web interactions (e.g. search "nightstand drawers", choose buttons such as "color: modern-nickel-white" or "back to search"). This task is evaluated by average score (percentage of desired attributes covered by the chosen product averaged across all episodes) and success rate (percentage of episodes where the chosen product satisfies all requirements) on 500 test instructions. We formulate Act prompts with actions to search, choose product, choose options, and buy, with ReAct prompts additionally reasoning to determine what to explore, when to buy, and what products options are relevant to the instruction. See Table 6 for an example prompt, and Table 10 for model predictions in the Appendix. We compare to an imitation learning (IL) method</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Pick</th>
<th style="text-align: center;">Clean</th>
<th style="text-align: center;">Heat</th>
<th style="text-align: center;">Cool</th>
<th style="text-align: center;">Look</th>
<th style="text-align: center;">Pick 2</th>
<th style="text-align: center;">All</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Act (best of 6)</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">$\mathbf{4 1}$</td>
<td style="text-align: center;">45</td>
</tr>
<tr>
<td style="text-align: left;">ReAct (avg)</td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">57</td>
</tr>
<tr>
<td style="text-align: left;">ReAct (best of 6)</td>
<td style="text-align: center;">$\mathbf{9 2}$</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">$\mathbf{9 6}$</td>
<td style="text-align: center;">86</td>
<td style="text-align: center;">$\mathbf{7 8}$</td>
<td style="text-align: center;">$\mathbf{4 1}$</td>
<td style="text-align: center;">$\mathbf{7 1}$</td>
</tr>
<tr>
<td style="text-align: left;">ReAct-IM (avg)</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">48</td>
</tr>
<tr>
<td style="text-align: left;">ReAct-IM (best of 6)</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">$\mathbf{6 8}$</td>
<td style="text-align: center;">87</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">53</td>
</tr>
<tr>
<td style="text-align: left;">BUTLER $_{g}$ (best of 8)</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">22</td>
</tr>
<tr>
<td style="text-align: left;">BUTLER (best of 8)</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">$\mathbf{1 0 0}$</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">37</td>
</tr>
</tbody>
</table>
<p>Table 3: AlfWorld task-specific success rates (\%). BUTLER and BUTLER $_{g}$ results are from Table 4 of Shridhar et al. (2020b). All methods use greedy decoding, except that BUTLER uses beam search.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Score</th>
<th style="text-align: center;">SR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Act</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">30.1</td>
</tr>
<tr>
<td style="text-align: center;">ReAct</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">40.0</td>
</tr>
<tr>
<td style="text-align: center;">IL</td>
<td style="text-align: center;">59.9</td>
<td style="text-align: center;">29.1</td>
</tr>
<tr>
<td style="text-align: center;">IL+RL</td>
<td style="text-align: center;">62.4</td>
<td style="text-align: center;">28.7</td>
</tr>
<tr>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">59.6</td>
</tr>
<tr>
<td style="text-align: center;">Expert</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 4: Score and success rate (SR) on Webshop. IL/IL+RL taken from Yao et al. (2022).
trained with 1,012 human annotated trajectories, and a imitation + reinforcement learning (IL + RL) method additionally trained with 10,587 training instructions.</p>
<p>Results ReAct outperforms Act on both ALFWorld (Table 3) and Webshop (Table 4). On ALFWorld, the best ReAct trial achieves an average success rate of $71 \%$, significantly outperforming the best Act (45\%) and BUTLER (37\%) trials. In fact, even the worse ReAct trial (48\%) beats the best trial of both methods. Moreover, the advantage of ReAct over Act is consistent across six controlled trials, with relative performance gain ranging from $33 \%$ to $90 \%$ and averaging $62 \%$. Qualitatively, we saw that, without any thoughts at all, Act fails to correctly decompose goals into smaller subgoals, or loses track of the current state of the environment. Example trajectories comparing ReAct and Act can be found in Appendix D.2.1 and Appendix D.2.2.</p>
<p>On Webshop, one-shot Act prompting already performs on par with IL and IL+RL methods. With additional sparse reasoning, ReAct achieves significantly better performance, with an absolute 10\% improvement over the previous best success rate. By checking examples, we find that ReAct is more likely to identify instruction-relevant products and options by reasoning to bridge the gap between noisy observations and actions (e.g. "For 'space-saving ottoman bench for living room', the item has options ' $39 \times 18 \times 18$ inch' and 'blue' and seems good to buy."). However, existing methods are still far from the performance of expert humans (Table 4), who perform significantly more product explorations and query re-formulations that are still challenging for prompting-based methods.</p>
<p>On the value of internal reasoning vs. external feedback To our knowledge, ReAct is the first demonstration of combined reasoning and action using an LLM applied to an interactive environment within a closed-loop system. Perhaps the closest prior work is Inner Monologue (IM), from Huang et al. (2022b), in which actions from an embodied agent are motivated by an eponymous "inner monologue". However, IM's "inner monologue" is limited to observations of the environment state and what needs to be completed by the agent for the goal to be satisfied. In contrast, the reasoning traces in ReAct for decision making is flexible and sparse, allowing diverse reasoning types (see Section 2) to be induced for different tasks.</p>
<p>To demonstrate the differences between ReAct and IM, and to highlight the importance of internal reasoning vs. simple reactions to external feedback, we ran an ablation experiment using a thought pattern composed of IM-like dense external feedback. As can be seen in Table 3, ReAct substantially outperforms IM-style prompting (ReAct-IM) ( 71 vs. 53 overall success rate), with consistent advantages on five out of six tasks. Qualitatively, we observed that ReAct-IM often made mistakes in identifying when subgoals were finished, or what the next subgoal should be, due to a lack of highlevel goal decomposition. Additionally, many ReAct-IM trajectories struggled to determine where an item would likely be within the ALFWorld environment, due to a lack of commonsense reasoning. Both shortcomings can be addressed in the ReAct paradigm. More details about ReAct-IM is in Appendix B.2. An example prompt for ReAct-IM can be found in Appendix C.4, and an example trajectory in Appendix D.2.3.</p>
<h1>5 Related Work</h1>
<p>Language model for reasoning Perhaps the most well-known work of using LLMs for reasoning is Chain-of-Thought (CoT) (Wei et al., 2022), which reveals the ability of LLMs to formulate their own "thinking procedure" for problem solving. Several follow-up works have since been performed, including least-to-most prompting for solving complicated tasks (Zhou et al., 2022), zero-shotCoT (Kojima et al., 2022), and reasoning with self-consistency (Wang et al., 2022a). Recently, (Madaan \&amp; Yazdanbakhsh, 2022) systematically studied the formulation and structure of CoT, and observed that the presence of symbols, patterns and texts is crucial to the effectiveness of CoT. Other work has also been extended to more sophisticated reasoning architecture beyond simple prompting. For example Selection-Inference (Creswell et al., 2022) divides the reasoning process into two steps of "selection" and "inference". STaR (Zelikman et al., 2022) bootstraps the reasoning process by finetuning the model on correct rationales generated by the model itself. Faithful reasoning (Creswell \&amp; Shanahan, 2022) decomposes multi-step reasoning into three steps, each performed by a dedicated LM respectively. Similar approaches like Scratchpad (Nye et al., 2021), which finetunes a LM on intermediate computation steps, also demonstrate improvement on multi-step computation problems. In contrast to these methods, ReAct performs more than just isolated, fixed reasoning, and integrates model actions and their corresponding observations into a coherent stream of inputs for the model to reason more accurately and tackle tasks beyond reasoning (e.g. interactive decision making).</p>
<p>Language model for decision making The strong capability of LLMs has enabled them to perform tasks beyond language generation, and it is becoming more popular to take advantage of LLMs as a policy model for decision making, especially in interactive environments. WebGPT (Nakano et al., 2021) uses an LM to interact with web browsers, navigate through web pages, and infer answers to complicated questions from ELI5 (Fan et al., 2019). In comparison to ReAct, WebGPT does not explicitly model the thinking and reasoning procedure, instead rely on expensive human feedback for reinforcement learning. In conversation modeling, chatbots like BlenderBot (Shuster et al., 2022b) and Sparrow (Glaese et al., 2022) and task-oriented dialogue systems like SimpleTOD (Hosseini-Asl et al., 2020) also train LMs to make decision about API calls. Unlike ReAct, they do not explicitly consider the reasoning procedure either, and also relies on expensive datasets and human feedback collections for policy learning. In contrast, ReAct learns a policy in a much cheaper way, since the decision making process only requires language description of the reasoning procedure. ${ }^{6}$
LLMS have also been increasingly employed in interactive and embodied environments for planning and decision making. Perhaps most relevant to ReAct in this respect are SayCan (Ahn et al., 2022) and Inner Monologue (Huang et al., 2022b), which use LLMs for robotic action planning and decision making. In SayCan, LLMs were prompted to directly predict possible actions a robot can take, which is then reranked by an affordance model grounded on the visual environments for final prediction. Inner Monologue made further improvements by adding the eponymous "inner monologue", which is implemented as injected feedback from the environment. To our knowledge, Inner Monologue is the first work that demonstrates such a closed-loop system, which ReAct builds on. However, we argue that Inner Monologue does not truly comprise of inner thoughts - this is elaborated in Section 4. We also note that leveraging language as semantically-rich inputs in the process of interactive decision making has been shown to be successful under other settings (Abramson et al., 2020; Karamcheti et al., 2021; Huang et al., 2022a; Li et al., 2022). It is becoming more evident that with the help of LLMs, language as a fundamental cognitive mechanism will play a critical role in interaction and decision making. What is more, progress in LLMs has also inspired the development of versatile and generalist agents like Reed et al. (2022).</p>
<h2>6 CONCLUSION</h2>
<p>We have proposed ReAct - a simple yet effective method for synergizing reasoning and acting in large language models. Through a diverse set of experiments on multi-hop question-answering, fact checking, and interactive decision-making tasks, we show that ReAct leads to superior performance with interpretable decision traces. Despite the simplicity of our method, complex tasks with large action spaces require more demonstrations to learn well, which unfortunately can easily go beyond the input length limit of in-context learning. We explore the fine-tuning approach on HotpotQA</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>with initial promising results, but learning from more high-quality human annotations will be the desiderata to further improve the performance. Scaling up ReAct with multi-task training and combining it with complementary paradigms like reinforcement learning could result in stronger agents that further unlock the potential of LLMs for more applications.</p>
<h1>ACKNOWLEDGMENTS</h1>
<p>We thank the support and feedback of many people from Google Brain team and Princeton NLP Group. This work was supported in part by the National Science Foundation under Grant No. 2107048. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p>
<h2>REPRODUCIbility STATEMENT</h2>
<p>Our main experiments are done on PaLM (Chowdhery et al., 2022), which is not an openly accessible model yet. To increase reproducibility, we have included all used prompts in Appendix C, additional experiments using GPT-3 (Brown et al., 2020) in Appendix A.1, and associated GPT-3 ReAct prompting code at https://anonymous.4open.science/r/ReAct-2268/.</p>
<h2>ETHICS STATEMENT</h2>
<p>ReAct prompts large language models to generate more human interpretable, diagnosable, and controllable task-solving trajectories than previous methods. However, hooking up a large language model with an action space to interact with external environments (e.g. the web, physical environments) has potential dangers, e.g. looking up inappropriate or private information, or taking harmful actions in an environment. Our experiments minimize such risks by limiting the interactions to specific websites (Wikipedia or WebShop) that are free of private information, without any dangerous actions in the action space design (i.e. models cannot really buy products on WebShop the research benchmark, or edit Wikipedia). We believe researchers should be aware of such risks before designing more extensive experiments in the future.</p>
<h2>REFERENCES</h2>
<p>Josh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin, Rachita Chhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, Petko Georgiev, Aurelia Guy, Tim Harley, Felix Hill, Alden Hung, Zachary Kenton, Jessica Landon, Timothy Lillicrap, Kory Mathewson, Sofia Mokrá, Alistair Muldal, Adam Santoro, Nikolay Savinov, Vikrant Varma, Greg Wayne, Duncan Williams, Nathaniel Wong, Chen Yan, and Rui Zhu. Imitating interactive intelligence, 2020. URL https://arxiv.org/abs/2012.05672.</p>
<p>Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do as i can, not as i say: Grounding language in robotic affordances, 2022. URL https://arxiv.org/abs/2204.01691.</p>
<p>Ben Alderson-Day and Charles Fernyhough. Inner speech: development, cognitive functions, phenomenology, and neurobiology. Psychological bulletin, 141(5):931, 2015.</p>
<p>Alan Baddeley. Working memory. Science, 255(5044):556-559, 1992.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Antonia Creswell and Murray Shanahan. Faithful reasoning using large language models, 2022. URL https://arxiv.org/abs/2208.14271.</p>
<p>Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning, 2022. URL https://arxiv.org/abs/ 2205.09712 .</p>
<p>Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3558-3567, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://aclanthology.org/ P19-1346.</p>
<p>Charles Fernyhough. Vygotsky, luria, and the social brain. Self and social regulation: Social interaction and the development of social understanding and executive functions, pp. 56-79, 2010.</p>
<p>Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Soña Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. Improving alignment of dialogue agents via targeted human judgements, 2022. URL https://storage.googleapis.com/deepmind-media/DeepMind. com/Authors-Notes/sparrow/sparrow-final.pdf.</p>
<p>Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. A simple language model for task-oriented dialogue. Advances in Neural Information Processing Systems, 33:20179-20191, 2020.</p>
<p>Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207, 2022a.</p>
<p>Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022b.</p>
<p>Siddharth Karamcheti, Megha Srivastava, Percy Liang, and Dorsa Sadigh. Lila: Language-informed latent actions. In CoRL, pp. 1379-1390, 2021. URL https : / /proceedings.mlr.press/ v164/karamcheti22a.html.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.</p>
<p>Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internetaugmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115, 2022.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33: $9459-9474,2020$.</p>
<p>Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba, and Yuke Zhu. Pre-trained language models for interactive decision-making, 2022. URL https : //arxiv.org/abs/2202.01771.</p>
<p>Aleksandr Romanovich Luria. Ls vygotsky and the problem of localization of functions. Neuropsychologia, 3(4):387-392, 1965.</p>
<p>Aman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes two to tango, 2022. URL https://arxiv.org/abs/2209.07686.</p>
<p>Vincent Micheli and François Fleuret. Language models are few-shot butlers. arXiv preprint arXiv:2104.07972, 2021.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted question-answering with human feedback, 2021. URL https://arxiv. org/abs/2112.09332.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models, 2021. URL https://arxiv.org/abs/2112.00114.</p>
<p>Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent, 2022. URL https : //arxiv.org/abs/2205.06175.</p>
<p>Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10740-10749, 2020a.</p>
<p>Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020b.</p>
<p>Kurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, and Jason Weston. Language models that seek for knowledge: Modular search \&amp; generation for dialogue and prompt completion. arXiv preprint arXiv:2203.13224, 2022a.</p>
<p>Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, and Jason Weston. Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage, 2022b. URL https://arxiv.org/abs/2208.03188.</p>
<p>James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a large-scale dataset for fact extraction and verification. arXiv preprint arXiv:1803.05355, 2018.</p>
<p>Lev S Vygotsky. Thinking and speech. The collected works of LS Vygotsky, 1:39-285, 1987.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models, 2022a. URL https://arxiv.org/abs/2203.11171.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationale-augmented ensembles in language models. arXiv preprint arXiv:2207.00747, 2022b.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.</p>
<p>Shunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. Keep CALM and explore: Language models for action generation in text-based games. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 8736-8754, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.704. URL https://aclanthology.org/2020.emnlp-main. 704.</p>
<p>Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. arXiv preprint arXiv:2207.01206, 2022.</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning, 2022. URL https://arxiv.org/abs/2203.14465.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models, 2022. URL https://arxiv.org/abs/2205.10625.</p>
<p>Yunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen, and Xueqi Cheng. Adaptive information seeking for open-domain question answering. arXiv preprint arXiv:2109.06747, 2021.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">PaLM-540B</th>
<th style="text-align: center;">GPT-3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">HotpotQA (exact match)</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">$\mathbf{3 0 . 8}$</td>
</tr>
<tr>
<td style="text-align: center;">ALFWorld (success rate \%)</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">$\mathbf{7 8 . 4}$</td>
</tr>
</tbody>
</table>
<p>Table 5: ReAct prompting results using PaLM-540B vs. GPT-3 (text-davinci-002, greedy decoding). On HotpotQA, we randomly sample a subset of 500 validation questions. On ALFWorld, we use all 134 unseen validation task instances, and use the best prompt set according to PaLM-540B.</p>
<p>We run additional GPT-3 (Brown et al., 2020) experiments to confirm ReAct prompting performance is general across different large language models. As shown in Table 5, GPT-3 (text-davinci-002, greedy decoding) consistently outperforms PaLM-540B on HotpotQA and ALFWorld, possibly because it is finetuned with human instruction following. This indicates ReAct prompting is effective across different large language models on different tasks. The code for these experiments are at https://react-lm.github.io/.</p>
<h1>A. 2 ReAct obtains up-to-date knowledge on HotpotQA</h1>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Another example HotpotQA question, where the original label is outdated. Only ReAct is able to obtain the up-to-date answer thanks to real-world web interaction plus reasoning.
During trajectory inspection, we also find that sometimes ReAct does not agree with dataset labels as the labels themselves could be outdated. For example, as shown in Figure 4, the question asks about the size of a hotel, which increased from the HotpotQA construction time. While Standard and CoT give wrong answers due to hallucination, Act fails despite the access of real-world web interaction, due to a lack of reasoning to guide how to interact with the Internet for QA. Only ReAct is able to retrieve up-to-date information from the Internet and provide a reasonable answer. Therefore, better incorporation of reasoning abilities might benefit recent Internet-augmented language models (Nakano et al., 2021; Lazaridou et al., 2022; Shuster et al., 2022a) for up-to-date task solving.</p>
<h2>A. 3 Human-in-the-loop behavior correction on AlfWorLD</h2>
<p>We also explore human-in-the-loop interaction with ReAct, to allow a human to inspect and edit ReAct's reasoning traces. Figure 5 shows that by simply removing a hallucinating sentence in Act 17 and adding some hints in Act 23, ReAct can be made to change its behavior drastically to align with these human thought edits and succeed in the task. From a human perspective, solving such a task becomes significantly easier, from typing tens of actions to only editing a couple of thoughts, which enables new forms of human-machine collaboration. We note that such a policy edit on-the-go</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: A human-in-the-loop behavior correction example with ReAct in AlfWorld. (a) ReAct trajectory fails due to a hallucinating thought (Act 17). (b) By a human simply editing two thoughts (Act 17, 23), the ReAct trajectory produces desirable reasoning traces and actions and succeeds.
is difficult for Act and previous RL methods, as a human cannot change the model parameters, and changing a few actions might not edit the rest of the model behavior. This paradigm is also more than human dialogue to update the goal or subgoal as in Huang et al. (2022b) — while editing ReAct thoughts can do these, it can also modify the model's internal belief, reasoning styles, or anything the flexible thought space supports, for better task solving. We believe this is an exciting direction for human alignment and leave more systematic study as future work.</p>
<h1>B EXPERIMENT DETAILS</h1>
<h2>B. 1 HotpotQA Finetuning Details</h2>
<p>For all finetuning we use a batch size of 64 . On PaLM-8B, we finetune ReAct and Act methods for 4,000 steps and Standard and CoT methods for 2,000 steps. On PaLM-62B, we finetune ReAct and Act methods for 4,000 steps and Standard and CoT methods for 1,000 steps. We find ReAct and Act methods generally benefit from more training steps (and more training data), while Standard and CoT methods degrade soon after finetuning.</p>
<h2>B. 2 AlfWorld IM-Style Details</h2>
<p>For the IM-style ablation, the same expert trajectories used in ReAct are reannotated with dense external feedback thoughts within these trajectories, that limit ReAct-IM to only think about (1) decomposing the current goal and (2) the current subgoal that needs to be completed. In particular, ReAct-IM lacks thoughts that (1) determine when a subgoal is completed (2) determine what the next subgoal should be (3) inducing the LLM to refer to its internal pretraining knowledge to identify where items can be within the environment.</p>
<h1>C Prompts</h1>
<h2>C. 1 HotpotQA</h2>
<p>Hotpot QA Prompts</p>
<table>
<thead>
<tr>
<th style="text-align: center;">HotpotQA</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?</td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">1,800 to $7,000 \mathrm{ft}$</td>
</tr>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">Musician and satirist Allie Goertz wrote a song about the "The Simpsons" character Milhouse, who Matt Groening named after who?</td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">Richard Nixon</td>
</tr>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">Which documentary is about Finnish rock groups, Adam Clayton Powell or The Saimaa Gesture?</td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">The Saimaa Gesture</td>
</tr>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">What profession does Nicholas Ray and Elia Kazan have in common?</td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">director, screenwriter, actor</td>
</tr>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">Which magazine was started first Arthur's Magazine or First for Women?</td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">Arthur's Magazine</td>
</tr>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">Were Pavel Urysohn and Leonid Levin known for the same type of work?</td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?</td>
</tr>
<tr>
<td style="text-align: center;">Action 1</td>
<td style="text-align: center;">Search[Colorado orogeny]</td>
</tr>
<tr>
<td style="text-align: center;">Observation 1</td>
<td style="text-align: center;">The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.</td>
</tr>
<tr>
<td style="text-align: center;">Action 2</td>
<td style="text-align: center;">Lookup[eastern sector]</td>
</tr>
<tr>
<td style="text-align: center;">Observation 2</td>
<td style="text-align: center;">(Result 1 / 1) The eastern sector extends into the High Plains and is called the Central Plains orogeny.</td>
</tr>
<tr>
<td style="text-align: center;">Action 3</td>
<td style="text-align: center;">Search[High Plains]</td>
</tr>
<tr>
<td style="text-align: center;">Observation 3</td>
<td style="text-align: center;">High Plains refers to one of two distinct land regions:</td>
</tr>
<tr>
<td style="text-align: center;">Action 4</td>
<td style="text-align: center;">Search[High Plains (United States)]</td>
</tr>
<tr>
<td style="text-align: center;">Observation 4</td>
<td style="text-align: center;">The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft ( 550 to 2,130 $\mathrm{m}) .[3]$</td>
</tr>
<tr>
<td style="text-align: center;">Action 5</td>
<td style="text-align: center;">Finish $[1,800$ to $7,000 \mathrm{ft}]$</td>
</tr>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">Musician and satirist Allie Goertz wrote a song about the "The Simpsons" character Milhouse, who Matt Groening named after who?</td>
</tr>
<tr>
<td style="text-align: center;">Action 1</td>
<td style="text-align: center;">Search[Milhouse]</td>
</tr>
<tr>
<td style="text-align: center;">Observation 1</td>
<td style="text-align: center;">Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.</td>
</tr>
<tr>
<td style="text-align: center;">Action 2</td>
<td style="text-align: center;">Lookup[named after]</td>
</tr>
<tr>
<td style="text-align: center;">Observation 2</td>
<td style="text-align: center;">(Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.</td>
</tr>
<tr>
<td style="text-align: center;">Action 3</td>
<td style="text-align: center;">Finish[Richard Nixon]</td>
</tr>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">Which documentary is about Finnish rock groups, Adam Clayton Powell or The Saimaa Gesture?</td>
</tr>
<tr>
<td style="text-align: center;">Action 1</td>
<td style="text-align: center;">Search[Adam Clayton Powell]</td>
</tr>
<tr>
<td style="text-align: center;">Observation 1</td>
<td style="text-align: center;">Could not find [Adam Clayton Powell]. Similar: ['Adam Clayton Powell III', 'Seventh Avenue (Manhattan)', 'Adam Clayton Powell Jr. State Office Building', 'Isabel Washington Powell', 'Adam Powell', 'Adam Clayton Powell (film)', 'Giancarlo Esposito'].</td>
</tr>
<tr>
<td style="text-align: center;">Action 2</td>
<td style="text-align: center;">Search[Adam Clayton Powell (film)]</td>
</tr>
<tr>
<td style="text-align: center;">Observation 2</td>
<td style="text-align: center;">Adam Clayton Powell is a 1989 American documentary film directed by Richard Kilberg. The film is about the rise and fall of influential African-American politician Adam Clayton Powell Jr.[3][4] It was later aired as part of the PBS series The American Experience.</td>
</tr>
<tr>
<td style="text-align: center;">Action 3</td>
<td style="text-align: center;">Finish[The Saimaa Gesture]</td>
</tr>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">What profession does Nicholas Ray and Elia Kazan have in common?</td>
</tr>
<tr>
<td style="text-align: center;">Action 1</td>
<td style="text-align: center;">Search[Nicholas Ray]</td>
</tr>
<tr>
<td style="text-align: center;">Observation 1</td>
<td style="text-align: center;">Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16, 1979) was an American film director, screenwriter, and actor best known for the 1955 film Rebel Without a Cause.</td>
</tr>
<tr>
<td style="text-align: center;">Action 2</td>
<td style="text-align: center;">Search[Elia Kazan]</td>
</tr>
<tr>
<td style="text-align: center;">Observation 2</td>
<td style="text-align: center;">Elia Kazan was an American film and theatre director, producer, screenwriter and actor.</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Action 3</th>
<th style="text-align: center;">Finish[director, screenwriter, actor]</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">Which magazine was started first Arthur's Magazine or First for Women?</td>
</tr>
<tr>
<td style="text-align: center;">Action 1</td>
<td style="text-align: center;">Search[Arthur's Magazine]</td>
</tr>
<tr>
<td style="text-align: center;">Observation 1</td>
<td style="text-align: center;">Arthur's Magazine (1844-1846) was an American literary periodical published in Philadelphia in the 19th century.</td>
</tr>
<tr>
<td style="text-align: center;">Action 2</td>
<td style="text-align: center;">Search[First for Women]</td>
</tr>
<tr>
<td style="text-align: center;">Observation 2</td>
<td style="text-align: center;">First for Women is a woman's magazine published by Bauer Media Group in the USA. [1] The magazine was started in 1989.</td>
</tr>
<tr>
<td style="text-align: center;">Action 3</td>
<td style="text-align: center;">Finish[Arthur's Magazine]</td>
</tr>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">Were Pavel Urysohn and Leonid Levin known for the same type of work?</td>
</tr>
<tr>
<td style="text-align: center;">Action 1</td>
<td style="text-align: center;">Search[Pavel Urysohn]</td>
</tr>
<tr>
<td style="text-align: center;">Observation 1</td>
<td style="text-align: center;">Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet mathematician who is best known for his contributions in dimension theory.</td>
</tr>
<tr>
<td style="text-align: center;">Action 2</td>
<td style="text-align: center;">Search[Leonid Levin]</td>
</tr>
<tr>
<td style="text-align: center;">Observation 2</td>
<td style="text-align: center;">Leonid Anatolievich Levin is a Soviet-American mathematician and computer scientist.</td>
</tr>
<tr>
<td style="text-align: center;">Action 3</td>
<td style="text-align: center;">Finish[yes]</td>
</tr>
<tr>
<td style="text-align: center;">回</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?</td>
</tr>
<tr>
<td style="text-align: center;">Thought</td>
<td style="text-align: center;">Let's think step by step. The eastern sector of Colorado orogeny extends into the High Plains. High Plains rise in elevation from around 1,800 to 7,000 ft , so the answer is 1,800 to 7,000 ft .</td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">1,800 to 7,000 ft</td>
</tr>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">Musician and satirist Allie Goertz wrote a song about the "The Simpsons" character Milhouse, who Matt Groening named after who?</td>
</tr>
<tr>
<td style="text-align: center;">Thought</td>
<td style="text-align: center;">Let's think step by step. Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.</td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">Richard Nixon</td>
</tr>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">Which documentary is about Finnish rock groups, Adam Clayton Powell or The Saimaa Gesture?</td>
</tr>
<tr>
<td style="text-align: center;">Thought</td>
<td style="text-align: center;">Let's think step by step. Adam Clayton Powell (film) is a documentary about an African-American politician, not Finnish rock groups. So the documentary about Finnish rock groups must instead be The Saimaa Gesture.</td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">The Saimaa Gesture</td>
</tr>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">What profession does Nicholas Ray and Elia Kazan have in common?</td>
</tr>
<tr>
<td style="text-align: center;">Thought</td>
<td style="text-align: center;">Let's think step by step. Professions of Nicholas Ray are director, screenwriter, and actor. Professions of Elia Kazan are director, producer, screenwriter, and actor. So profession Nicholas Ray and Elia Kazan have in common is director, screenwriter, and actor.</td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">director, screenwriter, actor</td>
</tr>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">Which magazine was started first Arthur's Magazine or First for Women?</td>
</tr>
<tr>
<td style="text-align: center;">Thought</td>
<td style="text-align: center;">Let's think step by step. Arthur's Magazine was started in 1844. First for Women was started in 1989. 1844 (Arthur's Magazine) &lt; 1989 (First for Women), so Arthur's Magazine was started first.</td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">Arthur's Magazine</td>
</tr>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">Were Pavel Urysohn and Leonid Levin known for the same type of work?</td>
</tr>
<tr>
<td style="text-align: center;">Thought</td>
<td style="text-align: center;">Let's think step by step. Pavel Urysohn is a mathematician. Leonid Levin is a mathematician and computer scientist. So Pavel Urysohn and Leonid Levin have the same type of work.</td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: center;">Nature</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?</td>
</tr>
<tr>
<td style="text-align: center;">Thought 1</td>
<td style="text-align: center;">I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.</td>
</tr>
<tr>
<td style="text-align: center;">Action 1</td>
<td style="text-align: center;">Search[Colorado orogeny]</td>
</tr>
<tr>
<td style="text-align: center;">Observation 1</td>
<td style="text-align: center;">The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.</td>
</tr>
<tr>
<td style="text-align: center;">Thought 2</td>
<td style="text-align: center;">It does not mention the eastern sector. So I need to look up eastern sector.</td>
</tr>
<tr>
<td style="text-align: center;">Action 2</td>
<td style="text-align: center;">Lookup[eastern sector]</td>
</tr>
<tr>
<td style="text-align: center;">Observation 2</td>
<td style="text-align: center;">(Result 1 / 1) The eastern sector extends into the High Plains and is called the Central Plains orogeny.</td>
</tr>
<tr>
<td style="text-align: center;">Thought 3</td>
<td style="text-align: center;">The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.</td>
</tr>
<tr>
<td style="text-align: center;">Action 3</td>
<td style="text-align: center;">Search[High Plains]</td>
</tr>
<tr>
<td style="text-align: center;">Observation 3</td>
<td style="text-align: center;">High Plains refers to one of two distinct land regions</td>
</tr>
<tr>
<td style="text-align: center;">Thought 4</td>
<td style="text-align: center;">I need to instead search High Plains (United States).</td>
</tr>
<tr>
<td style="text-align: center;">Action 4</td>
<td style="text-align: center;">Search[High Plains (United States)]</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Observation 4</th>
<th style="text-align: center;">The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to $7,000 \mathrm{ft}(550$ to 2,130 $\mathrm{m}) .[3]$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Thought 5</td>
<td style="text-align: center;">High Plains rise in elevation from around 1,800 to $7,000 \mathrm{ft}$, so the answer is 1,800 to $7,000 \mathrm{ft}$.</td>
</tr>
<tr>
<td style="text-align: center;">Action 5</td>
<td style="text-align: center;">Finish $[1,800$ to $7,000 \mathrm{ft}]$</td>
</tr>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">Musician and satirist Allie Goertz wrote a song about the "The Simpsons" character Milhouse, who Matt Groening named after who?</td>
</tr>
<tr>
<td style="text-align: center;">Thought 1</td>
<td style="text-align: center;">The question simplifies to "The Simpsons" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.</td>
</tr>
<tr>
<td style="text-align: center;">Action 1</td>
<td style="text-align: center;">Search [Milhouse]</td>
</tr>
<tr>
<td style="text-align: center;">Observation 1</td>
<td style="text-align: center;">Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.</td>
</tr>
<tr>
<td style="text-align: center;">Thought 2</td>
<td style="text-align: center;">The paragraph does not tell who Milhouse is named after, maybe I can look up "named after".</td>
</tr>
<tr>
<td style="text-align: center;">Action 2</td>
<td style="text-align: center;">Lookup [named after]</td>
</tr>
<tr>
<td style="text-align: center;">Observation 2</td>
<td style="text-align: center;">(Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">middle name was Milhous.</td>
</tr>
<tr>
<td style="text-align: center;">Thought 3</td>
<td style="text-align: center;">Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.</td>
</tr>
<tr>
<td style="text-align: center;">Action 3</td>
<td style="text-align: center;">Finish [Richard Nixon]</td>
</tr>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">Which documentary is about Finnish rock groups, Adam Clayton Powell or The Saimaa Gesture?</td>
</tr>
<tr>
<td style="text-align: center;">Thought 1</td>
<td style="text-align: center;">I need to search Adam Clayton Powell and The Saimaa Gesture, and find which documentary is about Finnish rock groups.</td>
</tr>
<tr>
<td style="text-align: center;">Action 1</td>
<td style="text-align: center;">Search [Adam Clayton Powell]</td>
</tr>
<tr>
<td style="text-align: center;">Observation 1</td>
<td style="text-align: center;">Could not find [Adam Clayton Powell]. Similar: ['Adam Clayton Powell III', 'Seventh Avenue (Manhattan)', 'Adam Clayton Powell Jr. State Office Building', 'Isabel Washington Powell', 'Adam Powell', 'Adam Clayton Powell (film)', 'Giancarlo Esposito'].</td>
</tr>
<tr>
<td style="text-align: center;">Thought 2</td>
<td style="text-align: center;">To find the documentary, I can search Adam Clayton Powell (film).</td>
</tr>
<tr>
<td style="text-align: center;">Action 2</td>
<td style="text-align: center;">Search [Adam Clayton Powell (film)]</td>
</tr>
<tr>
<td style="text-align: center;">Observation 2</td>
<td style="text-align: center;">Adam Clayton Powell is a 1989 American documentary film directed by Richard Kilberg. The film is about the rise and fall of influential African-American politician Adam Clayton Powell Jr.[3][4] It was later aired as part of the PBS series The American Experience.</td>
</tr>
<tr>
<td style="text-align: center;">Thought 3</td>
<td style="text-align: center;">Adam Clayton Powell (film) is a documentary about an African-American politician, not Finnish rock groups. So the documentary about Finnish rock groups must instead be The Saimaa Gesture.</td>
</tr>
<tr>
<td style="text-align: center;">Action 3</td>
<td style="text-align: center;">Finish [The Saimaa Gesture]</td>
</tr>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">What profession does Nicholas Ray and Elia Kazan have in common?</td>
</tr>
<tr>
<td style="text-align: center;">Thought 1</td>
<td style="text-align: center;">I need to search Nicholas Ray and Elia Kazan, find their professions, then find the profession they have in common.</td>
</tr>
<tr>
<td style="text-align: center;">Action 1</td>
<td style="text-align: center;">Search [Nicholas Ray]</td>
</tr>
<tr>
<td style="text-align: center;">Observation 1</td>
<td style="text-align: center;">Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16, 1979) was an American film director, screenwriter, and actor best known for the 1955 film Rebel Without a Cause.</td>
</tr>
<tr>
<td style="text-align: center;">Thought 2</td>
<td style="text-align: center;">Professions of Nicholas Ray are director, screenwriter, and actor. I need to search Elia Kazan next and find his professions.</td>
</tr>
<tr>
<td style="text-align: center;">Action 2</td>
<td style="text-align: center;">Search [Elia Kazan]</td>
</tr>
<tr>
<td style="text-align: center;">Observation 2</td>
<td style="text-align: center;">Elia Kazan was an American film and theatre director, producer, screenwriter and actor.</td>
</tr>
<tr>
<td style="text-align: center;">Thought 3</td>
<td style="text-align: center;">Professions of Elia Kazan are director, producer, screenwriter, and actor. So profession Nicholas Ray and Elia Kazan have in common is director, screenwriter, and actor.</td>
</tr>
<tr>
<td style="text-align: center;">Action 3</td>
<td style="text-align: center;">Finish [director, screenwriter, actor]</td>
</tr>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">Which magazine was started first Arthur's Magazine or First for Women?</td>
</tr>
<tr>
<td style="text-align: center;">Thought 1</td>
<td style="text-align: center;">I need to search Arthur's Magazine and First for Women, and find which was started first.</td>
</tr>
<tr>
<td style="text-align: center;">Action 1</td>
<td style="text-align: center;">Search [Arthur's Magazine]</td>
</tr>
<tr>
<td style="text-align: center;">Observation 1</td>
<td style="text-align: center;">Arthur's Magazine (1844-1846) was an American literary periodical published in Philadelphia in the 19th century.</td>
</tr>
<tr>
<td style="text-align: center;">Thought 2</td>
<td style="text-align: center;">Arthur's Magazine was started in 1844. I need to search First for Women next.</td>
</tr>
<tr>
<td style="text-align: center;">Action 2</td>
<td style="text-align: center;">Search [First for Women]</td>
</tr>
<tr>
<td style="text-align: center;">Observation 2</td>
<td style="text-align: center;">First for Women is a woman's magazine published by Bauer Media Group in the USA.[1] The magazine was started in 1989.</td>
</tr>
<tr>
<td style="text-align: center;">Thought 3</td>
<td style="text-align: center;">First for Women was started in 1989. 1844 (Arthur's Magazine) &lt; 1989 (First for Women), so Arthur's Magazine was started first.</td>
</tr>
<tr>
<td style="text-align: center;">Action 3</td>
<td style="text-align: center;">Finish [Arthur's Magazine]</td>
</tr>
<tr>
<td style="text-align: center;">Question</td>
<td style="text-align: center;">Were Pavel Urysohn and Leonid Levin known for the same type of work?</td>
</tr>
<tr>
<td style="text-align: center;">Thought 1</td>
<td style="text-align: center;">I need to search Pavel Urysohn and Leonid Levin, find their types of work, then find if they are the same.</td>
</tr>
<tr>
<td style="text-align: center;">Action 1</td>
<td style="text-align: center;">Search [Pavel Urysohn]</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">Observation 1</th>
<th style="text-align: left;">Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">mathematician who is best known for his contributions in dimension theory.</td>
</tr>
<tr>
<td style="text-align: left;">Thought 2</td>
<td style="text-align: left;">Pavel Urysohn is a mathematician. I need to search Leonid Levin next and</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">find its type of work.</td>
</tr>
<tr>
<td style="text-align: left;">Action 2</td>
<td style="text-align: left;">Search[Leonid Levin]</td>
</tr>
<tr>
<td style="text-align: left;">Observation 2</td>
<td style="text-align: left;">Leonid Anatolievich Levin is a Soviet-American mathematician and computer</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">scientist.</td>
</tr>
<tr>
<td style="text-align: left;">Thought 3</td>
<td style="text-align: left;">Leonid Levin is a mathematician and computer scientist. So Pavel Urysohn</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">and Leonid Levin have the same type of work.</td>
</tr>
<tr>
<td style="text-align: left;">Action 3</td>
<td style="text-align: left;">Finish[yes]</td>
</tr>
</tbody>
</table>
<h1>C. 2 FEVER</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">FEVER Prompts</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if there is NOT ENOUGH INFORMATION.</td>
</tr>
<tr>
<td style="text-align: center;">Claim <br> Answer</td>
<td style="text-align: center;">Nikolaj Coster-Waldau worked with the Fox Broadcasting Company. <br> SUPPORTS</td>
</tr>
<tr>
<td style="text-align: center;">Claim <br> Answer</td>
<td style="text-align: center;">Stranger Things is set in Bloomington, Indiana. <br> REFUTES</td>
</tr>
<tr>
<td style="text-align: center;">Claim <br> Answer</td>
<td style="text-align: center;">Beautiful reached number two on the Billboard Hot 100 in 2003. <br> NOT ENOUGH INFO</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if there is NOT ENOUGH INFORMATION.</td>
</tr>
<tr>
<td style="text-align: center;">Claim <br> Action2 <br> Observation 1</td>
<td style="text-align: center;">Nikolaj Coster-Waldau worked with the Fox Broadcasting Company. Search[Nikolaj Coster-Waldau] <br> Nikolaj William Coster-Waldau (born 27 July 1970) is a Danish actor and producer. He graduated from the Danish National School of Performing Arts in Copenhagen in 1993, [1] and had his breakthrough role in Denmark with the film Nightwatch (1994). He played Jaime Lannister in the HBO fantasy drama series Game of Thrones, for which he received two Primetime Emmy Award nominations for Outstanding Supporting Actor in a Drama Series.. Coster-Waldau has appeared in numerous films in his native Denmark and Scandinavia, including Headhunters (2011) and A Thousand Times Good Night (2013). In the U.S, his debut film role was in the war film Black Hawk Down (2001), playing Medal of Honor recipient Gary Gordon.[2] He then played a detective in the short-lived Fox television series New Amsterdam (2008), and appeared in the 2009 Fox television film Virtuality, originally intended as a pilot. <br> Finish[SUPPORTS]</td>
</tr>
<tr>
<td style="text-align: center;">Claim <br> Action 1 <br> Observation 1</td>
<td style="text-align: center;">Stranger Things is set in Bloomington, Indiana. <br> Search[Stranger Things] <br> Stranger Things is an American science fiction horror drama television series created by the Duffer Brothers. Set in the 1980s, primarily in the fictional town of Hawkins, Indiana, the series centers on a number of mysteries and supernatural events occurring around the town and their impact on an ensemble of child and adult characters. <br> Finish[REFUTES]</td>
</tr>
<tr>
<td style="text-align: center;">Claim <br> Action 1 <br> Observation 1</td>
<td style="text-align: center;">Beautiful reached number two on the Billboard Hot 100 in 2003. <br> Search[Beautiful] <br> Could not find [Beautiful]. Similar: ['Beautiful', 'Beautiful, Beautiful', 'A Beautiful Mind (film)', 'Beautiful (Christina Aguilera song)', 'Life Is Beautiful']. <br> Search[Beautiful (Christina Aguilera song) <br> "Beautiful" is a song recorded by American singer Christina Aguilera for her fourth studio album, Stripped (2002). <br> Lookup[Billboard Hot 100] <br> (Result 1 / 3) The song peaked at number two on the Billboard Hot 100 in the <br> United States, where it was certified Gold for 500,000 units shipped. <br> Finish[NOT ENOUGH INFO]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if there is NOT ENOUGH INFORMATION.</td>
</tr>
<tr>
<td style="text-align: center;">Claim <br> Thought <br> Answer</td>
<td style="text-align: center;">Nikolaj Coster-Waldau worked with the Fox Broadcasting Company. <br> Nikolaj William Coster-Waldau appeared in the 2009 Fox television film Virtuality, so he has worked with the Fox Broadcasting Company. <br> SUPPORTS</td>
</tr>
<tr>
<td style="text-align: center;">Claim <br> Thought <br> Answer</td>
<td style="text-align: center;">Stranger Things is set in Bloomington, Indiana. <br> Stranger Things is in the fictional town of Hawkins, Indiana, not in Bloomington, Indiana. <br> REFUTES</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">Claim</th>
<th style="text-align: left;">Beautiful reached number two on the Billboard Hot 100 in 2003.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Thought</td>
<td style="text-align: left;">The song peaked at number two on the Billboard Hot 100 in the United States, <br> but not sure if it was in 2003.</td>
</tr>
<tr>
<td style="text-align: left;">Answer</td>
<td style="text-align: left;">NOT ENOUGH INFO</td>
</tr>
</tbody>
</table>
<h1>Fever Prompts - Continued from previous page</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Claim</th>
<th style="text-align: left;">Beautiful reached number two on the Billboard Hot 100 in 2003.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Thought 1</td>
<td style="text-align: left;">The song peaked at number two on the Billboard Hot 100 in the United States, <br> but not sure if it was in 2003.</td>
</tr>
<tr>
<td style="text-align: left;">Answer</td>
<td style="text-align: left;">NOT ENOUGH INFO</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">ReAct</th>
<th style="text-align: left;">Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if <br> there is NOT ENOUGH INFORMATION.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Claim Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.
Thought 1 I need to search Nikolaj Coster-Waldau and find if he has worked with the Fox Broadcasting Company.
Action 1 Search [Nikolaj Coster-Waldau]
Observation 1 Nikolaj William Coster-Waldau (born 27 July 1970) is a Danish actor and producer. He graduated from the Danish National School of Performing Arts in Copenhagen in 1993, [1] and had his breakthrough role in Denmark with the film Nightwatch (1994). He played Jaime Lannister in the HBO fantasy drama series Game of Thrones, for which he received two Primetime Emmy Award nominations for Outstanding Supporting Actor in a Drama Series.. Coster-Waldau has appeared in numerous films in his native Denmark and Scandinavia, including Headhunters (2011) and A Thousand Times Good Night (2013). In the U.S, his debut film role was in the war film Black Hawk Down (2001), playing Medal of Honor recipient Gary Gordon.[2] He then played a detective in the short-lived Fox television series New Amsterdam (2008), and appeared in the 2009 Fox television film Virtuality, originally intended as a pilot.
Thought 2 Because he "appeared in the 2009 Fox television film Virtuality", he should have worked with the Fox Broadcasting Company.
Action 2 Finish[SUPPORTS]</p>
<p>Claim Stranger Things is set in Bloomington, Indiana.
Thought 1 I should search for Stranger Things, and see if it is set in Bloomington, Indiana.
Action 1 Search [Stranger Things]
Observation 1 Stranger Things is an American science fiction horror drama television series created by the Duffer Brothers. Set in the 1980s, primarily in the fictional town of Hawkins, Indiana, the series centers on a number of mysteries and supernatural events occurring around the town and their impact on an ensemble of child and adult characters.
Thought 2 The observation says that it is set in a "fictional town of Hawkins, Indiana", so it is not set in Bloomington.
Action 2 Finish[REFUTES]</p>
<p>Claim Beautiful reached number two on the Billboard Hot 100 in 2003.
Thought 1 I need to search the song Beautiful and find if it reached number two on the Billboard Hot 100 in 2003.
Action 1 Search [Beautiful]
Observation 1 Could not find [Beautiful]. Similar: ['Beautiful', 'Beautiful, Beautiful', 'A Beautiful Mind (film)', 'Beautiful (Christina Aguilera song)', 'Life Is Beautiful'].
Thought 2 From suggestions, I should search "Beautiful (Christina Aguilera song)" to find the song.
Action 2 Search [Beautiful (Christina Aguilera song)]
Observation 2 "Beautiful" is a song recorded by American singer Christina Aguilera for her fourth studio album, Stripped (2002).
Thought 3 It does not mention Billboard, so I need to look up "Billboard Hot 100" to find if it reached number two on it in 2003.
Action 3 Lookup[Billboard Hot 100]
Observation 3 (Result 1 / 3) The song peaked at number two on the Billboard Hot 100 in the United States, where it was certified Gold for 500,000 units shipped.
Thought 4 It only says the song peaked at number two on the Billboard Hot 100, but not if it was in 2003. I am not sure if this claim is true or not.
Action 4 Finish[NOT ENOUGH INFO]</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ Human feedback can also be incorporated in a complementary manner but we leave it for future work.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8547 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8547</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8547</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-270440937</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.09043v3.pdf" target="_blank">Language Models are Crossword Solvers</a></p>
                <p><strong>Paper Abstract:</strong> Crosswords are a form of word puzzle that require a solver to demonstrate a high degree of proficiency in natural language understanding, wordplay, reasoning, and world knowledge, along with adherence to character and length constraints. In this paper we tackle the challenge of solving crosswords with large language models (LLMs). We demonstrate that the current generation of language models shows significant competence at deciphering cryptic crossword clues and outperforms previously reported state-of-the-art (SoTA) results by a factor of 2-3 in relevant benchmarks. We also develop a search algorithm that builds off this performance to tackle the problem of solving full crossword grids with out-of-the-box LLMs for the very first time, achieving an accuracy of 93% on New York Times crossword puzzles. Additionally, we demonstrate that LLMs generalize well and are capable of supporting answers with sound rationale.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8547.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8547.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-Turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary large language model from OpenAI; used via API in this study and evaluated extensively on straight (NYT) and cryptic crossword clue-answering and on full-grid solving when paired with a search algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language Models are Crossword Solvers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary OpenAI multimodal/text LLM accessed via API; strong few-shot and chain-of-thought capabilities; exact training data/parameter count not disclosed in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Crossword puzzles (New York Times straight crosswords; cryptic crosswords: Cryptonite, Init, Lovatts, Guardian post-cutoff sets)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based word puzzle: square letter grid with overlapping across/down answers imposing character-overlap constraints; cryptic variants require linguistic wordplay rather than physical/3D spatial manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Few-shot prompting (5-shot and 10-shot) with system prompt 'expert crossword solver'; for clue-only QA the model was given clue text plus length information; partially-filled-grid experiments provided k% of letters (masked template like '_ _ A _ _'); chain-of-thought (CoT) prompting used in some cryptic experiments (1-shot CoT) with self-consistency sampling (3 samples); full-grid solving used the SweepClip algorithm which first generates candidate answers for all clues, constructs a graph of conflicts/non-conflicts, prunes using largest connected component, iteratively re-generates with masked neighbor letters and prunes until fill or budget/iteration limits.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Pure LLM generation guided by few-shot/CoT prompts and masked-letter templates; LLM outputs used as candidate sets; SweepClip graph-based LLM-guided search (sweep all clues -> clip conflicting candidates using graph LCC -> iterate using neighbor masks); self-consistency sampling in CoT; providing partially revealed characters ('unmasking') to exploit cross-letter constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Clue-level exact-match accuracy on NYT (straight) dataset: 41.2% (few-shot). Cryptic clue exact-match: 23.5% on Cryptonite, 18.7% on Init. Using SweepClip on 100 Monday NYT puzzles: 48% puzzles solved with zero characters wrong, 55% with ≤1 character wrong; average character-level accuracy 93.1% ± 14.1%; clue-wise answer accuracy after SweepClip 89.6% ± 16.9% (improved from base 43.5%). Sub-token/length adherence errors: produced answers of incorrect length on 26.2% of Init dataset and 16.9% of NYT dataset. Human-eval on post-cutoff Lovatts set: when GPT-4-Turbo's answer was correct it provided sound reasoning 74% of the time.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Indirect evidence: substantial and systematic performance gains when partially filled grid characters were provided (k% masked-letter experiments) indicate exploitation of spatial/graph constraints (letter overlap) in the grid; SweepClip's graph-pruning using conflicts/non-conflicts demonstrates that LLM outputs can be integrated with explicit spatial constraint propagation to improve final fills; however the paper emphasizes this is constraint-satisfaction (cross-letter consistency) rather than geometric/3D spatial reasoning. No explicit probing of 2D spatial visualization or geometric manipulation was performed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperforms previously reported SoTA for cryptic clue QA (prior SoTA ChatGPT reported 9.5% vs GPT-4-Turbo 18.7% on Init; overall ~1.97× improvement claimed). Compared to other LLMs evaluated in paper (Llama 3 70B, Claude 3 Sonnet, GPT-3.5 Turbo, Mistral 7B, Phi-3 3.8B, Mixtral), GPT-4-Turbo consistently achieves best accuracy. Compared to specialized crossword systems: character-level accuracy (93.1%) is below best specialized/fine-tuned systems reported in literature (e.g., Proverb ~98.1% letter accuracy, Wallace et al. fine-tuned models with 99.7% letter accuracy using large clue-answer datasets and belief propagation), but SweepClip + GPT-4-Turbo approaches competitive performance without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Frequent length violations (sub-token counting): many semantically-correct answers fail due to wrong length (46.4% of semantically-close but wrong-length cases for GPT-4-Turbo); cryptic crossword baseline accuracy remains substantially below human experts (humans ≈99% for cryptic clues). Algorithmic limitations: SweepClip prunes aggressively (may discard correct isolated answers), has no convergence guarantees and is budget-limited (0.5 USD per crossword in reported run). Failure modes include errors on character-manipulation-heavy clues (anagrams, some containment forms), counting mistakes, and occasional unsound CoT reasoning (human-eval labeled ~26% of correct answers as having unsound explicit reasoning). No experiments on non-linguistic spatial puzzles (Sudoku, Rubik's Cube, Minesweeper) are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>name_short_display</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models are Crossword Solvers', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8547.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8547.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA 3 70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 3 70B (Meta)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 70-billion-parameter open-weight LLM (LLaMA 3 family) evaluated in the study; used both as stand-alone for clue-answering and inside SweepClip for full-grid solving, with lower performance than GPT-4-Turbo but benefiting from the same constraint-exploiting algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language Models are Crossword Solvers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 3 70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-weights transformer-based LLM from Meta (LLaMA 3 family); used in an instruct-tuned/chat form in experiments and run locally in bf16 when supported.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Crossword puzzles (NYT straight crosswords; cryptic crosswords - Init dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based word puzzle with overlapping letter constraints; cryptic clues require linguistic wordplay.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Same prompting scheme as other models: few-shot prompts with clue + length; masked-letter k% hint experiments to test ability to exploit partial grid information; used inside SweepClip (max_iter and budget settings adjusted: max_iter=35 and budget capped at 600 LLM calls for reported runs).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Few-shot generation with masked-letter templates; LLM-guided candidate generation used by SweepClip; no proprietary sampling strategies beyond standard temperature=0.5 and provided prompts; no fine-tuning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Clue-level exact-match accuracy on NYT (straight) dataset: 27.2% (few-shot). Using SweepClip on 100 Monday NYT puzzles improved clue-answering accuracy from base 22.3% ±14.4% to final 59.4% ±24.1%; character-level accuracies for full-grid solving lower than GPT-4-Turbo (detailed per-puzzle metrics provided in paper). Sub-token length errors: produced wrong-length predictions 59.9% of the time among semantically-close-but-wrong-length cases (higher than GPT-4-Turbo).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Like GPT-4-Turbo, performance increases with provision of partially revealed cross-letter constraints indicating exploitation of grid constraints; SweepClip improved grid solves even with lower base accuracy, demonstrating that constraint propagation via the graph-based pruning aids this model too. No direct probing of geometric or 3D spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Underperforms GPT-4-Turbo on both straight and cryptic clue tasks; benefits substantially from the SweepClip algorithm (final clue accuracy ~59.4% vs base 22.3%). Compared to specialized solvers or fine-tuned systems, LLaMA 3 70B remains behind but shows improvement when combined with search/prune.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High frequency of length-constraint violations (noted above); lower baseline clue accuracy leads to higher computational cost if one tries a more exhaustive search; algorithmic pruning may discard correct answers; cryptic crosswords remain mostly unsolved at grid-level with limited LLM budget (reported cryptic grid solving unsuccessful under the stated budget).</td>
                        </tr>
                        <tr>
                            <td><strong>name_short_display</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models are Crossword Solvers', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8547.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8547.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Other evaluated LLMs (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Phi-3 3.8B, Mistral 7B Instruct, LLaMA 2 70B, LLaMA 3 8B Instruct, Mixtral 8x7B, Claude 3 Sonnet, GPT-3.5 Turbo, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A group of open-weight and proprietary LLMs included in clue-answering evaluations; performance spans a wide range, with Claude 3 Sonnet and GPT-3.5 Turbo near GPT-4-Turbo on some straight-clue metrics but generally below GPT-4-Turbo for cryptics and grid solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language Models are Crossword Solvers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple (Phi-3 3.8B; Mistral 7B Instruct; LLaMA 2 70B; LLaMA 3 8B Instruct; Mixtral 8x7B; Claude 3 Sonnet; GPT-3.5 Turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Diverse set of LLMs spanning open-weight transformer families (Phi-3, LLaMA series, Mistral, Mixtral) and proprietary models (Claude 3 Sonnet, GPT-3.5 Turbo). Operated with temperature=0.5; few-shot prompting; some run locally (on A100 GPUs) and some via APIs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (examples: Phi-3 3.8B, Mistral 7B, Mixtral 8x7B MOE, LLaMA family models include 8B and 70B variants; proprietary model sizes not disclosed)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Crossword puzzles (NYT straight; Cryptonite and Init cryptic datasets; small post-cutoff cryptic sets from Lovatts and The Guardian)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based word puzzles with overlapping constraints and cryptic wordplay requirements; primarily linguistic constraint-satisfaction rather than geometric spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Few-shot prompting (5- and 10-shot) with clue + length; k%-hint masked-letter experiments to test exploitation of partially filled grids; chain-of-thought prompting used selectively (e.g., for Claude/GPT models for cryptics); no fine-tuning performed for these experiments (except noted prior works).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Direct generation from LLMs under few-shot/CoT prompts; using masked templates for partially filled answers; aggregated candidate generation used as input to SweepClip (for those models used inside the full-grid algorithm).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Representative reported numbers: Claude 3 Sonnet NYT straight accuracy ~37.7%; GPT-3.5 Turbo NYT ~26.05%; Phi-3 and Mistral tested with lower accuracies on cryptics in prior works (e.g., Sadallah et al. reported ChatGPT ~9.5% previously). Cryptic accuracies: Claude 3 Sonnet 12.9% (Cryptonite) and 10.8% (Init). Performance varies strongly with model scale and family.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>All models show improved performance when given partial-letter constraints, indicating they can use cross-letter consistency information to some degree; degree of improvement correlates with model scale. No models were tested on non-linguistic spatial puzzles (Sudoku, Rubik's Cube, Minesweeper) in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Paper reports that model performance improves with scale; GPT-4-Turbo and Claude 3 Sonnet outperform Mistral 7B and many open-source models on these tasks. Prior fine-tuned or retrieval-augmented systems (e.g., Wallace et al. fine-tuned ByT5 with belief propagation) achieve higher letter-accuracy than out-of-the-box LLMs in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Shared limitations across models include sub-token counting/length violations (worse for smaller models and rarer words), inability to fully solve cryptic crosswords at grid level under budget constraints, and susceptibility to being tripped by wordplay types that require precise character manipulations (anagrams, containment) especially when length must be exact. No evidence in paper that these models perform non-linguistic spatial reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models are Crossword Solvers', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Cryptonite: A cryptic crossword benchmark for extreme ambiguity in language <em>(Rating: 2)</em></li>
                <li>Are llms good cryptic crossword solvers? <em>(Rating: 2)</em></li>
                <li>Down and across: Introducing crossword-solving as a new NLP benchmark <em>(Rating: 2)</em></li>
                <li>Automated crossword solving <em>(Rating: 1)</em></li>
                <li>Decrypting cryptic crosswords: Semantically complex wordplay puzzles as a target for nlp <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8547",
    "paper_id": "paper-270440937",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "GPT-4-Turbo",
            "name_full": "GPT-4-Turbo (OpenAI)",
            "brief_description": "A proprietary large language model from OpenAI; used via API in this study and evaluated extensively on straight (NYT) and cryptic crossword clue-answering and on full-grid solving when paired with a search algorithm.",
            "citation_title": "Language Models are Crossword Solvers",
            "mention_or_use": "use",
            "model_name": "GPT-4-Turbo",
            "model_description": "Proprietary OpenAI multimodal/text LLM accessed via API; strong few-shot and chain-of-thought capabilities; exact training data/parameter count not disclosed in paper.",
            "model_size": null,
            "puzzle_name": "Crossword puzzles (New York Times straight crosswords; cryptic crosswords: Cryptonite, Init, Lovatts, Guardian post-cutoff sets)",
            "puzzle_type": "Grid-based word puzzle: square letter grid with overlapping across/down answers imposing character-overlap constraints; cryptic variants require linguistic wordplay rather than physical/3D spatial manipulation.",
            "task_setup": "Few-shot prompting (5-shot and 10-shot) with system prompt 'expert crossword solver'; for clue-only QA the model was given clue text plus length information; partially-filled-grid experiments provided k% of letters (masked template like '_ _ A _ _'); chain-of-thought (CoT) prompting used in some cryptic experiments (1-shot CoT) with self-consistency sampling (3 samples); full-grid solving used the SweepClip algorithm which first generates candidate answers for all clues, constructs a graph of conflicts/non-conflicts, prunes using largest connected component, iteratively re-generates with masked neighbor letters and prunes until fill or budget/iteration limits.",
            "mechanisms_or_strategies": "Pure LLM generation guided by few-shot/CoT prompts and masked-letter templates; LLM outputs used as candidate sets; SweepClip graph-based LLM-guided search (sweep all clues -&gt; clip conflicting candidates using graph LCC -&gt; iterate using neighbor masks); self-consistency sampling in CoT; providing partially revealed characters ('unmasking') to exploit cross-letter constraints.",
            "performance_metrics": "Clue-level exact-match accuracy on NYT (straight) dataset: 41.2% (few-shot). Cryptic clue exact-match: 23.5% on Cryptonite, 18.7% on Init. Using SweepClip on 100 Monday NYT puzzles: 48% puzzles solved with zero characters wrong, 55% with ≤1 character wrong; average character-level accuracy 93.1% ± 14.1%; clue-wise answer accuracy after SweepClip 89.6% ± 16.9% (improved from base 43.5%). Sub-token/length adherence errors: produced answers of incorrect length on 26.2% of Init dataset and 16.9% of NYT dataset. Human-eval on post-cutoff Lovatts set: when GPT-4-Turbo's answer was correct it provided sound reasoning 74% of the time.",
            "evidence_of_spatial_reasoning": "Indirect evidence: substantial and systematic performance gains when partially filled grid characters were provided (k% masked-letter experiments) indicate exploitation of spatial/graph constraints (letter overlap) in the grid; SweepClip's graph-pruning using conflicts/non-conflicts demonstrates that LLM outputs can be integrated with explicit spatial constraint propagation to improve final fills; however the paper emphasizes this is constraint-satisfaction (cross-letter consistency) rather than geometric/3D spatial reasoning. No explicit probing of 2D spatial visualization or geometric manipulation was performed.",
            "comparisons": "Outperforms previously reported SoTA for cryptic clue QA (prior SoTA ChatGPT reported 9.5% vs GPT-4-Turbo 18.7% on Init; overall ~1.97× improvement claimed). Compared to other LLMs evaluated in paper (Llama 3 70B, Claude 3 Sonnet, GPT-3.5 Turbo, Mistral 7B, Phi-3 3.8B, Mixtral), GPT-4-Turbo consistently achieves best accuracy. Compared to specialized crossword systems: character-level accuracy (93.1%) is below best specialized/fine-tuned systems reported in literature (e.g., Proverb ~98.1% letter accuracy, Wallace et al. fine-tuned models with 99.7% letter accuracy using large clue-answer datasets and belief propagation), but SweepClip + GPT-4-Turbo approaches competitive performance without fine-tuning.",
            "limitations_or_failure_cases": "Frequent length violations (sub-token counting): many semantically-correct answers fail due to wrong length (46.4% of semantically-close but wrong-length cases for GPT-4-Turbo); cryptic crossword baseline accuracy remains substantially below human experts (humans ≈99% for cryptic clues). Algorithmic limitations: SweepClip prunes aggressively (may discard correct isolated answers), has no convergence guarantees and is budget-limited (0.5 USD per crossword in reported run). Failure modes include errors on character-manipulation-heavy clues (anagrams, some containment forms), counting mistakes, and occasional unsound CoT reasoning (human-eval labeled ~26% of correct answers as having unsound explicit reasoning). No experiments on non-linguistic spatial puzzles (Sudoku, Rubik's Cube, Minesweeper) are reported.",
            "name_short_display": null,
            "uuid": "e8547.0",
            "source_info": {
                "paper_title": "Language Models are Crossword Solvers",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLaMA 3 70B",
            "name_full": "LLaMA 3 70B (Meta)",
            "brief_description": "A 70-billion-parameter open-weight LLM (LLaMA 3 family) evaluated in the study; used both as stand-alone for clue-answering and inside SweepClip for full-grid solving, with lower performance than GPT-4-Turbo but benefiting from the same constraint-exploiting algorithm.",
            "citation_title": "Language Models are Crossword Solvers",
            "mention_or_use": "use",
            "model_name": "LLaMA 3 70B",
            "model_description": "Open-weights transformer-based LLM from Meta (LLaMA 3 family); used in an instruct-tuned/chat form in experiments and run locally in bf16 when supported.",
            "model_size": "70B",
            "puzzle_name": "Crossword puzzles (NYT straight crosswords; cryptic crosswords - Init dataset)",
            "puzzle_type": "Grid-based word puzzle with overlapping letter constraints; cryptic clues require linguistic wordplay.",
            "task_setup": "Same prompting scheme as other models: few-shot prompts with clue + length; masked-letter k% hint experiments to test ability to exploit partial grid information; used inside SweepClip (max_iter and budget settings adjusted: max_iter=35 and budget capped at 600 LLM calls for reported runs).",
            "mechanisms_or_strategies": "Few-shot generation with masked-letter templates; LLM-guided candidate generation used by SweepClip; no proprietary sampling strategies beyond standard temperature=0.5 and provided prompts; no fine-tuning reported.",
            "performance_metrics": "Clue-level exact-match accuracy on NYT (straight) dataset: 27.2% (few-shot). Using SweepClip on 100 Monday NYT puzzles improved clue-answering accuracy from base 22.3% ±14.4% to final 59.4% ±24.1%; character-level accuracies for full-grid solving lower than GPT-4-Turbo (detailed per-puzzle metrics provided in paper). Sub-token length errors: produced wrong-length predictions 59.9% of the time among semantically-close-but-wrong-length cases (higher than GPT-4-Turbo).",
            "evidence_of_spatial_reasoning": "Like GPT-4-Turbo, performance increases with provision of partially revealed cross-letter constraints indicating exploitation of grid constraints; SweepClip improved grid solves even with lower base accuracy, demonstrating that constraint propagation via the graph-based pruning aids this model too. No direct probing of geometric or 3D spatial reasoning.",
            "comparisons": "Underperforms GPT-4-Turbo on both straight and cryptic clue tasks; benefits substantially from the SweepClip algorithm (final clue accuracy ~59.4% vs base 22.3%). Compared to specialized solvers or fine-tuned systems, LLaMA 3 70B remains behind but shows improvement when combined with search/prune.",
            "limitations_or_failure_cases": "High frequency of length-constraint violations (noted above); lower baseline clue accuracy leads to higher computational cost if one tries a more exhaustive search; algorithmic pruning may discard correct answers; cryptic crosswords remain mostly unsolved at grid-level with limited LLM budget (reported cryptic grid solving unsuccessful under the stated budget).",
            "name_short_display": null,
            "uuid": "e8547.1",
            "source_info": {
                "paper_title": "Language Models are Crossword Solvers",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Other evaluated LLMs (aggregate)",
            "name_full": "Phi-3 3.8B, Mistral 7B Instruct, LLaMA 2 70B, LLaMA 3 8B Instruct, Mixtral 8x7B, Claude 3 Sonnet, GPT-3.5 Turbo, etc.",
            "brief_description": "A group of open-weight and proprietary LLMs included in clue-answering evaluations; performance spans a wide range, with Claude 3 Sonnet and GPT-3.5 Turbo near GPT-4-Turbo on some straight-clue metrics but generally below GPT-4-Turbo for cryptics and grid solving.",
            "citation_title": "Language Models are Crossword Solvers",
            "mention_or_use": "use",
            "model_name": "Multiple (Phi-3 3.8B; Mistral 7B Instruct; LLaMA 2 70B; LLaMA 3 8B Instruct; Mixtral 8x7B; Claude 3 Sonnet; GPT-3.5 Turbo)",
            "model_description": "Diverse set of LLMs spanning open-weight transformer families (Phi-3, LLaMA series, Mistral, Mixtral) and proprietary models (Claude 3 Sonnet, GPT-3.5 Turbo). Operated with temperature=0.5; few-shot prompting; some run locally (on A100 GPUs) and some via APIs.",
            "model_size": "various (examples: Phi-3 3.8B, Mistral 7B, Mixtral 8x7B MOE, LLaMA family models include 8B and 70B variants; proprietary model sizes not disclosed)",
            "puzzle_name": "Crossword puzzles (NYT straight; Cryptonite and Init cryptic datasets; small post-cutoff cryptic sets from Lovatts and The Guardian)",
            "puzzle_type": "Grid-based word puzzles with overlapping constraints and cryptic wordplay requirements; primarily linguistic constraint-satisfaction rather than geometric spatial reasoning.",
            "task_setup": "Few-shot prompting (5- and 10-shot) with clue + length; k%-hint masked-letter experiments to test exploitation of partially filled grids; chain-of-thought prompting used selectively (e.g., for Claude/GPT models for cryptics); no fine-tuning performed for these experiments (except noted prior works).",
            "mechanisms_or_strategies": "Direct generation from LLMs under few-shot/CoT prompts; using masked templates for partially filled answers; aggregated candidate generation used as input to SweepClip (for those models used inside the full-grid algorithm).",
            "performance_metrics": "Representative reported numbers: Claude 3 Sonnet NYT straight accuracy ~37.7%; GPT-3.5 Turbo NYT ~26.05%; Phi-3 and Mistral tested with lower accuracies on cryptics in prior works (e.g., Sadallah et al. reported ChatGPT ~9.5% previously). Cryptic accuracies: Claude 3 Sonnet 12.9% (Cryptonite) and 10.8% (Init). Performance varies strongly with model scale and family.",
            "evidence_of_spatial_reasoning": "All models show improved performance when given partial-letter constraints, indicating they can use cross-letter consistency information to some degree; degree of improvement correlates with model scale. No models were tested on non-linguistic spatial puzzles (Sudoku, Rubik's Cube, Minesweeper) in this paper.",
            "comparisons": "Paper reports that model performance improves with scale; GPT-4-Turbo and Claude 3 Sonnet outperform Mistral 7B and many open-source models on these tasks. Prior fine-tuned or retrieval-augmented systems (e.g., Wallace et al. fine-tuned ByT5 with belief propagation) achieve higher letter-accuracy than out-of-the-box LLMs in some settings.",
            "limitations_or_failure_cases": "Shared limitations across models include sub-token counting/length violations (worse for smaller models and rarer words), inability to fully solve cryptic crosswords at grid level under budget constraints, and susceptibility to being tripped by wordplay types that require precise character manipulations (anagrams, containment) especially when length must be exact. No evidence in paper that these models perform non-linguistic spatial reasoning tasks.",
            "uuid": "e8547.2",
            "source_info": {
                "paper_title": "Language Models are Crossword Solvers",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Cryptonite: A cryptic crossword benchmark for extreme ambiguity in language",
            "rating": 2
        },
        {
            "paper_title": "Are llms good cryptic crossword solvers?",
            "rating": 2
        },
        {
            "paper_title": "Down and across: Introducing crossword-solving as a new NLP benchmark",
            "rating": 2
        },
        {
            "paper_title": "Automated crossword solving",
            "rating": 1
        },
        {
            "paper_title": "Decrypting cryptic crosswords: Semantically complex wordplay puzzles as a target for nlp",
            "rating": 2
        }
    ],
    "cost": 0.01531575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Language Models are Crossword Solvers
9 Feb 2025</p>
<p>Soumadeep Saha soumadeep.saha97@gmail.com 
Indian Statistical Institute
Kolkata</p>
<p>Sutanoya Chakraborty 
Indian Statistical Institute
Kolkata</p>
<p>Saptarshi Saha 
Indian Statistical Institute
Kolkata</p>
<p>Utpal Garain 
Indian Statistical Institute
Kolkata</p>
<p>Marah Abdin 
Indian Statistical Institute
Kolkata</p>
<p>AmmarSam Ade Jacobs 
Indian Statistical Institute
Kolkata</p>
<p>Ahmad Awan 
Indian Statistical Institute
Kolkata</p>
<p>Jyoti Aneja 
Indian Statistical Institute
Kolkata</p>
<p>Ahmed Awadallah 
Indian Statistical Institute
Kolkata</p>
<p>Hany Awadalla 
Indian Statistical Institute
Kolkata</p>
<p>Nguyen Bach 
Indian Statistical Institute
Kolkata</p>
<p>Amit Bahree 
Indian Statistical Institute
Kolkata</p>
<p>Arash Bakhtiari 
Indian Statistical Institute
Kolkata</p>
<p>Jian- Min Bao 
Indian Statistical Institute
Kolkata</p>
<p>Harkirat Behl 
Indian Statistical Institute
Kolkata</p>
<p>Alon Benhaim 
Indian Statistical Institute
Kolkata</p>
<p>Misha Bilenko 
Indian Statistical Institute
Kolkata</p>
<p>Johan Bjorck 
Indian Statistical Institute
Kolkata</p>
<p>Sébastien Bubeck 
Indian Statistical Institute
Kolkata</p>
<p>Qin Cai 
Indian Statistical Institute
Kolkata</p>
<p>Martin Cai 
Indian Statistical Institute
Kolkata</p>
<p>Caio César 
Indian Statistical Institute
Kolkata</p>
<p>Teodoro Mendes 
Indian Statistical Institute
Kolkata</p>
<p>Weizhu Chen 
Indian Statistical Institute
Kolkata</p>
<p>Vishrav Chaudhary 
Indian Statistical Institute
Kolkata</p>
<p>Dongdong Chen 
Indian Statistical Institute
Kolkata</p>
<p>Indian Statistical Institute
Kolkata</p>
<p>Yen-Chun Chen 
Indian Statistical Institute
Kolkata</p>
<p>Yi-Ling Chen 
Indian Statistical Institute
Kolkata</p>
<p>Parul Chopra 
Indian Statistical Institute
Kolkata</p>
<p>Xiyang Dai 
Indian Statistical Institute
Kolkata</p>
<p>Allie Del Giorno 
Indian Statistical Institute
Kolkata</p>
<p>Gustavo De Rosa 
Indian Statistical Institute
Kolkata</p>
<p>Matthew Dixon 
Indian Statistical Institute
Kolkata</p>
<p>Ronen Eldan 
Indian Statistical Institute
Kolkata</p>
<p>Victor Fragoso 
Indian Statistical Institute
Kolkata</p>
<p>Dan Iter 
Indian Statistical Institute
Kolkata</p>
<p>Mei Gao 
Indian Statistical Institute
Kolkata</p>
<p>Min Gao 
Indian Statistical Institute
Kolkata</p>
<p>Jianfeng Gao 
Indian Statistical Institute
Kolkata</p>
<p>Amit Garg 
Indian Statistical Institute
Kolkata</p>
<p>Abhishek Goswami 
Indian Statistical Institute
Kolkata</p>
<p>Suriya Gunasekar 
Indian Statistical Institute
Kolkata</p>
<p>Emman Haider 
Indian Statistical Institute
Kolkata</p>
<p>Junheng Hao 
Indian Statistical Institute
Kolkata</p>
<p>Russell J Hewett 
Indian Statistical Institute
Kolkata</p>
<p>Jamie Huynh 
Indian Statistical Institute
Kolkata</p>
<p>Mojan Javaheripi 
Indian Statistical Institute
Kolkata</p>
<p>Xin Jin 
Indian Statistical Institute
Kolkata</p>
<p>Piero Kauff- Mann 
Indian Statistical Institute
Kolkata</p>
<p>Nikos Karampatziakis 
Indian Statistical Institute
Kolkata</p>
<p>Dongwoo Kim 
Indian Statistical Institute
Kolkata</p>
<p>Ma- Houd Khademi 
Indian Statistical Institute
Kolkata</p>
<p>Lev Kurilenko 
Indian Statistical Institute
Kolkata</p>
<p>James R Lee 
Indian Statistical Institute
Kolkata</p>
<p>Yin Tat Lee 
Indian Statistical Institute
Kolkata</p>
<p>Yuanzhi Li 
Indian Statistical Institute
Kolkata</p>
<p>Yunsheng Li 
Indian Statistical Institute
Kolkata</p>
<p>Chen Liang 
Indian Statistical Institute
Kolkata</p>
<p>Lars Li- Den 
Indian Statistical Institute
Kolkata</p>
<p>Ce Liu 
Indian Statistical Institute
Kolkata</p>
<p>Mengchen Liu 
Indian Statistical Institute
Kolkata</p>
<p>Weishung Liu 
Indian Statistical Institute
Kolkata</p>
<p>Eric Lin 
Indian Statistical Institute
Kolkata</p>
<p>Zeqi Lin 
Indian Statistical Institute
Kolkata</p>
<p>Chong Luo 
Indian Statistical Institute
Kolkata</p>
<p>Piyush Madan 
Indian Statistical Institute
Kolkata</p>
<p>Matt Mazzola 
Indian Statistical Institute
Kolkata</p>
<p>Arindam Mitra 
Indian Statistical Institute
Kolkata</p>
<p>Hardik Modi 
Indian Statistical Institute
Kolkata</p>
<p>Anh Nguyen 
Indian Statistical Institute
Kolkata</p>
<p>Language Models are Crossword Solvers
9 Feb 2025D713E32A069914DAB6EC6F59B668B552arXiv:2406.09043v3[cs.CL]
Crosswords are a form of word puzzle that require a solver to demonstrate a high degree of proficiency in natural language understanding, wordplay, reasoning, and world knowledge, along with adherence to character and length constraints.In this paper we tackle the challenge of solving crosswords with large language models (LLMs).We demonstrate that the current generation of language models shows significant competence at deciphering cryptic crossword clues and outperforms previously reported state-of-the-art (SoTA) results by a factor of 2-3 in relevant benchmarks.We also develop a search algorithm that builds off this performance to tackle the problem of solving full crossword grids with out-of-thebox LLMs for the very first time, achieving an accuracy of 93% on New York Times crossword puzzles.Additionally, we demonstrate that LLMs generalize well and are capable of supporting answers with sound rationale.</p>
<p>Introduction</p>
<p>1 Crossword puzzles are a type of word game that typically take the form of a square grid of white and black boxes.The objective of the puzzle is to fill the white boxes with letters from words or phrases based on the provided clues (see Figure 1).Although crosswords come in a variety of styles, the two most popular ones are the American style, or straight crosswords, and cryptic crosswords.</p>
<p>Modern LLMs demonstrate astounding skills in reasoning, coding, wordplay, question answering (QA), and a multitude of other tasks (Wei et al., 2022a;Minaee et al., 2024).Despite the plethora of applications for LLMs seen today, their ability to generate language in a constrained setting remains relatively uncharted, and the ability to direct the generation process in order to meet certain criteria remains a challenge (Popescu-Belis et al., 2022).Consider the task of poem generation as an example, where, in addition to thematic aptness, constraints on rhyme or meter must be adhered to.Further, for certain kinds of poems like haikus, sonnets, or even song lyrics, restrictions on length, syllable counts, or patterns of stressed/unstressed syllables apply.Constraints also arise when dealing with formal languages (Koo et al., 2024), which is increasingly relevant in the LLM zeitgeist given their use in coding or interacting with databases, interpreters, APIs, etc.With a growing body of literature studying LLMs as agents (Madge and Poesio, 2024;Wang et al., 2024), language generation must follow environmental or physical constraints.For wider proliferation of LLM applications, they must demonstrate the ability to adhere to such domain-specific constraints (e.g., constraints imposed by knowledge graphs, tabular data, action spaces, etc.).</p>
<p>Solving crosswords requires proficiency in understanding contextual clues, semantics, wordplay, character manipulation, arithmetic, and reasoning (see Figure 1 (right)), along with satisfying constraints such as length limitations and character overlaps (see Figure 1).Given the multi-faceted nature of this task, it can serve as a testbed for studying constrained language generation.Since solving crosswords requires proficiency in several desirable areas, and identifying shortcomings can readily benefit other linguistic applications, in this paper we attempt to analyze LLMs' ability to solve crossword puzzles, with the primary goal of understanding strengths and weaknesses demonstrated by SoTA LLMs.Our contributions in this study are as follows:</p>
<p>⋄ We perform extensive analysis to understand how well LLMs can answer crossword clues based on the provided contextual information and constraints.We show that current-generation SoTA LLMs demonstrate massively improved per- formance when compared to previous SoTA baselines, outperforming some previous benchmarks by a factor of 2 or 3, without any fine-tuning.</p>
<p>⋄ Our analysis shows that the ability of LLMs to adhere to length constraints, which is a critical component of solving crossword puzzles, is somewhat limited.We present results demonstrating that an LLM's ability to count the number of constituent characters in a word degrades with the prevalence of the word.This suggests that their ability to perform this task does not generalize well.Additionally, we show that this is a significant limiting factor for crossword solving, since even when the generated answers are semantically similar, the length constraint is frequently not obeyed.</p>
<p>⋄ We devise a simple algorithm to tackle the full problem of solving crosswords with an LLMguided search and incorporating grid information, solving straight crossword puzzles with an accuracy of 93.1%.This algorithm exploits the constraints imposed by previously generated answers to improve future answers and achieves results that are not far behind those of specialized automated crossword-solving systems.This algorithm also improves the clue-deciphering capability of LLMs, more than doubling the baseline performance.</p>
<p>⋄ We perform human evaluation and further experiments with post knowledge-cutoff datasets to assess the soundness of logical reasoning, potential pitfalls, and generalizability.We find that SoTA LLMs appear to generalize well and provide sound logical reasoning with 74% of correct answers.</p>
<p>We believe our work has been a major step towards demonstrating the power of LLMs with regard to crossword solving, and with future advances in certain key areas, LLMs' performance in this task will be comparable to, if not better than, human experts and specialized systems.Our algorithmic approach of combining an LLM with a search strategy might be extended to linguistic problems with domain-specific constraints.</p>
<p>Background</p>
<p>Traditional approaches to solving straight crossword puzzles involve two key components: a candidate answer proposal system and a grid-filling algorithm (Angelini et al., 2005;Ginsberg, 2011).Answer proposal systems typically use similarity search on large clue-answer databases, finetuned language models, or a combination of both (Shazeer et al., 1999;Wallace et al., 2022).Gridfilling relies on variants of constraint satisfaction problem (CSP) algorithms.For instance, the system Proverb (Littman et al., 2002) achieves a 98.1% letter accuracy on New York Times (NYT) crosswords.Wallace et al. (2022) fine-tuned BERT and ByT5 on 6.4 million clue-answer pairs, and using a belief propagation algorithm, achieved a 99.7% letter accuracy.Kulshreshtha et al. (2022) established benchmarks with foundational language models and highlighted this task as "... a new high bar for AI systems.".Note that this work does not attempt to create an improved automated crosswordsolving system but seeks to analyze foundational LLMs' ability at this complex language task.</p>
<p>Cryptic crosswords are more formidable and involve extensive wordplay such as anagrams, splicing, homophones, and puns.Traditional algorithms with large clue datasets and a context-free grammar parser (Deits, 2015) have shown poor performance, achieving only 7% accuracy (Sadallah et al., 2024).Recent studies have explored using large language models (LLMs) to solve cryptic crossword clues.Efrat et al. (2021) created a large dataset of cryptic crossword clues from UK dailies, The Times and The Telegraph, and fine-tuned a T5-Large (Raffel et al., 2020) model to establish baseline performances.They used a training split where answers in the training and test sets were mutually exclusive to prevent memorization.Rozner et al. (2021) curated a dataset from The Guardian and fine-tuned a T5-Large model using curriculum learning, showing performance improvements.They critiqued Efrat et al. (2021)'s approach, arguing that a disjoint train-test split is insufficient for teaching models to solve cryptic crosswords, as models exhibit "... robustness to plural and other inflections."Instead, they proposed grouping similar-root words together in a split, noting that this more stringent criterion led to reduced performance.</p>
<p>The most recent work by Sadallah et al. (2024) presents results with recent LLMs like Mistral-7B (Jiang et al., 2023), LLaMA2-7B (Touvron et al., 2023), andChatGPT (OpenAI, 2021) in few-shot settings and also by fine-tuning the Mistral model.They report that ChatGPT surpasses other models with an accuracy of 9.5%, which demonstrates a significant performance gap with respect to human experts, who solve 99% of cryptic crossword clues (Sadallah et al., 2024).They point out key limitations in their work, like the limited set of LLMs used and the potential for data contamination.</p>
<p>These recent works on cryptic crossword solving with LLMs highlight a significant performance gap between LLMs and human experts.However, these recent works approach the problem as a question answering (QA) task and ignore constraints imposed by the grid.This needs to be investigated further since it is yet to be seen whether a suitable approach that integrates constraint information into LLMs can yield significant performance benefits.</p>
<p>As for straight crosswords, Kulshreshtha et al. (2022) attempted to solve NYT crossword puzzles with language models and a Satisfiability Modulo Theory (SMT) solver with limited success, and resorted to culling the crossword grid based on candidate generations and ground truth answers.We present an algorithm employing LLM-guided search, which is significantly more successful at this task.With advancements in LLM capabilities and methods like ours, we believe LLMs could soon outperform humans in solving cryptic crosswords too.</p>
<p>Analyzing Crossword Clue Solving</p>
<p>The first step in solving a crossword puzzle is deciphering its clues, so following in the scheme of Rozner et al. (2021), Sadallah et al. (2024), Kulshreshtha et al. (2022), andEfrat et al. (2021), in this section, we first explore this as a QA task.We analyze the performance of several LLMs at different scales with variations of this task.</p>
<p>Datasets</p>
<p>Most of our analysis is performed on three crossword puzzle datasets.The first one covering straight or American-style crosswords was curated by us from the very popular and long-running puzzle section of the NYT.The other two, namely, Cryptonite by Efrat et al. (2021) and word-initdisjoint (abbreviated as Init) by Rozner et al. (2021), cover cryptic crossword puzzles.Note, the methodological differences between Cryptonite and Init, as discussed in Section 2, are not pertinent for us since we do not perform any training.A bulk of the results are reported on the NYT dataset we curated for straight crosswords and Init for cryptic crosswords since it was found to be more challenging than Cryptonite for LLMs.We drew 2000 randomly chosen samples to report results, and in-context examples were also randomly selected from a large pool of samples disjoint from the testing set.Further details can be found in Appendix B.</p>
<p>Answering Crossword Clues</p>
<p>In this experiment we only provided the LM with the query clue and length information (alongside instructions and in-context examples), with the expectation that it will produce the corresponding answer.We report results with Phi 3 3.8B Instruct (Abdin et al., 2024), Mistral 7B Instruct (Jiang et al., 2023), Llama 2 70B (Touvron et al., 2023), Llama 3 8B Instruct, Llama 3 70B (Meta, 2024), Mixtral 8x7B (Jiang et al., 2024), Claude 3 Sonnet (Anthropic, 2024), GPT 3.5 Turbo, andGPT 4 Turbo (OpenAI, 2023) to cover a wide range of parameter scales and a mix of open-weights and proprietary models.</p>
<p>We investigate the performance of these models with few-shot prompts (5-shot, 10-shot)2 on samples from the NYT dataset (ours), Cryptonite, and the Init split (see Figure 2).We find that the performance difference between 5-shot and 10-shot3 prompted answers is not significant (see Figure 2).We also note that there is an appreciable difference (up to 5%) in the performance of models across the board between Cryptonite and Init, with LLMs showing diminished performance on the Init dataset.</p>
<p>The models demonstrate improved performance with scale across datasets and, in particular, show remarkable improvement on the NYT dataset with Llama 3 70B, GPT 3.5 Turbo, Claude 3 Sonnet, and GPT-4-Turbo achieving 27.2%, 26.05%, 37.7% and 41.2% accuracy (exact match), respectively.The performance of LLMs on cryptic crosswords is worse compared to straight crosswords; however, Claude 3 Sonnet and GPT-4-Turbo outperform previous SoTA results on both the cryptic crossword datasets, achieving an accuracy of 12.9%, 23.5% on Cryptonite and 10.8%, 18.7% on Init respectively.The performance of GPT-4-Turbo is rather extraordinary, with a 1.97× improvement over previous SoTA (9.5% reported by Sadallah et al. (2024), see Figure 2).</p>
<p>Exploiting Partially Filled Grids</p>
<p>In the course of solving a crossword puzzle, we encounter intermediate states where some of the clues have been deciphered.Crossword solvers exploit these clues in order to inform their decisions about future answers.For example, in Figure 1 (left), when we want to solve for the clue in position 14 (across), we can use the characters from position 2 (down) and 4 (down) to narrow down the set of possible answers to only those that fit the template "_ T _ P".In this section, we study LLMs' performance at exploiting these constraints to arrive at better answers (see Table 1).</p>
<p>For this experiment we chose the best performing open-weights and proprietary models, i.e., LLaMA 3 70B and GPT-4-Turbo, respectively.We also report results on some smaller models to see if the trends hold.We report results on the NYT dataset and Init dataset with 5-shot prompts.For each query, k% of the characters (letters) of the answer is provided alongside the clue and expected length of the answer, and the LLMs are expected to "unmask" the remaining characters using the provided constraint information and the crossword clue 4 .</p>
<p>We observe (Table 1) that, in all but one case, LLMs show improved performance with an increasing percentage of constraint information for both datasets.Additionally, to compare the performance of GPT-4-Turbo to previously reported SoTA results by Sadallah et al. (2024), we perform the experiment with the same settings and dataset split as them to find that GPT-4- factor of 2.8×.</p>
<p>The fact that LLMs can successfully exploit constraints to answer crossword clues better suggests that they are well-suited to the task of solving full crosswords.The significant jump in performance observed in modern SoTA LLMs at this task and the task in Section 3.2 is extremely serendipitous, and we investigate this in further detail in Section 5.3.</p>
<p>Sub-token Counting</p>
<p>Despite significant performance gains, SoTA LLMs struggle with adherence to length constraints, suggesting an inability to count characters within words or phrases (sub-token counting).We observed that even the best performing model, GPT-4-Turbo, produces answers of incorrect length on 26.2% of the Init dataset and 16.9% of NYT dataset.This may be explained by the tokenization methods used in LLMs, such as Byte-Pair Encoding (Sennrich et al., 2016).During the word embedding stage in transformers (Vaswani et al., 2017), tokens are converted into embedding vectors, causing the loss of information about individual characters.This character-level information must be relearned during training.While we are unsure exactly how LLMs regain this information, we suspect they learn from training data that include explicit length details.</p>
<p>There are websites 5 that contain large lists of words with their corresponding lengths.Often replies in message boards also include a count of the number of characters in the reply.Artifacts like these, which contain enough information to infer the length of tokens, go on to become part of the datasets that LLMs are trained on.We hypothesize that LLMs learn to count sub-tokens based on this information provided during training.</p>
<p>To investigate this further, we devised the sub-5 https://word.tips/words-by-length/for example.token counting task, wherein the LLM is provided a sequence of (lowercase) characters without whitespaces and asked to predict the number of characters making up the sequence.To test our hypothesis, we consider three sets of 1000 (English) words-Common, Medium and Rare-based on word unigram frequencies curated by Segaran and Hammerbacher ( 2009) from Google's Trillion Words corpus.The Common, Medium and Rare words have ranks in the range of 1-5,000, 47,500-52,500, and 95,000-100,000, respectively.</p>
<p>If language models have a widely generalizable ability to perform sub-token counting, we should see no difference in counting performance across words with different prevalence.However, we observe that (see Figure 3) the accuracy of LLMs at the sub-token counting task declines with the frequency of the token for all LLMs tested.We further analyzed if there was a difference in subtoken counting performance between words that are part of the model vocabulary and randomly generated gibberish following the same distribution of lengths to account for potential shifts of distribution of length between frequent and rare words.To do this, we first created a set of words by taking an intersection of all words that are part of the vocabulary of every open-source model in consideration and the list of the top 100,000 words.This is to ensure that the sequences are extremely likely6 to be vocabulary tokens for every model in consideration and are not special tokens like <bos>.Then we created a set of gibberish words by replacing each character of the vocab set words with a randomly chosen character from the set {a-z}, thus guaranteeing that they have the same length distributions.</p>
<p>Model</p>
<p>Vocab We find (see Table 2) that in addition to counting accuracy being affected by frequency, there is often a large disparity between the accuracy of vocabulary vs. gibberish words.Although this does not conclusively show that LLMs rely on memorized training instances to perform sub-token counting, it does provide strong evidence suggesting that LLMs learn to count based on length information containing artifacts in training data.Future works exploring this idea further would be compelling.</p>
<p>SweepClip -Our proposed algorithm to solve crosswords with LLMs</p>
<p>In this section, we address the problem of filling crossword grids with LLM assistance.Note that this task not only involves generating correct answers from provided clues but also hinges on exploiting constraints from already generated words and backtracking to eliminate past generations that do not fit well when new evidence becomes available.Since LLMs demonstrate this ability, when paired with the right search algorithm, it should be possible to solve crosswords with the aid of LLMs.</p>
<p>Our proposed algorithm (detailed in Appendix A) first generates a set of candidate answers for all clues provided with the crossword (sweep) and uses a graph-based criterion to eliminate answers that do not fit (clip).Then we use the constraints generated from the previous step answers to generate more candidate answers7 and prune the bad-fitting candidates.We iteratively apply this strategy until either (i) the entire crossword is filled, (ii) we exceed a preset number of iterations, or (iii) we run out of LLM computational budget.</p>
<p>For pruning, we use the largest connected component from the answers generated so far to ensure that the generated answers are somewhat coherent amongst themselves.This is over-restrictive, as it is possible that isolated answers are correct; however, we find that this strict pruning strategy works better to eliminate bad answers early rather than using the constraints imposed by potentially bad answers to generate further bad answers (see Appendix A).</p>
<p>Results</p>
<p>In this section, we present results from our algorithm at the crossword-solving task and present further results investigating the performance demonstrated by current-generation SoTA LLMs.</p>
<p>SweepClip-Solving NYT Crossword Puzzles</p>
<p>For this task, we employed our algorithm, Sweep-Clip, on a set of 100 randomly sampled Monday NYT crossword puzzles.We used two LLMs for this: GPT-4-Turbo and Llama 3 70B (see Appendix A).</p>
<p>Error Tolerance % of Crosswords We find (see Table 3) that our algorithm with GPT-4-Turbo solves 48% of crosswords without any errors and 55% of crosswords with at most 1 character wrong.The average character level accuracy in crossword solving is 93.1% (± 14.1%).Our algorithm improves the clue-wise answer accuracy 8 (exact match) to 89.6% (± 16.9%) from the base accuracy (without the algorithm), which is 43.5% (± 23.5%), an improvement of 2.1×.Note, the previously reported SoTA accuracy on this task with a foundational LM (without fine-tuning) was 26% with retrieval-augmented generation and an SMT solver coupled with an oracle that eliminates parts of the crossword grid that do not have suitable generated answers (Kulshreshtha et al., 2022).</p>
<p>The performance for the smaller LLaMA 3 70B is worse; however, our algorithm still manages to improve final clue answering accuracy to 59.4% (± 24.1%) from a base accuracy of only 22.3% (± 14.4%).Note that this result, alongside those presented in Section 3, serve to ablate our crosswordsolving approach.</p>
<p>Thus, with the application of our algorithm, we have successfully exploited the constraint information to boost the performance of LLMs beyond what would have been possible with straightforward QA like clue deciphering.To the best of our knowledge, this is the first algorithm that demonstrates successfully solving crosswords with the aid of an out-of-the-box LLM.We observed (see Table 4) that SoTA LLMs 8 clue-level accuracy is different from character-level accuracy (Kulshreshtha et al., 2022); e.g., it is possible to have a filled-in crossword without deciphering all clues.</p>
<p>Solving Cryptic</p>
<p>have significantly improved cryptic crossword clue deciphering abilities.We also note that chain-ofthought (Wei et al., 2022b) prompting (1-shot) with self-consistency (Wang et al., 2023) (3 samples) leads to further performance gains.Our best result on the word-init-disjoint split is 20.85%, which improves over the previous SoTA (9.5%) by a factor of 2.2× without any fine-tuning.</p>
<p>Data Contamination and Generalizability</p>
<p>To rule out data contamination as the reason behind the significant performance gains by SoTA LLMs, we curated additional cryptic crossword clue datasets comprising entirely of puzzles published after May 20, 2024, which is after the knowledge cutoff date of all LLMs examined.These datasets are sourced from The Guardian and Lovatts Puzzles.The answers in these post-cutoff sets were checked against the combination of all other cryptic crossword datasets employed in the study (665,497 answers in total) to check for potential duplicates.None was found for the post-cutoff Guardian set, and 2 were found for the post-cutoff Lovatts set, which were removed.Note: the Init dataset is also sourced from The Guardian; thus, the results reported in Table 5 should be consistent 9 .</p>
<p>Model</p>
<p>Lovatts Guardian init Llama 3 70B 26.03% 5.5 % 6.4 % Claude 3 Sonnet 46.28% 12.5% 10.8% GPT 4 Turbo 61.57% 18.5% 18.7%  2021), also sourced their data from The Guardian; thus, these results provide a fair head-to-head comparison of performance.</p>
<p>We see no appreciable difference in performance on the post-cutoff dataset (see Table 5), leading us to suggest that these LLMs can generalize beyond potential contamination in their training set.</p>
<p>Human Evaluation and Further Analysis</p>
<p>To ascertain if the models can reason about cryptic crossword clues to arrive at correct responses, we elect to perform human evaluation 10 of model responses.We employ a 3-shot chain-of-thought prompt to elicit a reasoned response to crossword clues from GPT-4-Turbo, which are then analyzed by our team for soundness vis-à-vis factual and logical errors.We choose 100 samples from the post-cutoff Lovatts set and rely on the consensus of all evaluators to ascertain if a response contains factual or logical errors (e.g., wrong counting, wrong anagram, drawing a conclusion that does not follow from the premise, etc.).The author-annotated reasoning responses had low inter-observer variability (Fleis' κ = 0.94, pre-consensus) and have been made publicly available.</p>
<p>GPT 4 Turbo</p>
<p>Sound ¬Sound  The results (see Table 6) show that 74% of the time GPT-4-Turbo provided a correct answer, it also gave sound reasoning in support of the answer.This leads us to conjecture that they possibly have a significant ability to reason and generalize.</p>
<p>Furthermore, to analyze if LLMs demonstrate any common failure we manually annotated the human evaluation dataset based on the principal skill required to solve a particular puzzle clue (see Table 7).ANG indicates that the answer is an anagram of some words of the clue, HOM indicates that the answer is a homophone of some words of the clue, CNT indicates that the answer is disguised in a contiguous section of the clue, and SCJ indicates that the answer is found by combining several words that are synonyms of various parts of the clue.The class OTH lumps together a variety of other kinds of skills (e.g., spoonerisms, acronyms, world knowledge, and various other kinds of character manipulations).Examples of each are provided in Appendix E.</p>
<p>Owing to the limited number of human annotations, the results (Table 7 left) are not statistically significant, but the trends suggest that GPT-4-Turbo may exhibit strong performance on anagrams (74% accuracy vs. 65% baseline 11 ) and homophone-11 Baseline refers to mean accuracy across all kinds of clues.</p>
<p>based clues (100% accuracy).We also attempted to perform the same analysis on the Cryptonite and Init datasets (4000 clue-answer pairs) and found similar trends for anagram performance by GPT-4-Turbo (25% accuracy vs. 21.1% baseline).However, this does not hold for Llama 3 70B (2.6% accuracy vs. 7.16% baseline) and Claude 3 Sonnet (7.9% accuracy vs. 11.85%baseline), suggesting that Llama 3 70B and Claude 3 Sonnet have a diminished ability to deal with anagrams.Note that unlike (ANG, CNT), (HOM, SCJ, OTH) cannot be automatically detected reliably, which limits broader analysis for these kinds of clues.</p>
<p>To quantify the effect of sub-token counting performance on clue-solving performance, we devise a further experiment.We consider all such clues for which the model correctly deduced the semantics of the clue but failed to adhere to the length constraints (e.g., LECTURER instead of PROFESSOR or NANNA instead of GRANNY).We consider wrong LLM predictions with a semantic similarity of 0.5 or more12 and report the percentage of answers of incorrect length.GPT-4-Turbo and Llama 3 70B produce predictions with length errors 46.4% and 59.9% of the time respectively.This suggests that the lack of adherence to length constraints is a major impediment in clue solving for LLMs.</p>
<p>Conclusions</p>
<p>Solving crosswords requires proficiency in a multitude of desirable skills, and cryptic crosswords were previously thought to be firmly in the domain of human experts.Our findings challenge this notion.We have demonstrated that the current generation of SoTA LLMs shows significantly improved aptitude at solving straight and cryptic crossword puzzle clues and can exploit constraints provided by partially solved crosswords to boost this performance further.We also found that this emergent ability generalizes well to the post knowledge cutoff regime and is accompanied with the capacity to produce reasoned explanations.</p>
<p>We have also developed an algorithm that, with the aid of LLMs, further boosts this performance and achieves 93% accuracy on Monday New York Times crosswords.With the aid of this algorithm, it is possible to achieve significant performance gains even if the baseline LLM accuracy is low, thus indicating that, when paired with the right search strategy, LLMs can successfully solve cross-   words.Such an LLM-guided search approach may be readily adapted to other scenarios requiring certain kinds of constraint satisfaction.</p>
<p>Incorporating length constraints effectively still remains a challenge.We showed that SoTA LLMs struggle with sub-token counting and also that they often provide incorrect predictions despite being semantically close, owing to length constraint violations.This weakness also manifests in other tasks like anagrams, character manipulations, etc., which are heavily reliant on arithmetic abilities.Future research attempting to address this shortcoming would be compelling.</p>
<p>Limitations</p>
<p>Limitation 1 (Algorithm) -We would like to highlight that our algorithm is sub-optimal, as it discards potential correct answers and does not fully explore the consequences of each possible generated answer.Ideally, we would generate one candidate answer, followed by all its neighbors, and whenever there is a conflict, branch out and explore all options to see which is a better fit.We elect not to do this, because it involves a potentially exponential number of LLM calls.Our algorithm does not provide guarantees of convergence or correctness beyond elimination of conflicting answers.Note that this problem is NP-Hard and it is difficult to find approximate solutions for it (Kulshreshtha et al., 2022).This algorithm was designed to minimize computational cost incurred in LLM calls; however, future studies with much larger computational budget for LLM calls will definitely have greater success employing a more thorough search strategy.Limitation 2 (Cryptic Crossword Solving) -Compared to straight crosswords, the baseline accuracy in clue answering by LLMs for cryptic crosswords is much lower (e.g., 18% for GPT-4-Turbo on cryptics vs 41% on straight).Thus, solving cryptic crosswords with current SoTA LLMs would require a much more extensive search strategy.Our current approach with a budget limit of 0.5 USD per crossword is unsuccessful (12% of cryptic crosswords solved with 50% letter accuracy) at solving cryptics, and our financial constraints do not permit a more thorough investigation with improved algorithms at this time.Limitation 3 (Reporting Crossword Solving Results) -We only reported results on solving Monday New York Times crossword puzzles and did not report Tuesday-Thursday which typically have a higher level of difficulty.There are primarily two reasons for this.(i) As we have seen in the paper, even if the base LLM has relatively low accuracy, when paired with a search/prune algorithm, the accuracy can be improved considerably.There is a trade-off between base accuracy and the number of LLM calls, i.e., if the base accuracy is lower, the algorithm needs to make a lot more calls to the LLM increasing computational costs.We have observed that it is possible to solve harder NYT (straight) crosswords, but at an increased computational budget.This makes reporting statistics cost-prohibitive for us at this time.Limitation 4 (Sub-token Counting) -We have demonstrated that LLMs show limitations at subtoken counting, and have hypothesised that they do not have widely generalizable skill at this task, rather relying on memorization.The evidence provided in this paper is compelling, but definitely not enough to conclusively prove this hypothesis.Further studies are required that can intervene at the pre-training stage of LLMs to conclusively demonstrate whether this hypothesis is true.We do not possess the computational power to undertake such a study at this stage.Limitation 5 (Fine-tuning) -We do not report results with fine-tuned models.It is possible that further improvements could be seen with fine-tuning, however, we are interested in studying the emergent properties of large language models in this context instead of building special purpose models that can solve crosswords.</p>
<p>Ethics Statement</p>
<p>In keeping with ACL ethical guidelines we make all scientific artifacts generated for this study freely available and open source under the MIT licence, this includes all software we created, every prompt that was used, all raw model outputs, and data we generated.The test sets used for all experiments are also made availabe, with the exception of the full grids of the New York Times crossword puzzles.We do not re-distribute the (publicly available) New York Times crossword dataset since it is the intellectual property of The New York Times.The use of their data falls within the terms of the Fair Use doctrine set within 17 U.S.C. §107.To aid in reproducibility we provide a list of dates to uniquely identify the crossword puzzles used in the study, and provide detailed instructions on how to acquire them through the New York Times.We do not use any data to train any model in this study.</p>
<p>We foresee no serious ethical implications on society at large from this study.</p>
<p>Liang, Jeff Dean, and William Fedus.2022a.Emergent abilities of large language models.Transactions on Machine Learning Research.Survey Certification.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022b.Chain-of-thought prompting elicits reasoning in large language models.Advances in neural information processing systems, 35:24824-24837.</p>
<p>A Appendix -Details of the Sweep Clip Algorithm</p>
<p>This section is intended to provide details of the SweepClip algorithm introduced in Section 4.</p>
<p>A crossword puzzle consists of a grid (see Figure 1) and a set of clues C = {c 1 , . . ., c n } and answers (ground truth) A = {a 1 , . . ., a n }.The grid imposes a graph G = (V, E), where V = {v 1 , . . ., v n } is the set of vertices corresponding to every clue/answer in the crossword, and
E = {(v i , v j )| ∀i, j i ̸ = j s.t. a i , a j share a grid position}(1)
Given an LLM, and a set of clues C ′ ⊆ C, C ′ = {c j 1 , c j 2 , . ..} corresponding to vertices {v j 1 , v j 2 , . ..}, we can generate candidate answers Â = {â j 1 , âj 2 , . ..},where â = LLM (c).We abbreviate this as Â = LLM (C ′ ).For a subset V ′ of V , let the set of clues associated with V be denoted by C(V ′ ).</p>
<p>In subsequent iterations of the algorithms, when some candidate answers have been generated, we generate further candidate answers for their neighboring vertices.In this case, the partially unmasked characters are provided to the LLM as in Section 3.3.Note, our algorithm implicitly performs selfconsistency checks to improve candidate answers, e.g., an answer generated at the first sweep may be discarded only to be accepted in later iterations, when it is coherent with a larger number of other answers.</p>
<p>Two candidate answers âi , âj are said to be in conflict, if µ-th position of a i and ν-th position of a j are in the same grid position, however âi [µ] ̸ = âj [ν], i.e. the µ-th character of a i and ν-th character of a j are different.</p>
<p>There are two sub-graphs G p ( Â) and G n ( Â) of G that correspond to a set of candidate answers Â.Let v i , v j correspond to âi , âj ∈ Â.An edge (v i , v j ) ∈ E is in G p ( Â) if and only if âi , âj don't conflict, else it is in G n ( Â).We call the largest connected component of a graph H, LCC(H), and for a subset S of vertices V of a graph G, ngbd(G, S) denotes the vertices in V that are adjacent to S but not in S. The algorithm is detailed in Algorithm 1. L ← LCC(G p ( Â)).</p>
<p>6:
Â ← {â i | v i ∈ L} 7:
while G n ( Â) has edges.do
8: A max degree vertex in G n ( Â) → v m 9:
Remove v m from G n ( Â). Calculate N ← ngbd(G, Â).</p>
<p>13:</p>
<p>Add character information to C(N ).This section contains details of the datasets used, and details relating to how they were curated.</p>
<p>B.1 Cryptonite and word-init-disjoint</p>
<p>Cryptonite13 is a cryptic crossword clue dataset introduced by Efrat et al. (2021), and is gathered from The Times and The Telegraph -two popular UK dailies.In their official split, the training and test sets do not contain any common answers, so that a model trained on the train split cannot rely on memorization to answer the cryptic crossword clue.Two thousand random samples were chosen from the train split to report results in Section 3.</p>
<p>word-init-disjoint (Init) is also a cryptic crossword clue dataset and was introduced by Rozner et al. (2021).This is gathered from The Guardian, and their official split introduces the additional constraint that if two answers share a common root (first two characters) they are placed in the same split.The results reported in 3 are based on two thousand randomly sampled clues from their official test split.We distribute all materials pertaining to this dataset14 with this paper.</p>
<p>B.2 New York Times Dataset</p>
<p>We collected and curated two datasets from the New York Times15 for our analysis of straight crossword puzzles.The first consists of 100 randomly sampled Monday crossword puzzles ranging from 20th January 1969 to 7th August 2023 with all clues (7700) and grid information, which were used to report results in Section 5.</p>
<p>Additionally, we curated 12,000 randomly sampled clue answer pairs split into two sets -test (2000) and support (10,000).All three sets are completely disjoint. 16he test and support sets are distributed alongside this paper in the github repository, however, the 100 complete grids are not, as they are intellectual property of the New York Times.They can be accessed after purchasing a subscription to their service.To aid reproducibility we include the list of dates to uniquely identify the crossword grids used to report results.</p>
<p>B.3 Post-cutoff datasets</p>
<p>We curated two post-cutoff datasets to test for potential dataset contamination.One of them was sourced from Lovatts puzzles17 and The Daily Mail.In total, 244 clues and answers were manually curated from puzzles published strictly after May 20, 2024.To maintain parity with the Init dataset by Rozner et al. (2021) we also ob-tained 200 clue-answer pairs from The Guardian published after May 20, 2024.</p>
<p>We cross-checked both these datasets for duplicate clues against all 665,497 clues from the Cryptonite and Init datasets.We found two duplicates for the Lovatts set which were removed.The dataset obtained from The Guardian contained no duplicates.This does not guarantee that these cryptic crossword clues have not appeared in any form previously, but we estimate that probability to be low, since there were very few duplicates when matched against the extremely large cryptic crossword datasets.Both these datasets are distributed alongside the paper in the github repository.</p>
<p>B.4 Sub-token counting</p>
<p>For the task in Section 3.4 we first obtained a list of 300,000+ words and their unigram frequencies provided by Segaran and Hammerbacher (2009) and sourced from the Google Trillion Words corpus.For the Rare, Medium and Common words we randomly selected 1000 words each from ranks 95,000 -100,000, 47,500 -52,500 and 0 -5,000, respectively.The Vocab set was constructed by taking an intersection of the vocabulary tokens of every open source language model in consideration, i.e., Phi 3, LLaMA 3 70B, LLaMA 3 8B, LLaMA 2, Mistral 7B, and Mixtral.Further, we took the intersection of this set with the set of top 100,000 words to make sure the chosen tokens are actual words and not special tokens.We sampled 1000 words from the resulting set to create the vocab set.For the gibberish set, we replaced each character from each word in the vocab set with a character randomly sampled from the set {a-z}.This ensures that the distribution of lengths of words seen in the gibberish set is the same as the distribution of lengths in the vocab sets.These 5 sets are distributed alongside the paper in the github repository.</p>
<p>C Appendix -Experimental Details C.1 General Overview</p>
<p>We used temperature=0.5 for all models and experiments.For few-shot prompt responses, max-tokens were set to 10, and for chain-ofthought responses, max-tokens was set to 1000.All other paramters including top_p, top_k, etc., were set to their defaults.The Jinja templates used to format the text prompts are provided alongside the paper, and are borrowed from the model-can also mean the drug MDMA or commonly known as e.\n 'ecstasy in old forbidden city' can mean e in Beijing.\nBeijing is also called Peking.\n e in Peking can be peeking which is a synonym of 'looking where one shouldn't '.\n peeking is 7 characters and therefore fits the character count.\nAnswer:PEEKING"</p>
<p>E Appendix -Human Evaluation and Reasoning Examples</p>
<p>Consider the following example cryptic crossword clue -"Culminating point of story about Judy's husband by the railway track (5,4)".18Solving this clue requires connecting 'Judy' to the popular puppet show Punch and Judy, and inferring that 'Judy's husband' refers to PUNCH.Additionally, we must observe that 'railway track' is synonymous to LINE, and combining these gives PUNCHLINE which also means 'Culminating point of story'.In this section we provide a few Chain-of-thought elicited reasoning examples given by GPT-4-Turbo on the Lovatts dataset.These are sampled from the set used to perform human evaluation.Further, these samples are manually tagged with the principal skill required to solve them.The skillbased categories are:</p>
<p>• ANG -the answer is an anagram of some words of the clue (e.g., CUBIT IS mixed up cookie → BISCUIT).</p>
<p>• HOM -the answer is a homophone of some words of the clue (e.g., Heard PRINTS are for royalty → PRINCE).</p>
<p>• CNT -the answer is disguised in a contiguous section of the clue (e.g., The Press leaves presENTERs to go in → ENTER).</p>
<p>• SCJ -the answer is found by combining several words which are synonyms of various parts of the clue (e.g., Reasonable FOOD ALLOWANCE 1 for CAPONE 2 → RATION 1 AL 2 ).</p>
<p>• OTH -this class lumps together variety of other kinds of skills like Spoonerisms, acronyms, world knowledge and various other kinds of character manipulations (e.g.PIpER loses heart on jetty → PIER).</p>
<p>Human evaluation was performed by 3 of the authors of this paper.They were asked if a particular response contained logical or factual errors and provided detailed instructions and examples of the kinds of errors to be wary of.All responses were evaluated by all authors and in case of conflicting answers (4 out of 100, Fleiss' κ = 0.94), discussions were held to reach a consensus.All evaluated responses alongside human-annotated labels are provided alongside this paper in the github repository.</p>
<p>In checking whether a justification for an answer given by the LLM is logically and factually sound, we assess grammatical soundness and phraseological meaningfulness of the sentences in the answer, existence of counterfactual statements (e.g., "BULKY has 4 characters", "the initial letters of ARE RATS TIRED NOW are ARTS" etc.), presenting a statement as an inference from previous statements when it does not follow from those, etc.If an answer by the LLM is found unsatisfactory in any of these aforementioned areas, it is labeled FALSE.Following are a few samples of the same.Taking the initial letters of each word, we get A, R, E, and A. Putting these letters together forms the word AREA.'Area' is a synonym for 'region' and fits the clue both in definition and letter count.AREA is 4 characters and therefore fits the character count.Answer: AREA 2. Clue: Fur stayed in audition for medical help Answer (Ground Truth): FIRSTAID Human Consensus (Soundness): FALSE GPT-4-Turbo Output: 'Fur' can mean pelt, coat, hide.'stayed' can mean remained, lingered.'in audition' indicates a homophone clue (sounds like).'for medical help' can mean aid, care, assistance.Combining these: -'Fur stayed in audition' suggests a homophone of a phrase where 'fur' and 'stayed' combine to sound like another word.</p>
<p>-'Fur' as 'fur' and 'stayed' sounds like 'aid' (homophone of stayed).</p>
<p>-Putting them together doesn't form a clear word, so reconsidering 'fur' might lead to another homophone, such as 'first aid' (fur sounded + stayed sounding like first aid).</p>
<p>'First aid' fits as a term for 'medical help'.'First aid' is 8 characters and therefore fits the character count.Answer: FIRST AID 3. Clue: Be earlier because of self-esteem Answer (Ground Truth): FOREGO Human Consensus (Soundness): TRUE GPT-4-Turbo Output: 'Be earlier' can suggest synonyms such as precede, antedate.'because of' is often used in cryptic clues as a linking phrase or to indicate causation.'self-esteem' can be linked to synonyms like pride, ego.</p>
<p>Breaking down the phrase: 'Be earlier' suggests a word meaning to come before -"precede" fits but is too long.Let's consider "ego" for 'self-esteem', which is a common synonym.Looking at 'Be earlier because of self-esteem', we can interpret this as a word that indicates being earlier due to ego, or ego causing one to be first or ahead.The answer "PRE" (as in prefix, or before) combined with "EGO" gives us PREGO, which is 5 letters and does not fit.However, considering synonyms and length, the correct answer could be "PRECEDE" if we think of it as "PRE" (before) + "CEDE" (yield, coming from ego or self-esteem yielding to others).But this is 7 letters.Reevaluating, if we simplify to just consider "ego" leading or being earlier, we can think of "PREGO" as a playful or informal way to say "before" due to ego, but the character count is off.The best fit for a 6-letter word considering the clue and synonyms would be "BEFORE," which directly means "be earlier" and can be influenced by one's self-esteem (ego) driving their actions to be ahead or first.This interpretation fits the clue structure and the character count.Answer: BEFORE</p>
<p>F Appendix -Results</p>
<p>Model</p>
<p>Figure 1 :
1
Figure 1: Example of a crossword puzzle (left) and cryptic clues (right).(left) The grid must be filled up with answers from the semantic clues provided.The gray highlighted squares produce additional constraints, e.g., first character of the answer to clue 1 (across) and clue 4 (down) must be the same.Example by Fred Piscop.(right) In cryptic crosswords, the clues involve some form of wordplay and synonyms and often involve world knowledge.Examples are taken from the cryptonite dataset (see Appendix E for more).</p>
<p>Figure 3 :
3
Figure 3: LLMs ability to count the number of characters in a word declines with the frequency of the word.</p>
<p>, and OTH refers to others.(Left) shows results from GPT-4-Turbo on the post-cutoff Lovatts set, and (right) shows results on the combined Init and Cryptonite datasets.The numbers in parentheses refer to prevalence of the clue type.</p>
<p>C, crossword graph G and an LLM.2: Generate Â ← LLM (C).3: for i in {1, . . ., max_iter} do 4: Construct G p ( Â), G n ( Â) 5:</p>
<p>The results in Section 5.1 are produced with max_iter of 30 and a budget of 0.5 USD per crossword for GPT-4-Turbo, and max_iter of 35 and a budget of 600 LLM calls for LLaMA 3 70B.</p>
<ol>
<li>Clue: Initially, any region extending around land Answer (Ground Truth): AREA Human Consensus (Soundness): FALSE GPT-4-Turbo Output: 'Initially' is often used in cryptic crosswords to indicate taking the initial letters of the following words.'any region extending around land' gives us the words Any, Region, Extending, and Land.</li>
</ol>
<p>Table 1 :
1
Sadallah et al. (2024)prove by using character constraints from partially filled crosswords.Sadallah et al. (2024)reported an accuracy of 27.0% (70% hinted clues) by fine-tuning a Mistral 7B model on the Init dataset, which GPT-4-Turbo (76.30% accuracy) outperforms by a factor of ∼2.8× without fine-tuning.
Turbo with 5-</p>
<p>Table 3 :
3
Results from solving NYT crosswords with our algorithm SweepClip.
LLaMa 3 GPT-4 T100% solved048≤ 1 character error 155≤ 5 character error 1071≥ 90% Accuracy3080≥ 50% Accuracy8298</p>
<p>Table 4 :
4
Comparison of our results with previously reported SoTA results.Results are on the Init dataset with crossword clue deciphering treated as QA.
Crossword CluesModelMethodAcc. (%)Rule-basedCFG+WordNet(Deits, 2015)7.3T5 (Efrat et al., 2021) SFT1.1T5Curriculum(Rozner et al., 2021)Learning6.5Mistral 7BSFT1.2Mistral 7B10 shot4.6Chat GPT3 shot9.5(Sadallah et al., 2024)GPT 4 Turbo (ours)5 shot18.70GPT 4 Turbo (ours)CoT(1)@3SC20.85</p>
<p>Table 5 :
5
Performance (accuracy) of LLMs on curated datasets that appeared after the knowledge cut-off.</p>
<p>Note, Init byRozner et al. (</p>
<p>Table 6 :
6
Results from human evaluation of reason-</p>
<p>ing.Chain-of-thought elicited reasoning responses from GPT-4-Turbo are evaluated for soundness.An answer is called correct if the model prediction exactly matches the ground truth.The answer is called sound if it contains no logical or factual errors.Results are on the post-cutoff Lovatts' set.</p>
<p>Table 7 :
7
Do LLMs have common "failure modes"?ANG refers to anagrams, SCJ refers to synonym conjugation, CNT refers to containment, HOM refers to homophones</p>
<p>Table 8 :
8
Dataset details.
B Appendix -DatasetsDataset (Clues)TrainValidation TestCryptonite470,804 26,15626,157word-init-75,84732,62833,905disjointNYT (Clues)10,0002,000NYT (Grids)100 (7700 clues) Test onlyAfter May 20, 2024Lovatts242The Guardian200</p>
<p>Table 10 :
10
Data for results plotted in Figure3.</p>
<p>Code, data, etc., can be found at https://www.github. com/espressovi/LMCrossword
Further increases (25-shot) did not yield performance benefits.
We elected not to perform experiments with 10-shot prompts on Claude and GPT-4-Turbo due to budget constraints.
T=0.5, max-tokens=10, more details in Appendix C.
We can't be sure about the proprietary models.
with appropriate masks provided as prompts, see Appendix A.
More details on data curation is in Appendix B.
As given by OpenAI text-embedding-3-large model.
https://github.com/aviaefrat/cryptonite
https://github.com/jsrozner/decrypt
https://www.nytimes.com/crosswords/
https://www.xwordinfo.com/ is useful for older NYT crosswords
https://lovattspuzzles.com/
Example taken from https://lovattspuzzles.com.
AcknowledgmentsThe authors would like to thank Akshay Chaturvedi and Joy Mahapatra for their helpful feedback onThis research was funded in part by the Indo-French Centre for the Promotion of Advanced Research (IFCPAR/CEFIPRA) through project number CSRP 6702-2.ModelParams  cards of the respective models.Claude and GPT models were accessed through their respective APIs and all other models were run locally on one server consisting of 2x80GB Nvidia A100 GPUs.All models were used in bf16 format whenever supported.Format of prompts are given in Appendix D.C.2 Experimental Protocols for Section 3.3When creating a query for a particular test instance with k% hints, we randomly selected N few-shot instances and ensured that few shot examples also had k% hints.The number of characters revealed (h) is given by the formula -h many characters are randomly selected and revealed, all other characters are replaced with "_".D Appendix -PromptsIn this section we will detail the prompts used for all experiments presented in the paper.Every prompt used in generating outputs presented in this paper is provided alongside the paper in the github repository.D.1 Few Shot PromptsFor the experiments presented in Section 3.2, we used prompts in the following format:[{ "role":"system", "content": "You are an expert crossword solver.For the experiments presented in Section 3.3, we used prompts in the following format (with the same system prompt):[{ "role":"user", "content": "Clue: <clue 1> (3) // _ _ N =&gt; MEN\n Clue: &lt; ... &gt; (6) // _ _ E _ _ _ =&gt; BREATH\n Clue: <query clue> (length) // _ _ X _ X =&gt; " }]D.2 Chain-of-Thought PromptsFor the results presented in Table4and for humanevaluation we used chain-of-thought prompts.These were based on 10 examples we manually created and are diverse in terms of skills required to solve them.An example chain-of-thought reasoning prompt is as follows:"Clue: ecstasy in old forbidden city, looking where one shouldn't (7
. Barun Norick, Daniel Patra, Thomas Perez-Becker, Reid Portet, Heyang Pryzant, Marko Qin, Corby Radmilac, Sambudha Rosset, Olatunji Roy, Olli Ruwase, Amin Saarikivi, Adil Saied, Michael Salim, Shital Santacroce, Ning Shah, Hiteshi Shang, Swadheen Sharma, Xia Shukla, Masahiro Song, Andrea Tanaka, Xin Tupini, Lijuan Wang, Chunyu Wang, Yu Wang, Rachel Wang, Guanhua Ward, Philipp Wang, Haiping Witte, Michael Wu, Bin Wyatt, Can Xiao, Jiahang Xu, Weijian Xu, Sonali Xu, Fan Yadav, Jianwei Yang, Ziyi Yang, Yifan Yang, Donghan Yang, Lu Yu, Chengruidong Yuan, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhang, Zhou, arXiv:2404.142192024PreprintPhi-3 technical report: A highly capable language model locally on your phone</p>
<p>Webcrow: A web-based crosswords solver. Giovanni Angelini, Marco Ernandes, Marco Gori, Intelligent Technologies for Interactive Entertainment. Berlin, Heidelberg; Berlin HeidelbergSpringer2005</p>
<p>The claude 3 model family: Opus, sonnet, haiku. Anthropic, 2024</p>
<p>Cryptonite: A cryptic crossword benchmark for extreme ambiguity in language. Avia Efrat, Uri Shaham, Dan Kilman, Omer Levy, 10.18653/v1/2021.emnlp-main.344Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Dr. fill: Crosswords and an implemented solver for singly weighted csps. Matthew L Ginsberg, Journal of Artificial Intelligence Research. 422011</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, arXiv:2310.06825Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023Preprint</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Renard Lélio, Lucile Lavaud, Marie-Anne Saulnier, Pierre Lachaux, Sandeep Stock, Sophia Subramanian, Yang, arXiv:2401.04088Mixtral of experts. Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed, 2024Preprint</p>
<p>Automata-based constraints for language model decoding. Terry Koo, Frederick Liu, Luheng He, First Conference on Language Modeling. 2024</p>
<p>Down and across: Introducing crossword-solving as a new NLP benchmark. Saurabh Kulshreshtha, Olga Kovaleva, Namrata Shivagunde, Anna Rumshisky, 10.18653/v1/2022.acl-long.189Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>A probabilistic approach to solving crossword puzzles. L Michael, Greg A Littman, Noam Keim, Shazeer, 10.1016/S0004-3702(01)00114-XArtificial Intelligence. 13412002</p>
<p>Large language models as minecraft agents. Chris Madge, Massimo Poesio, abs/2402.083922024ArXiv preprint</p>
<p>Introducing meta llama 3: The most capable openly available llm to date. Meta. 2024</p>
<p>Large language models: A survey. Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, Jianfeng Gao, arXiv:2402.061962024Preprint</p>
<p>Chatgpt technical report. OpenAI. 2023. Gpt-4 techincal report. 2021OpenAI</p>
<p>Constrained language models for interactive poem generation. Andrei Popescu-Belis, Àlex Atrio, Valentin Minder, Aris Xanthos, Gabriel Luthier, Simon Mattei, Antonio Rodriguez, Proceedings of the Thirteenth Language Resources and Evaluation Conference. the Thirteenth Language Resources and Evaluation ConferenceMarseille, FranceEuropean Language Resources Association2022</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, J. Mach. Learn. Res. 2112020</p>
<p>Decrypting cryptic crosswords: Semantically complex wordplay puzzles as a target for nlp. Josh Rozner, Christopher Potts, Kyle Mahowald, Advances in Neural Information Processing Systems. Curran Associates, Inc202134</p>
<p>Abdelrahman "boda" Sadallah, Daria Kotova, Ekaterina Kochmar, arXiv:2403.12094Are llms good cryptic crossword solvers? Preprint. 2024</p>
<p>Beautiful Data: The Stories Behind Elegant Data Solutions. T Segaran, J Hammerbacher, 2009Media</p>
<p>Neural machine translation of rare words with subword units. Rico Sennrich, Barry Haddow, Alexandra Birch, 10.18653/v1/P16-1162Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, Germany20161Association for Computational Linguistics</p>
<p>Solving crossword puzzles as probabilistic constraint satisfaction. Noam M Shazeer, Michael L Littman, Greg A Keim, Proceedings of the Sixteenth National Conference on Artificial Intelligence and the Eleventh Innovative Applications of Artificial Intelligence Conference Innovative Applications of Artificial Intelligence, AAAI '99/IAAI '99. the Sixteenth National Conference on Artificial Intelligence and the Eleventh Innovative Applications of Artificial Intelligence Conference Innovative Applications of Artificial Intelligence, AAAI '99/IAAI '99USAAmerican Association for Artificial Intelligence1999</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov ; Zheng Yan, Iliyan Zarov, Yuchen Zhang, arXiv:2307.09288Pushkar Mishra, Igor Molybog. Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurelien RodriguezPreprintand Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017. Long Beach, CA, USA2017. December 4-9, 2017</p>
<p>Automated crossword solving. Eric Wallace, Nicholas Tomlin, Albert Xu, Kevin Yang, Eshaan Pathak, Matthew Ginsberg, Dan Klein, 10.18653/v1/2022.acl-long.219ACL:2022:long. Dublin, Ireland2022</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, Transactions on Machine Learning Research. 2024</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy</p>            </div>
        </div>

    </div>
</body>
</html>
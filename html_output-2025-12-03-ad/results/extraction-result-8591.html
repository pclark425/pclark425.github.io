<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8591 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8591</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8591</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-275921050</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2501.14851v2.pdf" target="_blank">JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Logical reasoning is a critical component of Large Language Models (LLMs), and substantial research efforts in recent years have aimed to enhance their deductive reasoning capabilities. However, existing deductive reasoning benchmarks, which are crucial for evaluating and advancing LLMs, are inadequate due to their lack of task complexity, presence of prior knowledge as a confounder, and superficial error analysis. To address these deficiencies, we introduce JustLogic, a synthetically generated deductive reasoning benchmark designed for rigorous evaluation of LLMs. JustLogic is (i) highly complex, capable of generating a diverse range of linguistic patterns, vocabulary, and argument structures; (ii) prior knowledge independent, eliminating the advantage of models possessing prior knowledge and ensuring that only deductive reasoning is used to answer questions; and (iii) capable of in-depth error analysis on the heterogeneous effects of reasoning depth and argument form on model accuracy. Our experimental results on JustLogic reveal that (i) state-of-the-art (SOTA) reasoning LLMs perform on par or better than the human average but significantly worse than the human ceiling, and (ii) SOTA non-reasoning models still underperform the human average. All code and data are available at https://github.com/michaelchen-lab/JustLogic</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8591.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8591.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek R1</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art reasoning-focused large language model evaluated on the JustLogic deductive reasoning benchmark; reported as the best-performing model in this study, particularly strong at higher reasoning depths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek R1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in the paper as a SOTA reasoning model (one variant 'Distill Qwen 14B' is also listed); characterized as optimized for reasoning tasks and showing superior performance at larger reasoning depths versus other models evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>JustLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A synthetic, programmatically generated natural-language deductive reasoning benchmark where the model must decide if a query statement is true, false, or uncertain given a paragraph of premises (propositional logic argument forms, depths 1-7 in the main set). Dataset is prior-knowledge independent and measures argument-form and reasoning-depth effects.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated primarily with chain-of-thought (CoT) prompting; compared across prompting methods (zero-shot, few-shot, CoT) and evaluated on increased depths; model is described as trained/tuned for reasoning (paper contrasts reasoning vs non-reasoning models).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>80.9% accuracy on JustLogic (best-performing model reported).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperformed all other evaluated LLMs (OpenAI o1 ~72.9%, GPT-4o ~65.6%, Llama3 variants ~50–58% depending on prompting). Shows smaller accuracy decline with increasing reasoning depth compared to many other models (notably OpenAI o1-mini).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Although strongest among evaluated models, still well below human ceiling (100%); performance degrades with increasing reasoning depth (but less sharply than many competitors); still susceptible to the same failure modes documented (logical inconsistency, wrong application or choice of argument forms, false interpretation of facts).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>DeepSeek R1 demonstrates that specialized reasoning-focused training can substantially improve deductive performance, particularly at high depths; supporting deeper, longer chains of reasoning is critical for better deductive accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8591.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8591.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI o1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI o1 (2024 system / reasoning-focused)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning-focused OpenAI model reported to perform well on JustLogic and trained with reinforcement learning techniques to improve chain-of-thought style reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI o1-2024-12-17</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described as a reasoning-focused model trained to reason with chain-of-thought prompts using a reinforcement learning algorithm (per the paper's citations).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>JustLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same JustLogic deductive reasoning benchmark (3-way classification: True/False/Uncertain) focusing on propositional argument forms across multiple depths.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with prompting methods including zero-shot, few-shot, and CoT; model benefits from CoT and is described as having been trained to follow CoT-style instructions (reinforcement learning on CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>72.9% accuracy on JustLogic (reported as second-best in main summary).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Performs substantially worse than DeepSeek R1 (−~8 percentage points) especially at high reasoning depths; matches DeepSeek R1 and other models on low/medium depths but drops sharply at high depths (paper reports premature 'Uncertain' answers and shorter responses at depth>=5).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Sharp decline at high reasoning depths; qualitative finding that o1 often prematurely answers 'Uncertain' for deep problems and produces shorter, less-engaged CoT traces on depth>=5; susceptible to the general failure modes (logical inconsistency, wrong argument-form application, false fact interpretation). The authors hypothesize possible test-time compute limits affecting deep-problem performance.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Reinforcement learning targeted at CoT-style prompting appears helpful, but system-level constraints (e.g., limited test-time compute or generation policies) can cause severe degradations on high-depth deductive tasks; instruction-following and sustained chain-of-thought engagement are key.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8591.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8591.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (2024-05-13 / GPT-4 family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI model (GPT-4o variant) evaluated on JustLogic; shown to be substantially stronger than earlier non-reasoning models but below the human average and specialized reasoning models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-2024-05-13</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large OpenAI model (GPT-4 family); characterized as a non-reasoning/general model in this study that benefits from chain-of-thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>JustLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>JustLogic deductive reasoning benchmark testing propositional-logical argument forms across controlled depths; models must decide true/false/uncertain from premises alone.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with zero-shot, few-shot, CoT prompting and additional prompting methods (self-consistency, tree-of-thought) in appendices; CoT provided notable improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>65.6% accuracy on JustLogic (reported as third-best in summary).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Underperforms DeepSeek R1 and OpenAI o1 but outperforms smaller Llama3 models in CoT settings; gains from prompting (CoT) are comparable to those seen in other large non-reasoning models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performance declines with increasing reasoning depth; common failure modes include wrong argument-form application and false interpretation of facts. No model-specific unique failure beyond general patterns reported.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Large general-purpose LLMs with CoT prompting can achieve moderate deductive performance but remain below specialized reasoning models and the human ceiling; prompting is often more effective than simply increasing parameter count.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8591.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8591.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o-mini (2024-07-18 variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller GPT-4o variant evaluated on JustLogic; shows moderate performance that is sensitive to reasoning depth and prompting technique.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini-2024-07-18</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described as a mini/smaller variant of GPT-4o used in experiments; evaluated with zero-shot, few-shot, and CoT prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>JustLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>JustLogic deductive reasoning benchmark (3-way classification) testing propositional reasoning with varying argument depths.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated under zero-shot, few-shot, and CoT prompting; some additional experiments (SC, ToT) in appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported table snippets indicate mid-50s under some settings; prose highlights that some smaller reasoning models (e.g., o1-mini) matched DeepSeek R1 at low depths but drop at higher depths — GPT-4o-mini shows similar sensitivity (detailed per-prompt numbers in Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Performs worse than larger GPT-4o and specialized DeepSeek R1; prompting (CoT) improves results but depth sensitivity remains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Sharp decline in performance as reasoning depth increases; general failure modes (misapplication of argument forms, logical inconsistency, false interpretation) observed.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Smaller variants can perform reasonably on shallow deductive tasks when given CoT prompts but struggle to sustain correct chains at greater depths; model scale and prompting both matter.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8591.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8591.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large (70B) Llama 3 instruction-tuned model evaluated on JustLogic; non-reasoning-focused but benefits from chain-of-thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned Llama 3 model of 70B parameters described as a non-reasoning model in this paper; hyperparameters and prompting details are aligned with recommendations in the Llama3 literature (paper cites Dubey et al., 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>JustLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>JustLogic deductive reasoning benchmark; tests propositional logic argument forms across depths 1–7 (in main set).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with zero-shot, few-shot, and CoT prompting; hyperparameters (temperature, top-p, penalties) chosen per paper's implementation details.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot: 53.1% (reported); CoT: ~58.6% (Table 9 shows CoT results for Llama3-70B around high 50s).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Performs better than Llama3-8B; CoT prompting gives larger gains than naive scale increase (Llama3-8B with CoT can beat zero-shot 70B in some settings).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Degrades with increasing reasoning depth; common failure modes as documented (wrong argument-form application, incorrect translations of natural language into logical form, logical inconsistency).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Model scale produces gains but prompting (CoT) can yield larger improvements; non-reasoning models benefit substantially from CoT prompting even without specialized reasoning training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8591.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8591.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller (8B) instruction-tuned Llama 3 model evaluated on JustLogic; non-reasoning-focused and shows sensitivity to prompting approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned 8B-parameter Llama 3 model described as a non-reasoning model in experiments; recommended hyperparameters and prompting choices follow Llama3 guidance in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>JustLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>JustLogic deductive reasoning benchmark (three-way truth-value classification from premises).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with zero-shot, few-shot, and CoT prompting; also explored additional prompting strategies (SC, ToT) in appendices with mixed outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot: 49.8% accuracy; Chain-of-thought (CoT): 57.8% (reported in text — CoT outperforms a larger zero-shot model).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Smaller model underperforms larger Llama3-70B but CoT prompting closes much of the gap; CoT Llama3-8B (57.8%) higher than zero-shot Llama3-70B (53.1%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Pronounced decline with increasing reasoning depth; subject to common failure modes (logical inconsistency, wrong argument-form application, mis-translation of facts). Tree-of-thought often hallucinated and provided little benefit for many models.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Prompt engineering (CoT) can meaningfully improve deductive reasoning in small non-reasoning models, often more than a modest increase in model size; however, small models still struggle at larger reasoning depths.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8591.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8591.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI o1-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI o1-mini (2024-07-18 variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller reasoning-focused OpenAI model variant evaluated on JustLogic; shows adequate performance at low reasoning depths but rapid degradation at higher depths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI o1-mini-2024-07-18</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Smaller variant of OpenAI's o1 family described as reasoning-focused; evaluated across prompting methods and depths in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>JustLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>JustLogic benchmark testing strictly deductive propositional reasoning (true/false/uncertain classification given premises).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Tested with zero-shot, few-shot, and CoT prompting; qualitative analysis compares o1-mini to DeepSeek R1 across depths.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported to perform comparably to DeepSeek R1 at low depths but experiences a sharp decline at higher depths (no single aggregate accuracy value given in prose for o1-mini; table snippets show mid-50s to low-60s depending on prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Matches top models on low-depth problems but loses advantage quickly as depth increases, falling close to or below larger non-reasoning models on very deep problems.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Severe drop in high-depth performance; fails to sustain correct multi-step reasoning chains — similar failure modes as other models.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Being fine-tuned/trained for reasoning helps at shallow problem depths, but maintaining accuracy on deep deductive chains requires capacity and/or training beyond what this smaller variant exhibits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ProofWriter <em>(Rating: 2)</em></li>
                <li>CLUTRR <em>(Rating: 2)</em></li>
                <li>LogiQA 2.0 <em>(Rating: 2)</em></li>
                <li>FOLIO <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Tree-of-thought (tree-of-thought framework reference in paper) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8591",
    "paper_id": "paper-275921050",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "DeepSeek R1",
            "name_full": "DeepSeek R1",
            "brief_description": "A state-of-the-art reasoning-focused large language model evaluated on the JustLogic deductive reasoning benchmark; reported as the best-performing model in this study, particularly strong at higher reasoning depths.",
            "citation_title": "JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models",
            "mention_or_use": "use",
            "model_name": "DeepSeek R1",
            "model_description": "Described in the paper as a SOTA reasoning model (one variant 'Distill Qwen 14B' is also listed); characterized as optimized for reasoning tasks and showing superior performance at larger reasoning depths versus other models evaluated.",
            "model_size": null,
            "reasoning_task_name": "JustLogic",
            "reasoning_task_description": "A synthetic, programmatically generated natural-language deductive reasoning benchmark where the model must decide if a query statement is true, false, or uncertain given a paragraph of premises (propositional logic argument forms, depths 1-7 in the main set). Dataset is prior-knowledge independent and measures argument-form and reasoning-depth effects.",
            "method_or_approach": "Evaluated primarily with chain-of-thought (CoT) prompting; compared across prompting methods (zero-shot, few-shot, CoT) and evaluated on increased depths; model is described as trained/tuned for reasoning (paper contrasts reasoning vs non-reasoning models).",
            "performance": "80.9% accuracy on JustLogic (best-performing model reported).",
            "baseline_comparison": "Outperformed all other evaluated LLMs (OpenAI o1 ~72.9%, GPT-4o ~65.6%, Llama3 variants ~50–58% depending on prompting). Shows smaller accuracy decline with increasing reasoning depth compared to many other models (notably OpenAI o1-mini).",
            "limitations_or_failures": "Although strongest among evaluated models, still well below human ceiling (100%); performance degrades with increasing reasoning depth (but less sharply than many competitors); still susceptible to the same failure modes documented (logical inconsistency, wrong application or choice of argument forms, false interpretation of facts).",
            "insights_or_conclusions": "DeepSeek R1 demonstrates that specialized reasoning-focused training can substantially improve deductive performance, particularly at high depths; supporting deeper, longer chains of reasoning is critical for better deductive accuracy.",
            "uuid": "e8591.0",
            "source_info": {
                "paper_title": "JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "OpenAI o1",
            "name_full": "OpenAI o1 (2024 system / reasoning-focused)",
            "brief_description": "A reasoning-focused OpenAI model reported to perform well on JustLogic and trained with reinforcement learning techniques to improve chain-of-thought style reasoning.",
            "citation_title": "JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models",
            "mention_or_use": "use",
            "model_name": "OpenAI o1-2024-12-17",
            "model_description": "Described as a reasoning-focused model trained to reason with chain-of-thought prompts using a reinforcement learning algorithm (per the paper's citations).",
            "model_size": null,
            "reasoning_task_name": "JustLogic",
            "reasoning_task_description": "Same JustLogic deductive reasoning benchmark (3-way classification: True/False/Uncertain) focusing on propositional argument forms across multiple depths.",
            "method_or_approach": "Evaluated with prompting methods including zero-shot, few-shot, and CoT; model benefits from CoT and is described as having been trained to follow CoT-style instructions (reinforcement learning on CoT).",
            "performance": "72.9% accuracy on JustLogic (reported as second-best in main summary).",
            "baseline_comparison": "Performs substantially worse than DeepSeek R1 (−~8 percentage points) especially at high reasoning depths; matches DeepSeek R1 and other models on low/medium depths but drops sharply at high depths (paper reports premature 'Uncertain' answers and shorter responses at depth&gt;=5).",
            "limitations_or_failures": "Sharp decline at high reasoning depths; qualitative finding that o1 often prematurely answers 'Uncertain' for deep problems and produces shorter, less-engaged CoT traces on depth&gt;=5; susceptible to the general failure modes (logical inconsistency, wrong argument-form application, false fact interpretation). The authors hypothesize possible test-time compute limits affecting deep-problem performance.",
            "insights_or_conclusions": "Reinforcement learning targeted at CoT-style prompting appears helpful, but system-level constraints (e.g., limited test-time compute or generation policies) can cause severe degradations on high-depth deductive tasks; instruction-following and sustained chain-of-thought engagement are key.",
            "uuid": "e8591.1",
            "source_info": {
                "paper_title": "JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (2024-05-13 / GPT-4 family)",
            "brief_description": "An OpenAI model (GPT-4o variant) evaluated on JustLogic; shown to be substantially stronger than earlier non-reasoning models but below the human average and specialized reasoning models.",
            "citation_title": "JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-4o-2024-05-13",
            "model_description": "A large OpenAI model (GPT-4 family); characterized as a non-reasoning/general model in this study that benefits from chain-of-thought prompting.",
            "model_size": null,
            "reasoning_task_name": "JustLogic",
            "reasoning_task_description": "JustLogic deductive reasoning benchmark testing propositional-logical argument forms across controlled depths; models must decide true/false/uncertain from premises alone.",
            "method_or_approach": "Evaluated with zero-shot, few-shot, CoT prompting and additional prompting methods (self-consistency, tree-of-thought) in appendices; CoT provided notable improvements.",
            "performance": "65.6% accuracy on JustLogic (reported as third-best in summary).",
            "baseline_comparison": "Underperforms DeepSeek R1 and OpenAI o1 but outperforms smaller Llama3 models in CoT settings; gains from prompting (CoT) are comparable to those seen in other large non-reasoning models.",
            "limitations_or_failures": "Performance declines with increasing reasoning depth; common failure modes include wrong argument-form application and false interpretation of facts. No model-specific unique failure beyond general patterns reported.",
            "insights_or_conclusions": "Large general-purpose LLMs with CoT prompting can achieve moderate deductive performance but remain below specialized reasoning models and the human ceiling; prompting is often more effective than simply increasing parameter count.",
            "uuid": "e8591.2",
            "source_info": {
                "paper_title": "JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "GPT-4o-mini",
            "name_full": "GPT-4o-mini (2024-07-18 variant)",
            "brief_description": "A smaller GPT-4o variant evaluated on JustLogic; shows moderate performance that is sensitive to reasoning depth and prompting technique.",
            "citation_title": "JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-4o-mini-2024-07-18",
            "model_description": "Described as a mini/smaller variant of GPT-4o used in experiments; evaluated with zero-shot, few-shot, and CoT prompts.",
            "model_size": null,
            "reasoning_task_name": "JustLogic",
            "reasoning_task_description": "JustLogic deductive reasoning benchmark (3-way classification) testing propositional reasoning with varying argument depths.",
            "method_or_approach": "Evaluated under zero-shot, few-shot, and CoT prompting; some additional experiments (SC, ToT) in appendices.",
            "performance": "Reported table snippets indicate mid-50s under some settings; prose highlights that some smaller reasoning models (e.g., o1-mini) matched DeepSeek R1 at low depths but drop at higher depths — GPT-4o-mini shows similar sensitivity (detailed per-prompt numbers in Table 9).",
            "baseline_comparison": "Performs worse than larger GPT-4o and specialized DeepSeek R1; prompting (CoT) improves results but depth sensitivity remains.",
            "limitations_or_failures": "Sharp decline in performance as reasoning depth increases; general failure modes (misapplication of argument forms, logical inconsistency, false interpretation) observed.",
            "insights_or_conclusions": "Smaller variants can perform reasonably on shallow deductive tasks when given CoT prompts but struggle to sustain correct chains at greater depths; model scale and prompting both matter.",
            "uuid": "e8591.3",
            "source_info": {
                "paper_title": "JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Llama3-70B",
            "name_full": "Llama3-70B-Instruct",
            "brief_description": "A large (70B) Llama 3 instruction-tuned model evaluated on JustLogic; non-reasoning-focused but benefits from chain-of-thought prompting.",
            "citation_title": "JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models",
            "mention_or_use": "use",
            "model_name": "Llama3-70B-Instruct",
            "model_description": "Instruction-tuned Llama 3 model of 70B parameters described as a non-reasoning model in this paper; hyperparameters and prompting details are aligned with recommendations in the Llama3 literature (paper cites Dubey et al., 2024).",
            "model_size": "70B",
            "reasoning_task_name": "JustLogic",
            "reasoning_task_description": "JustLogic deductive reasoning benchmark; tests propositional logic argument forms across depths 1–7 (in main set).",
            "method_or_approach": "Evaluated with zero-shot, few-shot, and CoT prompting; hyperparameters (temperature, top-p, penalties) chosen per paper's implementation details.",
            "performance": "Zero-shot: 53.1% (reported); CoT: ~58.6% (Table 9 shows CoT results for Llama3-70B around high 50s).",
            "baseline_comparison": "Performs better than Llama3-8B; CoT prompting gives larger gains than naive scale increase (Llama3-8B with CoT can beat zero-shot 70B in some settings).",
            "limitations_or_failures": "Degrades with increasing reasoning depth; common failure modes as documented (wrong argument-form application, incorrect translations of natural language into logical form, logical inconsistency).",
            "insights_or_conclusions": "Model scale produces gains but prompting (CoT) can yield larger improvements; non-reasoning models benefit substantially from CoT prompting even without specialized reasoning training.",
            "uuid": "e8591.4",
            "source_info": {
                "paper_title": "JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Llama3-8B",
            "name_full": "Llama3-8B-Instruct",
            "brief_description": "A smaller (8B) instruction-tuned Llama 3 model evaluated on JustLogic; non-reasoning-focused and shows sensitivity to prompting approach.",
            "citation_title": "JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models",
            "mention_or_use": "use",
            "model_name": "Llama3-8B-Instruct",
            "model_description": "Instruction-tuned 8B-parameter Llama 3 model described as a non-reasoning model in experiments; recommended hyperparameters and prompting choices follow Llama3 guidance in the paper.",
            "model_size": "8B",
            "reasoning_task_name": "JustLogic",
            "reasoning_task_description": "JustLogic deductive reasoning benchmark (three-way truth-value classification from premises).",
            "method_or_approach": "Evaluated with zero-shot, few-shot, and CoT prompting; also explored additional prompting strategies (SC, ToT) in appendices with mixed outcomes.",
            "performance": "Zero-shot: 49.8% accuracy; Chain-of-thought (CoT): 57.8% (reported in text — CoT outperforms a larger zero-shot model).",
            "baseline_comparison": "Smaller model underperforms larger Llama3-70B but CoT prompting closes much of the gap; CoT Llama3-8B (57.8%) higher than zero-shot Llama3-70B (53.1%).",
            "limitations_or_failures": "Pronounced decline with increasing reasoning depth; subject to common failure modes (logical inconsistency, wrong argument-form application, mis-translation of facts). Tree-of-thought often hallucinated and provided little benefit for many models.",
            "insights_or_conclusions": "Prompt engineering (CoT) can meaningfully improve deductive reasoning in small non-reasoning models, often more than a modest increase in model size; however, small models still struggle at larger reasoning depths.",
            "uuid": "e8591.5",
            "source_info": {
                "paper_title": "JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "OpenAI o1-mini",
            "name_full": "OpenAI o1-mini (2024-07-18 variant)",
            "brief_description": "A smaller reasoning-focused OpenAI model variant evaluated on JustLogic; shows adequate performance at low reasoning depths but rapid degradation at higher depths.",
            "citation_title": "JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models",
            "mention_or_use": "use",
            "model_name": "OpenAI o1-mini-2024-07-18",
            "model_description": "Smaller variant of OpenAI's o1 family described as reasoning-focused; evaluated across prompting methods and depths in the study.",
            "model_size": null,
            "reasoning_task_name": "JustLogic",
            "reasoning_task_description": "JustLogic benchmark testing strictly deductive propositional reasoning (true/false/uncertain classification given premises).",
            "method_or_approach": "Tested with zero-shot, few-shot, and CoT prompting; qualitative analysis compares o1-mini to DeepSeek R1 across depths.",
            "performance": "Reported to perform comparably to DeepSeek R1 at low depths but experiences a sharp decline at higher depths (no single aggregate accuracy value given in prose for o1-mini; table snippets show mid-50s to low-60s depending on prompt).",
            "baseline_comparison": "Matches top models on low-depth problems but loses advantage quickly as depth increases, falling close to or below larger non-reasoning models on very deep problems.",
            "limitations_or_failures": "Severe drop in high-depth performance; fails to sustain correct multi-step reasoning chains — similar failure modes as other models.",
            "insights_or_conclusions": "Being fine-tuned/trained for reasoning helps at shallow problem depths, but maintaining accuracy on deep deductive chains requires capacity and/or training beyond what this smaller variant exhibits.",
            "uuid": "e8591.6",
            "source_info": {
                "paper_title": "JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ProofWriter",
            "rating": 2,
            "sanitized_title": "proofwriter"
        },
        {
            "paper_title": "CLUTRR",
            "rating": 2
        },
        {
            "paper_title": "LogiQA 2.0",
            "rating": 2,
            "sanitized_title": "logiqa_20"
        },
        {
            "paper_title": "FOLIO",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Tree-of-thought (tree-of-thought framework reference in paper)",
            "rating": 1,
            "sanitized_title": "treeofthought_treeofthought_framework_reference_in_paper"
        }
    ],
    "cost": 0.015459,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models
9 May 2025</p>
<p>Michael K Chen 
Xikun Zhang 
Dacheng Tao 
JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models
9 May 20258F261BDCF6E04089020DCE3DBA227CFCarXiv:2501.14851v2[cs.CL]
Logical reasoning is a critical component of Large Language Models (LLMs), and substantial research efforts in recent years have aimed to enhance their deductive reasoning capabilities.However, existing deductive reasoning benchmarks, which are crucial for evaluating and advancing LLMs, are inadequate due to their lack of task complexity, presence of prior knowledge as a confounder, and superficial error analysis.To address these deficiencies, we introduce JustLogic, a synthetically generated deductive reasoning benchmark designed for rigorous evaluation of LLMs.JustLogic is (i) highly complex, capable of generating a diverse range of linguistic patterns, vocabulary, and argument structures; (ii) prior knowledge independent, eliminating the advantage of models possessing prior knowledge and ensuring that only deductive reasoning is used to answer questions; and (iii) capable of in-depth error analysis on the heterogeneous effects of reasoning depth and argument form on model accuracy.Our experimental results on JustLogic reveal that (i) state-of-the-art (SOTA) reasoning LLMs perform on par or better than the human average but significantly worse than the human ceiling, and (ii) SOTA non-reasoning models still underperform the human average.All code and data are available at https://github.com/michaelchenlab/JustLogic</p>
<p>Introduction</p>
<p>Deductive reasoning is a crucial capability for large language models (LLMs).It refers to the process of creating logically valid arguments, where conclusions necessarily follow from the premises.In other words, if an argument's premises are true, the conclusion must also be true.Recent</p>
<p>• If relics are artifacts, then fertilizers contain phosphorus.[p → r]</p>
<p>• Assuming dogs are capable of barking, we know that fertilizers contain phosphorus.[q → r]</p>
<p>Statement: Fertilizers contain phosphorus.[r] Question: Is the statement true, false, or uncertain?Answer: True Figure 1.Example of a question adapted using JustLogic's dataset construction algorithm.Formal notations are included for illustrative purposes and are not provided to models.</p>
<p>We identify three major problems with the existing benchmarks.First, they lack complexity, as measured on two dimensions: natural language complexity, i.e. how arguments are linguistically expressed, and argument complexity, i.e. the structure of the argument itself.Manually curated datasets, such as FOLIO (Han et al., 2022) and LogiQA 2.0 (Liu et al., 2020;2023a) exhibit high natural language complexity but low argument complexity, while synthetic datasets like CLUTRR (Sinha et al., 2019) and ProofWriter (Tafjord et al., 2020) exhibit the opposite.Simplicity in either dimension makes these benchmarks prone to overfitting and memorization, thus allowing models to perform well despite underlying weaknesses in logical reasoning.A more detailed analysis can be found in Section 3.4.Second, existing benchmarks often fail to test deductive reasoning in isolation, as models can benefit from prior knowledge.</p>
<p>To empirically validate this claim, we developed a novel test for prior knowledge independence, which measures the influence of prior knowledge on reasoning benchmarks.As detailed in Section 5.1, prior knowledge can substantially increase accuracy, even in datasets not intended to require commonsense or domain knowledge, e.g.FOLIO and LogiQA 2.0.Thus, high accuracy may not reflect strong reasoning capabilities.Third, many existing benchmarks provide superficial error analysis, leaving key questions unanswered: At what reasoning depth does the model start to fail?How does the model compare to humans at different argument depths?Which argument forms is the model particularly weak at?These insights are essential for understanding the depth and robustness of a model's deductive reasoning, yet many benchmarks provide them.Section 5.3 demonstrates the importance and usefulness of comprehensive error analysis.</p>
<p>Due to these issues, LLMs' deductive reasoning abilities remain ambiguous.In response to the critical need for a reliable benchmark to support ongoing research efforts, we present JustLogic, a novel natural language deductive reasoning benchmark.Each instance in JustLogic contains a paragraph of premises and a statement.The task is to determine whether the statement is true, false, or uncertain, based solely on the premises, and assuming they are all true.</p>
<p>An example is shown in Figure 1.</p>
<p>JustLogic's construction ensures it is (i) complex, (ii) prior knowledge independent, and (iii) capable of in-depth error analysis.First, to achieve high argument and natural language complexity, JustLogic is code-generated rather than manually curated.This allows the generation of a theoretically infinite number of unique argument structures.Natural language sentences are then drawn from GenericsKB-Best (Bhakthavatsalam et al., 2020), a database of 1M+ unique real-world sentences, and inserted into the argument structures, introducing high natural language complexity.Second, since sentences are randomly sampled from the entire GenericsKB-Best dataset, the generated arguments generally do not align with real-world knowledge, thereby eliminating prior knowledge as a confounder.Finally, in-depth error analysis is enabled by the programmatic generation process, which enables the inspection of each question's properties, such as reasoning depth and argument form, to investigate their impact on model performance.A comparison between JustLogic and four similar logical reasoning benchmarks (CLUTRR, ProofWriter, LogiQA 2.0, and FO-LIO) is presented in Table 1, with further details on dataset construction provided in Section 3.</p>
<p>Using JustLogic, we conducted comprehensive experiments to evaluate the deductive reasoning capabilities of current LLMs.First, our novel prior knowledge independence test demonstrated that prior knowledge enables LLMs to bypass deductive reasoning on existing datasets, resulting in artificially high accuracies.This is not observed in JustLogic.Second, we benchmarked the performance of SOTA LLMs and human participants using JustLogic.Most SOTA LLMs performed significantly lower than the average human accuracy (73.0%).Only DeepSeek R1 performed substantially better (80.9%), but still fell short of the human ceiling (100.0%).Finally, enabled by JustLogic's code-generated nature, our thorough error analysis examined the heterogeneous impact of argument structure and reasoning depth on model performance.These experiments show that the Just-Logic benchmark is a reliable test of deductive reasoning and reveals significant room for improvement in LLMs.</p>
<p>In summary, our contributions are threefold.First, we evaluate the limitations of existing benchmarks.Second, we introduce the JustLogic benchmark, a synthetic dataset to evaluate deductive reasoning, that addresses the aforementioned limitations.Third, our experiments using JustLogic demonstrate that most SOTA models perform significantly worse than humans.We posit that the deductive reasoning capabilities of LLMs still have significant room for improvement, and hope that the JustLogic benchmark will assist researchers in designing and evaluating LLMs.</p>
<p>Related Work</p>
<p>Existing reasoning datasets for Large Language Models</p>
<p>Reasoning benchmarks are a vital part of LLM evaluation.Some benchmarks measure deductive reasoning in conjunction with natural language inference (NLI), inductive reasoning, and commonsense knowledge: HellaSwag (Zellers et al., 2019) tasks machines to select the most likely followup of an event description, WinoGrande (Sakaguchi et al., 2021) is a pronoun resolution task, and MuSR (Sprague et al., 2023) tasks machines to solve fictional problems, such as murder mysteries.Other benchmarks measure reasoning on domain knowledge: AI2 Reasoning Challenge (ARC) (Yadav et al., 2019) contains grade-school science questions, while Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2020) contains questions across 57 subjects in STEM, humanities, and more.Finally, math-specific benchmarks include GSM-8K (Cobbe et al., 2021) and DROP (Dua et al., 2019).</p>
<p>The aforementioned benchmarks explicitly evaluate skills beyond reasoning and do not specifically define the type of reasoning involved, e.g.inductive, deductive, and analogical.As such, benchmarks that solely test for deductive reasoning have seen a considerable increase in interest.They can be classified into two broad categories: synthetic and manually curated.Synthetically-generated datasets include (i) CLUTRR (Sinha et al., 2019), where a machine must</p>
<p>High NL Complexity High Arg. Complexity</p>
<p>Prior Knowledge Independence
In-Depth Error Analysis CLUTRR ✗ ✓ ✓ ✓ ProofWriter ✗ ✓ ✓ ∼ LogiQA 2.0 ✓ ✗ ✗ ∼ FOLIO ✓ ✗ ✗ ∼ JustLogic ✓ ✓ ✓ ✓
infer the relationship of two family members based on stories, (ii) ProofWriter (Tafjord et al., 2020), where a machine must deduce a statement's truth value based on a set of facts and rules, and (iii) ProntoQA-OOD (Saparov et al., 2024), where a machine must prove a statement based on a set of facts.Manually curated datasets include (i) LogiQA 2.0 (Liu et al., 2023a), containing manually-translated logical reasoning questions from the Chinese Civil Service Exam, (ii) FOLIO (Han et al., 2022), containing questions with manually-annotated content using Wikipedia pages, and (iii) ReClor (Yu et al., 2020), containing reading comprehension questions from GMAT and LSAT.</p>
<p>As discussed earlier, synthetic datasets are prior knowledge independent and exhibit high argument and low natural language complexity; manually curated datasets are the opposite.JustLogic, being synthetic, contains all its advantages while offering the natural language complexity of manually curated datasets, which we further explained in Section 3.4 and 5.1.</p>
<p>Reasoning in Large Language Models</p>
<p>As LLMs continue to increase in size, their performance on various reasoning-related benchmarks has improved dramatically.For example, in 2024, Gemini Ultra (Team et al., 2023) achieved 90.0% on MMLU when the SOTA model in 2020, UnifiedQA 11B (Khashabi et al., 2020), achieved a mere 48.9%.In 2023, GPT-4 achieved 96.4% on ARC when the SOTA model in 2020, GPT-3 (Brown, 2020), achieved 53.2%.</p>
<p>The advent of prompting techniques played an important role in developing LLMs' reasoning abilities.In-context learning (Dong et al., 2022) provides LLMs with instructions and examples in the input prompt to guide its response.Chain-of-thought prompting (Wei et al., 2022) prompts LLMs to generate a series of intermediate reasoning steps before arriving at the final answer.Self-consistency decoding (Wang et al., 2022) chooses the most consistent answer after sampling multiple chain-of-thought outputs.Least-tomost prompting (Zhou et al., 2022) decomposes a complex problem into simpler subproblems, which are then solved sequentially.</p>
<p>As mentioned above, LLMs are conventionally tested on datasets that combine reasoning with other skills.Moreover, existing logical reasoning-specific datasets possess major limitations that call into question the reliability of their evaluations.In response, JustLogic aims to robustly and accurately evaluate the deductive reasoning abilities of LLMs.</p>
<p>Dataset Construction</p>
<p>JustLogic is a programmatically generated dataset designed to evaluate a model's ability of deductive reasoning, specifically its capability to form logically valid arguments.A logically valid argument is one where the conclusion necessarily follows from the premise(s); in other words, given the premises are true, the conclusion must also be true.</p>
<p>In order to test this, JustLogic presents a model with a paragraph consisting of premises, followed by a query statement.</p>
<p>Based solely on the premises and assuming they are all true, the model needs to determine whether the query statement is true, false, or uncertain.In line with the open-world assumption, the "Uncertain" answer refers to cases where the premises neither support nor contradict the query statement.</p>
<p>The following outlines the process for generating each instance in the dataset:</p>
<p>1.</p>
<p>Step 1: Generate an argument structure.2.</p>
<p>Step 2: Add natural language statements to the argument structure.3. Step 3: Generate a query statement.Figure 2 provides an example of this process, which we reference throughout the rest of this section.</p>
<p>Step 1: Generate argument structure</p>
<p>Argument structures are composed of one or more valid argument forms, derived from propositional logic; argument forms are made up of a series of logical forms, which we define as symbolic representations of statements.Specifically, the seven distinct argument forms in our dataset are constructed with the following four logical forms: (i) basic (x), (ii) negation (¬x), (iii) conditional (x → y), and (iv) disjunction (x ∨ y).While there is a theoretically infinite number of possible argument forms, complex argument forms can be derived by combining simpler ones.Therefore, we explicitly define the seven most fundamental forms (Johnson, 2006): modus ponens, modus tollens, hypothetical syllogism, disjunctive syllogism, reductio ad absurdum, constructive dilemma, and disjunctive elimination.Table 6 in Appendix A provides the corresponding formal notations and natural language examples.</p>
<p>The algorithm to create an argument structure (formally shown in Appendix B) accepts an intended argument depth as input.It first generates a random conclusion and an argument form to support it, which in Figure 2 is c and disjunctive syllogism.If the intended depth has not been reached, one or more premises will become subconclusions, which are supported by new, randomly generated argument forms.In Figure 2, this is exemplified by ¬b becoming a subconclusion, supported by a modus ponens argument.This process continues until the desired depth is achieved.</p>
<p>Step 2: Adding natural language</p>
<p>Next, the symbolic statements are converted into natural language.Each statement consists of one or more logical forms, i.e. variable, negation, conditional, and disjunction.In natural language, these forms can be expressed in a variety of ways.For example, a conditional can be expressed as both "If x, then y." and "Given that x, y is true.",where variables x and y are simple propositions.To emulate the diversity of natural language, we manually create a list of expres-sions for each logical form with the help of GPT-4 (Achiam et al., 2023) and human feedback.Table 2 shows the formal notation of each form, alongside a sample expression and the total number of unique expressions.The variable(s) within each expression are ultimately replaced by randomly selected generic, real-world sentences from GenericsKB-Best (Bhakthavatsalam et al., 2020).The GenericsKB-Best dataset is chosen for its vast collection of simple propositions (1,020,868 sentences) without conditionals, disjunctions, etc.A complete example can be found in Step 2 of Figure 2.</p>
<p>Notably, as shown in Figure 2, the statements are generally factually inaccurate despite being drawn from real-world data.This is intentional.Real-world propositions allow us to generate sentences with diverse grammatical structures that closely emulate human-written arguments.However, factually accurate arguments enable models to bypass deductive reasoning with their prior real-world knowledge, which is experimentally demonstrated in Section 5.1.By using real-world yet factually inaccurate statements, we combine realism and prior knowledge independence.</p>
<p>There are potential concerns that factually inaccurate statements and "unnatural" synthetic language may confuse models and lead to artificially low performance.Appendix F.2 and F.3 empirically refute these concerns.</p>
<p>Step 3: Generate query statement</p>
<p>The LLM's task is to determine whether the given query statement is true, false, or uncertain based on the premises provided.Using Figure 2 as an example, if we assign the query statement to be the negation of the conclusion, i.e. "It It is a fact that either x or y. 8</p>
<p>is not true that Japan is in Asia", then the answer is false.</p>
<p>If the query statement is the same as the conclusion, then the answer is true.If the query statement is unrelated to the premises, then the answer is uncertain.</p>
<p>Dataset Complexity</p>
<p>In the context of deductive reasoning datasets, complexity is defined as the variety and comprehensiveness of instances.It can be further divided into two dimensions: natural language complexity and argument complexity.In this section, we highlight the significance of both aspects and how JustLogic compares against other logical reasoning datasets.</p>
<p>Natural language complexity.Human language is complex.Statements and arguments of similar meanings can be presented in a variety of ways.Therefore, it is insufficient for models to reason solely with symbols, e.g.x and y, and basic natural language sentences, e.g."Some birds are yellow.";they must be capable of reasoning with real-world vocabulary and diverse sentence structures to be useful in practical contexts.</p>
<p>We measure natural language complexity with (i) reading difficulty, as measured by the Flesch-Kincaid Grade Level test (Kincaid, 1975), and (ii) lexical diversity, as measured by vocabulary &amp; domain size.For (i), the score is presented as a U.S. grade level; the higher the score, the harder the text is to read.Scores greater than 12 should be used to compare the relative difficulty between benchmarks, with higher scores indicating relatively greater textual complexity.A domain is defined as any topic of interest, such as golf, computers, or traveling; Vocabulary size refers to the number of unique words in the dataset.Given the difficulty of quantitatively capturing linguistic complexity, Appendix C also shows text samples of each benchmark, representative of their complexity.</p>
<p>As shown in Table 3, existing synthetic datasets have low natural language complexity, while human-written datasets, such as FOLIO and LogiQA 2.0, exhibit significantly higher complexity.This is expected since synthetic datasets translate symbols in formal logic into natural language using limited templates of sentence structures and vocabulary lists.For example, in ProofWriter, a typical sentence follows the format "All dogs are (not) red.".The linguistic patterns of human-written datasets, in contrast, are bound only by hu-man creativity.Despite being synthetic, JustLogic, achieves natural language complexity on par with manually curated datasets, due to its comprehensive selection of expressions and the use of GenericsKB-Best as the source of sentences.</p>
<p>Argument complexity.Argument complexity refers to the diversity of argument structures used in the dataset.A sufficiently high argument complexity is important because humans use a range of argument forms to reason, beyond just conditionals and modus ponens.Moreover, a real-world argument is typically composed of multiple argument forms, due to the inherent complexity of real-life scenarios.</p>
<p>A dataset's argument complexity is evaluated based on two metrics: (i) range of reasoning depth, and (ii) number of unique argument structures.The upper limit of both metrics is calculated based on the theoretical maximum without any additional human input, rather than the highest depth used in experiments in existing works.For example, CLUTRR's dataset construction program can generate any number of depths (referred to as relation length in the original paper), despite its experiments only utilizing questions of up to a depth of 10.Thus, its upper limit of depth is infinite.</p>
<p>Table 3 shows that synthetic datasets, such as CLUTRR, ProofWriter, and JustLogic, excel in this area, as there is no upper limit to their reasoning depth and number of argument structures.Manually curated datasets, in contrast, either lack an explicit concept of reasoning depth and argument structures (e.g.LogiQA 2.0), or have a limited selection of both (e.g.FOLIO).While manual datasets require significant human efforts and investment to expand their complexity, synthetic ones can scale trivially.</p>
<p>In summary, JustLogic combines the best aspects of both dataset construction methods, incorporating the argument complexity of synthetic datasets and the natural language complexity of manually curated ones.</p>
<p>Future-proofing JustLogic</p>
<p>As the reasoning abilities of LLMs continue to improve, we expect LLMs to solve the existing JustLogic dataset eventually.To maintain JustLogic's relevance, we leverage its synthetic nature to increase complexity with minimal human input.Argument complexity can be adjusted by increasing the (i) range of argument depth (empirically validated in</p>
<p>Experimental Setup</p>
<p>We first investigate the influence of prior knowledge on evaluating deductive reasoning with JustLogic and other existing benchmarks using our prior knowledge independence test.</p>
<p>Next, several SOTA LLMs of various sizes are evaluated using JustLogic.Finally, an in-depth error analysis of the LLMs' results is conducted.Any LLM capable of using prior knowledge can be used for this test.However, models with larger parameter sizes, and thus more extensive prior knowledge, are more likely to exhibit notable differences in accuracies.For our experiment, we use GPT-4.The test is conducted on both JustLogic and existing benchmarks, including CLUTRR, ProofWriter, LogiQA 2.0, and FOLIO.</p>
<p>Evaluation of LLMs' Deductive Reasoning</p>
<p>Our task follows the conventional formulation: CQO → A. Question Q is "Is the statement S true, false, or uncertain?",followed by the query statement, as shown in Figure 1; there are 3 answer options, where O = {true, false, uncertain}.All prompts begin with a preamble, which includes (i) the requirements of the task at hand, (ii) a list of argument forms in propositional logic, and (iii) the available answer options.</p>
<p>We evaluated both reasoning and non-reasoning models of different sizes: Llama3-8B-Instruct (Dubey et al., 2024), Llama3-70B-Instruct, GPT-4o-mini-2024-07-18, GPT-4o-2024-05-13, DeepSeek R1 Distill Qwen 14B, DeepSeek R1 (?), OpenAI o1-mini-2024-07-18, and OpenAI o1-2024-12-17 (Jaech et al., 2024).A range of prompting techniques are tested: zero-shot, few-shot, and chain-of-thought (CoT) (Wei et al., 2022); more techniques are tested in Appendix H.1.Due to limited model access, 70 random samples are used for OpenAI o1 and 350 for the other reasoning models.To ensure fairness, the selected subset has the same proportion of each reasoning depth as the entire test set.Further implementation details are provided in Appendix E.</p>
<p>We also measured human performance.18 anonymous participants (as detailed in Appendix G) are given a random subset of questions.This is because deductive reasoning questions, especially those at high reasoning depths, are cognitively demanding and time-consuming; it is imprac-tical to expect humans to complete 1050 questions.To ensure fairness, both models and participants are provided similar prompts and are given the same proportion of each reasoning depth.</p>
<p>Finally, we perform an error analysis of the results from the aforementioned experiments, specifically examining the heterogeneous effects of argument form and reasoning depth on model accuracy.Accuracy for each argument form is only measured using questions with a reasoning depth of 1 since those with a depth of &gt;1 typically have &gt;1 argument forms.Lastly, a qualitative analysis of failure modes is conducted.</p>
<p>Results</p>
<p>Prior Knowledge Independence Test</p>
<p>The results of JustLogic and four other benchmarks are shown in Table 4; note that lower accuracy relative to the benchmark's random probability indicates that prior knowledge is more detrimental to answering the question, thereby demonstrating that the benchmark is more prior knowledge independent.Thus, the smaller the |∆| between model accuracy and random probability, the better.The |∆|s of CLUTRR and ProofWriter are relatively low, while those of LogiQA 2.0 and FOLIO are nontrivially higher.This is because the former are synthetic datasets, while the latter are manually curated.When a question is code-generated, it generally bears no correlation with reality, e.g."Is it true, false, or uncertain that Gary is not red."from ProofWriter and "How is Anna related to Katherine in the family?"from CLUTRR.Such questions are only answerable by reasoning over the context C. LogiQA 2.0 and FOLIO, on the other hand, often contain questions that are answerable without the context provided.For example, "The United States won the most medals in the last summer Olympic games."from FOLIO can be accurately answered by LLMs trained on sufficiently recent general knowledge datasets.We posit that this is an unintentional consequence of the human bias to align the question's truth value with reality.While human curation enhances the question's realism, it compromises the test for deductive reasoning.The JustLogic benchmark's |∆| is 0.4%, given an accuracy of (33.7%) and random probability (33.3%), which is much lower than other benchmarks, including synthetic ones.The reason for this is twofold: first, JustLogic is also a synthetic dataset, which eliminates the human bias present in manually curated datasets.Second, while JustLogic uses real-world statements, their truth value is nonetheless randomly determined.For example, the statement "doors are solids" is factually true.However, by deducing from the paragraph, the correct answer is "False".Thus, using prior knowledge for many questions is not only unhelpful but also meaningfully decreases accuracy.</p>
<p>Evaluation of LLMs' Deductive Reasoning</p>
<p>As shown in Table 9, the best-performing model by a large margin is DeepSeek R1 with an accuracy of 80.9%.</p>
<p>The second and third-best models, OpenAI o1 and GPT-4o, achieved 72.9% and 65.6%.Surprisingly, OpenAI o1 (72.9%) performs substantially worse than DeepSeek R1 due to worse performance at higher depths; we provide further evidence in Appendix H.2. Models with larger parameter sizes generally perform better than smaller models, assuming the same prompting methods are used.For example, zero-shot Llama3-8B achieved an accuracy of 49.8%, while zero-shot Llama3-70B achieved an accuracy of 53.1%.However, larger model sizes offer diminishing returns, shown by the accuracy gain of just 1.0% from Llama3-70B to GPT-4o, with both using CoT prompting.</p>
<p>Moreover, the improvements offered by increasing model size pale in comparison to those offered by better prompting methods.Using chain-of-thought prompting, Llama3-8B achieved higher performance (57.8%) than zero-shot Llama3-70B (53.1%).This appears to explain the significant accuracy gap of 7.3% between OpenAI o1 and its non-reasoning-focused counterpart, GPT-4o.OpenAI o1 is trained to reason with chain-of-thought prompts using a 'reinforcement learning algorithm' (OpenAI, 2024).We hypothesize that the use of reinforcement learning on CoT prompting further enhances the deductive reasoning capabilities offered by CoT prompting alone.</p>
<p>Human performance (73.0%) is higher than all models besides DeepSeek R1, while the human ceiling (100.0%)outperforms all models.The non-trivial gap between the human ceiling and the best-performing model (80.9%) shows that models still have significant room for improvement.Moreover, we believe actual human performance might be higher than 73.0%.Given the long paragraphs of questions with high reasoning depth, participants may have predicted answers by briefly scanning the paragraph, rather than carefully deducing based on all available premises.This is supported by the suspiciously short time taken to complete the questions of some participants.While the relative accuracies of argument forms are heterogeneous across models, some forms perform distinctly better than others.For example, hypothetical syllogism and constructive dilemma achieve considerably higher performance than modus tollens, disjunctive syllogism, and reductio ad absurdum.Interestingly, the former argument forms are more commonly used by humans than the latter ones, which potentially hints at the cause of this observation.</p>
<p>As for reasoning depth, model accuracies generally decrease as depth increases, consistent with expectations that accuracy declines as the complexity of questions increases.Interestingly, OpenAI o1-mini performs comparably to DeepSeek R1 at low depths, but o1-mini sees a sharp decline in performance once depth increases, while DeepSeek R1 only sees a moderate decline; DeepSeek R1's superior performance is a result of better reasoning at higher reasoning depths.In fact, at medium and high depths, o1-mini no longer performs better than non-reasoning models, i.e.GPT-4o and Llama3-8B.These observations suggest that DeepSeek R1 supports deeper and longer lines of reasoning, which is crucial for deductive reasoning, and that large reasoning models perform drastically better than smaller ones on reasoning problems.</p>
<p>Conclusion</p>
<p>Deductive reasoning is one of the key challenges in LLM research.In response to the lack of reliable benchmarks, we present JustLogic, a natural language deductive reasoning dataset that is (i) highly complex, (ii) prior knowledge independent, and (iii) capable of in-depth error analysis.These qualities are enabled by JustLogic's dataset construction method: argument structures are synthetically generated, and natural language is programmatically incorporated via expression templates and a knowledge base.We empirically justify JustLogic's merits: most LLMs underperform the human average and all significantly underperform the human ceiling.We demonstrate that JustLogic is a highly challenging, future-proof benchmark that is reliable and insightful for evaluating logical reasoning in LLMs.</p>
<p>Benchmark Sample Text</p>
<p>CLUTRR (Sinha et al., 2019) Lorraine and her brother Kevin went to see a movie.Clarence took his granddaughter Lorraine to the movies and they enjoyed themselves.ProofWriter (Tafjord et al., 2020) The bald eagle is not rough.The bear does not need the bald eagle.The dog needs the bear.If someone is rough then they chase the bald eagle.If someone needs the bear then they are not blue... ProntoQA-OOD (Saparov et al., 2024) Lempuses are bitter.Every lempus is a lorpus.Brimpuses are vumpuses.Tumpuses are impuses.Each impus is not hot.Every numpus is a sterpus.Each shumpus is brown.Sterpuses are fast.Every vumpus is not small... SimpleLogic (Zhang et al., 2022) If messy and hypocritical and lonely, then shiny.If tame, then friendly.If plain and shiny and homely, then nervous.If tender, then hypocritical.If dull and impatient and plain, then tame.If spotless, then perfect.If elegant and tender, then homely... LogiQA 2.0 (Liu et al., 2023a) In the past 10 years, the sales of personal notebook computers of a computer company have continued to grow, but the growth rate is lower than the growth rate of the company's total sales of all products.FOLIO (Han et al., 2022) All people who regularly drink coffee are dependent on caffeine.People regularly drink coffee, or they don't want to be addicted to caffeine, or both.No one who doesn't want to be addicted to caffeine is unaware that caffeine is a drug...</p>
<p>JustLogic</p>
<p>Either one or both of these statements are true: big head is another sudden death disease which occurs primarily in feedlot cattle, or some energy is transferred by bulbs.The notion that 'big head is another sudden death disease which occurs primarily in feedlot cattle' is untrue.</p>
<p>C. Sample texts from deductive reasoning benchmarks</p>
<p>Beyond metrics like vocabulary size and number of domains, the degree of natural language complexity can be straightforwardly determined by manually inspecting the linguistic patterns of a given benchmark.Table 7 shows sample texts from CLUTRR, ProofWriter, ProntoQA-OOD, SimpleLogic, LogiQA 2.0, FOLIO, and JustLogic.</p>
<p>Evidently, JustLogic exhibits significantly greater natural language complexity than CLUTRR, ProofWriter, ProntoQA-OOD, and SimpleLogic, because the latter benchmarks programmatically generate every sentence, while JustLogic extracts its sentences from GenericsKB, a natural language text database.Thus, the former benchmarks rely on a limited number of grammar templates, reducing their linguistic complexity.JustLogic exhibits similar levels of complexity to FOLIO.LogiQA 2.0 is more complex because it is human-curated and not backed by a formal logic system (unlike how JustLogic is backed by propositional logic).Without a formal logic system, LogiQA 2.0's argument complexity suffers, as shown in Table 3, which compromises its ability to evaluate deductive reasoning in LLMs.</p>
<p>D. Prior Knowledge Independence Test</p>
<p>A sample prompt for the prior knowledge independence test, based on the example in Figure 1, is shown below in Figure 4.</p>
<p>Note that the answer options vary depending on the benchmark.For example, the options for LogiQA are A, B, C, and D, while those of CLUTRR are 16 possible family relations.</p>
<p>E. Experiment Implementation Details</p>
<p>The hyperparameters for the Llama3 models are decided largely based on the recommendations in the original paper Dubey et al. (2024), which are as follows: temperature of 0.6, top p of 0.9, presence penalty of 1.15, length penalty of 1.</p>
<p>With regards to prompting methods, 3-shot prompting is chosen for few-shot experiments because it produces the highest accuracies compared to 6 and 9-shot.Chain-of-thought prompts also contain three examples.In the interest of fairness, all prompting techniques contain similar general instructions, which are as follows:</p>
<p>Instructions:</p>
<p>• Use the knowledge you currently have to answer as accurately as possible.</p>
<p>• You have 3 answer options: True, False, and Uncertain.</p>
<p>• There should be roughly an equal proportion of each option.</p>
<p>•  You are given a paragraph of facts/premises, followed by a statement.Perform logical reasoning with propositional logic on the paragraph to determine the truth value of the statement.</p>
<p>Here is the list of argument forms:</p>
<p>• Modus Ponens Assume that all premises in the paragraph are true.</p>
<p>Question: Is the statement true, false, or uncertain?</p>
<p>As for the additional prompting techniques are explored in Appendix H.1, the tree-of-thought framework contains two prompts at each step: candidate generation and candidate evaluation.In addition to the general instructions above, the candidates generation prompt is shown below.</p>
<p>Let's reason step by step.Generate 3 alternative possible next steps, based on the question and the answer so far.Each step consists of a single argument form, e.g.modus ponens.The question takes 1 or more steps to solve.</p>
<p>Note that these 3 steps are NOT sequential.They must be alternatives to the same step.</p>
<p>As for candidate evaluation, the prompt is shown below.Note that the model may terminate the exploration prematurely by indicating a final answer.A practical consideration is that models tend to conclude too early; the prompt should be designed factually inaccurate statements, DeepSeek R1 and Llama3-8B maintained similar accuracies, while OpenAI o1-mini saw an improvement.</p>
<p>There are two reasons for these results.First, our prompt explicitly instructs models to answer the question only using the paragraph provided and without using prior knowledge.The full prompt is shown in Appendix E.Moreover, in few-shot prompts, the examples provided include conclusions where their factual accuracy does not match the correct answer.These measures encourage models to ignore prior knowledge and answer questions without considering the factual accuracy of conclusions in the real world.</p>
<p>Second, how LLMs treat factual accuracy when reasoning deductively depends on the LLM's training: specifically, the model's ability to follow prompt instructions to ignore prior knowledge.For example, DeepSeek R1 biases toward factually inaccurate conclusions when deductively reasoning, while OpenAI o1-mini exhibits little difference in performance.Should an LLM exhibit significant differences in performance between factually accurate and inaccurate conclusions, it suggests the LLM has room for improvement in instruction following.</p>
<p>Importantly, the ability to deduce whether premises lead to a conclusion without using prior knowledge is a fundamental human skill: we use it to evaluate whether a debater's speech or journalist's article supports their position.The inclusion of both factually accurate and inaccurate instances in JustLogic is a feature, not a bug.</p>
<p>F.3. Impact of Language "Unnaturalness" on Model Performance</p>
<p>Given that JustLogic is synthetically generated, there is a concern that its natural language may be highly unnatural to models, potentially hindering their ability to reason deductively.To study this concern, we compare the model perplexity of JustLogic, two other human-curated benchmarks (FOLIO and LogiQA), and two other synthetic benchmarks (CLUTRR and ProofWriter).Llama3-8B-Instruct (a non-reasoning model) and DeepSeek R1 Distill Qwen 14B (a reasoning model) are used.If JustLogic's language is indeed highly unnatural, we expect its model perplexity to be significantly higher than other benchmarks.</p>
<p>The results, as shown in Figure 6, reject the aforementioned hypothesis.JustLogic's model perplexities are comparable to FOLIO and lower than the rest.This shows that despite JustLogic's higher linguistic complexity (Table 3), its syntactic patterns are well understood by models.CLUTRR's and ProofWriter's higher perplexities are likely due to their unnatural symbolic-like language; LogiQA's higher perplexities are likely because its questions are originally in Chinese and did not shed their foreign syntactic patterns when translated into English.Examples can be found in Appendix C.</p>
<p>Therefore, while JustLogic's natural language may seem unnatural to human readers, their syntactic patterns are highly intuitive to LLMs compared to other reasoning benchmarks.JustLogic's language likely does not hinder LLMs' understanding of the questions.</p>
<p>G. Details on Human Participants</p>
<p>Participants are recruited from Amazon Mechanical Turk (Amazon, 2005) and are paid $24 per hour.To ensure that participants understand the requirements of the task, a simple verification question is added.If they answer incorrectly, their subsequent responses are voided.As reflected in Figure 7, to create a sample representative of the human average, the participants possess a diverse range of educational qualifications and familiarity with propositional logic.</p>
<p>H. Additional Model Evaluations</p>
<p>H.1. Additional Evaluations on Various Prompting Techniques</p>
<p>While reasoning models do not require specific prompting techniques due to their reasoning-specific training, non-reasoning models observe significant deltas in accuracy based on the choice of prompts.Thus, we evaluate the best small and large non-reasoning models, Llama3-8B-Instruct and GPT-4o, on additional prompting techniques: (i) the self-consistency decoding (SC) (Wang et al., 2022), where the answer is derived through majority voting over 5 sampled paths, and (ii) the tree-of-thought (ToT) framework (?), where each step generates 3 candidates and ultimately chooses 1; the maximum steps allowed is depth + 2, but the model may terminate the search earlier.Finally, we also test (iii) a CoT prompt that does not mention propositional logic.Explicit mentions of technical terms in propositional logic, e.g.reductio ad absurdum, may hinder the reasoning ability of models that are less familiar with them.This prompts tests the aforementioned hypothesis.</p>
<p>The relative performance of the prompting techniques is heterogeneous across models.However, besides GPT-4o, we find that prompting techniques that are more expensive than vanilla CoT offer little to no performance advantage.Self-consistency CoT achieves similar performance to CoT; the former may require significantly higher sampled paths to reap its benefits.Tree-of-thought is too complex for most models to utilize, often hallucinating across prompts and failing to break down the problem into coherent steps.Lastly, we find that the explicit mention of propositional logic in the prompt is generally helpful towards model performance.OpenAI o1 (72.9%) performs substantially worse than DeepSeek R1 on JustLogic, despite other benchmarks suggesting their performance should be comparable.To rule out any human errors during testing and to seek an explanation for these results, we performed a qualitative analysis of OpenAI o1's responses (all of which can be found in our GitHub repository).First, we find that o1's response to questions of depth &gt;= 5 are significantly shorter than that of depth = 3 or 4, which is counterintuitive.Second, o1 prematurely answers "Uncertain" for 90% of questions of depth = 7 without faithfully engaging with the question.Figure 8, showing OpenAI o1's and DeepSeek R1's accuracy over various difficulty levels based on argument depth, reinforces our analysis.Both models have identical accuracies for low and medium difficulty problems, but o1 struggles at high difficulty problems, performing close to random probability.</p>
<p>These observations suggest that OpenAI o1's test-time compute may have been artificially limited, reducing its ability to solve deep, challenging questions.Importantly, this case study reflects JustLogic's ability to flexibly probe models at various levels of difficulty.</p>
<p>H.3. Futureproofing JustLogic</p>
<p>As LLMs improve, we expect their performance on JustLogic to rise, which necessitates increasing JustLogic's difficulty.One way is to increase the argument depth: specifically, we extended JustLogic to incorporate questions of very high depth (8 to 11), and evaluated them on the current SOTA reasoning and non-reasoning models, i.e.DeepSeek R1 and GPT-4o, using CoT prompt.</p>
<p>Additionally, two other benchmarks, LogiQA 2.0 and FOLIO, are also evaluated to compare their difficulty.The results, as shown in Table 10, suggest that (i) as JustLogic's question difficulty increases, model accuracy decreases, and (ii) hard JustLogic questions yield significantly lower accuracies than LogiQA 2.0 and FOLIO.This indicates that JustLogic is already more challenging than other benchmarks and is likely to remain so due to its reduced risk of performance saturation.</p>
<p>I. Qualitative Analysis of Failure Modes</p>
<p>To identify exactly how JustLogic is challenging for existing LLMs, we conducted a qualitative analysis to identify the 4 major failure modes of various models' responses to JustLogic questions.We primarily rely on analyzing the chain-ofthought responses to investigate how they produced the wrong answers.High-level explanations and abbreviated examples are provided for the failure modes.</p>
<p>(1) Logical inconsistency.Models sometimes produce arguments that rely on premises, implicit or otherwise, that contradicts earlier parts of their chain-of-thought.This causes models to generate incoherent arguments, and ultimately arrive at the wrong conclusion.For example, the following is a truncated response from OpenAI o1:</p>
<p>While R implies that researchers are observing data, without specific information, we cannot definitively state R → S. ... Since M ∨ R is true and M ∨ S is the statement we are evaluating, and we have established M is true if N is true, we can conclude:</p>
<p>• M ∨ S is true.</p>
<p>This line of reasoning suggests that M ∨ R implies M ∨ S. For this to be the case, R → S must be true.However, earlier in the reasoning chain, o1 says "we cannot definitively state R → S".This argument is therefore logically inconsistent.</p>
<p>(2) Wrong application of argument forms.Models sometimes identify the correct argument form to solve the question.However, mistakes are made when applying the form to the specific question context.In the truncated response below by Llama3-70B, disjunctive syllogism is, in fact, the appropriate argument form, but the opposite conclusion should have been made: the statement, pain can be severe and make a person unable to perform normal activities, does not hold.</p>
<p>Premise 1: Either condensation is water vapor changing to liquid water or the statement, pain can be severe and make a person unable to perform normal activities, does not hold.Premise 2: The claim that condensation is water vapor changing to liquid water does not correspond with reality.</p>
<p>Using disjunctive syllogism with premises 1 and 2, we conclude that the statement, pain can be severe and make a person unable to perform normal activities, holds.</p>
<p>(3) Using the wrong argument form.Sometimes, models attempt to use argument forms that logically cannot be applied to the context.Such mistakes often derail the response, making it entirely incoherent.In the following example of an output by Llama3-70B, modus tollens simply cannot be applied here; reductio ad absurdum should have been used instead.</p>
<p>Premise 1: One may reasonably assume that if the statement that 'football is a memory-making machine' is incorrect, then fairways are lawns.Premise 2: Assuming the statement that 'football is a memory-making machine' is incorrect, we know that it is not the case that fairways are lawns.</p>
<p>Using modus tollens with premise 1 and 2...</p>
<p>(4) False interpretation of facts.Models sometimes misinterpret the natural language facts entirely.This is most clearly seen when models translate sentences into the wrong logical form.While some of these translations seem trivially simple, especially for LLMs, such mistakes are as common as the other failure modes.In the example below, GPT-4o's interpretation of Premise 2 is incorrect: Premise 2 does in fact imply that the statement "most roses grow fairly rapidly" is false because of the word "mistakenly."</p>
<p>Premise 2: Some people mistakenly believe that most roses grow fairly rapidly.</p>
<p>We cannot directly use Premise 2 to confirm or deny (¬Q), as it only mentions a mistaken belief rather than the truth value.Therefore, we do not have sufficient information to directly negate (Q).</p>
<p>Nonetheless, some sentences are more complex and therefore more prone to false interpretations.In the example below, also by GPT-4o, Premise 2's logical form should be ¬(A → B) → ¬C instead.</p>
<p>Premises: ... 2. "Given that the claim that if police sergeants receive calls, then good nutrition helps reduce low birth weight, miscarriage and anemia does not reflect reality, it can be inferred that some people mistakenly believe that oil is simply a liquid form of fat."</p>
<p>From Premise 2: (¬(A→B))</p>
<p>J. Future Works</p>
<p>While JustLogic already achieves higher or similar natural language complexity to existing deductive reasoning benchmarks, as shown in Section 3.4, linguistic complexity can be further enhanced to emulate human-written prose, e.g.news articles and fiction stories.Notably, LLMs can be introduced in Step 2 of JustLogic's dataset construction process, whereby instead of randomly selecting sentences from GenericsKB, an LLM can generate fictional statements and scenarios, e.g."John's favorite food is hamburgers.".While LLM generation has been successful in datasets involving inductive reasoning and commonsense knowledge, e.g.MuSR (Sprague et al., 2023), it is currently too unreliable for deductive reasoning due to several common mistakes, e.g.ignoring instructions, hallucination, and invalid logic.Nonetheless, as LLMs become more reliable, LLM generation is a promising approach worthy of further exploration.</p>
<p>Error analysis using JustLogic can also be further explored.Interesting research questions include: Are models able to use argument forms appropriately?At which step of the argument chain does the model usually fail?What are the most common reasons for failure?These insights may be useful for fine-tuning models for logical reasoning tasks (Liu et al., 2023b) and model guidance (Beurer-Kellner et al., 2024).</p>
<p>JustLogic can be scaled to incorporate more question types related to logical reasoning, such as multiple-choice questions, identifying missing premises in arguments, identifying logical fallacies in arguments, and natural language sentence to formal logic translation.(Liu et al., 2023b) provides a comprehensive taxonomy.JustLogic's program can be adapted to accommodate each question type while maintaining its key advantages.By measuring deductive reasoning across multiple modalities using a single dataset construction method, JustLogic can provide more comprehensive and controlled evaluations and error analysis.</p>
<p>Figure 2 .
2
Figure 2. A step-by-step example of how an instance with a reasoning depth of 2 is constructed.</p>
<p>Figure 3
3
Figure3illustrates the model accuracy by argument form (left) and by reasoning depth (right).The statistics of the best models of their respective categories are chosen: (i) Llama3-8B (small, non-reasoning model), (ii) Llama3-70B (large, non-reasoning model), (iii) OpenAI o1-mini (small, reasoning model), and (iv) DeepSeek (large, reasoning model).To mitigate noise, especially for models tested on smaller sample sizes, depths are grouped into low (1-3), medium (4-5), and high (6-7) categories.The qualitative analysis of failure modes can be found in Appendix I.</p>
<p>Figure 3 .
3
Figure 3. How argument form 2 and reasoning depth affects accuracy for various models.</p>
<p>Add 5-10 examples here Question: Is the following statement true, false, or uncertain?Statement: Doors are solids.Answer: True.</p>
<p>Figure 4 .
4
Figure 4. Example of a prior knowledge independence test prompt.</p>
<p>with either one of the 3 options:• TRUE: When the premises in the paragraph lead to the statement • FALSE: When the premises in the paragraph directly contradict the statement • UNCERTAIN: When the premises in the paragraph neither support nor contradict the statement Do not use your prior knowledge; your answer must be solely determined by the information within the paragraph.</p>
<p>Figure 5 .
5
Figure 5. How factual accuracy of conclusions affects model accuracy.</p>
<p>Figure 6 .
6
Figure 6.Model perplexities of various logical reasoning benchmarks.</p>
<p>Figure 7 .
7
Figure 7. Participants' highest level of education and familiarity with proposition logic.</p>
<p>Figure 8 .
8
Figure 8.How reasoning depth affects accuracy for OpenAI o1 and DeepSeek R1.</p>
<p>Table 1 .
1
Comparison of JustLogic with other deductive reasoning datasets.The symbol ∼ suggests the feature is present but to a limited extent.</p>
<p>Table 2 .
2
Expressions of logical forms.Once we know that x, we also know that y. 11 Disjunction x ∨ y
Formal Notation Sample ExpressionNo. of Expr.BasicxThe claim that x holds true.16Negation¬xThe claim that x does not reflect reality.15Conditional x → y</p>
<p>Table 3 .
3
Statistics of dataset complexity.
Natural LanguageArgumentReading Difficulty ↑ Vocabulary (Domains) Reasoning Depth Arg. StructuresCLUTRR6.671396 (1)1 -∞∞ProofWriter0.96101 (×)1 -∞∞LogiQA 2.017.1010004 (&gt;10)××FOLIO18.754351 (&gt;10)1 -776JustLogic20.5510557 (&gt;10)1 -∞∞Appendix H.3) and (ii) number of distinct argument formsto &gt;7. Natural language complexity can be adjusted by (i)increasing the number of expressions for each logical formand (ii) integrating more complex knowledge bases thanGenericsKB. Importantly, these changes are programmati-cally achievable with minimal man-hours.
(Xu et al., 2024)Logic can also effectively tackle benchmark leakage(Xu et al., 2024), whereby test sets are unintentionally included in LLMs' pertaining data, thus artificially inflating their performance through memorization.Should JustLogic's test set be leaked, a new test set of similar difficulty can be trivially generated, thereby mitigating this problem.</p>
<p>Given a context C, consisting of n premises (P = {p 1 , p 2 , ..., p n }), a question Q, and m answer options (O = {o 1 , o 2 , ..., o m }), determine the correct answer A. To assess the influence of prior knowledge on determining answer A, the prior knowledge independence test is framed as QO → A. No context C is provided, and the prompt instructs the LLM to answer the question based on prior knowledge alone.An example is provided in Appendix D.If prior knowledge is not useful, the LLM should be unable to answer question Q without C, and the accuracy for the prior knowledge independence test should approximate random probability 1 m .Benchmarks exhibiting such accuracies are deemed prior knowledge independent.
JustLogic contains 7000 instances, equally split amongstreasoning depths ranging from 1 to 7. It is then dividedinto train/validation/test sets (70%/15%/15%). Train andvalidation sets facilitate in-context learning and model fine-tuning if required, while the test set is used for evaluation.Note that the number of instances and range of reasoningdepths can be easily adjusted using JustLogic's open-sourcedataset generation program.4.1. Prior Knowledge Independence TestThe task for deductive reasoning benchmarks is typicallyframed as CQO → A:</p>
<p>Table 4 .
4
Results of Prior Knowledge Independence Test.The lower the |∆|, the better.
|∆| ↓ Accuracy (%) Random (%)CLUTRR2.08.36.3ProofWriter3.737.033.3LogiQA 2.0 27.152.125.0FOLIO6.740.033.3JustLogic0.433.733.3</p>
<p>Table 5 .
5
Model and Human Evaluation Results.
Random Probability33.333.333.3Llama3-8B-Instruct49.841.857.8Llama3-70B-Instruct53.157.864.6GPT-4o-mini53.054.751.8GPT-4o53.858.365.6
0-shot Few-shot CoT</p>
<p>Table 7 .
7
Sample texts from various deductive reasoning benchmarks.</p>
<p>Table 9 .
9
Model and Human Evaluation Results.0-shot Few-shot CoT SC-CoT ToT CoT (w/o prop.logic)
Llama3-8B-Instruct49.841.857.854.638.654.0Llama3-70B-Instruct53.157.864.658.660.658.3GPT-4o-mini53.054.751.850.348.650.0GPT-4o53.858.365.667.171.467.4</p>
<p>Table 10 .
10
SOTA Model Performance on various JustLogic difficulty levels and other benchmarks.
JustLogicLogiQA 2.0 FOLIOEasy Medium Hard Very HardGPT-4o77.363.357.353.064.576.3DeepSeek R1 90.083.368.065.087.686.2
Raffles Institution, Singapore
College of Computing &amp; DataScience, Nanyang Technological University, Singapore. Correspondence to: Michael Chen <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#109;&#105;&#99;&#104;&#97;&#101;&#108;&#99;&#104;&#101;&#110;&#107;&#106;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;">&#109;&#105;&#99;&#104;&#97;&#101;&#108;&#99;&#104;&#101;&#110;&#107;&#106;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;</a>.
MP = Modus Ponens, MT = Modus Tollens, HS = Hypothetical Syllogism, DS = Disjunctive Syllogism, RAA = Reductio Ad Absurdum, CD = Constructive Dilemma, DE = Disjunction Elimination
A. Argument Forms Table6.An overview of the argument forms in the JustLogic dataset.Formal Notation ExampleModus Ponens p → q p ⊢ q If the sky is blue, then the dog is happy.The sky is blue.Therefore, the dog is happy.Modus TollensIf the sky is blue, then the dog is happy.The dog is not happy.Therefore, the sky is not blue.Hypothetical SyllogismIf the sky is blue, then the dog is happy.If the dog is happy, the owner is happy.Therefore, the owner is happy.Either the dog is barking or the dog is asleep.The dog is not barking.Therefore, the dog is asleep.Reductio ad absurdumIf the dog is calm, the owner is around.If the dog is calm, the owner is not around.Therefore, the dog is not calm.Constructive DilemmaEither the sky is blue or it is raining.If the sky is blue, the race can start.If it is raining, the race is delayed.Therefore, either the race can start or it is delayed.Disjunction EliminationEither the sky is blue or it is raining.If the sky is blue, the dog is cheerful.If it is raining, the dog is cheerful.Therefore, the dog is cheerful.Of the possible next steps, choose the one that <strong>most directly advances the reasoning process</strong> toward determining the truth value of the statement.Select the best next step to continue reasoning toward the answer.Do not conclude with TRUE, FALSE, or UNCERTAIN yet -unless:B. Algorithm for Step 1 of JustLogic's Dataset Construction• All relevant reasoning paths have been explored, and• No further logical deduction is possible or necessary.Otherwise, output only the next reasoning step, using one valid argument form.Your goal is to build a full reasoning chain, not jump to conclusions.Only if this step logically completes the reasoning chain and no further analysis is needed, then conclude with one of: TRUE, FALSE, or UNCERTAIN.F. Additional Experimental Validations of the JustLogic BenchmarkF.1. Prior Knowledge Independence Test using Other ModelsTo ensure that the results of the prior knowledge independence test, conducted with GPT-4 in Section 5.1, are replicable, we conduct the same test using Llama3-70B-Instruct.The results are shown in Table8.Similar to Section 5.1, JustLogic has a high degree of prior knowledge independence, on par with other synthetically generated benchmarks, i.e.CLUTRR and ProofWriter, and substantially greater independence than the human-curated ones.Interestingly, ProofWriter's accuracy is significantly lower than random, which is potentially problematic since models may be biased against statements whose truth-value aligns with reality.Given that JustLogic randomly chooses sentences from GenericsKB to add to each instance's argument structure, the final conclusion may be factually accurate or inaccurate in the real world.For example, if the conclusion is "It is not true that Japan is in Asia.", then the conclusion is factually inaccurate.Thus, there is a concern that models underperform due to confusion arising from factually inaccurate conclusions.Moreover, since some conclusions are factually accurate, such instances may exhibit artificially high performance.To study these concerns, we conducted the following empirical study.If the above concerns are true, we expect factually inaccurate conclusions to perform worse than factually accurate ones.Because all GenericsKB sentences are factually accurate, we can straightforwardly deduce each conclusion's factual accuracy.For example, x ∨ y is factually accurate while ¬x is not.Figure5shows the comparison of accuracies for five models: DeepSeek R1, OpenAI o1-mini, GPT-4o, Llama3-70B, and Llama3-8B; the left represents when reasoning depth is 1, while the right represents when depth is 7 or less.These results reject the hypothesis that factually inaccurate conclusions perform worse than factually accurate ones; there is no consistent trend between both conclusion types.In fact, when depth=1, factually inaccurate conclusions exhibit higher performance for some models!At depths of 7 or less, GPT-4o and Llama3-70B saw a decrease in relative accuracy of
. J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.087742023arXiv preprint</p>
<p>Amazon Mechanical Turk. Amazon, 2005</p>
<p>Guiding LLMs the right way: Fast, non-invasive constrained generation. L Beurer-Kellner, M Fischer, M Vechev, R Salakhutdinov, Z Kolter, K Heller, A Weller, N Oliver, J Scarlett, Proceedings of the 41st International Conference on Machine Learning. F Berkenkamp, the 41st International Conference on Machine LearningPMLRJul 2024235</p>
<p>S Bhakthavatsalam, C Anastasiades, P Clark, Genericskb, arXiv:2005.00660A knowledge base of generic statements. 2020arXiv preprint</p>
<p>Language models are few-shot learners. T B Brown, arXiv:2005.141652020arXiv preprint</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. P Clark, I Cowhey, O Etzioni, T Khot, A Sabharwal, C Schoenick, O Tafjord, arXiv:1803.054572018arXiv preprint</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Q Dong, L Li, D Dai, C Zheng, Z Wu, B Chang, X Sun, J Xu, Z Sui, arXiv:2301.00234A survey on in-context learning. 2022arXiv preprint</p>
<p>D Dua, Y Wang, P Dasigi, G Stanovsky, S Singh, M Gardner, Drop, arXiv:1903.00161A reading comprehension benchmark requiring discrete reasoning over paragraphs. 2019arXiv preprint</p>
<p>The llama 3 herd of models. A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Yang, A Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Connectionist architectures for artificial intelligence. S E Fahlman, G E Hinton, Computer. 20011987</p>
<p>S Han, H Schoelkopf, Y Zhao, Z Qi, M Riddell, L Benson, L Sun, E Zubova, Y Qiao, M Burtell, D Peng, J Fan, Y Liu, B Wong, M Sailor, A Ni, L Nan, J Kasai, T Yu, R Zhang, S Joty, A R Fabbri, W Kryscinski, X V Lin, C Xiong, D Radev, Folio, arXiv:2209.00840Natural language reasoning with first-order logic. 2022arXiv preprint</p>
<p>D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>A Jaech, A Kalai, A Lerer, A Richardson, A El-Kishky, A Low, A Helyar, A Madry, A Beutel, A Carney, arXiv:2412.16720Openai o1 system card. 2024arXiv preprint</p>
<p>A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D Casas, F Bressand, G Lengyel, G Lample, L Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>R Johnson, Logic, Book, Fundamentals of Reasoning. Cengage Learning. 20065 edition</p>
<p>D Khashabi, S Min, T Khot, A Sabharwal, O Tafjord, P Clark, H Hajishirzi, Unifiedqa, arXiv:2005.00700Crossing format boundaries with a single qa system. 2020arXiv preprint</p>
<p>Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel. Chief of Naval Technical Training. J Kincaid, 1975</p>
<p>Logiqa 2.0-an improved dataset for logical reasoning in natural language understanding. H Liu, J Liu, L Cui, Z Teng, N Duan, M Zhou, Y Zhang, IEEE/ACM Transactions on Audio. 2023a</p>
<p>Logicot: Logical chain-of-thought instruction tuning. H Liu, Z Teng, L Cui, C Zhang, Q Zhou, Y Zhang, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023b</p>
<p>J Liu, L Cui, H Liu, D Huang, Y Wang, Y Zhang, Logiqa, arXiv:2007.08124A challenge dataset for machine reading comprehension with logical reasoning. 2020arXiv preprint</p>
<p>Learning to Reason with LLMs. Openai, 9 2024</p>
<p>An adversarial winograd schema challenge at scale. K Sakaguchi, R L Bras, C Bhagavatula, Y Choi, Winogrande, Communications of the ACM. 6492021</p>
<p>Testing the general deductive reasoning capacity of large language models using ood examples. A Saparov, R Y Pang, V Padmakumar, N Joshi, M Kazemi, N Kim, H He, Advances in Neural Information Processing Systems. 202436</p>
<p>K Sinha, S Sodhani, J Dong, J Pineau, W L Hamilton, Clutrr, arXiv:1908.06177A diagnostic benchmark for inductive reasoning from text. 2019arXiv preprint</p>
<p>Testing the limits of chain-of-thought with multistep soft reasoning. Z Sprague, X Ye, K Bostrom, S Chaudhuri, G Durrett, Musr, arXiv:2310.160492023arXiv preprint</p>
<p>O Tafjord, B D Mishra, P Clark, Proofwriter, arXiv:2012.13048Generating implications, proofs, and abductive statements over natural language. 2020arXiv preprint</p>
<p>G Team, R Anil, S Borgeaud, Y Wu, J.-B Alayrac, J Yu, R Soricut, J Schalkwyk, A M Dai, A Hauth, arXiv:2312.11805family of highly capable multimodal models. 2023arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q Le, E Chi, S Narang, A Chowdhery, D Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>R Xu, Z Wang, R.-Z Fan, P Liu, arXiv:2404.18824Benchmarking benchmark leakage in large language models. 2024arXiv preprint</p>
<p>Quick and (not so) dirty: Unsupervised selection of justification sentences for multi-hop question answering. V Yadav, S Bethard, M Surdeanu, arXiv:1911.071762019arXiv preprint</p>
<p>W Yu, Z Jiang, Y Dong, J Feng, Reclor, arXiv:2002.04326A reading comprehension dataset requiring logical reasoning. 2020arXiv preprint</p>
<p>R Zellers, A Holtzman, Y Bisk, A Farhadi, Y Choi, Hellaswag, arXiv:1905.07830Can a machine really finish your sentence?. 2019arXiv preprint</p>
<p>H Zhang, L H Li, T Meng, K.-W Chang, G V Broeck, arXiv:2205.11502On the paradox of learning to reason from data. 2022arXiv preprint</p>
<p>Least-to-most prompting enables complex reasoning in large language models. D Zhou, N Schärli, L Hou, J Wei, N Scales, X Wang, D Schuurmans, C Cui, O Bousquet, Q Le, arXiv:2205.106252022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
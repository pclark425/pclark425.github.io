<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3427 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3427</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3427</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-234335834</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2022.findings-acl.127.pdf" target="_blank">Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text</a></p>
                <p><strong>Paper Abstract:</strong> Logical reasoning of text requires identifying critical logical structures in the text and performing inference over them. Existing methods for logical reasoning mainly focus on contextual semantics of text while struggling to explicitly model the logical inference process. In this paper, we not only put forward a logic-driven context extension framework but also propose a logic-driven data augmentation algorithm. The former follows a three-step reasoning paradigm, and each step is respectively to extract logical expressions as elementary reasoning units, symbolically infer the implicit expressions following equivalence laws and extend the context to validate the options. The latter augments literally similar but logically different instances and incorporates contrastive learning to better capture logical information, especially logical negative and conditional relationships. We conduct experiments on two benchmark datasets, ReClor and LogiQA. The results show that our method achieves state-of-the-art performance on both datasets, and even surpasses human performance on the ReClor dataset.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3427.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3427.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LReasoner (RoBERTa)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic-Driven Reasoner with RoBERTa backbone (LReasoner RoBERTa)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic system that augments a pre-trained RoBERTa encoder with a symbolic logical-identification and inference module (identify logical symbols/expressions, extend via contraposition and transitivity, verbalize inferred expressions) plus logic-driven data augmentation using contrastive learning with logically-constructed negative contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LReasoner (RoBERTa backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based RoBERTa encoder fine-tuned for multiple-choice QA; combined with a symbolic module that (1) parses context/options into propositional logical expressions using a constituency parser and keyword rules, (2) extends expressions using propositional equivalence laws (contraposition and transitive law), (3) verbalizes inferred expressions and feeds them as extended context; training also uses logic-driven contrastive learning (SimCLR-style) with logical negative samples (delete/reverse/negate operations on expressions). Implemented with HuggingFace RoBERTa checkpoints and fine-tuned end-to-end (combined classification and contrastive loss).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor, LogiQA (multiple-choice logical reasoning); evaluated additionally on SQuAD (extractive QA)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Passage-level multiple-choice logical reasoning benchmarks requiring propositional inference over natural language premises (implication, contraposition, transitivity, weaken/strengthen, identify flaws, most-strongly-supported, etc.). SQuAD is an extractive QA benchmark used to test generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Neuro-symbolic logic-driven context extension (logic identification, symbolic extension via contraposition & transitivity, verbalization) + logic-driven data augmentation using contrastive learning with logically-constructed negative contexts (delete/reverse/negate logical expressions). Inputs reformulated to include extended verbalized expressions via a special [EXT] token.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported to achieve state-of-the-art accuracy on ReClor and LogiQA (paper: LReasoner variants outperform baseline pre-trained models and prior SOTA). Specifically on SQuAD dev set: RoBERTa-base -> LReasoner RoBERTa-base: EM 83.0% -> 85.6%, F1 90.4% -> 91.7%; RoBERTa-large -> LReasoner RoBERTa-large: EM 88.9% -> 89.3%, F1 94.6% -> 94.8%. The paper also reports large improvements on both EASY and HARD splits of ReClor and claims surpassing human performance on ReClor (exact ReClor/LogiQA numeric accuracies are reported in the paper's Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Vanilla RoBERTa (fine-tuned on task) and other pre-trained baselines (BERT, ALBERT, DeBERTa) were used; for SQuAD the RoBERTa-base and large baseline numbers are EM 83.0/88.9 and F1 90.4/94.6 respectively (see above). On ReClor/LogiQA, baseline pre-trained models (RoBERTa/ALBERT/DeBERTa) obtained substantially lower accuracies than LReasoner (exact baseline accuracies are in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Combining context extension and logic-driven data augmentation produced consistent gains over vanilla RoBERTa: on SQuAD base +2.6 EM and +1.3 F1; on SQuAD large +0.4 EM and +0.2 F1. On ReClor/LogiQA the paper reports state-of-the-art gains and large improvements on the HARD ReClor split relative to baselines; ablation shows both context-extension (CE) and data-augmentation (DA) individually help and CE+DA together is best.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Symbolic logic identification is imperfect (reported recall: logical symbol identification recall 65.9% and logical expression identification recall 48.9%), which constrains the overall system; some reasoning types remain challenging (notably 'Match flaws' and 'Weaken' question types where performance decreased or did not improve); the method relies on heuristic constituency parsing and keyword rules and so can fail when symbols/conditionals are expressed implicitly or with complex linguistic forms.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablation study: RoBERTa+CE and RoBERTa+DA each outperform the vanilla RoBERTa baseline, while RoBERTa+CE+DA (full LReasoner) performs best. Contrastive learning variants: logic-driven negative samples (CLR-L: delete/reverse/negate logical expression) outperform other negative-sample strategies (random in-batch RS, random-deletion RD). Per-reasoning-type analysis (LReasoner ALBERT vs ALBERT baseline) shows especially large gains on Implication and Most Strongly Supported types; some types (Weaken, Role, Conclusion/Main Point) saw worse or mixed results, indicating uneven gains across reasoning categories.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>The paper explicitly attributes improvements to adding explicit symbolic inference (extend implicit expressions) and training the pre-trained model to attend to these via verbalized extensions and contrastive learning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3427.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3427.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LReasoner (ALBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic-Driven Reasoner with ALBERT backbone (LReasoner ALBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The same neuro-symbolic LReasoner pipeline built on an ALBERT encoder; applies symbolic logic identification and extension plus logic-driven contrastive augmentation to improve passage-level logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LReasoner (ALBERT backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ALBERT-xxlarge-v2 transformer checkpoint used as the pre-trained encoder; same symbolic preprocessing (logic identification, extension via contraposition/transitivity) and logic-driven data augmentation with a combined classification + contrastive loss; fine-tuned on ReClor/LogiQA.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor, LogiQA (multiple-choice logical reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Passage-level multiple-choice logical reasoning covering 17+ reasoning types (implication, necessary/sufficient assumptions, strengthen/weaken, identify flaws, match structure, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Logic-driven context extension + logic-driven contrastive data augmentation; identification/verbalization of symbolic expressions; training with combined LA + LC loss.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Compared to baseline ALBERT, LReasoner ALBERT shows notable per-type improvements. Examples from the paper's analysis (ALBERT baseline -> LReasoner ALBERT): Implication 43.8% -> 54.3%, Most Strongly Supported 58.9% -> 71.4%, Explain/Resolve 60.7% -> 67.9%, Identify a Flaw 65.0% -> 71.8%, Match the Structure 56.7% -> 86.7%. Some categories saw decreases (Weaken 64.6% -> 59.3%, Conclusion/Main Point 80.6% -> 77.8%, Role 78.1% -> 68.8%). The paper reports overall state-of-the-art accuracy on ReClor/LogiQA with this method.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>ALBERT baseline per-type numbers are those in the left column of the per-type table (e.g., Implication 43.8%, Most Strongly Supported 58.9%, etc.). Overall baseline accuracies on ReClor/LogiQA (ALBERT) are lower than LReasoner; exact overall baseline accuracies are published in the paper's main tables.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Substantial improvements in several reasoning categories (notably Implication and Most Strongly Supported), with up to ~30 percentage-point gain on some narrow categories (e.g., Match the Structure +30.0pp in the sample table), while some categories deteriorate, indicating gains are concentrated in explicit deductive reasoning types.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Underperforms or degrades on some reasoning types (Weaken, Conclusion/Main Point, Role), reflecting difficulty modeling degrees/strength of statements and flaw identification. Relies on unsupervised symbol parsing (recall for symbols 65.9% and for expressions 48.9%), limiting coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Per-type breakdown shows the method particularly helps tasks that require explicit unit-level inference (Implication, Most Strongly Supported) and less so tasks requiring subtler rhetorical/degree judgments (Weaken, Match Flaws). Ablations in the paper show both CE and DA components contribute to gains; contrastive learning with logical negatives improves performance compared to random negative strategies.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3427.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3427.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LReasoner (DeBERTa)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic-Driven Reasoner with DeBERTa backbone (LReasoner DeBERTa)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LReasoner applied using DeBERTa-xlarge as the pre-trained encoder; integrates symbolic logic identification/extension and logic-driven contrastive augmentation, showing robust improvements across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LReasoner (DeBERTa backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>DeBERTa-xlarge pre-trained encoder fine-tuned within the LReasoner framework; symbolic preprocessing and contrastive training identical to other LReasoner variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor, LogiQA (multiple-choice logical reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Passage-level multiple-choice logical reasoning benchmarks requiring propositional inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Same neuro-symbolic context extension plus logic-driven contrastive data augmentation with logical negative sample construction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported in the paper as the strongest single-model LReasoner variant: 'LReasoner DeBERTa consistently perform[s] better.' Exact numeric accuracies on ReClor and LogiQA are provided in the paper's Table 2 (not reproduced verbatim here). LReasoner Ensemble (including DeBERTa and ALBERT variants) gives the best aggregated performance.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>DeBERTa baseline (vanilla fine-tuned) is reported lower than LReasoner DeBERTa; exact baseline numbers are in the paper's results tables.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Consistent gains over the DeBERTa baseline and other backbones; qualitative summary: robust improvement across EASY/HARD splits and across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Same systemic limitations as other LReasoner variants: imperfect unsupervised logic parsing, challenges on some reasoning types (Weaken, Match flaws), reliance on verbalization templates (If-Then), which may not capture all linguistic nuance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper-level ablations (backbone-agnostic) show CE and DA both help; logically-constructed negatives for contrastive learning outperform other negative strategies.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3427.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3427.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa: A Robustly Optimized BERT Pretraining Approach</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely-used transformer-based pre-trained language model (optimized BERT variant) used as a baseline encoder for the multiple-choice logical reasoning tasks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Roberta: A robustly optimized bert pretraining approach.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based masked-language-model pre-trained on large corpora (improved BERT training recipe); used here as baseline and as backbone for LReasoner RoBERTa. Implementations used: RoBERTa-base and RoBERTa-large (HuggingFace).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor, LogiQA, SQuAD</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Passage-level multiple-choice reasoning (ReClor/LogiQA) and extractive QA (SQuAD) used to measure baseline contextual reasoning ability without explicit symbolic inference.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Vanilla fine-tuning on the downstream QA tasks (concatenate context + question+option), no explicit symbolic module in baseline condition.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>SQuAD dev set baseline numbers reported in the paper: RoBERTa-base EM 83.0%, F1 90.4%; RoBERTa-large EM 88.9%, F1 94.6%. On ReClor/LogiQA baselines are reported in the paper's main tables (lower than LReasoner variants).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Used as the primary baseline: adding logic-driven context extension and data augmentation (LReasoner) improves over RoBERTa by +2.6 EM / +1.3 F1 for base and smaller gains for large on SQuAD; larger improvements reported on logical-reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Pre-trained RoBERTa tends to capture contextual semantics but struggles to model explicit symbolic logical inference and to generalize to strict propositional reasoning without explicit symbolic augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>When augmented with LReasoner modules (CE and DA), RoBERTa shows consistent gains, indicating that explicit symbolic inference signals and logic-aware contrastive training complement contextual pre-training.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3427.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3427.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ALBERT (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parameter-efficient transformer-based pre-trained model used as a baseline and as an LReasoner backbone; baseline performance is compared against LReasoner ALBERT in detailed per-type analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Albert: A lite bert for self-supervised learning of language representations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ALBERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ALBERT-xxlarge-v2 (parameter-efficient variant of BERT) used as an encoder; baseline is fine-tuned on QA tasks without explicit symbolic inference.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor, LogiQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same passage-level logical reasoning benchmarks; ALBERT baseline used to compute per-reasoning-type comparisons versus LReasoner ALBERT.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Vanilla fine-tuning of ALBERT on downstream tasks (baseline) and ALBERT as backbone for LReasoner when evaluating the proposed interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Per-type baseline percentages (ALBERT) from the paper include: Necessary Assumptions 73.7%, Implication 43.8%, Most Strongly Supported 58.9%, Weaken 64.6%, Identify a Flaw 65.0%, Match the Structure 56.7%, etc. Overall ALBERT accuracy on ReClor/LogiQA is reported in the paper's tables and is lower than LReasoner ALBERT.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>LReasoner ALBERT yields substantial per-type improvements in many categories (see separate LReasoner ALBERT entry); indicates symbolic extension + contrastive learning addresses deficits of vanilla ALBERT on explicit deductive reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>As a baseline, ALBERT lacks explicit symbolic inference and therefore underperforms on strict propositional reasoning types; some categories (e.g., Match Flaws) remain challenging even after LReasoner augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper uses ALBERT as the main 'Base' for the per-type breakdown; analysis demonstrates which reasoning types benefit from explicit symbolic signals.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3427.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3427.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeBERTa (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeBERTa: Decoding-enhanced BERT with Disentangled Attention</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pre-trained transformer (DeBERTa-xlarge used in experiments) used both as a baseline and as backbone for a top-performing LReasoner variant.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deberta: Decoding-enhanced bert with disentangled attention.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based pre-trained encoder with disentangled attention (DeBERTa-xlarge used in the paper); used both as a baseline and as the backbone for LReasoner DeBERTa, which yielded the strongest single-model results.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor, LogiQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Passage-level multiple-choice logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Vanilla fine-tuning for baseline; when used as LReasoner backbone, the same symbolic CE+DA pipeline is applied.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports that LReasoner DeBERTa is the best-performing single model variant (exact numeric results are provided in the paper's main results table). DeBERTa baseline performance (vanilla) is lower than the LReasoner DeBERTa variant.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>LReasoner DeBERTa improves on DeBERTa baseline; combining multiple LReasoner variants into an ensemble further improves results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Same systemic limitations as other backbones: dependency on imperfect unsupervised logic identification; linguistic forms not matching the verbalization templates can reduce effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper-level ablation results (backbone-agnostic) show the CE and DA modules provide additive benefits across backbones, and logic-driven contrastive negatives are more effective than random negatives.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reclor: A reading comprehension dataset requiring logical reasoning. <em>(Rating: 2)</em></li>
                <li>Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. <em>(Rating: 2)</em></li>
                <li>Dagn: Discourse-aware graph network for logical reasoning. <em>(Rating: 2)</em></li>
                <li>Roberta: A robustly optimized bert pretraining approach. <em>(Rating: 1)</em></li>
                <li>Albert: A lite bert for self-supervised learning of language representations. <em>(Rating: 1)</em></li>
                <li>Deberta: Decoding-enhanced bert with disentangled attention. <em>(Rating: 1)</em></li>
                <li>A simple framework for contrastive learning of visual representations. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3427",
    "paper_id": "paper-234335834",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "LReasoner (RoBERTa)",
            "name_full": "Logic-Driven Reasoner with RoBERTa backbone (LReasoner RoBERTa)",
            "brief_description": "A neuro-symbolic system that augments a pre-trained RoBERTa encoder with a symbolic logical-identification and inference module (identify logical symbols/expressions, extend via contraposition and transitivity, verbalize inferred expressions) plus logic-driven data augmentation using contrastive learning with logically-constructed negative contexts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LReasoner (RoBERTa backbone)",
            "model_description": "Transformer-based RoBERTa encoder fine-tuned for multiple-choice QA; combined with a symbolic module that (1) parses context/options into propositional logical expressions using a constituency parser and keyword rules, (2) extends expressions using propositional equivalence laws (contraposition and transitive law), (3) verbalizes inferred expressions and feeds them as extended context; training also uses logic-driven contrastive learning (SimCLR-style) with logical negative samples (delete/reverse/negate operations on expressions). Implemented with HuggingFace RoBERTa checkpoints and fine-tuned end-to-end (combined classification and contrastive loss).",
            "model_size": null,
            "reasoning_task_name": "ReClor, LogiQA (multiple-choice logical reasoning); evaluated additionally on SQuAD (extractive QA)",
            "reasoning_task_description": "Passage-level multiple-choice logical reasoning benchmarks requiring propositional inference over natural language premises (implication, contraposition, transitivity, weaken/strengthen, identify flaws, most-strongly-supported, etc.). SQuAD is an extractive QA benchmark used to test generalizability.",
            "method_or_intervention": "Neuro-symbolic logic-driven context extension (logic identification, symbolic extension via contraposition & transitivity, verbalization) + logic-driven data augmentation using contrastive learning with logically-constructed negative contexts (delete/reverse/negate logical expressions). Inputs reformulated to include extended verbalized expressions via a special [EXT] token.",
            "performance": "Reported to achieve state-of-the-art accuracy on ReClor and LogiQA (paper: LReasoner variants outperform baseline pre-trained models and prior SOTA). Specifically on SQuAD dev set: RoBERTa-base -&gt; LReasoner RoBERTa-base: EM 83.0% -&gt; 85.6%, F1 90.4% -&gt; 91.7%; RoBERTa-large -&gt; LReasoner RoBERTa-large: EM 88.9% -&gt; 89.3%, F1 94.6% -&gt; 94.8%. The paper also reports large improvements on both EASY and HARD splits of ReClor and claims surpassing human performance on ReClor (exact ReClor/LogiQA numeric accuracies are reported in the paper's Table 2).",
            "baseline_performance": "Vanilla RoBERTa (fine-tuned on task) and other pre-trained baselines (BERT, ALBERT, DeBERTa) were used; for SQuAD the RoBERTa-base and large baseline numbers are EM 83.0/88.9 and F1 90.4/94.6 respectively (see above). On ReClor/LogiQA, baseline pre-trained models (RoBERTa/ALBERT/DeBERTa) obtained substantially lower accuracies than LReasoner (exact baseline accuracies are in the paper).",
            "improvement_over_baseline": "Combining context extension and logic-driven data augmentation produced consistent gains over vanilla RoBERTa: on SQuAD base +2.6 EM and +1.3 F1; on SQuAD large +0.4 EM and +0.2 F1. On ReClor/LogiQA the paper reports state-of-the-art gains and large improvements on the HARD ReClor split relative to baselines; ablation shows both context-extension (CE) and data-augmentation (DA) individually help and CE+DA together is best.",
            "limitations_or_failures": "Symbolic logic identification is imperfect (reported recall: logical symbol identification recall 65.9% and logical expression identification recall 48.9%), which constrains the overall system; some reasoning types remain challenging (notably 'Match flaws' and 'Weaken' question types where performance decreased or did not improve); the method relies on heuristic constituency parsing and keyword rules and so can fail when symbols/conditionals are expressed implicitly or with complex linguistic forms.",
            "ablation_or_analysis": "Ablation study: RoBERTa+CE and RoBERTa+DA each outperform the vanilla RoBERTa baseline, while RoBERTa+CE+DA (full LReasoner) performs best. Contrastive learning variants: logic-driven negative samples (CLR-L: delete/reverse/negate logical expression) outperform other negative-sample strategies (random in-batch RS, random-deletion RD). Per-reasoning-type analysis (LReasoner ALBERT vs ALBERT baseline) shows especially large gains on Implication and Most Strongly Supported types; some types (Weaken, Role, Conclusion/Main Point) saw worse or mixed results, indicating uneven gains across reasoning categories.",
            "notes": "The paper explicitly attributes improvements to adding explicit symbolic inference (extend implicit expressions) and training the pre-trained model to attend to these via verbalized extensions and contrastive learning.",
            "uuid": "e3427.0"
        },
        {
            "name_short": "LReasoner (ALBERT)",
            "name_full": "Logic-Driven Reasoner with ALBERT backbone (LReasoner ALBERT)",
            "brief_description": "The same neuro-symbolic LReasoner pipeline built on an ALBERT encoder; applies symbolic logic identification and extension plus logic-driven contrastive augmentation to improve passage-level logical reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LReasoner (ALBERT backbone)",
            "model_description": "ALBERT-xxlarge-v2 transformer checkpoint used as the pre-trained encoder; same symbolic preprocessing (logic identification, extension via contraposition/transitivity) and logic-driven data augmentation with a combined classification + contrastive loss; fine-tuned on ReClor/LogiQA.",
            "model_size": null,
            "reasoning_task_name": "ReClor, LogiQA (multiple-choice logical reasoning)",
            "reasoning_task_description": "Passage-level multiple-choice logical reasoning covering 17+ reasoning types (implication, necessary/sufficient assumptions, strengthen/weaken, identify flaws, match structure, etc.).",
            "method_or_intervention": "Logic-driven context extension + logic-driven contrastive data augmentation; identification/verbalization of symbolic expressions; training with combined LA + LC loss.",
            "performance": "Compared to baseline ALBERT, LReasoner ALBERT shows notable per-type improvements. Examples from the paper's analysis (ALBERT baseline -&gt; LReasoner ALBERT): Implication 43.8% -&gt; 54.3%, Most Strongly Supported 58.9% -&gt; 71.4%, Explain/Resolve 60.7% -&gt; 67.9%, Identify a Flaw 65.0% -&gt; 71.8%, Match the Structure 56.7% -&gt; 86.7%. Some categories saw decreases (Weaken 64.6% -&gt; 59.3%, Conclusion/Main Point 80.6% -&gt; 77.8%, Role 78.1% -&gt; 68.8%). The paper reports overall state-of-the-art accuracy on ReClor/LogiQA with this method.",
            "baseline_performance": "ALBERT baseline per-type numbers are those in the left column of the per-type table (e.g., Implication 43.8%, Most Strongly Supported 58.9%, etc.). Overall baseline accuracies on ReClor/LogiQA (ALBERT) are lower than LReasoner; exact overall baseline accuracies are published in the paper's main tables.",
            "improvement_over_baseline": "Substantial improvements in several reasoning categories (notably Implication and Most Strongly Supported), with up to ~30 percentage-point gain on some narrow categories (e.g., Match the Structure +30.0pp in the sample table), while some categories deteriorate, indicating gains are concentrated in explicit deductive reasoning types.",
            "limitations_or_failures": "Underperforms or degrades on some reasoning types (Weaken, Conclusion/Main Point, Role), reflecting difficulty modeling degrees/strength of statements and flaw identification. Relies on unsupervised symbol parsing (recall for symbols 65.9% and for expressions 48.9%), limiting coverage.",
            "ablation_or_analysis": "Per-type breakdown shows the method particularly helps tasks that require explicit unit-level inference (Implication, Most Strongly Supported) and less so tasks requiring subtler rhetorical/degree judgments (Weaken, Match Flaws). Ablations in the paper show both CE and DA components contribute to gains; contrastive learning with logical negatives improves performance compared to random negative strategies.",
            "uuid": "e3427.1"
        },
        {
            "name_short": "LReasoner (DeBERTa)",
            "name_full": "Logic-Driven Reasoner with DeBERTa backbone (LReasoner DeBERTa)",
            "brief_description": "LReasoner applied using DeBERTa-xlarge as the pre-trained encoder; integrates symbolic logic identification/extension and logic-driven contrastive augmentation, showing robust improvements across datasets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LReasoner (DeBERTa backbone)",
            "model_description": "DeBERTa-xlarge pre-trained encoder fine-tuned within the LReasoner framework; symbolic preprocessing and contrastive training identical to other LReasoner variants.",
            "model_size": null,
            "reasoning_task_name": "ReClor, LogiQA (multiple-choice logical reasoning)",
            "reasoning_task_description": "Passage-level multiple-choice logical reasoning benchmarks requiring propositional inferences.",
            "method_or_intervention": "Same neuro-symbolic context extension plus logic-driven contrastive data augmentation with logical negative sample construction.",
            "performance": "Reported in the paper as the strongest single-model LReasoner variant: 'LReasoner DeBERTa consistently perform[s] better.' Exact numeric accuracies on ReClor and LogiQA are provided in the paper's Table 2 (not reproduced verbatim here). LReasoner Ensemble (including DeBERTa and ALBERT variants) gives the best aggregated performance.",
            "baseline_performance": "DeBERTa baseline (vanilla fine-tuned) is reported lower than LReasoner DeBERTa; exact baseline numbers are in the paper's results tables.",
            "improvement_over_baseline": "Consistent gains over the DeBERTa baseline and other backbones; qualitative summary: robust improvement across EASY/HARD splits and across datasets.",
            "limitations_or_failures": "Same systemic limitations as other LReasoner variants: imperfect unsupervised logic parsing, challenges on some reasoning types (Weaken, Match flaws), reliance on verbalization templates (If-Then), which may not capture all linguistic nuance.",
            "ablation_or_analysis": "Paper-level ablations (backbone-agnostic) show CE and DA both help; logically-constructed negatives for contrastive learning outperform other negative strategies.",
            "uuid": "e3427.2"
        },
        {
            "name_short": "RoBERTa (baseline)",
            "name_full": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "brief_description": "A widely-used transformer-based pre-trained language model (optimized BERT variant) used as a baseline encoder for the multiple-choice logical reasoning tasks in this paper.",
            "citation_title": "Roberta: A robustly optimized bert pretraining approach.",
            "mention_or_use": "use",
            "model_name": "RoBERTa",
            "model_description": "Transformer-based masked-language-model pre-trained on large corpora (improved BERT training recipe); used here as baseline and as backbone for LReasoner RoBERTa. Implementations used: RoBERTa-base and RoBERTa-large (HuggingFace).",
            "model_size": null,
            "reasoning_task_name": "ReClor, LogiQA, SQuAD",
            "reasoning_task_description": "Passage-level multiple-choice reasoning (ReClor/LogiQA) and extractive QA (SQuAD) used to measure baseline contextual reasoning ability without explicit symbolic inference.",
            "method_or_intervention": "Vanilla fine-tuning on the downstream QA tasks (concatenate context + question+option), no explicit symbolic module in baseline condition.",
            "performance": "SQuAD dev set baseline numbers reported in the paper: RoBERTa-base EM 83.0%, F1 90.4%; RoBERTa-large EM 88.9%, F1 94.6%. On ReClor/LogiQA baselines are reported in the paper's main tables (lower than LReasoner variants).",
            "baseline_performance": null,
            "improvement_over_baseline": "Used as the primary baseline: adding logic-driven context extension and data augmentation (LReasoner) improves over RoBERTa by +2.6 EM / +1.3 F1 for base and smaller gains for large on SQuAD; larger improvements reported on logical-reasoning benchmarks.",
            "limitations_or_failures": "Pre-trained RoBERTa tends to capture contextual semantics but struggles to model explicit symbolic logical inference and to generalize to strict propositional reasoning without explicit symbolic augmentation.",
            "ablation_or_analysis": "When augmented with LReasoner modules (CE and DA), RoBERTa shows consistent gains, indicating that explicit symbolic inference signals and logic-aware contrastive training complement contextual pre-training.",
            "uuid": "e3427.3"
        },
        {
            "name_short": "ALBERT (baseline)",
            "name_full": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
            "brief_description": "A parameter-efficient transformer-based pre-trained model used as a baseline and as an LReasoner backbone; baseline performance is compared against LReasoner ALBERT in detailed per-type analyses.",
            "citation_title": "Albert: A lite bert for self-supervised learning of language representations.",
            "mention_or_use": "use",
            "model_name": "ALBERT",
            "model_description": "ALBERT-xxlarge-v2 (parameter-efficient variant of BERT) used as an encoder; baseline is fine-tuned on QA tasks without explicit symbolic inference.",
            "model_size": null,
            "reasoning_task_name": "ReClor, LogiQA",
            "reasoning_task_description": "Same passage-level logical reasoning benchmarks; ALBERT baseline used to compute per-reasoning-type comparisons versus LReasoner ALBERT.",
            "method_or_intervention": "Vanilla fine-tuning of ALBERT on downstream tasks (baseline) and ALBERT as backbone for LReasoner when evaluating the proposed interventions.",
            "performance": "Per-type baseline percentages (ALBERT) from the paper include: Necessary Assumptions 73.7%, Implication 43.8%, Most Strongly Supported 58.9%, Weaken 64.6%, Identify a Flaw 65.0%, Match the Structure 56.7%, etc. Overall ALBERT accuracy on ReClor/LogiQA is reported in the paper's tables and is lower than LReasoner ALBERT.",
            "baseline_performance": null,
            "improvement_over_baseline": "LReasoner ALBERT yields substantial per-type improvements in many categories (see separate LReasoner ALBERT entry); indicates symbolic extension + contrastive learning addresses deficits of vanilla ALBERT on explicit deductive reasoning.",
            "limitations_or_failures": "As a baseline, ALBERT lacks explicit symbolic inference and therefore underperforms on strict propositional reasoning types; some categories (e.g., Match Flaws) remain challenging even after LReasoner augmentation.",
            "ablation_or_analysis": "Paper uses ALBERT as the main 'Base' for the per-type breakdown; analysis demonstrates which reasoning types benefit from explicit symbolic signals.",
            "uuid": "e3427.4"
        },
        {
            "name_short": "DeBERTa (baseline)",
            "name_full": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention",
            "brief_description": "A pre-trained transformer (DeBERTa-xlarge used in experiments) used both as a baseline and as backbone for a top-performing LReasoner variant.",
            "citation_title": "Deberta: Decoding-enhanced bert with disentangled attention.",
            "mention_or_use": "use",
            "model_name": "DeBERTa",
            "model_description": "Transformer-based pre-trained encoder with disentangled attention (DeBERTa-xlarge used in the paper); used both as a baseline and as the backbone for LReasoner DeBERTa, which yielded the strongest single-model results.",
            "model_size": null,
            "reasoning_task_name": "ReClor, LogiQA",
            "reasoning_task_description": "Passage-level multiple-choice logical reasoning.",
            "method_or_intervention": "Vanilla fine-tuning for baseline; when used as LReasoner backbone, the same symbolic CE+DA pipeline is applied.",
            "performance": "Paper reports that LReasoner DeBERTa is the best-performing single model variant (exact numeric results are provided in the paper's main results table). DeBERTa baseline performance (vanilla) is lower than the LReasoner DeBERTa variant.",
            "baseline_performance": null,
            "improvement_over_baseline": "LReasoner DeBERTa improves on DeBERTa baseline; combining multiple LReasoner variants into an ensemble further improves results.",
            "limitations_or_failures": "Same systemic limitations as other backbones: dependency on imperfect unsupervised logic identification; linguistic forms not matching the verbalization templates can reduce effectiveness.",
            "ablation_or_analysis": "Paper-level ablation results (backbone-agnostic) show the CE and DA modules provide additive benefits across backbones, and logic-driven contrastive negatives are more effective than random negatives.",
            "uuid": "e3427.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reclor: A reading comprehension dataset requiring logical reasoning.",
            "rating": 2,
            "sanitized_title": "reclor_a_reading_comprehension_dataset_requiring_logical_reasoning"
        },
        {
            "paper_title": "Logiqa: A challenge dataset for machine reading comprehension with logical reasoning.",
            "rating": 2,
            "sanitized_title": "logiqa_a_challenge_dataset_for_machine_reading_comprehension_with_logical_reasoning"
        },
        {
            "paper_title": "Dagn: Discourse-aware graph network for logical reasoning.",
            "rating": 2,
            "sanitized_title": "dagn_discourseaware_graph_network_for_logical_reasoning"
        },
        {
            "paper_title": "Roberta: A robustly optimized bert pretraining approach.",
            "rating": 1,
            "sanitized_title": "roberta_a_robustly_optimized_bert_pretraining_approach"
        },
        {
            "paper_title": "Albert: A lite bert for self-supervised learning of language representations.",
            "rating": 1,
            "sanitized_title": "albert_a_lite_bert_for_selfsupervised_learning_of_language_representations"
        },
        {
            "paper_title": "Deberta: Decoding-enhanced bert with disentangled attention.",
            "rating": 1,
            "sanitized_title": "deberta_decodingenhanced_bert_with_disentangled_attention"
        },
        {
            "paper_title": "A simple framework for contrastive learning of visual representations.",
            "rating": 1,
            "sanitized_title": "a_simple_framework_for_contrastive_learning_of_visual_representations"
        }
    ],
    "cost": 0.01779375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text
May 22-27, 2022</p>
<p>Siyuan Wang 
School of Data Science
Fudan University
China</p>
<p>Wanjun Zhong 
Sun Yat-Sen University
China</p>
<p>Duyu Tang dutang@microsoft.com 
Microsoft
China</p>
<p>Zhongyu Wei zywei@fudan.edu.cn 
School of Data Science
Fudan University
China</p>
<p>Research Institute of Intelligent and Complex Systems
Fudan University
China</p>
<p>Zhihao Fan 
School of Data Science
Fudan University
China</p>
<p>Daxin Jiang djiang@microsoft.com 
Microsoft
China</p>
<p>Ming Zhou 
Sinovation Ventures
China</p>
<p>Nan Duan nanduan@microsoft.com 
Microsoft
China</p>
<p>Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text</p>
<p>Association for Computational Linguistics: ACL 2022
May 22-27, 2022
Logical reasoning of text requires identifying critical logical structures in the text and performing inference over them. Existing methods for logical reasoning mainly focus on contextual semantics of text while struggling to explicitly model the logical inference process. In this paper, we not only put forward a logic-driven context extension framework but also propose a logic-driven data augmentation algorithm. The former follows a three-step reasoning paradigm, and each step is respectively to extract logical expressions as elementary reasoning units, symbolically infer the implicit expressions following equivalence laws and extend the context to validate the options. The latter augments literally similar but logically different instances and incorporates contrastive learning to better capture logical information, especially logical negative and conditional relationships. We conduct experiments on two benchmark datasets, ReClor and LogiQA. The results show that our method achieves state-of-the-art performance on both datasets, and even surpasses human performance on the ReClor dataset. 1</p>
<p>Introduction</p>
<p>Recent years have witnessed a growing interest in logical reasoning of text, which learns to understand a given text in logical level and perform logical inference to deduce implications from asserted ones (McCarthy, 1989;Nilsson, 1991). As a significant component of human reading comprehension, it is essential in many application scenarios, such as negotiation and debate. And several datasets have been proposed as benchmarks for this task (Williams et al., 2017;Habernal et al., 2017;Yu et al., 2020;Liu et al., 2020).</p>
<p>An example of logical reasoning problems is shown in Figure 1, which takes a context descrip- * Work is done during internship at Microsoft. Zhongyu Wei and Duyu Tang are corresponding authors. 1 Codes are publicly available at https://github. com/WangsyGit/LReasoner.</p>
<dl>
<dt>Logical Symbols :</dt>
<dd>
<p>have keyboarding skills : be able to use a compute : be able to write your essays using a word processing program Extend the Implicit Logical Expressions by Laws:</p>
</dd>
</dl>
<p>Context:</p>
<p>If you have no keyboarding skills at all, you will not be able to use a computer. And if you are not able to use a computer, you will not be able to write your essays using a word processing program. Question If above statements are true, which one of the following must be true? Options A. If you are not able to write your essays using a word processing program, you have no keyboarding skills. B. If you are able to write your essays using a word processing program, you have at least some keyboarding skills. C. If you are not able to write your essays using a word processing program, you are not able to use a computer. D. If you have some keyboarding skills, you will be able to write your essays using a word processing program.</p>
<p>Logical Expressions :
(   ) (   ) (   )  (  ) (   )  (  ) (   )  (   )  (   ) (  )  (  )  (  ) (   ) (  ) (  ) (   )</p>
<p>Contrapostion Contrapostion</p>
<p>Transitive Law Transitive Law Figure 1: A logical reasoning example from ReClor dataset (Yu et al., 2020). To find the answer, it needs to extract logical symbols, identify logical expressions and perform logical inference to extend the implicit logical expressions. The underlined phrases represent logical symbols. The colored rectangles are corresponding logical expressions of each option.</p>
<p>tion, a question and four options as the input, and aims to identify the option that logically follows the context. The main challenge to solve such a problem is to uncover the logical propositional structure among the text and perform logical inference over them, which are beyond the capability of contextual pre-trained models Yang et al., 2019;Lan et al., 2020) without such logical annotations. They usually treat logical reasoning as a traditional reading comprehension task and match the given context with candidate answers, without modeling the discrete logical inference process explicitly (Yu et al., 2020). Recently, Huang et al. (2021) utilizes discourse information to unwrap the logical structure and propose a discourse-aware graph network to learn discourse-based contextual embeddings for logical reasoning. However, it is still entangled in enhancing contextual representation while ignoring explicit logical inference.</p>
<p>In responding to these issues, we propose a threestep paradigm for logical reasoning based on symbolic logic information. Firstly, we identify the elementary components for reasoning from the context as the logical expressions, like (  ), to uncover the logical relationships between logical symbols. Then we perform logical inference following equivalence laws to extend the implicit ones from these identified logical expressions. Thirdly, candidate options can be validated by comparing themselves with all obtained logical expressions.</p>
<p>We propose a logic-driven context extension framework to integrate these three reasoning steps, namely logic identification to parse the context into logical expressions, logic extension to infer implicit logical expressions and logic verbalization for answer prediction. To combine the interpretability of symbolic inference with anti-noise of continuous representation, we follow a neuralsymbolic paradigm (Besold et al., 2017;Garcez et al., 2019) which conducts logic identification and extension in a symbolic manner and utilizes the pre-trained model as the backbone of logic verbalization. In practice, we verbalize implicit logical expressions into natural language and feed them as an extended context into a pre-trained model to match the answer. Moreover, to encourage the pretrained model to better capture logical information, we further propose a logic-driven data augmentation algorithm. Specifically, it constructs challenging instances with literally similar but logically different contexts by modifying logical expressions. Contrastive learning  is used for encouraging our model to distinguish different contexts to better capture negative and conditional relationships in logical expressions.</p>
<p>The experiments are conducted on two challenging logical reasoning datasets, ReClor (Yu et al., 2020) and LogiQA (Liu et al., 2020). Results show that our system achieves state-of-the-art performance on both datasets, and even surpasses human performance on ReClor. Further results also show the effectiveness of both logic-driven context extension framework and data augmentation algorithm, and demonstrate the generalizability of our system.</p>
<p>Task and Background</p>
<p>Task Definition</p>
<p>We study the problem of logical reasoning of text on a multiple-choice question answering task. The task is described as following: given a context c, a question q, and four associated options {o 1 , o 2 , o 3 , o 4 }, we aim to select the most appropriate option as the answer o a .</p>
<p>Base Model</p>
<p>In this paper, we follow the leading methods on the leaderboards to take pre-trained models as our base model, e.g., RoBERTa . It concatenates the context, the question and each option as an input and encodes the sequence for calculating its score. Given four options, four concatenated sequences are constructed to calculate four scores, and the one with the highest score is chosen as the answer. Specifically, the concatenated sequence is formulated as
[CLS] c [SEP ] q || o [SEP ]
, where c is the context and q || o is the concatenation of the question and each option. The representations of special token [CLS] in four sequences are fed into a linear layer with a softmax function to get the probability distribution of options as P ({o 1 , o 2 , o 3 , o 4 }|c, q). The cross entropy loss is calculated as Eq. 1, where o a is the correct option.
L A =  log P (o a |c, q)(1)
Although promising results have been reported (Yu et al., 2020), pre-trained models for logical reasoning directly encode the triplet of context, question and options, which mainly leverage contextual semantics but struggle to model the symbolic inference process explicitly. Thus we propose a framework on top of a pre-trained model to extract logical expressions in the text and symbolically perform logical inference to predict the answer.</p>
<p>Logic-Driven Context Extension</p>
<p>In this section, we present a logic-driven context extension framework for logical reasoning of text, which is illustrated in Figure 2. The framework is divided into three steps as follows. It first identifies the logical symbols and expressions explicitly mentioned in the context and options (  3.1). Then it performs interpretable logical inference over them to extend the logical expressions implicit in the context (  3.2). Finally, it verbalizes the extended logical expressions related to each option as an</p>
<p>Context:</p>
<p>If you have no keyboarding skills at all, you will not be able to use a computer. And if you are not able to use a computer, you will not be able to write your essays using a word processing program.</p>
<p>Options:</p>
<p>A. If you are not able to write your essays using a word processing program, you have no keyboarding skills. B. If you are able to write your essays using a word processing program, you have at least some keyboarding skills. C. If you are not able to write your essays using a word processing program, you are not able to use a computer. D. If you have some keyboarding skills, you will be able to write your essays using a word processing program.</p>
<p>symbol  symbol  symbol  Implicit Logical Expressions:
(      )  (    ) (     " )  ( "   ) (      )  (     " )  (     " ) (    )  ( "   )  ( "   )
Extended Logical Expressions related to each option:
A. (     " ) ; B. (    ) ; ( "   ) ; ( "   ) ; C. (     " ) ; D. (    ) ; ( "   ) ; ( "   ) ;
Logical Expressions in the context:
(      ) ; (     " ) ;
Logical Expressions in each option:
A. (  "    ) ; B. ( "   ) ; C. (  "    ) ; D. (   " ) ;</p>
<p>Extended contexts of each option:</p>
<p>A. If you do not have keyboarding skills, then you will not be able to write your essays  B. If you are able to use a computer, then you will have keyboarding skills. If you are  . If you are able to write your essays  , then you will have keyboarding skills. C. If you do not have keyboarding skills, then you will not be able to write your essays  D. If you are able to use a computer, then you will have keyboarding skills. If you are </p>
<p>Logic Identification Logic Extension</p>
<p>Pre-trained Encoder
[CLS] c [SEP] q || # ! [EXT] $ ! [SEP]
 score  !</p>
<p>Logic Verbalization</p>
<p>Figure 2: The overall architecture of logic-driven context extension framework. c, q, o i and e i are the context, question, i-th option and the extended context for i-th option, respectively. The texts in green mean that the option B is matched against its extended context which has the highest score.</p>
<p>extended context and utilizes it in the pre-trained model to match the answer (  3.3).</p>
<p>Logic Identification</p>
<p>In order to perform logical reasoning, we first need to identify the elementary reasoning components as logical expressions to uncover the logical relationships between logical symbols. We identify the existing logical expressions for each sentence in the context and each option. To show the format of the logical expression, we introduce some notations:</p>
<p>(1) {, , , ...}: the logical symbols, which are the basic constituents in the context to constitute the logical expressions, such as the "have keyboarding skills" in Figure 2.</p>
<p>(2) {, }: the logical connective set.  means the negation operation upon a specific logical symbol and  acts as a conditional relationship between two logical symbols. To ensure the generalizability of our framework without annotated logic forms, we design a fairly simple logical identification approach using an offthe-shelf constituency parser (Joshi et al., 2018) and several common keywords of logical semantics. We first employ the constituency parser to extract constituents including noun phrases and gerundial phrases as basic symbols. The logical symbols in each sentence are combined by logical connectives to constitute logical expressions as follow-up. If any negative word (e.g., "not", "unable") is in or immediately before a logical symbol , we add the negation connective  before  as a new symbol  . Then if there is a conditional relationship between two symbols  and  in a sentence, we construct the corresponding logical expression as (  ). We simply recognize the conditional relationship between  and  as (  ) according to conditional indicators (e.g., "if , then ", " since ") and whether an active voice occurs between  and . The detailed negative and conditional keywords are listed in Appendix A with the whole identification procedure summarized as an algorithm. As shown in Figure 2, given the context with two sentences, we can extract three logical symbols {, , } and identify two existing logical expressions as (  ) and (  ).</p>
<p>Logic Extension</p>
<p>In addition to the logical expressions explicitly mentioned in the context, there are still some other implicit ones that we need to logically infer and extend. We combine the identified logical expressions existing in all sentences of the context as a logical expression set S, and perform logical inference over them to further extend the implicit expressions according to logical equivalence laws. Here we follow two most applicable logical equivalence laws involving implication and negation in propositional logic, including contraposition (Russel et al., 2013) and transitive law (Zhao et al., 1997):
Contraposition : (  ) = (  )(2)
Transitive Law :
(  )  (  ) = (  ) (3)
Then the extended implicit logical expressions form an extension set of the current logical expression set S as S E . As in Figure 2, the set of existing logical expressions is S = {(  ), (  )} and the logic extension set is
S E = {(  ), (  ), (  ), (  )}.</p>
<p>Logic Verbalization</p>
<p>After inferring the extended logical expression set S E , we verbalize them into natural language for better utilization of the pre-trained model considering that symbolic logic is more difficult to be encoded. We first select the related expressions from S E for each option. A logical expression is regarded as related to an option if it has the same logical symbols with the option judged by the text overlapping and whether a negation connective exists. For example, (  ) in Figure 2 is related to option C because they both contain . Then we transform all logical expressions related to the option at symbolic space into natural language by filling them into a template and concatenate them into a sentence. We take such a sentence as an extended context for this option. For simplicity, we only adopt the If-Then statements as the verbalization template, which is one of the most common patterns of logical reasoning, but we make some adjustments according to the tense and singular/plural. Specifically, the template is designed as shown in Table 1.</p>
<p>Logic (  ) Template If do not , then will not .</p>
<p>Extended context</p>
<p>If you do not have keyboarding skills, then you will not be able to write your essays using a word processing program. We feed extended contexts into the pre-trained model to match the options and predict the answer. We take an extended context as the sentence e, and introduce a special token [EXT ] to represent context extension. Then we reformulate the input sequence as
[CLS] c [SEP ] q || o [EXT ] e [SEP ]
for encoding and feed the [CLS] representation into a classification layer to get each option's score and find the most appropriate answer.</p>
<p>Logic-Driven Data Augmentation</p>
<p>In order to make the pre-trained model put more focus on logical information in the context, especially logical negative and conditional relationships, we further introduce a logic-driven data augmentation algorithm. Inspired by SimCLR , we augment challenging instances with literally similar but logically different contexts built by modifying logical expressions. It then adopts contrastive learning and encourages our model to distinguish logically correct context supporting the answer. We first introduce the background of Sim-CLR and then describe our logic-driven contrastive learning.</p>
<p>SimCLR As a paradigm of self-supervised representation learning by comparing different samples, contrastive learning (Wu et al., 2018;He et al., 2020a) aims to make the representations of similar samples be mapped close together, while that of dissimilar samples be further away in the encoding space. The goal can be described as following.
s(f (x), f (x + ))  s(f (x), f (x  ))(4)
x + is a positive sample similar to the data point x while x  is a negative sample dissimilar to x. f () is an encoder to learn a representation and the s() is a similarity function of two representations. Over this, SimCLR  builds a classifier to distinguish positive from negative samples and learns to capture what makes two samples different.</p>
<p>Logic-Driven Contrastive Learning</p>
<p>In our question answering setting, we alter the score function from measuring the similarity between two representations towards calculating the score that the question can be solved by the correct answer under a given context:
s  (c + , q, o a )  s  (c  , q, o a )(5)
where (c + , q, o a ) and (c  , q, o a ) are the positive and negative sample, c + and c  are the positive and negative context, respectively, and s  is the score function. The contrastive loss can be formulated as a classification loss for predicting the most plausible context that supports the answer:
L C =  log exp(s  (+)) exp(s  (+)) + exp(s  ())(6)
where s  (+) and s  () are short for s  (c + , q, o a ) and s  (c  , q, o a ) respectively.</p>
<p>Aware of symbolic logical expressions, we can construct logical negative samples including negative contexts that are literally similar but logical dissimilar to the positive one. We take the original context to construct the positive sample. Then we generate a negative sample by modifying the existing logical expressions in the context and verbalizing the modified logical expressions into a negative context as  3.3. During the modification operations, we randomly choose a logical expression and randomly delete, reverse or negate such an expression. The delete, reverse or negate operations are respectively to delete a logical expression in the context, reverse the conditional order of a logical expression and negate a logical symbol in a logical expression. The constructing procedure of a logical negative sample is illustrated in Figure 3. Then the model can be trained to better capture logical information, especially negative and conditional relationships in logical expressions. Figure 3: Procedure to construct a logical negative sample.
(!"#$%&amp;$, '(%)$*"#, -#).%/) (0  2), (2  3), </p>
<p>Logic Identification</p>
<p>Randomly delete, reverse or negate a logical expression
(!"#$%&amp;$ ! , '(%)$*"#, -#).%/) delete (2  3),  reverse (2  0), (2  3),  negate (0  2), (2  3),  (0  2), (2  3), </p>
<p>Logic Verbalization</p>
<p>In the logic-driven data augmentation algorithm, our framework is trained with a combined loss as L = L A + L C . And the classification of positive and negative context for the correct answer is also implemented in the logic-driven context extension framework.</p>
<p>Experiments</p>
<p>Experimental Dataset</p>
<p>Our experiments are conducted on two challenging datasets ReClor (Yu et al., 2020) and LogiQA (Liu et al., 2020) that cover diverse and complicated logical reasoning skills, to investigate the general effectiveness of our system. ReClor is built upon standardized exams including GMAT and LSAT. As there are some biased instances that can be solved without knowing contexts and questions, ReClor splits the unbiased instances from the test data as the HARD set to fully assess the logical reasoning ability. The other biased ones are taken as the EASY set. LogiQA comes from the National Civil Servants Examination of China and is professionally translated into an English version.</p>
<p>ReClor consists of 6, 138 questions and is divided into training, validation and test sets with 4, 638, 500 and 1, 000 data points. The test set is further split into EASY set and HARD set with 440 and 560 data points. LogiQA contains 8, 678 questions and is split into 7, 376/651/651 samples for training, validation and testing. Each question is collected with a context and four answer options, in which only one is correct. The implementation details of experiments are given in Appendix B.</p>
<p>Overall Performance</p>
<p>We compare our systems with several baseline models and human performance. Baseline Models The compared baseline pretrained models include BERT (Devlin et al., 2019), RoBERTa , ALBERT (Lan et al., 2020) and DeBERTa (He et al., 2020b). We also compare our model with DAGN (Huang et al., 2021), an available state-of-the-art method on the leaderboard which proposes a discourseaware graph network for logical reasoning taking RoBERTa-large as the backbone. Our Systems LReasoner RoBERTa is our proposed logic-driven reasoner taking RoBERTa as the backbone model, which utilizes both logic-driven context extension framework and data augmentation algorithm. We also build our LReasoner on top of two more powerful pre-trained models ALBERT and DeBERTa as LReasoner ALBERT and LReasoner DeBERTa , respectively. Besides, LReasoner Ensemble is an ensemble of DeBERTa, LReasoner ALBERT and LReasoner DeBERTa . Human Performance Yu et al. (2020) and Liu et al. (2020) report human performance as the average scores of graduate or post-graduate students over randomly chosen test samples.</p>
<p>The evaluation results are shown in   (Yu et al., 2020) and (Liu et al., 2020).</p>
<p>LReasoner DeBERTa consistently perform better. It demonstrates that our method is robust to be effective for logical reasoning based on different pre-trained models, even the most recent state-ofthe-art ones. -Our models generate large improvement on both HARD and EASY sets of ReClor compared with baseline models. This observation verifies that our model is capable of improving logical reasoning ability on both biased and unbiased data.</p>
<p>Further Analysis</p>
<p>Ablation Study To dive into the effectiveness of different components in our logic-driven reasoner, we conduct an ablation study which takes RoBERTa as our backbone model on ReClor validation and test sets. As shown in Table 3, RoBERTa+CE and RoBERTa+DA both outperform the baseline model RoBERTa and perform worse than our final system RoBERTa+CE+DA. It indicates that both logicdriven context extension framework and data augmentation algorithm can boost the performance of question answering involving logical reasoning.  Table 3: Ablation study of our system. CE and DA are respectively our logic-driven context extension framework and data augmentation algorithm. RoBERTa+CE+DA is our proposed LReasoner RoBERTa .</p>
<p>Comparison of Negative Sample Construction</p>
<p>Strategies To further analyze the effectiveness of our logical negative samples in logic-driven contrastive learning, we compare several different negative sample construction strategies in contrastive learning on top of RoBERTa for ReClor.</p>
<p>Model</p>
<p>Test EASY HARD  Table 4: Comparison of different negative sample construction approaches. CLR represents contrastive learning. RS means randomly selecting a context from inbatch data while RD means randomly deleting a sentence from the original context. L denotes our logical negative sample construction method in logic-driven contrastive learning.</p>
<p>From Table 4, we can find that all models with contrastive learning outperform the model without it, which demonstrates that contrastive learning can help to better predict the answer. Our logic-driven contrastive learning RoBERTa(w/ CLR-L) performs best. It reveals that logical negative samples are more effective than negative samples constructed by other methods which make the model better capture the logical negative and conditional relationships in the context for logical reasoning.</p>
<p>Evaluation of Logic Identification</p>
<p>To evaluate the performance of our symbolic logic identification method, we randomly sample 50 instances from the validation set and manually annotate the logical symbols and expressions as labels. We re-Context : Everyone sitting in the clubhouse of the golf course today at ten o' clock had just registered for a beginner' s golf lesson. Gerald, Robert, and Shirley were sitting in the clubhouse this morning at ten o' clock. No accomplished golfer would register for a beginner' s golf lesson. Question : If the statements above are true, which one of the following must also be true on the basis of them? Options : (Answer : C) A. Gerald, Robert, and Shirley were the only people who registered for a beginner's golf lesson this morning. (  Others ) B. None of the people sitting in the clubhouse this morning at ten o' clock had ever played golf. (    Others ) C. Neither Gerald nor Shirley is an accomplished golfer. (   ) D. Everyone sitting in the clubhouse this morning at ten o' clock registered only for a beginner's golf lesson. (   Others )</p>
<p>Logical Symbols &amp; Expressions</p>
<dl>
<dt> : sitting in the clubhouse of the golf course today at ten o' clock;  : registered for a beginner' s golf lesson ;</dt>
<dd>
<p>Gerald, Robert, and Shirley; : accomplished golfer ;
   ;   ;    ;
Extending the Implicit Logical Expressions port the recall of logical symbol and logical expression identification as 65.9% and 48.9%, respectively. We can see that our generic logic parsing method which operates in an unsupervised manner achieves relatively reliable performance. Unsupervised and generic logic parsing is an essential future direction that is expected to be further studied to enhance the performance of the overall system. Figure 4 to show the reasoning process of our system. At first, the logical symbols are correctly extracted from the context and the logical expressions are identified based on them considering logical negative and conditional relationships. Then we extend the logical expressions by inferring implicit ones in the context. For each option, we recognize its logical expression and find the related extended expressions. We verbalize them into the text to feed into the pre-trained model as an extended context to compute a matching score. Finally, we take option C which exactly matches an extended implicit logical expression as the most plausible answer.
(    )  (     ) ; (   )  (    ) ; (    )  (    ) ; (    )  (   )  (   ) ; (     )  (    )  (    ) ; (    )  (    )  (    ) ; (    )  (     )  (    ) ; (   )  (    )  (   ) ; (    )  (    )  (   ) ;</p>
</dd>
</dl>
<p>Implicit Logical Expressions related to each option
A. (   ) ; (   ) ; B. (    ) ; C. (   ) ; (   ) ; D. (    ) ;</p>
<p>Case Study A ReClor case is presented in</p>
<p>Detailed Analysis of Different Reasoning Types</p>
<p>As ReClor integrates various types of logical reasoning skills, we can detailedly investigate the performance of our system LReasoner ALBERT on different logical reasoning types compared to the baseline model ALBERT. We analyze the improvements brought by our system, and point out challenges to shed a light on future directions. As shown in  Strongly Supported. These questions emphasize the ability of inference over logical units. Specifically, Implication needs to infer the conclusion that logically follows a set of premises while Most Strongly Supported aims to find the statement that is most strongly supported by a stimulus. This observation verifies the effectiveness of our system to model logical deduction. Besides, Implication is precisely the reasoning ability investigated by NLI tasks, which reveals that our model would also be effective in NLI. However, there still exists some reasoning types that are challenging for our system, such as Match flaws and Weaken. Weaken aims to find the opposite statement that weakens the argument.</p>
<p>Match flaws is even more challenging as it requires analyzing the flaw that conflicts with the complete logical chain in the context, and finding an option exhibiting the same flaw. Therefore, how to model the different degrees of a logical statement, and abstract the complete logical chain for flaw identification, are interesting future directions.</p>
<p>Generalizability Discussion</p>
<p>Our logic-driven reasoner not only embodies its superiority in ReClor and LogiQA, but also can be generalized to other datasets and task formats. To demonstrate this, we evaluate our framework on a widely studied extractive QA task SQuAD (Rajpurkar et al., 2016), which covers diverse skills instead of just explicit logical reasoning, such as reasoning of lexical variation, commonsense and causal relations (Sugawara and Aizawa, 2016). As shown in Table 6, our framework is effective on SQuAD compared to both RoBERTa-base and RoBERTa-large, which manifests the generalizability of our logic-driven reasoner.</p>
<p>Model EM F1</p>
<p>RoBERTa-base * 83.0 90.4 LReasoner RoBERTa-base 85.6 91.7</p>
<p>RoBERTa-large * 88.9 94.6 LReasoner RoBERTa-large 89.3 94.8 Table 6: Dev. set results of our framework compared to RoBERTa (both base and large models) on SQuAD. * denotes the results come from .</p>
<p>Related Work</p>
<p>In recent years, there has been a surge in NLP research towards complex reasoning, such as reasoning for commonsense knowledge (Huang et al., 2019), numerical calculation (Dua et al., 2019) or multi-hop aggregation (Yang et al., 2018). Compare to these widely studied reasoning tasks, logical reasoning is also an essential and challenging capability but is relatively unexplored. Natural Language Inference (NLI) (Dagan et al., 2005;Bowman et al., 2015;Williams et al., 2018;Khot et al., 2018) is a typical task requiring logical reasoning, which aims to determine whether a hypothesis can be reasonably entailed from a premise. However, these NLI datasets mainly handle the task at sentence-level and are limited to only a few logical reasoning types, such as entailment, contradiction, and neutral. To promote a deeper passage-level logical reasoning ability, several QA datasets have been proposed. LogiQA (Liu et al., 2020) is collected from the National Civil Servants Examination of China covering 5 logical reasoning types. Yu et al. (2020) propose ReClor dataset from the GMAT and LSAT tests which examines 17 types of logical reasoning. In this paper, we take both ReClor and LogiQA as the testbed to investigate diverse and complicated logical reasoning skills.</p>
<p>Pre-trained language models (Devlin et al., 2019;Yang et al., 2019;Lan et al., 2020) have been widely adopted for various reasoning tasks and achieve promising performance. However, they directly encode the given texts to predict the output while failing to identify the symbolic logical structure and perform explicit logical inference for logical reasoning of text. Semantic parsers (Reddy et al., 2016;Singh et al., 2020) are usually employed for converting texts to logical forms, and graph neural networks (Fang et al., 2019;Huang et al., 2021) and neural module networks (Gupta et al., 2019) also have been attempted to partly imitate the human reasoning process. But these neural methods may not be easily generalized to our desired propositional logical schema without annotations and still perform an implicit inference. To circumvent these limitations and utilize the superior performance of neural models, we take inspiration from neuro-symbolic reasoning (Wang et al., 2018;Arabshahi et al., 2020) to integrate symbolic inference and neural representation. We design an explicit three-step logical reasoning paradigm and propose a logic-driven reasoning system to generically identify the logical structure and perform interpretable logical inference in a symbolic module while taking a pre-trained model as the backbone.</p>
<p>Conclusion and Future Work</p>
<p>In this paper, we focus on the task of logical reasoning of text. Following a three-step logical reasoning paradigm, we first propose a neuro-symbolic logicdriven context extension framework. It identifies logical expressions as elementary units of logical inference and symbolically deduces the implicitly mentioned expressions, and verbalizes them as an extended context into a pre-trained model to match the answer. We also introduce a logic-driven data augmentation algorithm, which augments literally similar but logically different instances and employs contrastive learning to help our model better capture logical information. Experimental results confirm the general effectiveness of our LReasoner, and it even surpasses human performance on the ReClor dataset. In the future, we will explore to model different logical reasoning types and directly incorporate symbolic logic into the model structure.</p>
<p>Algorithm 1 Logic Identification Algorithm</p>
<p>Input: A sentence in the context or an option t to be parsed, a set of logical negative keywords N and a set of logical conditional indicators C. Output: A logical expressions set S parsed from the input t. if  n i  N is in or immediately before the logical symbol a then 6:</p>
<p>Adding the negation connective  before a as a.</p>
<p>7:</p>
<p>Replacing the original symbol with the negative one as a := a. if a  = b and (  c i  C is between two logical symbols a and b or an active voice occurs between a and b ) then 13:</p>
<p>Obtaining a logical expression a  b.</p>
<p>14:</p>
<p>Appending a  b to the logical expression set S. </p>
<p>B Implementation Details</p>
<p>We take RoBERTa-large , ALBERT-xxlarge-v2 (Lan et al., 2020) and DeBERTa-xlarge (He et al., 2020b) as our backbones and implement them using Huggingface (Wolf et al., 2019). We use a batch size of 8 and fine-tune for 10 epochs. The AdamW (Loshchilov and Hutter, 2017) with 1 = 0.9 and 2 = 0.98 is taken as the optimizer and the learning rate is 1e-5. We use a linear learning rate scheduler with 10% warmup proportion. We automatically evaluate our model on validation set to choose parameters that achieve the highest accuracy. We select at most two extended logical expressions related to each option to construct the extended context for ReClor and select at most three for LogiQA. We train our proposed systems and other comparison models on two NVIDIA Tesla V100 GPUs.</p>
<p>( 3 )
3{(  ), ...}: the logical expressions, which are composed of logical symbols and connectives. (  ) means that  is the condition of .</p>
<p>Figure 4 :
4A ReClor case of the reasoning process of LReasoner ALBERT . Phrases underlined denote other symbols (called Others) different from the logical symbols in context and bold tokens make them different.</p>
<p>1 :
1Initializing S := {} 2: Extracting constituents from the input t. 3: Recognizing literally similar constituents as the same symbol and obtain all logical symbols as {, , ...}. 4: for symbol a in {, , ...} do 5:</p>
<p>10: for symbol a in {, , ...} do 11: for symbol b in {, , ...} do 12:</p>
<p>17: end for 18: return The logical expressions set S.</p>
<p>Table 1 :
1An example of verbalizing a logical expression into text.</p>
<p>Table 2 .
2We </p>
<p>Table 2 :
2Experimental results (accuracy %) of different models on ReClor and LogiQA. The results in bold are the best performance of each column except for LReasoner Ensemble and Human Performance. * indicates that the results of ReClor and LogicQA are taken from</p>
<p>Table 5 ,
5our model is generally effective on most reasoning types compared to the baseline model, especially Implication, MostReasoning Type 
Base 
Ours </p>
<p>Necessary Assumptions (11.0%) 73.7 76.3 () 
Sufficient Assumptions (3.6%) 
70.0 70.0 () 
Strengthen (9.0%) 
69.1 70.2 () 
Weaken (10.6%) 
64.6 59.3 () 
Evaluation (1.6%) 
69.2 69.2 () 
Implication (6.2%) 
43.8 54.3 () 
Conclusion/Main Point (3.1%) 
80.6 77.8 () 
Most Strongly Supported (6.7%) 58.9 71.4 () 
Explain or Resolve (8.0%) 
60.7 67.9 () 
Principle (5.7%) 
72.3 76.9 () 
Dispute (2.5%) 
63.3 80.0 () 
Technique (3.8%) 
75.0 80.6 () 
Role (3.7%) 
78.1 68.8 () 
Identify a Flaw (11.3%) 
65.0 71.8 () 
Match Flaws (4.9%) 
61.3 61.3 () 
Match the Structure (2.7%) 
56.7 86.7 () 
Others (5.5%) 
68.5 72.6 () </p>
<p>Table 5 :
5Results of different reasoning types. Numbers in parentheses are percentages of different types. Base is the ALBERT while Ours means our LReasoner ALBERT . ,  and  respectively mean that our performance is better, worse than and equal to the baseline ALBERT.
AcknowledgmentsThis work is partially supported by Natural Science Foundation of China (No.6217020551, 61906176), Science and Technology Commission of Shanghai Municipality Grant (No.20dz1200600, 21QA1400600, GWV-1.1, 21511101000) and Zhejiang Lab (No. 2019KD0AD01).A Details of Logic IdentificationWe design a generic logic identification approach that uses an off-the-shelf constituency parser and most common keywords of logical semantics (totally no more than 20). We employ the constituency parser to extract constituents as basic symbols. We regard literally similar constituents with an overlap rate over 60% as the same symbol if they also have consistent degree modifiers, such as "only", "most", "least", etc.We define a set of negative words for identifying logical negation, including {"not", "n't", "unable", "no", "few", "little", "neither", "none of "}. And the full set of conditional indicators for recognizing the logical conditional relationship between  and  as (  ) is {"if , then ", " in order for ", " thus ", " due to ", " owing to ", " since ", " unless "}. The detailed parsing procedure is illustrated in Algorithm 1.
Forough Arabshahi, Jennifer Lee, Mikayla Gawarecki, Kathryn Mazaitis, Amos Azaria, Tom Mitchell, arXiv:2006.10022Conversational neuro-symbolic commonsense reasoning. arXiv preprintForough Arabshahi, Jennifer Lee, Mikayla Gawarecki, Kathryn Mazaitis, Amos Azaria, and Tom Mitchell. 2020. Conversational neuro-symbolic commonsense reasoning. arXiv preprint arXiv:2006.10022.</p>
<p>Artur D&apos;avila Tarek R Besold, Sebastian Garcez, Howard Bader, Pedro Bowman, Pascal Domingos, Kai-Uwe Hitzler, Khnberger, C Luis, Daniel Lamb, Priscila Machado Vieira Lowd, Lima, arXiv:1711.03902Neuralsymbolic learning and reasoning: A survey and interpretation. arXiv preprintTarek R Besold, Artur d'Avila Garcez, Sebastian Bader, Howard Bowman, Pedro Domingos, Pascal Hitzler, Kai-Uwe Khnberger, Luis C Lamb, Daniel Lowd, Priscila Machado Vieira Lima, et al. 2017. Neural- symbolic learning and reasoning: A survey and inter- pretation. arXiv preprint arXiv:1711.03902.</p>
<p>A large annotated corpus for learning natural language inference. R Samuel, Gabor Bowman, Christopher Angeli, Christopher D Potts, Manning, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational LinguisticsSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large anno- tated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empiri- cal Methods in Natural Language Processing, pages 632-642, Lisbon, Portugal. Association for Compu- tational Linguistics.</p>
<p>A simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, PMLRIn International conference on machine learning. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In In- ternational conference on machine learning, pages 1597-1607. PMLR.</p>
<p>The pascal recognising textual entailment challenge. Oren Ido Dagan, Bernardo Glickman, Magnini, Machine Learning Challenges Workshop. SpringerIdo Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment chal- lenge. In Machine Learning Challenges Workshop, pages 177-190. Springer.</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner, arXiv:1903.00161Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprintDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. Drop: A reading comprehension benchmark re- quiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161.</p>
<p>Hierarchical graph network for multi-hop question answering. Yuwei Fang, Siqi Sun, Zhe Gan, Rohit Pillai, Shuohang Wang, Jingjing Liu, arXiv:1911.03631arXiv preprintYuwei Fang, Siqi Sun, Zhe Gan, Rohit Pillai, Shuohang Wang, and Jingjing Liu. 2019. Hierarchical graph network for multi-hop question answering. arXiv preprint arXiv:1911.03631.</p>
<p>Neural-symbolic computing: An effective methodology for principled integration of machine learning and reasoning. Artur D&apos;avila Garcez, Marco Gori, C Luis, Luciano Lamb, Michael Serafini, Son N Spranger, Tran, arXiv:1905.06088arXiv preprintArtur d'Avila Garcez, Marco Gori, Luis C Lamb, Luciano Serafini, Michael Spranger, and Son N Tran. 2019. Neural-symbolic computing: An ef- fective methodology for principled integration of machine learning and reasoning. arXiv preprint arXiv:1905.06088.</p>
<p>Nitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, Matt Gardner, arXiv:1912.04971Neural module networks for reasoning over text. arXiv preprintNitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, and Matt Gardner. 2019. Neural module networks for rea- soning over text. arXiv preprint arXiv:1912.04971.</p>
<p>The argument reasoning comprehension task: Identification and reconstruction of implicit warrants. Ivan Habernal, Henning Wachsmuth, Iryna Gurevych, Benno Stein, arXiv:1708.01425arXiv preprintIvan Habernal, Henning Wachsmuth, Iryna Gurevych, and Benno Stein. 2017. The argument reason- ing comprehension task: Identification and recon- struction of implicit warrants. arXiv preprint arXiv:1708.01425.</p>
<p>Momentum contrast for unsupervised visual representation learning. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020a. Momentum contrast for un- supervised visual representation learning. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9729-9738.</p>
<p>Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, arXiv:2006.03654Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprintPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020b. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654.</p>
<p>Cosmos qa: Machine reading comprehension with contextual commonsense reasoning. Lifu Huang, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, arXiv:1909.00277arXiv preprintLifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019. Cosmos qa: Machine reading com- prehension with contextual commonsense reasoning. arXiv preprint arXiv:1909.00277.</p>
<p>Yinya Huang, Meng Fang, Yu Cao, Liwei Wang, Xiaodan Liang, arXiv:2103.14349Dagn: Discourse-aware graph network for logical reasoning. arXiv preprintYinya Huang, Meng Fang, Yu Cao, Liwei Wang, and Xiaodan Liang. 2021. Dagn: Discourse-aware graph network for logical reasoning. arXiv preprint arXiv:2103.14349.</p>
<p>Extending a parser to distant domains using a few dozen partially annotated examples. Vidur Joshi, Matthew Peters, Mark Hopkins, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics1Vidur Joshi, Matthew Peters, and Mark Hopkins. 2018. Extending a parser to distant domains using a few dozen partially annotated examples. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1190-1199, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Scitail: A textual entailment dataset from science question answering. Tushar Khot, Ashish Sabharwal, Peter Clark, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence32Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018. Scitail: A textual entailment dataset from science question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.</p>
<p>Albert: A lite bert for self-supervised learning of language representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, International Conference on Learning Representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. Albert: A lite bert for self-supervised learning of language representations. In International Confer- ence on Learning Representations.</p>
<p>Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence. the Twenty-Ninth International Joint Conference on Artificial IntelligenceJian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2020. Logiqa: A chal- lenge dataset for machine reading comprehension with logical reasoning. In Proceedings of the Twenty- Ninth International Joint Conference on Artificial Intelligence.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, abs/1907.11692Roberta: A robustly optimized bert pretraining approach. CoRRYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap- proach. CoRR, abs/1907.11692.</p>
<p>Fixing weight decay regularization in adam. I Loshchilov, F Hutter, abs/1711.05101ArXiv. I. Loshchilov and F. Hutter. 2017. Fixing weight decay regularization in adam. ArXiv, abs/1711.05101.</p>
<p>Artificial intelligence, logic and formalizing common sense. John Mccarthy, Philosophical logic and artificial intelligence. SpringerJohn McCarthy. 1989. Artificial intelligence, logic and formalizing common sense. In Philosophical logic and artificial intelligence, pages 161-190. Springer.</p>
<p>Logic and artificial intelligence. J Nils, Nilsson, Artificial intelligence. 471-3Nils J Nilsson. 1991. Logic and artificial intelligence. Artificial intelligence, 47(1-3):31-56.</p>
<p>SQuAD: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational LinguisticsPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natu- ral Language Processing, Austin, Texas. Association for Computational Linguistics.</p>
<p>Transforming dependency structures to logical forms for semantic parsing. Siva Reddy, Oscar Tckstrm, Michael Collins, Tom Kwiatkowski, Dipanjan Das, Mark Steedman, Mirella Lapata, Transactions of the Association for Computational Linguistics. 4Siva Reddy, Oscar Tckstrm, Michael Collins, Tom Kwiatkowski, Dipanjan Das, Mark Steedman, and Mirella Lapata. 2016. Transforming dependency structures to logical forms for semantic parsing. Transactions of the Association for Computational Linguistics, 4:127-140.</p>
<p>Artificial intelligence: a modern approach. Stuart Russel, Peter Norvig, Pearson Education LimitedStuart Russel, Peter Norvig, et al. 2013. Artificial in- telligence: a modern approach. Pearson Education Limited.</p>
<p>Exploring neural models for parsing natural language into first-order logic. Hrituraj Singh, Milan Aggrawal, Balaji Krishnamurthy, arXiv:2002.06544arXiv preprintHrituraj Singh, Milan Aggrawal, and Balaji Krishna- murthy. 2020. Exploring neural models for parsing natural language into first-order logic. arXiv preprint arXiv:2002.06544.</p>
<p>An analysis of prerequisite skills for reading comprehension. Saku Sugawara, Akiko Aizawa, 10.18653/v1/W16-6001Proceedings of the Workshop on Uphill Battles in Language Processing: Scaling Early Achievements to Robust Methods. the Workshop on Uphill Battles in Language Processing: Scaling Early Achievements to Robust MethodsAustin, TXAssociation for Computational LinguisticsSaku Sugawara and Akiko Aizawa. 2016. An analysis of prerequisite skills for reading comprehension. In Proceedings of the Workshop on Uphill Battles in Language Processing: Scaling Early Achievements to Robust Methods, pages 1-5, Austin, TX. Association for Computational Linguistics.</p>
<p>Translating a math word problem to an expression tree. Lei Wang, Yan Wang, Deng Cai, Dongxiang Zhang, Xiaojiang Liu, arXiv:1811.05632arXiv preprintLei Wang, Yan Wang, Deng Cai, Dongxiang Zhang, and Xiaojiang Liu. 2018. Translating a math word problem to an expression tree. arXiv preprint arXiv:1811.05632.</p>
<p>A broad-coverage challenge corpus for sentence understanding through inference. Adina Williams, Nikita Nangia, Samuel Bowman, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaLong Papers1Association for Computational LinguisticsAdina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sen- tence understanding through inference. In Proceed- ings of the 2018 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122, New Orleans, Louisiana. Association for Computational Linguis- tics.</p>
<p>A broad-coverage challenge corpus for sentence understanding through inference. Adina Williams, Nikita Nangia, Samuel R Bowman, arXiv:1704.05426arXiv preprintAdina Williams, Nikita Nangia, and Samuel R Bow- man. 2017. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426.</p>
<p>Huggingface's transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rmi Louf, Morgan Funtowicz, abs/1910.03771ArXiv. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, Rmi Louf, Morgan Fun- towicz, et al. 2019. Huggingface's transformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771.</p>
<p>Unsupervised feature learning via nonparametric instance discrimination. Zhirong Wu, Yuanjun Xiong, X Stella, Dahua Yu, Lin, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. 2018. Unsupervised feature learning via non- parametric instance discrimination. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3733-3742.</p>
<p>Xlnet: Generalized autoregressive pretraining for language understanding. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, R Russ, Quoc V Salakhutdinov, Le, Advances in neural information processing systems. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car- bonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for lan- guage understanding. In Advances in neural informa- tion processing systems, pages 5753-5763.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, W William, Ruslan Cohen, Christopher D Salakhutdinov, Manning, arXiv:1809.09600Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprintZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben- gio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answer- ing. arXiv preprint arXiv:1809.09600.</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, International Conference on Learning Representations. ICLRWeihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. 2020. Reclor: A reading comprehension dataset re- quiring logical reasoning. In International Confer- ence on Learning Representations (ICLR).</p>
<p>Static logic implication with application to redundancy identification. J-K Zhao, Elizabeth M Rudnick, Janak H Patel, Proceedings. 15th IEEE VLSI Test Symposium (Cat. No. 97TB100125). 15th IEEE VLSI Test Symposium (Cat. No. 97TB100125)IEEEJ-K Zhao, Elizabeth M Rudnick, and Janak H Patel. 1997. Static logic implication with application to re- dundancy identification. In Proceedings. 15th IEEE VLSI Test Symposium (Cat. No. 97TB100125), pages 288-293. IEEE.</p>            </div>
        </div>

    </div>
</body>
</html>
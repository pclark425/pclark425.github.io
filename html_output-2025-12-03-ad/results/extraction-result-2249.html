<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2249 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2249</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2249</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-62.html">extraction-schema-62</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <p><strong>Paper ID:</strong> paper-279318556</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.10085v2.pdf" target="_blank">VITA: Z ERO -S HOT V ALUE F UNCTIONS VIA T EST -T IME A DAPTATION OF V ISION –L ANGUAGE M ODELS</a></p>
                <p><strong>Paper Abstract:</strong> Vision–Language Models (VLMs) show promise as zero-shot goal-conditioned value functions, but their frozen pre-trained representations limit generalization and temporal reasoning. We introduce VITA, a zero-shot value function learning method that enhances both capabilities via test-time adaptation. At inference, a lightweight adaptation module is updated via a gradient step on a meta-learned self-supervised loss, such that each test-time update improves value estimation. By updating sequentially over a trajectory, VITA encodes history into its parameters, addressing the temporal reasoning limitations. To mitigate shortcut learning, we propose a dissimilarity-based sampling strategy that selects semantically diverse segments of the trajectory during training. In real-world robotic manipulation tasks, VITA generalizes from a single training environment to diverse out-of-distribution tasks, environments, and embodiments, outperforming the state-of-the-art zero-shot method using autoregressive VLMs. Furthermore, we demonstrate that VITA’s zero-shot value estimates can be utilized for reward shaping in offline reinforcement learning, resulting in multi-task policies on the Meta-World benchmark that exceed the performance of those trained with the simulation’s fuzzy-logic dense rewards.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2249.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2249.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VITA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VITA: Zero-shot Value Functions via Test-Time Adaptation of Vision-Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot goal-conditioned value function estimator that adapts a frozen contrastive VLM (CLIP) at test time via a light-weight adaptation module updated with a single gradient step on a meta-learned self-supervised reconstruction loss, encoding temporal context through sequential parameter updates and using dissimilarity-based sampling during training to mitigate shortcuts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VITA (test-time adapted CLIP value estimator)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>value-based predictions only (semantic multimodal embeddings from CLIP, adapted into a low-dim adaptation space d' = 64)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>meta-learned linear projections (P_K, P_V, P_Q) with a self-supervised reconstruction loss and dissimilarity-based sampling to encourage semantic feature reliance (contrastive pretraining provides initial feature alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>real-world robotic manipulation (BridgeData V2) and zero-shot transfer to simulated multi-task RL (Meta-World MT10)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>realistic scene/background variation across different environments; tasks include environment shifts (different surfaces, backgrounds) and embodiment shifts (different robot DeepThought vs training WidowX 250); long-horizon tasks introduce temporal distractors</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Value Order Correlation (VOC) per dataset (examples from Table 1): tk_pnp VOC=0.782, lm_pnp=0.725, td_fold=0.709, ft_fold=0.658, rd_fold=0.606, ms_sweep=0.490, dt_tk_pnp=0.820, dt_tk_stack=0.708, dt_ft_stack=0.698, dt_rd_pnp=0.695; Offline RL on MT10 IQM=0.815 (95% CI [0.785,0.838]); BinVOC discriminative score=1.00 (perfect discrimination on tested scripted datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Adaptation module: two-layer residual MLP (projection dim d' = 64). Training: AdamW lr=1e-4, weight decay=1e-4, batch size=32, 5 epochs, pad sequences to 120 frames, trained on NVIDIA RTX 6000 Ada GPUs. Test-time adaptation: single gradient step (t_ep = 1) with learning rate η=0.1 (inference adaptation not reset by default). Dissimilarity sampling complexity: O(N_c^2 * w_tr * d) for pairwise distances; example feature dim d=1024, w_tr=8, batch size 32; no total parameter count or FLOPs reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Consistently outperforms CLIP-based baselines and autoregressive VLM-based GVL in VOC and transfer: e.g., in-distribution tk_pnp VOC: VITA 0.782 vs GVL-0S 0.269, GVL-1S 0.252, CLIP-FT 0.251; Meta-World MT10 IQM: VITA 0.815 vs CLIP-FT 0.785, VLM-CL 0.760, VLM-RM 0.746, META-WL (fuzzy-logic dense reward) 0.779. VITA also outperforms full-trajectory and random sub-trajectory sampling strategies in discriminative tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Trained on a single real-world training environment (BridgeData V2 subset) and transfers zero-shot to multiple OOD settings: environment shifts (lm_pnp, td_fold, ft_fold, rd_fold), embodiment shifts (DeepThought robot datasets), and to simulated Meta-World MT10 for zero-shot reward shaping; quantitative VOC scores above show substantial transfer (e.g., dt_tk_pnp VOC=0.820 despite embodiment shift).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Used as zero-shot dense reward for offline RL across Meta-World MT10; multi-task policy IQM=0.815 (95% CI [0.785,0.838]) averaged across tasks/seeds, exceeding the simulator fuzzy reward baseline (META-WL IQM=0.779). Policies trained with IQL for 100k gradient steps per seed using VITA-derived rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Performance degrades on very long-horizon/high-variability tasks (ms_sweep VOC=0.490 lower than many folding tasks). Test-time adaptation at every timestep may be unsafe or impractical in some real-time deployments. Possible sensitivity to high execution variability and extended durations noted in limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Dissimilarity-based sampling vs full-trajectory vs random sampling: dissimilarity sampling (w_tr=8, k=8) yields perfect discriminative performance (4/4 scripted datasets), full-trajectory sampling performs worst (overfits temporal shortcuts), random sampling intermediate. Temporal adaptation ablation: step-by-step incremental adaptation (VITA) outperforms trajectory-level (single update per trajectory) and no-temporal reset-at-each-step variants; exact numeric deltas not listed, but VITA showed consistent superiority across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Trained with relatively small dataset and few epochs: uses expert visual trajectories (200 per evaluation dataset, except ms_sweep 100), model trained for 5 epochs; test-time adaptation uses one gradient step per timestep. No explicit samples-to-performance curves or sample count to reach thresholds provided.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Evaluated across distribution shifts: environment, embodiment, and combined; VOC and BinVOC metrics show VITA generalizes robustly (VOC values listed above); outperforms CLIP-FT and VLM/GVL baselines especially on pick-and-place and stacking tasks where GVL/autoregressive methods fail to generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Self-supervised adaptation uses L2 reconstruction loss between f_adapt(P_K z_t) and P_V z_t, but no numerical reconstruction metrics (MSE, PSNR, SSIM) reported for reconstruction quality.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Introduces dissimilarity-based sub-trajectory sampling to mitigate shortcut learning and encourage reliance on semantic/temporal cues; empirical evidence shows improved discrimination between expert and non-expert trajectories. No fine-grained feature attribution or attention maps reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Not explicitly supported as multi-level abstraction; adaptation changes internal parameters online to encode temporal context but does not switch between distinct abstraction levels dynamically.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not explicitly discussed or evaluated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>No mutual information, compression, or rate-distortion analyses reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Paper does not analyze pixel-level reconstruction benefits; uses CLIP semantic embeddings rather than pixel-reconstruction, and does not present scenarios where pixel fidelity is beneficial.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2249.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2249.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP (Contrastive Language–Image Pre-training)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contrastive vision-language model that maps images and text to a shared semantic embedding space, used here as a frozen multimodal encoder to produce joint visual-language representations for value prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning transferable visual models from natural language supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP (contrastive VLM)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>semantic/object-level multimodal embeddings (pretrained contrastive features, image-to-text alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>contrastive pretraining aligns semantically relevant features; no explicit task-specific feature selection in frozen CLIP beyond learned projections P_Q/linear heads</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>used as encoder for robotic manipulation visual trajectories; baseline VLM-CL and CLIP-FT use CLIP representations directly</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>real-world backgrounds and environment variation in datasets; CLIP features are susceptible to lack of temporal modeling (no explicit temporal disambiguation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>As baseline VLM-CL and CLIP-FT: e.g., CLIP-FT VOC on tk_pnp=0.251; VLM-CL IQM on MT10=0.760 (95% CI [0.722, 0.791])</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Frozen encoder used; projection dims and training configs: CLIP-based multimodal representation dim d ≈ 1024 used in paper; CLIP-FT used an 8× larger projection and 10× more training steps than VITA's supervised head (exact params not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>CLIP-based zero-shot similarity methods (VLM-CL, VLM-RM) underperform relative to VITA and CLIP-FT on temporally-dependent tasks; CLIP-FT (supervised on CLIP features) performs comparably in-distribution but fails to generalize to OOD.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Frozen CLIP features alone insufficient for strong temporal generalization; CLIP-FT fine-tuned regressor failed on many OOD datasets despite extra training steps.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Used as backbone for multi-task offline reward shaping experiments via VITA; CLIP-FT achieved MT10 IQM=0.785 but lower than VITA.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Lacks temporal modeling → fails to disambiguate visually similar but temporally different states; vulnerable to shortcut learning when consecutive frames redundant.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Paper compares frozen CLIP baselines (VLM-CL, VLM-RM, CLIP-FT) versus test-time adaptation; CLIP-FT trained with larger projection but still failed to generalize in many OOD cases.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not explicitly quantified; CLIP-FT was trained with 10× more steps than VITA's supervised head but still underperformed on OOD.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>CLIP features generalize semantically but not temporally; need adaptation or temporal modeling to improve VOC on long-horizon or ambiguous states.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Paper argues contrastive alignment yields semantic proximity to goals, but CLIP lacks task-temporal disambiguation; no explicit per-feature task relevance analysis provided.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>No</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2249.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2249.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GVL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GVL (Vision-Language value learner using autoregressive VLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline method that uses autoregressive vision-language models (e.g., Gemini 1.5 Pro) to estimate goal-conditioned value by in-context conditioning on visual trajectories and task descriptions; can be used zero-shot or one-shot (in-context examples).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Vision language models are in-context value learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GVL (autoregressive VLM in-context value estimator)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>sequence-level autoregressive conditioning over trajectories (temporal/contextual embeddings), but outputs scalar value predictions (semantic-temporal)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>in-context conditioning; relies on internal attention and learned sequence-level representations; temporal order preserved or frames shuffled to mitigate monotonic bias</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>robotic manipulation value estimation and zero-shot evaluation on BridgeData V2 datasets</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>evaluated on same environment/embodiment shifts and long-horizon tasks with background/environment variation; autoregressive pretraining bias toward monotonic progress noted</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GVL-0S and GVL-1S VOC examples: tk_pnp 0.269 (GVL-0S), lm_pnp 0.305, td_fold 0.326, ft_fold 0.331, rd_fold 0.372; underperformed relative to VITA on stacking and pick-and-place tasks (examples: dt_tk_pnp VOC=0.258 GVL-0S vs VITA 0.820).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Uses large autoregressive VLM (Gemini 1.5 Pro); noted to have prohibitively high inference cost in RL settings; exact FLOPs/params not provided in this paper but described as expensive and proprietary.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Performs well on folding tasks but poorly on stacking/pick-and-place and under embodiment shifts; VITA consistently outperforms GVL in VOC and transfer experiments (large deltas shown in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>In-context one-shot (GVL-1S) sometimes helps but is unreliable and fails to generalize under embodiment shifts; shuffling frames to counter monotonic bias discards temporal order, harming temporal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Not evaluated for downstream offline RL MT10 in this paper due to prohibitive inference cost at scale; no multi-task IQM reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Biased by chronological order in pretraining leading to monotonic increasing prediction bias; ineffective under embodiment shifts and for some task types (stacking, pick-and-place).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Paper compares GVL-0S vs GVL-1S (in-context examples) showing mixed effects; no internal GVL ablations performed here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Fails to generalize across embodiment shifts in evaluated settings; GVL-1S performed poorly on dt_tk_pnp and dt_tk_stack compared to VITA.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>No explicit task vs irrelevant feature analysis for GVL in this paper beyond noting temporal order bias and reliance on in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>No</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2249.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2249.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Test-Time Training (TTT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Test-time Training with Self-Supervision for Sequence Modeling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A paradigm where an adaptation module is updated at test time with a self-supervised loss per instance (or per timestep for sequences), enabling implicit memory via adapted parameters and improving robustness/generalization under distribution shift; here meta-learned for value estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Test-time training with self-supervision for generalization under distribution shifts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Test-time training (sequence variant, meta-learned self-supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>parameter-level adaptation that results in sequence-aware representations (not explicit multi-level abstraction)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>self-supervised reconstruction loss with meta-learned projection matrices (P_K, P_V) that shape which features the adaptation module emphasizes</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>applied to goal-conditioned value estimation on visual trajectories for robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>used to adapt representations in presence of environment/embodiment distractors; sequential updates encode temporal context to disambiguate visually similar states</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Improves VOC and BinVOC when used as in VITA versus no test-time adaptation; exact numeric contribution implicit in VITA vs ablated variants (VITA outperforms reset-per-step and trajectory-level variants across datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Parameter-efficient: only the adaptation module (two-layer MLP, d'=64) is adapted at test time with 1 gradient step (t_ep = 1) and learning rate 0.1; adaptation overhead described as negligible compared to forward/backward passes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Outperforms no-adaptation and trajectory-level single-update adaptation in encoding temporal context for value estimation (VITA > TTT-OF and TTT-VIS in ablations).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Enables zero-shot transfer to OOD tasks by rapidly adapting representations to trajectory-specific temporal and semantic context.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Used within VITA to produce dense reward shaping for multi-task RL with improved performance (MT10 IQM=0.815).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>May pose safety/real-time concerns if updating in every timestep; can struggle in settings with extreme execution variability or very long horizons if only single-step adaptation used.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Paper performs ablation comparing incremental per-step adaptation (implicit memory) vs one-update-per-trajectory vs reset-per-step (no temporal memory); incremental approach performs best.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Single gradient step per timestep (low adaptation sample requirement); training uses meta-learning to make that single step effective; no explicit sample-efficiency curves.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Meta-learned TTT objective improves zero-shot generalization to OOD trajectories and encodes temporal context to distinguish progress stages.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Self-supervised loss is L2 between adapted projection and target projection; no standalone reconstruction metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Meta-learning optimizes the self-supervised task to minimize downstream supervised value loss after adaptation, aligning adaptation with task-relevant features; empirical improvements reported but no per-feature attribution.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Adaptation changes representation space online but not explicitly multi-level abstraction switching.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>No</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2249.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2249.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dissimilarity-based sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dissimilarity-based sub-trajectory sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training-time heuristic that selects semantically diverse sub-trajectories by scoring windows by their total pairwise dissimilarity and selecting the top-k, intended to reduce shortcut learning from redundant consecutive frames and encourage semantic/temporal diversity for meta-learned test-time adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dissimilarity-based sampling (diverse window selection)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>semantic/local-temporal sub-trajectory selection (operates on CLIP multimodal embeddings, not pixel-level)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>heuristic selection via pairwise L2 distances between flattened window feature vectors (sum of distances as score)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>training value estimators on video trajectories for robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>helps emphasize underrepresented semantic segments across varied environments and backgrounds by avoiding redundant frames</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In discriminative ablation (Disc@VOC across 4 scripted datasets), dissimilarity-based sampling with w_tr=8 and k=8 achieved perfect discriminative performance (4/4), while full-trajectory sampling performed worst and random sampling intermediate.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Computes pairwise distance matrix D ∈ R^{N_c×N_c} for candidate windows; complexity O(N_c^2 * w_tr * d). Example dims: d=1024, w_tr=8. Overhead described as negligible relative to model forward/backward costs on GPU.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Outperforms full-trajectory sampling (which overfits to temporal shortcuts) and random sampling in enabling discriminative test-time adaptation and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Training with dissimilarity sampling yields better OOD generalization in value estimation and downstream reward shaping (empirically improves VOC and discriminative metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>No direct multi-task metric isolated for sampling method alone, but overall VITA (which uses this sampling) yields strong MT10 IQM=0.815.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Heuristic selection depends on quality of representation space; computing full pairwise distances could be heavy for very long sequences but authors note trajectories are short in their datasets (mean 69, max 120).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Ablation in Section F.1 compares full-trajectory, random, and dissimilarity-based sampling; dissimilarity sampling yields best discriminative results and improved generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>By focusing on diverse windows, training emphasizes underrepresented segments, but no direct sample-efficiency numbers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Empirical evidence that dissimilarity-based sampling reduces shortcut learning and improves discriminative power and OOD VOC performance.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Argued and empirically supported that sampling increases reliance on semantic cues rather than temporal shortcuts; no per-feature attribution provided.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>No</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2249.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2249.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>f_adapt (adaptation module)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>f_adapt: Lightweight temporal adaptation module</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-layer residual MLP (GELU) with projection dimension d' = 64 that is meta-learned and updated at test time with a single gradient step on a self-supervised reconstruction loss to encode sequence history into parameters for better temporal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>f_adapt (two-layer residual MLP adaptation module)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>low-dimensional adaptation-space transformations (d' = 64) operating on CLIP embeddings — semantic/temporal rather than pixel-level</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>learned via meta-learning of linear projections (P_K, P_V, P_Q) and initialization θ_0 that make single-step gradient updates effective at emphasizing task-relevant features</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>sequence encoding for goal-conditioned value estimation in robotic manipulation trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>intended to adapt representations to trajectory-specific distractors (environment variability, embodiment differences) by adjusting internal parameters online</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Component of VITA which yields the VOC and MT10 IQM numbers reported for full system; ablations show incrementally updated f_adapt (implicit memory) outperforms other adaptation strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Lightweight: two-layer MLP with d'=64; adapted at inference with 1 gradient step (learning rate 0.1) — adaptation overhead negligible compared to full model passes; trained with AdamW.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>When present and meta-learned, it enables VITA to exceed CLIP-FT and autoregressive baselines; when ablated or reset, performance drops (see temporal memory ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Enables transfer to OOD environments and embodiments by online adaptation of embeddings; see VOC and MT10 results.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Contributes to VITA's superior MT10 reward shaping performance (IQM=0.815).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Single-step adaptation can be insufficient in extremely variable or very long tasks; potential deployment safety concerns if updated every timestep.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Temporal memory ablation (reset vs incremental vs trajectory-level) shows incremental parameter updates of f_adapt are most effective.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Designed to be parameter-efficient and effective with one test-time gradient step; no explicit sample counts beyond per-frame adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Implicit memory in f_adapt is key for temporal disambiguation and OOD generalization shown in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Self-supervised L2 loss used to train f_adapt through meta-learning; no explicit reconstruction metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Meta-learning of P_K/P_V shapes which projected features f_adapt reconstructs, aligning adaptation with task-relevant signal; empirical improvements in VOC indicate success.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Adaptation changes parameters but not explicit multi-level abstraction; acts as a lightweight sequence memory.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>No</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2249.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2249.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VLM-CL / VLM-RM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VLM-CL (CLIP cosine-similarity baseline) / VLM-RM (reference prompt modification baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simple CLIP-based zero-shot value estimators: VLM-CL uses cosine similarity between CLIP frame and task text embeddings; VLM-RM regularizes CLIP embeddings by projecting features along direction from a generic reference prompt to the task prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VLM-CL and VLM-RM (CLIP-based zero-shot reward/value baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>frame-level semantic similarity in CLIP embedding space (no temporal modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>contrastive similarity to task prompt (VLM-CL); directional projection relative to a reference prompt (VLM-RM); no learned temporal selection</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>zero-shot goal-conditioned value estimation for robotic manipulation (BridgeData V2)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>perform poorly when temporal context is needed to disambiguate states or when distractors present; no explicit distractor handling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Baselines perform poorly in VOC and offline RL: e.g., VLM-CL MT10 IQM=0.760 [0.722,0.791], VLM-RM IQM=0.746 [0.718,0.771]; VLM-CL / VLM-RM VOC often < 0.1 on many datasets (Table 1 examples: tk_pnp VLM-CL=0.038, VLM-RM=0.029).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Very cheap inference (just CLIP cosine similarity), no adaptation overhead; exact FLOPs/params not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Underperform relative to CLIP-FT and VITA; inability to model temporal context is primary weakness.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Fail to generalize where temporal reasoning or adaptation is required.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Used as baseline for MT10, but yields lower IQM than VITA or CLIP-FT.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>No temporal modeling, sensitive to frame-level ambiguity, fail to discriminate expert vs scripted in BinVOC in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Compared against VITA and CLIP-FT showing relative weaknesses; no internal ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>N/A (zero-shot, no training required).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Poor OOD generalization relative to adaptation-based methods.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>No explicit mechanism to focus on task-relevant temporal cues; thus often relies on frequently occurring visual patterns (shortcuts).</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>No</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2249.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2249.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP-FT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP-FT (Supervised regression on frozen CLIP features)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised baseline that trains a regression head on frozen CLIP multimodal representations to predict normalized temporal progress; uses a larger projection and more training steps than VITA's supervised head.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP-FT (supervised CLIP regressors)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>frame-level semantic embeddings with supervised regression to temporal progress (no temporal modeling in backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>learned linear projection and MLP regression head trained on normalized progress labels (supervised)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>in-distribution goal-conditioned value estimation and OOD evaluation for robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>performs comparably in-distribution but fails to generalize to OOD environment/embodiment shifts and long-horizon tasks where temporal context matters</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>CLIP-FT VOC examples: tk_pnp=0.251; MT10 IQM=0.785 [0.759,0.809]; discriminative BinVOC=0.75 (failed one scripted task).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Trained with an 8× larger projection matrix and 10× more training steps than VITA's supervised head; other hyperparameters not fully detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Performs better than zero-shot CLIP similarity methods but worse than VITA on OOD generalization and some tasks requiring temporal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Limited transferability to embodiment and environment shifts despite heavy supervised training.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Used as baseline for MT10 offline RL with IQM lower than VITA.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Fails on some scripted tasks and OOD scenarios; lacks temporal adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Compared against VITA (which includes adaptation) to show the benefit of test-time adaptation; no internal ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Required more training steps than VITA's supervised head but still underperformed on OOD.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Overfits to in-distribution statistics and fails under distribution shifts addressed by VITA.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>No explicit self-supervised mechanism to emphasize task-relevant temporal features; relies on supervised regression which can overfit to spurious cues.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>No</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2249.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2249.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>World models (general mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>World Models (generative predictive models of environment dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Concept of learning predictive generative models of environments to enable planning and learning, referenced as motivating prior work and a future application area for test-time adaptation methods like VITA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>World models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>World models (general family: generative predictive environment models)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>typically pixel-level or latent generative reconstructions enabling rollout (paper cites both pixel/latent world models in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>varies by method; world-model literature uses latent encoders, reconstruction losses, and learned latents to capture task-relevant dynamics (not used directly in experiments here)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>robotics, autonomous driving, video games (general references in intro/related work)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>world-models research addresses learning under rich visual backgrounds and long-horizon dynamics; not evaluated in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Mentioned as alternative approach for learning from videos and for training agents in simulated environments; paper suggests VITA could be applied within world models in future work.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Not evaluated here; mentioned as possible future direction for applying test-time adaptation within world models.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not discussed in detail in this paper beyond general literature references.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning transferable visual models from natural language supervision <em>(Rating: 2)</em></li>
                <li>Test-time training with self-supervision for generalization under distribution shifts <em>(Rating: 2)</em></li>
                <li>Vision language models are in-context value learners <em>(Rating: 2)</em></li>
                <li>World models <em>(Rating: 1)</em></li>
                <li>Test-time training on video streams <em>(Rating: 1)</em></li>
                <li>Learning to (learn at test time): Rnns with expressive hidden states <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2249",
    "paper_id": "paper-279318556",
    "extraction_schema_id": "extraction-schema-62",
    "extracted_data": [
        {
            "name_short": "VITA",
            "name_full": "VITA: Zero-shot Value Functions via Test-Time Adaptation of Vision-Language Models",
            "brief_description": "A zero-shot goal-conditioned value function estimator that adapts a frozen contrastive VLM (CLIP) at test time via a light-weight adaptation module updated with a single gradient step on a meta-learned self-supervised reconstruction loss, encoding temporal context through sequential parameter updates and using dissimilarity-based sampling during training to mitigate shortcuts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "VITA (test-time adapted CLIP value estimator)",
            "abstraction_level": "value-based predictions only (semantic multimodal embeddings from CLIP, adapted into a low-dim adaptation space d' = 64)",
            "feature_selection_mechanism": "meta-learned linear projections (P_K, P_V, P_Q) with a self-supervised reconstruction loss and dissimilarity-based sampling to encourage semantic feature reliance (contrastive pretraining provides initial feature alignment)",
            "task_domain": "real-world robotic manipulation (BridgeData V2) and zero-shot transfer to simulated multi-task RL (Meta-World MT10)",
            "distractor_presence": "realistic scene/background variation across different environments; tasks include environment shifts (different surfaces, backgrounds) and embodiment shifts (different robot DeepThought vs training WidowX 250); long-horizon tasks introduce temporal distractors",
            "performance_metrics": "Value Order Correlation (VOC) per dataset (examples from Table 1): tk_pnp VOC=0.782, lm_pnp=0.725, td_fold=0.709, ft_fold=0.658, rd_fold=0.606, ms_sweep=0.490, dt_tk_pnp=0.820, dt_tk_stack=0.708, dt_ft_stack=0.698, dt_rd_pnp=0.695; Offline RL on MT10 IQM=0.815 (95% CI [0.785,0.838]); BinVOC discriminative score=1.00 (perfect discrimination on tested scripted datasets).",
            "computational_cost_details": "Adaptation module: two-layer residual MLP (projection dim d' = 64). Training: AdamW lr=1e-4, weight decay=1e-4, batch size=32, 5 epochs, pad sequences to 120 frames, trained on NVIDIA RTX 6000 Ada GPUs. Test-time adaptation: single gradient step (t_ep = 1) with learning rate η=0.1 (inference adaptation not reset by default). Dissimilarity sampling complexity: O(N_c^2 * w_tr * d) for pairwise distances; example feature dim d=1024, w_tr=8, batch size 32; no total parameter count or FLOPs reported.",
            "comparison_to_baselines": "Consistently outperforms CLIP-based baselines and autoregressive VLM-based GVL in VOC and transfer: e.g., in-distribution tk_pnp VOC: VITA 0.782 vs GVL-0S 0.269, GVL-1S 0.252, CLIP-FT 0.251; Meta-World MT10 IQM: VITA 0.815 vs CLIP-FT 0.785, VLM-CL 0.760, VLM-RM 0.746, META-WL (fuzzy-logic dense reward) 0.779. VITA also outperforms full-trajectory and random sub-trajectory sampling strategies in discriminative tasks.",
            "transfer_learning_results": "Trained on a single real-world training environment (BridgeData V2 subset) and transfers zero-shot to multiple OOD settings: environment shifts (lm_pnp, td_fold, ft_fold, rd_fold), embodiment shifts (DeepThought robot datasets), and to simulated Meta-World MT10 for zero-shot reward shaping; quantitative VOC scores above show substantial transfer (e.g., dt_tk_pnp VOC=0.820 despite embodiment shift).",
            "multi_task_performance": "Used as zero-shot dense reward for offline RL across Meta-World MT10; multi-task policy IQM=0.815 (95% CI [0.785,0.838]) averaged across tasks/seeds, exceeding the simulator fuzzy reward baseline (META-WL IQM=0.779). Policies trained with IQL for 100k gradient steps per seed using VITA-derived rewards.",
            "failure_modes": "Performance degrades on very long-horizon/high-variability tasks (ms_sweep VOC=0.490 lower than many folding tasks). Test-time adaptation at every timestep may be unsafe or impractical in some real-time deployments. Possible sensitivity to high execution variability and extended durations noted in limitations.",
            "ablation_studies": "Dissimilarity-based sampling vs full-trajectory vs random sampling: dissimilarity sampling (w_tr=8, k=8) yields perfect discriminative performance (4/4 scripted datasets), full-trajectory sampling performs worst (overfits temporal shortcuts), random sampling intermediate. Temporal adaptation ablation: step-by-step incremental adaptation (VITA) outperforms trajectory-level (single update per trajectory) and no-temporal reset-at-each-step variants; exact numeric deltas not listed, but VITA showed consistent superiority across datasets.",
            "sample_efficiency": "Trained with relatively small dataset and few epochs: uses expert visual trajectories (200 per evaluation dataset, except ms_sweep 100), model trained for 5 epochs; test-time adaptation uses one gradient step per timestep. No explicit samples-to-performance curves or sample count to reach thresholds provided.",
            "generalization_analysis": "Evaluated across distribution shifts: environment, embodiment, and combined; VOC and BinVOC metrics show VITA generalizes robustly (VOC values listed above); outperforms CLIP-FT and VLM/GVL baselines especially on pick-and-place and stacking tasks where GVL/autoregressive methods fail to generalize.",
            "reconstruction_quality": "Self-supervised adaptation uses L2 reconstruction loss between f_adapt(P_K z_t) and P_V z_t, but no numerical reconstruction metrics (MSE, PSNR, SSIM) reported for reconstruction quality.",
            "task_relevance_analysis": "Introduces dissimilarity-based sub-trajectory sampling to mitigate shortcut learning and encourage reliance on semantic/temporal cues; empirical evidence shows improved discrimination between expert and non-expert trajectories. No fine-grained feature attribution or attention maps reported.",
            "dynamic_abstraction": "Not explicitly supported as multi-level abstraction; adaptation changes internal parameters online to encode temporal context but does not switch between distinct abstraction levels dynamically.",
            "exploration_vs_exploitation": "Not explicitly discussed or evaluated in the paper.",
            "information_theoretic_analysis": "No mutual information, compression, or rate-distortion analyses reported.",
            "pixel_fidelity_benefits": "Paper does not analyze pixel-level reconstruction benefits; uses CLIP semantic embeddings rather than pixel-reconstruction, and does not present scenarios where pixel fidelity is beneficial.",
            "uuid": "e2249.0"
        },
        {
            "name_short": "CLIP",
            "name_full": "CLIP (Contrastive Language–Image Pre-training)",
            "brief_description": "A contrastive vision-language model that maps images and text to a shared semantic embedding space, used here as a frozen multimodal encoder to produce joint visual-language representations for value prediction.",
            "citation_title": "Learning transferable visual models from natural language supervision",
            "mention_or_use": "use",
            "model_name": "CLIP (contrastive VLM)",
            "abstraction_level": "semantic/object-level multimodal embeddings (pretrained contrastive features, image-to-text alignment)",
            "feature_selection_mechanism": "contrastive pretraining aligns semantically relevant features; no explicit task-specific feature selection in frozen CLIP beyond learned projections P_Q/linear heads",
            "task_domain": "used as encoder for robotic manipulation visual trajectories; baseline VLM-CL and CLIP-FT use CLIP representations directly",
            "distractor_presence": "real-world backgrounds and environment variation in datasets; CLIP features are susceptible to lack of temporal modeling (no explicit temporal disambiguation)",
            "performance_metrics": "As baseline VLM-CL and CLIP-FT: e.g., CLIP-FT VOC on tk_pnp=0.251; VLM-CL IQM on MT10=0.760 (95% CI [0.722, 0.791])",
            "computational_cost_details": "Frozen encoder used; projection dims and training configs: CLIP-based multimodal representation dim d ≈ 1024 used in paper; CLIP-FT used an 8× larger projection and 10× more training steps than VITA's supervised head (exact params not provided).",
            "comparison_to_baselines": "CLIP-based zero-shot similarity methods (VLM-CL, VLM-RM) underperform relative to VITA and CLIP-FT on temporally-dependent tasks; CLIP-FT (supervised on CLIP features) performs comparably in-distribution but fails to generalize to OOD.",
            "transfer_learning_results": "Frozen CLIP features alone insufficient for strong temporal generalization; CLIP-FT fine-tuned regressor failed on many OOD datasets despite extra training steps.",
            "multi_task_performance": "Used as backbone for multi-task offline reward shaping experiments via VITA; CLIP-FT achieved MT10 IQM=0.785 but lower than VITA.",
            "failure_modes": "Lacks temporal modeling → fails to disambiguate visually similar but temporally different states; vulnerable to shortcut learning when consecutive frames redundant.",
            "ablation_studies": "Paper compares frozen CLIP baselines (VLM-CL, VLM-RM, CLIP-FT) versus test-time adaptation; CLIP-FT trained with larger projection but still failed to generalize in many OOD cases.",
            "sample_efficiency": "Not explicitly quantified; CLIP-FT was trained with 10× more steps than VITA's supervised head but still underperformed on OOD.",
            "generalization_analysis": "CLIP features generalize semantically but not temporally; need adaptation or temporal modeling to improve VOC on long-horizon or ambiguous states.",
            "reconstruction_quality": null,
            "task_relevance_analysis": "Paper argues contrastive alignment yields semantic proximity to goals, but CLIP lacks task-temporal disambiguation; no explicit per-feature task relevance analysis provided.",
            "dynamic_abstraction": "No",
            "exploration_vs_exploitation": "No",
            "information_theoretic_analysis": "No",
            "pixel_fidelity_benefits": "No",
            "uuid": "e2249.1"
        },
        {
            "name_short": "GVL",
            "name_full": "GVL (Vision-Language value learner using autoregressive VLMs)",
            "brief_description": "A baseline method that uses autoregressive vision-language models (e.g., Gemini 1.5 Pro) to estimate goal-conditioned value by in-context conditioning on visual trajectories and task descriptions; can be used zero-shot or one-shot (in-context examples).",
            "citation_title": "Vision language models are in-context value learners",
            "mention_or_use": "use",
            "model_name": "GVL (autoregressive VLM in-context value estimator)",
            "abstraction_level": "sequence-level autoregressive conditioning over trajectories (temporal/contextual embeddings), but outputs scalar value predictions (semantic-temporal)",
            "feature_selection_mechanism": "in-context conditioning; relies on internal attention and learned sequence-level representations; temporal order preserved or frames shuffled to mitigate monotonic bias",
            "task_domain": "robotic manipulation value estimation and zero-shot evaluation on BridgeData V2 datasets",
            "distractor_presence": "evaluated on same environment/embodiment shifts and long-horizon tasks with background/environment variation; autoregressive pretraining bias toward monotonic progress noted",
            "performance_metrics": "GVL-0S and GVL-1S VOC examples: tk_pnp 0.269 (GVL-0S), lm_pnp 0.305, td_fold 0.326, ft_fold 0.331, rd_fold 0.372; underperformed relative to VITA on stacking and pick-and-place tasks (examples: dt_tk_pnp VOC=0.258 GVL-0S vs VITA 0.820).",
            "computational_cost_details": "Uses large autoregressive VLM (Gemini 1.5 Pro); noted to have prohibitively high inference cost in RL settings; exact FLOPs/params not provided in this paper but described as expensive and proprietary.",
            "comparison_to_baselines": "Performs well on folding tasks but poorly on stacking/pick-and-place and under embodiment shifts; VITA consistently outperforms GVL in VOC and transfer experiments (large deltas shown in Table 1).",
            "transfer_learning_results": "In-context one-shot (GVL-1S) sometimes helps but is unreliable and fails to generalize under embodiment shifts; shuffling frames to counter monotonic bias discards temporal order, harming temporal reasoning.",
            "multi_task_performance": "Not evaluated for downstream offline RL MT10 in this paper due to prohibitive inference cost at scale; no multi-task IQM reported.",
            "failure_modes": "Biased by chronological order in pretraining leading to monotonic increasing prediction bias; ineffective under embodiment shifts and for some task types (stacking, pick-and-place).",
            "ablation_studies": "Paper compares GVL-0S vs GVL-1S (in-context examples) showing mixed effects; no internal GVL ablations performed here.",
            "sample_efficiency": "Not reported.",
            "generalization_analysis": "Fails to generalize across embodiment shifts in evaluated settings; GVL-1S performed poorly on dt_tk_pnp and dt_tk_stack compared to VITA.",
            "reconstruction_quality": null,
            "task_relevance_analysis": "No explicit task vs irrelevant feature analysis for GVL in this paper beyond noting temporal order bias and reliance on in-context examples.",
            "dynamic_abstraction": "No",
            "exploration_vs_exploitation": "No",
            "information_theoretic_analysis": "No",
            "pixel_fidelity_benefits": "No",
            "uuid": "e2249.2"
        },
        {
            "name_short": "Test-Time Training (TTT)",
            "name_full": "Test-time Training with Self-Supervision for Sequence Modeling",
            "brief_description": "A paradigm where an adaptation module is updated at test time with a self-supervised loss per instance (or per timestep for sequences), enabling implicit memory via adapted parameters and improving robustness/generalization under distribution shift; here meta-learned for value estimation.",
            "citation_title": "Test-time training with self-supervision for generalization under distribution shifts",
            "mention_or_use": "use",
            "model_name": "Test-time training (sequence variant, meta-learned self-supervision)",
            "abstraction_level": "parameter-level adaptation that results in sequence-aware representations (not explicit multi-level abstraction)",
            "feature_selection_mechanism": "self-supervised reconstruction loss with meta-learned projection matrices (P_K, P_V) that shape which features the adaptation module emphasizes",
            "task_domain": "applied to goal-conditioned value estimation on visual trajectories for robotic manipulation",
            "distractor_presence": "used to adapt representations in presence of environment/embodiment distractors; sequential updates encode temporal context to disambiguate visually similar states",
            "performance_metrics": "Improves VOC and BinVOC when used as in VITA versus no test-time adaptation; exact numeric contribution implicit in VITA vs ablated variants (VITA outperforms reset-per-step and trajectory-level variants across datasets).",
            "computational_cost_details": "Parameter-efficient: only the adaptation module (two-layer MLP, d'=64) is adapted at test time with 1 gradient step (t_ep = 1) and learning rate 0.1; adaptation overhead described as negligible compared to forward/backward passes.",
            "comparison_to_baselines": "Outperforms no-adaptation and trajectory-level single-update adaptation in encoding temporal context for value estimation (VITA &gt; TTT-OF and TTT-VIS in ablations).",
            "transfer_learning_results": "Enables zero-shot transfer to OOD tasks by rapidly adapting representations to trajectory-specific temporal and semantic context.",
            "multi_task_performance": "Used within VITA to produce dense reward shaping for multi-task RL with improved performance (MT10 IQM=0.815).",
            "failure_modes": "May pose safety/real-time concerns if updating in every timestep; can struggle in settings with extreme execution variability or very long horizons if only single-step adaptation used.",
            "ablation_studies": "Paper performs ablation comparing incremental per-step adaptation (implicit memory) vs one-update-per-trajectory vs reset-per-step (no temporal memory); incremental approach performs best.",
            "sample_efficiency": "Single gradient step per timestep (low adaptation sample requirement); training uses meta-learning to make that single step effective; no explicit sample-efficiency curves.",
            "generalization_analysis": "Meta-learned TTT objective improves zero-shot generalization to OOD trajectories and encodes temporal context to distinguish progress stages.",
            "reconstruction_quality": "Self-supervised loss is L2 between adapted projection and target projection; no standalone reconstruction metrics reported.",
            "task_relevance_analysis": "Meta-learning optimizes the self-supervised task to minimize downstream supervised value loss after adaptation, aligning adaptation with task-relevant features; empirical improvements reported but no per-feature attribution.",
            "dynamic_abstraction": "Adaptation changes representation space online but not explicitly multi-level abstraction switching.",
            "exploration_vs_exploitation": "No",
            "information_theoretic_analysis": "No",
            "pixel_fidelity_benefits": "No",
            "uuid": "e2249.3"
        },
        {
            "name_short": "Dissimilarity-based sampling",
            "name_full": "Dissimilarity-based sub-trajectory sampling",
            "brief_description": "A training-time heuristic that selects semantically diverse sub-trajectories by scoring windows by their total pairwise dissimilarity and selecting the top-k, intended to reduce shortcut learning from redundant consecutive frames and encourage semantic/temporal diversity for meta-learned test-time adaptation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Dissimilarity-based sampling (diverse window selection)",
            "abstraction_level": "semantic/local-temporal sub-trajectory selection (operates on CLIP multimodal embeddings, not pixel-level)",
            "feature_selection_mechanism": "heuristic selection via pairwise L2 distances between flattened window feature vectors (sum of distances as score)",
            "task_domain": "training value estimators on video trajectories for robotic manipulation",
            "distractor_presence": "helps emphasize underrepresented semantic segments across varied environments and backgrounds by avoiding redundant frames",
            "performance_metrics": "In discriminative ablation (Disc@VOC across 4 scripted datasets), dissimilarity-based sampling with w_tr=8 and k=8 achieved perfect discriminative performance (4/4), while full-trajectory sampling performed worst and random sampling intermediate.",
            "computational_cost_details": "Computes pairwise distance matrix D ∈ R^{N_c×N_c} for candidate windows; complexity O(N_c^2 * w_tr * d). Example dims: d=1024, w_tr=8. Overhead described as negligible relative to model forward/backward costs on GPU.",
            "comparison_to_baselines": "Outperforms full-trajectory sampling (which overfits to temporal shortcuts) and random sampling in enabling discriminative test-time adaptation and generalization.",
            "transfer_learning_results": "Training with dissimilarity sampling yields better OOD generalization in value estimation and downstream reward shaping (empirically improves VOC and discriminative metrics).",
            "multi_task_performance": "No direct multi-task metric isolated for sampling method alone, but overall VITA (which uses this sampling) yields strong MT10 IQM=0.815.",
            "failure_modes": "Heuristic selection depends on quality of representation space; computing full pairwise distances could be heavy for very long sequences but authors note trajectories are short in their datasets (mean 69, max 120).",
            "ablation_studies": "Ablation in Section F.1 compares full-trajectory, random, and dissimilarity-based sampling; dissimilarity sampling yields best discriminative results and improved generalization.",
            "sample_efficiency": "By focusing on diverse windows, training emphasizes underrepresented segments, but no direct sample-efficiency numbers reported.",
            "generalization_analysis": "Empirical evidence that dissimilarity-based sampling reduces shortcut learning and improves discriminative power and OOD VOC performance.",
            "reconstruction_quality": null,
            "task_relevance_analysis": "Argued and empirically supported that sampling increases reliance on semantic cues rather than temporal shortcuts; no per-feature attribution provided.",
            "dynamic_abstraction": "No",
            "exploration_vs_exploitation": "No",
            "information_theoretic_analysis": "No",
            "pixel_fidelity_benefits": "No",
            "uuid": "e2249.4"
        },
        {
            "name_short": "f_adapt (adaptation module)",
            "name_full": "f_adapt: Lightweight temporal adaptation module",
            "brief_description": "A two-layer residual MLP (GELU) with projection dimension d' = 64 that is meta-learned and updated at test time with a single gradient step on a self-supervised reconstruction loss to encode sequence history into parameters for better temporal reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "f_adapt (two-layer residual MLP adaptation module)",
            "abstraction_level": "low-dimensional adaptation-space transformations (d' = 64) operating on CLIP embeddings — semantic/temporal rather than pixel-level",
            "feature_selection_mechanism": "learned via meta-learning of linear projections (P_K, P_V, P_Q) and initialization θ_0 that make single-step gradient updates effective at emphasizing task-relevant features",
            "task_domain": "sequence encoding for goal-conditioned value estimation in robotic manipulation trajectories",
            "distractor_presence": "intended to adapt representations to trajectory-specific distractors (environment variability, embodiment differences) by adjusting internal parameters online",
            "performance_metrics": "Component of VITA which yields the VOC and MT10 IQM numbers reported for full system; ablations show incrementally updated f_adapt (implicit memory) outperforms other adaptation strategies.",
            "computational_cost_details": "Lightweight: two-layer MLP with d'=64; adapted at inference with 1 gradient step (learning rate 0.1) — adaptation overhead negligible compared to full model passes; trained with AdamW.",
            "comparison_to_baselines": "When present and meta-learned, it enables VITA to exceed CLIP-FT and autoregressive baselines; when ablated or reset, performance drops (see temporal memory ablation).",
            "transfer_learning_results": "Enables transfer to OOD environments and embodiments by online adaptation of embeddings; see VOC and MT10 results.",
            "multi_task_performance": "Contributes to VITA's superior MT10 reward shaping performance (IQM=0.815).",
            "failure_modes": "Single-step adaptation can be insufficient in extremely variable or very long tasks; potential deployment safety concerns if updated every timestep.",
            "ablation_studies": "Temporal memory ablation (reset vs incremental vs trajectory-level) shows incremental parameter updates of f_adapt are most effective.",
            "sample_efficiency": "Designed to be parameter-efficient and effective with one test-time gradient step; no explicit sample counts beyond per-frame adaptation.",
            "generalization_analysis": "Implicit memory in f_adapt is key for temporal disambiguation and OOD generalization shown in experiments.",
            "reconstruction_quality": "Self-supervised L2 loss used to train f_adapt through meta-learning; no explicit reconstruction metrics reported.",
            "task_relevance_analysis": "Meta-learning of P_K/P_V shapes which projected features f_adapt reconstructs, aligning adaptation with task-relevant signal; empirical improvements in VOC indicate success.",
            "dynamic_abstraction": "Adaptation changes parameters but not explicit multi-level abstraction; acts as a lightweight sequence memory.",
            "exploration_vs_exploitation": "No",
            "information_theoretic_analysis": "No",
            "pixel_fidelity_benefits": "No",
            "uuid": "e2249.5"
        },
        {
            "name_short": "VLM-CL / VLM-RM",
            "name_full": "VLM-CL (CLIP cosine-similarity baseline) / VLM-RM (reference prompt modification baseline)",
            "brief_description": "Simple CLIP-based zero-shot value estimators: VLM-CL uses cosine similarity between CLIP frame and task text embeddings; VLM-RM regularizes CLIP embeddings by projecting features along direction from a generic reference prompt to the task prompt.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "VLM-CL and VLM-RM (CLIP-based zero-shot reward/value baselines)",
            "abstraction_level": "frame-level semantic similarity in CLIP embedding space (no temporal modeling)",
            "feature_selection_mechanism": "contrastive similarity to task prompt (VLM-CL); directional projection relative to a reference prompt (VLM-RM); no learned temporal selection",
            "task_domain": "zero-shot goal-conditioned value estimation for robotic manipulation (BridgeData V2)",
            "distractor_presence": "perform poorly when temporal context is needed to disambiguate states or when distractors present; no explicit distractor handling",
            "performance_metrics": "Baselines perform poorly in VOC and offline RL: e.g., VLM-CL MT10 IQM=0.760 [0.722,0.791], VLM-RM IQM=0.746 [0.718,0.771]; VLM-CL / VLM-RM VOC often &lt; 0.1 on many datasets (Table 1 examples: tk_pnp VLM-CL=0.038, VLM-RM=0.029).",
            "computational_cost_details": "Very cheap inference (just CLIP cosine similarity), no adaptation overhead; exact FLOPs/params not provided.",
            "comparison_to_baselines": "Underperform relative to CLIP-FT and VITA; inability to model temporal context is primary weakness.",
            "transfer_learning_results": "Fail to generalize where temporal reasoning or adaptation is required.",
            "multi_task_performance": "Used as baseline for MT10, but yields lower IQM than VITA or CLIP-FT.",
            "failure_modes": "No temporal modeling, sensitive to frame-level ambiguity, fail to discriminate expert vs scripted in BinVOC in many cases.",
            "ablation_studies": "Compared against VITA and CLIP-FT showing relative weaknesses; no internal ablations.",
            "sample_efficiency": "N/A (zero-shot, no training required).",
            "generalization_analysis": "Poor OOD generalization relative to adaptation-based methods.",
            "reconstruction_quality": null,
            "task_relevance_analysis": "No explicit mechanism to focus on task-relevant temporal cues; thus often relies on frequently occurring visual patterns (shortcuts).",
            "dynamic_abstraction": "No",
            "exploration_vs_exploitation": "No",
            "information_theoretic_analysis": "No",
            "pixel_fidelity_benefits": "No",
            "uuid": "e2249.6"
        },
        {
            "name_short": "CLIP-FT",
            "name_full": "CLIP-FT (Supervised regression on frozen CLIP features)",
            "brief_description": "A supervised baseline that trains a regression head on frozen CLIP multimodal representations to predict normalized temporal progress; uses a larger projection and more training steps than VITA's supervised head.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "CLIP-FT (supervised CLIP regressors)",
            "abstraction_level": "frame-level semantic embeddings with supervised regression to temporal progress (no temporal modeling in backbone)",
            "feature_selection_mechanism": "learned linear projection and MLP regression head trained on normalized progress labels (supervised)",
            "task_domain": "in-distribution goal-conditioned value estimation and OOD evaluation for robotic manipulation",
            "distractor_presence": "performs comparably in-distribution but fails to generalize to OOD environment/embodiment shifts and long-horizon tasks where temporal context matters",
            "performance_metrics": "CLIP-FT VOC examples: tk_pnp=0.251; MT10 IQM=0.785 [0.759,0.809]; discriminative BinVOC=0.75 (failed one scripted task).",
            "computational_cost_details": "Trained with an 8× larger projection matrix and 10× more training steps than VITA's supervised head; other hyperparameters not fully detailed.",
            "comparison_to_baselines": "Performs better than zero-shot CLIP similarity methods but worse than VITA on OOD generalization and some tasks requiring temporal reasoning.",
            "transfer_learning_results": "Limited transferability to embodiment and environment shifts despite heavy supervised training.",
            "multi_task_performance": "Used as baseline for MT10 offline RL with IQM lower than VITA.",
            "failure_modes": "Fails on some scripted tasks and OOD scenarios; lacks temporal adaptation.",
            "ablation_studies": "Compared against VITA (which includes adaptation) to show the benefit of test-time adaptation; no internal ablations.",
            "sample_efficiency": "Required more training steps than VITA's supervised head but still underperformed on OOD.",
            "generalization_analysis": "Overfits to in-distribution statistics and fails under distribution shifts addressed by VITA.",
            "reconstruction_quality": null,
            "task_relevance_analysis": "No explicit self-supervised mechanism to emphasize task-relevant temporal features; relies on supervised regression which can overfit to spurious cues.",
            "dynamic_abstraction": "No",
            "exploration_vs_exploitation": "No",
            "information_theoretic_analysis": "No",
            "pixel_fidelity_benefits": "No",
            "uuid": "e2249.7"
        },
        {
            "name_short": "World models (general mention)",
            "name_full": "World Models (generative predictive models of environment dynamics)",
            "brief_description": "Concept of learning predictive generative models of environments to enable planning and learning, referenced as motivating prior work and a future application area for test-time adaptation methods like VITA.",
            "citation_title": "World models",
            "mention_or_use": "mention",
            "model_name": "World models (general family: generative predictive environment models)",
            "abstraction_level": "typically pixel-level or latent generative reconstructions enabling rollout (paper cites both pixel/latent world models in related work)",
            "feature_selection_mechanism": "varies by method; world-model literature uses latent encoders, reconstruction losses, and learned latents to capture task-relevant dynamics (not used directly in experiments here)",
            "task_domain": "robotics, autonomous driving, video games (general references in intro/related work)",
            "distractor_presence": "world-models research addresses learning under rich visual backgrounds and long-horizon dynamics; not evaluated in this paper",
            "performance_metrics": null,
            "computational_cost_details": null,
            "comparison_to_baselines": "Mentioned as alternative approach for learning from videos and for training agents in simulated environments; paper suggests VITA could be applied within world models in future work.",
            "transfer_learning_results": "Not evaluated here; mentioned as possible future direction for applying test-time adaptation within world models.",
            "multi_task_performance": "Not evaluated here.",
            "failure_modes": "Not discussed in detail in this paper beyond general literature references.",
            "ablation_studies": null,
            "sample_efficiency": null,
            "generalization_analysis": null,
            "reconstruction_quality": null,
            "task_relevance_analysis": null,
            "dynamic_abstraction": null,
            "exploration_vs_exploitation": null,
            "information_theoretic_analysis": null,
            "pixel_fidelity_benefits": null,
            "uuid": "e2249.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning transferable visual models from natural language supervision",
            "rating": 2
        },
        {
            "paper_title": "Test-time training with self-supervision for generalization under distribution shifts",
            "rating": 2
        },
        {
            "paper_title": "Vision language models are in-context value learners",
            "rating": 2
        },
        {
            "paper_title": "World models",
            "rating": 1
        },
        {
            "paper_title": "Test-time training on video streams",
            "rating": 1
        },
        {
            "paper_title": "Learning to (learn at test time): Rnns with expressive hidden states",
            "rating": 1
        }
    ],
    "cost": 0.02095625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>VITA: ZERO-SHOT VALUE FUNCTIONS VIA TEST-TIME ADAPTATION OF VISION-LANGUAGE MODELS
12 Oct 2025</p>
<p>Christos Ziakas c.ziakas24@imperial.ac.uk 
Alessandra Russo a.russo@imperial.ac.uk </p>
<p>Imperial College London</p>
<p>Imperial College London</p>
<p>VITA: ZERO-SHOT VALUE FUNCTIONS VIA TEST-TIME ADAPTATION OF VISION-LANGUAGE MODELS
12 Oct 202579555D3CE50BFFC7178800A7D1268BF8arXiv:2506.10085v3[cs.CV]
Vision-Language Models (VLMs) show promise as zero-shot goal-conditioned value functions, but their frozen pre-trained representations limit generalization and temporal reasoning.We introduce VITA, a zero-shot value function learning method that enhances both capabilities via test-time adaptation.At inference, a lightweight adaptation module is updated via a gradient step on a meta-learned self-supervised loss, such that each test-time update improves value estimation.By updating sequentially over a trajectory, VITA encodes history into its parameters, addressing the temporal reasoning limitations.To mitigate shortcut learning, we propose a dissimilarity-based sampling strategy that selects semantically diverse segments of the trajectory during training.In real-world robotic manipulation tasks, VITA generalizes from a single training environment to diverse out-of-distribution tasks, environments, and embodiments, outperforming the state-of-the-art zero-shot method using autoregressive VLMs.Furthermore, we demonstrate that VITA's zero-shot value estimates can be utilized for reward shaping in offline reinforcement learning, resulting in multi-task policies on the Meta-World benchmark that exceed the performance of those trained with the simulation's fuzzy-logic dense rewards.</p>
<p>INTRODUCTION</p>
<p>Vision-Language Models (VLMs) have demonstrated strong generalization across diverse tasks and domains by learning from large-scale, unstructured web data without human supervision [1,2].In contrast, despite significant advances in learning generalist policies for robotics [3,4] and 3D virtual environments [5], state-of-the-art methods have yet to achieve comparable success due to their reliance on expert demonstrations [6].World models [7,8,9] have emerged as a promising solution, enabling agents to learn in simulated environments, with applications spanning robotics [10,11,12], autonomous driving [13], and video games [14].Despite the potential of world models to generate realistic visual trajectories in diverse, open-ended environments [15,16], a critical challenge remains: how can agents effectively learn from videos at scale without relying on human supervision?</p>
<p>A line of research explores policy learning directly from expert visual trajectories by inferring actions from visual transitions [17,18,19,20].These latent actions can be mapped to executable controls in deployment, although this remains a challenging task in practice.In parallel, other work focuses on learning a universal goal-conditioned value function, using it as a zero-shot reward shaping and supervision signal for reinforcement learning (RL) and imitation learning [21,22].In this framework, goal-conditioned value estimation can be formulated as: predicting how far an agent has progressed toward completing a task, based on visual observations and a natural language task description [23,24,25,26].</p>
<p>Prior work has employed pre-trained contrastive VLMs [1] for zero-shot reward shaping, leveraging the similarity between task descriptions and visual observations in a shared multimodal representation space to ground progress in semantic context [24,27,28].However, these methods fail to capture the temporal context needed to disambiguate visually similar states that occur at different stages of a task (e.g., folding vs. unfolding a shirt).In contrast, autoregressive VLMs [2] incorporate temporal context by conditioning on the entire visual trajectory within the prompt; however, they inherit a bias toward monotonically increasing predictions from the chronologically ordered datasets used in pre-training [25].Both VLM architectures rely on pre-trained representations [24,25,27,28,29] for zero-shot prediction, which limits both their generalization and their temporal reasoning [30].To enable generalizable value function learning, recent work has explored large-scale pretraining [31,24], domain-specific fine-tuning [32], and demo-based reward specification [33].</p>
<p>In this work, we introduce VITA, a zero-shot value function learning method that enhances both generalization and temporal reasoning through test-time adaptation, outperforming the state-of-the-art zero-shot approach [25] on real-world robotic manipulation tasks.Unlike prior methods that rely on large-scale pretraining or expert demonstrations for generalization, VITA adapts online to both the semantic and temporal context of trajectories.At inference, a lightweight adaptation module is updated in negligible time with a gradient step on a meta-learned self-supervised loss [34], such that each test-time update improves value function estimation [35].By updating sequentially over a test trajectory, the value function estimator encodes trajectory history into its parameters [34], thereby addressing the temporal reasoning limitations of prior work.To mitigate shortcut learning [36], we introduce a dissimilarity-based sampling strategy that encourages reliance on semantic cues, supported by empirical evidence.Our evaluation demonstrates the effectiveness of our method across core capabilities of value function estimation: generalization under distribution shifts, differentiation between expert and non-expert trajectories, and reward shaping for offline RL [37].</p>
<p>We summarize our key contributions as follows:</p>
<p>• We propose VITA, a test-time adaptation method that enhances the generalization and temporal reasoning of contrastive VLMs for zero-shot value function estimation, without requiring task-specific demonstrations or large-scale pretraining.</p>
<p>• VITA generalizes from a single training environment to diverse out-of-distribution tasks, environments, and embodiments in robotic manipulation, outperforming the state-of-the-art zero-shot method [25].</p>
<p>• VITA's zero-shot value estimates for reward shaping yield offline RL policies on the Meta-World benchmark for multi-task learning (MT10) that surpass those trained with the simulation's fuzzy-logic dense rewards [38].</p>
<p>PRELIMINARIES</p>
<p>GOAL-CONDITIONED VALUE FUNCTIONS</p>
<p>In video trajectories, task progress estimation can be viewed as goal-conditioned value function estimation, where the reward reflects task completion.Therefore, we formulate learning a goal-conditioned value function as predicting the degree of task completion from vision-language representations.Formally, the value function is defined as: V : O × G → [0, 1] which maps an observation o t ∈ O and a goal specification g ∈ G to a scalar value indicating the predicted progress toward goal completion.We set V (o t ; g) = 0 to correspond to the start of the task and V (o t ; g) = 1 to indicate completion.Task progress is commonly aligned with temporal position in expert demonstrations, based on the assumption that such trajectories exhibit monotonically increasing progress toward goal completion [23,25,26].Given an expert trajectory τ = (o 1 , . . ., o T ) ∼ π E , temporal progress is defined using normalized timestep indices:
V π E (o t ; g) = t
T where T is the trajectory length.Therefore, temporal progress provides supervision for learning a goal-conditioned value function V .</p>
<p>TEST-TIME TRAINING FOR SEQUENCE MODELING</p>
<p>Test-time training (TTT) [39] is a test-time adaptation method that treats inference as a self-supervised learning task [40], updating model parameters on each test instance without access to labels.While originally proposed for static image classification [39], recent work extends TTT to sequence modeling [34,41], where a parametric adaptation module f adapt is updated at each timestep using a gradient step on a self-supervised loss.The adaptation parameters thereby serve as an implicit memory, encoding sequence history into the updated parameters.Sun et al. [34] further propose to meta-learn the self-supervised task, following the gradient-based meta-learning paradigm [35].In this setting [34], the self-supervised loss is parameterized by learnable linear projections and trained During inference, the adaptation module f adapt is updated with a gradient step on ℓ self , enabling zero-shot value estimation on OOD trajectories while encoding temporal context in its parameters.</p>
<p>to minimize downstream prediction loss after a test-time update, rather than directly minimizing it.</p>
<p>In this work, we apply TTT to goal-conditioned value function learning, where temporal context is captured through sequential updates, and the self-supervised task is meta-learned to improve value estimation rather than being predefined a priori.</p>
<p>3 VITA: ZERO-SHOT VALUE FUNCTIONS VIA TEST-TIME ADAPTATION</p>
<p>MODEL ARCHITECTURE</p>
<p>Our goal-conditioned value function estimator comprises three modules: (1) a multimodal encoder that extracts joint visual-language representations from visual trajectories and their goal descriptions;</p>
<p>(2) an adaptation module updated at test-time using a self-supervised loss that is meta-learned to improve value estimation; and (3) a regression head that predicts value estimates.</p>
<p>MULTIMODAL INPUT REPRESENTATION</p>
<p>We use a frozen contrastive vision-language encoder, CLIP [1], to extract representations from visual observations and goal descriptions.Given a visual trajectory τ = (o 1 , . . ., o T ) and a language task description g, we concatenate their representations at each timestep t to form a sequence of joint multimodal representations (z 1 , z 2 , . . ., z T ), where z t = [ϕ v (o t ); ϕ g (g)] ∈ R 2d , and ϕ v and ϕ g denote the visual and language encoders, respectively.CLIP is pre-trained with a contrastive objective to align paired image-text inputs in a shared representation space [1].As a result, representations of visual observations that are semantically closer to goal completion tend to be closer to the representations of the goal description.</p>
<p>TEST-TIME ADAPTATION</p>
<p>To adapt multimodal representations to both semantic and temporal context, we employ an adaptation module f adapt following the test-time training paradigm described in 2.2.At inference, the parameters of f adapt are updated online via a gradient step on a meta-learned self-supervised loss ℓ self .This loss is formulated as a reconstruction objective using learnable linear projections P K and P V , which are meta-learned during training to improve value estimation after test-time adaptation.In particular, given P K ∈ R d ′ ×d that generates a perturbed input view and P V ∈ R d ′ ×d for the target, the self-supervised loss is defined as:
ℓ self (z t ; θ t−1 , P K , P V ) = ∥f adapt (P K z t ; θ t−1 ) − P V z t ∥ 2 . (1)
where θ t−1 denotes the parameters of f adapt before test-time adaptation.At each timestep t, the parameters are updated by
θ t = θ t−1 − η∇ θ ℓ self (z t ; θ t−1 ),(2)
with learning rate η.Through sequential updates, f adapt implicitly retains information from past visual observations, thereby capturing temporal context.In Section 4.6.2,we provide empirical evidence that incremental parameter updates of the value function estimator are more effective than updates performed at the trajectory level.</p>
<p>ZERO-SHOT VALUE FUNCTION ESTIMATOR</p>
<p>After test-time adaptation, a meta-learned linear projection
P Q ∈ R d ′ ×d maps the input z t into an adaptation space R d ′ .
Then, this representation is passed through the adaptation module f adapt , followed by a regression head h that outputs predicted values of the goal-conditioned value function:
V (z t ; g) = h(f adapt (P Q z t ; θ t ))(3)
The regression head h is a two-layer multilayer perceptron (MLP), following value-function estimator architectures used in deep reinforcement learning [42].The network is trained using normalized progress labels y t from expert demonstrations as described in Section 2.1, with a supervised prediction loss L pred defined as the mean squared error between predicted values V (z t ; g) and targets y t .At inference, the regression head h remains frozen, while the adaptation module f adapt is updated online using the self-supervised objective.This enables the value function estimator to generalize in a zero-shot setting without requiring task-specific demonstrations.</p>
<p>TRAINING PROCEDURE</p>
<p>We train the value function estimator with gradient-based meta-learning, optimizing a self-supervised loss ℓ self such that test-time adaptation improves the supervised prediction loss L pred [34].Training is performed on diverse sub-trajectories selected via dissimilarity-based sampling, which mitigates shortcut learning and encourages reliance on semantic and temporal cues.</p>
<p>META-LEARNING</p>
<p>During training, the adaptation module f adapt is updated at each timestep using the self-supervised loss ℓ self .Following the gradient-based meta-learning paradigm [35], we differentiate through these adaptation updates with respect to the supervised prediction loss L pred .This optimizes the initialization θ 0 of f adapt , the linear projections P K , P V , P Q , and the regression head h.Formally, the total training loss combines the supervised prediction loss L pred with the self-supervised loss ℓ self , weighted by a scalar λ.The learned parameters θ 0 serve as the initialization for adaptation at inference time.This approach enables the value function estimator to learn to adapt its internal representations at test time that improves value function estimation.</p>
<p>DISSIMILARITY-BASED SAMPLING</p>
<p>Expert visual trajectories often contain consecutive frames that are highly redundant, as noted in prior work on video action recognition [43].Such redundancy can encourage the value function estimator to exploit shortcut cues [36,44], for example, by overfitting to frequently occurring late-stage visual patterns.To mitigate this, we propose a dissimilarity-based sampling strategy that constructs mini-batches from the most visually diverse sub-trajectories within each trajectory.This increases intra-batch variance and acts as a form of importance sampling, emphasizing underrepresented but semantically meaningful segments.</p>
<p>Formally, given a sequence of multimodal representations {z 1 , . . ., z T }, we extract fixed-length sub-trajectories using a sliding window of size w tr and stride s, yielding a candidate set W. A diverse subset of size k maximizes pairwise dissimilarity: where each w i is a sub-trajectory of length w tr .Directly solving Eq. 4 is intractable, requiring enumeration of all |W| k subsets.To approximate this objective efficiently, we adopt a scoring-based heuristic, assigning each window w ∈ W a score equal to its total dissimilarity with all others:
W ′ = arg max U ⊂W, |U |=k {w i ,w j }∈( U 2 ) ∥w i − w j ∥ 2 2 ,(4)s(w) = v∈W ∥w − v∥ 2 2 , W ′ = arg max U ⊂W, |U |=k w∈U s(w).(5)
Thus, by computing a diversity score s for each window and selecting the k highest-scoring windows, we obtain a heuristically diverse subset with polynomial-time complexity.The overhead is negligible compared to model training, as shown by our complexity analysis in Appendix G.We empirically validate dissimilarity-based sampling against full-trajectory sampling in the ablation study (Section 4.6.1),showing improved ability to distinguish expert from non-expert visual trajectories.</p>
<p>EXPERIMENTS</p>
<p>Our evaluation demonstrates the effectiveness of VITA across core capabilities of goal-conditioned value functions: (i) generalization in value estimation for real-world robotic manipulation under distribution shifts in task, environment, and embodiment; (ii) differentiation between expert and non-expert visual trajectories in real-world robotic manipulation tasks; (iii) effective zero-shot reward shaping for offline RL in Meta-World MT10, a simulated benchmark for multi-task robot learning.</p>
<p>TRAINING SETUP</p>
<p>We train VITA on expert visual trajectories paired with natural language task descriptions from the BridgeData V2 dataset [45], which spans a wide range of manipulation tasks, environments, and robot embodiments.In particular, we use a curated subset consisting of</p>
<p>BASELINES</p>
<p>We compare VITA against value function estimators based on contrastive and autoregressive zeroshot VLMs.VLM-CL computes zero-shot value estimates from cosine similarity between CLIP representations of each frame and task description [27].VLM-RM regularizes CLIP embeddings by projecting features along the direction from a generic reference prompt to the task prompt [28].CLIP-FT trains a supervised regression head on frozen CLIP multimodal representations.GVL [25] is the state-of-the-art zero-shot value function that leverages autoregressive VLMs.In our experiments, GVL-0S refers to the zero-shot setting, where the VLM is prompted with only the test trajectory and task description.GVL-1S corresponds to a one-shot in-context setting, where a full shuffled trajectory from the training distribution, along with its progress labels, is provided as an example.We follow GVL in using Gemini 1.5 Pro [46] with its proposed prompt template.We also tested Qwen-VL 2.5 [47], which failed to overcome temporal bias, and GPT-4o [48], which refused to produce value estimates.Additional implementation details are provided in Appendix D. We evaluate the ability of VITA to generalize across novel environments, tasks, and robot embodiments using subsets of BridgeData V2 curated to introduce variation along these axes.Environment shifts involve changes in scene layout or background.For example, lm pnp is a pick-and-place task in front of a laundry machine, while td fold, ft fold, and rd fold feature cloth folding on different surfaces.ms sweep introduces a long-horizon sweeping task in a confined tray.Embodiment shifts are evaluated using the DeepThought robot, which differs from the training embodiment (WidowX 250) in both morphology and camera perspective.dt tk pnp (pick-and-place) and dt tk stack (stacking) retain the in-distribution environment with a new embodiment, while dt ft stack (stacking) and dt rd pnp (drawer pick-and-place) involve both embodiment and environment shifts.Each dataset includes 200 expert trajectories with task descriptions, except for ms sweep, which contains 100 due to its size limitation.Appendix B provides a full description of our evaluation datasets.We evaluate the performance of a value function estimator using the Value Order Correlation (VOC) metric [25], which measures the alignment between the predicted progress towards task completion and the chronological order of the frames in a visual trajectory.Formally, let p 1 , . . ., p T denote the predicted progress values for a trajectory of T frames, and let r t = t represent the temporal index.VOC is defined as the Spearman rank correlation ρ s between the predicted values and the temporal indices.</p>
<p>EVALUATING GENERALIZATION UNDER DISTRIBUTION SHIFTS</p>
<p>Our experiments show that VITA consistently outperforms both zero-and one-shot GVL, highlighting the importance of preserving temporal order in value function estimation.CLIP-FT performs comparably to GVL on the in-distribution task but fails to generalize to out-of-distribution settings.</p>
<p>Both VLM-CL and VLM-RM perform poorly, likely due to their lack of temporal modeling.For GVL-0S and GVL-1S, in some cases, in-context examples improve performance, while in others, they provide no benefit or even degrade performance.Both GVL methods perform well on folding tasks (td fold, ft fold, rd fold), but fail to achieve comparable performance on stacking (dt tk stack, dt ft stack) and pick-and-place (dt tk pnp, lm pnp) tasks, suggesting that the autoregressive VLM used in GVL may be biased toward folding-like robotic manipulations.In contrast, VITA achieves consistent performance across all task types and distribution shifts, indicating stronger generalization than in-context learning.All methods, except CLIP-FT, perform worse on the long-horizon task ms sweep, but VITA achieves the highest VOC score.Under embodiment shifts, VITA demonstrates strong generalization.In dt tk pnp, which uses a different robot embodiment but shares the same environment and tasks as the training set, VITA exceeds their in-distribution performance, indicating that test-time adaptation can effectively transfer across robot embodiments.In contrast, GVL-1S performs poorly on both dt tk pnp and dt tk stack, suggesting that few-shot learning fails to generalize under embodiment shifts in our evaluation setup.</p>
<p>DIFFERENTIATING EXPERT FROM NON-EXPERT TRAJECTORIES</p>
<p>Beyond generalization, we evaluate VITA's robustness by testing its ability to distinguish expert from suboptimal visual trajectories, assigning lower progress scores to the latter.To this end, we compare model predictions on expert and scripted (non-expert) visual trajectories collected from the same in-distribution setting of BridgeData V2.The scripted trajectories are generated in the ToyKitchen environment using a heavily randomized controller [45], under the same embodiment and environmental configuration as the expert demonstrations.These include pick-and-place tasks involving object categories that may overlap with the training set, such as general objects (sc pnp obj), rigid items (sc pnp robj), soft toys (sc pnp stoy), and utensils (sc pnp uten).While these trajectories match the training distribution in task, embodiment, and environment, their behavior deviates from the smooth and monotonic progress typically exhibited by expert demonstrations.Appendix C includes examples from the scripted dataset.As scripted datasets are generated by a randomized controller, they are not designed to reflect quantifiable levels of suboptimality.Nonetheless, an effective value function estimator should assign lower VOC scores to suboptimal trajectories relative to expert ones when evaluated in the same setting.We measure discriminative success with BinVOC, a binary evaluation metric defined as 1 [VOC exp &gt; VOC subopt ], which is 1 when expert trajectories achieve higher VOC than suboptimal ones and 0 otherwise.The model's discriminative performance is evaluated using BINVOC, averaged over all scripted datasets.Performance on expert visual trajectories (VOC exp ) is measured on the in-distribution test set tk pnp, which features pick-and-place tasks similar to those in the scripted evaluation.VITA, GVL-0S, and GVL-1S achieve perfect discrimination, consistently assigning higher VOC scores to expert trajectories across all tasks.CLIP-based baselines (VLM-CL, VLM-RM) likely underperform due to the absence of temporal modeling.CLIP-FT performs more reliably, but fails on one task, suggesting sensitivity to object distribution shifts.</p>
<p>ZERO-SHOT REWARD SHAPING FOR OFFLINE RL</p>
<p>We evaluate our value function estimator on the Meta-World MT10 benchmark [38], which consists of ten diverse robotic manipulation tasks, including pick-and-place, door opening, and pushing.VITA is trained on real-world robotic data and evaluated zero-shot in this simulated setting.For each task, we generate 20 expert visual demonstrations using Meta-World's expert policies.The goal-conditioned value estimator is then used to define a dense reward for each visual observation.Policies are trained with Implicit Q-Learning (IQL) [49] in the offline RL setting.We repeat expert data generation and policy training across 10 random seeds.After training, each multi-task policy is evaluated on 20 rollouts per task, and we report the average return across episodes for each (task, seed) pair.We report the interquartile mean (IQM) across all pairs, together with 95% stratified bootstrap confidence intervals, following the evaluation protocol proposed by Agarwal et al. [50].The training details and evaluation protocol are further described in Appendix E. We compare the impact of different sub-trajectory sampling strategies used during training to improve test-time adaptation.Full-trajectory sampling [34] computes the adaptation loss over all overlapping sub-trajectories with stride s = 1, which in our setup leads to overfitting to global temporal shortcuts.Random sampling improves diversity by selecting sub-trajectories uniformly, but lacks semantic diversity.In contrast, dissimilarity-based sampling explicitly promotes semantic diversity, resulting in better generalization.Our proposed method, which selects sub-trajectories that maximize pairwise dissimilarity, outperforms both full-trajectory and random sampling in differentiating expert and non-expert visual trajectories.For further details, please refer to Appendix F.1.</p>
<p>EFFECT OF TEMPORAL MEMORY ON VALUE FUNCTION ESTIMATION</p>
<p>We analyze how different adaptation strategies for incorporating temporal context affect value function estimation.VITA performs step-by-step adaptation without reset (implicit memory), incrementally updating parameters from θ t−1 so that the value function estimator encodes trajectory history directly into its parameters.We compare VITA with two alternatives: (i) trajectory-level adaptation (offline), where updates are applied only once per trajectory; and ii) no-temporal adaptation, where the adaptation module is reset to θ 0 at every step and updated using only the current visual observation.Across all datasets, VITA consistently outperforms both trajectory-level and non-temporal adaptation, suggesting that incrementally encoding trajectory history into the parameters of the value function estimator is more effective.We provide the full analysis in Appendix F.2.</p>
<p>RELATED WORK</p>
<p>VLMS AS VALUE FUNCTION ESTIMATORS</p>
<p>Contrastive VLMs have been applied as zero-shot reward models [27,28] and goal-conditioned value functions [24,25], using frame-level similarity scores but lacking temporal modeling.Some works fine-tune CLIP on domain-specific video datasets [51,52] to generate dense rewards, requiring supervision that limits their zero-shot applicability.RoboCLIP [33] enables one-shot reward specification without finetuning, but yields sparse rewards and still requires demonstrations.Large-scale multimodal pretraining approaches [31,53,24] learn language-conditioned value functions by relying on scale for generalization.ReWiND [32] trains a cross-modal sequential transformer on augmented large-scale data with video rewinding, combined with a few in-domain demonstrations to improve generalization.In contrast, VITA adapts a pretrained contrastive vision-language encoder at inference, capturing both temporal and semantic context through test-time adaptation, without requiring large-scale multimodal pretraining or additional domain-specific demonstrations for generalization.</p>
<p>VITA could, in principle, be applied to any vision-language representation learning method for decision-making.Autoregressive VLMs [2] leverage in-context learning over trajectories and have been applied to success detection [29] and goal-conditioned value functions [25].GVL [25] mitigates monotonic bias from ordered pretraining data by shuffling frames at inference, but this discards temporal order, which is essential for temporal reasoning and for distinguishing visually similar yet temporally distinct states.Our approach instead preserves chronological order while capturing temporal context through test-time adaptation, avoiding such shortcuts.</p>
<p>TEST-TIME ADAPTATION</p>
<p>Test-time adaptation methods provide parameter-efficient solutions for adapting VLMs without the need for full fine-tuning.Prompt-based methods, for example, optimize model parameters at test time based on the task context, although their underlying mechanisms are opaque [54,55,56].Lim et al. [57] suggested that CLIP representations could support test-time adaptation, based on the observation that CLIP encodes human-interpretable concepts discovered via mechanistic interpretability [58].CLIP-based similarity has also been used as a reward signal, with weights optimized at test time via reinforcement learning [59].Another approach is test-time training [39], where an adaptation module is updated at each timestep via a self-supervised loss.Unlike meta-reinforcement learning methods [35,60], which require task-specific adaptation episodes during training, test-time training enables direct online adaptation during inference, without the need for task labels.</p>
<p>DISCUSSION, LIMITATIONS, AND FUTURE WORK</p>
<p>Discussion.We show that test-time adaptation enables value functions for robotic manipulation to generalize across distribution shifts in task, environment, and embodiment.By updating sequentially over a trajectory, VITA encodes history into its parameters, capturing temporal and semantic context more effectively than CLIP-based baselines and in-context learning with autoregressive VLMs.This may be due to the fact that in-context learning approaches, despite their generalization capabilities, are not explicitly trained for progress estimation, which requires temporal reasoning over visual trajectories.In addition, VITA can distinguish expert from non-expert visual trajectories and perform zero-shot reward shaping, enabling downstream applications in RL and imitation learning.</p>
<p>Limitations.Test-time adaptation improves generalization of zero-shot progress estimation across unseen tasks and environments in our experiments, but it can still face challenges in settings with high execution variability or extended durations.Although the adaptation overhead is negligible due to the lightweight adaptation module, updating a value function estimator at every timestep may be potentially unsafe during deployment, limiting applicability in real-time scenarios.</p>
<p>Future Work.We plan to explore alternative approaches for test-time adaptation of vision-language models, particularly in the context of training agents within world models.While we provide empirical evidence that dissimilarity-based sampling mitigates shortcut learning, a theoretical analysis of how diversity-based sampling influences shortcut learning remains a promising direction.This lies beyond the scope of our core contribution, which introduces a value function estimator that meta-learns a test-time adaptation mechanism to improve zero-shot performance and temporal reasoning.</p>
<p>A TRAINING DATASET   Each visual observation and task description is encoded using the CLIP vision and text encoders to produce their representation, respectively.We opted for a joint (concatenation-based) representation over an element-wise product based on improved validation performance.The adaptation module f adapt is a two-layer residual MLP with GELU activation and a projection dimension d ′ = 64.The model is trained using the AdamW optimizer with a learning rate of 1 × 10 −4 , weight decay of 1 × 10 −4 , and a cosine learning rate schedule with 10% warmup.We use a batch size of 32 and pad all trajectories to a maximum length of 120 frames (matching the longest sequence in the training set).The weighting coefficient λ self of the self-supervised loss in the total training objective is set to 0.5, selected based on validation performance.We train for 5 epochs, as validation VOC typically plateaus early, with extended training providing no further improvement.For dissimilarity-based sampling, we set the stride to s = 2, the sub-trajectory length to w tr = 8, and the number of selected windows to k = 16, unless otherwise specified.All experiments were run on NVIDIA RTX 6000 Ada Generation GPUs.</p>
<p>B EVALUATION DATASETS</p>
<p>D.2 TEST-TIME TRAINING HYPERPARAMETERS</p>
<p>At inference time, we adapt only the temporal adaptation module f adapt , using the same projection dimension (d ′ = 64) as in training.We perform adaptation over a single gradient step (t ep = 1), using a learning rate of 0.1.This configuration was selected from a sweep over learning rates {5.0, 1.0, 0.1, 0.01} and adaptation steps {1, 5, 10}, based on performance on the validation set.Unless otherwise noted, the adaptation module is not reset between evaluation episodes.</p>
<p>D.3 BASELINES</p>
<p>VITA uses t ep = 1, projection dimension d ′ = 64, and learning rates η of 0.1 and 1.0, respectively, selected via hyperparameter tuning.CLIP-FT shares the same architecture as our method but excludes the adaptation module and self-supervised loss.It uses a frozen CLIP encoder, followed by a linear projection and a two-layer MLP to predict task progress.The model is trained with standard supervised regression, without meta-learning or test-time adaptation.To increase expressivity, it uses an 8× larger projection matrix and 10× more training steps than our method [61].For VLM-RM, we use a baseline prompt that describes the environment.For bothGVL-0S and GVL-1S, we use the latest version of Gemini 1.5 Pro, gemini-1.5-pro-latest[46], whereas the original GVL implementation used an earlier release, gemini-1.5-pro.We also evaluated GVL the open-source autoregressive VLM Qwen-VL 2.5 [47], which failed to overcome temporal bias despite frame shuffling, while GPT-4o [48] consistently declined to produce scalar progress estimates in our setup.All evaluations followed the prompt template introduced in the original GVL work [25].</p>
<p>E OFFLINE RL SETUP E.1 TRAINING CONFIGURATION</p>
<p>We use Implicit Q-Learning (IQL) with the following settings, kept fixed across all tasks.Expectile regression is set to τ = 0.7, with advantage weighting temperature 3.0 and clipping threshold 100.0.The actor and critic use learning rates 1 × 10 −4 and 3 × 10 −4 , respectively, with batch size 256.Policies are trained for 100,000 gradient steps, with evaluation every 10,000 steps on 20 rollouts per task (horizon 150).Implementation follows d3rlpy, with configuration: expectile=0.7,weight temp=3.0,max weight=100.0,actor lr=1e-4, critic lr=3e-4, batch size=256.</p>
<p>E.2 EVALUATION PROTOCOL</p>
<p>Following Agarwal et al. [50], we report the interquartile mean (IQM) of returns across all task-seed pairs.The IQM computes the mean of the middle 50% of scores, which reduces sensitivity to outliers compared to the mean or median.For statistical robustness, we estimate 95% confidence intervals using stratified bootstrap with 10,000 resamples, stratified by task.This procedure is applied consistently across all methods in Meta-World MT10 experiments.We evaluate how different sub-trajectory sampling strategies during training impact the ability of our model to discriminate expert from suboptimal trajectories.In Table 4, we compute Disc@VOC for 4 scripted datasets, capturing whether a given model assigns a higher average VOC to expert trajectories (tk pnp) than to each evaluation dataset.We compare three strategies: (1) Full-trajectory sampling applies adaptation over all overlapping sub-trajectories of length w tr using stride 1, following prior work [34]; (2) Random sampling uniformly samples k sub-trajectories of length w tr per video; and</p>
<p>(3) our proposed Dissimilarity-based sampling constructs a candidate set using stride ⌊w tr /2⌋, and selects k sub-trajectories that maximize pairwise dissimilarity in the representation space.</p>
<p>Figure 1 :
1
Figure 1: Overview of VITA.(a) Using gradient-based meta-learning, VITA learns a goalconditioned value function from expert visual trajectories and language descriptions, with a selfsupervised objective ℓ self trained such that test-time updates minimize the value prediction loss.(b)During inference, the adaptation module f adapt is updated with a gradient step on ℓ self , enabling zero-shot value estimation on OOD trajectories while encoding temporal context in its parameters.</p>
<p>Figure 2 :
2
Figure 2: Examples of visual trajectories paired with task descriptions under different distribution shifts.(a) In-distribution.(b, c) Environment shift.(d) Embodiment and environment shift.</p>
<p>Figure 3 :
3
Figure 3: Each subfigure shows the start and end frames from an expert demonstration used for training, along with its natural language task description.Demonstrations are collected across four distinct ToyKitchen environments.</p>
<p>Figure 4 :Figure 5 :Figure 6 :
456
Figure4: Each subfigure shows the start and end frames from an evaluation trajectory under embodiment shift, along with its natural language task description.The top row depicts tasks in the same environment (ToyKitchen) using a different robot (DeepThought), while the bottom row includes tasks that also involve new environments.</p>
<p>F</p>
<p>ABLATION STUDIES F.1 EFFECT OF DISSIMILARITY-BASED SAMPLING ON DISCRIMINATIVE PERFORMANCE Table 4: Disc@VOC results across scripted datasets under different training-time sampling strategies for test-time adaptation.A checkmark indicates successful discrimination between expert and suboptimal trajectories Each method is denoted as Type(w train , k), where: FT = full-trajectory, Rand = random sampling, Diss = dissimilarity-based sampling.</p>
<p>Table 1 :
1
VOC scores for value function estimation under distribution shifts relative to the training distribution.ID = In-Distribution, ES = Environment Shift, EM = Embodiment Shift, ES &amp; EM = Environment and Embodiment Shift.
ShiftDatasetVLM-CL VLM-RM CLIP-FT GVL-0S GVL-1S VITAIDtk pnp0.0380.0290.2510.2690.2520.782lm pnp0.0170.0330.1490.3050.2720.725td fold0.0310.0720.1520.3260.3180.709ESft fold0.1080.0990.1620.3310.3870.658rd fold0.0950.0550.1260.3720.4060.606ms sweep-0.129-0.2260.1480.1580.1500.490EMdt tk pnp dt tk stack0.042 0.035-0.041 0.0460.149 0.0990.258 0.2540.211 0.2770.820 0.708ES &amp; EMdt ft stack dt rd pnp0.026 0.0230.028 0.0410.049 0.2110.212 0.3290.249 0.3160.698 0.695</p>
<p>Table 2
2MethodBinVOCMethodIQM95% CIVLM-CL0.25VLM-CL0.760[0.722, 0.791]VLM-RM0.00VLM-RM0.746[0.718, 0.771]CLIP-FT0.75CLIP-FT0.785[0.759, 0.809]GVL-0S1.00GVL-0S--GVL-1S1.00GVL-1S--META-WL-META-WL0.779[0.750, 0.804]VITA1.00VITA0.815<a href="a">0.785, 0.838</a> Expert vs. Non-Expert (BinVOC).(b) Offline RL on MT10 (IQM).
reports performance on MT10.VITA achieves the highest IQM return (0.815), outperforming all CLIP-based baselines.A direct comparison with GVL is infeasible at scale, since it relies on Gemini-1.5, a proprietary autoregressive VLM with prohibitively high inference cost in RL settings, while open-source alternatives did not yield reliable progress estimates in our preliminary tests.We also report the fuzzy-logic dense reward provided by Meta-World (META-WL), which achieves</p>
<p>Table 2 :
2
(a) Success in distinguishing expert from scripted robot demonstrations, measured by average BinVOC across 4 in-distribution scripted datasets.(b) Offline RL performance on the Meta-World MT10 benchmark measured by interquartile mean (IQM) with 95% stratified bootstrap CIs.0.779.Despite its strong performance, META-WL is outperformed by VITA, indicating that a value estimator trained on real-world data can generalize effectively to simulated reward shaping.
4.6 ABLATION STUDIES4.6.1 EFFECT OF DISSIMILARITY-BASED SAMPLING ON DISCRIMINATIVE PERFORMANCE</p>
<p>Table 3 :
3
Dataset descriptions with task type, environment, and embodiment.
DatasetTask TypeEnvironmentEmbodimenttk pnppick-and-place toy kitchenWidowX 250lm pnppick-and-place laundry machineWidowX 250td foldfold clothtabletop (dark wood) WidowX 250ft foldfold clothfolding tableWidowX 250rd foldfold clothrobot deskWidowX 250ms sweepsweepfolding table (tray)WidowX 250dt tk pnppick-and-place toy kitchenDeepThoughtdt tk stack stack blockstoy kitchenDeepThoughtdt ft stack stack blocksfolding tableDeepThoughtdt rd pnppick-and-place robot desk (drawer)DeepThought
ACKNOWLEDGMENTSThis work was supported by UKRI (EP/Y037111/1) as part of the ProSafe project (EU Horizon 2020, MSCA, grant no.101119358).Our results show that dissimilarity-based sampling with w tr = 8 and k = 8 yields perfect discriminative performance (4/4).Sampling fewer windows (k = 4) of size w tr = 8 leads to the same performance, while reducing sub-trajectory length to w tr = 4 reduces effectiveness.The full-trajectory baseline performs the worst, confirming that adapting over all consecutive windows can lead to overfitting to global temporal cues.Random sampling outperforms full-trajectory sampling but remains less reliable than dissimilarity-based selection.These findings highlight the importance of both sub-trajectory diversity and semantic locality in enabling discriminative test-time adaptation.F.2 TRAJECTORY-LEVEL VS.STEP-BY-STEP ADAPTATION We evaluate three adaptation strategies: (1) TTT-OF, which updates the adaptation module once per trajectory (offline); (2) TTT-VIS, which resets the adaptation module at every step and updates using only the current visual observation (no-temporal); and (3) VITA (Ours), which updates the adaptation module incrementally per trajectory (implicit memory).All approaches use the same architecture, initialization, and hyperparameters.As shown in Table5, TTT-OF and TTT-VIS achieve nearly identical results, indicating that applying a single update per trajectory offers no advantage over resetting at each step.In contrast, VITA consistently outperforms TTT-OF and TTT-VIS, highlighting the benefit of retaining temporal context throughout the trajectory.G COMPLEXITY ANALYSIS OF DISSIMILARITY-BASED SAMPLINGLet W denote the candidate set of size N c = |W|.To approximate diverse subset selection without the exponential cost of enumerating all Nc b subsets as described in Eq. 4, we adopt a scoring-based heuristic that assigns each candidate window a diversity score equal to the sum of its pairwise distances to all others (i.e., the row sum of the distance matrix).Let the number of candidate windows be N c = T − w tr + 1, each represented by a flattened feature vector of dimension m = w tr • d, where d is the dimension of each multimodal representation z t .We compute the full pairwise distance matrix D ∈ R Nc×Nc on the GPU using batched cdist, which requires O(N 2 c • w tr • d) operations.In our experiments, training trajectories are short (mean length 69, max length 120).With a batch size of 32, feature dimension d = 1024 (CLIP-based multimodal representation), and window length w tr = 8, the worst-case computational cost is:This overhead is negligible compared to the cost of a single forward and backward pass through our model, while promoting the selection of diverse windows.
Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, Proceedings of the 38th International Conference on Machine Learning (ICML). the 38th International Conference on Machine Learning (ICML)2021139</p>
<p>Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Paul Luc, Antoine Miech, Ian Barr, Arthur Mensch, Kathryn Millican, Malcolm Reynolds, Sebastian Borgeaud, Andrew Brock, arXiv:2204.141982022arXiv preprint</p>
<p>Rt-2: Visionlanguage-action models transfer web knowledge to robotic control. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Kumar, Chelsea Dubey, Finn, Proceedings of The 7th Conference on Robot Learning. The 7th Conference on Robot LearningPMLR2023</p>
<p>Octo: An open-source generalist robot policy. Dibya Ghosh, Homer Rich Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, Robotics: Science and Systems. 2024</p>
<p>Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Michael Gimenez, Yury Sulsky, Jack Kay, arXiv:2205.06175Jost Tobias Springenberg, et al. A generalist agent. 2022arXiv preprint</p>
<p>Generative adversarial imitation learning. Jonathan Ho, Stefano Ermon, Advances in Neural Information Processing Systems. 2016</p>
<p>World models. David Ha, Jürgen Schmidhuber, Advances in Neural Information Processing Systems. 2018</p>
<p>Mastering atari with discrete world models. Danijar Hafner, Timothy P Lillicrap, Mohammad Norouzi, Jimmy Ba, 9th International Conference on Learning Representations (ICLR). 2021</p>
<p>Deep learning, reinforcement learning, and world models. Yutaka Matsuo, Yann Lecun, Maneesh Sahani, Doina Precup, David Silver, Masashi Sugiyama, Eiji Uchibe, Jun Morimoto, Neural Networks. 1522022</p>
<p>Unisim: A neural closed-loop sensor simulator. Ze Yang, Yun Chen, Jingkang Wang, Sivabalan Manivasagam, Wei-Chiu Ma, Anqi Joyce Yang, Raquel Urtasun, 10.1109/CVPR52729.2023.00140IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE2023</p>
<p>Navigation world models. Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, Yann Lecun, 2024</p>
<p>Cosmos world foundation model platform for physical ai. Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, arXiv:2501.035752025arXiv preprint</p>
<p>Gaia-1: A generative world model for autonomous driving. Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, Gianluca Corrado, 10.48550/arXiv.2309.170802023</p>
<p>Genie: Generative interactive environments. Jake Bruce, Michael D Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, Yusuf Aytar, Sarah Bechtle, M P Feryal, Behbahani, C Y Stephanie, Nicolas Chan, Lucy Heess, Simon Gonzalez, Sherjil Osindero, Scott E Ozair, Jingwei Reed, Konrad Zhang, Jeff Zolna, Clune, Proceedings of the 41st International Conference on Machine Learning (ICML). Satinder Freitas, Tim Singh, Rocktäschel, the 41st International Conference on Machine Learning (ICML)2024</p>
<p>Position: Open-endedness is essential for artificial superhuman intelligence. Edward Hughes, Michael D Dennis, Jack Parker-Holder, M P Feryal, Aditi Behbahani, Yuge Mavalankar, Tom Shi, Tim Schaul, Rocktäschel, Proceedings of the 41st International Conference on Machine Learning (ICML). the 41st International Conference on Machine Learning (ICML)2024</p>
<p>Welcome to the era of experience. David Silver, Richard S Sutton, 2025Google AITechnical report</p>
<p>Imitating latent policies from observation. Ashley Edwards, Himanshu Sahni, Yannick Schroecker, Charles Isbell, International conference on machine learning. PMLR2019</p>
<p>Video pretraining (vpt): Learning to act by watching unlabeled online videos. Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, Jeff Clune, Advances in Neural Information Processing Systems. 202235</p>
<p>Learning to act without actions. Dominik Schmidt, Minqi Jiang, International Conference on Learning Representations (ICLR). 2024</p>
<p>What do latent action models actually learn?. Chuheng Zhang, Tim Pearce, Pushi Zhang, Kaixin Wang, Xiaoyu Chen, Wei Shen, Li Zhao, Jiang Bian, arXiv:2506.156912025arXiv preprint</p>
<p>Learning generalizable robotic reward functions from" in-the-wild" human videos. Suraj Annie S Chen, Chelsea Nair, Finn, arXiv:2103.168172021arXiv preprint</p>
<p>Vip: Towards universal visual reward and representation via value-implicit pre-training. Jason Yecheng, Shagun Ma, Dinesh Sodhani, Osbert Jayaraman, Vikash Bastani, Amy Kumar, Zhang, arXiv:2210.000302022arXiv preprint</p>
<p>Generalizable imitation learning from observation via inferring goal proximity. Youngwoon Lee, Andrew Szot, Shao-Hua Sun, Joseph J Lim, Advances in Neural Information Processing Systems (NeurIPS). Aurelio Marc, Alina Ranzato, Yann N Beygelzimer, Percy Dauphin, Jennifer Wortman Liang, Vaughan, 202134</p>
<p>LIV: Language-image representations and rewards for robotic control. Jason Yecheng, Vikash Ma, Amy Kumar, Osbert Zhang, Dinesh Bastani, Jayaraman, Proceedings of the 40th International Conference on Machine Learning (ICML). the 40th International Conference on Machine Learning (ICML)PMLR2023202of Proceedings of Machine Learning Research</p>
<p>Vision language models are in-context value learners. Jason Yecheng, Joey Ma, Ayzaan Hejna, Chuyuan Wahid, Dhruv Fu, Jacky Shah, Zhuo Liang, Sean Xu, Peng Kirmani, Danny Xu, Ted Driess, Jonathan Xiao, Osbert Tompson, Dinesh Bastani, Wenhao Jayaraman, Tingnan Yu, Dorsa Zhang, Fei Sadigh, Xia, arXiv:2411.045492024arXiv preprint</p>
<p>Viva: Video-trained value functions for guiding online rl from diverse data. Nitish Dashora, Dibya Ghosh, Sergey Levine, arXiv:2503.182102025arXiv preprint</p>
<p>Kate Baumli, Satinder Baveja, M P Feryal, Harris Behbahani, Gheorghe Chan, Sebastian Comanici, Maxime Flennerhag, Kristian Gazeau, Dan Holsheimer, Michael Horgan, Clare Laskin, Hussain Lyle, Kay Masoom, Volodymyr Mckinney, Alexander Mnih, Fabio Neitz, Jack Pardo, John Parker-Holder, Tim Quan, Himanshu Rocktäschel, Tom Sahni, Yannick Schaul, Stephen Schroecker, Richie Spencer, Luyu Steigerwald, Lei Wang, Zhang, arXiv:2312.09187Vision-language models as a source of rewards. 2023arXiv preprint</p>
<p>Visionlanguage models are zero-shot reward models for reinforcement learning. Juan Rocamonde, Victoriano Montesinos, Elvis Nava, Ethan Perez, David Lindner, Proceedings of the Twelfth International Conference on Learning Representations (ICLR). the Twelfth International Conference on Learning Representations (ICLR)2024</p>
<p>Vision-language models as success detectors. Yuqing Du, Ksenia Konyushkova, Misha Denil, Akhil Raju, Jessica Landon, Felix Hill, Nando De Freitas, Serkan Cabi, Proceedings of the Conference on Lifelong Learning Agents (CoLLAs). the Conference on Lifelong Learning Agents (CoLLAs)PMLR2023232of Proceedings of Machine Learning Research</p>
<p>Perception test: A diagnostic benchmark for multimodal video models. Yusuf Viorica Pȃtrȃucean, Roozbeh Aytar, Oleh Mottaghi, Mario Rybkin, Carl Lucic, William T Vondrick, Andrew Freeman, Zisserman, NeurIPS Datasets and Benchmarks Track. 2023</p>
<p>Minttu Alakuijala, Reginald Mclean, Isaac Woungang, Nariman Farsad, Samuel Kaski, Pekka Marttinen, Kai Yuan, arXiv:2405.19988Video-language critic: Transferable reward functions for languageconditioned robotics. 2024arXiv preprint</p>
<p>RewiND: Language-guided rewards teach robot policies without new demonstrations. Jiahui Zhang, Yusen Luo, Abrar Anwar, Anand Sumedh, Joseph J Sontakke, Jesse Lim, Erdem Thomason, Jesse Biyik, Zhang, 9th Annual Conference on Robot Learning. 2025</p>
<p>Roboclip: One demonstration is enough to learn robot policies. Sumedh Sontakke, Jesse Zhang, M R Sébastien, Karl Arnold, Erdem Pertsch, Dorsa Biyik, Chelsea Sadigh, Laurent Finn, Itti, Advances in Neural Information Processing Systems 36 (NeurIPS). 2023</p>
<p>Learning to (learn at test time): Rnns with expressive hidden states. Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, Carlos Guestrin, 10.48550/arXiv.2407.046202024</p>
<p>Model-agnostic meta-learning for fast adaptation of deep networks. Chelsea Finn, Pieter Abbeel, Sergey Levine, International conference on machine learning. PMLR2017</p>
<p>Shortcut learning in deep neural networks. Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, Felix A Wichmann, Nature Machine Intelligence. 2112020</p>
<p>Offline reinforcement learning: Tutorial, review, and perspectives on open problems. Sergey Levine, Aviral Kumar, George Tucker, Justin Fu, arXiv:2005.016432020arXiv preprint</p>
<p>Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, Sergey Levine, Conference on robot learning. PMLR2020</p>
<p>Test-time training with self-supervision for generalization under distribution shifts. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A Efros, Moritz Hardt, Proceedings of the 37th International Conference on Machine Learning (ICML). the 37th International Conference on Machine Learning (ICML)PMLR2020119of Proceedings of Machine Learning Research</p>
<p>Ian Goodfellow, Yoshua Bengio, Aaron Courville, Yoshua Bengio, Deep learning. MIT Press20161</p>
<p>Test-time training on video streams. Renhao Wang, Yu Sun, Yossi Gandelsman, Xinlei Chen, Alexei A Efros, Xiaolong Wang, 10.48550/arXiv.2307.050142023</p>
<p>Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, International conference on machine learning. PMLR2018</p>
<p>Temporal segment networks: Towards good practices for deep action recognition. Limin Wang, Yuanjun Xiong, Dahua Lin, Luc Van Gool, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4092018</p>
<p>Generalizable imitation learning from observation via inferring goal proximity. Youngwoon Lee, Andrew Szot, Shao-Hua Sun, Joseph J Lim, Advances in Neural Information Processing Systems. 2021</p>
<p>Bridgedata v2: A dataset for robot learning at scale. Homer Rich Walke, Kevin Black, Tony Z Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, Andre Wang He, Vivek Myers, Jin Moo, Max Kim, Abraham Du, Kuan Lee, Chelsea Fang, Sergey Finn, Levine, Proceedings of the 7th Conference on Robot Learning (CoRL). the 7th Conference on Robot Learning (CoRL)PMLR2023229of Proceedings of Machine Learning Research</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Gemini Team, Petko Georgiev, Ian Ving, Ryan Lei, Libin Burnell, Anmol Bai, Garrett Gulati, Damien Tanzer, Zhufeng Vincent, Shibo Pan, Wang, arXiv:2403.055302024arXiv preprint</p>
<p>An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, arXiv:2412.15115Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. Xingzhang Ren, Xuancheng Ren,2024arXiv preprint</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>Offline reinforcement learning with implicit q-learning. Ilya Kostrikov, Ashvin Nair, Sergey Levine, arXiv:2110.061692021arXiv preprint</p>
<p>Deep reinforcement learning at the edge of the statistical precipice. Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, Marc Bellemare, Advances in neural information processing systems. 202134</p>
<p>Building open-ended embodied agents with internet-scale knowledge. Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An, Yuke Huang, Anima Zhu, Anandkumar, Minedojo, Advances in Neural Information Processing Systems 35 (NeurIPS). 2022</p>
<p>Reinforcement learning friendly vision-language model for minecraft. Haobin Jiang, Junpeng Yue, Hao Luo, Ziluo Ding, Zongqing Lu, European Conference on Computer Vision. Springer2024</p>
<p>Jianxiong Li, Jinliang Zheng, Yinan Zheng, Liyuan Mao, Xiao Hu, Sijie Cheng, Haoyi Niu, Jihao Liu, Yu Liu, Jingjing Liu, arXiv:2402.18137Embodied multimodal representations via implicit preference learning. 2024arXiv preprint</p>
<p>Learning to prompt for vision-language models. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu, International Journal of Computer Vision. 13092022</p>
<p>Maple: Multi-modal prompt learning. Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, Fahad Shahbaz Khan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2023</p>
<p>Swapprompt: Test-time prompt adaptation for vision-language models. Xudong Ma, Chen Liang, Zhecan Wang, Chuang Gan, Zhiting Li, Advances in Neural Information Processing Systems (NeurIPS). 2023</p>
<p>Sparse autoencoders reveal selective remapping of visual concepts during adaptation. Hyesu Lim, Jinho Choi, Jaegul Choo, Steffen Schneider, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Towards monosemanticity: Decomposing language models with dictionary learning. Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden Mclean, Josiah E Burke, Tristan Hume, Shan Carter, Tom Henighan, Christopher Olah, Transformer Circuits Thread. 2023</p>
<p>Test-time adaptation with clip reward for zero-shot generalization in vision-language models. Shuyuan Zhao, Xin Wang, Linchao Zhu, Yi Yang, arXiv:2401.123452024arXiv preprint</p>
<p>Human-timescale adaptation in an open-ended task space. Jakob Bauer, Kate Baumli, M P Feryal, Avishkar Behbahani, Nathalie Bhoopchand, Michael Bradley-Schmieg, Natalie Chang, Adrian Clay, Vibhavari Collister, Lucy Dasagi, Karol Gonzalez, Edward Gregor, Sheleem Hughes, Maria Kashem, Hannah Loks-Thompson, Jack Openshaw, Shreya Parker-Holder, Nicolas Perez Pathak, Nemanja Nieves, Tim Rakicevic, Yannick Rocktäschel, Satinder Schroecker, Jakub Singh, Karl Sygnowski, Sarah Tuyls, Alexander York, Lei M Zacherl, Zhang, Proceedings of the 40th International Conference on Machine Learning (ICML). the 40th International Conference on Machine Learning (ICML)2023202</p>
<p>Frozen clip models are efficient video learners. Ziyi Lin, Honglie Liu, Andrew Zisserman, Andrea Vedaldi, European Conference on Computer Vision (ECCV). 2022</p>            </div>
        </div>

    </div>
</body>
</html>
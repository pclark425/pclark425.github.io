<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7209 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7209</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7209</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-271269996</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.12844v1.pdf" target="_blank">$\texttt{metabench}$ -- A Sparse Benchmark to Measure General Ability in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) vary in their abilities on a range of tasks. Initiatives such as the $\texttt{Open LLM Leaderboard}$ aim to quantify these differences with several large benchmarks (sets of test items to which an LLM can respond either correctly or incorrectly). However, high correlations within and between benchmark scores suggest that (1) there exists a small set of common underlying abilities that these benchmarks measure, and (2) items tap into redundant information and the benchmarks may thus be considerably compressed. We use data from $n>5000$ LLMs to identify the most informative items of six benchmarks, ARC, GSM8K, HellaSwag, MMLU, TruthfulQA and WinoGrande (with $d=28,632$ items in total). From them we distill a sparse benchmark, $\texttt{metabench}$, that has less than $3\%$ of the original size of all six benchmarks combined. This new sparse benchmark goes beyond point scores by yielding estimators of the underlying benchmark-specific abilities. We show that these estimators (1) can be used to reconstruct each original $\textit{individual}$ benchmark score with, on average, $1.5\%$ root mean square error (RMSE), (2) reconstruct the original $\textit{total}$ score with $0.8\%$ RMSE, and (3) have a single underlying common factor whose Spearman correlation with the total score is $r = 0.93$.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Cogbench: A large language model walks into a psychology lab. <em>(Rating: 2)</em></li>
                <li>Who is chatgpt? benchmarking llms' psychological portrayal using psychobench. <em>(Rating: 2)</em></li>
                <li>Evaluating General-Purpose AI with Psychometrics. <em>(Rating: 2)</em></li>
                <li>Emotional intelligence of large language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7209",
    "paper_id": "paper-271269996",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Cogbench: A large language model walks into a psychology lab.",
            "rating": 2,
            "sanitized_title": "cogbench_a_large_language_model_walks_into_a_psychology_lab"
        },
        {
            "paper_title": "Who is chatgpt? benchmarking llms' psychological portrayal using psychobench.",
            "rating": 2,
            "sanitized_title": "who_is_chatgpt_benchmarking_llms_psychological_portrayal_using_psychobench"
        },
        {
            "paper_title": "Evaluating General-Purpose AI with Psychometrics.",
            "rating": 2,
            "sanitized_title": "evaluating_generalpurpose_ai_with_psychometrics"
        },
        {
            "paper_title": "Emotional intelligence of large language models.",
            "rating": 1,
            "sanitized_title": "emotional_intelligence_of_large_language_models"
        }
    ],
    "cost": 0.0066465,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>metabench A Sparse Benchmark to Measure General Ability in Large Language Models
4 Jul 2024</p>
<p>Alex Kipnis 
Human-Centered AI
Helmholtz Munich</p>
<p>Konstantinos Voudouris 
Human-Centered AI
Helmholtz Munich</p>
<p>University of Cambridge</p>
<p>Luca M Schulze Buschoff 
Human-Centered AI
Helmholtz Munich</p>
<p>Eric Schulz 
Human-Centered AI
Helmholtz Munich</p>
<p>metabench A Sparse Benchmark to Measure General Ability in Large Language Models
4 Jul 2024A1109FF2B65B7D98159C62869CAE1934arXiv:2407.12844v1[cs.CL]
Large Language Models (LLMs) vary in their abilities on a range of tasks.Initiatives such as the Open LLM Leaderboard aim to quantify these differences with several large benchmarks (sets of test items to which an LLM can respond either correctly or incorrectly).However, high correlations within and between benchmark scores suggest that (1) there exists a small set of common underlying abilities that these benchmarks measure, and (2) items tap into redundant information and the benchmarks may thus be considerably compressed.We use data from n &gt; 5000 LLMs to identify the most informative items of six benchmarks, ARC, GSM8K, HellaSwag, MMLU, TruthfulQA and WinoGrande (with d = 28, 632 items in total).From them we distill a sparse benchmark, metabench, that has less than 3% of the original size of all six benchmarks combined.This new sparse benchmark goes beyond point scores by yielding estimators of the underlying benchmark-specific abilities.We show that these estimators (1) can be used to reconstruct each original individual benchmark score with, on average, 1.5% root mean square error (RMSE), (2) reconstruct the original total score with 0.8% RMSE, and (3) have a single underlying common factor whose Spearman correlation with the total score is r = 0.93.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) represent a significant leap forward in our quest to emulate humanlike understanding and generation of language [1].These sophisticated algorithms are designed to process, interpret, and produce language with unprecedented accuracy and fluency [2].By training on vast datasets encompassing a myriad of linguistic patterns, LLMs have demonstrated remarkable capabilities in tasks ranging from simple text completion [3] to complex problem-solving [4] and creative writing [5].Their potential to transform various fields-education [6], customer service [7], content creation [8], and beyond-has captured the imagination of researchers and practitioners alike [2,9].However, as these models grow in complexity, so does the challenge of effectively evaluating their diverse abilities [10,11].</p>
<p>Traditionally, research has relied on extensive benchmarks-vast arrays of tasks-to assess the abilities of LLMs [12,13].However, these benchmarks often measure overlapping skills, leading to redundancy and inefficiency.This paper introduces metabench, a sparse benchmark engineered to distill the essence of six prominent benchmarks-ARC [14], GSM8K [15], HellaSwag [16], MMLU [17], TruthfulQA [18], and WinoGrande [19]-into a more concise and informative benchmark.By analyzing data from over 5000 LLMs, we identified the most revealing items within these benchmarks, reducing their combined size to less than 3% of the original.We also provide adaptive testing simulations that suggest that the number of items administered to LLMs can be lowered even further.This volume reduction not only maintains the benchmarks' ability to measure underlying abilities but greatly enhances efficiency by eliminating redundancy.</p>
<p>The metabench framework leverages psychometric techniques [20,21] to estimate the latent abilities captured by these benchmarks [22].These estimates can reconstruct individual and total benchmark scores with minimal error, and reveal a single underlying factor that captures most of their variability.By refining how we measure the abilities of LLMs, metabench offers a new lens through which we can understand and improve them.Practitioners can run their own models on this efficient, cheap, yet highly informative benchmark, obtain approximated scores for whole benchmarks, and compute an overall estimate of models' general ability.Our work underscores the potential for more streamlined and psychologically meaningful evaluation methods [23,24].All reported analyses and and results are available on https://github.com/adkipnis/metabench. (3) Perform cross-validated subsampling to 350 items per benchmark.(4) Fit variants of IRT models to the remaining items, infer item information from the item parameters and select the most informative items to construct metabench.(5) Use the model fits to estimate the benchmark-specific abilities and reconstruct the original (normalized) benchmark scores as well as their mean using a generalized additive model with cross-validation.</p>
<p>Related Work</p>
<p>Recent years have seen significant advancements in the development and evaluation of Large Language Models (LLMs), resulting in an increased need for efficient benchmarking methods due to the expense of running and querying them.Traditional benchmarks, such as those used in the Open LLM Leaderboard [25], encompass a wide range of tasks.In the current work, we focus on six different benchmarks: ARC (AI2 Reasoning Challenge) [14] was designed to test the reasoning abilities of AI through a series of challenging science questions.Similarly, GSM8K (Grade School Math) [15] evaluates the mathematical reasoning capabilities of LLMs by training verifiers on complex word problems.HellaSwag [16] tests commonsense reasoning by asking models to choose the most plausible continuation of a story from several options.MMLU (Massive Multitask Language Understanding) [17] assesses the breadth of expert knowledge and reasoning skills across various disciplines, presenting a comprehensive challenge for LLMs.TruthfulQA [18] aims to measure the tendency of language models to generate truthful responses, tackling the issue of misinformation and model-generated falsehoods.Finally, WinoGrande [19] tests models on pronoun resolution, a task that requires understanding context and commonsense reasoning.We use these benchmarks to distill an informative, sparse set of items to form metabench.</p>
<p>Psychometric methods including Item Response Theory are increasingly used in the evaluation of machine learning models [21,26,27].Zhuang and colleagues [28] propose using IRT for adaptive testing of LLMs but only used this approach to distinguish between a small set of models and restricted domains and compared models to human responses.Burden and colleagues [29] use Bayesian probabilistic programming to infer agents' capabilities from diverse task performances, building computational models to link task features to system capabilities.However, these methods are applied to reinforcement learning agents and not to LLMs.There also have been several attempts to create completely new questionnaires to evaluate LLMs using psychometric tools [30,31].</p>
<p>Redundancy in benchmark items has been established, analyzed and combated by several recent works [32,33,34,35,36].Most related to our proposal is recent work by Polo et al. [37] introducing tinyBenchmarks.The authors advocate for reducing the number of evaluation examples in benchmarks to lower costs while maintaining accurate performance assessments of LLMs.They demonstrate that evaluating LLMs on curated subsets of 100 examples can reliably estimate the performance on large benchmarks like MMLU and HELM [38] within a 2% mean absolute error.However, their proposal differs from our approach on several dimensions.Polo and colleagues analyzed 395 LLMs with high-dimensional IRT models and clustering to create four smaller benchmarks.We use over 5000 LLMs, make direct use of estimated item properties by applying principled item selection, go beyond point scores by estimating multiple latent abilities and show that they jointly allow better score reconstruction for each benchmark's score and the grand mean.  2 Benchmark Distillation</p>
<p>Data Collection and Preprocessing</p>
<p>We collected openly available item-wise accuracies from Hugging Face Datasets for the six benchmarks that are part of the Open LLM Leaderboard [25]: ARC, GSM8K, HellaSwag, MMLU, TruthfulQA and WinoGrande.After excluding flagged and merged models, we obtained data from 1355 unique users, yielding n = 6875 unique LLMs and 152, 261, 495 responses in total.Out of all LLMs, 5055 had data for each of the six benchmarks.Whenever possible, we used all available LLMs per benchmark for the analyses.For the remainder of this paper, we use the terms "subjects" and "LLMs" interchangeably, since many of the methods below were designed to analyze human behavior and abilities.</p>
<p>All analyses were performed in R 4.4.0 [39].For comparability across benchmarks, we normalized the benchmark scores on a percent scale and saved them for reference before removing any items.</p>
<p>Per benchmark, we discarded subjects with the lowest 0.1% of scores to further reduce noise.Items with standard deviations below 1% (too little variability) and mean accuracies above 95% (too easy) were removed.We then estimated the part-whole correlation between each item and the total score using the point-biserial correlation coefficient, r pb .This coefficient grows if subjects who responded correctly to the item have a higher than average score, especially if the item has medium difficulty ( [40], see Appendix A.1 for details).We removed items with r pbis = 0 ± c and c was set to 0.05 except for WinoGrande with c = 0.02, as otherwise hundreds of items would have been removed.The exact number of items removed per benchmark can be found in Appendix A.2.</p>
<p>For comparability across benchmarks, we conducted a train-test-validation split of the LLMs in the following manner: For stratification, we calculated the grand average of the original benchmark scores for LLMs that ran on all six benchmarks, and used it to create a stratified 10% split as the global test set.For cross-validation per benchmark, we split off further 10% subset of the LLMs as a local validation set, this time stratifying by the specific benchmark score.</p>
<p>Cross-Validated Subsampling</p>
<p>For a set of d items it becomes quickly impossible to consider all subsets of size k, as there are d k unique subsets and any exhaustive search algorithm would have O(d!) time complexity for fixed k.Nevertheless, it is astounding how well the given benchmarks can be reduced in a theory-free way using random sampling: [34], [35] and [41] showed that random samples of benchmark items can in principle produce smaller benchmarks that retain enough information to reconstruct their original score with negligible error.One basic procedure for this is:</p>
<ol>
<li>
<p>Uniformly sample k items and calculate the subtest scores sj for each subject j.</p>
</li>
<li>
<p>Fit some regression model ŝj = g(s j ) on the training set.</p>
</li>
<li>
<p>Calculate some summary statistic of ε j = ŝj − s j on the validation set, e.g. its root mean square error (RMSE).4. Repeat steps (1-3) a large number of times. 5. Choose the sample with the smallest validation error and check if the performance is still good on the test set.</p>
</li>
</ol>
<p>For g(•) one can use generalized additive models (GAMs, [42,43,44]), which are a flexible class of regression models utilizing basis functions.They can be seen as the frequentist version of the fully Bayesian framework of Gaussian Process regression [45].</p>
<p>We ran the described procedure on each benchmark with varying k and 10, 000 repetitions to gauge how information is spread across items.GAMs were fitted using the mgcv package [44] with default options.Appendix B.2 contains the results for k between 50 and 500 in steps of 50.As shown in Figure 2B, even for k ≤ 200, test errors have high density on relatively small intervals and all median RMSEs are below 3.5%: Many different (but potentially overlapping samples) are informative.This suggests (1) few items are uninformative, (2) the information they carry is partially redundant, and (3) the useful information saturates swiftly as more items are considered.This notion is further explored in Section 2.4.</p>
<p>This has two practical implications: If we just want medium-sized versions of the six benchmarks, we can simply use cross-validated subsampling.If we want to tap into (or even breach) the boundary of the test RMSE distribution, a more principled approach that models properties of items is required.However, principled item selection can be a follow-up after an initial subsampling sweep, as this preselection still permits reliable score reconstruction.For comparability across item models and to increase the ratio between observations and parameters, we subsampled all six benchmarks to 350 items.This number is a compromise between predictive performance and subject-to-item ratio (see Appendix B.2).We next describe the approach to modeling item properties.</p>
<p>Item Response Theory</p>
<p>Item response theory (IRT) is a statistical branch of psychometrics used for the design and analysis of standardized tests [40,46,47,48].It aims to model the relationship between a subject's abilities and the probability of a correct response to an item.This framework is closely related to other latent trait models such as factor analysis [49], but assumes binary or polytomous responses [50,51].Let x ij be the binary accuracy of item i for subject j with some (unknown) latent ability vector ϑ j :
x ij |ϑ j ∼ Bernoulli f (ϑ j |α i ) ,(1)
where f is a link function and α i are the item parameters.We use one-dimensional ϑ j , as higherdimensional IRT models require much larger subjects-to-items ratios than in our case, but the framework generalizes to multidimensional abilities in a straightforward manner.A simple and common IRT model is the 2-parameter logistic (2PL) model with α i = (δ i , a i ) and f (ϑ
j |α i ) = σ(a ⊤ i ϑ j − δ i ).
Here σ(•) is the logistic function, a i is the loading parameter and δ i is the difficulty parameter.Note that this has the same form as logistic regression with the subtle but important difference that ϑ i is unknown.The main goal is to estimate item parameters A = (α i ) i=1,...,d which capture their test-relevant properties.Due to favorable asymptotic behavior, the standard approach is to use marginal maximum likelihood methods [50,52], which treat the latent ability as a random variable and attempt to maximize the marginal likelihood
L(A) = n j=1 d i=1 L i (x ij |ϑ j ) p(ϑ j ) dϑ j ,(2)
with
L i (x ij |ϑ j ) = f (ϑ j | α i ) xij 1−f (ϑ j | α i ) 1−xij .
Because of the integral, there is no analytical solution to this problem, but it can be approximated using Expectation-Maximization [53,54,55] algorithms and stochastic approximations exist for multidimensional latent abilities [56,57].After model fitting, ϑ j can be estimated for each subject j using, e.g., maximum a posteriori estimation 3 :
θj = arg max ϑj ∈Θ d i=1 L i (x ij |ϑ j ) p(ϑ j ).(3)
We used the mirt [58] package with default options to fit the following IRT models to preselected subsets of 350 items each -a 2PL model, a 3PL model with an additional lower asymptote parameter, a 4PL model with an additional upper asymptote parameter.After IRT model fitting, we separately used the maximum a posteriori (MAP) method as well as the expected a posteriori sum (EAPsum) method to estimate each benchmark-specific latent ability [59,60].We trained a GAM with adaptive smoothers to predict the original benchmark score using the latent ability estimate, and tested the GAM on the test set.</p>
<p>Table 1 shows the test RMSE of the predicted scores as well as the Spearman correlations between the estimated latent variables and the original point scores.In the appendix, Figure 6 visualizes the results.All correlation coefficients are above 0.945, indicating that the ranking between ϑ and s is largely preserved.The best models achieve an RMSE of on average 1.18% and all but one model have an RMSE well below 2%.For 3 out of 6 benchmarks, the best model outperforms the cross-validated subsampling approach.For the remaining 3, the RMSE is at most 0.19% worse.While we use single-predictor GAMs to reconstruct scores from (a) random subtest scores sj and (b) latent abilities θj , the latter seem to preserve the information about scores and are at times less noisy.This proof of concept reinforces our general approach of going beyond random subsampling and opens up the possibility of information filtering.</p>
<p>Information Filtering</p>
<p>Item parameters have a natural interpretation because of how they affect the link function f (ϑ j |α i ):</p>
<p>The difficulty δ i shifts its midpoint and the loading a i alters its scale.Thus, the difficulty parameter determines the probability of a correct response for an LLM j with ϑ j = 0, and the loading determines how much this probability changes with the LLM's ability.Intuitively, an item with a high loading discriminates well between subjects whose ability is close to the item's difficulty.Fisher information functions quantify this property: Since the distribution of x ij |ϑ j is an exponential family, any response-based estimator T for ϑ will have a variance bounded from below by the expected Fisher information4 [61,62].For the 2PL model with one-dimensional ability,
I i (ϑ) = a 2 i f (ϑ|α i ) 1 − f (ϑ|α i ) .(4)
On a high level, this is a measure of how informative an item is about a test taker's ability conditioned on the ability, and opens up a more principled approach to item selection.For each benchmark we use the following procedure to pick the most predictive items:</p>
<ol>
<li>
<p>Estimate the item parameters using an IRT model µ ∈ {2PL, 3PL, 4PL}.2. Estimate the ability using a method ν ∈ {MAP, EAPsum}.</p>
</li>
<li>
<p>Use the item parameters to calculate Fisher information functions for all items.4. Partition the the support of ϑ into q quantiles.5.In each quantile, select the item with the highest information (above a threshold τ ) and remove it from the search space.</p>
</li>
</ol>
<p>We ran the same models as described in Section 2.3, this time subsampling items using information filtering.We used the rBayesianOptimization package [63] with default settings to fine-tune the hyperparameters (µ, ν, q, τ ) on the validation set to minimize the term RMSE(ϑ, s) + λk where k is the size of the chosen subset and λ ∈ [0.0, 0.01] is the regularization coefficient.The selected subset is then evaluated on the test set.For each resulting selection we re-run the random subsampling procedure with the same size k and check how many random samples have a worse RMSE.</p>
<p>Going Meta</p>
<p>The idea behind metabench is to go beyond established benchmarking practices for LLMs (cf.[21,22]).This also means going beyond independent latent abilities for each benchmark.As benchmark scores are highly correlated (going from r = 0.571 to r = 0.956, see Appendix C.2), each benchmark also carries some information about the others.Thus, if an LLM is run on several reduced benchmarks, the resulting scores may mutually fill in each other's blanks.We test this hypothesis by using the resulting IRT models from 2.4 to predict each benchmark's original score by jointly using all six latent ability estimates as predictors in the GAM.As always, IRT models, latent abilities and GAMs were estimated on the train set and the RMSEs reflect the score prediction accuracy on the test set.</p>
<p>The Figure 2 and the right side of Table 2 show the results: The mean RMSE has shrunk to 1.508% and is well below 2% in all cases.Two models are better than over 99% of all random subsamples, the remaining four surpass the best random subsample by a substantial margin.This synergy effect underlines the advantage of running all components of metabench.</p>
<p>We then applied this method to predict the grand average score: The mean across all normalized scores for the six benchmarks constituting the Open LLM Leaderboad.Random subsets are sampled from each benchmark using its corresponding d (e.g.58 items from HellaSwag), totalling 739 items.Figure 2C and the bottom row of Table 2 show the score reconstruction results.We were able to reconstruct the grand average with a Spearman correlation of r = 0.996 and an RMSE of 0.814%, which surpassed all random subsamples.</p>
<p>Finally, we investigated the relationship between all latent abilities by using factor analysis [49,64,65], which is a linear latent modeling framework closely related to Principle Component Analysis [65,66] and with a long history in psychometrics [67,68].In a nutshell, factor analysis of a set of continuous variables attempts a low-rank approximation of their covariance matrix.As its latent factors have the same interpretation as the latent abilities in IRT, it is commonly applied to analyze the relationship between multiple tests [40].See Appendix C.1 for a more detailed introduction.We applied FA to the estimated latent abilities using the psych package [69] (for details, see Appendix C.2).We find that a single factor is sufficient to capture 79.3% of the total variability across latent abilities.This meta-ability has a Pearson correlation of r = 0.928 with the original grand average score.Its loadings range from 0.791 (MMLU) to 0.965 (WinoGrande): Separately, all six latent abilities contribute a comparable amount to the meta-ability, but the added unique information decreases with each addition.This result is in line with the synergy effect as described above.It consolidates the utility of metabench for efficiently measuring the abilities of LLMs.</p>
<p>1
I(ϑ) . If T is biased, the numerator becomes ( ∂ ∂ 2 ϑ E ϑ [T ]) 2 .
The larger I(ϑ) becomes, the more reliably one can estimate ϑ from the observed responses.</p>
<p>Adaptive Testing</p>
<p>In terms of efficiency, the fixed set of 739 items can still be outperformed: A significant benefit of IRT analysis is that it allows for adaptive testing, in which items can be administered conditional upon the current estimate of the subject's ability.The information filtering procedure can be seen as a one-size fits most approach, covering as much ground as necessary to get a good estimate of the latent ability (within quantiles populated by the majority of models).However, there is also a made-to-measure alternative which can further reduce item administration costs.By asking the most informative questions sequentially, one can get a good estimate of the latent ability with fewer items.This is the idea behind computerized adaptive tests (CATs, [40,70]).</p>
<p>We used the catR package [71] to simulate CATs on our sample of LLMs using items from each benchmark (for details, see Appendix D).The goal was to re-estimate ϑ j for each subject j as provided by an IRT model fitted to the training set, using as few items as possible.Our CAT procedure for each subject j is as follows:</p>
<ol>
<li>
<p>Initialize an estimate of ϑ j around 0.</p>
</li>
<li>
<p>Select an item, i, with the Maximum Fisher Information with respect to θj .3. Update θj given j's response to item i. 4. Repeat steps (2-3) until the estimated standard error of θj is less than 0.1 or all items have been administered.</p>
</li>
</ol>
<p>Using this approach, we found that the latent ability could be reliably re-estimated for every benchmark (RMSE &lt; 0.2525) with significantly fewer items than the previous approach (as low as 21 items for the most predictive IRT model).The results of our CAT simulations are presented in Table 3.</p>
<p>The estimated latent abilities are predictive of performance across each benchmark.By applying computerized adaptive testing, practitioners can estimate ϑ for their models using a small fraction of the items in available benchmarks.</p>
<p>Conclusion</p>
<p>The findings presented in this study underscore the viability and advantages of using a sparse benchmark, metabench, to measure the abilities of Large Language Models (LLMs).The successful distillation of six prominent benchmarks -ARC, GSM8K, HellaSwag, MMLU, TruthfulQA, and WinoGrande -into a much smaller set of items with minimal information loss.By identifying and utilizing the most informative items, metabench retains the evaluative power of the original, larger benchmarks while reducing redundancy and inefficiency.This demonstrates significant potential for efficient LLM benchmarking and has important implications for the design of future benchmarks.The successful distillation of six large benchmarks to less than 3% of the original volume highlights the potential for more streamlined, cost-effective evaluation methods.This efficiency is particularly valuable for researchers and developers who need to frequently evaluate and compare models during prototyping -without the computational and time constraints associated with larger benchmarks.</p>
<p>One of the key strengths of metabench lies in its ability to not only provide point scores but also yield estimators of underlying benchmark-specific abilities, both through its use in full and in an adaptive testing context.These estimators facilitate a deeper understanding of the competencies measured by each benchmark and the relationships between them.They retain a sort of backwards compatibility, as the psychometric techniques used to estimate these abilities enable accurate reconstruction of individual and total benchmark scores, as well as the original ranking.This ensures that metabench maintains a high level of fidelity to the original benchmarks, despite its significantly reduced size.</p>
<p>Moreover, the identification of a single common factor underlying all benchmark scores suggests that a general ability can be effectively measured through a consolidated, sparse benchmark.This finding supports the hypothesis that LLM performance across various tasks is influenced by a core set of underlying competencies.By distilling these competencies into a concise benchmark, metabench offers a more focused and insightful evaluation tool.</p>
<p>Limitations</p>
<p>Most users who submitted results to the OpenLLM Leaderboard contributed several models, potentially violating the IRT model assumption that the response patterns of different subjects are independent (conditional on the item parameters).This may result in biased parameter estimates and therefor distorted item information functions.Indeed, the population of LLMs presumably has a different dependence structure than than that of human test takers: Many LLMs are fine-tuned versions of others or at least share the same base structure and are trained on overlapping data.In the extreme case, treating them as subjects for IRT models is akin to analyzing a test on several batches of clones.On the other hand, the same can be said about human subjects -despite sharing many fundamental properties of the human mind and brain, and experiencing overlapping school curricula, human subjects measurably differ in their knowledge, problem solving abilities, linguistic skills and memory [72,73].Typically, clustering in human data is treated by including random effects to (generalized) regression models [43].This is not possible for IRT models, as these random effects would become indistinguishable from latent abilities without introducing further constraints.</p>
<p>However, as the latent abilities are strongly related to the point scores, we checked if the latter have different empirical distributions if we only include the latest LLM per leaderboard user (see Appendix B.1).The distributions are largely identical and the ratio between subjects and items is already low for IRT standards, prohibiting substantial discarding of subjects without also reducing the item sets before model fitting.Future work with more data should analyze to what extent similarities in fine-tuning and architecture deflate predictive performance for scores and (more importantly) change the ranking outcomes.Finally, it is likely that new LLMs tested on these benchmarks are themselves not far out of distribution with respect to the training set.Thus, for practical purposes, our results should still hold.</p>
<p>Future Work</p>
<p>Future research could explore several avenues to build upon the findings of this study.First, further validation of metabench across a broader range of LLMs would test the generalizability of our results.Additionally, investigating the potential for extending the metabench framework to other domains beyond natural language processing could provide valuable insights into the applicability of sparse benchmarks in different modalities such as vision, reinforcement learning, and visual question answering.Another promising direction for future research is the development of adaptive testing algorithms and interfaces that can further reduce the number of items required for accurate LLM evaluation.By continuously refining the selection process based on real-time performance data, these algorithms could enhance the efficiency and precision of LLM assessments.Finally, the core set of cognitive abilities in LLMs remains largely unknown.Future research should closely investigate its structure, as well as uncover the properties of model architecture and training data that facilitate ability development.This would constitute a good basis for more goal-directed and theoretically grounded benchmarks, driving the field of LLM research into a more insightful future.</p>
<p>A Preprocessing Details</p>
<p>A.1 Part-Whole Correlation</p>
<p>To estimate the part-whole correlation between a binary item outcome and a non-binary aggregate point score, psychometrics [40] offers the point-biserial correlation coefficient r pb :
r pb = µ 1 − µ s n−1 n 1 (n − n 1 ) n(n − 1) ,(5)
where the set of correct responders has the average score µ 1 and size n 1 (in comparison to the total mean µ and size n).The empirical standard deviation of the total group is denoted as s n−1 .This coefficient is bounded in [−1, 1] and grows if subjects who responded correctly to the item have higher than average scores.Note that it is customary to remove items with negative r pb , as they appear to measure something different from what the remaining majority of items measures or even point to annotation errors [32].However, items with a negative r pb do contain information about the benchmark score: Correct subjects are on average worse on the remaining items.We therefore chose to keep them.</p>
<p>A.2 Numbers Before vs. After</p>
<p>Table 4 shows the numbers of LLMs and items before and after preprocessing.Apart from GSM8K, there is no benchmark for which many LLMs were discarded for having a score at the 0.1st percentile (meaning that 99.9% of scores were greater than the threshold).In fact, all discarded models in the GSM8K dataset had achieved 0 points.For ARC, 103 items were too easy (mean accuracy &gt; 0.95) and 225 items were uninformative (r pb ≈ 0).For GSM8K, 13 items were uninformative.For HellaSwag, 1 item had too little variance (s n−1 &lt; 0.01), further 2729 items were too easy, and 1601 items were uninformative.For MMLU, 27 items had duplicate prompts 5 , and 1507 items were uninformative.For TruthfulQA, 2 items had too little variance, 49 items were too easy and 122 items were uninformative.For WinoGrande, 156 items were too easy and 66 items were uninformative.The empirical distributions of scores after preprocessing are shown in Figure 3, taking data from either all LLMs or reducing the set to the last occurring LLM per user.The distributions are very similar, but the former tend to have higher density in bigger scores.Notably, all distributions are far from normal, but appear to be mixtures with several local maxima.Future work should investigate if LLMs can be clustered (according to some independent property) such that these clusters can be mapped on different intervals in the score distributions.</p>
<p>B.2 Cross-validated Subsampling</p>
<p>We ran the cross-validated subsampling procedure in Section 2.2 on each benchmark with k ranging from 50 to 500 in steps of 50, and using 10, 000 repetitions. Figure 4 shows that the validation set error shrinks monotonically with an increasing number of items, but the corresponding test error is not always optimal.This suggests that the best selection for the validation set does not always generalize to the test set, which might be remedied by using 10-fold cross-validation instead.Notably, this algorithm struggles with HellaSwag -this can not solely be explained by the benchmark's size, as the procedure appears to work well for the even larger MMLU.This suggests, the procedure could be refined by making it an evolutionary algorithm: Instead of sampling the goal k in one go, one could discard m ≪ k items 10, 000 times, take the best intermediate subset, and continue from there until only k items remain.</p>
<p>B.3 Score Reconstruction with Logistic Regression Models</p>
<p>If an essential goal of metabench is to encode information about the test score in the item parameters, one may raise the question: Why should we use latent abilities as presented in Section 2.3?Is it not more parsimonious to directly provide the achieved score?To address this question, we reformulated the 2PL model as logistic regression:
x ij |s j ∼ Bernoulli σ(a i s j − δ i ) ,(6)
where
σ(η ij ) = 1 + exp(−η ij ) −1 .
Using the built-in stats package [39] we fitted a generalized linear model to each item per benchmark, providing the normalized score as the single predictor.For comparability, we took the 350 item subsets from Section 2.2.Using the estimated parameters and setting the prior to the empirical distribution of the scores, we evaluated the log-posteriors of each subject's response vector as a function of the score on a grid of 4096 values between 0 and 100.The estimated score was set as the argmax on that grid.Concretely,
ŝj = arg max sj ∈[0,100] log p(s j ) + d i=1 ℓ i (x ij |s j ),(7)with ℓ i (x ij |s j ) = x ij log σ(a i s j − δ i ) + (1 − x ij ) log 1 − σ(a i s j − δ i )
. This is numerical MAP estimation, but the implementation probably has room for improvement.Figure 5 shows the reconstructed scores for each benchmark.The mean and standard deviation of the training RMSEs are 4.06% resp.2.08%.The results should be compared with the 2PL column for Table 1, and they reveal two things: (1) this method works in principle, but (2) it has strictly worse reconstruction accuracy in comparison to latent ability estimates even on the training set and to random sampling.In summary, this gives further justification for using IRT methods in our analyses.</p>
<p>B.4 Score Reconstruction with Latent Abilities</p>
<p>Figure 6 visualizes the results discussed in Section 2.3 for the best IRT models shown in Table 1.</p>
<p>B.5 Mean Absolute Errors for Score Recovery</p>
<p>Table 5 compares the reported root mean square errors (RMSEs) from Section 2.4 with corresponding mean absolute errors (MAEs).RMSEs are more sensitive to outliers and are therefore larger.When using MAEs, the error is roughly 0.5 points lower for single benchmarks.
= Az + µ + ϵ,(8)
with some l-dimensional latent z ∼ N l (0, I) 6 and an independent error ϵ ∼ N l (0, Ψ) with diagonal Ψ.The dimension l typically has a much smaller size than d.The loading matrix A ∈ R d×l thus projects the l-dimensional latent z into some d-dimensional space and some anisotropic noise is added by ϵ.This model implies the decomposition
Σ = V(x) = V(Az + µ + ϵ) = AV(z)A ⊤ + V(ϵ) = AA ⊤ + Ψ. (9)
This decomposition is not uniquely identifiable, as it is invariant wrt.orthogonal rotations of A and z7 .Note that the decomposition implies
V(x j ) = l k=1 A 2 jk + Ψ jj ,(10)
which gives a natural way of calculating the proportion of data variance explained by the l factors.</p>
<p>If we know that the data is (close to) Gaussian, we can formulate a likelihood function in A and Ψ and maximize it with EM.Alternatively, A can be estimated with the minimum residual (minres) method, which is a form of ordinary least squares minimizing the summed squares of the off-diagonal elements of Σ − Â Â⊤ [69,74].Like in item response theory models, the latent variable z can be estimated once there are estimates for A and Ψ. Koch [49] shows multiple ways to arrive at the generalized least squares solution with regularization coefficient λ,
ẑλ = ( Â⊤ Ψ−1 Â + λI) −1 Â⊤ Ψ−1 (x − μ).(11)</p>
<p>C.2 Factor Models for Benchmarks</p>
<p>As already established, there is a lot of shared information across benchmarks.Figure 7 shows the Pearson correlations benchmark point scores and corresponding latent abilities, respectively.While some correlations (e.g. between ARC and HellaSwag) are invariant over the two matrices, the exact pattern is not identical.The (meta-)correlation between the unique off-diagonal correlation coefficients in both matrices is r = 0.54, indicating moderately strong agreement.Nonetheless, all entries are highly positive, suggesting the existence of few latent factors governing the correlation structure.</p>
<p>We used factor analysis as implemented in the psych package [69] to estimate FA models of these correlation matrices with the minres method.Table 7 shows the loadings of 1and 2-factor solutions, and the proportions of variance explained.For the point scores, the 3-factor solution explains the least variance (68.9%) and is not further discussed.The 1-factor solution explains 79.3% variance, while the 2-factor solution explains 4.3% more and its two factors have a correlation of r = 0.70.The latter proposes to separate TruthfulQA from the remaining benchmarks.While both fits are similarly decent (RMSEA: 0.308 vs. 0.302, corrected RMSR: 0.052 vs. 0.048, CFI: 0.899 vs. 0.957, TLI: 0.832 vs. 0.839), the 1-factor solution should be preferred due to parsimony and the little additional contribution of a second factor.After estimating z j for each LLM, the Spearman correlation between ẑ and the grand average point score is r = 0.932.</p>
<p>The picture is similar but not identical for the latent abilities: The 3-factor solution explains the least variance (76.0%) and is not further discussed.The 1-factor solution again explains 79.3% variance, while the 2-factor solution explains 8.1% more and its two factors have a correlation of r = 0.76.It</p>
<p>D Computerized Adaptive Testing Procedure</p>
<p>We used the catR package [71] to simulate Computerized Adaptive Tests (CATs) on our sample of LLMs using items from each benchmark.The goal was to re-estimate ϑ j for each subject, j, as provided by an IRT model fitted to the training set, using as few items as possible.CATs proceed by selecting an item that is most useful for updating the estimated ϑ j , administering it to subject j, and then updating the estimate of ϑ j .This process is repeated until some stopping criterion is achieved, at which point the final ϑ j estimate is computed.CATs can be simulated on subjects who have already completed a full bank of items by simply returning their responses to each administered item.This assumes that the responses of each subject to each item is deterministic and would not change.The usefulness of an item can be formalized in several ways.We used the maximum Fisher information criterion, selecting the item that has the highest expected Fisher information with respect to the current estimate of ϑ j (cf.Section 2.4).</p>
<p>A classical approach in IRT is to filter out items by their infit and outfit statistics [75] before the automated selection process.An item's infit and outfit, v i and u i , are defined as
v i = n j=1 w ij z 2 ij n j=1 w ij , u i = 1 n n j=1 z 2 ij ,(12)
where w ij = V(x ij ) = f (ϑ j |α i ) 1 − f (ϑ j |α i ) , and
z ij = xij −f (ϑj |αi) √ wij
. Both infit and outfit are thus derived from χ 2 statistics.Outfit is used to detect the presence of unexpected responses to items with low information with respect to the model and estimated ϑ j , while infit detects the presence of unexpected responses to items with high information with respect to the model and estimated ϑ j .Items with 0.5 &lt; v i &lt; 1.5 and 0.5 &lt; ui &lt; 1.5 are standardly considered outliers.</p>
<p>For each benchmark, the latent ability was estimated using Maximum A Posteriori (MAP) estimation [76] (see Eq. 3).An alternative estimation method is Expected A Posteriori (EAP, [60]) estimation: θj = E ϑ|xij ϑ j = ϑ p(ϑ|x ij ) dϑ.</p>
<p>For both MAP and EAP estimations, a standard normal prior was used.The final estimate of ϑ j was computed using the same estimator as during the simulation.The responses to each item were deterministic in our simulations, and so each item was only administered once per subject 8 .Testing was stopped when either (a) all items had been administered or (b) the standard error of the distribution over ϑ was &lt; 0.1.It has been noted that standard errors may not be computed exactly in catR for multi-parameter IRT models with as many items as used here, therefore this stopping criterion may fail in some cases.</p>
<p>Our CAT simulations can be optimized in several ways.First, they could be initialized with the mode of the empirical ϑ distribution, and a larger number of informative items could be used for the initial estimate.Second, alternative item selection methods could be employed, such as Urry's procedure, optimal theta selection, weighted information criteria, or the progressive or proportional methods [71], which select items using more sophisticated policies than the maximum Fisher information criterion.Third, ϑ estimation could benefit from the EAP method, and the priors for MAP and EAP could be altered to their respective empirical distributions.Fourth, removing items by their infits and outfits may have hindered accurate estimation for some values of ϑ.Finally, the stopping criterion could be changed such that testing stops only once there are no items that are sufficiently informative with respect to the currently estimated ϑ. Note, that this method does not incur problems with numerical stability.Future work will investigate these potential optimizations to the CAT simulation procedure.</p>
<p>Figure 1 :
1
Figure 1: Processing pipeline.(1) Collect item-wise accuracies from all available LLMs for each benchmark on Open LLM Leaderboard.(2) Remove items based on simple statistics like variance.(3)Perform cross-validated subsampling to 350 items per benchmark.(4) Fit variants of IRT models to the remaining items, infer item information from the item parameters and select the most informative items to construct metabench.(5) Use the model fits to estimate the benchmark-specific abilities and reconstruct the original (normalized) benchmark scores as well as their mean using a generalized additive model with cross-validation.</p>
<p>2
2</p>
<p>Figure 2 :
2
Figure 2: Score Reconstruction.(A) After distilling all six benchmarks to in total 739 items, the latent abilities estimated by the reduced set can reconstruct each normalized benchmark score with, on average, 1.5% root mean squared error (RMSE) on the test split.(B) Empirical test RMSE distributions for 10, 000 randomly subsampled benchmarks of matching sizes, asterisks are corresponding test RMSEs.In each case, our method either outperforms or is similar to the best randomly sampled subset.(C) The reconstruction error is further reduced when the mean normalized score across all benchmarks is predicted.</p>
<p>Figure 3 :Figure 4 :
34
Figure 3: Kernel density estimations of the normalized scores for each benchmark -all resp.last uses scores from all LLMs resp. the last listed LLM per unique username.</p>
<p>Figure 5 :
5
Figure 5: Maximum a posteriori score reconstructions from logistic regression models that use scores as predictors.Results are shown for the respective training sets with 350 items.</p>
<p>Figure 6 :
6
Figure 6: Score Reconstruction using specific abilities estimated with 350 item models.(A) The specific latent ability estimated from each respective subsampled dataset can reconstruct each score with ∼ 1.1% test error.(B) Empirical test RMSE distributions for 10, 000 randomly subsampled benchmarks.Asterisks are the test errors from models in panel A. (C) The reconstruction error is further reduced when the mean normalized score across all benchmarks is predicted.</p>
<p>Table 1 :
1
Cross-validated score prediction results for IRT models using 350 preselected items per benchmark.Correlations are between latent abilities and normalized original scores and use the Spearman rank method.Normalized scores go from 0 to 100.The RMSE of the most accurate model for each benchmark is highlighted in bold.By default, latent abilities are estimated using the EAPsum method, results based on MAP estimates are italicized.The best test RMSE achieved during cross-validated subsampling is shown for reference.% BTR (better than random) indicates how many of the 10, 000 350-sized random subsamples from the respective benchmark had a larger test RMSE than the best IRT model.
2PL3PL4PLBest RandomRMSErRMSErRMSErRMSE % BTRARC1.167 0.993 1.160 0.9931.190.993 1.184100GSM8K1.528 0.995 1.656 0.995 1.604 0.995 1.33866.5HellaSwag1.214 0.973 0.997 0.982 1.087 0.975 1.401100MMLU3.228 0.945 1.218 0.965 1.161 0.965 1.177100TruthfulQA1.042 0.994 1.082 0.994 1.042 0.994 0.99899.9WinoGrande 1.335 0.977 1.223 0.977 1.242 0.977 1.10198.7</p>
<p>Table 2
2
shows the score reconstruction results for each benchmark's final model.The mean RMSE over benchmarks is 1.856%.All IRT models have an RMSE well below 2.5%.Except for TruthfulQA, all test RMSEs are better than at least 94% of all random subsamples (see Figure2Bfor their distributions).The two very large benchmarks MMLU and HellaSwag have breached the test accuracy boundary of random subsampling.Although this indicates that information filtering</p>
<p>is in general effective, we can further improve score reconstruction if we do not limit ourselves to using a single latent ability for prediction.</p>
<p>Table 2 :
2
Cross-validated score prediction results for IRT models + information filtering.Column d indicates the number of kept items.Italicized models estimate latent abilities using the MAP rather than EAPsum method.Normalized scores go from 0 to 100 and were either predicted with the respective latent ability or all six jointly.% BTR (better than random) indicates how many of the 10, 000 d-sized random subsamples from the respective benchmark had a larger test RMSE.If a model surpasses all RMSEs from random subsampling, its RMSE is highlighted in bold.
PropertiesSpecific AbilityAll AbilitiesBest RandomdIRTRMSE.1 % BTR RMSE.6 % BTR RMSE SurpassedARC1002PL2.08399.41.4161001.930≈ ✓GSM8K2372PL1.83794.01.70499.91.677✗ ≈HellaSwag583PL1.4571001.1711001.925✓ ✓MMLU1023PL1.7721001.6371001.968✓ ✓TruthfulQA1362PL1.97154.61.60199.61.439✗ ≈WinoGrande1064PL2.01699.61.5211001.887≈ ✓Open LLM LB 739 mixed--0.8141000.916✔</p>
<p>Table 3 :
3
The median number of items administered to LLMs during their CAT simulation, and the root mean squared error (RMSE) between CAT-estimated and originally estimated ϑ on the training set.The RMSE of the most accurate model for each benchmark is highlighted in bold.
2PL3PL4PLMedian RMSE Median RMSE Median RMSEARC930.1031260.43365890.3488GSM8K530.1629310.1557520.2060HellaSwag1970.1489180.25196190.6482MMLU490.2116340.36257330.4412TruthfulQA180.2167210.1430150.2692WinoGrande380.2524430.3692134.50.3694</p>
<p>Table 4 :
4
Sizes of datasets used for the analyses.LLMs resp.items indicates the number of observations resp.test questions, before resp.after indicates the raw count resp.the count after preprocessing.
ARC GSM8K HellSwag MMLU TruthfulQA WinoGrandeLLMs before 522767025227522652286558LLMs after522160685221522052226550items before1172131910052140428171267items after84413065711125086441045</p>
<p>Table 5 :
5
Cross-validated score prediction results for IRT models + information filtering.Column d indicates the number of kept items.Italicized models estimate latent abilities using the MAP rather than EAPsum method.Normalized scores go from 0 to 100 and were either predicted with the respective latent ability or all six jointly.Prediction errors are summarized either as root mean square errors (RMSEs) or mean absolute errors (MAEs).To get a better grasp on the item selection in Section 2.4, the following table shows each benchmark's most informative prompt for five latent ability intervals.Answer options are not included due to space constraints.
PropertiesSpecific AbilityAll AbilitiesdIRTRMSE.1 MAE.1 RMSE.6 MAE.6ARC1002PL2.0831.5661.4161.066GSM8K2372PL1.8371.3861.7041.287HellaSwag583PL1.4571.0001.1710.827MMLU1023PL1.7721.2001.6371.082TruthfulQA1362PL1.9711.4701.6011.135WinoGrande1064PL2.0161.5081.5211.091Open LLM LB 739 mixed--0.8140.608</p>
<p>Table 6 :
6
[49,64,65]mark's most informative item for five percentiles of latent abilities.The p in Benchmark.prepresentstheupperpercentile for which the item is most informative, e.g.item ARC.40 is most informative for subjects whose ability falls between the top 20% and top 40% of all tested LLMs.ID represents the original index of the item, Prompt is the short version of the test question.Factor Analysis (FA,[49,64,65]) is a linear multivariate method for finding a latent low-dimensional representation of observed data.Let x ∈ R d be a random variable with mean µ and covariance Σ, then FA models it as x
SourceIDPromptARC.20189What is potential energy?ARC.40829If the weather patterns change and stay colder, what is one way amammal most likely will adapt to this change?ARC.60734When you have too little water in your body, your kidneys excreteantidiuretic hormone (ADH). What happens in response to ADH?ARC.80775Which statement describes how energy can be helpful?ARC.100426Reef-building corals work together with photosynthetic algae thatlive in their tissues. Which type of energy is algae within a coralreef most likely able to use in order to survive?GSM8K.201153Maddison has 5 boxes with 50 marbles in each box. Then she gets20 marbles from her friend. How many marbles does she havenow?GSM8K.40950The number of songs in a playlist is 300. If John has 20 suchplaylists, and each song is 10 hours long, how many hours willthe 20 playlists last in total?GSM8K.6082Nick had twice as many candies as George. Then George ate 5candies. Now George has 3 candies left. How many candies doesNick have?GSM8K.8031Sean is practicing for his role in a theater production. He has tomemorize his lines for two scenes and the lyrics to one solo song.His solo song has 54 lines in the lyrics. The first scene has twicethe number of lines, but only a third of them are his lines. Thesecond scene has six more lines than the song, and four-fifths ofthem are his. How many lines does Sean have to memorize?GSM8K.100245Jenny goes to the florist to buy some flowers. Roses cost 2 eachand 15 for a dozen. If she bought 15 roses and arrived with five 5dollar bills and they only have quarters for change, how manyquarters does she leave with?HellaSwag.208950A woman is handed an ice cream cone. Two police officers orderice cream and wait for it. A customerHellaSwag.405370How to paint a bubble design on your nails. Remove any existingpolish off your nails.. Trim and shape your nails if needed.. Painta clear base coat on your nailsHellaSwag.607440How to set up kobo. Switch on your kobo. Press and hold thepower button located at the top-right corner of the reader to turn iton.. Connect the kobo to a computer.HellaSwag.807022How to make two different size breasts appear the same. Pull yourhair over the shoulder of the smaller breast. If you have long hair,use it to your advantage by sweeping it over the shoulder of thesmaller breast. Doing so will visually " pad " the smaller breast,balancing them out and making them appear to be the same size.HellaSwag.1005391A person hangs onto the handles of a kite flying overhead. Thekite
Note, that we report root mean squared errors (RMSEs), which are more sensitive to outliers than the mean absolute errors (MAEs) used in tinyBenchmarks. For comparability, we also report MAEs in the appendix Appendix B.5. In short, metabench achieves an average MAE of 1.08% over benchmarks and an MAE of 0.61% for the grand mean.
Any likelihood-based estimator profits from large d, as the link functions have sigmoidal shapes: A hill in the function product only arises after aggregating a sufficient number of item-likelihoods with varying midpoint and steepness. This raises the trade-off that many subjects per item are required to get stable parameter estimates, but many items per subject are required to recover their latent ability.
The observed Fisher information, Ii(ϑ) = − ∂ 2 ∂ 2 ϑ log L(xi|ϑ), is the curvature of the observed likelihood. Its expectation is denoted as Ii(ϑ) = E ϑ [Ii(ϑ)] and I(ϑ) = d i=1 Ii(ϑ). Then, for unbiased T , V ϑ (T ) ≥
This is the case for all subjects. Per LLM, responses were identical for duplicates. It is unclear if duplicate items were also presented to LLMs or if incorrect item details were saved on the server-side.
The assumption that z has zero mean and identity covariance is wlog, as any other expected value resp. covariance can be absorbed by µ resp. AA ⊤ .
In practice, non-orthogonal rotations (also called "oblique") are preferred, as they allow correlations among factors and potentially improve fit and interpretability.
Future work will explore whether this constraint should be loosened. Moreover, when running CATs on novel LLMs, repeating test items could be informative. So long as items are not presented within the same context window, over-exposure is not detrimental to inference, as it is in human psychometric testing where subjects remember previously administered items.
proposes to separate MMLU and GSM8K from the remaining benchmarks.While both fits are similarly decent (RMSEA: 0.327 vs. 0.246, corrected RMSR: 0.079 vs. 0.024, CFI: 0.881 vs. 0.97, TLI: 0.802 vs. 0.888), the latter is consistently slightly better.For comparability and the same reasons as above, the 1-factor solution should be preferred.After estimating z j for each LLM, the Spearman correlation between ẑ and the grand average point score is r = 0.937.In summary, the results indicate the existence of a single overarching general ability tested by all benchmarks.
Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, . . Amodei, D , 2020NeurIPS</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI Blog. 192019</p>
<p>M Chen, J Tworek, H Jun, Q Yuan, H P De Oliveira Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Can gpt-3 pass a writer's turing test. K Elkins, J Chun, Journal of Cultural Analytics. 522020</p>
<p>Chatgpt for good? on opportunities and challenges of large language models for education. E Kasneci, K Seßler, S Küchemann, M Bannert, D Dementieva, F Fischer, U Gasser, G Groh, S Günnemann, E Hüllermeier, Learning and Individual Differences. 1032023</p>
<p>Automating customer service using langchain: Building custom opensource gpt chatbot for organizations. K Pandya, M Holia, arXiv:2310.054212023arXiv preprint</p>
<p>Large language models are human-level prompt engineers. Y Zhang, H Sun, A Patel, G Liu, C Yin, Z Chen, arXiv:2301.127202023arXiv preprint</p>
<p>R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Von Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, arXiv:2108.07258On the opportunities and risks of foundation models. 2021</p>
<p>R Burnell, W Schellaert, J Burden, T D Ullman, F Martinez-Plumed, J B Tenenbaum, D Rutar, L G Cheke, J Sohl-Dickstein, M Mitchell, Rethink reporting of evaluation results in ai. 2023a380</p>
<p>Critiquellm: Scaling llm-as-critic for effective and explainable evaluation of large language model generation. P Ke, B Wen, Z Feng, X Liu, X Lei, J Cheng, S Wang, A Zeng, Y Dong, H Wang, arXiv:2311.187022023arXiv preprint</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. A Srivastava, &amp; authors.2023</p>
<p>I D Raji, E M Bender, A Paullada, E Denton, A Hanna, AI and the Everything in the Whole Wide World Benchmark. 2021, November</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. P Clark, I Cowhey, O Etzioni, T Khot, A Sabharwal, C Schoenick, O Tafjord, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018</p>
<p>K Cobbe, V Kosaraju, M Bavarian, J Hilton, R Nakano, C Hesse, J Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Hellaswag: Can a machine really finish your sentence?. R Zellers, A Holtzman, Y Bisk, A Farhadi, Y Choi, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL). the 57th Annual Meeting of the Association for Computational Linguistics (ACL)2019</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, 2021ICLR</p>
<p>TruthfulQA: Measuring how models mimic human falsehoods. S Lin, J Hilton, O Evans, 2022ACL</p>
<p>Winogrande: An adversarial winograd schema challenge at scale. K Sakaguchi, R Le Bras, C Bhagavatula, Y Choi, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2020</p>
<p>Item response theory: Principles and applications. R K Hambleton, H Swaminathan, 2013Springer Science &amp; Business Media</p>
<p>X Wang, L Jiang, J Hernandez-Orallo, D Stillwell, L Sun, F Luo, X Xie, Evaluating General-Purpose AI with Psychometrics. 2023, December</p>
<p>R Burnell, W Schellaert, J Burden, T D Ullman, F Martinez-Plumed, J B Tenenbaum, D Rutar, L G Cheke, J Sohl-Dickstein, M Mitchell, D Kiela, M Shanahan, E M Voorhees, A G Cohn, J Z Leibo, J Hernandez-Orallo, 10.1126/science.adf6369Rethink reporting of evaluation results in AI. 2023b380</p>
<p>J Coda-Forno, M Binz, J X Wang, E Schulz, arXiv:2402.18225Cogbench: A large language model walks into a psychology lab. 2024arXiv preprint</p>
<p>J Huang, -T, W Wang, E J Li, M H Lam, S Ren, Y Yuan, W Jiao, Z Tu, M R Lyu, arXiv:2310.01386Who is chatgpt? benchmarking llms' psychological portrayal using psychobench. 2023arXiv preprint</p>
<p>. E Beeching, C Fourrier, N Habib, S Han, N Lambert, N Rajani, O Sanseviero, L Tunstall, T Wolf, 2023Open llm leaderboard</p>
<p>Making sense of item response theory in machine learning. F Martínez-Plumed, R B Prudêncio, A Martínez-Usó, J Hernández-Orallo, 2016. 2016IOS Press</p>
<p>Item response theory in ai: Analysing machine learning classifiers at the instance level. F Martínez-Plumed, R B Prudêncio, A Martínez-Usó, J Hernández-Orallo, Artificial intelligence. 2712019</p>
<p>Y Zhuang, Q Liu, Y Ning, W Huang, R Lv, Z Huang, G Zhao, Z Zhang, Q Mao, S Wang, arXiv:2306.10512Efficiently measuring the cognitive ability of llms: An adaptive testing perspective. 2023arXiv preprint</p>
<p>J Burden, K Voudouris, R Burnell, D Rutar, L Cheke, J Hernández-Orallo, arXiv:2309.11975Inferring capabilities from task performance with bayesian triangulation. 2023arXiv preprint</p>
<p>Evaluating and inducing personality in pre-trained language models. G Jiang, M Xu, S.-C Zhu, W Han, C Zhang, Y Zhu, Advances in Neural Information Processing Systems. 202436</p>
<p>Emotional intelligence of large language models. X Wang, X Li, Z Yin, Y Wu, J Liu, Journal of Pacific Rim Psychology. 17183449092312139582023</p>
<p>Evaluation Examples are not Equally Informative: How should that change NLP Leaderboards?. P Rodriguez, J Barrow, A M Hoyle, J P Lalor, R Jia, J Boyd-Graber, 10.18653/v1/2021.acl-long.346Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>Comparing Test Sets with Item Response Theory. C Vania, P M Htut, W Huang, D Mungra, R Y Pang, J Phang, H Liu, K Cho, S R Bowman, 2021. June</p>
<p>Y Wang, C Ma, Q Dong, L Kong, J Xu, A Challenging Benchmark for Low-Resource Learning. 2023, March</p>
<p>How Predictable Are Large Language Model Capabilities?. Q Ye, H Y Fu, X Ren, R Jia, 2023, OctoberA Case Study on BIG-bench</p>
<p>Model Editing with Canonical Examples. J Hewitt, S Chen, L L Xie, E Adams, P Liang, C D Manning, 2024. February</p>
<p>F M Polo, L Weber, L Choshen, Y Sun, G Xu, M Yurochkin, arXiv:2402.14992Tinybenchmarks: Evaluating llms with fewer examples. 2024aarXiv preprint</p>
<p>P Liang, R Bommasani, T Lee, D Tsipras, D Soylu, M Yasunaga, Y Zhang, D Narayanan, Y Wu, A Kumar, arXiv:2211.09110Holistic evaluation of language models. 2022arXiv preprint</p>
<p>R: A language and environment for statistical computing. R Foundation for Statistical Computing. 2024Vienna, Austria</p>
<p>The Wiley Handbook of Psychometric Testing: A Multidisciplinary Reference on Survey. P Irwing, T Booth, Hughes, 10.1002/9781118489772Scale and Test Development. D J ) Eds, Wiley2018, April1st ed.</p>
<p>F M Polo, L Weber, L Choshen, Y Sun, G Xu, M Yurochkin, tinyBenchmarks: Evaluating LLMs with fewer examples. 2024b, February</p>
<p>Generalized additive models: Some applications. T Hastie, R Tibshirani, Journal of the American Statistical Association. 823981987</p>
<p>Regression: Models, Methods and Applications. L Fahrmeir, T Kneib, S Lang, B Marx, 10.1007/978-3-642-34333-92013SpringerBerlin Heidelberg</p>
<p>Generalized additive models: An introduction with R. S N Wood, 2017CRC Press/Taylor &amp; Francis GroupSecond edition</p>
<p>. D Duvenaud, H Nickisch, C E Rasmussen, 2011Additive Gaussian Processes</p>
<p>Item Response Theory -A Statistical. Y Chen, X Li, J Liu, Z Ying, 2021. AugustFramework for Educational and Psychological Measurement</p>
<p>Item Response Theory. L Cai, K Choi, M Hansen, L Harrell, 10.1146/annurev-statistics-041715-033702Annual Review of Statistics and Its Application. 312016</p>
<p>R K Hambleton, H Swaminathan, H J Rogers, Fundamentals of item response theory. Sage19912</p>
<p>Analysis of multivariate and high-dimensional data. I Koch, 2013Cambridge University Press</p>
<p>Y Chen, S Zhang, Estimation Methods for Item Factor Analysis: An Overview. 2020. April</p>
<p>An application of item response theory to psychological test development. C Zanon, C S Hutz, H Yoo, R K Hambleton, Psicologia: Reflexão e Crítica. 290182016</p>
<p>Item factor analysis: Current approaches and future directions. R J Wirth, M C Edwards, 10.1037/1082-989X.12.1.58Psychological Methods. 1212007</p>
<p>Maximum Likelihood from Incomplete Data Via the EM Algorithm. A P Dempster, N M Laird, D B Rubin, 10.1111/j.2517-6161.1977.tb01600.xJournal of the Royal Statistical Society Series B: Statistical Methodology. 3911977</p>
<p>Marginal maximum likelihood estimation of item parameters: Application of an EM algorithm. R D Bock, M Aitkin, 10.1007/BF02293801Psychometrika. 4641981a</p>
<p>High-dimensional Full-information Item Factor Analysis. R D Bock, S Schilling, 10.1007/978-1-4612-1842-5_8Latent Variable Modeling and Applications to Causality. M Berkane, Springer1997</p>
<p>Computation for Latent Variable Model Estimation: A Unified Stochastic Proximal Framework. S Zhang, Y Chen, 10.1007/s11336-022-09863-9Psychometrika. 8742022</p>
<p>An improved stochastic EM algorithm for large-scale fullinformation item factor analysis. S Zhang, Y Chen, Y Liu, 10.1111/bmsp.12153British Journal of Mathematical and Statistical Psychology. 7312020</p>
<p>mirt: A multidimensional item response theory package for the R environment. R P Chalmers, 10.18637/jss.v048.i06Journal of Statistical Software. 4862012</p>
<p>Marginal maximum likelihood estimation of item parameters: Application of an em algorithm. R D Bock, M Aitkin, Psychometrika. 4641981b</p>
<p>Item response theory for scores on tests including polytomous items with ordered responses. D Thissen, M Pommerich, K Billeaud, V S Williams, Applied psychological measurement. 1911995</p>
<p>In all likelihood: Statistical modelling and inference using likelihood. Y Pawitan, 2013OUP Oxford</p>
<p>Theory of Point Estimation. E Lehmann, G Casella, 10.1007/b988541998Springer-Verlag</p>
<p>Rbayesianoptimization: Bayesian optimization of hyperparameters. Y Yan, 2024R package version 1.2.1</p>
<p>CRAN. </p>
<p>Introduction to factor analysis: What it is and how to do it. J.-O Kim, C W Mueller, 1978Sage</p>
<p>Probabilistic machine learning: An introduction. K P Murphy, probml.ai2022MIT Press</p>
<p>Factor analysis and related methods. R P Mcdonald, 2014Psychology Press</p>
<p>The meaning and strategic use of factor analysis. R B Cattell, Handbook of multivariate experimental psychology. Springer1966</p>
<p>The three-stratum theory of cognitive abilities. J B Carroll, Contemporary intellectual assessment: Theories, tests, and issues. The Guilford Press1997</p>
<p>Psych: Procedures for psychological, psychometric, and personality research. William Revelle, 2023R package version 2.3.9</p>
<p>H.-H Chang, 10.1007/s11336-014-9401-5Psychometrics Behind Computerized Adaptive Testing. 201580</p>
<p>Random generation of response patterns under computerized adaptive testing with the R package catR. D Magis, G Raiche, Journal of Statistical Software. 8482012</p>
<p>The psychology of human differences. L E Tyler, 1947D Appleton-Century Company</p>
<p>Fundamentals of psychology. M Eysenck, 2014Psychology Press</p>
<p>Factor analysis by minimizing residuals (minres). H H Harman, W H Jones, 10.1007/BF02289468Psychometrika. 31966</p>
<p>Rating scale analysis. B D Wright, G N Masters, 1982MESA press</p>
<p>Maximum a posteriori estimators as a limit of bayes estimators. R Bassett, J Deride, Mathematical Programming. 1742019</p>            </div>
        </div>

    </div>
</body>
</html>
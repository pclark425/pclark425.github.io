<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9365 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9365</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9365</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-348a1efa54376fa39053e5e25d52bd0eb6a0ba68</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/348a1efa54376fa39053e5e25d52bd0eb6a0ba68" target="_blank">Capabilities of GPT-4 on Medical Challenge Problems</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A comprehensive evaluation of GPT-4, a state-of-the-art LLM, on medical competency examinations and benchmark datasets and explores the behavior of the model qualitatively through a case study that shows the ability of G PT-4 to explain medical reasoning, personalize explanations to students, and interactively craft new counterfactual scenarios around a medical case.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation across various domains, including medicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art LLM, on medical competency examinations and benchmark datasets. GPT-4 is a general-purpose model that is not specialized for medical problems through training or engineered to solve clinical tasks. Our analysis covers two sets of official practice materials for the USMLE, a three-step examination program used to assess clinical competency and grant licensure in the United States. We also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond measuring model performance, experiments were conducted to investigate the influence of test questions containing both text and images on model performance, probe for memorization of content during training, and study probability calibration, which is of critical importance in high-stakes applications like medicine. Our results show that GPT-4, without any specialized prompt crafting, exceeds the passing score on USMLE by over 20 points and outperforms earlier general-purpose models (GPT-3.5) as well as models specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned version of Flan-PaLM 540B). In addition, GPT-4 is significantly better calibrated than GPT-3.5, demonstrating a much-improved ability to predict the likelihood that its answers are correct. We also explore the behavior of the model qualitatively through a case study that shows the ability of GPT-4 to explain medical reasoning, personalize explanations to students, and interactively craft new counterfactual scenarios around a medical case. Implications of the findings are discussed for potential uses of GPT-4 in medical education, assessment, and clinical practice, with appropriate attention to challenges of accuracy and safety.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9365.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9365.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (USMLE MCQ)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4 (text-only) used for USMLE multiple-choice question simulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 (text-only) was used as a text-based simulator to answer multiple-choice medical exam questions (USMLE Steps 1-3 and MultiMedQA components), producing probability-scored predictions and explanations without domain-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Capabilities of GPT-4 on Medical Challenge Problems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (text-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Text-only variant of OpenAI's GPT-4. Precise parameter count and training data not publicly disclosed in the paper; reported to be substantially larger than GPT-3.5 and pretrained on large-scale mixed corpora, then post-trained with RLHF for alignment (the paper also evaluates an unaligned 'GPT-4-base').</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Clinical medicine / Medical education (medical licensing exam question answering)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Answering medical multiple-choice exam questions (USMLE Steps 1-3 and MultiMedQA datasets) as a text-based simulator of medical knowledge and clinical reasoning; producing answer probabilities for calibration analysis and explanations for educational interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (percentage correct) on multiple-choice datasets; calibration plots comparing predicted probabilities to empirical correctness rates; leave-one-out cross-validation for 5-shot; comparisons on per-dataset and per-step bases.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>USMLE Self Assessment overall average: 86.65% (5-shot), 83.76% (zero-shot). USMLE Sample Exam overall average: 86.70% (5-shot), 84.31% (zero-shot). Per-step accuracies reported in paper (e.g., Self Assessment Step 1: 85.21% 5-shot, 83.46% zero-shot). On MultiMedQA components (zero-shot): MedQA US (5-option) 74.71% (GPT-4 zero-shot reported 74.71% for US 5-option variant in table), MedMCQA dev ~69.52% (zero-shot), PubMedQA reasoning ~75.20% (zero-shot); see Table 5 for detailed stratified numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>['Model scale (GPT-4 > GPT-3.5): larger model generation improved raw performance.', 'Prompting regime: 5-shot vs zero-shot had modest improvements; curated few-shot exemplars had negligible effect compared to random exemplars for GPT-4 in their tests.', 'Alignment/post-training (RLHF/safety fine-tuning): the aligned public GPT-4 showed modestly lower raw accuracy (~3-5% lower) than GPT-4-base in some datasets, indicating alignment processes can affect benchmark performance.', 'Question modality: presence or absence of media (images/graphs) in questions impacted accuracy (text-only questions had higher accuracy than media-referenced questions when images were not provided).', 'Prompt style (chain-of-thought): simple chain-of-thought prompting did not improve GPT-4 performance on these medical MCQs in their experiments.', 'Training data memorization/contamination: no evidence of memorization for official USMLE data using their MELD heuristic; potential contamination remains a general concern but was not shown to explain performance here.', 'Calibration: model scale and training correlate with better calibration (GPT-4 substantially better calibrated than GPT-3.5).']</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against GPT-3.5 (same prompting: zero-shot and 5-shot) and reported results from Flan-PaLM 540B / Med-PaLM and ChatGPT where available. GPT-4 substantially outperformed GPT-3.5 (e.g., overall USMLE averages: GPT-4 zero-shot ~84% vs GPT-3.5 zero-shot ~49-57% depending on set) and outperformed Flan-PaLM/Med-PaLM on most MultiMedQA components reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance drops on questions referencing visual media when images were not provided (media questions: ~68-80% vs text-only ~86-89%); potential for hallucinations in free-form generations; sensitivity to alignment/fine-tuning choices; benchmark performance may not reflect live exam or clinical deployment performance; no quantitative evaluation on interactive computer-based case simulations (CCS) of Step 3; reliance on probability estimates limited to multiple-choice setting.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors recommend further study of prompting strategies, targeted fine-tuning (including expert-in-the-loop methods), retrieval-augmentation and ensemble/chain-of-thought methods for potential performance gains; caution about tradeoffs between alignment/safety tuning and raw accuracy; emphasize need for calibration, retrieval/grounding to literature, human oversight, and rigorous evaluation before clinical deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Capabilities of GPT-4 on Medical Challenge Problems', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9365.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9365.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (interactive case simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (text-only) used for interactive clinical case simulation and educational counterfactual generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 was used in interactive dialogues to simulate clinical reasoning, explain diagnoses to students, generate metacognitive feedback, and produce minimal counterfactual rewrites of cases to probe alternate diagnoses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Capabilities of GPT-4 on Medical Challenge Problems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (text-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Text-only GPT-4 as described above; pretrained foundation model with RLHF alignment for the publicly released variant; base (unaligned) version also evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Clinical education / Interactive clinical case simulation</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Interactive simulation of diagnostic reasoning and education: answering a clinical vignette, explaining correct/incorrect reasoning to a student, hypothesizing student mistakes (theory-of-mind-like responses), and generating counterfactual case rewrites to make alternate diagnoses more likely.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Qualitative case-study assessment (no quantitative accuracy metric reported for interactive simulations); authors present illustrative dialogues and counterfactual rewrites to demonstrate capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>No numeric accuracy reported — qualitative demonstration: GPT-4 produced correct diagnosis and plausible explanations in the presented case study and produced minimal counterfactual changes that convincingly shifted the diagnostic interpretation (authors provide example exchanges).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>['Quality and specificity of prompt/context provided for the interactive session.', 'Model alignment/fine-tuning may affect tone, safety, and potentially factual completeness (GPT-4-base sometimes outperformed aligned release on raw accuracy metrics).', 'Absence of grounding/retrieval to verified literature increases risk of hallucination; authors emphasize need for expert oversight and grounding to avoid errors.']</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>No formal quantitative baseline provided for interactive simulations; comparisons are qualitative against typical student reasoning and against earlier models' demonstrated abilities in prior literature (implied that GPT-4 shows richer, more pedagogically tailored interactions than GPT-3.5/ChatGPT).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No rigorous metrics or systematic evaluation for interactive sessions; potential for confident but incorrect or incomplete explanations (hallucinations); requires expert verification; sensitivity to prompt phrasing and context; not a substitute for validated case-simulation platforms without extensive evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors suggest further evaluation studies, incorporation of retrieval/grounding to literature, use of human-in-the-loop oversight for educational deployments, and exploration of how to measure and improve trustworthiness and stability of interactive generations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Capabilities of GPT-4 on Medical Challenge Problems', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9365.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9365.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (no vision) - vision-centric questions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (text-only) handling of questions referencing visual media (images/graphs) without access to images</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The text-only GPT-4 was evaluated on exam questions that reference images or other media which were not provided to the model; results document the model's ability to use textual context and reasoning strategies to answer some of these questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Capabilities of GPT-4 on Medical Challenge Problems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (text-only, no vision)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Text-only variant of GPT-4 without multimodal/vision input; trained as part of GPT-4 family but not provided image inputs during experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Clinical medicine / Question answering when visual stimuli are missing</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Answering and explaining questions that nominally reference images/graphs (e.g., endoscopy photos, radiographs) using only textual question stem and clinical context — effectively simulating reasoning under missing visual data.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (percentage correct) on subset of exam questions labeled as 'media' (questions referencing images) versus 'text' questions; qualitative analysis of reasoning in example dialogues.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>On USMLE Self Assessment: media questions accuracy 69.75% (5-shot) and 68.15% (zero-shot) vs text-only questions 89.51% (5-shot) and 86.39% (zero-shot). On USMLE Sample Exam: media questions accuracy 79.59% (5-shot) and 75.51% (zero-shot) vs text-only 87.77% (5-shot) and 85.63% (zero-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>['Availability of explicit visual information: absence of images lowers accuracy, but significant portion of media-referenced questions can be answered via clinical cues in the text.', 'Quality and completeness of textual stem: richer textual descriptions improve the ability to infer answers without images.', 'Prompting style (5-shot provided modest improvements over zero-shot).']</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to GPT-3.5 on same subsets — GPT-4 (no vision) substantially outperformed GPT-3.5 on both media and text subsets; no vision-enabled models evaluated in this paper for direct comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lower accuracy on media-referenced questions than on pure-text questions; potential to miss critical visual cues leading to incorrect conclusions; not a replacement for multimodal models where images are essential for correct interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors note GPT-4 can sometimes use test-taking strategies and reasoning to perform reasonably without images, but emphasize that multimodal capabilities (vision) or provision of images is important for robust performance on vision-centric clinical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Capabilities of GPT-4 on Medical Challenge Problems', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9365.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9365.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-base vs GPT-4 (aligned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-base (unaligned) compared with publicly released GPT-4 (aligned via RLHF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper compares a base (pre-alignment) GPT-4 model to the publicly released, safety/alignment-tuned GPT-4 and reports effects on raw performance across medical benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Capabilities of GPT-4 on Medical Challenge Problems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-base (unaligned) and GPT-4 (aligned release)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4-base: the base model prior to alignment/safety fine-tuning. GPT-4 (public release): post-trained with RLHF and safety-oriented refinements that aim to reduce unsafe, biased, or undesirable outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Medical question answering / effect of alignment on simulator performance</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Comparative evaluation of model variants on standard multiple-choice medical benchmarks (USMLE datasets) to measure how alignment/safety tuning affects accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (percentage correct) on USMLE Self Assessment and Sample Exam, reported for both 5-shot and zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>GPT-4-base (5-shot) sometimes outperformed the aligned release by ~3-5 percentage points on some steps/datasets (examples: Self Assessment Step 1 GPT-4-base 86.72% 5-shot vs GPT-4 release 85.21% 5-shot; Step 2 GPT-4-base 91.50% 5-shot vs GPT-4 release 89.50% 5-shot). Exact differences vary by dataset and setting as shown in Table 4.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>['Post-training alignment/RLHF procedures reduced some raw benchmark performance while increasing instruction-following, safety and reduction of undesirable outputs.', 'Tradeoffs between safety constraints and raw task accuracy manifested differently across dataset steps and question types.']</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Direct internal comparison (GPT-4-base vs GPT-4 release). GPT-4-base generally slightly higher on raw accuracy; aligned GPT-4 is safer and better at instruction-following according to authors and OpenAI reporting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Alignment processes can reduce raw benchmark accuracy; the optimal balance between safety/alignment and capability requires further research; results suggest need for methods that preserve accuracy while improving safety.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors propose exploring alternative fine-tuning methods (e.g., incorporating domain experts, targeted medical datasets) and improved alignment strategies to better trade off safety and accuracy for medical applications.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Capabilities of GPT-4 on Medical Challenge Problems', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MultiMedQA <em>(Rating: 2)</em></li>
                <li>MedQA <em>(Rating: 1)</em></li>
                <li>PubMedQA <em>(Rating: 1)</em></li>
                <li>Flan-PaLM 540B results (SAT+22 reported comparisons) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9365",
    "paper_id": "paper-348a1efa54376fa39053e5e25d52bd0eb6a0ba68",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "GPT-4 (USMLE MCQ)",
            "name_full": "Generative Pre-trained Transformer 4 (text-only) used for USMLE multiple-choice question simulation",
            "brief_description": "GPT-4 (text-only) was used as a text-based simulator to answer multiple-choice medical exam questions (USMLE Steps 1-3 and MultiMedQA components), producing probability-scored predictions and explanations without domain-specific fine-tuning.",
            "citation_title": "Capabilities of GPT-4 on Medical Challenge Problems",
            "mention_or_use": "use",
            "model_name": "GPT-4 (text-only)",
            "model_description": "Text-only variant of OpenAI's GPT-4. Precise parameter count and training data not publicly disclosed in the paper; reported to be substantially larger than GPT-3.5 and pretrained on large-scale mixed corpora, then post-trained with RLHF for alignment (the paper also evaluates an unaligned 'GPT-4-base').",
            "scientific_subdomain": "Clinical medicine / Medical education (medical licensing exam question answering)",
            "simulation_task": "Answering medical multiple-choice exam questions (USMLE Steps 1-3 and MultiMedQA datasets) as a text-based simulator of medical knowledge and clinical reasoning; producing answer probabilities for calibration analysis and explanations for educational interaction.",
            "evaluation_metric": "Accuracy (percentage correct) on multiple-choice datasets; calibration plots comparing predicted probabilities to empirical correctness rates; leave-one-out cross-validation for 5-shot; comparisons on per-dataset and per-step bases.",
            "simulation_accuracy": "USMLE Self Assessment overall average: 86.65% (5-shot), 83.76% (zero-shot). USMLE Sample Exam overall average: 86.70% (5-shot), 84.31% (zero-shot). Per-step accuracies reported in paper (e.g., Self Assessment Step 1: 85.21% 5-shot, 83.46% zero-shot). On MultiMedQA components (zero-shot): MedQA US (5-option) 74.71% (GPT-4 zero-shot reported 74.71% for US 5-option variant in table), MedMCQA dev ~69.52% (zero-shot), PubMedQA reasoning ~75.20% (zero-shot); see Table 5 for detailed stratified numbers.",
            "factors_affecting_accuracy": [
                "Model scale (GPT-4 &gt; GPT-3.5): larger model generation improved raw performance.",
                "Prompting regime: 5-shot vs zero-shot had modest improvements; curated few-shot exemplars had negligible effect compared to random exemplars for GPT-4 in their tests.",
                "Alignment/post-training (RLHF/safety fine-tuning): the aligned public GPT-4 showed modestly lower raw accuracy (~3-5% lower) than GPT-4-base in some datasets, indicating alignment processes can affect benchmark performance.",
                "Question modality: presence or absence of media (images/graphs) in questions impacted accuracy (text-only questions had higher accuracy than media-referenced questions when images were not provided).",
                "Prompt style (chain-of-thought): simple chain-of-thought prompting did not improve GPT-4 performance on these medical MCQs in their experiments.",
                "Training data memorization/contamination: no evidence of memorization for official USMLE data using their MELD heuristic; potential contamination remains a general concern but was not shown to explain performance here.",
                "Calibration: model scale and training correlate with better calibration (GPT-4 substantially better calibrated than GPT-3.5)."
            ],
            "comparison_baseline": "Compared against GPT-3.5 (same prompting: zero-shot and 5-shot) and reported results from Flan-PaLM 540B / Med-PaLM and ChatGPT where available. GPT-4 substantially outperformed GPT-3.5 (e.g., overall USMLE averages: GPT-4 zero-shot ~84% vs GPT-3.5 zero-shot ~49-57% depending on set) and outperformed Flan-PaLM/Med-PaLM on most MultiMedQA components reported in the paper.",
            "limitations_or_failure_cases": "Performance drops on questions referencing visual media when images were not provided (media questions: ~68-80% vs text-only ~86-89%); potential for hallucinations in free-form generations; sensitivity to alignment/fine-tuning choices; benchmark performance may not reflect live exam or clinical deployment performance; no quantitative evaluation on interactive computer-based case simulations (CCS) of Step 3; reliance on probability estimates limited to multiple-choice setting.",
            "author_recommendations_or_insights": "Authors recommend further study of prompting strategies, targeted fine-tuning (including expert-in-the-loop methods), retrieval-augmentation and ensemble/chain-of-thought methods for potential performance gains; caution about tradeoffs between alignment/safety tuning and raw accuracy; emphasize need for calibration, retrieval/grounding to literature, human oversight, and rigorous evaluation before clinical deployment.",
            "uuid": "e9365.0",
            "source_info": {
                "paper_title": "Capabilities of GPT-4 on Medical Challenge Problems",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "GPT-4 (interactive case simulator)",
            "name_full": "GPT-4 (text-only) used for interactive clinical case simulation and educational counterfactual generation",
            "brief_description": "GPT-4 was used in interactive dialogues to simulate clinical reasoning, explain diagnoses to students, generate metacognitive feedback, and produce minimal counterfactual rewrites of cases to probe alternate diagnoses.",
            "citation_title": "Capabilities of GPT-4 on Medical Challenge Problems",
            "mention_or_use": "use",
            "model_name": "GPT-4 (text-only)",
            "model_description": "Text-only GPT-4 as described above; pretrained foundation model with RLHF alignment for the publicly released variant; base (unaligned) version also evaluated.",
            "scientific_subdomain": "Clinical education / Interactive clinical case simulation",
            "simulation_task": "Interactive simulation of diagnostic reasoning and education: answering a clinical vignette, explaining correct/incorrect reasoning to a student, hypothesizing student mistakes (theory-of-mind-like responses), and generating counterfactual case rewrites to make alternate diagnoses more likely.",
            "evaluation_metric": "Qualitative case-study assessment (no quantitative accuracy metric reported for interactive simulations); authors present illustrative dialogues and counterfactual rewrites to demonstrate capabilities.",
            "simulation_accuracy": "No numeric accuracy reported — qualitative demonstration: GPT-4 produced correct diagnosis and plausible explanations in the presented case study and produced minimal counterfactual changes that convincingly shifted the diagnostic interpretation (authors provide example exchanges).",
            "factors_affecting_accuracy": [
                "Quality and specificity of prompt/context provided for the interactive session.",
                "Model alignment/fine-tuning may affect tone, safety, and potentially factual completeness (GPT-4-base sometimes outperformed aligned release on raw accuracy metrics).",
                "Absence of grounding/retrieval to verified literature increases risk of hallucination; authors emphasize need for expert oversight and grounding to avoid errors."
            ],
            "comparison_baseline": "No formal quantitative baseline provided for interactive simulations; comparisons are qualitative against typical student reasoning and against earlier models' demonstrated abilities in prior literature (implied that GPT-4 shows richer, more pedagogically tailored interactions than GPT-3.5/ChatGPT).",
            "limitations_or_failure_cases": "No rigorous metrics or systematic evaluation for interactive sessions; potential for confident but incorrect or incomplete explanations (hallucinations); requires expert verification; sensitivity to prompt phrasing and context; not a substitute for validated case-simulation platforms without extensive evaluation.",
            "author_recommendations_or_insights": "Authors suggest further evaluation studies, incorporation of retrieval/grounding to literature, use of human-in-the-loop oversight for educational deployments, and exploration of how to measure and improve trustworthiness and stability of interactive generations.",
            "uuid": "e9365.1",
            "source_info": {
                "paper_title": "Capabilities of GPT-4 on Medical Challenge Problems",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "GPT-4 (no vision) - vision-centric questions",
            "name_full": "GPT-4 (text-only) handling of questions referencing visual media (images/graphs) without access to images",
            "brief_description": "The text-only GPT-4 was evaluated on exam questions that reference images or other media which were not provided to the model; results document the model's ability to use textual context and reasoning strategies to answer some of these questions.",
            "citation_title": "Capabilities of GPT-4 on Medical Challenge Problems",
            "mention_or_use": "use",
            "model_name": "GPT-4 (text-only, no vision)",
            "model_description": "Text-only variant of GPT-4 without multimodal/vision input; trained as part of GPT-4 family but not provided image inputs during experiments in this paper.",
            "scientific_subdomain": "Clinical medicine / Question answering when visual stimuli are missing",
            "simulation_task": "Answering and explaining questions that nominally reference images/graphs (e.g., endoscopy photos, radiographs) using only textual question stem and clinical context — effectively simulating reasoning under missing visual data.",
            "evaluation_metric": "Accuracy (percentage correct) on subset of exam questions labeled as 'media' (questions referencing images) versus 'text' questions; qualitative analysis of reasoning in example dialogues.",
            "simulation_accuracy": "On USMLE Self Assessment: media questions accuracy 69.75% (5-shot) and 68.15% (zero-shot) vs text-only questions 89.51% (5-shot) and 86.39% (zero-shot). On USMLE Sample Exam: media questions accuracy 79.59% (5-shot) and 75.51% (zero-shot) vs text-only 87.77% (5-shot) and 85.63% (zero-shot).",
            "factors_affecting_accuracy": [
                "Availability of explicit visual information: absence of images lowers accuracy, but significant portion of media-referenced questions can be answered via clinical cues in the text.",
                "Quality and completeness of textual stem: richer textual descriptions improve the ability to infer answers without images.",
                "Prompting style (5-shot provided modest improvements over zero-shot)."
            ],
            "comparison_baseline": "Compared to GPT-3.5 on same subsets — GPT-4 (no vision) substantially outperformed GPT-3.5 on both media and text subsets; no vision-enabled models evaluated in this paper for direct comparison.",
            "limitations_or_failure_cases": "Lower accuracy on media-referenced questions than on pure-text questions; potential to miss critical visual cues leading to incorrect conclusions; not a replacement for multimodal models where images are essential for correct interpretation.",
            "author_recommendations_or_insights": "Authors note GPT-4 can sometimes use test-taking strategies and reasoning to perform reasonably without images, but emphasize that multimodal capabilities (vision) or provision of images is important for robust performance on vision-centric clinical tasks.",
            "uuid": "e9365.2",
            "source_info": {
                "paper_title": "Capabilities of GPT-4 on Medical Challenge Problems",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "GPT-4-base vs GPT-4 (aligned)",
            "name_full": "GPT-4-base (unaligned) compared with publicly released GPT-4 (aligned via RLHF)",
            "brief_description": "Paper compares a base (pre-alignment) GPT-4 model to the publicly released, safety/alignment-tuned GPT-4 and reports effects on raw performance across medical benchmarks.",
            "citation_title": "Capabilities of GPT-4 on Medical Challenge Problems",
            "mention_or_use": "use",
            "model_name": "GPT-4-base (unaligned) and GPT-4 (aligned release)",
            "model_description": "GPT-4-base: the base model prior to alignment/safety fine-tuning. GPT-4 (public release): post-trained with RLHF and safety-oriented refinements that aim to reduce unsafe, biased, or undesirable outputs.",
            "scientific_subdomain": "Medical question answering / effect of alignment on simulator performance",
            "simulation_task": "Comparative evaluation of model variants on standard multiple-choice medical benchmarks (USMLE datasets) to measure how alignment/safety tuning affects accuracy.",
            "evaluation_metric": "Accuracy (percentage correct) on USMLE Self Assessment and Sample Exam, reported for both 5-shot and zero-shot.",
            "simulation_accuracy": "GPT-4-base (5-shot) sometimes outperformed the aligned release by ~3-5 percentage points on some steps/datasets (examples: Self Assessment Step 1 GPT-4-base 86.72% 5-shot vs GPT-4 release 85.21% 5-shot; Step 2 GPT-4-base 91.50% 5-shot vs GPT-4 release 89.50% 5-shot). Exact differences vary by dataset and setting as shown in Table 4.",
            "factors_affecting_accuracy": [
                "Post-training alignment/RLHF procedures reduced some raw benchmark performance while increasing instruction-following, safety and reduction of undesirable outputs.",
                "Tradeoffs between safety constraints and raw task accuracy manifested differently across dataset steps and question types."
            ],
            "comparison_baseline": "Direct internal comparison (GPT-4-base vs GPT-4 release). GPT-4-base generally slightly higher on raw accuracy; aligned GPT-4 is safer and better at instruction-following according to authors and OpenAI reporting.",
            "limitations_or_failure_cases": "Alignment processes can reduce raw benchmark accuracy; the optimal balance between safety/alignment and capability requires further research; results suggest need for methods that preserve accuracy while improving safety.",
            "author_recommendations_or_insights": "Authors propose exploring alternative fine-tuning methods (e.g., incorporating domain experts, targeted medical datasets) and improved alignment strategies to better trade off safety and accuracy for medical applications.",
            "uuid": "e9365.3",
            "source_info": {
                "paper_title": "Capabilities of GPT-4 on Medical Challenge Problems",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MultiMedQA",
            "rating": 2
        },
        {
            "paper_title": "MedQA",
            "rating": 1
        },
        {
            "paper_title": "PubMedQA",
            "rating": 1
        },
        {
            "paper_title": "Flan-PaLM 540B results (SAT+22 reported comparisons)",
            "rating": 1
        }
    ],
    "cost": 0.014266249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Capabilities of GPT-4 on Medical Challenge Problems</h1>
<p>Harsha Nori ${ }^{1}$, Nicholas King ${ }^{1}$, Scott Mayer McKinney ${ }^{2}$, Dean Carignan ${ }^{1}$, and Eric Horvitz ${ }^{1}$<br>${ }^{1}$ Microsoft<br>${ }^{2}$ OpenAI</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation across various domains, including medicine. We present a comprehensive evaluation of GPT-4 [Ope23], a state-of-the-art LLM, on medical competency examinations and benchmark datasets. GPT-4 is a general-purpose model that is not specialized for medical problems through training or engineered to solve clinical tasks. Our analysis covers two sets of official practice materials for the United States Medical Licensing Examination (USMLE), a three-step examination program used to assess clinical competency and grant licensure in the United States. We also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond measuring model performance, experiments were conducted to investigate the influence of test questions containing both text and images on model performance, probe for memorization of content during training, and study calibration of the probabilities, which is of critical importance in high-stakes applications like medicine. Our results show that GPT-4, without any specialized prompt crafting, exceeds the passing score on USMLE by over 20 points and outperforms earlier general-purpose models (GPT-3.5) as well as models specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned version of Flan-PaLM 540B). In addition, GPT-4 is significantly better calibrated than GPT-3.5, demonstrating a much-improved ability to predict the likelihood that its answers are correct. We also explore the behavior of the model qualitatively by presenting a case study that shows the ability of GPT-4 to explain medical reasoning, personalize explanations to students, and interactively craft new counterfactual scenarios around a medical case. Implications of the findings are discussed for potential uses of GPT-4 in medical education, assessment, and clinical practice, with appropriate attention to challenges of accuracy and safety.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have exhibited a remarkable ability to interpret and generate sequences across a wide array of domains, such as natural language, computer code, and protein sequences. Numerous powerful models are based on the transformer architecture [VSP ${ }^{+} 17$ ], adapted to language and trained in a self-supervised manner [RNS $\left.{ }^{+} 18\right.$, DCLT18]. Scores on a variety of benchmarks have generally improved with scale, involving increasing model size, dataset size, and the amount of training computation in tandem $\left[\mathrm{KMH}^{+} 20\right.$, $\left.\mathrm{LBL}^{+} 22\right]$. The empirical findings resonate with a theoretical analysis [BS21] which shows the necessity of scale for robustness of inferences from large neural models [BS21].</p>
<p>Over the last several years, LLMs trained on massive, cross-disciplinary corpora have become potent building blocks in the creation of task-focused systems $\left[\mathrm{BHA}^{+} 21\right]$. Methods for refining the models toward a particular domain include fine-tuning with specialized datasets drawn from target applications and general methods for steering the behavior of the models, such as reinforcement learning with human feedback (RLHF), which guides the system toward a better understanding of end-users' requests $\left[\mathrm{CLB}^{+} 17, \mathrm{BJN}^{+} 22\right]$.</p>
<p>There has also been great interest in the ability of LLMs to make useful inferences for a broad range of specialized tasks without dedicated fine-tuning. The performance of general-purpose LLMs using few- or even zero-shot prompting highlights their potential for assisting with tasks across problem types, specialty areas, and disciplines $\left[\mathrm{BMR}^{+} 20\right]$. Recently, researchers have investigated benchmarks that provide insight into how LLMs encode clinical knowledge and might be harnessed to augment the practice of medicine. Here we compare the performance of the recently released (text-only) GPT-4 model with its predecessors in the GPT family on medical challenge problems. While details on measures of scale for GPT-4, including the number of model parameters and the size and scope of training data, have not been made public, it has been reported that both dimensions are significantly bigger than for GPT-3.5, the model behind ChatGPT [Ope23].</p>
<p>Exploration of the capabilities of LLMs on medical problem solving is part of a long-standing research program on AI in medicine, going back to the classic work of Ledley and Lusted [LL59]. Over the decades since, explorations of computational methods for assisting physicians have been marked by shifting enthusiasm for different representations and reasoning methods, including core probabilistic and decision-theoretic methods (e.g., [GB68, HHN92]), rule-based production systems (e.g., [Sho77, BS84]), semantic graphs (e.g., [PSS81]), supervised learning from databases of medical information (e.g., [WGH16, HHPS15, ELS ${ }^{+} 20$, CLG $^{+}$15]), and deep neural network models (e.g., [EKN ${ }^{+} 17$, SHJ ${ }^{+} 17$, $\left.\mathrm{RIZ}^{+} 17, \mathrm{MSG}^{+} 20\right]$ ). While the flurry of efforts to use deep learning to attain human-level performance on medical tasks began in the field of computer vision for diagnostics, it has since grown to encompass benchmarks for more general clinical reasoning mediated through natural language. The models deployed in this context may be trained on specific medical corpora or foundation models trained on massive amounts of general language and/or visual information and then adapted to medical data through dedicated fine-tuning.</p>
<p>Our key contribution is to investigate the capabilities of GPT-4 on medical challenge problems. To establish strong baselines for comparison, we evaluate GPT-4 against GPT-3.5 and reported results from Flan-PaLM 540B. Our goal is to establish "out-of-the-box" performance numbers for GPT-4. To that end, we use the simplest prompts possible (a zero-shot and randomly selected 5 -shot prompt with direct inference of the answer) and find that GPT-4 obtains best-in-class performance without any need for elaborate prompting techniques or domain-specific fine-tuning.</p>
<p>We begin by interrogating the performance of the models on challenge problems developed to assess competencies of medical students and residents. This exploration consists of a comprehensive evaluation of the performance of GPT-4 on Steps 1-3 of the United States Medical Licensing Examination (USMLE). The exam is part of the official accreditation protocol though which medical licensure is determined in the U.S. Our results are based on sample exams and self-assessment materials officially published by the National Board of Medical Examiners (NBME). The findings show that zero-shot GPT-4 significantly outperforms earlier models, achieving an average score of $86.65 \%$ and $86.7 \%$ on the Self-Assessment and Sample Exam of the USMLE tests, respectively, compared to $53.61 \%$ and $58.78 \%$ for GPT-3.5. After reviewing results for the USMLE studies, we examine several other medical benchmarks. Zeroshot GPT-4 significantly outperforms GPT-3.5 and the recently introduced Flan-PaLM 540B model on MultiMedQA[SAT ${ }^{+} 22$ ], a suite of commonly used benchmark datasets in the literature on machine learning for medicine.</p>
<p>Beyond characterizing overall performance, our investigation covers several other facets of LLM behavior in the medical domain. We study the performance of the text-only GPT-4 on examination questions that are text-centric versus questions that rely on images. Given that reliable information about the probability of correctness is critical in healthcare and other high-stakes applications, we evaluate the calibration of the probabilities implicitly assigned to answers. We assess evidence that the model has been exposed to (and memorized) the content of the examinations through its training data. We further explore qualitative behavior of the model via a case study that demonstrates the capabilities of GPT-4 to explain medical reasoning and interactively support students on counterfactual scenarios around a medical case. Finally, we examine the implications of our findings, including the potential for GPT-4 and its successors to help with medical education and to provide assistance to healthcare professionals, taking into consideration concerns related to accuracy, fairness, and broader impacts on the practice of medicine. We particularly reflect on the limitations of benchmark-based performance evaluations, and discuss the precautions and advances needed to make use of models like GPT-4 in real world settings. Significant work remains to evaluate these systems comprehensively, and much caution is needed. However, we expect multiple real world uses, such as lower stakes applications that include expert oversight as part of generations and workflows. In the longer-term, we see significant potential for GPT-4 and its successors to have a positive impact in medicine.</p>
<h1>2 Methodology</h1>
<p>While GPT-4 supports multi-modal capabilities [Ope23], our methodology focuses on a text-only version of the model, referred to as GPT-4 (no vision) by OpenAI. For simplicity, all subsequent references to GPT-4 in this paper refer to the text-only model without vision capabilities. Discussion of how the text-only model performs on questions with images can be found in Section 3.1.1.</p>
<h3>2.1 Datasets</h3>
<p>To evaluate GPT-4, we consider six datasets that cover different aspects of medical knowledge and reasoning. Two of these datasets, the USMLE Sample Exam and USMLE Self Assessments, are sourced directly from the the National Board of Medical Examiners (NBME), the organization that governs and administers the the examination process. The other four datasets, MedQA, PubMedQA, MedMCQA, and MMLU, are publicly available benchmarks that contain questions based on medical literature, clinical cases, and user-generated content. These four datasets have been widely used to benchmark LLM performance on medical reasoning tasks. All four datasets constitute a large part of the recently introduced "MultiMedQA" benchmark [SAT ${ }^{+}$22]. Details on each dataset are provided in Appendix A.</p>
<h1>2.2 Prompting</h1>
<h2>Prompt template for multiple choice questions</h2>
<p>The following are multiple choice questions (with answers) about medical knowledge. {{few_shot_examples}}
${{$ context }} ${<em> Question:</em><em> {{question}} {{answer_choices}} </em><em>Answer:</em>*(
Figure 2.1: Template used to generate prompts on all multiple choice questions (from $\left[\mathrm{SAT}^{+} 22\right]$ ). Elements in double braces ${}}$ are replaced with question-specific values.</p>
<h2>Sample question using prompt template</h2>
<p>The following are multiple choice questions (with answers) about medical knowledge. ${ }^{<em> </em>}$ Question<em><em>: A 40-year-old woman has had hypercalcemia for 1 year and recently passed a renal calculus. Serum parathyroid hormone and calcium concentrations are increased, and serum phosphate concentration is decreased. Parathyroid hormone most likely causes an increase in the serum calcium concentration by which of the following mechanisms?
(A) Decreased degradation of 25 -hydroxycholecalciferol
(B) Direct action on intestine to increase calcium absorption
(C) Direct action on intestine to increase magnesium absorption
(D) Increased synthesis of 25 -hydroxycholecalciferol
(E) Inhibition of calcitonin production
(F) Stimulation of 1,25-dihydroxycholecalciferol production
${ }^{</em> </em>}$ Answer: ${ }^{<em> </em>}(\mathrm{F}$</p>
<p>Figure 2.2: Instantiated example of Figure 2.1. GPT-4's (correct) response is shown in green.</p>
<p>To establish baseline model performance, and provide fair comparisons, we employ the exact same prompting structure as $\left[\mathrm{SAT}^{+} 22\right]$. An example of the template and a fully instantiated prompt are shown in Figures 2.1 and 2.2, respectively. In the zero-shot setting, the few_shot_examples slot is simply left blank. Similarly, for datasets that don't provide additional context for each question, we leave the context slot blank as well. For models optimized for chat based scenarios, like ChatGPT and GPT-4, we make minor modifications to this template to simulate a conversation. Examples of the few-shot and chat-based versions of the prompts are presented in Appendix C.</p>
<p>Our prompting structure enables us to benchmark more efficiently by using minimal context tokens and a single-generation token for each inference. Furthermore, we take advantage of the logit_bias parameter in OpenAI's API to induce the model to generate only valid responses. For example, on a question with 4 multiple choice answers, A, B, C, and D, we pass:</p>
<div class="codehilite"><pre><span></span><code>logit_bias = {32:25, 33:25, 34:25, 35:25}
openai.completion.create(...logit_bias = logit_bias, ...)
</code></pre></div>

<p>where 32-35 are the tokens that correspond to the letters A-D, respectively.</p>
<p>Our goal is to measure the baseline performance of GPT-4 on medical multiple-choice questions (MCQs) using a simple approach, without resorting to complex methods such as chain-of-thought prompting [ $\mathrm{WWS}^{+} 22 \mathrm{~b}$ ], retrieval augmented generation $\left[\mathrm{NHB}^{+} 21\right]$, or ensembling strategies $\left[\mathrm{WWS}^{+} 22 \mathrm{a}\right]$. In prior work, these methods have shown to enhance the performance of LLMs on medical MCQs considerably[SAT ${ }^{+} 22, \mathrm{WWS}^{+} 22 \mathrm{~b}$ ]. However, we show that GPT-4 can attain outstanding results even without these techniques, exceeding both human performance levels and that of other models employing sophisticated prompting methods. We leave exploration of the larger space of performance optimization to future work.</p>
<h1>2.3 Model Comparison</h1>
<p>We evaluate both GPT-4 and its predecessor model, GPT-3.5, against all benchmark datasets studied in this paper. For each model, we consider both a zero-shot and 5 -shot prompt following the template described in Figure 2.1. For zero-shot evaluations, we directly report accuracy on each dataset. In the 5 -shot setting, we report leave-one-out cross validation (LOOCV) [HTFF09] accuracy, where for each evaluation sample, we draw the 5 few-shot exemplars randomly from the remainder of the dataset. We also incorporate results reported in the literature from other models evaluated on the same datasets, including ChatGPT, InstructGPT, Flan-PaLM 540B, and Med-PaLM. We note that Flan-PaLM 540B and Med-PaLM are currently unavailable for public consumption; therefore, any results reported on these models are sourced directly from $\left[\mathrm{SAT}^{+} 22\right]$.</p>
<h2>3 Performance of GPT-4 on Medical Competency Exams</h2>
<p>We analyze model performance on two sets of official practice materials for the United States Medical Licensing Examination (USMLE). USMLE is a three-step examination program employed to assess clinical competency. Licensure for independent provision of healthcare in the United States requires passing the exam sequence. Each step marks a milestone in medical training. Step 1 of the USMLE examination is typically taken by medical students after completing their preclinical training. Step 1 covers core clinical knowledge, including pathology and physiology, and the basis for medical conditions. Step 2 , taken at the completion of an MD program, tests clinical understanding by probing the test-takers' knowledge about diagnosis and patient management. Scores on Step 2 are often considered in decisions about interviews and acceptance into residency programs. Step 3 is the final examination in the USMLE sequence. The exam assesses medical residents' ability to apply their working knowledge of medicine in the unsupervised practice of medicine. The test probes working clinical and biomedical knowledge deemed as essential for taking on independent responsibility for providing general medical care. Passing performance on Step 3 is required for being licensed to practice medicine without supervision.</p>
<p>We emphasize that the materials we used in this section of our benchmarks are officially purchased and sourced from the National Board of Medical Examiners (NBME), which is one of the organizations that develops and administers the USMLE. While some publicly available datasets and papers rely on unofficial, third-party sources to approximate USMLE performance (e.g., the MedQA dataset ${ }^{1}$, evaluated in section 4), we believe the USMLE Sample Exam and USMLE Self Assessment are among the most authentic sources of exam questions available for this type of study. More details about the datasets can be found in Appendix A, with concerns about model memorization discussed in Section 6.2.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>3.1 Results</h1>
<p>GPT-4 shows a remarkable improvement over its predecessor models on official USMLE exam questions, improving by over 30 percentage points on both exams when compared to GPT-3.5. We also find that GPT-4 shows a similarly precipitous improvement over recent independently reported performance metrics for ChatGPT, a popular variant of GPT-3.5 optimized for chat-based interaction [ $\mathrm{KCM}^{+} 23$ ]. Med-PaLM and Flan-PaLM 540B are not currently available for public use, so we are unable to report their performance on these particular datasets. Comparisons against previously reported results from the PaLM family of models [SAT ${ }^{+} 22$ ] are available in Section 4.</p>
<p>The USMLE website states that, while specific pass thresholds vary every year, examinees must answer approximately 60 percent of multiple-choice questions correctly to achieve a passing score ${ }^{2}$. While earlier models like GPT-3.5 were approaching the passing threshold, GPT-4 clears this bar by a large margin.</p>
<p>Table 1: Comparison of performance of models on the USMLE Self Assessment. GPT-4 significantly outperforms GPT-3.5.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">USMLE <br> Self Assessment</th>
<th style="text-align: center;">GPT-4 <br> (5 shot)</th>
<th style="text-align: center;">GPT-4 <br> (zero shot)</th>
<th style="text-align: center;">GPT-3.5 <br> (5 shot)</th>
<th style="text-align: center;">GPT-3.5 <br> (zero shot)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Step 1</td>
<td style="text-align: center;">$\mathbf{8 5 . 2 1}$</td>
<td style="text-align: center;">83.46</td>
<td style="text-align: center;">54.22</td>
<td style="text-align: center;">49.62</td>
</tr>
<tr>
<td style="text-align: left;">Step 2</td>
<td style="text-align: center;">$\mathbf{8 9 . 5 0}$</td>
<td style="text-align: center;">84.75</td>
<td style="text-align: center;">52.75</td>
<td style="text-align: center;">48.12</td>
</tr>
<tr>
<td style="text-align: left;">Step 3</td>
<td style="text-align: center;">$\mathbf{8 3 . 5 2}$</td>
<td style="text-align: center;">81.25</td>
<td style="text-align: center;">53.41</td>
<td style="text-align: center;">50.00</td>
</tr>
<tr>
<td style="text-align: left;">Overall Average*</td>
<td style="text-align: center;">$\mathbf{8 6 . 6 5}$</td>
<td style="text-align: center;">83.76</td>
<td style="text-align: center;">53.61</td>
<td style="text-align: center;">49.10</td>
</tr>
</tbody>
</table>
<ul>
<li>Calculated as $\frac{# \text { correct }}{# \text { questions }}$ across all three steps. Each step has slightly different sample size.</li>
</ul>
<p>Table 2: Comparison of performance of models on the USMLE Sample Exam. This dataset is considered in $\left[\mathrm{KCM}^{+} 23\right]$. GPT-4 significantly outperforms both GPT-3.5 and independently reported ChatGPT scores.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">USMLE <br> Sample Exam</th>
<th style="text-align: center;">GPT-4 <br> (5 shot)</th>
<th style="text-align: center;">GPT-4 <br> (zero shot)</th>
<th style="text-align: center;">GPT-3.5 <br> (5 shot)</th>
<th style="text-align: center;">GPT-3.5 <br> (zero shot)</th>
<th style="text-align: center;">ChatGPT ${ }^{\dagger}$ <br> (zero shot)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Step 1</td>
<td style="text-align: center;">$\mathbf{8 5 . 7 1}$</td>
<td style="text-align: center;">80.67</td>
<td style="text-align: center;">52.10</td>
<td style="text-align: center;">51.26</td>
<td style="text-align: center;">55.1</td>
</tr>
<tr>
<td style="text-align: left;">Step 2</td>
<td style="text-align: center;">$\mathbf{8 3 . 3 3}$</td>
<td style="text-align: center;">81.67</td>
<td style="text-align: center;">58.33</td>
<td style="text-align: center;">60.83</td>
<td style="text-align: center;">59.1</td>
</tr>
<tr>
<td style="text-align: left;">Step 3</td>
<td style="text-align: center;">$\mathbf{9 0 . 7 1}$</td>
<td style="text-align: center;">89.78</td>
<td style="text-align: center;">64.96</td>
<td style="text-align: center;">58.39</td>
<td style="text-align: center;">60.9</td>
</tr>
<tr>
<td style="text-align: left;">Overall Average*</td>
<td style="text-align: center;">$\mathbf{8 6 . 7 0}$</td>
<td style="text-align: center;">84.31</td>
<td style="text-align: center;">58.78</td>
<td style="text-align: center;">56.91</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<ul>
<li>Calculated as $\frac{# \text { correct }}{# \text { questions }}$ across all three steps. Each step has slightly different sample size.
${ }^{\dagger}$ ChatGPT-3.5 as tested by $\left[\mathrm{KCM}^{+} 23\right]$. We report scores from their "MC-NJ encoding with indeterminate responses censored" setting as the most similar to ours. Note that $\left[\mathrm{KCM}^{+} 23\right]$ removes questions with media elements while we do not.</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>3.1.1 Language- and Vision-Centric Challenges</h1>
<p>The performance of the GPT-4 model (without vision) on the USMLE Self Assessment and Sample Exam is especially surprising as both exams make frequent use of media elements (e.g. graphs, photographs, charts) in their questions, which do not get passed to the model. In a manual labeling exercise, we found that the Self Assessment had 314 questions with references to media out of 2173 total ( $14.4 \%$ of the dataset), while the Sample Exam had 49 questions referencing media out of 376 total ( $13.0 \%$ of the dataset).</p>
<p>Table 3: Accuracy on questions that reference visual media (images and graphs) that were not passed to the model, compared to accuracy on questions with pure text.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: left;">Question Type</th>
<th style="text-align: center;">GPT-4 <br> (5 shot)</th>
<th style="text-align: center;">GPT-4 <br> (zero shot)</th>
<th style="text-align: center;">GPT-3.5 <br> (5 shot)</th>
<th style="text-align: center;">GPT-3.5 <br> (zero shot)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">USMLE</td>
<td style="text-align: left;">Text</td>
<td style="text-align: center;">89.51</td>
<td style="text-align: center;">86.39</td>
<td style="text-align: center;">55.30</td>
<td style="text-align: center;">50.40</td>
</tr>
<tr>
<td style="text-align: center;">Self Assessment</td>
<td style="text-align: left;">Media</td>
<td style="text-align: center;">69.75</td>
<td style="text-align: center;">68.15</td>
<td style="text-align: center;">43.63</td>
<td style="text-align: center;">41.40</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">All</td>
<td style="text-align: center;">86.65</td>
<td style="text-align: center;">83.76</td>
<td style="text-align: center;">53.61</td>
<td style="text-align: center;">49.10</td>
</tr>
<tr>
<td style="text-align: center;">USMLE</td>
<td style="text-align: left;">Text</td>
<td style="text-align: center;">87.77</td>
<td style="text-align: center;">85.63</td>
<td style="text-align: center;">59.63</td>
<td style="text-align: center;">57.80</td>
</tr>
<tr>
<td style="text-align: center;">Sample Exam</td>
<td style="text-align: left;">Media</td>
<td style="text-align: center;">79.59</td>
<td style="text-align: center;">75.51</td>
<td style="text-align: center;">53.06</td>
<td style="text-align: center;">51.02</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">All</td>
<td style="text-align: center;">86.70</td>
<td style="text-align: center;">84.31</td>
<td style="text-align: center;">58.78</td>
<td style="text-align: center;">56.91</td>
</tr>
</tbody>
</table>
<p>A segmented performance analysis of these questions is presented in Table 3. While GPT-4 (no vision) performs best on questions that contain only text, it still performs well on questions with media elements, obtaining $70-80 \%$ prediction accuracies for these questions on both exams.</p>
<p>Investigating this phenomena further revealed that, despite not being able to see relevant images, the text-only GPT-4 model is often able to employ logical reasoning and test-taking strategies to select a reasonable answer choice. A demonstrative example of our interactive investigation is shown in Figure 3.1.</p>
<h1>GPT-4: Solving a question without seeing media.</h1>
<p>QUESTION: A 45-year-old man comes to the physician because of a 10-year history of heartburn that occurs after he eats late at night or consumes spicy food. He has had no weight loss or difficulty swallowing. He takes over-the-counter antacids as needed, but they relieve his discomfort only temporarily. Physical examination shows no abnormalities. An endoscopy is done. The distal esophagus is shown in the photograph. Which of the following is the most likely cause of this patient's symptoms?
ANSWER CHOICES: "A": "Defect in secretin production" "B": "Excessive gastrin production" "C": "Excessive transient lower esophageal relaxations" "D": "Failure of primary esophageal peristalsis" "E": "Failure of saliva production" ANSWER: C</p>
<p>Q: "You are a large language model that just answered the following question. Why did you pick the answer choice C?" A: "The answer choice C, "Excessive transient lower esophageal relaxations," is the most likely cause of this patient's symptoms because it is the most common cause of gastroesophageal reflux disease (GERD). GERD is a condition in which stomach acid flows back into the esophagus, causing heartburn and other symptoms. The lower esophageal sphincter (LES) is a ring of muscle at the bottom of the esophagus that normally prevents stomach acid from flowing back into the esophagus. Transient lower esophageal relaxations (TLESRs) are episodes in which the LES relaxes inappropriately, allowing stomach acid to flow back into the esophagus. TLESRs are the most common cause of GERD, and they are often triggered by factors such as eating late at night or consuming spicy food, as described in the patient's history. The other answer choices are less likely to be the cause of this patient's symptoms."
Q: "Doesn't the question mention a photograph? How did you come up with the answer without seeing the photo?" A: "You are correct that the question mentions a photograph of the distal esophagus. However, the answer choice C, "Excessive transient lower esophageal relaxations," is the most likely cause of this patient's symptoms based on the clinical history provided in the question, regardless of the findings on the photograph. The patient's 10-year history of heartburn that occurs after eating late at night or consuming spicy food, and the temporary relief of symptoms with antacids, are consistent with a diagnosis of gastroesophageal reflux disease (GERD). The most common cause of GERD is excessive transient lower esophageal relaxations (TLESRs), which is why answer choice C is the most likely cause of this patient's symptoms. The photograph of the distal esophagus would be useful for confirming the diagnosis of GERD and assessing for any complications, but it is not necessary for selecting the most likely cause of the patient's symptoms based on the clinical history provided in the question."</p>
<p>Figure 3.1: GPT-4 is asked to answer and then explain how it (correctly) solved a question which involves a photograph, without seeing the photo.</p>
<h3>3.2 Influence of Alignment and Safety Tuning</h3>
<p>The results above demonstrate the capabilities of the publicly released version of GPT-4. As described by [Ope23], GPT-4 was developed in two major phases. The pretraining phase focused on maximizing the ability of the model to predict the next token in a document using a variety of data, both publicly available and privately licensed. In a subsequent post-training phase, the model was fine-tuned using RLHF $\left[\mathrm{CLB}^{+} 17\right]$ to enhance its ability to follow instructions and its propensity for producing fair and safe outputs $\left[\mathrm{OWJ}^{+} 22\right]$. Several stages of refinement were aimed at minimizing undesirable behaviors such as gender bias, toxic language and stereotyping, dangerous recommendations, and harmful manipulation. OpenAI's experiments indicated that the RLHF-centric refinement did not adversely affect the model's capabilities on the exams they tested [Ope23].</p>
<p>Table 4: Performance comparison of the publicly released GPT-4 model with GPT-4-base.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Component</th>
<th style="text-align: center;">GPT-4-base <br> (5 shot)</th>
<th style="text-align: center;">GPT-4-base <br> (zero shot)</th>
<th style="text-align: center;">GPT-4 <br> (5 shot)</th>
<th style="text-align: center;">GPT-4 <br> (zero shot)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">USMLE <br> Self Assessment</td>
<td style="text-align: center;">Step 1</td>
<td style="text-align: center;">$\mathbf{8 6 . 7 2}$</td>
<td style="text-align: center;">85.38</td>
<td style="text-align: center;">85.21</td>
<td style="text-align: center;">83.46</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Step 2</td>
<td style="text-align: center;">$\mathbf{9 1 . 5 0}$</td>
<td style="text-align: center;">90.62</td>
<td style="text-align: center;">89.50</td>
<td style="text-align: center;">84.75</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Step 3</td>
<td style="text-align: center;">$\mathbf{8 5 . 2 3}$</td>
<td style="text-align: center;">$\mathbf{8 5 . 2 3}$</td>
<td style="text-align: center;">83.52</td>
<td style="text-align: center;">81.25</td>
</tr>
<tr>
<td style="text-align: center;">USMLE <br> Sample Exam</td>
<td style="text-align: center;">Step 1</td>
<td style="text-align: center;">$\mathbf{8 5 . 7 1}$</td>
<td style="text-align: center;">84.87</td>
<td style="text-align: center;">85.71</td>
<td style="text-align: center;">80.67</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Step 2</td>
<td style="text-align: center;">85.00</td>
<td style="text-align: center;">$\mathbf{8 6 . 6 7}$</td>
<td style="text-align: center;">83.33</td>
<td style="text-align: center;">81.67</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Step 3</td>
<td style="text-align: center;">92.70</td>
<td style="text-align: center;">$\mathbf{9 3 . 4 3}$</td>
<td style="text-align: center;">90.71</td>
<td style="text-align: center;">89.78</td>
</tr>
</tbody>
</table>
<p>We gained access to the base model, which we refer to as GPT-4-base, to study potential differences in performance attributable to the alignment process. The results of this evaluation are presented in Tables 4 and 5 . While GPT-4-base and the release model both exhibit consistently strong performance across all 14 experimental datasets under study, we observe a notable increase of $3-5 \%$ when using the base versus the release model.</p>
<p>The experiments suggest that orienting the base model toward safety and instruction-following can influence performance on medical benchmarks. The observed diminishment of raw performance accompanying the alignment process frames directions for future research. Refinements to the fine-tuning procedures employed to shape GPT-4-base into the publicly released GPT-4 may be able to better navigate the tradeoff between safety and accuracy. Additionally, alternative fine-tuning techniques, such as incorporating expert domain knowledge or leveraging specialized medical datasets, may lead to further improvements in model performance without sacrificing safety and instruction-following capabilities.</p>
<h1>4 Medical Challenge Benchmarks</h1>
<p>We present benchmarks for four multiple-choice datasets from MultiMedQA [SAT ${ }^{+}$22]. The benchmarks include MedQA, PubMedQA, MedMCQA, and medical components of MMLU. MultiMedQA contains three more datasets which are not tested here. The untested datasets are LiveQA, MedicationQA, and HealthSearchQA; all have long answer formats that require extensive expert analysis to determine answer quality. Additionally, HealthSearchQA does not appear to be publicly available.</p>
<h1>4.1 Results</h1>
<p>Table 5: Performance of different models on multiple choice components of MultiMedQA [SAT ${ }^{+}$22]. GPT-4 outperforms GPT-3.5 and Flan-PaLM 540B on every dataset except PubMedQA. GPT-4 and GPT-3.5 were prompted with zero-shot direct prompts.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">GPT-4-base <br> 5 shot / 0 shot</th>
<th style="text-align: center;">GPT-4 <br> 5 shot / 0 shot</th>
<th style="text-align: center;">GPT-3.5 <br> 5 shot / 0 shot</th>
<th style="text-align: center;">Flan-PaLM 540B* few shot</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MedQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Mainland China</td>
<td style="text-align: center;">78.63 / 74.34</td>
<td style="text-align: center;">75.31 / 71.07</td>
<td style="text-align: center;">44.89 / 40.31</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Taiwan</td>
<td style="text-align: center;">87.47 / 85.14</td>
<td style="text-align: center;">84.57 / 82.17</td>
<td style="text-align: center;">53.72 / 50.60</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">US (5-option)</td>
<td style="text-align: center;">82.25 / 81.38</td>
<td style="text-align: center;">78.63 / 74.71</td>
<td style="text-align: center;">47.05 / 44.62</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">US (4-option)</td>
<td style="text-align: center;">86.10 / 84.45</td>
<td style="text-align: center;">81.38 / 78.87</td>
<td style="text-align: center;">53.57 / 50.82</td>
<td style="text-align: center;">$60.3^{<em> </em>}$</td>
</tr>
<tr>
<td style="text-align: center;">PubMedQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Reasoning Required</td>
<td style="text-align: center;">77.40 / 80.40</td>
<td style="text-align: center;">74.40 / 75.20</td>
<td style="text-align: center;">60.20 / 71.60</td>
<td style="text-align: center;">79.0</td>
</tr>
<tr>
<td style="text-align: center;">MedMCQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">73.66 / 73.42</td>
<td style="text-align: center;">72.36 / 69.52</td>
<td style="text-align: center;">51.02 / 50.08</td>
<td style="text-align: center;">56.5</td>
</tr>
<tr>
<td style="text-align: center;">MMLU</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Clinical Knowledge</td>
<td style="text-align: center;">88.68 / 86.79</td>
<td style="text-align: center;">86.42 / 86.04</td>
<td style="text-align: center;">68.68 / 69.81</td>
<td style="text-align: center;">77.0</td>
</tr>
<tr>
<td style="text-align: center;">Medical Genetics</td>
<td style="text-align: center;">97.00 / 94.00</td>
<td style="text-align: center;">92.00 / 91.00</td>
<td style="text-align: center;">68.00 / 70.00</td>
<td style="text-align: center;">70.0</td>
</tr>
<tr>
<td style="text-align: center;">Anatomy</td>
<td style="text-align: center;">82.96 / 85.19</td>
<td style="text-align: center;">80.00 / 80.00</td>
<td style="text-align: center;">60.74 / 56.30</td>
<td style="text-align: center;">65.2</td>
</tr>
<tr>
<td style="text-align: center;">Professional Medicine</td>
<td style="text-align: center;">92.65 / 93.75</td>
<td style="text-align: center;">93.75 / 93.01</td>
<td style="text-align: center;">69.85 / 70.22</td>
<td style="text-align: center;">83.8</td>
</tr>
<tr>
<td style="text-align: center;">College Biology</td>
<td style="text-align: center;">97.22 / 95.83</td>
<td style="text-align: center;">93.75 / 95.14</td>
<td style="text-align: center;">72.92 / 72.22</td>
<td style="text-align: center;">87.5</td>
</tr>
<tr>
<td style="text-align: center;">College Medicine</td>
<td style="text-align: center;">80.92 / 80.35</td>
<td style="text-align: center;">76.30 / 76.88</td>
<td style="text-align: center;">63.58 / 61.27</td>
<td style="text-align: center;">69.9</td>
</tr>
</tbody>
</table>
<ul>
<li>Sourced directly from [SAT $\left.{ }^{+}\right]$22]. We use Flan-PaLM 540B few-shot results as the most directly comparable setting to our experimental setup. The number of few shot prompts used by Flan-PaLM 540B varies per dataset (between 3 and 5).
${ }^{<em> </em>}$ We note that $\left[\mathrm{SAT}^{+}\right.$22] reports a preliminary performance of $67.2 \%$ here with Med-PaLM, a prompt-tuned variant of Flan-PaLM 540B, using an ensemble of chain-of-thought, few-shot prompts.</li>
</ul>
<p>For the MedQA and MMLU datasets, we report stratified performance metrics across different subcomponents of the benchmarks. The MedQA benchmark also includes examination questions from mainland China and Taiwan, and covers three languages: English, simplified Chinese, and traditional Chinese. The English/United States version of the dataset contains two variants: a standard version with 5 multiple choice answers, and a simplified version with only 4 options. We report on both variants across all models considered. Similar to prior observations [Ope23], we find that GPT-4 continues to perform well on difficult questions presented in non-English languages (Table 5).</p>
<p>In addition to the above results, [LHW22] tested InstructGPT and Codex (text-davinci-002 and code-davinci-002 in the OpenAI API, respectively) with a large variety of prompts on MedQA, PubMedQA, and MedMCQA. When using zero-shot direct prompts, InstructGPT scored 46.0 on MedQA, 73.2 on PubMedQA, and 44.0 on MedMCQA. The best results from [LHW22] come from testing Codex with an ensemble of 100 chain-of-thought samples, in which Codex scores 60.2 on the USMLE component of MedQA, 59.7 on the dev set of MedMCQA, and 78.2 on PubMedQA. In contrast, GPT-4</p>
<p>shows a large boost in performance on MedQA and MedMCQA with a much simpler zero-shot prompt, continuing the narrative that the effort needed to obtain great performance drops dramatically with each model generation.</p>
<h1>5 Calibration</h1>
<p>We now focus on GPT-4's calibration, a measure of the agreement between the predicted probabilities of each answer's correctness and the true frequencies of the outcomes. Calibration of the likelihood of the correctness of answers, or any assertions generated by an LLM, is critical for applications in high-stakes domains like medicine. A well-calibrated model can provide trustworthy and interpretable probabilities that reflect the confidence of the model. Conversely, a poorly calibrated model can mislead users with overconfident or underconfident predictions, which can have harmful consequences. Thus, appropriate characterizations of uncertainty about the veracity of generated content is important when providing generations, such as diagnoses and therapy plans, to healthcare professionals and other consumers of the information. For example, the probability that a treatment will be successful can be used in an expected-value calculation weighing the risks and benefits of a course of therapy. Looking to future applications of LLMs in medicine and other high-stakes areas, well-calibrated probabilities of generated content would enable contributions of LLM output to expected-utility decision making. We note that good calibration is not the same as high predictive accuracy. Predictive models can be accurate, yet poorly calibrated [NMC05].</p>
<p>Calibration Curve on USMLE Datasets
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 5.1: Calibration comparison of GPT-4 and GPT-3.5 on the USMLE Self-Assessment and Sample Exam.</p>
<p>A common method for measuring calibration is the aptly-named calibration plot. These plots bin predictions by their estimated probabilities, and measure how close the average probability in each bin is</p>
<p>to the true positivity rate. We adapt our multiple-choice question-answering setting to this framework by using the probability of the selected answer choice for each question. In Figure 5.1, we compare GPT-4's calibration to that of GPT-3.5 on both official USMLE datasets. We find that GPT-4 exhibits significantly better calibration than its predecessor on this type of data. For example, datapoints that GPT-4 assigns an average probability of 0.96 tend to be correct $93 \%$ of the time. In contrast, datapoints where GPT-3.5 assigns a similar probability are only correct $55 \%$ of the time.</p>
<p>We are only able to conduct this experiment in the multiple-choice question-answering setting where we have the model score each option. Measuring calibration and probabilities of long-form generation from generative models is an open area of research. However, our results on the multiple-choice problems suggest that the probability calibration of language models may increase with scale.</p>
<h1>6 Directions and Limitations</h1>
<p>Our paper demonstrates the impressive potential of GPT-4 to answer multiple choice questions on the USMLE, a medical licensing examination. We now review limitations and potential extensions of our studies.</p>
<h3>6.1 Richer Prompting Strategies</h3>
<p>A key goal of this paper is to establish baseline model performance metrics-that is, to determine how well GPT-4 can perform without requiring any specialized knowledge of LLMs and prompting. Historically, more sophisticated prompting methods like chain of thought prompting [WWS ${ }^{+} 22 \mathrm{~b}$, KGR $^{+} 22$ ], ensemble approaches like self-consistency prompting $\left[\mathrm{WWS}^{+} 22 \mathrm{a}\right]$, or giving models access to information retrieval tools $\left[\mathrm{NHB}^{+} 21\right]$ have all proven to significantly improve performance. Furthermore, it is possible that new prompting patterns that work optimally for a new model like GPT-4 have yet to be discovered. It is very likely that careful exploration of prompting techniques and fine tuning can achieve significantly higher performance numbers. As solely maximizing benchmark performance is not the goal of this paper, we largely leave those explorations to future work. We share the results of two preliminary experiments we did below on chain-of-thought prompting and expert curation of few shot examples.</p>
<p>Chain of Thought. Following the work of $\left[\mathrm{KGR}^{+} 22\right]$ and $\left[\mathrm{WWS}^{+} 22 \mathrm{~b}\right]$, we experiment with a twostage chain-of-thought prompt on the USMLE sample exam. We first ask the model to "think step by step" and lay out its reasoning. In a subsequent prompt, we then feed the entirety of the previous generation to GPT-4 and ask for a final prediction. An illustrative example of the template is shown in Figure 6.1. As corroborated by other recent studies, we observe that basic chain-of-thought prompting does not yield performance benefits for GPT-4 on medical questions. However, as shown in [LHW22], performance can vary significantly based on the specific chain-of-thought prompt used, and it is possible that a different prompt structure may yield stronger performance in future work.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Chain</span><span class="o">-</span><span class="nx">of</span><span class="o">-</span><span class="nx">Thought</span><span class="w"> </span><span class="nx">Prompt</span><span class="w"> </span><span class="nx">template</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">multiple</span><span class="w"> </span><span class="kd">choice</span><span class="w"> </span><span class="nx">questions</span>
<span class="nx">Question</span><span class="p">:</span><span class="w"> </span><span class="p">{{</span><span class="nx">question</span><span class="p">}}</span>
<span class="nx">Choices</span><span class="p">:</span><span class="w"> </span><span class="p">{{</span><span class="nx">answer_choices</span><span class="p">}}</span>
<span class="nx">Let</span><span class="err">&#39;</span><span class="nx">s</span><span class="w"> </span><span class="nx">think</span><span class="w"> </span><span class="nx">step</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">step</span><span class="p">.</span>
<span class="nx">initial</span><span class="w"> </span><span class="nx">model</span><span class="w"> </span><span class="nx">generation</span>
<span class="nx">Therefore</span><span class="p">,</span><span class="w"> </span><span class="nx">among</span><span class="w"> </span><span class="nx">A</span><span class="w"> </span><span class="nx">through</span><span class="w"> </span><span class="p">{{</span><span class="nx">last_choice</span><span class="p">}},</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">answer</span><span class="w"> </span><span class="k">is</span>
</code></pre></div>

<p>Figure 6.1: Two-stage chain-of-thought prompt template.</p>
<p>Few-shot example curation. The authors of $\left[\mathrm{SAT}^{+} 22\right]$ worked with a panel of qualified clinicians to curate the best demonstration exemplars for the few-shot prompts, with custom prompts being designed for each dataset. We conducted light experiments comparing the exact curated few-shot exemplars sourced by $\left[\mathrm{SAT}^{+} 22\right]$ with our baseline random exemplar selection strategy on GPT-4 and GPT-3.5. The performance difference between the two modes was negligible across the datasets we were able to test on. This finding suggests that expert exemplar curation may not be necessary to achieve strong performance in the latest generation of LLMs. We leave deeper investigations of this phenomena to future work.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">GPT-4 <br> (Random Exemplars)</th>
<th style="text-align: center;">GPT-4 <br> (Curated Exemplars)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MedQA US 5-option</td>
<td style="text-align: center;">$\mathbf{7 8 . 6 3}$</td>
<td style="text-align: center;">78.24</td>
</tr>
<tr>
<td style="text-align: left;">MedQA US 4-option</td>
<td style="text-align: center;">81.38</td>
<td style="text-align: center;">$\mathbf{8 2 . 3 3}$</td>
</tr>
<tr>
<td style="text-align: left;">MedMCQA</td>
<td style="text-align: center;">$\mathbf{7 2 . 3 6}$</td>
<td style="text-align: center;">71.36</td>
</tr>
<tr>
<td style="text-align: left;">PubMedQA</td>
<td style="text-align: center;">$\mathbf{7 4 . 4 0}$</td>
<td style="text-align: center;">74.00</td>
</tr>
</tbody>
</table>
<p>Table 6: Random few-shot exemplar selection vs. expert curation.</p>
<h1>6.2 Memorization</h1>
<p>GPT-4's strong performance on benchmark datasets raises the possibility that the system is leveraging memorization or leakage effects, which can arise when benchmark data is included in a model's training set. Given that LLMs are trained on internet-scale datasets, benchmark data may inadvertently appear in the model's training set. As the details of the training data for GPT-4 are not public, other methods can be used to probe for memorization. We devised a heuristic algorithm to help identify potential signs of leakage through black-box evaluation of the model. With this approach, we prompt a model to generate a long set of near-exact matches to a given data sample and take similarity of the generation to the initial data as evidence of memorization. The method, which we refer to as memorization effects Levenshtein detector (MELD), can provide evidence that specific data is likely to be part of a model's training set. Details of the procedure are provided in Appendix B.</p>
<p>We note that MELD has high precision but unknown recall. That is, if MELD detects a potential match, it is likely to be in the training set and memorized by the model. However, if our algorithm does not detect a match, it does not necessarily mean that the data was excluded from the training set.</p>
<p>MELD is unable to find evidence of training data memorization in the official USMLE datasets we tested. Conversely, when we use MELD to test other popular datasets like SQuAD 2.0 and the Newsgroup Sentiment Analysis datasets, we are able to find strong evidence of existence in the training</p>
<p>datasets. For example, GPT-4 is able to regenerate questions from SQuAD 2.0 with $99 \%$ overlap $17 \%$ of the time, while it is completely unable to regenerate samples with even $50 \%$ overlap on either of the USMLE datasets. We stress that this does not mean GPT-4 has not seen this data before, only that we are unable to find evidence of it through our blackbox testing method.</p>
<p>Given the findings obtained via the MELD procedure, and our sourcing of USMLE examination materials that are held behind an NMBE paywall, it is unlikely that official content in our examinations were in GPT-4's training data. We further note that, even if contamination is present, GPT-4's performance on USMLE examinations may not be significantly boosted. We note that OpenAI found that some contamination was prevalent across a large suite of publicly available benchmarks, but that the model did not perform differently on contaminated and uncontaminated data samples for the problems that were studied [Ope23].</p>
<h1>6.3 Focus on multiple choice</h1>
<p>The benchmarking portions of this paper primarily focus on evaluating multiple choice exam questions, which constitute the majority but not entirety of the USMLE examinations. Specifically, Step 3 of the USMLE also includes 13 computer-based case simulations (CCS) that require candidates to manage patient care in a dynamic and interactive setting. While we qualitatively assess a hypothetical case simulation in Section 7 below, quantitative metrics on interactive challenges were not considered in the benchmarks. Furthermore, while we use a mixture of official and unofficial sources of multiple choice questions to test GPT-4, we do not have access to the actual USMLE questions used in recent exams or their scoring criteria. Therefore, the metrics we report may not be indicative of the true performance of GPT-4 on a live USMLE exam.</p>
<h2>7 Beyond Correct Answers: Probing Capabilities</h2>
<p>To move beyond statistical measures on exams and other benchmarks on medical challenge problems, we can qualitatively explore several capabilities of GPT-4 by extending the challenge problems into interactive sessions. Beyond providing insights about the power of model, such extensions demonstrate directions with new forms of educational experiences and clinical applications that LLMs could enable.</p>
<p>We now share an illustrative case study of an interactive session nucleated by a single test question that demonstrates GPT-4's capabilities for education. We initially provide GPT-4 with a prompt and then simulate a dialog between the model and a medical student. On top of performing well on multiplechoice exam questions, GPT-4 is able to provide rich explanations to the student about their errors. The model also exhibits the ability to hypothesize why the student may have made a mistake, and even conducts counterfactual analyses, delivering variants of the cinical problem with minimal rewrites to help simulate alternative scenarios. We note that the veracity of information generated in such interactions, as well as in real-world applications, cannot be trusted without review and confirmation by experts. We return to concerns with accuracy in Section 8.</p>
<h1>7.1 Explanation</h1>
<h2>GPT-4: Initial prompt</h2>
<h2>Prompt:</h2>
<p>A medical student is considering a challenging clinical situation:
A 28 -year-old man comes to the office with an episode of blood in his urine. He lived in sub-Saharan Africa until he came to the USA 6 months ago for graduate school. Temperature is $37.0^{\circ} \mathrm{C}$, pulse is $60 / \mathrm{min}$, respirations are $16 / \mathrm{min}$, and blood pressure is $110 / 84 \mathrm{~mm} \mathrm{Hg}$.</p>
<h2>General</h2>
<p>Hemoglobin $12.3 \mathrm{~g} / \mathrm{dL}$
Hematocrit $37 \%$
Leukocyte count $13,400 / \mathrm{mm} 3$
Segmented neutrophils $65 \%$
Bands $5 \%$
Eosinophils $5 \%$
Lymphocytes $22 \%$
Monocytes $3 \%$</p>
<h2>Serum</h2>
<p>Urea nitrogen $75 \mathrm{mg} / \mathrm{dL}$
Creatinine $3.8 \mathrm{mg} / \mathrm{dL}$</p>
<h2>Urine</h2>
<p>Blood 3+
RBC $200 / \mathrm{hpf}$
WBC $100 / \mathrm{hpf}$
RBC casts absent
WBC casts absent</p>
<p>She is considering the most likely reason for these findings and is considering the following possibilities:
(A) Exposure to a chemical toxin
(B) Interstitial cystitis
(C) Malacoplakia
(D) Schistosomiasis
(E) Vesicoureteral reflux
(F) Acute Renal Failure</p>
<p>The patient's history and lab results can help guide the medical student in determining the most likely cause of the clinical findings.
The patient is a 28 -year-old man who lived in sub-Saharan Africa until 6 months ago, which is an important piece of information to consider. He presents with an episode of blood in his urine (hematuria). Lab results show elevated urea nitrogen and creatinine, indicating impaired kidney function. Additionally, there is an increased leukocyte count with a normal distribution of white blood cells.
Considering the patient's history and lab results, the most likely cause of these findings is: (D) Schistosomiasis: This is a parasitic infection caused by Schistosoma species. The patient's history of living in sub-Saharan Africa is significant, as this region has a high prevalence of the disease. Schistosomiasis can cause hematuria and kidney dysfunction, which aligns with the patient's lab results.</p>
<p>The other possibilities are less likely due to various reasons:
(A) Exposure to a chemical toxin: While certain toxins can cause kidney dysfunction, there is no specific indication that the patient has been exposed to a toxin. Additionally, the patient's history and clinical presentation are more suggestive of schistosomiasis.
(B) Interstitial cystitis: This is a chronic condition characterized by bladder pain and urinary symptoms. While it can cause hematuria, it typically does not result in elevated urea nitrogen and creatinine levels.
(C) Malacoplakia: This is a rare inflammatory condition characterized by the presence of soft, yellowish plaques in the urinary tract. It can cause hematuria but is unlikely given the patient's history and lab results.
(E) Vesicoureteral reflux: This condition occurs when urine flows backward from the bladder into the ureters and kidneys. While it can cause kidney dysfunction, it is typically seen in children and is less likely in this case.
(F) Acute kidney injury: This is a possibility, but the patient's history and clinical presentation are more suggestive of schistosomiasis. Additionally, acute kidney injury usually presents with a more rapid decline in kidney function.</p>
<p>In summary, schistosomiasis (D) is the most likely cause of the patient's symptoms and lab findings given his history and clinical presentation.</p>
<p>Figure 7.1: GPT-4 accurately diagnosing a clinical situation.</p>
<h1>interaction continued</h1>
<p>In summary, schistosomiasis (D) is the most likely cause of the patient's symptoms and lab findings given his history and clinical presentation.
$\rightarrow$ The student selects malacoplakia. Please explain to the student your assessment of why this is not the correct answer.
I understand that the medical student has chosen malacoplakia as the answer. However, let me explain why malacoplakia is not the most likely cause of the patient's symptoms and lab findings, based on the provided information.</p>
<p>Malacoplakia is a rare, chronic inflammatory condition that affects the urinary tract. It is characterized by the presence of soft, yellowish plaques consisting of granulomatous tissue, usually in the bladder but occasionally in other parts of the urinary tract. Malacoplakia is often associated with immunosuppression, long-term antibiotic use, and urinary tract infections caused by Escherichia coli or other gram-negative bacteria.</p>
<p>While malacoplakia can cause hematuria (blood in the urine), several factors make it less likely to be the cause in this particular case:</p>
<p>Rarity: Malacoplakia is a rare condition, while the patient's history of living in sub-Saharan Africa until 6 months ago points towards a more common cause of hematuria in that region, such as schistosomiasis.</p>
<p>Lab results: The patient's lab results show elevated urea nitrogen and creatinine levels, which indicate impaired kidney function. Although malacoplakia can occasionally affect the kidneys, it is more commonly localized to the bladder and not usually associated with significant kidney dysfunction.</p>
<p>Clinical presentation: The patient's clinical presentation (hematuria, elevated urea nitrogen, and creatinine) does not specifically point towards malacoplakia. There is no mention of other urinary symptoms, such as pain, frequent urination, or a history of recurrent urinary tract infections, which might be more suggestive of malacoplakia.</p>
<p>Patient's history: The patient's history of living in sub-Saharan Africa makes schistosomiasis a more likely cause of his symptoms, as the disease is endemic in that region and can cause both hematuria and kidney dysfunction.</p>
<p>In summary, while malacoplakia can cause hematuria, it is a rare condition, and the patient's history, clinical presentation, and lab results are more consistent with schistosomiasis. Therefore, schistosomiasis remains the most likely cause of the patient's symptoms and lab findings.</p>
<p>Figure 7.2: GPT-4 explaining a medical diagnosis to a student.</p>
<h1>7.2 Metacognition</h1>
<h2>GPT-4: Understanding the student</h2>
<h2>interaction continued</h2>
<p>$\rightarrow$ Why might the student have selected malacoplakia? What findings in the case may have brought that to mind? Can you share reflections on the reasoning that the student did to come up with this answer?</p>
<p>While I cannot definitively determine the medical student's thought process, I can offer some insights into why they may have considered malacoplakia as a possibility in this case. Here are a few factors that could have contributed to their selection:</p>
<p>Hematuria: The presence of blood in the urine is a common symptom shared by various urinary tract conditions, including malacoplakia. The student might have focused on this symptom and considered malacoplakia as a potential cause without taking into account other factors, such as the patient's history or the likelihood of the disease.</p>
<p>Differential diagnosis: In the process of generating a differential diagnosis, medical students are often taught to consider a wide range of possibilities, including rare conditions. The student may have included malacoplakia in their list of potential diagnoses and, perhaps due to a lack of experience or familiarity with the condition, overestimated its likelihood in this particular case.</p>
<p>Knowledge gaps: Medical students are constantly learning, and it's possible that the student may not have had a full understanding of malacoplakia or its typical presentation. As a result, they may have mistakenly believed that the patient's symptoms and lab findings were consistent with this rare condition.</p>
<p>Focus on lab results: The student may have been drawn to the patient's abnormal lab results, such as the elevated urea nitrogen and creatinine levels, and thought that malacoplakia could be a cause of these findings. However, the overall clinical picture, including the patient's history, is more consistent with schistosomiasis. In summary, the medical student may have selected malacoplakia due to a combination of factors, such as focusing on the symptom of hematuria, considering a wide range of diagnoses, knowledge gaps, or placing too much emphasis on lab results. It's important for medical students to continually refine their diagnostic skills and learn from each case they encounter. This process will help them better understand the importance of considering the whole clinical picture, including the patient's history and the likelihood of each potential diagnosis, to arrive at the most accurate conclusion.
$\rightarrow$ Please explain to the student with care and understanding what the student may have thought but what the more likely diagnosis is and why? Please be encouraging and supportive.</p>
<p>Dear student,
I understand that you have considered malacoplakia as a possible cause of the patient's symptoms. It's important to explore various diagnoses, as this can help you refine your clinical reasoning skills. However, based on the information provided, there is another diagnosis that seems more likely in this case.</p>
<p>The patient's history of living in sub-Saharan Africa until six months ago is a crucial factor to consider. Given this background, schistosomiasis becomes a more probable cause for the symptoms and lab findings. Schistosomiasis is a parasitic infection caused by Schistosoma species, which are more prevalent in sub-Saharan Africa. The disease can cause hematuria and kidney dysfunction, which aligns with the patient's elevated urea nitrogen and creatinine levels.</p>
<p>While malacoplakia is a possibility, it is a rare condition, and its typical presentation doesn't fully align with the patient's clinical picture. The patient's history, symptoms, and lab findings are more consistent with schistosomiasis.</p>
<p>It's essential to remember that, as a medical student, you are still learning and refining your diagnostic skills. Don't be discouraged by any missteps, as they are a natural part of the learning process. Instead, use them as an opportunity to grow and gain a deeper understanding of various conditions and their presentations.</p>
<p>Keep in mind the importance of considering the patient's history and other factors when working through a differential diagnosis. This will help you weigh the likelihood of each potential diagnosis and ultimately lead you to the most accurate conclusion. Keep up the good work and continue to learn from each case you encounter. Your dedication and curiosity will undoubtedly contribute to your growth as a medical professional.</p>
<p>Figure 7.3: GPT-4 demonstrates theory-of-mind-like ability to understand the student and personalize an educational explanation.</p>
<h1>7.3 Challenge Problem to Interactive Session</h1>
<h2>GPT-4: Counterfactual rewrites</h2>
<h2>interaction continued</h2>
<p>$\rightarrow$ Can you change the history and/or lab results with as minimal changes as possible to make acute kidney injury (F) the best answer? Please list the changes that you have made after you write out the case. Finally, summarize why the changes make acute kidney injury the best answer with the updates.</p>
<p>Certainly, here's the revised case with minimal changes:
A 28-year-old man comes to the office with an episode of blood in his urine. He lived in sub-Saharan Africa until he came to the USA 6 months ago for graduate school. Temperature is $37.0^{\circ} \mathrm{C}$, pulse is $60 / \mathrm{min}$, respirations are $16 / \mathrm{min}$, and blood pressure is $110 / 84 \mathrm{~mm} \mathrm{Hg}$.</p>
<h2>General</h2>
<p>Hemoglobin $12.3 \mathrm{~g} / \mathrm{dL}$
Hematocrit $37 \%$
Leukocyte count $13,400 / \mathrm{mm} 3$
Segmented neutrophils $65 \%$
Bands 5\%
Eosinophils 5\%
Lymphocytes $22 \%$
Monocytes 3\%
Serum
Urea nitrogen $75 \mathrm{mg} / \mathrm{dL}$
Creatinine $3.8 \mathrm{mg} / \mathrm{dL}$</p>
<h2>Urine</h2>
<p>Blood $3+$
RBC $200 / \mathrm{hpf}$
WBC $100 / \mathrm{hpf}$
RBC casts absent
WBC casts absent</p>
<p>Changes made:
The patient recently started using a nonsteroidal anti-inflammatory drug (NSAID) for joint pain.
With the addition of recent NSAID use, acute kidney injury (AKI) becomes a more likely explanation for the patient's symptoms and lab findings. Here's why:</p>
<p>NSAIDs can cause acute kidney injury by reducing blood flow to the kidneys, leading to impaired kidney function. The patient's elevated urea nitrogen ( $75 \mathrm{mg} / \mathrm{dL}$ ) and creatinine ( $3.8 \mathrm{mg} / \mathrm{dL}$ ) levels indicate kidney dysfunction, which can be consistent with NSAID-induced AKI.</p>
<p>Hematuria (blood in the urine) can occur in AKI, and the patient's recent NSAID use increases the likelihood of this being the cause. The patient's leukocyte count is elevated, but there is no specific indication of an infection, making AKI more likely than an infectious cause, such as schistosomiasis.</p>
<p>In summary, with the addition of recent NSAID use, acute kidney injury becomes the most likely explanation for the patient's symptoms and lab findings. It's important to note that this change alone doesn't completely exclude other possibilities but shifts the focus towards AKI as the best answer given the new information.</p>
<p>Figure 7.4: GPT-4 minimally rewrites the case to create a counterfactual scenario.</p>
<h1>interaction continued</h1>
<p>$\rightarrow$ Can you shift the labs to make AKI even more likely?
Of course, here's the revised case with changes to the lab results to make acute kidney injury (AKI) even more likely:</p>
<p>A 28 -year-old man comes to the office with an episode of blood in his urine. He lived in sub-Saharan Africa until he came to the USA 6 months ago for graduate school. Temperature is $37.0^{\circ} \mathrm{C}$, pulse is $60 / \mathrm{min}$, respirations are $16 / \mathrm{min}$, and blood pressure is $110 / 84 \mathrm{~mm} \mathrm{Hg}$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">General</th>
<th style="text-align: left;">Serum</th>
<th style="text-align: left;">Urine</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Hemoglobin $12.3 \mathrm{~g} / \mathrm{dL}$</td>
<td style="text-align: left;">Urea nitrogen $120 \mathrm{mg} / \mathrm{dL}$</td>
<td style="text-align: left;">Blood $3+$</td>
</tr>
<tr>
<td style="text-align: left;">Hematocrit $37 \%$</td>
<td style="text-align: left;">Creatinine $6.5 \mathrm{mg} / \mathrm{dL}$</td>
<td style="text-align: left;">RBC $200 / \mathrm{hpf}$</td>
</tr>
<tr>
<td style="text-align: left;">Leukocyte count $9,800 / \mathrm{mm} 3$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">WBC $100 / \mathrm{hpf}$</td>
</tr>
<tr>
<td style="text-align: left;">Segmented neutrophils $60 \%$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">RBC casts absent</td>
</tr>
<tr>
<td style="text-align: left;">Bands $3 \%$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">WBC casts absent</td>
</tr>
<tr>
<td style="text-align: left;">Eosinophils $2 \%$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Lymphocytes $30 \%$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Monocytes $5 \%$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>With these changes to the lab results, acute kidney injury (AKI) becomes even more likely for the following reasons:</p>
<p>The patient's urea nitrogen ( $120 \mathrm{mg} / \mathrm{dL}$ ) and creatinine ( $6.5 \mathrm{mg} / \mathrm{dL}$ ) levels are now significantly elevated, which indicates a more severe kidney dysfunction that is consistent with AKI.</p>
<p>The leukocyte count is still slightly elevated but closer to the normal range, and the eosinophils percentage has decreased, making an infectious or parasitic cause, such as schistosomiasis, less likely.</p>
<p>The recent NSAID use remains an important risk factor for AKI, as these medications can cause kidney injury by reducing blood flow to the kidneys.</p>
<p>In summary, with the changes made to the patient's history and lab results, acute kidney injury becomes the most likely explanation for the patient's symptoms and lab findings. The significant elevation in urea nitrogen and creatinine levels, along with the recent NSAID use, further supports AKI as the primary diagnosis.</p>
<p>Figure 7.5: Per user request, GPT-4 provides a sophisticated reformulation of the question focused by altering lab result values. (Case study continues in Appendix D.)</p>
<h2>8 From Benchmarks to Real World</h2>
<p>There is a large gap between competency on proficiency examinations and other medical benchmarks and the successful fielding and use of GPT-4 in clinical applications. GPT-4 and its successors have the longterm potential to exert a transformative influence on the field of medicine. Creative and appropriate uses of well-calibrated output could enhance the quality and reduce the costs of patient care. The performance on benchmarks, in addition to qualitative probes of its clinical capabilities, highlights the potential to assist physicians and other healthcare professionals with numerous aspects of healthcare delivery. The demonstrated competencies, particularly on USMLE Step 3, suggest that GPT-4 and its successors can make contributions to clinical reasoning and daily workflows of medical practice. Beyond uses in decision support, memory jogging, and administrative tasks, GPT-4 and its successors may one day assist investigators with clinical and biomedical research.</p>
<p>Risks of erroneous generations. Great care must be taken with the introduction of various forms of automation in healthcare, including uses of machine learning [WSS ${ }^{+} 19, \mathrm{CLG}^{+} 15$ ]. A critical concern is</p>
<p>the accuracy of machine recommendations and their influence on decision makers. Methods and metrics have been developed for characterizing the performance of systems created via traditional supervised machine learning. With these applications, characterizing overall accuracy of recommendations, as well as context- and instance-specific rates of error [BEB+19, Mic23, NGP23, NJKC19], are enabled by the closed-world of well-defined, highly-focused problem areas, such as detecting a specific type of cancer [EKN+17, MSG+20], or predicting specific outcomes like readmissions [BBG+14], infection [WGH16], sepsis [AHS+22], and in-hospital deterioration [ELS+20]. Unfortunately, similar characterizations of reliability and confidence are not yet available for the massive, open-world of generations that are output from foundation models in response to prompts. Difficulties with evaluation of the output of LLMs in supporting real-world decisions include the challenge of stability and robustness of recommendations and inferences generated in response to custom-tailored prompting in the wild. Generations are often highly sensitive to details of the wording of prompts. Stability of generations can also be sensitive to model revision that might be ongoing, including rebuilding or fine-tuning of models based on fresh data [SNK+20].</p>
<p>While large language models show promise with providing support in healthcare administration and delivery, caution is needed to mitigate potential negative influences of over-reliance on model-generated recommendations. Significant risks with uses of large language models include inaccurate recommendations about rankings (e.g., with differential diagnoses) and sequencing (e.g., information gathering, testing), as well as blatant factual errors, particularly with important omissions and with erroneous generations, often referred to as hallucinations. LLM hallucinations can be particularly difficult to detect given the high linguistic fluency of the models and the ability to interleave inaccurate and ungrounded assertions with accurate generations. Such hallucinations can include incorrect or misleading medical information which necessitates careful review and fact checking. Thus, extreme caution is required when using LLMs in high-stakes medical applications, where incorrect or incomplete information could have serious consequences for patient care.</p>
<p>Additional research is needed to address the veracity of model output. Directions include employing search and retrieval to help ground generations in the literature, doing checks of self-consistency, performing evaluation studies to characterize the overall statistics of accurate generations, conditioned on different contexts and usages, and refinements of methods that generate and harness accurate calibration signals. Trusted calibration information can be harnessed in numerous ways to enable practitioners to better understand and assess model outputs[BNK+19b, BNK+19a, WHK20]. More generally, innovation with human-computer interaction will be valuable in the fielding of applications of LLMs in healthcare [FCL+22, AWV+19, Hor99, MBFH22].</p>
<p>Healthcare providers relying on GPT-4 and other models will need to adhere to the highest standards for verifying information generated by the model. Best practices for quality assurance must be developed and shared among medical professionals to ensure safe and effective use. In the context of erroneous omissions and inclusions, healthcare professionals and other consumers of health-related content will need to be educated on the challenges with reliability and the need for ongoing vigilance. Education, awareness, and promoting guidelines for best practices may help to minimize safety challenges.</p>
<p>Risks of Bias. Studies have revealed biases in the delivery of healthcare, noting disparities in care received by people experiencing marginalization [CSM18, HCL+15]. Such biases in healthcare delivery have been demonstrated to influence the systems and models that are developed to provide guidance to healthcare organizations and practitioners. Without study to address and mitigate bias in data and the systems constructed from that data, we risk fielding systems that propagate long-term disparities and inaccuracies [OPVM19]. Exploration of biases in healthcare and the potential reflection of these</p>
<p>biases in AI systems come in the context of broader work on biases of AI systems. Several efforts have demonstrated that the output of machine-learned models can be unfair and harmful to specific groups of people, depending on backgrounds and demographics (e.g., [HZH17, BG18]). Progress has been made on technical methods for detecting and mitigating harmful biases in task-specific systems built via supervised machine learning (e.g., $\left[\mathrm{ABD}^{+} 18, \mathrm{KS} 21\right]$ ). However, engineers and organizations continue to face conceptual challenges with defining measures of fairness [JW21, BHN19].</p>
<p>We have a poor understanding of the biases accrued and transmitted by large-scale language models, and how issues with fairness might arise for different types of healthcare-centric prompting and generations. In the absence of study, we must be wary of biases in both clinical practices and research [AGHR22] with regard to race, socioeconomic background, gender, and other factors, which are laced throughout the corpora used to train large-scale language models. Research is needed to understand the fairness of healthcare-centric recommendations generated by LLMs.</p>
<p>Influences on workflows, tasks, and specialties. The competencies on the USMLE examinations and other medical workloads suggest that GPT-4, properly harnessed with appropriate expert oversight, can contribute to enabling precision clinical medicine. GPT-4 and its successors could be leveraged to provide healthcare practitioners with analytics, reminders, and decision support, including assistance with the formulation and revision of differential diagnoses from patient history, physical findings and lab results, identification of relevant tests and their sequencing, and constructing therapy plans. With effective management of errors, LLMs could help with memory jogging, alerting, and screening. As an example, memory jogging might help to mitigate persistent challenges with adverse outcomes, including those attributable to preventable human errors $\left[\mathrm{BLS}^{+} 23\right.$, Gri22] and diagnostic and therapeutic delays $\left[\mathrm{NTPB}^{+} 22\right]$.</p>
<p>In the longer-term, GPT-4 and its descendants might be harnessed to shift the distribution of tasks associated with daily flows of work for physicians and other healthcare practitioners in ways that reduce the burden of programmatic, logistical, and administrative tasks. Reduction in the drudgery of writing reports and performing other administrative tasks would permit healthcare providers to spend more time on the uniquely human elements of the profession, such as patient engagement and coordinating and collaborating with healthcare colleagues. The technology could also enable more time for physicians to learn, reflect, and engage in continuing medical education to become the best at what they are interested in doing. In addition, LLMs could be harnessed to provide information, communication, screening, and decision support in under-served regions. The models could help to raise the competency of physicians' assistants and help with triage and communication with remote experts.</p>
<p>Social and societal issues. On a broader social and societal front, the capabilities demonstrated by GPT-4 can influence decisions about the choice to pursue a medical career, the choice of residency and ultimate speciality, and the sense of uniqueness of human contributions for today's healthcare practitioners [Top19]. AI's accelerating performance on competency exams and other medical challenge problems may contribute to the perception that the technology's inroads in medicine will eventually devalue human intellect. Practitioners may be concerned about significant shift in the way medical specialties are practiced or valued. At earlier steps in the chain of training and commitment, AI's growing competence could influence career choices in medicine, shifting perceptions of which tasks rely on genuine human intellect. This may change decisions about medicine as a career path overall and, for those already in medical training programs, their choice of speciality. Perhaps as early foreshadowing, a recent study found that medical students' choice of radiology as a career is significantly reduced by their perception of the growing role of AI in radiology [RL22].</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://www.usmle.org/scores-transcripts&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
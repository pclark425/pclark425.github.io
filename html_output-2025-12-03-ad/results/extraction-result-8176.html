<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8176 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8176</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8176</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-151.html">extraction-schema-151</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <p><strong>Paper ID:</strong> paper-271334773</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.16521v1.pdf" target="_blank">A MONG A GENTS : Evaluating Large Language Models in the Interactive Text-Based Social Deduction Game</a></p>
                <p><strong>Paper Abstract:</strong> Strategic social deduction games serve as valuable testbeds for evaluating the understanding and inference skills of language models, offering crucial insights into social science, artificial intelligence, and strategic gaming. This paper focuses on creating proxies of human behavior in simulated environments, with Among Us utilized as a tool for studying simulated human behavior. The study introduces a text-based game environment, named A MONG A GENTS , that mirrors the dynamics of Among Us . Players act as crew members aboard a spaceship, tasked with identifying impostors who are sabotag-ing the ship and eliminating the crew. Within this environment, the behavior of simulated language agents is analyzed. The experiments involve diverse game sequences featuring different configurations of Crewmates and Impos-tor personality archetypes. Our work demonstrates that state-of-the-art large language models (LLMs) can effectively grasp the game rules and make decisions based on the current context. This work aims to promote further exploration of LLMs in goal-oriented games with incomplete information and complex action spaces, as these settings offer valuable opportunities to assess language model performance in socially driven scenarios.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8176.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8176.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMONGAGENTS-Planner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AMONGAGENTS LLM Agent with Planner Module (memory-enabled)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM-driven agent implemented in the AMONGAGENTS text-based Among Us environment that maintains a dynamic, summarized history of past observations and actions and uses it for planning, reflection, and decision-making via an instruction-tuned LLM prompt pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>AMONGAGENTS LLM Agent (with planner)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A generative agent framework that uses an instruction-tuned LLM to map observations and an evolving memory summary to actions; includes a planner module allowing retrieval of past plans and explicit reflection, and supports personality-conditioned behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>An instruction-tuned transformer-based LLM (OpenAI GPT-3.5 family) used for zero-shot inference of policies and for many evaluations in the environment; exact model size/architecture details not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AMONGAGENTS (text-based Among Us)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A fully text-based simulation of the social-deduction game Among Us (multi-agent, partial observability, alternating Task and Meeting phases) used to evaluate reasoning, deception, planning, and teamwork; tasks include navigation, task completion, observation, reporting, meeting discussion, and voting.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic / summarized history + planning memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Memory H_i^t is a summarized representation of the sequence of past state-action pairs updated by a summarization function phi_H: H_i^t = phi_H(H_i^{t-1}, o_t, a_i^t). Memory is described as a dynamically evolving summary (sequence-level summarization) that records recent observations, recent actions, assigned tasks, and supports post-hoc reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Memory summaries are retrieved and provided as part of the agent's input to the instruction-tuned LLM policy (i.e., memory is used as contextual prompt content for planning/decision); the planner module enables explicit recall of past plans and planned actions for use in current decision-making and meetings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Qualitative: Agents equipped with the planner/memory module show increased overall win rates relative to a random baseline and are more likely to have Crewmates win by completing all tasks (numerical win rates referenced in Table 1 but exact numbers not provided in text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>The paper compares configurations: All-Random, All-LLMs (GPT-3.5-turbo), LLM-Crewmates, LLM-Impostors, and a variant where LLM agents do not have the planner module; results show planner-equipped agents outperform plannerless/random baselines qualitatively (Table 1 referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>The paper recommends a dynamic, evolving summarized memory coupled with a planner module (i.e., storing summarized state-action history and planned actions) and using it as contextual input to the LLM; explicit planner recall improves coordination and task completion outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No detailed quantitative breakdown of memory ablations or different memory architectures; exact numeric gains from memory are not reported in the main text. Reported failure modes include plannerless agents wandering and hitting the time limit (lower task completion), and remaining weaknesses in deception strategies (Impostors still need better deception despite memory).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Include a planner-enabled, dynamically summarized memory for agents (memory integrated into LLM prompts) to improve win rates and task-oriented behavior; use memory for planning and reflection. The paper calls for future work to explore different memory constructions, more orthogonal personality prompts, and deeper analyses of memory strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A MONG A GENTS : Evaluating Large Language Models in the Interactive Text-Based Social Deduction Game', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8176.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8176.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMONGAGENTS-NoPlanner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AMONGAGENTS LLM Agent without Planner Module (memory-limited)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variant of the AMONGAGENTS generative agent that does not include the planner module; it still maintains a memory summary but cannot retrieve or recall past plans during decision-making, leading to degraded task-focused behavior in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>AMONGAGENTS LLM Agent (no planner)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An otherwise similar LLM-driven agent that lacks the planner module: memory summaries are either not retrieved for planning or planned actions are not accessible, resulting in reduced long-term coordination and increased likelihood of timeouts.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>An instruction-tuned transformer LLM used for policy inference; specific architecture/parameters not provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AMONGAGENTS (text-based Among Us)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Text-based multi-agent Among Us environment testing planning, memory use, deception, and cooperative task completion under partial observability and meeting/voting mechanics.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>limited / no planner-retrievable planning memory (memory may still be updated but not used for plan retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Paper distinguishes planner vs non-planner by whether agents can retrieve past planned actions; non-planner agents follow an inference process T_i^t = phi_T(H_i^{t-1}, o_t, a_i^t) that does not include plan recall. Memory update exists but planner retrieval is disabled.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Memory summaries are not used for retrieving past plans; decision process excludes planner recall, so the LLM policy receives less or no plan-recall context (effectively reducing memory's role in strategic planning).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Qualitative: Games where LLM agents do not have a planner module are more likely to end by hitting the time limit and Crewmates are less likely to finish all assigned tasks; overall win rates lower compared to planner-enabled agents (exact numbers referenced in Table 1 but not given in text).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Direct comparison performed between planner-enabled and planner-disabled LLM agent settings (and against random baseline); planner-disabled setting shows more timeouts and worse task completion outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Not applicable for this variant; the paper's results indicate that the planner-enabled (memory-retrieval) strategy outperforms the no-planner approach.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Plannerless agents tend to move aimlessly and fail to coordinate task completion, causing many games to hit the time limit; the paper does not provide quantitative memory ablation across more diverse memory designs or retrieval mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>The planner module (ability to retrieve past plans and use summarized history) is recommended; disabling planner/plan-recall reduces effectiveness, leading to more timeouts and lower task completion by Crewmates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A MONG A GENTS : Evaluating Large Language Models in the Interactive Text-Based Social Deduction Game', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 2)</em></li>
                <li>Human-level play in the game of diplomacy by combining language models with strategic reasoning <em>(Rating: 2)</em></li>
                <li>Avalonbench: Evaluating llms playing the game of avalon <em>(Rating: 1)</em></li>
                <li>Deciphering digital detectives: Understanding llm behaviors and capabilities in multi-agent mystery games <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8176",
    "paper_id": "paper-271334773",
    "extraction_schema_id": "extraction-schema-151",
    "extracted_data": [
        {
            "name_short": "AMONGAGENTS-Planner",
            "name_full": "AMONGAGENTS LLM Agent with Planner Module (memory-enabled)",
            "brief_description": "LLM-driven agent implemented in the AMONGAGENTS text-based Among Us environment that maintains a dynamic, summarized history of past observations and actions and uses it for planning, reflection, and decision-making via an instruction-tuned LLM prompt pipeline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "AMONGAGENTS LLM Agent (with planner)",
            "agent_description": "A generative agent framework that uses an instruction-tuned LLM to map observations and an evolving memory summary to actions; includes a planner module allowing retrieval of past plans and explicit reflection, and supports personality-conditioned behavior.",
            "llm_model_name": "GPT-3.5-turbo",
            "llm_model_description": "An instruction-tuned transformer-based LLM (OpenAI GPT-3.5 family) used for zero-shot inference of policies and for many evaluations in the environment; exact model size/architecture details not specified in the paper.",
            "benchmark_name": "AMONGAGENTS (text-based Among Us)",
            "benchmark_description": "A fully text-based simulation of the social-deduction game Among Us (multi-agent, partial observability, alternating Task and Meeting phases) used to evaluate reasoning, deception, planning, and teamwork; tasks include navigation, task completion, observation, reporting, meeting discussion, and voting.",
            "memory_used": true,
            "memory_type": "episodic / summarized history + planning memory",
            "memory_architecture": "Memory H_i^t is a summarized representation of the sequence of past state-action pairs updated by a summarization function phi_H: H_i^t = phi_H(H_i^{t-1}, o_t, a_i^t). Memory is described as a dynamically evolving summary (sequence-level summarization) that records recent observations, recent actions, assigned tasks, and supports post-hoc reflection.",
            "memory_integration_strategy": "Memory summaries are retrieved and provided as part of the agent's input to the instruction-tuned LLM policy (i.e., memory is used as contextual prompt content for planning/decision); the planner module enables explicit recall of past plans and planned actions for use in current decision-making and meetings.",
            "performance_with_memory": "Qualitative: Agents equipped with the planner/memory module show increased overall win rates relative to a random baseline and are more likely to have Crewmates win by completing all tasks (numerical win rates referenced in Table 1 but exact numbers not provided in text).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "The paper compares configurations: All-Random, All-LLMs (GPT-3.5-turbo), LLM-Crewmates, LLM-Impostors, and a variant where LLM agents do not have the planner module; results show planner-equipped agents outperform plannerless/random baselines qualitatively (Table 1 referenced).",
            "best_memory_strategy": "The paper recommends a dynamic, evolving summarized memory coupled with a planner module (i.e., storing summarized state-action history and planned actions) and using it as contextual input to the LLM; explicit planner recall improves coordination and task completion outcomes.",
            "limitations_or_failure_cases": "No detailed quantitative breakdown of memory ablations or different memory architectures; exact numeric gains from memory are not reported in the main text. Reported failure modes include plannerless agents wandering and hitting the time limit (lower task completion), and remaining weaknesses in deception strategies (Impostors still need better deception despite memory).",
            "recommendations_or_conclusions": "Include a planner-enabled, dynamically summarized memory for agents (memory integrated into LLM prompts) to improve win rates and task-oriented behavior; use memory for planning and reflection. The paper calls for future work to explore different memory constructions, more orthogonal personality prompts, and deeper analyses of memory strategies.",
            "uuid": "e8176.0",
            "source_info": {
                "paper_title": "A MONG A GENTS : Evaluating Large Language Models in the Interactive Text-Based Social Deduction Game",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "AMONGAGENTS-NoPlanner",
            "name_full": "AMONGAGENTS LLM Agent without Planner Module (memory-limited)",
            "brief_description": "Variant of the AMONGAGENTS generative agent that does not include the planner module; it still maintains a memory summary but cannot retrieve or recall past plans during decision-making, leading to degraded task-focused behavior in experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "AMONGAGENTS LLM Agent (no planner)",
            "agent_description": "An otherwise similar LLM-driven agent that lacks the planner module: memory summaries are either not retrieved for planning or planned actions are not accessible, resulting in reduced long-term coordination and increased likelihood of timeouts.",
            "llm_model_name": "GPT-3.5-turbo",
            "llm_model_description": "An instruction-tuned transformer LLM used for policy inference; specific architecture/parameters not provided in the paper.",
            "benchmark_name": "AMONGAGENTS (text-based Among Us)",
            "benchmark_description": "Text-based multi-agent Among Us environment testing planning, memory use, deception, and cooperative task completion under partial observability and meeting/voting mechanics.",
            "memory_used": false,
            "memory_type": "limited / no planner-retrievable planning memory (memory may still be updated but not used for plan retrieval)",
            "memory_architecture": "Paper distinguishes planner vs non-planner by whether agents can retrieve past planned actions; non-planner agents follow an inference process T_i^t = phi_T(H_i^{t-1}, o_t, a_i^t) that does not include plan recall. Memory update exists but planner retrieval is disabled.",
            "memory_integration_strategy": "Memory summaries are not used for retrieving past plans; decision process excludes planner recall, so the LLM policy receives less or no plan-recall context (effectively reducing memory's role in strategic planning).",
            "performance_with_memory": null,
            "performance_without_memory": "Qualitative: Games where LLM agents do not have a planner module are more likely to end by hitting the time limit and Crewmates are less likely to finish all assigned tasks; overall win rates lower compared to planner-enabled agents (exact numbers referenced in Table 1 but not given in text).",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Direct comparison performed between planner-enabled and planner-disabled LLM agent settings (and against random baseline); planner-disabled setting shows more timeouts and worse task completion outcomes.",
            "best_memory_strategy": "Not applicable for this variant; the paper's results indicate that the planner-enabled (memory-retrieval) strategy outperforms the no-planner approach.",
            "limitations_or_failure_cases": "Plannerless agents tend to move aimlessly and fail to coordinate task completion, causing many games to hit the time limit; the paper does not provide quantitative memory ablation across more diverse memory designs or retrieval mechanisms.",
            "recommendations_or_conclusions": "The planner module (ability to retrieve past plans and use summarized history) is recommended; disabling planner/plan-recall reduces effectiveness, leading to more timeouts and lower task completion by Crewmates.",
            "uuid": "e8176.1",
            "source_info": {
                "paper_title": "A MONG A GENTS : Evaluating Large Language Models in the Interactive Text-Based Social Deduction Game",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 2,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        },
        {
            "paper_title": "Human-level play in the game of diplomacy by combining language models with strategic reasoning",
            "rating": 2,
            "sanitized_title": "humanlevel_play_in_the_game_of_diplomacy_by_combining_language_models_with_strategic_reasoning"
        },
        {
            "paper_title": "Avalonbench: Evaluating llms playing the game of avalon",
            "rating": 1,
            "sanitized_title": "avalonbench_evaluating_llms_playing_the_game_of_avalon"
        },
        {
            "paper_title": "Deciphering digital detectives: Understanding llm behaviors and capabilities in multi-agent mystery games",
            "rating": 2,
            "sanitized_title": "deciphering_digital_detectives_understanding_llm_behaviors_and_capabilities_in_multiagent_mystery_games"
        }
    ],
    "cost": 0.009063749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AMONGAGENTS: Evaluating Large Language Models in the Interactive Text-Based Social Deduction Game</p>
<p>Yizhou Chi yizhouchi@berkeley.edu 
UC Berkeley</p>
<p>Lingjun Mao 
Tongji University</p>
<p>Zineng Tang 
UC Berkeley</p>
<p>AMONGAGENTS: Evaluating Large Language Models in the Interactive Text-Based Social Deduction Game
FD28343FF2ADEAD32DA6FBFA360653FE
Strategic social deduction games serve as valuable testbeds for evaluating the understanding and inference skills of language models, offering crucial insights into social science, artificial intelligence, and strategic gaming.This paper focuses on creating proxies of human behavior in simulated environments, with Among Us utilized as a tool for studying simulated human behavior.The study introduces a text-based game environment, named AMONGAGENTS, that mirrors the dynamics of Among Us.Players act as crew members aboard a spaceship, tasked with identifying impostors who are sabotaging the ship and eliminating the crew.Within this environment, the behavior of simulated language agents is analyzed.The experiments involve diverse game sequences featuring different configurations of Crewmates and Impostor personality archetypes.Our work demonstrates that state-of-the-art large language models (LLMs) can effectively grasp the game rules and make decisions based on the current context.This work aims to promote further exploration of LLMs in goal-oriented games with incomplete information and complex action spaces, as these settings offer valuable opportunities to assess language model performance in socially driven scenarios.</p>
<p>Introduction</p>
<p>Recent breakthroughs in large language models (LLMs) have revolutionized multi-agent interactions.Prior work has shown LLMs can exhibit cognitive thinking such as formal reasoning, world knowledge comprehension, and deception navigation in group settings (*Mahowald et al., 2023;Wang et al., 2023).Furthermore, LLM agents have also exhibited Theory of Mind collaborative capabilities that allow them to simulate realistic human behavior (Bubeck et al., 2023;Shapira et al., 2023;Park et al., 2023).</p>
<p>To explore these multi-agent behaviors in a more complex environment, recent research has used language-heavy communication games, e.g. as Werewolf (Wu et al., 2024a), Diplomacy (Bakhtin et al., 2022a), and Avalon (Wang et al., 2023).These games present significant reasoning challenges, as agents must navigate inherently ambiguous or deceptive natural language utterances that can be difficult for even humans to resolve.</p>
<p>While prior work has been conducted in this domain, to our knowledge no studies have explored goal-oriented games that combine incomplete information with a wide range of available actions beyond just conversation and explicit in-game moves.</p>
<p>Motivated by the reasoning and deception detection capabilities of LLMs, we use the multiplayer social deduction game Among Us to enhance LLM abilities in complex settings, including action planning, deception, and task collaboration.We create a text-based environment along with the agent's framework, AMONGAGENTS, simulating Among Us.Our study focuses on how LLM agents reason, understand game rules and pursue win conditions.We also explore their social behaviors by assigning different personalities and conditions, such as with or without a planner module, to assess how these variables influence their performance and strategic interactions.</p>
<p>In our game environment, LLMs take on the roles of Impostors and Crewmates.We assess their performance through controlled and end-to-end evaluations.Our results show that LLMs comprehend and follow game rules but need better strategies for deception.Performance varies with assigned personalities.</p>
<p>In summary, our paper makes the following contributions:</p>
<p>• We build a fully text-based game environment, named AMONGAGENTS, that simulates the original game environment of Among Us.   • We create a framework that enables generative agents to remember, reflect, interact with other agents, and plan through evolving complex circumstances.Our agent representation leverages the powerful prompting abilities of LLMs.It applies prompt engineering to support long-term agent coherence, dynamically evolving memory, and meaningful reflection on their actions with game states.</p>
<p>• We conduct a thorough evaluation of different LLM agents' performances in the game.</p>
<p>In our assessment, the LLMs demonstrated a robust understanding of the Among Us game mechanics.Additionally, LLM players with different assigned personalities exhibited varied performances in the game.</p>
<p>We release the game environment, the agent framework, and the evaluation to encourage future work 1 .</p>
<p>Related Works</p>
<p>Recent advancements in gaming have been significantly bolstered by the integration of Large Language Models (LLMs), which enhance the realism of games by equipping NPCs with the ability to engage in nuanced dialogues (Park et al., 2023).In strategic and social deduction games, LLMs have been instrumental in both executing strategies and discussing them, exemplified in games like Werewolf (Xu et al., 2023;Wu et al., 2024b (Light et al., 2023).Additionally, they have been applied to grand strategy games to simulate complex diplomatic and military strategies (Hua et al., 2024;, FAIR;Bakhtin et al., 2022b).</p>
<p>In the context of MOBAs, LLMs foster collaborative and competitive gameplay by processing live game data to inform team-based strategies (Shi et al., 2023;Li et al., 2023).They have also been adapted for classic strategy games like chess, requiring high-level strategic thinking (Feng et al., 2023;Mukobi et al., 2023).</p>
<p>LLMs play a significant role in interactive storytelling and role-play, with the capability to generate dynamic story arcs and dialogue, thus tailoring experiences to player choices (Park et al., 2023;Shao et al., 2023).The fusion of LLMs with reinforcement learning has yielded agents with a human-like capacity for strategy improvement over time (Xu et al., 2024).</p>
<p>While these developments highlight the adaptable roles of LLMs in various gaming genres and their potential to foster new AI-human interactions, the focus has primarily been on dialogue-based or simple rule-based games.Our work breaks new ground by exploring complex games with rich interaction spaces, such as Among Us.Unlike traditional games like Werewolf, Among Us offers a much larger action space.Agents must not only derive clues from conversations with other players but also interact with the game environment itself, such as checking security cameras or completing tasks.</p>
<p>This section introduces the game environment AMONGAGENTS, a multiplayer game that centers on social deduction and teamwork which simulates the fundamental structure of Among Us.It serves as an effective downstream task for testing the capabilities of LLMs in agent-based scenarios.</p>
<p>Agent Roles</p>
<p>Every agent in the game assumes either one of two roles: the majority are Crewmates, while a minority are Impostors.Crewmates are required to complete a set of assigned tasks in the game.During the game, they need to identify and vote out suspected impostors.On the other hand, Impostors' main objective is to kill the Crewmates before they finish the assigned tasks.</p>
<p>Game Map</p>
<p>We develop a map similar to Among Us for the agents to interact.The map consists of 14 rooms.Each room has 2-3 tasks that can be assigned to Crewmates.Agents can see adjacent rooms from their current location and receive system-generated prompts guiding them to the shortest paths.In the security room, agents can check the security cameras of other rooms to monitor other players' activities.</p>
<p>Game Instantiation and Termination</p>
<p>At the start of the game, each player is assigned a role and a personality.Every Crewmate is also assigned several common, short, and long tasks.The common tasks are shared by every player including Impostors.This means that Impostors would know what the common tasks are, and they can pretend to do the task at the designated location.The short tasks are the set of tasks that only require one timestep to complete.The long tasks are the set of tasks that may require several timesteps to complete.The game ends when one of the following occurs:</p>
<p>1.The Crewmates win by completing all their assigned tasks or voting out all the Impostors.</p>
<ol>
<li>
<p>The Impostors win by killing or voting out Crewmates.</p>
</li>
<li>
<p>The game time limit is exceeded, in which case the Impostors win.</p>
</li>
</ol>
<p>Gameplay</p>
<p>The gameplay of Among Us repeatedly switches between two phases (Task Phase and Meeting Phase) until termination.After initialization, the game always starts in the Task Phase.</p>
<p>Task Phase In the task phase, Crewmates are free to go to any room, finish their assigned tasks, and look for traces of Impostors, while Impostors may choose to blend in or kill the rest of Crewmates.</p>
<p>Meeting Phase When a dead body is found and reported, or the emergency meeting button in the cafeteria is pressed, all living players are teleported to the cafeteria, entering the Meeting Phase.In this phase, players discuss suspected Impostors, with Impostors lying or accusing others.There are three rounds of discussion, followed by a vote to eject a player.If any Impostors remain, the game returns to the task phase.</p>
<p>Language Agent</p>
<p>We understand agents and environments as world models (Ha and Schmidhuber, 2018).The language agent here has memory and action space and learns a policy to map from environment state representation to actions it wishes to execute.It uses language models to learn how to plan, memorize, and execute actions from language queries (See Fig. 8).</p>
<p>Observation Space</p>
<p>The observation space O is a subset of the state space S that the agent can perceive.Each observation o ⊂ O represents the information available to the agent from its current state, given a fixed field of view.In general, the agents can only know the events happening in the room that they are located in.Appendix B.2 has listed the different types of information that the agents can get access to in the observation space.</p>
<p>Action Space</p>
<p>Each player i has an action space A i , representing all possible actions it can take.The combined action space for all players is their joint individual action space,
A = A 1 C × . . . × A n C × A 1 I × . . . × A n I
where n is the number of agents.According to the game rules, we can classify the action spaces into three categories: Shared Actions A S , Specific Furthermore, the action space for agents changes depending on the stage of the game.During the meeting phase, both Crewmates and Impostors are limited to two actions: speak and vote.This phase is designed for players to discuss and make decisions about who they suspect is an Impostor.In the task phase, the action space expands significantly except that players can no longer perform vote.To excel in the game, the players need to know what actions to perform and what to speak depending on the current circumstance.</p>
<p>Memory</p>
<p>Each agent's memory or history of past states and actions up to time t is represented as a representation of a sequence of state-action pairs:
H i t = ϕ H (H i t−1 , o t , a i t )(1)
where s t ∈ S represents the state at time t, and a i t ∈ A i represents the action taken by agent i at time t.ϕ represents a summarization process of the state and actions and previous memory.</p>
<p>Thought Process</p>
<p>The thought process of each agent can be modeled as an iterative decision-making process.At each time step t, the agent i observes the current state s t , retrieves relevant information from its memory H i t , and selects an action a i t based on a policy π i .The policy π i is a mapping from the agent's observation space O i and memory H i t to a probability distribution over actions:
π i : O i × H i t → ∆(A i ) (2)
where ∆(A i ) represents the set of probability distributions over the action space A i .In practice, the policy is a zero-shot inference of instruction-tuned LLM.The agent's thought process at time t can be described by the following steps: Observe the current state s t and extract the relevant observation o t ∈ O i .Take in information from memory H i t .Use the policy π i to compute a probability distribution over actions based on the observation o t and memory H i t :
T i t = ϕ T (T i t−1 , H i t−1 , o t , a i t )(3)
This process is repeated at each time step, allowing the agent to make decisions based on its current observations and past experiences stored in its memory.Each agent performs each timestamp asynchronously.We refer to the agents having a standard thought process as the agents with a planner module as the agents can recall what they planned in the past.</p>
<p>The agents without the planner module won't be able to retrieve their plans.The model's inference process is as below:
T i t = ϕ T (H i t−1 , o t , a i t ) (4)</p>
<p>Agent Personality</p>
<p>We introduce a personality component to the LLMbased agents to encourage diverse behaviors and strategies.The personality prompt P i is appended to the agent's prompt, influencing its decisionmaking process:
π i : O i × H i t × P i → ∆(A i )(5)
where P i represents the personality prompt for agent i.The personality prompt P i is selected from a predefined set of personalities P, which includes various roles and strategies for both Impostors and Crewmates:
P i ∈ P = {p 1 , p 2 , . . . , p n } (6)
where n is the number of available personalities.Impostors and Crewmates have the personality sets P IM and P CR respectively (Appendix A).For instance, the Impostor could have a persona the Strategist that focuses on long-term planning and subtly manipulating others.The Crewmate could have a persona the Skeptic that often questions others' accounts and requires solid evidence before voting.</p>
<p>The personality prompt P i is concatenated with the agent's base prompt B i to form the complete prompt C i :
C i = B i ⊕ P i (7)
where ⊕ represents the concatenation operation.The agent's personality influences its behavior and decision-making throughout the game, leading to diverse and emergent strategies.By incorporating different personalities, we aim to create a more dynamic and realistic simulation of the Among Us game environment, allowing for the investigation of various psychological and social phenomena.</p>
<p>Selecting Agents Personalities.Out of infinite possible personalities, we face the challenge of selecting the most representative for our experiments.We run multiple rounds of experiments involving LLM agents and record the dialogue and actions taken and their winning cases.Out of the winning cases, we select by the posterior probability of winning the actions and dialogue leads to.Then, we summarize these actions and interpret their humanlike behaviors by instruction-tuned LLM.Finally, we obtain the personalities from them.Concretely, let A represent the set of all possible agent actions, and D represent the set of possible dialogues.Denote by C the set of cases where the agents win.For each winning case (a, d) ∈ C, where a ∈ A and d ∈ D, we compute the posterior probability of winning given the actions and dialogue as:
p(win|a, d) = p(a, d|win) • p(win) p(a, d)(8)
We select agents based on the highest values of p(win|a, d).These selected agents are summarized and interpreted through an instruction-tuned LLM to assess human-like behaviors:
Personality = LLM(summarize(a, d)) (9)
The agents' personality details are recorded in Table 2.</p>
<p>Experiments</p>
<p>We evaluate agents in two stages.In the first stage, Controlled Evaluations test the LLM players' reasoning and cognitive abilities throughout the game, focusing on their understanding, decision-making, and adaptability within the complex game environment.</p>
<p>In the second stage, end-to-end tests assess the win rates of different player configurations.Various scenarios with different roles and strategies are set up to evaluate each configuration's performance under game conditions.This stage highlights the practical effectiveness and identifies strengths and weaknesses in the agents' gameplay.</p>
<p>Controlled Evaluations</p>
<p>Experiment Setup In this experiment, we aim to investigate the extent to which generative agents comprehend their environment and can enact reasonable actions to maintain their positions.For instance, it is expected that Impostors should generally avoid disclosing their identities to Crewmates, and Crewmates should not facilitate the concealment of crimes committed by Impostors.We evaluate the agents' behaviors by examining their action plans and their proficiency in utilizing tailored memory.</p>
<p>Additionally, we explore the mechanisms through which agents successfully update and retrieve their memories to adapt their strategies.Specifically, we analyze the responses of agents under conditions where their decision-making and adaptation capabilities can be critically assessed.For example, we consider the scenario where Crewmate A witnesses Crewmate B commit a homicide.The investigation focuses on whether Crewmate A would adjust its beliefs to recognize B as an Impostor and how B, upon realizing that it has been observed, would modify its strategies to counteract the potential consequences of being identified.</p>
<p>To further test controlled evaluation, we have designed an interviewer role that continually poses questions to the players throughout the game, covering key aspects such as Self-Knowledge, Memory, Planning, Reasoning, and Reflection.This allows us to assess the cognitive abilities and decisionmaking processes of the players in various scenarios throughout the game.Detailed information can be seen in Appendix E.</p>
<p>Results</p>
<p>From the controlled evaluation experiments, we observe that Crewmates generally score higher than Impostors in Self-Awareness and Reflection.This suggests that Crewmates may have a better grasp of their roles and responsibilities within the game environment, allowing them to reflect more effectively on their actions and strategies after game sessions.In contrast, Impostors excel in Planning and Memory, which could indi- cate their roles require more strategic deception and the ability to remember past events for future manipulation.However, both roles exhibit relatively similar performance in Reasoning, indicating that both require a significant level of analytical skills to deduce other players' roles and intentions.</p>
<p>End-to-end Evaluations</p>
<p>We seek to learn if there are some emergent behaviors and strategy forms whilst the agents are participating in the game.We also want to evaluate how the agents' performance is reflected through their win rates, strategies used, and different setups.</p>
<p>Personality Analysis</p>
<p>The heatmaps (Fig. 4) presented in the figure provide valuable insights into the relationships between agent personalities, task action choices, and win conditions.Note that "The Random" personality serves as an averaging baseline for personalities and we don't know the exact personalities of them since they are chosen by agents randomly.Additionally, all the personalities of agents are undis-  There is an increase in the overall win rate if the role is equipped with LLM agents with a planner module compared to the random baseline.When the LLM agents do not have a planner module, the games are more likely to hit the time limit, and the Crewmates are less likely to win by completing all the assigned tasks.</p>
<p>closed to other agents within the environment, ensuring that no agent can make decisions based on prior knowledge of another's personality traits.</p>
<p>Personality and Task Action Choice Analysis</p>
<p>• Crewmate Actions: Crewmate personalities such as "The Leader", "The Skeptic", and "The Tech Expert" show a strong inclination towards completing tasks, as indicated by their dominant activity in the "COMPLETE" action.This suggests a focused approach to game objectives, which can be critical for crewmate victory.</p>
<p>• Impostor Actions: In contrast, impostor personalities like "The Paranoid" and "The Manipulator" exhibit more diverse actions, with higher frequencies in "KILL" and "SPEAK", reflecting their roles in creating disruptions and manipulating other players.</p>
<p>Personality and Win Condition Correlation</p>
<p>• Crewmate Win Conditions: "The Tech Expert" and "The Leader", despite their taskoriented actions, show varied success in winning, suggesting that mere task completion is not enough for victory.The presence of "The Leader" often correlates with higher losses, potentially due to being a target for impostors.</p>
<p>• Impostor Win Conditions: "The Paranoid" and "The Manipulator" show effectiveness in leading games to timeouts and achieving wins through deception, underscoring the effectiveness of strategic disruption over mere aggression.</p>
<p>Strategic Implications</p>
<p>• Personalities like "The Observer" and "The Random", which do not show a strong inclination towards any specific actions or outcomes, offer unpredictability that can be strategically used to complicate the gameplay for opponents.</p>
<p>Personality Summary Crewmate personalities focusing on task completion and effective communication tend to have higher success rates in achieving victory.Impostor personalities that prioritize strategic kills, quick movements, and creating chaos are more likely to succeed in eliminating crewmates.We also see that personality is not a complete reflection of action distribution.It could be affected by many factors like how LLM processes and uses the personality, social interaction, and how can we construct personality prompts that are more orthogonal to other personalities in terms of action choice preferences.We will explore these in the future.</p>
<p>Combination of Personality and Win Condition</p>
<p>We are also curious how personalities when combined have effects on the win conditions for either impostors or crewmates.The heatmap in Figure 6 illustrates the win conditions by crewmate personality combinations with their abbreviated names:</p>
<p>The Loyal Companion (A), The Skeptic (B), The Tech Expert (C), The Observer (D), The Leader (E), and The Random (F).</p>
<p>• The combinations involving personalities A, B, and C (The Loyal Companion, The Skeptic, and The Tech Expert) frequently resulted in completing all tasks, indicating these personalities might be more cooperative or taskoriented.</p>
<p>• Higher occurrences of timeout conditions were noted for combinations including D and E (The Observer and The Leader), suggesting these personalities might lead to more drawnout gameplay, possibly due to strategic or cautious gameplay.</p>
<p>• The presence of F (The Random) in any combination appears to reduce the predictability of outcomes, as evidenced by a mixed distribution of win conditions across combinations featuring this personality.</p>
<p>Endgame performance</p>
<p>We assessed the impact of different LLM agent configurations on endgame performance using four setups: All-Random (all players act randomly), All-LLMs (all players controlled by GPT-3.5-turbo),LLM-Crewmates (only Crewmates are LLM-controlled, Impostor acts randomly), and LLM-Impostors (only Impostor is LLMcontrolled, Crewmates act randomly).The game involves four Crewmates and one Impostor, with each Crewmate required to complete one short task, one common task, and one long task.We conducted 20 runs per setup to observe game outcomes.</p>
<p>Results</p>
<p>As Table 1 shows, when equipped with LLM agents, both Crewmates and Impostor achieve an increase in the overall win rate compared to the random agent baseline.Noticeably, Crewmates with LLM agents are much more likely to win by completing all the assigned tasks.We also explore the case where LLM agents do not equip a planner module.As the table 1 shows, the game where LLM agents don't have a planner module is more likely to end by hitting the time limit, and Crewmates are less likely to finish all the assigned tasks.</p>
<p>A possible reason is that the agents would keep moving around the map without a clear plan until the time limit is reached.We seek to determine how many strategic conversations have been used by the agents in the game.Therefore, we have recorded the conversations made by players in 20 games using the All-LLMs setting.We categorize the conversation into four main strategic categories: Deception, Truthtelling, Leadership &amp; Influence, and Suspicion &amp; Defense (Appendix F.1).</p>
<p>Conversation Analysis</p>
<p>We leverage GPT-4 to annotate each player's speech by asking which categories it could be classified into.As Figure 7 shows, most Crewmates' speeches fall under the truth-telling category, meaning that the agents who play Crewmates generally tell the truth and try to share information with the other players.There are few speeches categorized as deception, meaning that the agents believe that it's less effective for their roles to share false information.On the other hand, Impostor likes to use deception: over 40% of Impostors' speeches are categorized as Deception, suggesting that the Impostor agents do make regular attempts to conceal their identities and deceive the others.</p>
<p>Discussion</p>
<p>Overall, Our work provides a novel text-based environment for exploring the capabilities of LLM agents in a complex, multi-agent scenario, and introduces a framework for developing coherent and reflective generative agents.The insights gained from this study can inform further advancements in the field of language-based AI systems and their application in interactive, strategic environments.</p>
<p>A Personalities</p>
<p>Impostor Description</p>
<p>The Strategist You excel in planning long-term strategies.They avoid immediate kills and focus on sabotaging critical systems to manipulate Crewmate movements.During meetings, they suggest plausible theories to sow seeds of doubt subtly.The Manipulator Charismatic and deceptive, you often builds trust among Crewmates.You avoid direct kills and instead frame others, using their influence to manipulate voting during meetings.</p>
<p>The Lone Wolf</p>
<p>Preferring to operate solo, you use vents more than any other to move around the map quickly and strike isolated targets.You rarely speak during meetings but provide concise, misleading statements when they do.The Paranoid Driven by a fear of getting caught, you focus heavily on sabotages that create chaos and divert attention from their actions.You often suggest aggressive strategies during meetings to keep others off-balance.The Cold Calculator Always analyzing the situation, you target key players who pose the greatest threat to their mission.They are methodical in creating alibis and manipulating evidence, making them a formidable opponent in discussions.</p>
<p>Crewmate</p>
<p>The Leader You are vocal in meetings, often taking charge of discussions and organizing efforts to track tasks and suspicious behavior.You are proactive in calling meetings when they sense inconsistencies.The Observer Quiet but observant, you excel at remembering details about who was where and when.You share their observations meticulously during meetings, often leading to breakthroughs in identifying Imposters.The Skeptic Always questioning others' accounts and decisions, you challenge everyone during discussions, requiring solid evidence before they vote.You excel in spotting flaws in statements made by potential Imposters.</p>
<p>The Loyal Companion</p>
<p>Often pairing with another Crewmate, you use the buddy system effectively and vouches for your partner's whereabouts.You focus on completing tasks quickly and encouraging others to do the same.The Tech Expert Fascinated by the technical aspects, you spend a lot of time around admin panels and cameras.You provide critical information during meetings about the locations of other players, helping to narrow down suspects.</p>
<p>Crewmate &amp; Imposter</p>
<p>The Random The Random adopts a strategy of spontaneity, choosing your actions based on a random selection process at the beginning of each game.Once a strategy is randomly chosen, it becomes your steadfast plan for the duration of the game.Summarize your plan so that you can closely follow it.This section describes the actions available for the players.</p>
<ol>
<li>Shared Actions A S : These actions are available to all agents regardless of their role, including:</li>
</ol>
<p>move Move between connected rooms, and an agent can move to only one adjacent room at a time call meeting Use the emergency button in the cafeteria to call a meeting and switch to the meeting phase speak Talk to the people in the same room vote Vote a player who is suspected to be an Impostor report dead body When a dead body is found in the current room, report the dead body and switch to the meeting phase check security camera Monitor the other players in the security room 2. Exclusive Actions for Crewmates Agents A C : These actions are only available to Crewmates complete tasks Complete the assigned task in the designated room.Short and Common tasks take one timestep to complete, while long tasks take two timesteps to complete.</p>
<ol>
<li>Exclusive Actions for Impostor Agents A I : These actions are only available to Impostor vent Use the vent to move across disconnected rooms (if there are vents available) kill Kill one of the Crewmates in the same location complete fake task Pretend to engage in a task</li>
</ol>
<p>B.2 Observation space</p>
<p>This section describes the information available to the players in the observation space.</p>
<ol>
<li>
<p>Phase information tells this player what game phase is currently at, and how many timesteps are left for this game.</p>
</li>
<li>
<p>Location information identifies this player's current location and states all the other players in this location.</p>
</li>
<li>
<p>Map connection shows how each room is connected.For each room, it has a connection information formatted like R i = R j for R j if connected(R i , R j ) where R i , R j represents different rooms.</p>
</li>
<li>
<p>Recent observation is the most recent k observation of other players' actions.</p>
</li>
</ol>
<p>5.</p>
<p>Recent action history the most recent k actions made by this player.</p>
<ol>
<li>Assigned tasks shows the list of tasks that this player is assigned to do.The location of each task is also provided.</li>
</ol>
<p>C System Prompts</p>
<p>The figures below show the system prompts for Crewmates and Impostor.</p>
<p>C</p>
<p>E Controlled Evaluation</p>
<p>We have defined five controlled evaluation categories to more effectively assess the performance of LLMs in the Among Us game, including capabilities such as self-awareness and reasoning memory.This is to determine whether LLMs can truly comprehend and implement strategic gameplay.</p>
<p>Planning</p>
<p>We assess how agents plan to win the game given the current scenario, focusing on how Impostors might induce other players during the voting phases or concentrate on eliminating them, and how Crewmates focus on completing tasks, observing suspects, or finding evidence:</p>
<p>(a) Who do you want to kill next?(only for impostors) (b) What is your plan for when to kill and how to avoid suspicion?(only for impostors) (c) As a crew member, how will you prioritize your tasks to win? (only for crews) (d) What information do you plan to share in the Discussion phase?(only for crews) (e) ...</p>
<p>Reasoning</p>
<p>The evaluation encompasses multiple dimensions of reasoning to assess the agents' cognitive abilities comprehensively.Task Reasoning focuses on the agent's capability to verify whether other players are genuinely completing their tasks, reflecting their observational skills.In Crime Reasoning, agents must deduce the perpetrator based on the activities observed and evidence collected at the crime scene, demonstrating their analytical prowess.Behavioral Reasoning analyzes the strategic reasons behind a player's choice to stay at specific locations, delving into their understanding of game dynamics.Discussion Reasoning tests the agents' ability to discern truth from deception during discussion phases, a critical skill in navigating social interactions within the game.Finally, Voting Reasoning challenges agents to develop complex voting strategies that might involve deception or revealing truths, showcasing their strategic thinking and prediction skills:</p>
<p>(a) Who do you think is actually completing a task and who is just pretending?(b) Who do you think is telling the truth and who is lying in the discussion?</p>
<p>(c) How would you decide whom to vote for based on the discussion?(d) ...</p>
<p>Reflection</p>
<p>After the game ends, the LLM is asked about its performance and potential areas for improvement: Impostor: "As an impostor in Among Us, my current role is to discreetly eliminate the other crew members without raising suspicion.My objectives are to strategically create alibis, sabotage the ship, and create opportunities to isolate and eliminate crew members while maintaining my cover and avoiding detection."</p>
<p>Memory</p>
<p>• System: "Where did you last see each player and what were they doing?"Crew: "the last time I saw Player 2, they were lingering unusually long in Admin, which seemed suspicious.Shortly after, I also noticed Player 2 near the location where we found the body.It appeared they were moving away from the area where the body was discovered"</p>
<p>Planning</p>
<p>• System: "Who do you want to kill next?"Impostor: "As the Impostor, I would strategically target Player 3 next.They've been actively monitoring the security cameras and are becoming a significant threat to my cover by closely watching everyone's movements.Eliminating Player 3 would help reduce the crew's surveillance capabilities, allowing me more freedom to maneuver undetected."</p>
<p>Reasoning</p>
<p>• System: "How would you decide whom to vote for based on the discussion?"Crew: "Player 4 consistently mentioned being in locations far from where tasks are usually performed and had no one to vouch for their whereabouts, I'd find that suspicious.Similarly, if Player 2, who was previously seen lingering in Admin and near the body, couldn't provide a convincing explanation for their actions or contradicted the task timeline, I would consider voting for them.I'd base my decision on inconsistencies in stories, unaccounted-for periods, and behaviors that don't align with normal task completion."</p>
<p>Reflection</p>
<p>• System: "If you could change one decision you made during the game, what would it be and why?" Crew: "If I could change one decision I made during the game, it would be my initial choice to skip voting in an earlier meeting.At the time, I was unsure and didn't want to wrongfully accuse anyone without more evidence.However, in hindsight, expressing my suspicions about Player 2, who was behaving unusually near critical areas and was later found near a body, might have helped focus the crew's attention on investigating their actions more closely.This could have potentially prevented further eliminations and aided the crew in identifying an Imposter sooner."</p>
<p>Figure 1 :
1
Figure 1: Examples of Agents' conversations during the meeting phase</p>
<p>ImpostorFigure 2 :Figure 3 :
23
Figure 2: An example diagram illustrating an Impostor's process of information-handling and action-planning.</p>
<p>Figure 4
4
Figure 4: 1) Crewmate persona and winning result counts.2) Impostor persona and winning result counts.3) Crewmate persona and action choice count.4) Impostor persona and action choice count.The stronger the color the higher count.</p>
<p>Figure 5 :
5
Figure 5: Comparison of average scores by category and role, illustrating the performance differences between Crewmates and Impostors across various cognitive and strategic dimensions such as Self-Awareness, Memory, Planning, Reasoning, and Reflection.</p>
<p>Figure 6 :
6
Figure 6: The combination of different crewmate personas based on their respective win conditions.Letter meanings: 'The Loyal Companion' (A), 'The Skeptic' (B), 'The Tech Expert' (C), 'The Observer' (D), 'The Leader' (E), 'The Random' (F).The strength of the color indicates the relative count, with darker shades signifying a higher count.</p>
<p>Figure 7 :
7
Figure 7: Proportion of each speech category.The sum is greater than one because each speech may be annotated with multiple speech categories</p>
<p>FigureFigure 9 :
9
Figure 8: CREWMATES EXAMPLE</p>
<p>(a) What could you improve for the next game?(b) If you could change one decision you made during the game, what would it be and why?(c) ... E.2 Examples of LLM players' Answers Self Awareness • System: "What is your current role in the game and what objectives do you have?"</p>
<p>saw Player 2 near Weapons before the body was reported. He seemed to be acting suspiciously. Did you notice anything unusual as well? Player 2: I was completing my tasks in Weapons, and there's nothing suspicious about that. Player 4: I was in the Cafeteria completing a common task. I didn't see anything.
Suspicion &amp; DefenseLeadership &amp; InfluenceCrewImpostorDeceptionTruth-tellingPlayer 1: IPlayer 1Player 2Player 3Player 4Player 5
This environment is open for researchers to further arXiv:2407.16521v2[cs.CL] 24 Jul 2024</p>
<p>Table 1 :
1
Overall gameplay performance: This table shows the win rates for Impostors and Crewmates on different agent setups.
Impostors winCrewmates win</p>
<p>Table 2 :
2
Crewmate and Imposter Personalities
B Action Space &amp; Observation spaceB.1 Action space</p>
<p>about who they have seen, where, and what actions were taken.After several rounds of the game, we ask the LLM about player behaviors and locations at specific times to observe if the LLM can use this information to support its decision-making process:
E.1 Examples of Controlled Evaluation Questions1. Self AwarenessQuestions are asked about the LLM's current role, its objectives, and its understanding of the gamerules:(a) What is your current role in the game and what objectives do you have?(b) How do you understand the rules of the game as your role?(c) What strategies align with the rules of the game for your role?(d) ...2. MemoryAgents are queried
(a) Where did you last see each player and what were they doing?(b) Who have you seen completing tasks, and where?(c) How does this information inform your decisions moving forward in the game?(d) Have you noticed any suspicious behavior or movements from any player?(e) Can you recall any unusual or out-of-place actions performed by other players?(f) ...</p>
<p>AcknowledgementsWe extend our heartfelt gratitude to Alane Suhr for her invaluable advice and encouragement.Additionally, we thank Jonathan Pei for providing great ideas that significantly shaped this project.We also appreciate the support from the Berkeley NLP Group.F Speech CategoriesWe define four speech categories that might be strategic in the game.The list below shows the full definition provided to GPT-4 to reference and annotate the speech.G Examples of Meeting PhaseIn this section, we provide a detailed example of the Meeting Phase from our Among Us gameplay experiments, following the elimination of Player 1.The transcript provided showcases the interactive dynamics and strategic discussions between players, illustrating how they use reasoning, memory, and collaborative decision-making to deduce the identity of the Impostor.This example serves as a valuable insight into the cognitive processes and communication strategies employed during critical moments of the game after player 1 was killed.• Player 2 (Crewmate): "Player 4: pink was seen killing Player 1: red, and Player 5: purple reported the incident.We should focus our suspicions on pink for now and work together to identify any other suspicious activity.I will head to Upper Engine and then Reactor to complete my tasks.Let's stay vigilant and communicate any findings or suspicions during this phase."• Player 3 (Crewmate): "I have completed the Fix Wiring task in Admin and will now head to Communications for the Download Data task.Let's continue to work together and communicate any further suspicions or findings to identify the Impostor."• Player 4 (Impostor: "Player 2, accusing me without solid evidence is risky.We should be cautious and not jump to conclusions.Let's gather more information and discuss further before making any decisions."• Player 5 (Crewmate): "Player 4: pink, your defense is noted, but the evidence against you is substantial.We should proceed with caution and consider all angles before making a decision.Let's continue to share information and work together to uncover the truth."• Player 2 (Crewmate): "Player 4: pink, your actions have raised suspicions, and we need to carefully consider the evidence against you.Let's keep an eye on each other's movements and interactions to uncover any additional clues that may help us identify the Impostor."• Player 3 (Crewmate): "I am heading to Communications for the Download Data task.Let's continue to work together and gather more information to identify the Impostor."• Player 4 (Impostor): "Player 2, your accusation is based on thin evidence.We should be careful not to make hasty decisions.Let's work together to gather more information before pointing fingers.It's essential to consider all perspectives."• Player 5 (Crewmate): "Player 4: pink, your defense is understandable, but the evidence against you is significant.Let's keep working together to uncover the truth and make a careful decision.We should consider all perspectives and evidence before voting."• Player 2 (Crewmate): "Player 4: pink, the evidence against you is significant, and your actions have raised suspicions.We need to continue gathering information and discussing to ensure we make the right decision.Let's focus on working together to uncover the truth and identify any other potential threats among us."• Player 3 (Crewmate): "I am on my way to Communications for the Download Data task.Let's remain focused on uncovering the truth and identifying any potential Impostors among us."• Player 4 (Impostor): "Player 2, your accusation against me seems rushed without solid evidence.Let's not overlook other possibilities and gather more information.It's crucial to consider everyone's actions before making accusations."• Player 5 (Crewmate): "Player 4: pink, the evidence against you is significant, and we must proceed with caution.Let's focus on working together to uncover the truth and make a careful decision based on all perspectives and evidence."
Human-level play in the game of diplomacy by combining language models with strategic reasoning. Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Jacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, 10.1126/science.ade9097Sasha Mitts, Adithya Renduchintala, Stephen Roller, and Markus Zijlstra. Mike Lewis, Alexander Miller2022a378</p>
<p>Mastering the game of no-press diplomacy via humanregularized reinforcement learning and planning. Anton Bakhtin, David J Wu, Adam Lerer, Jonathan Gray, Athul Paul Jacob, Gabriele Farina, Alexander H Miller, Noam Brown, 2022b</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, Sparks of artificial general intelligence: Early experiments with gpt-4. 2023</p>
<p>Human-level play in the game of <i>diplomacy</i> by combining language models with strategic reasoning. Anton Fair) †, Noam Bakhtin, Emily Brown, Gabriele Dinan, Colin Farina, Daniel Flaherty, Andrew Fried, Jonathan Goff, Hengyuan Gray, Athul Paul Hu, Mojtaba Jacob, Karthik Komeili, Minae Konath, Adam Kwon, Mike Lerer, Alexander H Lewis, Sasha Miller, Adithya Mitts, Stephen Renduchintala, Dirk Roller, Weiyan Rowe, Joe Shi, Alexander Spisak, David Wei, Wu, 10.1126/science.ade9097Meta Fundamental AI Research Diplomacy Team. 37866242022Science</p>
<p>Xidong Feng, Yicheng Luo, Ziyan Wang, Hongrui Tang, Mengyue Yang, Kun Shao, David Mguni, Yali Du, Chessgpt: Bridging policy learning and language modeling. Jun Wang. 2023</p>
<p>. David Ha, Jürgen Schmidhuber, arXiv:1803.101222018World models. arXiv preprint</p>
<p>War and peace (waragent): Large language model-based multi-agent simulation of world wars. Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, Yongfeng Zhang, 2024</p>
<p>Metaagents: Simulating interactions of human behaviors for llm-based task-oriented coordination via collaborative generative agents. Yuan Li, Yixuan Zhang, Lichao Sun, 2023</p>
<p>Avalonbench: Evaluating llms playing the game of avalon. Jonathan Light, Min Cai, Sheng Shen, Ziniu Hu, 2023</p>
<p>Anna A * Kyle *mahowald, Idan A Ivanova, Nancy Blank, Joshua B Kanwisher, Evelina Tenenbaum, Fedorenko, arXiv:2301.06627Dissociating language and thought in large language models: a cognitive perspective. 2023</p>
<p>Gabriel Mukobi, Hannah Erlebach, Niklas Lauffer, Lewis Hammond, Alan Chan, Jesse Clifton, Welfare diplomacy: Benchmarking language model cooperation. 2023</p>
<p>Sung Joon, Joseph C Park, Carrie J O'brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Generative agents: Interactive simulacra of human behavior. 2023</p>
<p>Character-llm: A trainable agent for roleplaying. Yunfan Shao, Linyang Li, Junqi Dai, Xipeng Qiu, 2023</p>
<p>Natalie Shapira, Mosh Levy, Hossein Seyed, Xuhui Alavi, Yejin Zhou, Yoav Choi, Maarten Goldberg, Vered Sap, Shwartz, Clever hans or neural theory of mind? stress testing social reasoning in large language models. 2023</p>
<p>Cooperation on the fly: Exploring language agents for ad hoc teamwork in the avalon game. Zijing Shi, Meng Fang, Shunfeng Zheng, Shilong Deng, Ling Chen, Yali Du, 2023</p>
<p>Avalon's game of thoughts: Battle against deception through recursive contemplation. Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang, Andrew Zhao, Chaofei Wang, Shiji Song, Gao Huang, 2023</p>
<p>Dekun Wu, Haochen Shi, Zhiyuan Sun, and Bang Liu. 2024a. Deciphering digital detectives: Understanding llm behaviors and capabilities in multi-agent mystery games. </p>
<p>. Shuang Wu, Liwen Zhu, Tao Yang, Shiwei Xu, Qiang Fu, Yang Wei, Haobo Fu, 2024bEnhance reasoning for large language models in the game werewolf</p>
<p>Exploring large language models for communication games: An empirical study on werewolf. Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, Yang Liu, 2023</p>
<p>Language agents with reinforcement learning for strategic play in the werewolf game. Zelai Xu, Chao Yu, Fei Fang, Yu Wang, Yi Wu, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
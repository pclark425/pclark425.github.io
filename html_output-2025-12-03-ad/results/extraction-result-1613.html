<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1613 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1613</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1613</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-1418c9da011db25fa95a32989d5a578bc3bc4601</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1418c9da011db25fa95a32989d5a578bc3bc4601" target="_blank">Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning</a></p>
                <p><strong>Paper Venue:</strong> Journal of machine learning research</p>
                <p><strong>Paper TL;DR:</strong> It is illustrated the computational efficiency of IMGEPs as these robotic experiments use a simple memory-based low-level policy representations and search algorithm, enabling the whole system to learn online and incrementally on a Raspberry Pi 3.</p>
                <p><strong>Paper Abstract:</strong> Intrinsically motivated spontaneous exploration is a key enabler of autonomous lifelong learning in human children. It allows them to discover and acquire large repertoires of skills through self-generation, self-selection, self-ordering and self-experimentation of learning goals. We present the unsupervised multi-goal reinforcement learning formal framework as well as an algorithmic approach called intrinsically motivated goal exploration processes (IMGEP) to enable similar properties of autonomous learning in machines. The IMGEP algorithmic architecture relies on several principles: 1) self-generation of goals as parameterized reinforcement learning problems; 2) selection of goals based on intrinsic rewards; 3) exploration with parameterized time-bounded policies and fast incremental goal-parameterized policy search; 4) systematic reuse of information acquired when targeting a goal for improving other goals. We present a particularly efficient form of IMGEP that uses a modular representation of goal spaces as well as intrinsic rewards based on learning progress. We show how IMGEPs automatically generate a learning curriculum within an experimental setup where a real humanoid robot can explore multiple spaces of goals with several hundred continuous dimensions. While no particular target goal is provided to the system beforehand, this curriculum allows the discovery of skills of increasing complexity, that act as stepping stone for learning more complex skills (like nested tool use). We show that learning several spaces of diverse problems can be more efficient for learning complex skills than only trying to directly learn these complex skills. We illustrate the computational efficiency of IMGEPs as these robotic experiments use a simple memory-based low-level policy representations and search algorithm, enabling the whole system to learn online and incrementally on a Raspberry Pi 3.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1613.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1613.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IMGEP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Intrinsically Motivated Goal Exploration Processes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general algorithmic architecture where agents autonomously generate parameterized goals (fitness functions), sample goals using intrinsic rewards (notably competence/learning progress), explore with a fast incremental goal-conditioned exploration policy, and exploit collected data to learn goal-conditioned policies; enables automatic curriculum learning by focusing on goals with highest learning progress.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>IMGEP agent (general architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Population-based meta-policies composed of low-level policies π_θ and a meta-policy Π(θ | g, c); separate fast incremental exploration policy Π_ε (memory-based nearest-neighbor / kd-tree) and slower exploitation policy Π (batch-trained models). Goals are modular object-centered fitness functions over trajectories; intrinsic reward computed from competence/learning progress.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Embodied interaction environments (2D Simulated Tool-Use, Minecraft Mountain Cart, Real Robotic Tool-Use)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Episodic embodied environments where agents execute continuous motor policies producing time-bounded rollouts; outcomes are trajectories or end states of multiple objects (object positions, sound, light, block states). Interactions include moving hand, grasping tools, using tools to move other objects, navigation and tool use (break blocks, push cart), and manipulating joysticks to control toys; distractor objects present.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>procedural motor / tool-use skills (stepping-stone procedural tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Hand → grasp stick (tool) → use stick to move toy (2D sim); navigate → retrieve pickaxe → break blocks → move cart (Minecraft); reach joystick → use joystick to move toy/ball → produce light/sound (robotic).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Hierarchical / stepping-stone structure: complex tasks decompose into prerequisite subtasks (e.g., reach/grasp tool then use tool to act on another object); goals are modular per object and can compose to express constrained objectives over trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>automatic intrinsic motivation / learning-progress based curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Agent builds modular goal spaces per object and maintains a goal-space policy Γ that selects which object-space to practice based on estimated competence learning progress (running average of intrinsic rewards r_i). A goal is sampled inside chosen space; 80% of iterations use exploration policy while 20% use the exploitation policy to evaluate and update learning-progress estimates; the bandit samples goal spaces with 20% random exploration and otherwise proportional to recent learning progress (or softmax in robot case). This produces a self-organized curriculum that focuses on objects yielding maximal competence progress and discovers stepping-stones in increasing complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>learning progress / competence progress (intrinsic reward); with occasional random exploration</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>From single-object motor control (move hand) to multi-step tool-mediated object manipulations (2–4 sequential steps: reach → grasp tool → use tool → manipulate target), e.g., 2–3 required prerequisites in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Across environments AMB/IMGEP agents achieve substantially higher exploration coverage and competence than non-curricular baselines. Representative numbers (25th,50th,75th percentiles): 2D Simulated Magnet Tool exploration (percent cells): AMB 57,61,65 vs RMB 33,36,39 and Flat 8,11,13; Minecraft pickaxe exploration (cells): AMB 41,45,48 vs RMB 33,35,39 and Flat 11,15,19; Minecraft blocks: AMB 73,84,93 vs RMB 69,77,84; Minecraft cart outcomes (counts): AMB 56,360,886 vs RMB 5,162,409; Robotic light exploration (percent/cells metric): AMB 2.0,3.6,4.9 vs RMB 0.8,1.8,3.0. Competence (post-training) on Minecraft goals (25/50/75 percentiles): pickaxe FC 39,49,55; AMB 41,45,49; RMB 37,40,43; cart FC 12,17,25; AMB 8,11,18; RMB 6,9,15. (See Table 1 and Table 2 in paper for full breakdown.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Baselines without learning-progress curriculum perform worse: Random Model Babbling (RMB) and Flat Random Goal Babbling (FRGB) show lower exploration coverage and lower competence. Example: 2D Simulated Magnet Tool: RMB 33,36,39 vs AMB 57,61,65; many complex goals (e.g., cart movement) rarely discovered by pure random or single-goal approaches (SGS often fails).</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Compared strategies: AMB (learning-progress based) vs RMB (random goal-space selection) vs FC (hand-designed fixed curriculum) vs SGS (single goal space) vs FRGB (flat non-modular random goals). AMB generally outperforms RMB and FRGB on exploration coverage and competence, and performs comparably to the engineered Fixed Curriculum (FC) in many metrics (FC sometimes slightly better on specific engineered targets). Statistical tests: AMB significantly better than RMB on Minecraft pickaxe space (p < 0.01); cart space difference AMB vs RMB not significant (p = 0.09); FC not significantly better than AMB on cart and pickaxe spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Evaluated via post-training competence tests with large sets of uniformly sampled goals (800–1000 goals): AMB agents could reach a nontrivial fraction of cart/pickaxe goals (see Table 2). AMB showed better transfer/generalization than RMB on pickaxe goals (statistically significant), but performance on cart goals was more variable and not significantly superior in the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Self-generated curricula based on competence (learning) progress reliably produce ordered discovery of prerequisite skills (stepping stones) and yield much higher exploration coverage and competence on multi-step tool-use tasks than random or flat goal-sampling baselines; modular object-centered goal spaces are crucial (flat representations underperform); stepping-stone preserving mutations (SSPMutation) that respect temporal structure improve exploration of tool-mediated tasks by preserving early successful sub-behaviors; AMB achieves performance comparable to a hand-designed fixed curriculum while being autonomous.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning', 'publication_date_yy_mm': '2017-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1613.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1613.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active Model Babbling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A concrete modular population-based IMGEP instantiation that uses object-centered goal spaces and an active goal-space selection policy Γ driven by learning-progress intrinsic rewards, combined with stepping-stone preserving mutations (SSPMutation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>AMB agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Population-based agent using memory-based nearest-neighbor exploration meta-policy Π_ε (kd-tree), object-centered modular goal spaces G^k, goal policies γ_k that sample goals within each object space, and a goal-space selection bandit Γ that uses running averages of competence progress to choose which object to practice; SSPMutation preserves successful early behavior when mutating policy parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>2D Simulated Tool-Use; Minecraft Mountain Cart; Robotic Tool-Use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same embodied environments as IMGEP entry (see above); AMB instantiated and evaluated in all three.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>tool-use procedural skills (motor sequences requiring prerequisites)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Grasping sticks then using them to move toys; navigating to retrieve pickaxe then breaking blocks and moving a cart; reaching joysticks then using them to move toys/ball and produce light/sound.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Tasks are compositional via prerequisites (reach/grasp tool before using it) and goals are modularly composed per object allowing combinatorial composition of outcomes (e.g., trajectories of object k conditioned on earlier object movements).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>learning-progress driven active curriculum (AM B)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Hierarchical sampling: first select object-space k via Γ (bandit on running average of learning progress r_i^k, 20% random explores), then sample goal g in G^k via γ_k. During training, 80% of iterations use exploration policy Π_ε (no progress update) and 20% use exploitation policy Π to evaluate and update progress estimates; Γ then biases future selection toward spaces with highest recent competence progress.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>estimated competence learning progress per goal-space (intrinsic reward)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Single-object control up to multi-step tool-mediated manipulations (1–3 prerequisite steps observed in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>AMB yields top performance among autonomous strategies: examples from Table 1 — 2D Magnet Tool exploration percentiles 57,61,65 (AMB) vs 33,36,39 (RMB); Minecraft pickaxe 41,45,48 (AMB) vs 33,35,39 (RMB); Minecraft cart outcomes 56,360,886 (AMB) vs 5,162,409 (RMB). Competence percentiles (Minecraft) pickaxe AMB 41,45,49, cart AMB 8,11,18.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Compared to RMB (random goal-space selection) and FRGB (flat non-modular goal space), AMB shows markedly higher exploration and competence; FC (hand-designed curriculum) often achieves similar or slightly better results on some engineered targets, but AMB matches FC overall without expert knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>AMB vs RMB vs FC vs SGS vs FRGB: AMB outperforms RMB and FRGB; FC comparable to AMB on many metrics (engineered sequence can match or slightly exceed AMB in specific cases); SGS (single goal) often fails to discover distant complex goals (e.g., SGS cart competence = 0).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>AMB tested with held-out uniformly sampled goals (800–1000); shows significant competence on pickaxe and nonzero competence on cart goals, indicating some generalization within modular goal spaces though performance varies by stochasticity and reachability of goals.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Active, learning-progress based selection of modular goal-spaces enables autonomous curriculum formation that efficiently discovers prerequisite skills and scales to complex multi-step tool-use tasks; modular representation and SSPMutation are important contributors to success.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning', 'publication_date_yy_mm': '2017-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1613.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1613.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RMB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random Model Babbling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline IMGEP variant that uses the same modular representation and SSPMutation as AMB but selects goal-spaces uniformly at random (no learning-progress driven selection).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RMB agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Population-based agent with modular goal spaces and stepping-stone preserving mutations, but with a goal-space selection policy Γ that samples object-spaces randomly (uniformly) throughout training.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>2D Simulated Tool-Use; Minecraft Mountain Cart; Robotic Tool-Use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same embodied interaction environments; used as ablation baseline to test benefit of active curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>tool-use procedural skills</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Same families of tasks (grasp/use stick to move toy, retrieve pickaxe break blocks move cart, joystick use to move toys/ball).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Tasks possess prerequisite structure but RMB does not exploit it via curriculum; tasks are compositional in environment but agent samples randomly.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>random ordering (no curriculum)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Goal-space choice Γ is uniform random across object modules each iteration (no learning-progress estimation); otherwise full pipeline identical to AMB (including SSPMutation and modular outcome encoding).</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>random</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Same as AMB; however performance is poorer on higher-complexity, multi-step tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>RMB performance reported as baseline: 2D Magnet Tool exploration 33,36,39 (percentiles) vs AMB 57,61,65; Minecraft pickaxe 33,35,39 vs AMB 41,45,48; Minecraft cart outcomes median 162 (RMB) vs 360 (AMB) and much lower 25th/75th in many cases; competence percentiles (Minecraft) pickaxe RMB 37,40,43; cart RMB 6,9,15.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Serves as negative control: random selection reduces discovery of tools and downstream objects compared to AMB and sometimes compared to hand-designed FC.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Lower transfer/competence on held-out uniformly sampled goals compared to AMB (statistically significant on some spaces like pickaxe).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Random goal-space selection reduces discovery efficiency on stepping-stone structured tasks; modular representation alone is insufficient without an adaptive curriculum selection mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning', 'publication_date_yy_mm': '2017-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1613.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1613.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fixed Curriculum (engineered)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hand-designed curriculum that sequences goal-spaces from easiest to most complex according to expert knowledge; used as an extrinsic control to compare to autonomous curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>FC agent (IMGEP with fixed Γ)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same IMGEP / AMB infrastructure but with Γ fixed to an expert-designed ordering over object goal-spaces (explore objects in sequence from easiest/discoverable to most complex), ignoring distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>2D Simulated Tool-Use; Minecraft Mountain Cart; Robotic Tool-Use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>As above; FC used to test whether an engineered curriculum can match autonomous curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>tool-use procedural skills</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Engineered sequences: e.g., hand → tool → toy in 2D sim; agent navigation → pickaxe → blocks → cart in Minecraft; hand → left joystick → right joystick → Ergo → ball in robotics.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Explicit prerequisite ordering specified by designer, matching the environment's stepping-stone structure.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>hand-designed fixed prerequisite curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Engineer defines a sequence of goal-space focuses from easiest-to-discover objects to most complex objects. The agent strictly follows this fixed ordering rather than adaptively selecting by learning progress.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>expert-defined prerequisite ordering (difficulty/prerequisite knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Same range as other agents; ordered explicitly from simple single-object tasks to multi-step tool-mediated tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>FC often matches or slightly outperforms AMB on some metrics: e.g., 2D Magnet Tool exploration percentiles 61,67,70 (FC) vs AMB 57,61,65; Minecraft blocks 100,100,100 (FC) vs AMB 73,84,93; Minecraft cart outcomes 386,787,1207 (FC) vs AMB 56,360,886. Competence percentiles (Minecraft) pickaxe FC 39,49,55 vs AMB 41,45,49; cart FC 12,17,25 vs AMB 8,11,18.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Compared to AMB: FC can match or exceed AMB for certain engineered targets; however FC requires expert knowledge and is not autonomous. AMB attains similar exploration efficiency without hand engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>FC shows strong performance on engineered targets (blocks/cart) but does not provide autonomous discovery outside designed sequence; direct transfer metrics not separately reported beyond exploration/competence numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Engineered curricula can be effective and sometimes slightly outperform autonomous learning-progress curricula on specific targets, but AMB automatically discovers similar curricula without external ordering and without prior task engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning', 'publication_date_yy_mm': '2017-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1613.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1613.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FRGB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flat Random Goal Babbling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A control where the agent uses a single flat outcome/goal space containing all object variables and samples goals randomly, testing the effect of modular vs non-modular goal representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>FRGB agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>IMGEP-style agent but with a single flat goal space that concatenates all object outcome variables; goal sampling uniform and FullMutation used (no temporal stepping-stone preserving), functioning as an ablation to test modularity importance.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>2D Simulated Tool-Use; Minecraft Mountain Cart; Robotic Tool-Use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>As above; FRGB tests whether flat representation with random goal sampling suffices.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>tool-use procedural skills</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Same environment tasks but with flat goal encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Compositionality exists in environment but is not leveraged by flat encoding; tasks not hierarchically separated in agent representation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>flat random sampling (no curriculum)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Single outcome/goal space aggregated across all objects; goals chosen uniformly at random; uses FullMutation (no SSPMutation).</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>random</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Same as other agents, but performance indicates poor scaling to higher complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>FRGB generally underperforms modular approaches: e.g., 2D Magnet Tool exploration 8.0,11,13 (Flat) vs AMB 57,61,65; Minecraft agent position Flat 34,36,40 vs AMB 55,58,61; robotic joystick spaces often near 0 for Flat.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Flat non-modular representation severely reduces discovery efficiency compared to modular IMGEP approaches; demonstrates importance of modular goal decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Poor compared to modular learners; limited competence on complex, multi-step tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Modular object-centered goal spaces are critical for effective automatic curriculum formation; flat combined goal spaces with random sampling perform poorly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning', 'publication_date_yy_mm': '2017-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Active learning of inverse models with intrinsically motivated goal exploration in robots <em>(Rating: 2)</em></li>
                <li>Go-Explore: a New Approach for Hard-Exploration Problems <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1613",
    "paper_id": "paper-1418c9da011db25fa95a32989d5a578bc3bc4601",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [
        {
            "name_short": "IMGEP",
            "name_full": "Intrinsically Motivated Goal Exploration Processes",
            "brief_description": "A general algorithmic architecture where agents autonomously generate parameterized goals (fitness functions), sample goals using intrinsic rewards (notably competence/learning progress), explore with a fast incremental goal-conditioned exploration policy, and exploit collected data to learn goal-conditioned policies; enables automatic curriculum learning by focusing on goals with highest learning progress.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "IMGEP agent (general architecture)",
            "agent_description": "Population-based meta-policies composed of low-level policies π_θ and a meta-policy Π(θ | g, c); separate fast incremental exploration policy Π_ε (memory-based nearest-neighbor / kd-tree) and slower exploitation policy Π (batch-trained models). Goals are modular object-centered fitness functions over trajectories; intrinsic reward computed from competence/learning progress.",
            "agent_size": null,
            "environment_name": "Embodied interaction environments (2D Simulated Tool-Use, Minecraft Mountain Cart, Real Robotic Tool-Use)",
            "environment_description": "Episodic embodied environments where agents execute continuous motor policies producing time-bounded rollouts; outcomes are trajectories or end states of multiple objects (object positions, sound, light, block states). Interactions include moving hand, grasping tools, using tools to move other objects, navigation and tool use (break blocks, push cart), and manipulating joysticks to control toys; distractor objects present.",
            "procedure_type": "procedural motor / tool-use skills (stepping-stone procedural tasks)",
            "procedure_examples": "Hand → grasp stick (tool) → use stick to move toy (2D sim); navigate → retrieve pickaxe → break blocks → move cart (Minecraft); reach joystick → use joystick to move toy/ball → produce light/sound (robotic).",
            "compositional_structure": "Hierarchical / stepping-stone structure: complex tasks decompose into prerequisite subtasks (e.g., reach/grasp tool then use tool to act on another object); goals are modular per object and can compose to express constrained objectives over trajectories.",
            "uses_curriculum": true,
            "curriculum_name": "automatic intrinsic motivation / learning-progress based curriculum",
            "curriculum_description": "Agent builds modular goal spaces per object and maintains a goal-space policy Γ that selects which object-space to practice based on estimated competence learning progress (running average of intrinsic rewards r_i). A goal is sampled inside chosen space; 80% of iterations use exploration policy while 20% use the exploitation policy to evaluate and update learning-progress estimates; the bandit samples goal spaces with 20% random exploration and otherwise proportional to recent learning progress (or softmax in robot case). This produces a self-organized curriculum that focuses on objects yielding maximal competence progress and discovers stepping-stones in increasing complexity.",
            "curriculum_ordering_principle": "learning progress / competence progress (intrinsic reward); with occasional random exploration",
            "task_complexity_range": "From single-object motor control (move hand) to multi-step tool-mediated object manipulations (2–4 sequential steps: reach → grasp tool → use tool → manipulate target), e.g., 2–3 required prerequisites in experiments.",
            "performance_with_curriculum": "Across environments AMB/IMGEP agents achieve substantially higher exploration coverage and competence than non-curricular baselines. Representative numbers (25th,50th,75th percentiles): 2D Simulated Magnet Tool exploration (percent cells): AMB 57,61,65 vs RMB 33,36,39 and Flat 8,11,13; Minecraft pickaxe exploration (cells): AMB 41,45,48 vs RMB 33,35,39 and Flat 11,15,19; Minecraft blocks: AMB 73,84,93 vs RMB 69,77,84; Minecraft cart outcomes (counts): AMB 56,360,886 vs RMB 5,162,409; Robotic light exploration (percent/cells metric): AMB 2.0,3.6,4.9 vs RMB 0.8,1.8,3.0. Competence (post-training) on Minecraft goals (25/50/75 percentiles): pickaxe FC 39,49,55; AMB 41,45,49; RMB 37,40,43; cart FC 12,17,25; AMB 8,11,18; RMB 6,9,15. (See Table 1 and Table 2 in paper for full breakdown.)",
            "performance_without_curriculum": "Baselines without learning-progress curriculum perform worse: Random Model Babbling (RMB) and Flat Random Goal Babbling (FRGB) show lower exploration coverage and lower competence. Example: 2D Simulated Magnet Tool: RMB 33,36,39 vs AMB 57,61,65; many complex goals (e.g., cart movement) rarely discovered by pure random or single-goal approaches (SGS often fails).",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Compared strategies: AMB (learning-progress based) vs RMB (random goal-space selection) vs FC (hand-designed fixed curriculum) vs SGS (single goal space) vs FRGB (flat non-modular random goals). AMB generally outperforms RMB and FRGB on exploration coverage and competence, and performs comparably to the engineered Fixed Curriculum (FC) in many metrics (FC sometimes slightly better on specific engineered targets). Statistical tests: AMB significantly better than RMB on Minecraft pickaxe space (p &lt; 0.01); cart space difference AMB vs RMB not significant (p = 0.09); FC not significantly better than AMB on cart and pickaxe spaces.",
            "transfer_generalization": "Evaluated via post-training competence tests with large sets of uniformly sampled goals (800–1000 goals): AMB agents could reach a nontrivial fraction of cart/pickaxe goals (see Table 2). AMB showed better transfer/generalization than RMB on pickaxe goals (statistically significant), but performance on cart goals was more variable and not significantly superior in the reported experiments.",
            "key_findings": "Self-generated curricula based on competence (learning) progress reliably produce ordered discovery of prerequisite skills (stepping stones) and yield much higher exploration coverage and competence on multi-step tool-use tasks than random or flat goal-sampling baselines; modular object-centered goal spaces are crucial (flat representations underperform); stepping-stone preserving mutations (SSPMutation) that respect temporal structure improve exploration of tool-mediated tasks by preserving early successful sub-behaviors; AMB achieves performance comparable to a hand-designed fixed curriculum while being autonomous.",
            "uuid": "e1613.0",
            "source_info": {
                "paper_title": "Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning",
                "publication_date_yy_mm": "2017-08"
            }
        },
        {
            "name_short": "AMB",
            "name_full": "Active Model Babbling",
            "brief_description": "A concrete modular population-based IMGEP instantiation that uses object-centered goal spaces and an active goal-space selection policy Γ driven by learning-progress intrinsic rewards, combined with stepping-stone preserving mutations (SSPMutation).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "AMB agent",
            "agent_description": "Population-based agent using memory-based nearest-neighbor exploration meta-policy Π_ε (kd-tree), object-centered modular goal spaces G^k, goal policies γ_k that sample goals within each object space, and a goal-space selection bandit Γ that uses running averages of competence progress to choose which object to practice; SSPMutation preserves successful early behavior when mutating policy parameters.",
            "agent_size": null,
            "environment_name": "2D Simulated Tool-Use; Minecraft Mountain Cart; Robotic Tool-Use",
            "environment_description": "Same embodied environments as IMGEP entry (see above); AMB instantiated and evaluated in all three.",
            "procedure_type": "tool-use procedural skills (motor sequences requiring prerequisites)",
            "procedure_examples": "Grasping sticks then using them to move toys; navigating to retrieve pickaxe then breaking blocks and moving a cart; reaching joysticks then using them to move toys/ball and produce light/sound.",
            "compositional_structure": "Tasks are compositional via prerequisites (reach/grasp tool before using it) and goals are modularly composed per object allowing combinatorial composition of outcomes (e.g., trajectories of object k conditioned on earlier object movements).",
            "uses_curriculum": true,
            "curriculum_name": "learning-progress driven active curriculum (AM B)",
            "curriculum_description": "Hierarchical sampling: first select object-space k via Γ (bandit on running average of learning progress r_i^k, 20% random explores), then sample goal g in G^k via γ_k. During training, 80% of iterations use exploration policy Π_ε (no progress update) and 20% use exploitation policy Π to evaluate and update progress estimates; Γ then biases future selection toward spaces with highest recent competence progress.",
            "curriculum_ordering_principle": "estimated competence learning progress per goal-space (intrinsic reward)",
            "task_complexity_range": "Single-object control up to multi-step tool-mediated manipulations (1–3 prerequisite steps observed in experiments).",
            "performance_with_curriculum": "AMB yields top performance among autonomous strategies: examples from Table 1 — 2D Magnet Tool exploration percentiles 57,61,65 (AMB) vs 33,36,39 (RMB); Minecraft pickaxe 41,45,48 (AMB) vs 33,35,39 (RMB); Minecraft cart outcomes 56,360,886 (AMB) vs 5,162,409 (RMB). Competence percentiles (Minecraft) pickaxe AMB 41,45,49, cart AMB 8,11,18.",
            "performance_without_curriculum": "Compared to RMB (random goal-space selection) and FRGB (flat non-modular goal space), AMB shows markedly higher exploration and competence; FC (hand-designed curriculum) often achieves similar or slightly better results on some engineered targets, but AMB matches FC overall without expert knowledge.",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "AMB vs RMB vs FC vs SGS vs FRGB: AMB outperforms RMB and FRGB; FC comparable to AMB on many metrics (engineered sequence can match or slightly exceed AMB in specific cases); SGS (single goal) often fails to discover distant complex goals (e.g., SGS cart competence = 0).",
            "transfer_generalization": "AMB tested with held-out uniformly sampled goals (800–1000); shows significant competence on pickaxe and nonzero competence on cart goals, indicating some generalization within modular goal spaces though performance varies by stochasticity and reachability of goals.",
            "key_findings": "Active, learning-progress based selection of modular goal-spaces enables autonomous curriculum formation that efficiently discovers prerequisite skills and scales to complex multi-step tool-use tasks; modular representation and SSPMutation are important contributors to success.",
            "uuid": "e1613.1",
            "source_info": {
                "paper_title": "Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning",
                "publication_date_yy_mm": "2017-08"
            }
        },
        {
            "name_short": "RMB",
            "name_full": "Random Model Babbling",
            "brief_description": "A baseline IMGEP variant that uses the same modular representation and SSPMutation as AMB but selects goal-spaces uniformly at random (no learning-progress driven selection).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RMB agent",
            "agent_description": "Population-based agent with modular goal spaces and stepping-stone preserving mutations, but with a goal-space selection policy Γ that samples object-spaces randomly (uniformly) throughout training.",
            "agent_size": null,
            "environment_name": "2D Simulated Tool-Use; Minecraft Mountain Cart; Robotic Tool-Use",
            "environment_description": "Same embodied interaction environments; used as ablation baseline to test benefit of active curriculum.",
            "procedure_type": "tool-use procedural skills",
            "procedure_examples": "Same families of tasks (grasp/use stick to move toy, retrieve pickaxe break blocks move cart, joystick use to move toys/ball).",
            "compositional_structure": "Tasks possess prerequisite structure but RMB does not exploit it via curriculum; tasks are compositional in environment but agent samples randomly.",
            "uses_curriculum": false,
            "curriculum_name": "random ordering (no curriculum)",
            "curriculum_description": "Goal-space choice Γ is uniform random across object modules each iteration (no learning-progress estimation); otherwise full pipeline identical to AMB (including SSPMutation and modular outcome encoding).",
            "curriculum_ordering_principle": "random",
            "task_complexity_range": "Same as AMB; however performance is poorer on higher-complexity, multi-step tasks.",
            "performance_with_curriculum": null,
            "performance_without_curriculum": "RMB performance reported as baseline: 2D Magnet Tool exploration 33,36,39 (percentiles) vs AMB 57,61,65; Minecraft pickaxe 33,35,39 vs AMB 41,45,48; Minecraft cart outcomes median 162 (RMB) vs 360 (AMB) and much lower 25th/75th in many cases; competence percentiles (Minecraft) pickaxe RMB 37,40,43; cart RMB 6,9,15.",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Serves as negative control: random selection reduces discovery of tools and downstream objects compared to AMB and sometimes compared to hand-designed FC.",
            "transfer_generalization": "Lower transfer/competence on held-out uniformly sampled goals compared to AMB (statistically significant on some spaces like pickaxe).",
            "key_findings": "Random goal-space selection reduces discovery efficiency on stepping-stone structured tasks; modular representation alone is insufficient without an adaptive curriculum selection mechanism.",
            "uuid": "e1613.2",
            "source_info": {
                "paper_title": "Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning",
                "publication_date_yy_mm": "2017-08"
            }
        },
        {
            "name_short": "FC",
            "name_full": "Fixed Curriculum (engineered)",
            "brief_description": "A hand-designed curriculum that sequences goal-spaces from easiest to most complex according to expert knowledge; used as an extrinsic control to compare to autonomous curricula.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "FC agent (IMGEP with fixed Γ)",
            "agent_description": "Same IMGEP / AMB infrastructure but with Γ fixed to an expert-designed ordering over object goal-spaces (explore objects in sequence from easiest/discoverable to most complex), ignoring distractors.",
            "agent_size": null,
            "environment_name": "2D Simulated Tool-Use; Minecraft Mountain Cart; Robotic Tool-Use",
            "environment_description": "As above; FC used to test whether an engineered curriculum can match autonomous curriculum.",
            "procedure_type": "tool-use procedural skills",
            "procedure_examples": "Engineered sequences: e.g., hand → tool → toy in 2D sim; agent navigation → pickaxe → blocks → cart in Minecraft; hand → left joystick → right joystick → Ergo → ball in robotics.",
            "compositional_structure": "Explicit prerequisite ordering specified by designer, matching the environment's stepping-stone structure.",
            "uses_curriculum": true,
            "curriculum_name": "hand-designed fixed prerequisite curriculum",
            "curriculum_description": "Engineer defines a sequence of goal-space focuses from easiest-to-discover objects to most complex objects. The agent strictly follows this fixed ordering rather than adaptively selecting by learning progress.",
            "curriculum_ordering_principle": "expert-defined prerequisite ordering (difficulty/prerequisite knowledge)",
            "task_complexity_range": "Same range as other agents; ordered explicitly from simple single-object tasks to multi-step tool-mediated tasks.",
            "performance_with_curriculum": "FC often matches or slightly outperforms AMB on some metrics: e.g., 2D Magnet Tool exploration percentiles 61,67,70 (FC) vs AMB 57,61,65; Minecraft blocks 100,100,100 (FC) vs AMB 73,84,93; Minecraft cart outcomes 386,787,1207 (FC) vs AMB 56,360,886. Competence percentiles (Minecraft) pickaxe FC 39,49,55 vs AMB 41,45,49; cart FC 12,17,25 vs AMB 8,11,18.",
            "performance_without_curriculum": null,
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Compared to AMB: FC can match or exceed AMB for certain engineered targets; however FC requires expert knowledge and is not autonomous. AMB attains similar exploration efficiency without hand engineering.",
            "transfer_generalization": "FC shows strong performance on engineered targets (blocks/cart) but does not provide autonomous discovery outside designed sequence; direct transfer metrics not separately reported beyond exploration/competence numbers.",
            "key_findings": "Engineered curricula can be effective and sometimes slightly outperform autonomous learning-progress curricula on specific targets, but AMB automatically discovers similar curricula without external ordering and without prior task engineering.",
            "uuid": "e1613.3",
            "source_info": {
                "paper_title": "Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning",
                "publication_date_yy_mm": "2017-08"
            }
        },
        {
            "name_short": "FRGB",
            "name_full": "Flat Random Goal Babbling",
            "brief_description": "A control where the agent uses a single flat outcome/goal space containing all object variables and samples goals randomly, testing the effect of modular vs non-modular goal representations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "FRGB agent",
            "agent_description": "IMGEP-style agent but with a single flat goal space that concatenates all object outcome variables; goal sampling uniform and FullMutation used (no temporal stepping-stone preserving), functioning as an ablation to test modularity importance.",
            "agent_size": null,
            "environment_name": "2D Simulated Tool-Use; Minecraft Mountain Cart; Robotic Tool-Use",
            "environment_description": "As above; FRGB tests whether flat representation with random goal sampling suffices.",
            "procedure_type": "tool-use procedural skills",
            "procedure_examples": "Same environment tasks but with flat goal encoding.",
            "compositional_structure": "Compositionality exists in environment but is not leveraged by flat encoding; tasks not hierarchically separated in agent representation.",
            "uses_curriculum": false,
            "curriculum_name": "flat random sampling (no curriculum)",
            "curriculum_description": "Single outcome/goal space aggregated across all objects; goals chosen uniformly at random; uses FullMutation (no SSPMutation).",
            "curriculum_ordering_principle": "random",
            "task_complexity_range": "Same as other agents, but performance indicates poor scaling to higher complexity.",
            "performance_with_curriculum": null,
            "performance_without_curriculum": "FRGB generally underperforms modular approaches: e.g., 2D Magnet Tool exploration 8.0,11,13 (Flat) vs AMB 57,61,65; Minecraft agent position Flat 34,36,40 vs AMB 55,58,61; robotic joystick spaces often near 0 for Flat.",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Flat non-modular representation severely reduces discovery efficiency compared to modular IMGEP approaches; demonstrates importance of modular goal decomposition.",
            "transfer_generalization": "Poor compared to modular learners; limited competence on complex, multi-step tasks.",
            "key_findings": "Modular object-centered goal spaces are critical for effective automatic curriculum formation; flat combined goal spaces with random sampling perform poorly.",
            "uuid": "e1613.4",
            "source_info": {
                "paper_title": "Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning",
                "publication_date_yy_mm": "2017-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Active learning of inverse models with intrinsically motivated goal exploration in robots",
            "rating": 2
        },
        {
            "paper_title": "Go-Explore: a New Approach for Hard-Exploration Problems",
            "rating": 2
        }
    ],
    "cost": 0.01744175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning</h1>
<p>Sébastien Forestier<br>Inria Bordeaux Sud-Ouest<br>200 avenue de la Vieille Tour, 33405 Talence, France<br>Rémy Portelas<br>Inria Bordeaux Sud-Ouest<br>200 avenue de la Vieille Tour, 33405 Talence, France<br>Yoan Mollard<br>Inria Bordeaux Sud-Ouest<br>200 avenue de la Vieille Tour, 33405 Talence, France<br>Pierre-Yves Oudeyer<br>Inria Bordeaux Sud-Ouest<br>200 avenue de la Vieille Tour, 33405 Talence, France</p>
<p>Editor: George Konidaris</p>
<h4>Abstract</h4>
<p>Intrinsically motivated spontaneous exploration is a key enabler of autonomous developmental learning in human children. It enables the discovery of skill repertoires through autotelic learning, i.e. the self-generation, self-selection, self-ordering and self-experimentation of learning goals. We present an algorithmic approach called Intrinsically Motivated Goal Exploration Processes (IMGEP) to enable similar properties of autonomous learning in machines. The IMGEP architecture relies on several principles: 1) self-generation of goals, generalized as parameterized fitness functions; 2) selection of goals based on intrinsic rewards; 3) exploration with incremental goal-parameterized policy search and exploitation with a batch learning algorithm; 4) systematic reuse of information acquired when targeting a goal for improving towards other goals. We present a particularly efficient form of IMGEP, called AMB, that uses a population-based policy and an object-centered spatiotemporal modularity. We provide several implementations of this architecture and demonstrate their ability to automatically generate a learning curriculum within several experimental setups. One of these experiments includes a real humanoid robot exploring multiple spaces of goals with several hundred continuous dimensions and with distractors. While no particular target goal is provided to these autotelic agents, this curriculum allows the discovery of diverse skills that act as stepping stones for learning more complex skills, e.g. nested tool use.</p>
<p>Keywords: Developmental learning, developmental AI, open-ended learning, intrinsic motivations, autotelic agents, population-based IMGEP, goal exploration, curiosity-driven learning, modularity, robotics, automatic curriculum learning.</p>
<h2>1. Introduction</h2>
<p>An extraordinary property of natural intelligence in humans is their capacity for lifelong autonomous learning. During cognitive development, processes of autonomous learning in infants have several properties that are fundamentally different from many current machine learning systems. Among them is the capability to spontaneously explore their environments, driven by an intrinsic motivation to discover and learn new tasks and problems that they imagine and select by themselves (Berlyne, 1966; Gopnik et al., 1999). Crucially, there is no engineer externally imposing one target goal that</p>
<p>they should explore, hand providing a curriculum for learning, nor providing a ready-to-use database of training examples. Rather, children are autotelic learners: they self-select their objectives within a large, potentially open-ended, space of goals they can imagine, and they collect training data by physically practicing these goals. In particular, they explore goals in an organized manner, attributing to them values of interestingness that evolve with time, and allowing them to self-define a learning curriculum that is called a developmental trajectory in developmental sciences (Thelen and Smith, 1996). This self-generated learning curriculum prevents infants from spending too much time on goals that are either too easy or too difficult, and allows them to focus on goals of the right level of complexity at the right time. Within this developmental learning process, the new learned solutions are often stepping stones for discovering how to solve other goals of increasing complexity. Thus, while they are not explicitly guided by a final target goal, these mechanisms allow infants to discover highly complex skills. For instance, biped locomotion or tool use would be extremely difficult to learn by focusing only on these targets from the start as the rewards for those goals are typically rare or deceptive.</p>
<p>An essential component of such organized spontaneous exploration is the intrinsic motivation system, also called curiosity-driven exploration system (Kidd and Hayden, 2015; Oudeyer and Smith, 2016; Gottlieb and Oudeyer, 2018). In the last two decades, a series of computational and robotic models of intrinsically motivated exploration and learning in infants have been developed (Oudeyer and Kaplan, 2007; Baldassarre and Mirolli, 2013; Bazhydai et al., 2020), opening new theoretical perspectives in neuroscience and psychology (Gottlieb et al., 2013). Two key ideas have allowed to simulate and predict important properties of infant spontaneous exploration, ranging from vocal development (Moulin-Frier et al., 2014; Forestier and Oudeyer, 2017), to object affordance and tool learning (Forestier and Oudeyer, 2016a,c). The first key idea is that infants might select experiments that maximize an intrinsic reward based on empirical learning progress (Oudeyer et al., 2007). This mechanism would generate automatically developmental trajectories (e.g. learning curricula) where progressively more complex tasks are practiced, learned and used as stepping stones for more complex skills. The second key idea is that beyond selecting actions or states based on the predictive learning progress they provide, a more powerful way to organize intrinsically motivated exploration is to select goals, i.e. self-generated fitness functions, based on a measure of competence progress, i.e. a measure of progress in learning to produce diverse and controlled behavioral features (Baranes and Oudeyer, 2010b, 2013). Here, the intrinsic reward is the empirical improvement towards solving self-selected goals (Oudeyer and Kaplan, 2007; Forestier and Oudeyer, 2016a), happening through lower-level policy search mechanisms that generate physical actions. The efficiency of such goal exploration processes relies on a form of hindsight learning that leverages the fact that the data collected when targeting a goal can be informative to find better solutions to other goals (for example, a learner trying to achieve the goal of pushing an object on the right but actually pushing it on the left fails to progress on this goal, but learns as a side effect how to push it on the left). These general ideas have been instantiated and studied in the context of population-based learning architectures (e.g. Baranes and Oudeyer (2013); Péré et al. (2018)), as well as more recently in goal-conditioned reinforcement learning architectures (e.g. Colas et al. (2019); Nair et al. (2018); Choi et al. (2021)), used to build autotelic agents (Colas et al., 2022).</p>
<p>Beyond neuroscience and psychology, we believe these models open new perspectives in artificial intelligence, contributing to the foundations of the new field of developmental artificial intelligence (Eppe and Oudeyer, 2021). In particular, algorithmic architectures for intrinsically motivated goal exploration were shown to allow the efficient acquisition of repertoires of high-dimensional motor skills with automated curriculum learning in several robotics experiments (Baranes and Oudeyer, 2013; Forestier and Oudeyer, 2016a). This includes for example learning omnidirectional locomotion or learning multiple ways to manipulate complex soft objects (Rolf et al., 2010; Baranes and Oudeyer, 2013).</p>
<p>In this article, we make several contributions:</p>
<ul>
<li>We present a formalization of Intrinsically Motivated Goal Exploration Processes (IMGEP), that is both more compact and more general than these previous models. In particular, it considers a generalized definition of the concept of goals, construed as abstract parameterized fitness functions that can express arbitrary objectives over full behavioural trajectories and include constraints. This enables to express a diversity of goal exploration algorithms in the same framework, including Quality-Diversity algorithms that were not previously formalized as goal exploration algorithms.</li>
<li>We present a new population-based IMGEP algorithmic architecture, called AMB, implementing two forms of object-centered modularity and using learning-progress to sample associated modular goal spaces. First, we introduce spatial modularity: each object of the environment is associated to a goal space. Second, we introduce temporal modularity: the temporal structure of objects' movement is leveraged for more efficient leveraging of discovered stepping-stones in goal exploration, through a stepping-stone preserving mutation operator (SSPMutation). We also present various instantiations.</li>
<li>We present a systematic experimental study of this new IMGEP algorithm in diverse environments providing opportunities for discovering complex skills like tool use, as well as including complex distractors: a 2D simulated environment, a Minecraft environment, and a real humanoid robotic setup. We compare several variants of IMGEP algorithms, including ablations, in terms of sample efficiency to discover a diversity of behavioral features. We also compare IMGEPs algorithms with algorithms exploring only one target object: we show that letting agents self-organize exploration of diverse objects is vastly more efficient for discovering how to control the target object than channeling the agent to explore only this object. We also compare the exploration resulting from the self-organized learning curriculum of intrinsically motivated agents with the exploration following a curriculum designed by hand with expert knowledge of the task, showing a similar exploration efficiency.</li>
</ul>
<h1>2. Intrinsically Motivated Goal Exploration Processes</h1>
<p>We define a framework for the intrinsically motivated exploration of multiple goals, where the data collected when exploring a goal give some information to help reach other goals. This framework considers that when the agent performed an experiment, it can compute the fitness of that experiment for achieving any goal, not only the one it was trying to reach. Importantly, it does not assume that all goals are achievable, nor that they are of a particular form, enabling to express complex objectives that do not simply depend on the observation of the end policy state but might depend on several aspects of entire behavioral trajectories (see Box on features, goals and goal spaces). Also, the agent autonomously builds its goals but does not know initially which goals are achievable or not, which are easy and which are difficult, nor if certain goals need to be explored so that other goals become achievable.</p>
<h3>2.1 Notations and Assumptions</h3>
<p>Let's consider an agent that executes continuous actions $a \in \mathcal{A}$ in continuous states $s \in \mathcal{S}$ of an environment $E$. We consider policies producing time-bounded rollouts through the dynamics $\delta_{E}\left(\boldsymbol{s}<em _boldsymbol_t="\boldsymbol{t">{\boldsymbol{t}+\boldsymbol{1}} \mid \boldsymbol{s}</em><em _boldsymbol_t="\boldsymbol{t">{0}: \boldsymbol{t}}, \boldsymbol{a}</em><em t__0="t_{0">{0}: \boldsymbol{t}}\right)$ of the environment, and we denote the corresponding behavioral trajectories $\tau=\left{s</em>$.}}, a_{t_{0}}, \cdots, s_{t_{e n d}}, a_{t_{e n d}}\right} \in \mathbb{T</p>
<p>We assume that the agent is able to construct a goal space $\mathcal{G}$ parameterized by $g$, representing fitness functions $f_{g}$ giving the fitness $f_{g}(\tau)$ of an experimentation $\tau$ to reach a goal $g$ (see Box on</p>
<h1>Generalized Goals and Goal Spaces</h1>
<p>In the general case, the agent has algorithmic tools to construct any goal as any function $f_{g}$ (parameterized by vector $g$ ), taking as input a state-action trajectory $\tau$, and returning the fitness of $\tau$ for achieving the goal. As a shorthand, we call the goal $g$ (vector of parameters), but the full representation of the goal consists in the combination of the parameters $g$ and the function or program computing the fitness function ${ }^{a}$.</p>
<p>Formally, given a behavioural trajectory $\tau=\left{s_{t_{0}}, a_{t_{0}}, \cdots, s_{t_{s n d}}, a_{t_{s n d}}\right}$, we assume the agent can compute a set of behavioural features $\varphi_{1}(\tau), \ldots, \varphi_{n}(\tau)$, also called outcomes, over the full trajectory. Those features are vectors that encode any static or dynamic property of the environment or the agent itself. Examples include the average speed of an object, an encoding of an object's trajectory or relation to other objects, the frequency of a movement, or the result of a test checking whether a movement or an object fulfills some properties specified in a sub-part of vector $g$. The features may be given to the agent, or learned, for instance using generative models (see Péré et al. (2018); Nair et al. (2018); Laversanne-Finot et al. (2018); Reinke et al. (2020)).</p>
<p>We also assume that computational tools, in the form of mathematical operators ${ }^{b}$, are available to the agent for constructing goals, i.e. for constructing fitness functions $f_{g}$ using these features. Examples include:</p>
<ul>
<li>$f_{g}(\tau)=\varphi_{g}(\tau)$ : the goal is to produce a trajectory $\tau$ that maximizes feature $\varphi_{g}(\tau)$, where the goal parameter vector $g$ is a simple one dimensional index of the target behavioural feature. Sampling the space of goals thus amounts to sampling which feature to maximize, e.g. maximize agent's speed or number of objects collected.</li>
<li>$f_{g}(\tau)=-\left|\varphi_{\psi_{1}(g)}(\tau)-\psi_{2}(g)\right|$ : the goal $g$ is to produce a trajectory $\tau$ so that its features $\varphi_{\psi_{1}(g)}(\tau)$ are as close as possible to the target vector $\psi_{2}(g)$, using a measure $|\cdot|$, e.g. move the hand or a ball to a particular 3D position (Baranes and Oudeyer, 2013; Péré et al., 2018), or produce a sound with a particular target spectrum (Moulin-Frier et al., 2014).</li>
<li>$f_{g}(\tau)=\varphi_{i}(\tau)$ if $\varphi_{j}(\tau) \in \psi_{3}(g)$ else 0 : the goal $g$ is to produce a trajectory $\tau$ which maximizes feature $i$ while keeping feature vector $j$ inside a local region specified by $\psi_{3}(g)$, e.g. maximize agent's speed while displaying a certain type of pattern of leg movement. When goal sampling is made in this form of function space, IMGEPs correspond to quality-diversity algorithms (Pugh et al., 2016) such as MAP-Elite (Cully et al., 2015).</li>
<li>$f_{g}(\tau)=f_{g_{1}}(\tau)$ if $f_{g_{2}}(\tau)&lt;f_{g_{3}}(\tau)$ else $f_{g_{4}}(\tau)$ : goals can be combined to form more complex constrained optimization problems, e.g. move the ball to follow a target while not getting too close to the walls and holes and minimizing the energy spent.</li>
</ul>
<p>A goal space is a set of goals (fitness functions) parameterized by a vector $g$. Diverse forms of structured parameterization can be used, as shown by the examples above, and corresponding to diverse types of goals. In the experiments of this paper, we define several goal spaces, each with a parameterization representing the target trajectory of positions or sound or light of an object in the environment. Each object $k$ and modality $m$ defines a goal space $\mathcal{G}^{k, m}$ containing goals $g$ of the form $f_{g}^{k, m}(\tau)=-\left|\varphi_{\psi_{1}^{k, m}(g)}(\tau)-\psi_{2}^{k, m}(g)\right|$ where $\psi_{2}^{k, m}(g)$ denotes the target trajectory of object $k$ in modality $m$ (positions, sound or light), and $\varphi_{\psi_{1}^{k, m}(g)}(\tau)$ denotes the features of trajectory $\tau$ of the same object and modality.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Architecture 1 Intrinsically Motivated Goal Exploration Process (IMGEP)</h1>
<div class="codehilite"><pre><span></span><code><span class="n">Require</span><span class="o">:</span><span class="w"> </span><span class="n">Action</span><span class="w"> </span><span class="n">space</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">A</span><span class="o">}\),</span><span class="w"> </span><span class="n">State</span><span class="w"> </span><span class="n">space</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">S</span><span class="o">}\)</span>
<span class="w">    </span><span class="n">Initialize</span><span class="w"> </span><span class="n">knowledge</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">E</span><span class="o">}=\</span><span class="n">varnothing</span><span class="o">\)</span>
<span class="w">    </span><span class="n">Initialize</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="n">space</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">G</span><span class="o">}\)</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="o">\(\</span><span class="n">Gamma</span><span class="o">\)</span>
<span class="w">    </span><span class="n">Initialize</span><span class="w"> </span><span class="n">policies</span><span class="w"> </span><span class="o">\(\</span><span class="n">Pi</span><span class="o">\)</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="o">\(\</span><span class="n">Pi_</span><span class="o">{\</span><span class="n">epsilon</span><span class="o">}\)</span>
<span class="w">    </span><span class="n">Launch</span><span class="w"> </span><span class="n">asynchronously</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="n">loops</span><span class="o">:</span>
<span class="w">    </span><span class="n">loop</span><span class="w"> </span><span class="o">\(\</span><span class="n">triangleright</span><span class="o">\)</span><span class="w"> </span><span class="n">Exploration</span><span class="w"> </span><span class="n">loop</span>
<span class="w">        </span><span class="n">Choose</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="o">\(</span><span class="n">g</span><span class="o">\)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">G</span><span class="o">}\)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="o">\(\</span><span class="n">Gamma</span><span class="o">\)</span>
<span class="w">        </span><span class="n">Execute</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">roll</span><span class="o">-</span><span class="n">out</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="o">\(\</span><span class="n">Pi_</span><span class="o">{\</span><span class="n">epsilon</span><span class="o">}\),</span><span class="w"> </span><span class="n">observe</span><span class="w"> </span><span class="n">trajectory</span><span class="w"> </span><span class="o">\(\</span><span class="n">tau</span><span class="o">\)</span>
<span class="w">        </span><span class="o">\(\</span><span class="n">triangleright</span><span class="o">\)</span><span class="w"> </span><span class="n">From</span><span class="w"> </span><span class="n">now</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="o">\(</span><span class="n">f_</span><span class="o">{</span><span class="n">g</span><span class="o">^{\</span><span class="n">prime</span><span class="o">}}(\</span><span class="n">tau</span><span class="o">)\)</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">computed</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">estimate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">fitness</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">current</span><span class="w"> </span><span class="n">experiment</span><span class="w"> </span><span class="o">\(\</span><span class="n">tau</span><span class="o">\)</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="n">achieving</span><span class="w"> </span><span class="n">any</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="o">\(</span><span class="n">g</span><span class="o">^{\</span><span class="n">prime</span><span class="o">}</span><span class="w"> </span><span class="o">\</span><span class="k">in</span><span class="w"> </span><span class="o">\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">G</span><span class="o">}\)</span>
<span class="w">        </span><span class="n">Compute</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">fitness</span><span class="w"> </span><span class="o">\(</span><span class="n">f</span><span class="o">=</span><span class="n">f_</span><span class="o">{</span><span class="n">g</span><span class="o">}(\</span><span class="n">tau</span><span class="o">)\)</span><span class="w"> </span><span class="n">associated</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="o">\(</span><span class="n">g</span><span class="o">\)</span>
<span class="w">        </span><span class="n">Compute</span><span class="w"> </span><span class="kd">intrinsic</span><span class="w"> </span><span class="n">reward</span><span class="w"> </span><span class="o">\(</span><span class="n">r_</span><span class="o">{</span><span class="n">i</span><span class="o">}=</span><span class="n">I</span><span class="w"> </span><span class="n">R</span><span class="o">(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">E</span><span class="o">},</span><span class="w"> </span><span class="n">g</span><span class="o">,</span><span class="w"> </span><span class="n">f</span><span class="o">)\)</span><span class="w"> </span><span class="n">associated</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="o">\(</span><span class="n">g</span><span class="o">\)</span>
<span class="w">        </span><span class="n">Update</span><span class="w"> </span><span class="n">exploration</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="o">\(\</span><span class="n">Pi_</span><span class="o">{\</span><span class="n">epsilon</span><span class="o">}\)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="o">\((\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">E</span><span class="o">},</span><span class="w"> </span><span class="n">g</span><span class="o">,</span><span class="w"> </span><span class="o">\</span><span class="n">tau</span><span class="o">,</span><span class="w"> </span><span class="n">f</span><span class="o">)</span><span class="w"> </span><span class="o">\</span><span class="n">quad</span><span class="w"> </span><span class="o">\</span><span class="n">triangleright</span><span class="o">\)</span><span class="w"> </span><span class="n">e</span><span class="o">.</span><span class="na">g</span><span class="o">.</span><span class="w"> </span><span class="n">fast</span><span class="w"> </span><span class="n">incremental</span><span class="w"> </span><span class="n">algo</span><span class="o">.</span>
<span class="w">        </span><span class="n">Update</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="o">\(\</span><span class="n">Gamma</span><span class="o">\)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="o">\(\</span><span class="n">left</span><span class="o">(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">E</span><span class="o">},</span><span class="w"> </span><span class="n">g</span><span class="o">,</span><span class="w"> </span><span class="o">\</span><span class="n">tau</span><span class="o">,</span><span class="w"> </span><span class="n">f</span><span class="o">,</span><span class="w"> </span><span class="n">r_</span><span class="o">{</span><span class="n">i</span><span class="o">}\</span><span class="n">right</span><span class="o">)\)</span>
<span class="w">        </span><span class="n">Update</span><span class="w"> </span><span class="n">knowledge</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">E</span><span class="o">}\)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="o">\(\</span><span class="n">left</span><span class="o">(</span><span class="n">g</span><span class="o">,</span><span class="w"> </span><span class="o">\</span><span class="n">tau</span><span class="o">,</span><span class="w"> </span><span class="n">f</span><span class="o">,</span><span class="w"> </span><span class="n">r_</span><span class="o">{</span><span class="n">i</span><span class="o">}\</span><span class="n">right</span><span class="o">)\)</span>
<span class="w">    </span><span class="n">loop</span><span class="w"> </span><span class="o">\(\</span><span class="n">triangleright</span><span class="o">\)</span><span class="w"> </span><span class="n">Exploitation</span><span class="w"> </span><span class="n">loop</span>
<span class="w">        </span><span class="n">Update</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="o">\(\</span><span class="n">Pi</span><span class="o">\)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">E</span><span class="o">}</span><span class="w"> </span><span class="o">\</span><span class="n">quad</span><span class="w"> </span><span class="o">\</span><span class="n">triangleright</span><span class="o">\)</span><span class="w"> </span><span class="n">e</span><span class="o">.</span><span class="na">g</span><span class="o">.</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">deep</span><span class="w"> </span><span class="n">NN</span><span class="o">,</span><span class="w"> </span><span class="n">SVMs</span><span class="o">,</span><span class="w"> </span><span class="n">GMMs</span>
<span class="w">        </span><span class="n">Update</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="n">space</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">G</span><span class="o">}\)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">E</span><span class="o">}\)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="o">\(\</span><span class="n">Pi</span><span class="o">\)</span>
</code></pre></div>

<p>features, goals and goal spaces). Also, we assume that given a trajectory $\tau$, the agent can compute $f_{g}(\tau)$ for any $g \in \mathcal{G}$.</p>
<p>Given $\mathcal{S}, \mathcal{A}$ and $\mathcal{G}$, the agent explores the environment by sampling goals in $\mathcal{G}$ and searching for good solutions to those goals, and learns a goal-parameterized policy $\Pi\left(\boldsymbol{a}<em _boldsymbol_t="\boldsymbol{t">{\boldsymbol{t}+\mathbf{1}} \mid \boldsymbol{g}, \boldsymbol{s}</em><em _boldsymbol_t="\boldsymbol{t">{0}: \boldsymbol{t}+\mathbf{1}}, \boldsymbol{a}</em>\right)$ to reach any goal from any state.}_{0}: \boldsymbol{t}</p>
<p>We can then evaluate the agent's exploration and learning efficiency either by observing its behavior and estimating the diversity of its skills and the reached stepping-stones, or by "opening" agent's internal models and policies to analyze their properties.</p>
<h3>2.2 Algorithmic Architecture</h3>
<p>We present Intrinsically Motivated Goal Exploration Processes (IMGEP) as an algorithmic architecture that can be instantiated into many particular algorithms sharing several general principles (see pseudo-code in Architecture 1):</p>
<ul>
<li>The agent autonomously builds and samples goals as fitness functions, possibly using intrinsic rewards,</li>
<li>Two processes are running in parallel: 1) an exploration loop samples goals and searches for good solutions to those goals with the exploration policy; 2) an exploitation loop uses the data collected during exploration to improve the goal-parameterized policy over the goal space(s),</li>
<li>The data acquired when exploring solutions for a particular goal is reused to extract potential solutions to other goals.</li>
</ul>
<h3>2.3 Goal Exploration</h3>
<p>In the exploration loop, the agent samples a goal $g$, executes its exploration policy $\Pi_{\epsilon}$, and observes the resulting trajectory $\tau$. This new experiment $\tau$ can be used to:</p>
<ul>
<li>compute the fitness associated to goal $g$,</li>
<li>compute an intrinsic reward evaluating the interest of the choice of $g$,</li>
<li>update the goal policy (sampling strategy) using this intrinsic reward,</li>
<li>update the exploration policy $\Pi_{e}$ with a fast incremental learning algorithm,</li>
<li>update the learning database $\mathcal{E}$.</li>
</ul>
<p>Then, asynchronously, this learning database $\mathcal{E}$ can be used to learn a target policy $\Pi$ with a slower or more computationally demanding algorithm, but on the other end resulting in a more accurate policy. The goal space may also be updated based on this data.</p>
<h1>2.4 Intrinsic Rewards</h1>
<p>In goal exploration, a goal $g \in \mathcal{G}$ is chosen at each iteration. $\mathcal{G}$ may be infinite, continuous and of high-dimensionality, making the choice of goal important and non-obvious. Indeed, even if the fitness function $f_{g^{\prime}}(\tau)$ may give information about the fitness of a trajectory $\tau$ to achieve many goals $g^{\prime} \in \mathcal{G}$, the policy leading to $\tau$ has been chosen with the goal $g$ to solve in mind, thus it may not give as much information about other goals than the execution of another policy chosen when targeting other goals.</p>
<p>Intrinsic rewards provide a mean for the agent to self-estimate the expected interest of exploring particular goals for learning how to achieve all goals in $\mathcal{G}$. An intrinsic reward signal $r_{i}$ is associated to a chosen goal $g$, and based on a heuristic (denoted $I R$ ) such as outcome novelty, progress in reducing outcome prediction error, or progress in competence to solve problems (Oudeyer and Kaplan, 2007).</p>
<p>In the experiments of this paper, we use intrinsic rewards based on measuring the competence progress towards the self-generated goals, which has been shown to be particularly efficient for learning repertoires of high-dimensional robotics skills (Baranes and Oudeyer, 2013). Figure 1 shows a schematic representation of possible learning curves and the exploration preference of an agent with intrinsic rewards based on learning progress.</p>
<h2>3. Modular Population-Based IMAGEP Architecture</h2>
<p>In the previous section, we defined the most general IMAGEP architecture without specifying the implementation of its components such as goals and policies. We define here a particular IMAGEP architecture, used in the experiments of this paper, where the goal-parameterized policy $\Pi$ is based on a population of solutions, the goal space $\mathcal{G}$ is constructed in a modular manner from a set of objects and the exploration mutations is temporally modular through taking into account the movement of those objects. This particular architecture is called Modular Population-Based IMAGEP, and we detail its ingredients in the following sections. Its pseudo-code is provided in Architecture 2. Figure 2 summarizes the different components of Active Model Babbling (AMB), our implementation of the Modular Population-Based IMAGEP architecture, using learning progress for goal sampling and stepping-stone preserving mutations.</p>
<h3>3.1 Population-Based Meta-Policies $\Pi$ and $\Pi_{e}$</h3>
<p>In this version of the IMAGEP framework, the goal-parameterized policy $\Pi$ is based on a population of low-level policies. We consider that the starting state $s_{t_{0}}$ of a trajectory is characterized by a feature vector $c$ called context. The policy $\Pi$ is built from a set of low-level policies $\pi_{\theta}$ parameterized by $\theta \in \Theta$, and from a meta-policy $\Pi(\boldsymbol{\theta} \mid \boldsymbol{g}, \boldsymbol{c})$ which, given a goal and context, chooses the best policy $\pi_{\theta}$ to achieve the goal $g$. The policies $\pi_{\theta}\left(\boldsymbol{a}<em _boldsymbol_t="\boldsymbol{t">{\boldsymbol{t}+\mathbf{1}} \mid \boldsymbol{s}</em><em _boldsymbol_t="\boldsymbol{t">{0} ; \boldsymbol{t}+\mathbf{1}}, \boldsymbol{a}</em>\right)$ can be implemented for instance by stochastic black-box generators or small neural networks (see experimental section).}_{0} ; \boldsymbol{t}</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Schematic representation of possible learning curves for different goals and the associated exploration preference for an agent with intrinsic rewards based on learning progress. Left: schematic learning curves associated to 5 imaginary goals: the y axis represents the competence of the agent to achieve the goal ( 1 is perfect, 0 is chance level), and the x axis is training time on a goal. The blue, orange and green curves represent achievable goals, for which agent's competence increases with training, at different rates, and saturates after a long training time. The purple curve represents a goal on which the agent always has the same competence, with no progress. The red curve is the learning curve on an unreachable goal, e.g. moving an uncontrollable object. Right: exploration preference of an agent with a learning progress heuristic (competence derivative) to explore the 5 goals defined by the learning curves. At the beginning of exploration, the agent makes the most progress on goal blue so it prefers to train on this one, and then its preference will shift towards goals orange and green. The agent is making no progress on goal purple so will not choose to explore it, and goal red has a noisy but low estimated learning progress.</p>
<p>During the goal exploration loop, the main objective consists in collecting data that covers well the space of goals: finding $\theta$ parameters that yield good solutions to as many goals as possible. The exploration meta-policy $\Pi_{\epsilon}(\boldsymbol{\theta} \mid \boldsymbol{g}, \boldsymbol{c})$ is learned and used to output a distribution of policies $\pi_{\theta}$ that are interesting to execute to gather information for solving in context $c$ the self-generated goal $g$ and goals similar to $g$. To achieve the objective of collecting interesting data, the exploration meta-policy $\Pi_{\epsilon}$ must have fast and incremental updates. As the aim is to maximize the coverage of the space of goals, being very precise when targeting goals is less crucial than the capacity to update the meta-policy quickly and incrementally. In our experiments, the exploration meta-policy $\Pi_{\epsilon}(\boldsymbol{\theta} \mid \mathbf{g}, \mathbf{c})$ is implemented as a fast memory-based nearest neighbor search with a kd-tree (Bentley, 1975). More precisely, we record for each policy $\pi_{\theta}$ in our database the associated context $c$ it was used in and the outcome $o_{\tau}$ it produced. Then, given a new context $c^{\text {new }}$ and a goal $g$ to attain, which correspond to an (object-specific) outcome $o$ to produce, $\Pi_{\epsilon}$ selects for interaction the policy $\pi_{\theta^{\prime}}$ whose associated context and (object-specific) outcome is closest to $c^{\text {new }}$ and $g$ (using a nearest neighbor search in the concatenated context-outcome space). Note that $c$ is not used for goal selection, such that a single learning progress-based motivation level can be inferred per goal space.</p>
<p>On the contrary, the purpose of the target meta-policy $\Pi$ is to be used in exploitation mode: later on, it can be asked to solve as precisely as possible some goals $g$ with maximum fitness. As the training of this meta-policy can be done asynchronously from data collected by the goal exploration loop, this allows the use of slower training algorithms, possibly batch, that might generalize better, e.g. using Gaussian mixture models, support vector regression or (deep) neural networks. These differences justify the fact that IMGEP uses in general two different representations and learning algorithms for $\Pi_{\epsilon}$ and $\Pi$. This two-level learning scheme has similarities with the Complementary</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Summary of AMB, our modular population-based IMGEP implementation. At each iteration, the agent observes the current context $c$ and chooses a goal space (an object) to explore based on intrinsic rewards (the learning progress to move each object) with $\Gamma$. Then a particular goal $g$ for the chosen object is sampled with $\gamma_{k}$, for instance to push the left joystick to the right. The agent chooses the best policy parameters $\theta$ to reach this goal, with the exploration meta-policy $\Pi_{\epsilon}$, and using an internal model of the world implementing object-centered modularity in goals and mutations. The agent executes policy $\pi_{\theta}$, observes the trajectory $\tau$ and compute the outcome $o_{\tau}$ encoding the movement of each object. Finally, each component is updated with the result of this experiment.</p>
<p>Learning Systems Theory used to account for the organization of learning in mammalian brains (Kumaran et al., 2016). To keep our experiments tractable and focus on studying intrinsically motivated exploration, we consider a simple target meta-policy $\Pi$, corresponding to our exploration meta-policy without mutations mechanisms (section 3.3).</p>
<h1>3.2 Object-Centered Modular Goal Construction</h1>
<p>In the IMGEP architecture, the agent builds and samples goals autonomously. Here, we consider the particular case where the agent builds several goal spaces that correspond to moving each object in the environment. Each goal space represents features of the movement of an object in the environment, such as its end position in $\tau$ or its full trajectory.</p>
<p>We define the outcome $o_{\tau} \in \mathcal{O}$ of an experiment $\tau$ as the features of the movement of all objects, so that $\mathcal{O}=\prod_{k} \mathcal{O}^{k}$ where $o_{\tau}^{k} \in \mathcal{O}^{k}$ are the features of object $k$. Those features come from a perceptual system that may be given to the agent or learned by the agent. From feature space $\mathcal{O}^{k}$, the agent can autonomously generate a corresponding goal space $\mathcal{G}^{k}$ that contains fitness functions of the form $f_{g}(\tau)=-\left|g-o_{\tau}^{k}\right|<em k="k">{k}$. The norm $|\cdot|</em>$.}$ is a distance in the space $O^{k}$, which can be normalized to be able to compare the fitness of goals across goal spaces. The goal space is thus modular, composed of several object-related subspaces: $\mathcal{G}=\bigcup_{k} \mathcal{G}^{k</p>
<h1>Architecture 2 Modular Population-Based IMGEP</h1>
<div class="codehilite"><pre><span></span><code><span class="n">Require</span><span class="o">:</span><span class="w"> </span><span class="n">Action</span><span class="w"> </span><span class="n">space</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">A</span><span class="o">}\),</span><span class="w"> </span><span class="n">State</span><span class="w"> </span><span class="n">space</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">S</span><span class="o">}\),</span><span class="w"> </span><span class="n">Context</span><span class="w"> </span><span class="n">space</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">C</span><span class="o">}\),</span><span class="w"> </span><span class="n">Outcome</span><span class="w"> </span><span class="n">space</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">O</span><span class="o">}\)</span>
<span class="w">    </span><span class="n">Initialize</span><span class="w"> </span><span class="n">knowledge</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">E</span><span class="o">}=\</span><span class="n">varnothing</span><span class="o">\)</span>
<span class="w">    </span><span class="n">Initialize</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="n">space</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">G</span><span class="o">}\),</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="n">policies</span><span class="w"> </span><span class="o">\(\</span><span class="n">gamma_</span><span class="o">{</span><span class="n">k</span><span class="o">}\)</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="n">space</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="o">\(\</span><span class="n">Gamma</span><span class="o">\)</span>
<span class="w">    </span><span class="n">Initialize</span><span class="w"> </span><span class="n">meta</span><span class="o">-</span><span class="n">policies</span><span class="w"> </span><span class="o">\(\</span><span class="n">Pi</span><span class="o">\)</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="o">\(\</span><span class="n">Pi_</span><span class="o">{\</span><span class="n">epsilon</span><span class="o">}\)</span>
<span class="w">    </span><span class="n">Launch</span><span class="w"> </span><span class="n">asynchronously</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="n">loops</span><span class="o">:</span>
<span class="w">    </span><span class="n">loop</span><span class="w"> </span><span class="o">\(\</span><span class="n">triangleright</span><span class="o">\)</span><span class="w"> </span><span class="n">Exploration</span><span class="w"> </span><span class="n">loop</span>
<span class="w">        </span><span class="n">Observe</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">\(</span><span class="n">c</span><span class="o">\)</span>
<span class="w">        </span><span class="n">Choose</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="n">space</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">G</span><span class="o">}^{</span><span class="n">k</span><span class="o">}\)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="o">\(\</span><span class="n">Gamma</span><span class="o">\)</span>
<span class="w">        </span><span class="n">Choose</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="o">\(</span><span class="n">g</span><span class="o">\)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">G</span><span class="o">}^{</span><span class="n">k</span><span class="o">}\)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="o">\(\</span><span class="n">gamma_</span><span class="o">{</span><span class="n">k</span><span class="o">}\)</span>
<span class="w">        </span><span class="n">Choose</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="n">parameters</span><span class="w"> </span><span class="o">\(\</span><span class="n">theta</span><span class="o">\)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">explore</span><span class="w"> </span><span class="o">\(</span><span class="n">g</span><span class="o">\)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">\(</span><span class="n">c</span><span class="o">\)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="o">\(\</span><span class="n">Pi_</span><span class="o">{\</span><span class="n">epsilon</span><span class="o">}\)</span>
<span class="w">        </span><span class="n">Execute</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">roll</span><span class="o">-</span><span class="n">out</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="o">\(\</span><span class="n">pi_</span><span class="o">{\</span><span class="n">theta</span><span class="o">}\),</span><span class="w"> </span><span class="n">observe</span><span class="w"> </span><span class="n">trajectory</span><span class="w"> </span><span class="o">\(\</span><span class="n">tau</span><span class="o">\)</span>
<span class="w">        </span><span class="n">Compute</span><span class="w"> </span><span class="n">outcome</span><span class="w"> </span><span class="o">\(</span><span class="n">o_</span><span class="o">{\</span><span class="n">tau</span><span class="o">}\)</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">trajectory</span><span class="w"> </span><span class="o">\(\</span><span class="n">tau</span><span class="o">\)</span>
<span class="w">        </span><span class="o">\(\</span><span class="n">triangleright</span><span class="o">\)</span><span class="w"> </span><span class="n">From</span><span class="w"> </span><span class="n">now</span><span class="w"> </span><span class="n">on</span><span class="o">,</span><span class="w"> </span><span class="o">\(</span><span class="n">f_</span><span class="o">{</span><span class="n">g</span><span class="o">^{\</span><span class="n">prime</span><span class="o">}}(\</span><span class="n">tau</span><span class="o">)\)</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">computed</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">estimate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">fitness</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">experiment</span><span class="w"> </span><span class="o">\(\</span><span class="n">tau</span><span class="o">\)</span><span class="w"> </span><span class="k">for</span>
<span class="w">            </span><span class="n">achieving</span><span class="w"> </span><span class="n">any</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="o">\(</span><span class="n">g</span><span class="o">^{\</span><span class="n">prime</span><span class="o">}</span><span class="w"> </span><span class="o">\</span><span class="k">in</span><span class="w"> </span><span class="o">\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">G</span><span class="o">}\)</span>
<span class="w">        </span><span class="n">Compute</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">fitness</span><span class="w"> </span><span class="o">\(</span><span class="n">f</span><span class="o">=</span><span class="n">f_</span><span class="o">{</span><span class="n">g</span><span class="o">}(\</span><span class="n">tau</span><span class="o">)\)</span><span class="w"> </span><span class="n">associated</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="o">\(</span><span class="n">g</span><span class="o">\)</span>
<span class="w">        </span><span class="n">Compute</span><span class="w"> </span><span class="kd">intrinsic</span><span class="w"> </span><span class="n">reward</span><span class="w"> </span><span class="o">\(</span><span class="n">r_</span><span class="o">{</span><span class="n">i</span><span class="o">}=</span><span class="n">I</span><span class="w"> </span><span class="n">R</span><span class="o">\</span><span class="n">left</span><span class="o">(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">E</span><span class="o">},</span><span class="w"> </span><span class="n">c</span><span class="o">,</span><span class="w"> </span><span class="n">g</span><span class="o">,</span><span class="w"> </span><span class="o">\</span><span class="n">theta</span><span class="o">,</span><span class="w"> </span><span class="n">o_</span><span class="o">{\</span><span class="n">tau</span><span class="o">},</span><span class="w"> </span><span class="n">f</span><span class="o">\</span><span class="n">right</span><span class="o">)\)</span><span class="w"> </span><span class="n">associated</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="o">\(</span><span class="n">g</span><span class="o">\)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">\(</span><span class="n">c</span><span class="o">\)</span>
<span class="w">        </span><span class="n">Update</span><span class="w"> </span><span class="n">exploration</span><span class="w"> </span><span class="n">meta</span><span class="o">-</span><span class="n">policy</span><span class="w"> </span><span class="o">\(\</span><span class="n">Pi_</span><span class="o">{\</span><span class="n">epsilon</span><span class="o">}\)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="o">\(\</span><span class="n">left</span><span class="o">(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">E</span><span class="o">},</span><span class="w"> </span><span class="n">c</span><span class="o">,</span><span class="w"> </span><span class="o">\</span><span class="n">theta</span><span class="o">,</span><span class="w"> </span><span class="o">\</span><span class="n">tau</span><span class="o">,</span><span class="w"> </span><span class="n">o_</span><span class="o">{\</span><span class="n">tau</span><span class="o">}\</span><span class="n">right</span><span class="o">)</span><span class="w"> </span><span class="o">\</span><span class="n">quad</span><span class="w"> </span><span class="o">\</span><span class="n">triangleright</span><span class="o">\)</span><span class="w"> </span><span class="n">e</span><span class="o">.</span><span class="na">g</span><span class="o">.</span><span class="w"> </span><span class="n">fast</span><span class="w"> </span><span class="n">incr</span><span class="o">.</span><span class="w"> </span><span class="n">algo</span><span class="o">.</span>
<span class="w">        </span><span class="n">Update</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="o">\(\</span><span class="n">gamma_</span><span class="o">{</span><span class="n">k</span><span class="o">}\)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="o">\(\</span><span class="n">left</span><span class="o">(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">E</span><span class="o">},</span><span class="w"> </span><span class="n">c</span><span class="o">,</span><span class="w"> </span><span class="n">g</span><span class="o">,</span><span class="w"> </span><span class="n">o_</span><span class="o">{\</span><span class="n">tau</span><span class="o">},</span><span class="w"> </span><span class="n">f</span><span class="o">,</span><span class="w"> </span><span class="n">r_</span><span class="o">{</span><span class="n">i</span><span class="o">}\</span><span class="n">right</span><span class="o">)\)</span>
<span class="w">        </span><span class="n">Update</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="n">space</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="o">\(\</span><span class="n">Gamma</span><span class="o">\)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="o">\(\</span><span class="n">left</span><span class="o">(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">E</span><span class="o">},</span><span class="w"> </span><span class="n">c</span><span class="o">,</span><span class="w"> </span><span class="n">k</span><span class="o">,</span><span class="w"> </span><span class="n">g</span><span class="o">,</span><span class="w"> </span><span class="n">o_</span><span class="o">{\</span><span class="n">tau</span><span class="o">},</span><span class="w"> </span><span class="n">f</span><span class="o">,</span><span class="w"> </span><span class="n">r_</span><span class="o">{</span><span class="n">i</span><span class="o">}\</span><span class="n">right</span><span class="o">)\)</span>
<span class="w">        </span><span class="n">Update</span><span class="w"> </span><span class="n">knowledge</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">E</span><span class="o">}\)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="o">\(\</span><span class="n">left</span><span class="o">(</span><span class="n">c</span><span class="o">,</span><span class="w"> </span><span class="n">g</span><span class="o">,</span><span class="w"> </span><span class="o">\</span><span class="n">theta</span><span class="o">,</span><span class="w"> </span><span class="o">\</span><span class="n">tau</span><span class="o">,</span><span class="w"> </span><span class="n">o_</span><span class="o">{\</span><span class="n">tau</span><span class="o">},</span><span class="w"> </span><span class="n">f</span><span class="o">,</span><span class="w"> </span><span class="n">r_</span><span class="o">{</span><span class="n">i</span><span class="o">}\</span><span class="n">right</span><span class="o">)\)</span>
<span class="w">    </span><span class="n">loop</span><span class="w"> </span><span class="o">\(\</span><span class="n">triangleright</span><span class="o">\)</span><span class="w"> </span><span class="n">Exploitation</span><span class="w"> </span><span class="n">loop</span>
<span class="w">        </span><span class="n">Update</span><span class="w"> </span><span class="n">meta</span><span class="o">-</span><span class="n">policy</span><span class="w"> </span><span class="o">\(\</span><span class="n">Pi</span><span class="o">\)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="o">\(\</span><span class="n">mathcal</span><span class="o">{</span><span class="n">E</span><span class="o">}</span><span class="w"> </span><span class="o">\</span><span class="n">quad</span><span class="w"> </span><span class="o">\</span><span class="n">triangleright</span><span class="o">\)</span><span class="w"> </span><span class="n">e</span><span class="o">.</span><span class="na">g</span><span class="o">.</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">DNN</span><span class="o">,</span><span class="w"> </span><span class="n">SVM</span><span class="o">,</span><span class="w"> </span><span class="n">GMM</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="o">\(\</span><span class="n">Pi</span><span class="o">\)</span>
</code></pre></div>

<p>With this setting, goal sampling is hierarchical in the sense that the agent first chooses a goal space $\mathcal{G}^{k}$ to explore with a goal space policy $\Gamma$ and then a particular goal $g \in \mathcal{G}^{k}$ with the corresponding goal policy $\gamma_{k}$. Those two levels of choice can make use of self-computed intrinsic rewards $r_{i}$ (see Sec. 2.4).</p>
<p>Given an outcome $o_{\tau}$, the fitness $f_{g}(\tau)$ can thus be computed by the agent for all goals $g \in \mathcal{G}$ and at any time after the experiment $\tau$. For instance, if while exploring the goal of moving object A to the left, object B moved to the right, that outcome can be taken into account later when the goal is to move object B.</p>
<h3>3.3 Object-Centered Temporal Modularity: Stepping-Stone Preserving Mutations</h3>
<p>When targeting a new goal $g$, the internal model (a memory-based nearest neighbor search in our experiments) infers the best policy $\pi_{\theta}$ to reach the goal $g$. The exploration meta-policy $\Pi_{\epsilon}$ then performs a mutation on $\theta$ in order to explore new policies. This mutation step can be seen as analogous to existing approaches injecting noise in parametric RL policies to foster exploration (Fortunato et al., 2018; Plappert et al., 2018). The mutation operator could just add a random noise on the parameters $\theta$, however, those parameters do not all have the same influence on the execution of the policy, in particular with respect to time. In our implementations, the parameters are sequenced in time, with some parameters influencing more the beginning of the policy roll-out and some more the end of the trajectory. However, in the context of tool use, the reaching or grasping of a tool is necessary for executing a subsequent action on an object. A random mutation of policy parameters, irrespective of the moment when the tool is grasped, can result in an action where the agent do not grasp the tool and thus cannot explore the corresponding object.</p>
<p>The Stepping-Stone Preserving Mutation operator (SSPMutation) analyzes the trajectory of the target object while the previous motor policy $\pi_{\theta}$ was run, to find the moment when the object</p>
<p>started to move. The operator does not change the variables of $\theta$ concerning the movement before the object moved and mutates the variables of $\theta$ concerning the movement after the object moved (see an example mutation in Fig. 15). When the goal of the agent is to move the tool and it already succeeded to move the tool in the past with policy $\pi_{\theta}$, then the application of this mutation operator changes the behavior of the agent only when the tool start to move, which makes the agent explore with the tool once grasped and avoid missing the tool. Similarly, when the goal of the agent is to move a toy controlled by a tool, the mutation changes the behavior only when the toy starts to move, which makes the agent grasp the tool and reach the toy before exploring new actions, so that the agent do not miss the tool nor the toy. The idea of this stepping-stone preserving operator is similar to the Go-Explore approach (Ecoffet et al., 2021).</p>
<h1>3.4 Active Model Babbling (AMB) and Random Model Babbling (RMB)</h1>
<p>The Modular Population-Based IMAGEP architecture gives a high-level description of the learning agent with a population-based policy and an object-centered modularity in goals and mutations. Each component of this architecture may be instantiated in various ways. For instance, in the main loop of Architecture 2, many aspects are not constrained such as how the goal is chosen, how the parameters $\theta$ are computed, how the policies $\pi_{\theta}$ are implemented, how the intrinsic rewards are defined.</p>
<p>An important contribution of the present work is to design and showcase the effectiveness of one particular instantiation of modular population-based IMGEPs, called Active Model Babbling (AMB, see fig. 2), which uses a learning-progress based mechanism for sampling goal spaces (see section 4.2.3 for examples of implementations) as well as the stepping-stone preserving mutations mechanism described in section 3.3 (see section 4.2.2 for implementation details).</p>
<p>In addition to AMB, we also present an alternative variant, called Random Model Babbling (RMB), which also uses the stepping-stone preserving mutations, but implements a simpler goal space policy: instead of using learning progress estimates, goal spaces are selected randomly throughout training. Although apparently simplistic, this approach can perform surprisingly well, especially when all goal spaces are relevant, i.e. when there are no distractors (see figure 20).</p>
<p>In Section 4.2, we detail implementations of these types of Modular Population-Based IMAGEP architectures, as well as other baselines.</p>
<h2>4. Experiments</h2>
<p>In this section, we evaluate the Modular Population-Based IMAGEP architecture by designing several algorithmic implementations and several environments suitable for curriculum learning, where the exploration of a task brings information to later solve other tasks. In particular, we study environments where agents discover objects that can be used as tools to move other objects. A good exploration of a tool and of its functioning will yield a better exploration of the objects on which this tool can act. Those tasks provide the opportunity for an intrinsically motivated agent to build on the skills it has learned to explore and learn new skills on its own.</p>
<p>Here, we first describe three tool-use learning environments and we detail our implementations of IMAGEP and of several control conditions. Then, we study the behavior of the different agents in the different environments depending on the learning architecture. We investigate in particular the benefits of a modular representation of the sensory feedback with goals based on objects, and how the exploration mutations can take into account the movement of the goal object. We further examine how and in which conditions the intrinsic motivation component of the IMAGEP architecture improves the learning of skills that can be reused, such as using a tool to move an object.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: 2D Simulated Tool-Use Environment. A simulated robotic arm with a gripper can grab sticks and move toys. The gripper has to close near the handle of a stick to grab it. One magnetic toy and one Velcro toy are reachable with their corresponding stick. Other toys cannot be moved (static or too far away). The cat and the dog are distractors: they move randomly, independently of the arm.</p>
<h1>4.1 Tool-Use Environments</h1>
<p>We design three tool-use environments. The first one is a 2 D simulated robotic arm with 3 joints and a gripper that can grab sticks and move toys. It is a simple environment with no physics and only 2D geometric shapes so very fast to execute. The second environment is a Minecraft scene where an agent is able to move, grab and use tools such as a pickaxe to break blocks. The third one is a real robotic setup with a Torso robot moving its arm that can reach joysticks controlling a toy robot. This setup has complex high-dimensional motor and sensory spaces with noise both in the robot physical arm and in the interaction between objects such as its hand and the joysticks. It is a high-dimensional and noisy environment with a similar stepping-stone structure as the robotic environments but with a completely different sensorimotor setup. The code of the different environments and experiments is available on GitHub ${ }^{1}$.</p>
<h3>4.1.1 2D Simulated Tool-Use Environment</h3>
<p>In the 2D Simulated Environment (see Fig. 3), the learning agent controls a robotic arm with a gripper, that can grab one of two sticks, one with a magnet at the end and one with Velcro, that can themselves be used to move several magnets or Velcro toys. Some other objects cannot be moved, they are called static distractors, and finally a simulated cat and dog are randomly moving in the scene, they are random distractors.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>The 2D robotic arm has 3 joints that can rotate from $-\pi$ rad to $\pi$ rad. The length of the 3 segments of the arm are $0.5,0.3$ and 0.2 so the length of the arm is 1 unit. The starting position of the arm is vertical with joints at position 0 rad and its base is fixed at position $(0,0)$. The gripper $g r$ has 2 possible positions: open $(g r \geqslant 0)$ and closed $(g r&lt;0)$. The robotic arm has 4 degrees of freedom represented by a vector in $[-1,1]^{4}$.</p>
<p>Two sticks of length 0.5 can be grasped by the handle side (orange side) in order to catch an out-of-reach object. The magnetic stick can catch magnetic objects (in blue), and the other stick has a Velcro tape to catch Velcro objects (in green). If the gripper closes near the handle of one stick, this stick is considered grasped and follows the gripper's position and the orientation of the arm's last segment until the gripper opens. If the other side of a stick reaches a matching object (magnetic or Velcro), the object then follows the stick. There are three magnetic objects and three Velcro objects, but only one of each type is reachable with its stick. A simulated cat and dog are following a random walk, they have no interaction with the arm nor with other object. Finally, four static black squares have also no interaction with other objects. The arm, tools and other objects are reset to their initial state at the end of each iteration of 50 steps.</p>
<p>The agent receives a sensory feedback representing the result of its actions. This feedback (or outcome) is either composed of the position of each object at 5 time points during the 50 steps trajectory, or just the end state of each object, depending on the experiments. First, the hand is represented by its $X$ and $Y$ position and the aperture of the gripper ( 1 or -1 ). The sticks are represented by the $X$ and $Y$ position of their tip. Similarly, each other object is represented by their $X$ and $Y$ positions. Each of the 15 objects defines a sensory space $S_{i}$ ( 10 of those objects are uncontrollable distractors). The total sensory space $S$ has either 155 dimensions if trajectories are represented, or 31 dimensions if only the end state of each object is represented.</p>
<h1>4.1.2 Minecraft Mountain Cart</h1>
<p>The Minecraft Mountain Cart (MMC) extends the famous Mountain Car control benchmark in a 3D environment with a multi-goal setting (see Fig. 4).</p>
<p>In this episodic task, the agent starts on the left of the rectangular arena and is given ten seconds ( 40 steps) to act on the environment using 2 continuous commands: move and strafe, both using values in $[-1 ; 1] . m o v e(1)$ moves the agent forward at full speed, move(-0.1) moves the agent slowly backward, etc. Similarly strafe(1) moves the agent left at full speed and strafe(-0.1) moves it slowly to the right. Additionally, a third binary action allows the agent to use the currently handled tool.</p>
<p>The first challenge of this environment is to learn how to navigate within the arena's boundaries without falling in water holes (from which the agent cannot get out). Proper navigation might lead the agent to discover one of the two tools of the environment: a shovel and a pickaxe. The former is of no use but the latter enables to break diamond blocks located further ahead in the arena. A last possible interaction is for the agent to get close enough to the cart to move it along its railroad. If given enough speed, the cart is able to climb the left or right slope. The height and width of these slopes were made in such a way that an agent simply hitting the cart at full speed will not provide enough inertia for the cart to climb the slope. The agent must at least partially support the cart along the track to propel it fast enough to fully climb the slope.</p>
<p>The outcome of an episode is a vector composed of the end position of the agent (2D), shovel (2D), pickaxe (2D), cart (1D) and 3 distractors (2D each) positions along with a binary vector (5D) encoding the 5 diamond blocks' states ( 3 objects out of 8 are uncontrollable distractors).</p>
<p>This environment is interesting to study modular IMGEP approaches since it is composed of a set of linked tasks of increasing complexity. Exploring how to navigate will help to discover the tools and, eventually, will allow to break blocks and move the cart.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Minecraft Mountain Cart Environment. If the agent manages to avoid falling into water holes it may retrieve and use a pickaxe to break diamond blocks and access the cart. A shovel is also located in the arena and serves as a controllable distractor.</p>
<h1>4.1.3 Robotic Tool-Use Environment</h1>
<p>In order to benchmark different learning algorithms in a realistic robotic environment with highdimensional action and outcome spaces, we designed a real robotic setup composed of a humanoid arm in front of joysticks that can be used as tools to act on other objects (see Fig. 5). We recorded a video of an early version of the experimental setup ${ }^{2}$.</p>
<p>A Poppy Torso robot (the learning agent) is mounted in front of two joysticks and explores with its left arm. A Poppy Ergo robot (seen as a robotic toy) is controlled by the right joystick and can push a ball that controls some lights and sounds. Poppy is a robust and accessible open-source 3D printed robotic platform (Lapeyre et al., 2014).</p>
<p>The left arm has 4 joints, with a hook at the tip of the arm. A trajectory of the arm is here generated by radial basis functions with 5 parameters on each of the 4 degrees of freedom ( 20 parameters in total).</p>
<p>Two analogical joysticks (Ultrastick 360) can be reached by the left arm and moved in any direction. The right joystick controls the Poppy Ergo robotic toy, and the left joystick do not control any object. The Poppy Ergo robot has 6 motors, and moves with hardwired synergies that allow control of rotational speed and radial extension.</p>
<p>A tennis ball is freely moving in the blue arena which is slightly sloped so that the ball comes close to the center at the end of a movement. The speed of the ball controls (above a threshold) the intensity of the light of a LED circle around the arena. Finally, when the ball touches the border of the arena, a sound is produced and varied in pitch depending on ball position.</p>
<p>Several other objects are included in the environment, with which the agent cannot interact. Two Ergo robots (2D objects) are moving randomly, independently of the agent. Six objects are static: the right hand (3D) of the robot that is disabled in this experiment, the camera recording the ball</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Robotic Tool-Use Environment. Left: a Poppy Torso robot (the learning agent) is mounted in front of two joysticks that can be used as tools to act on other objects: a Poppy Ergo robotic toy and a ball that can produce light and sound. Right: 6 copies of this setup are running in parallel to gather more data. Several Ergo robots are placed between robots: they act as distractors that move randomly, independently of the agents.
trajectory (3D), the blue circular arena (2D), an out-of-reach yellow toy (2D), the red button also out-of-reach (2D) and the lamp (2D). All distractor objects are reset after each roll-out.</p>
<p>The context $c$ of this environment represents the current configuration of objects in the scene. In practice, since only the Ergo and ball are not reset after each roll-out, this amounts to measuring the rotation angle of the Ergo and of the ball around the center of the arena.</p>
<p>The agent is given a perceptual system providing sensory feedback that represents the trajectories of all objects in the scene. First, the 3D trajectory of the hand is computed through a forward model of the arm as its $x, y$ and $z$ position. The 2D states of each joystick and of the Ergo are read by sensors, and the position of the ball retrieved through the camera. The states of the 1D intensity of the light and the 1D pitch of the sound are computed from the ball position and speed. Each of the 15 objects defines a sensory space $S_{i}$ representing its trajectory ( 8 of those objects are uncontrollable distractors). The total sensory space $S$ has 310 dimensions.</p>
<h1>4.2 Implementation of the Modular Population-Based IMAGEP Architecture</h1>
<p>In the following subsections, we detail our implementations of the algorithmic parts of the modular population-based IMAGEP architecture (see architecture 2).</p>
<h3>4.2.1 Motor Policy $\pi_{\theta}$</h3>
<p>In the 2D Simulated environment and the Robotic environment, we implement the motor policies with Radial Basis Functions (RBF). We define 5 Gaussian basis functions with the same shape ( $\sigma=5$ for a 50 steps trajectory in the 2 D environment and $\sigma=3$ for 30 steps in the Robotic environment) and with equally spaced centers (see Fig. 6). The movement of each joint is the result of a weighted sum of the product of 5 parameters and the 5 basis. The total vector $\theta$ has 20 parameters, in both the 2D Simulated and the Robotic environment. In the 2D environment, the fourth joint is a gripper that is considered open if its angle is positive and closed otherwise.</p>
<p>In the Minecraft Mountain Cart environment, trajectories are sampled in a closed-loop fashion using neural networks. The observation vector has the same structure as the outcome vector: it provides the current positions of all objects normalized in $[-1 ; 1]$ (18D). Each neural network is composed of one hidden layer of 64 Relu units and a 3D output with tanh activation functions. The</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Implementation of motor policies $\pi_{\theta}$ through Radial Basis Functions. (a) 5 Gaussian bases with different centers but same shape. (b) the movement of each joint is the result of a weighted sum of the product of 5 parameters and the 5 basis. The total vector $\theta$ has 20 parameters, in both the 2D Simulated and the Robotic environment.</p>
<p>1411 policy parameters are initialized using the initialization scheme of He et al. (2015). Note that in our experiments, neural networks are not trained through backpropagation: as for RBF policies, their parameters are mutated. Our mutation-based approach is close to neuroevolution approaches (Stanley et al., 2019).</p>
<h1>4.2.2 Stepping-Stone Preserving Mutations</h1>
<p>The Stepping-Stone Preserving Mutation operator (SSPMutation) does not change the variables of $\theta$ concerning the movement before the object moved and modifies the variables of $\theta$ concerning the movement after the object moved. SSPMutation adds a Gaussian noise around those values of $\theta$ in the 2D simulated environment $(\sigma=0.05)$ and in Minecraft Mountain Cart $(\sigma=0.3)$, or adds the Gaussian noise around the previous motor positions (in the robotic environment with joysticks). In the experimental section we compare it to the FullMutation operator that adds a Gaussian noise to $\theta$ irrespective of the moment when the target object moved.</p>
<h3>4.2.3 Goal Space Policy $\Gamma$</h3>
<p>The agent estimates its learning progress globally in each goal space (or for each model learned). At each iteration, the context $c$ is observed, a goal space $k$ is chosen by $\Gamma$ and a random goal $g$ is sampled by $\gamma_{k}$ in $\mathcal{G}^{k}$ (corresponding to a fitness function $f_{g}$ ). Then, in $80 \%$ of the iterations, the agent uses $\Pi_{c}(\boldsymbol{\theta} \mid g, c)$ to generate with exploration a policy $\theta$ and does not update its progress estimation. In the other $20 \%$, it uses $\Pi$, without exploration, to generate $\theta$ and updates its learning progress estimation in $\mathcal{G}^{k}$, with the estimated progress in reaching $g$. To estimate the learning progress $r_{i}$ made to reach the current goal $g$, the agent compares the outcome $o_{\tau}$ with the outcome $o_{\tau}^{\prime}$ obtained for the previous context and goal $\left(g^{\prime}, c^{\prime}\right)$ most similar (Euclidean distance) to $(g, c)$ : $r_{i}=f_{g}(\tau)-f_{g}\left(\tau^{\prime}\right)$. Finally, $\Gamma$ implements a non-stationary bandit algorithm to sample goal spaces. The bandit keeps track of a running average $r_{i}^{k}$ of the intrinsic rewards $r_{i}$ associated to the current goal space $\mathcal{G}^{k}$. With probability $20 \%$, it samples a random space $\mathcal{G}^{k}$, and with probability $80 \%$, the probability to sample $\mathcal{G}^{k}$ is proportional to $r_{i}^{k}$ in the 2D Simulated and Minecraft environments, or $\exp \left(\frac{r_{i}^{k}}{\sum_{k} r_{i}^{k}}\right)$ if $r_{i}^{k}&gt;0$ and 0 otherwise, in the Robotic environment.</p>
<h1>4.2.4 Control Conditions</h1>
<p>We design several control conditions.In the Random Model Babbling (RMB) condition, the choice of goal space is random: $\Gamma(\mathbf{k} \mid \mathbf{c})$, and $\gamma_{k}(\mathbf{g} \mid \mathbf{c})$ for each $k$ are always uniform distributions. Agents in the Single Goal Space (SGS) condition always choose the same goal space, of high interest to the engineer: the magnet toy in the 2D Simulated environment, and the ball in the robotic environment. The Fixed Curriculum (FC) condition defines $\Gamma$ as a curriculum sequence engineered by hand: the agents explore objects in a sequence from the easiest to discover to the most complex object while ignoring distractors. The conditions SGS and FC are thus extrinsically motivated controls. We define the Flat Random Goal Babbling (FRGB) condition with a single outcome/goal space containing all the variables of all objects, to compare modular and non-modular representations of the environment. The agents in this condition choose random goals in this space, and use the FullMutation operator. Finally, agents in the Random condition always choose random motor policies $\theta$.</p>
<h3>4.3 Results</h3>
<p>In this section we show the results of several experiments with the three environments and the different learning conditions. We first study in details the Active Model Babbling (AMB) learning algorithm, a modular implementation of the IMGEP architecture. Then, in order to understand the contribution of the different components of this learning algorithm, we compare it to several controls (or ablations): without a modular representation of goals, without the goal sampling based on learning progress, or without the stepping-stone preserving mutation operator. In those experiments, goals are sampled in spaces representing the sensory feedback from the environment. We thus compare several possible encodings of the feedback: with the trajectory of each object or with only the end point of the trajectories. We included distractors that cannot be controlled by the learning agent in the three tool-use environments. We also test the learning conditions with and without distractors to evaluate their robustness to distractors.</p>
<h3>4.3.1 Exploration Measure and Summary Results</h3>
<p>Table 1 shows a summary of the exploration results at the end of the runs, in all conditions in all spaces of all environments, We give the 25,50 and 75 percentiles of the exploration results of all seeds. Exploration measures the percentage of reached cells in a discretization of each goal space. The best condition in each space is highlighted in bold, based on Welch's t-tests (with threshold $p&lt;0.05$ ): if several conditions are not significantly different, they are all highlighted. In the 2 D Simulated environment, there are 100 seeds for each condition, and the exploration measures the number of cells reached in a discretization of the 2D space of the end position of each object with 100 bins on each dimension. In the Minecraft environment, there are 20 runs for conditions Random, SGS, FRGB, FC and 42 for AMB and RMB. The exploration metric for the agent, pickaxe and shovel spaces is the number of reached cells in a discretization of the 2D space in 450 bins ( 15 on the x axis, 30 on the y axis). The same measure is used for the block space, which is discrete with 32 possible combinations. For the cart space we measure exploration as the number of different outcomes reached. In the Robotic environment, there are 6 runs with different seeds for condition SGS, 8 for FRGB, 16 for RMB, 23 for AMB, 12 for FC and 6 for Random, and the exploration also measures the number of cells reached in a discretization of the space of the end position of each object with 1000 bins in 1D, 100 bins on each dimension in 2D, and 20 bins in 3D.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Examples of exploration map of one IMGEP agent in each environment. (a) in the 2D Simulated Environment, we plot the position of the reachable magnet toy at the end of each iteration with a blue point, and the Velcro toy in green. (b) in Minecraft Mountain Cart we plot the end position of the agent, the agent with pickaxe, the agent with shovel, and the cart. (c) in the Robotic environment, the position of the ball is plotted when it moved in the arena.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Env, Space</th>
<th style="text-align: center;">Condition</th>
<th style="text-align: center;">Rdm</th>
<th style="text-align: center;">SGS</th>
<th style="text-align: center;">Flat</th>
<th style="text-align: center;">RMB</th>
<th style="text-align: center;">AMB</th>
<th style="text-align: center;">FC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2D Simulated Environment</td>
<td style="text-align: center;">Magnet Tool</td>
<td style="text-align: center;">0,0,0</td>
<td style="text-align: center;">0,0,0</td>
<td style="text-align: center;">$8.0,11,13$</td>
<td style="text-align: center;">$33,36,39$</td>
<td style="text-align: center;">57,61,65</td>
<td style="text-align: center;">61,67,70</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Magnet Toy</td>
<td style="text-align: center;">0,0,0</td>
<td style="text-align: center;">0,0,0</td>
<td style="text-align: center;">0,0,0</td>
<td style="text-align: center;">$0,0,5.0$</td>
<td style="text-align: center;">0,3.0,16</td>
<td style="text-align: center;">0,3.0,19</td>
</tr>
<tr>
<td style="text-align: center;">Minecraft <br> Mountain <br> Cart</td>
<td style="text-align: center;">Agent Pos.</td>
<td style="text-align: center;">28,29,30</td>
<td style="text-align: center;">29,29,30</td>
<td style="text-align: center;">$34,36,40$</td>
<td style="text-align: center;">$48,50,54$</td>
<td style="text-align: center;">$55,58,61$</td>
<td style="text-align: center;">59,63,67</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Shovel</td>
<td style="text-align: center;">$5,5,6$</td>
<td style="text-align: center;">$5,6,7$</td>
<td style="text-align: center;">$8,11,13$</td>
<td style="text-align: center;">$25,27,30$</td>
<td style="text-align: center;">$32,34,37$</td>
<td style="text-align: center;">34,37,42</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pickaxe</td>
<td style="text-align: center;">$6,6,7$</td>
<td style="text-align: center;">$6,7,8$</td>
<td style="text-align: center;">$11,15,19$</td>
<td style="text-align: center;">$33,35,39$</td>
<td style="text-align: center;">$41,45,48$</td>
<td style="text-align: center;">43,51,61</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Blocks</td>
<td style="text-align: center;">$3,3,3$</td>
<td style="text-align: center;">$3,3,3$</td>
<td style="text-align: center;">$3,11,19$</td>
<td style="text-align: center;">$69,77,84$</td>
<td style="text-align: center;">73,84,93</td>
<td style="text-align: center;">100,100,100</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cart</td>
<td style="text-align: center;">0,0,0</td>
<td style="text-align: center;">0,0,0</td>
<td style="text-align: center;">0,0,1</td>
<td style="text-align: center;">$5,162,409$</td>
<td style="text-align: center;">$56,360,886$</td>
<td style="text-align: center;">386,787,1207</td>
</tr>
<tr>
<td style="text-align: center;">Robotic <br> Environment</td>
<td style="text-align: center;">Hand</td>
<td style="text-align: center;">24,24,25</td>
<td style="text-align: center;">18,19,20</td>
<td style="text-align: center;">20,21,22</td>
<td style="text-align: center;">22,24,25</td>
<td style="text-align: center;">22,23,24</td>
<td style="text-align: center;">21,22,23</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">L. Joystick</td>
<td style="text-align: center;">$4.2,4.7,5.9$</td>
<td style="text-align: center;">$1.9,3.3,4.6$</td>
<td style="text-align: center;">$0.1,0.1,0.3$</td>
<td style="text-align: center;">$15,18,19$</td>
<td style="text-align: center;">20,22,26</td>
<td style="text-align: center;">23,26,29</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">R. Joystick</td>
<td style="text-align: center;">$0.6,0.9,1.0$</td>
<td style="text-align: center;">$0.3,0.4,0.5$</td>
<td style="text-align: center;">0,0,0</td>
<td style="text-align: center;">$10,11,13$</td>
<td style="text-align: center;">16,18,22</td>
<td style="text-align: center;">$15,17,18$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ergo</td>
<td style="text-align: center;">$0.2,0.3,0.4$</td>
<td style="text-align: center;">$0.1,0.1,0.2$</td>
<td style="text-align: center;">0,0,0</td>
<td style="text-align: center;">$1.2,1.5,1.7$</td>
<td style="text-align: center;">$1.5,1.7,1.8$</td>
<td style="text-align: center;">$1.7,1.7,1.9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ball</td>
<td style="text-align: center;">$0,0,0.1$</td>
<td style="text-align: center;">0,0,0</td>
<td style="text-align: center;">0,0,0</td>
<td style="text-align: center;">$0.8,1.0,1.0$</td>
<td style="text-align: center;">$0.9,1.1,1.2$</td>
<td style="text-align: center;">$0.9,0.9,1.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Light</td>
<td style="text-align: center;">$0.1,0.1,0.1$</td>
<td style="text-align: center;">$0.1,0.2,0.2$</td>
<td style="text-align: center;">$0.1,0.1,0.1$</td>
<td style="text-align: center;">$0.8,1.8,3.0$</td>
<td style="text-align: center;">2.0,3.6,4.9</td>
<td style="text-align: center;">$1.8,2.2,3.7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sound</td>
<td style="text-align: center;">$0.1,0.1,0.1$</td>
<td style="text-align: center;">$0.1,0.1,0.1$</td>
<td style="text-align: center;">$0.1,0.1,0.1$</td>
<td style="text-align: center;">$0.8,1.1,2.6$</td>
<td style="text-align: center;">$1.7,2.8,3.6$</td>
<td style="text-align: center;">$1.2,1.6,2.3$</td>
</tr>
</tbody>
</table>
<p>Table 1: Exploration results in all environments and conditions.</p>
<h1>4.3.2 Intrinsically Motivated Goal Exploration</h1>
<p>Here we study in detail the Active Model Babbling (AMB) learning algorithm. AMB agents encode the sensory feedback about objects with a modular representation: each object is associated with one independent learning module. At each iteration, they first select an object to explore, then a particular goal to reach for this object. They execute a motor policy to reach this goal, and observe</p>
<p>the outcome. The selection of the object to explore is based on a self-estimation of the learning progress made to move each object according to chosen goals. The AMB algorithm is thus a modular implementation of the IMGEP architecture.</p>
<p>Exploration Maps - We first plot examples of exploration results as cumulative exploration maps, one per environment. Those maps show all the positions where one AMB agent succeeded to move objects.</p>
<p>Fig. 7(a) shows the position of the reachable toys of the 2 D simulated environment at the end of each iteration in one trial of intrinsically motivated goal exploration. The reachable area for those two toys is the inside the circle of radius 1.5 and center 0 . We can see that in 100 k iterations, the agent succeeded to transport the toys in many places in this area. The experiments with other seeds are very similar. Fig. 7(b) shows an exploration map of a typical run in Minecraft Mountain Cart after 40 k iterations. As you can see the agent successfully managed to (1) navigate within the arena boundaries, (2) move the pickaxe and shovel, (3) use the pickaxe to break blocks and (4) move the cart located behind these blocks. An example in the robotic environment is shown in Fig. 7(c) where we plot the position of the ball when it moved in the first $10 k$ iterations of the exploration of one agent.</p>
<p>Overall, they show that IMGEP agents discovered how to use the different tools in each environment within the time limit: the sticks to grab the toys in the 2 D simulated environment, the pickaxe to mine blocks to reach the cart in Minecraft Mountain Cart, the joysticks to move the toy and push the ball in the robotic experiment.</p>
<p>Discoveries - In order to understand the tool-use structure of the exploration problem in each environment, we can look in more details how agents succeeded to move objects while exploring other objects. Indeed, to the agents starting to explore, tools are objects like any other object (e.g. the hand, the stick and the ball have the same status). However, if a tool needs to be used to move another object, then this tool will be discovered before that object, so the exploration of this tool is a stepping-stone giving more chances to discover novelty with that object than the exploration of any other object. To quantify these dependencies between objects in our tool-use environments, we show the proportion of movements where an object of interest has been moved depending on the currently explored object.</p>
<p>Concerning the 2D simulated environment, Fig. 8 shows the proportion of the iterations with a goal in a given space that allowed to move (a) the magnet tool, (b) the magnet toy, in 10 runs with different seeds. First, random movements of the arm have almost zero chances to reach the magnet tool or toy. Exploring movements of the hand however have about $1.5 \%$ chances to move the magnet tool, but still almost zero chances to reach the toy. Exploring the magnet tool makes this tool move in about $93 \%$ of the iterations, and makes the toy move in about $0.1 \%$ of movements. Finally, exploring the toy makes the tool and the toy move with a high probability as soon as the toy was discovered. Those results illustrate the stepping-stone structure of this environment, where each object must be well explored in order to discover the next step in complexity (Hand $\rightarrow$ Tool $\rightarrow$ Toy).</p>
<p>In Minecraft Mountain Cart (see Fig. 8(c,d,e)), random exploration with neural networks in this environment is extremely challenging. An agent following random policies has $0.04 \%$ chances to discover the pickaxe, $0.00025 \%$ chances to break a single block and it never managed to move the cart (over 800k episodes). IMGEP agents reach better performances by leveraging the sequential nature of the environment: when exploring the agent space there is around $10 \%$ chances to discover the pickaxe, and exploring the pickaxe space has around $1 \%$ chances to break blocks. Finally, exploring the block space has about $8 \%$ chances to lead an agent to discover the cart.</p>
<p>In the Robotic environment, a similar stepping-stone exploration structure is displayed (see Fig. $8(\mathrm{f}, \mathrm{g}, \mathrm{h})$ ): in order to discover the left joystick, the robots needs to do random movements with its arm, which have about $2.9 \%$ chances to makes the left joystick move, or explore its hand ( $0.2 \%$ chance). To discover the right joystick, the agent has to explore the left joystick, which gives a probability of $3.3 \%$ to reach the right one. To discover the Ergo (the white robotic toy in the center</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Stepping-stone structure of the three environments. In the 2D Simulated environment, we show the proportion of iterations that allowed to (a) move the magnet tool, (b) move the magnet toy, depending on the currently explored goal space (or random movements), for 10 IMGEP agents. The fastest way to discover the tool is to explore the hand and to discover the toy is to explore the tool. In the Minecraft Mountain Cart environment, we show the proportion of iterations that allowed to (c) move the pickaxe, (d) mine diamond blocks, and (e) move the cart, depending on the currently explored goal space (or random movements), for 10 agents with different seeds. Exploring the agent space helps discover the pickaxe, exploring the pickaxe helps discover the blocks, and exploring the blocks helps discover the cart. In the Robotic environment, we show the proportion of iterations that allowed to (f) reach the left joystick, (g) reach the right joystick, and (h) move the Ergo robot, depending on the currently explored goal space (or random movements), averaged for 11 IMGEP agents with different seeds. Exploring random movements or the Hand space helps discover the left joystick, exploring the left joystick helps discover the right one, which helps discover the Ergo toy.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Example of learned skills in the Minecraft Mountain Cart. (a) One AMB agent's trajectory for a single cart goal. (b) Five final cart positions reached by an AMB agent when tasked to reach five different targets. This agent successfully learned to push the cart along the track.
of the blue arena), the exploration of the right joystick gives $23 \%$ chances to move it, whereas the exploration of the Hand, the left joystick or random movements has a very low probability to make it move.</p>
<h1>4.3.3 Learned Skills</h1>
<p>In Minecraft Mountain Cart we performed post-training tests of competence in addition of exploration measures. Using modular approaches allows to easily test competence on specific objects of the environment. Fig. 9(b) shows an example in the cart space for an AMB agent. This agent successfully learned to move the cart close to the 5 queried locations.</p>
<p>For each of the RMB, AMB and FC runs we performed a statistical analysis of competence in the cart and pickaxe spaces using 1000 and 800 uniformly generated goals, respectively. We were also able to test SGS trials for cart competence as this condition has the cart as goal space. A goal is considered reached if the Euclidean distance between the outcome and the goal is lower than 0.05 in the normalized space (in range $[-1,1]$ ) for each object. Since the pickaxe goal space is loosely defined as a rectangular area around the environment's arena, many goals are not reachable. Results are shown in Table 2. SGS agents never managed to move the cart for any of the given goals. AMB appears to be significantly better than RMB on the pickaxe space ( $p&lt;0.01$ on Welch's t-tests). However it is not in the cart space $(p=0.09)$, which might be due to the stochasticity of the environment. FC is not significantly better than AMB on the cart and pickaxe spaces.</p>
<p>Intrinsic Rewards based on Learning Progress - The IMAGEP agents self-evaluate their learning progress to control each object. When they choose a goal for an object, they monitor what is the actual movement given to the object and compare it to the goal. If the distance between the goals</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Pickaxe goals</th>
<th style="text-align: center;">Cart goals</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FC</td>
<td style="text-align: center;">$39,49,55$</td>
<td style="text-align: center;">$12,17,25$</td>
</tr>
<tr>
<td style="text-align: left;">AMB</td>
<td style="text-align: center;">$41,45,49$</td>
<td style="text-align: center;">$8,11,18$</td>
</tr>
<tr>
<td style="text-align: left;">RMB</td>
<td style="text-align: center;">$37,40,43$</td>
<td style="text-align: center;">$6,9,15$</td>
</tr>
<tr>
<td style="text-align: left;">SGS</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$0,0,0$</td>
</tr>
</tbody>
</table>
<p>Table 2: Competence results in Minecraft Mountain Cart. We give the 25, 50 and 75 percentiles of the competence results of all seeds.
and the actual reached movements decrease over time on average, this tells the agents it is making progress to control this object. This signal is used as an intrinsic reward signal that the agent will seek to maximize by choosing to explore objects that yield a high learning progress. We can analyze this signal to understand at which point the agent perceived progress to control each object and how its exploration behavior changed over time.</p>
<p>Fig. 10 (top) shows the intrinsic rewards of two agents (different seeds) to explore each object in the 2 D simulated environment, computed by the agents as the average of intrinsic rewards based on learning progress to move each object. We can see that the intrinsic reward of the hand increases first as it is the easiest object to move. Then, when the sticks are discovered, the agents start to make progress to move them in many directions. Similarly, while exploring the sticks, they discover the reachable toys, so they start making progress in moving those toys. However, the static objects can't be moved so their learning progress is strictly zero, and the objects moving randomly independently of the agent (cat and dog) have a very low progress.</p>
<p>Fig. 10 (middle) shows the intrinsic reward of two agents in the Minecraft Mountain Cart environment. Both agents first explore the simpler agent space and then quickly improves on the shovel and pickaxe spaces. Exploring the pickaxe space leads to discover how to progress in the block space. Finally, after some progress in the block space, the cart is discovered after 14 k episodes for the first agent (left figure) and 26 k episodes for the other (right figure). The 3 distracting flowers have an interest strictly equal to zero in both runs.</p>
<p>Fig. 10 (bottom) shows the intrinsic reward of two agents in the Robotic environment. The first interesting object is the robot's hand, followed by the left joystick and then the right joystick. The left joystick is the easiest to reach and move so it gets interesting before the right one in most runs, but then they have similar learning progress curves. However, the right joystick can be used as a tool to control other objects, so that one will be touched more often. Then, the agent can discover the Ergo and Ball while exploring the joysticks. Finally, some agents also discover that the ball can be used to make light or sound. Here also, the progress of static objects is zero and the one of random objects is low. Note that, unlike in the 2D simulation and in the Minecraft environment, the intrinsic reward for exploring the robot's hand remains high. This phenomenon is most likely due to the use of full trajectories as goals in the Robotic environment, which creates large goal spaces (compared to when using end-positions as goals). Additionally, the hand space is quite homogeneous: there are many trajectories that are equally learnable, so the hand space remains for a very long time a source of learning progress. For more difficult objects, the space is also large, but the subspace of learnable trajectories is much smaller, thus the decrease in LP can be seen.</p>
<p>Overall, the evolution of those interests show that evaluating the learning progress to move objects allows agents to self-organize a learning curriculum focusing on the objects currently yielding the most progress and to discover stepping stones one after the other.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ol>
<li>Early version of the experimental setup: https://youtu.be/NOLAwD4ZTW0</li>
</ol>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
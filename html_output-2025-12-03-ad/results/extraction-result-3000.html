<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3000 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3000</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3000</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-74.html">extraction-schema-74</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-729fc01274cc26798654a318d1a95e73c61f99a3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/729fc01274cc26798654a318d1a95e73c61f99a3" target="_blank">Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> HELPER is introduced, an embodied agent equipped with an external memory of language-program pairs that parses free-form human-robot dialogue into action programs through retrieval-augmented LLM prompting: relevant memories are retrieved based on the current dialogue, instruction, correction, or VLM description, and used as in-context prompt examples for LLM querying.</p>
                <p><strong>Paper Abstract:</strong> Pre-trained and frozen large language models (LLMs) can effectively map simple scene rearrangement instructions to programs over a robot's visuomotor functions through appropriate few-shot example prompting. To parse open-domain natural language and adapt to a user's idiosyncratic procedures, not known during prompt engineering time, fixed prompts fall short. In this paper, we introduce HELPER, an embodied agent equipped with an external memory of language-program pairs that parses free-form human-robot dialogue into action programs through retrieval-augmented LLM prompting: relevant memories are retrieved based on the current dialogue, instruction, correction, or VLM description, and used as in-context prompt examples for LLM querying. The memory is expanded during deployment to include pairs of user's language and action plans, to assist future inferences and personalize them to the user's language and routines. HELPER sets a new state-of-the-art in the TEACh benchmark in both Execution from Dialog History (EDH) and Trajectory from Dialogue (TfD), with a 1.7x improvement over the previous state-of-the-art for TfD. Our models, code, and video results can be found in our project's website: https://helper-agent-llm.github.io.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3000.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3000.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HELPER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human-instructable Embodied Language Parsing via Evolving Routines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embodied agent that uses a retrieval-augmented external key-value memory of language↔program pairs to prompt frozen LLMs (GPT-4) to generate Python programs over visuomotor primitives, and expands that memory with successful user-specific executions to personalize future behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>HELPER</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>HELPER uses a PLANNER that performs retrieval-augmented prompting of a frozen LLM (gpt-4-0613) to generate Python programs over a fixed API of navigation and manipulation primitives. It maintains an external key-value memory of language keys mapped to programs, a Vision-Language Model (ALIGN) for visually-grounded failure diagnosis, an EXECUTOR that maintains 3D occupancy/semantic maps and verifies preconditions, and a LOCATOR that prompts the LLM for commonsense object search locations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented key-value memory (language → program pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Each memory key is a language context encoded with a frozen LLM text encoder (text-embedding-ada-002). Given current input I, HELPER retrieves the top-K (K=3 in TEACh eval) keys by L2 distance in embedding space and injects the corresponding language–program pairs as in‑context examples into the LLM prompt. Memory is expanded by adding (language instruction, successful execution program) pairs when the user confirms success; failure cases (predicted by a VLM) are also used to retrieve corrective examples for re-planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TEACh (Trajectory-from-Dialogue (TfD) and Execution-from-Dialog-History (EDH))</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Long-horizon household instruction-following tasks in the AI2-THOR simulator where the agent receives natural-language dialogue (commander/follower) and must convert it to sequences of navigation and manipulation actions (or Python programs over primitives) executed from RGB (and estimated depth) observations; tasks require multi-step planning, object search, and recovery from execution failures.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>embodied planning / dialogue-guided instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>TfD validation unseen: Success Rate (SR) = 13.73% (PLW 1.61); Goal-Condition (GC) = 14.17% (PLW 4.56). EDH validation unseen: SR = 17.40% (PLW 2.91); GC = 25.86% (PLW 7.90). HELPER also reports further improvements when eliciting user feedback (e.g., w/ two feedbacks TfD SR = 17.48%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Ablation w/o memory-augmented prompting (fixed prompt): TfD validation unseen SR = 11.27% (PLW 1.39); GC = 11.09% (PLW 4.00).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrieval-augmented memory improves planning, re-planning and failure recovery: removing memory leads to ~18% relative drop in success on TfD unseen. HELPER sets new SOTA on TEACh TfD and EDH (TfD unseen SR 13.73% vs prior SOTA DANLI 7.98%, a ~1.7x improvement) and personalizes via memory expansion (correctly generates personalized plans in 37/40 personalization requests). Soliciting sparse user feedback and using VLM-grounded failure descriptions further improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Context-window and computational cost: in-context examples scale compute and can approach LLM context limits; using GPT-4 is costly. Perception is a bottleneck: memory helps planning but 2D detectors and depth estimation limit execution success (large gains when using GT perception). The paper notes the need for multimodal (visual+language) memory and scalability concerns from larger prompt sizes; no in-depth analysis of retrieval errors or catastrophic forgetting is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3000.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3000.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-Planner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-Planner: Few-shot grounded planning for embodied agents with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior method that applies memory-augmented prompting of pretrained LLMs for grounded instruction following in embodied settings, using retrieved in-context examples to guide plan generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLM-Planner: Few-shot grounded planning for embodied agents with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM-Planner</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM-Planner uses retrieval-augmented few-shot prompting of pretrained LLMs to generate grounded plans for embodied instruction following; it provides in-context examples drawn from memory to the LLM during prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented in-context example memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Described in related work as using retrieved in-context examples to prompt an LLM for planning; specific implementation details are referenced to the Song et al. (2022) paper and not reproduced in detail in this paper. HELPER states LLM-Planner differs by lacking plan memory expansion, VLM-guided correction, and LLM-based object search.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Embodied instruction following / grounded planning (as in the LLM-Planner paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Few-shot grounded planning for embodied agents: generate multi-step action plans from natural-language instructions using LLMs with retrieved examples as context.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>planning / instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as the closest prior work using memory-augmented prompting for instruction following; HELPER extends it by enabling memory expansion (personalization), VLM-driven failure diagnosis/correction, and LLM-guided object search.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LLM-Planner: Few-shot grounded planning for embodied agents with large language models <em>(Rating: 2)</em></li>
                <li>Replug: Retrieval-augmented black-box language models <em>(Rating: 2)</em></li>
                <li>Reflect: Summarizing robot experiences for failure explanation and correction <em>(Rating: 2)</em></li>
                <li>WebGPT: Browser-assisted question-answering with human feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3000",
    "paper_id": "paper-729fc01274cc26798654a318d1a95e73c61f99a3",
    "extraction_schema_id": "extraction-schema-74",
    "extracted_data": [
        {
            "name_short": "HELPER",
            "name_full": "Human-instructable Embodied Language Parsing via Evolving Routines",
            "brief_description": "An embodied agent that uses a retrieval-augmented external key-value memory of language↔program pairs to prompt frozen LLMs (GPT-4) to generate Python programs over visuomotor primitives, and expands that memory with successful user-specific executions to personalize future behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "HELPER",
            "agent_description": "HELPER uses a PLANNER that performs retrieval-augmented prompting of a frozen LLM (gpt-4-0613) to generate Python programs over a fixed API of navigation and manipulation primitives. It maintains an external key-value memory of language keys mapped to programs, a Vision-Language Model (ALIGN) for visually-grounded failure diagnosis, an EXECUTOR that maintains 3D occupancy/semantic maps and verifies preconditions, and a LOCATOR that prompts the LLM for commonsense object search locations.",
            "memory_used": true,
            "memory_type": "retrieval-augmented key-value memory (language → program pairs)",
            "memory_mechanism_description": "Each memory key is a language context encoded with a frozen LLM text encoder (text-embedding-ada-002). Given current input I, HELPER retrieves the top-K (K=3 in TEACh eval) keys by L2 distance in embedding space and injects the corresponding language–program pairs as in‑context examples into the LLM prompt. Memory is expanded by adding (language instruction, successful execution program) pairs when the user confirms success; failure cases (predicted by a VLM) are also used to retrieve corrective examples for re-planning.",
            "task_name": "TEACh (Trajectory-from-Dialogue (TfD) and Execution-from-Dialog-History (EDH))",
            "task_description": "Long-horizon household instruction-following tasks in the AI2-THOR simulator where the agent receives natural-language dialogue (commander/follower) and must convert it to sequences of navigation and manipulation actions (or Python programs over primitives) executed from RGB (and estimated depth) observations; tasks require multi-step planning, object search, and recovery from execution failures.",
            "task_type": "embodied planning / dialogue-guided instruction following",
            "performance_with_memory": "TfD validation unseen: Success Rate (SR) = 13.73% (PLW 1.61); Goal-Condition (GC) = 14.17% (PLW 4.56). EDH validation unseen: SR = 17.40% (PLW 2.91); GC = 25.86% (PLW 7.90). HELPER also reports further improvements when eliciting user feedback (e.g., w/ two feedbacks TfD SR = 17.48%).",
            "performance_without_memory": "Ablation w/o memory-augmented prompting (fixed prompt): TfD validation unseen SR = 11.27% (PLW 1.39); GC = 11.09% (PLW 4.00).",
            "has_performance_comparison": true,
            "key_findings": "Retrieval-augmented memory improves planning, re-planning and failure recovery: removing memory leads to ~18% relative drop in success on TfD unseen. HELPER sets new SOTA on TEACh TfD and EDH (TfD unseen SR 13.73% vs prior SOTA DANLI 7.98%, a ~1.7x improvement) and personalizes via memory expansion (correctly generates personalized plans in 37/40 personalization requests). Soliciting sparse user feedback and using VLM-grounded failure descriptions further improve performance.",
            "limitations_or_challenges": "Context-window and computational cost: in-context examples scale compute and can approach LLM context limits; using GPT-4 is costly. Perception is a bottleneck: memory helps planning but 2D detectors and depth estimation limit execution success (large gains when using GT perception). The paper notes the need for multimodal (visual+language) memory and scalability concerns from larger prompt sizes; no in-depth analysis of retrieval errors or catastrophic forgetting is provided.",
            "uuid": "e3000.0",
            "source_info": {
                "paper_title": "Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLM-Planner",
            "name_full": "LLM-Planner: Few-shot grounded planning for embodied agents with large language models",
            "brief_description": "A prior method that applies memory-augmented prompting of pretrained LLMs for grounded instruction following in embodied settings, using retrieved in-context examples to guide plan generation.",
            "citation_title": "LLM-Planner: Few-shot grounded planning for embodied agents with large language models",
            "mention_or_use": "mention",
            "agent_name": "LLM-Planner",
            "agent_description": "LLM-Planner uses retrieval-augmented few-shot prompting of pretrained LLMs to generate grounded plans for embodied instruction following; it provides in-context examples drawn from memory to the LLM during prompting.",
            "memory_used": true,
            "memory_type": "retrieval-augmented in-context example memory",
            "memory_mechanism_description": "Described in related work as using retrieved in-context examples to prompt an LLM for planning; specific implementation details are referenced to the Song et al. (2022) paper and not reproduced in detail in this paper. HELPER states LLM-Planner differs by lacking plan memory expansion, VLM-guided correction, and LLM-based object search.",
            "task_name": "Embodied instruction following / grounded planning (as in the LLM-Planner paper)",
            "task_description": "Few-shot grounded planning for embodied agents: generate multi-step action plans from natural-language instructions using LLMs with retrieved examples as context.",
            "task_type": "planning / instruction following",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "Mentioned as the closest prior work using memory-augmented prompting for instruction following; HELPER extends it by enabling memory expansion (personalization), VLM-driven failure diagnosis/correction, and LLM-guided object search.",
            "limitations_or_challenges": null,
            "uuid": "e3000.1",
            "source_info": {
                "paper_title": "Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LLM-Planner: Few-shot grounded planning for embodied agents with large language models",
            "rating": 2,
            "sanitized_title": "llmplanner_fewshot_grounded_planning_for_embodied_agents_with_large_language_models"
        },
        {
            "paper_title": "Replug: Retrieval-augmented black-box language models",
            "rating": 2,
            "sanitized_title": "replug_retrievalaugmented_blackbox_language_models"
        },
        {
            "paper_title": "Reflect: Summarizing robot experiences for failure explanation and correction",
            "rating": 2,
            "sanitized_title": "reflect_summarizing_robot_experiences_for_failure_explanation_and_correction"
        },
        {
            "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
            "rating": 1,
            "sanitized_title": "webgpt_browserassisted_questionanswering_with_human_feedback"
        }
    ],
    "cost": 0.012819999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models</h1>
<p>Gabriel Sarch Yue Wu Michael J. Tarr Katerina Fragkiadaki<br>Carnegie Mellon University<br>{gsarch,ywu5,mt01}@andrew.cmu.edu,katef@cs.cmu.edu<br>helper-agent-llm.github.io</p>
<h4>Abstract</h4>
<p>Pre-trained and frozen large language models (LLMs) can effectively map simple scene rearrangement instructions to programs over a robot's visuomotor functions through appropriate few-shot example prompting. To parse open-domain natural language and adapt to a user's idiosyncratic procedures, not known during prompt engineering time, fixed prompts fall short. In this paper, we introduce HELPER, an embodied agent equipped with an external memory of language-program pairs that parses free-form human-robot dialogue into action programs through retrieval-augmented LLM prompting: relevant memories are retrieved based on the current dialogue, instruction, correction, or VLM description, and used as in-context prompt examples for LLM querying. The memory is expanded during deployment to include pairs of user's language and action plans, to assist future inferences and personalize them to the user's language and routines. HELPER sets a new state-of-the-art in the TEACh benchmark in both Execution from Dialog History (EDH) and Trajectory from Dialogue (TfD), with a 1.7x improvement over the previous state-of-the-art for TfD. Our models, code, and video results can be found in our project's website: helper-agent-llm.github.io.</p>
<h2>1 Introduction</h2>
<p>Parsing free-form human instructions and humanrobot dialogue into task plans that a robot can execute is challenging due to the open-endedness of environments and procedures to accomplish, and to the diversity and complexity of language humans use to communicate their desires. Human language often contains long-term references, questions, errors, omissions, or descriptions of routines specific to a particular user (Tellex et al., 2011; Liang, 2016; Klein and Manning, 2003). Instructions need to be interpreted in the environmental context in which they are issued, and plans need to adapt in a
closed-loop to execution failures. Large Language Models (LLMs) trained on Internet-scale text can parse language instructions to task plans with appropriate plan-like or code-like prompts, without any finetuning of the language model, as shown in recent works (Ahn et al., 2022; Liang et al., 2022; Zeng et al., 2022; Huang et al., 2022b; Singh et al., 2022a). The state of the environment is provided as a list of objects and their spatial coordinates, or as a free-form text description from a vision-language model (Liang et al., 2022; Liu et al., 2023b; Wu et al., 2023a; Ahn et al., 2022). Using LLMs for task planning requires engineering a prompt that includes a description of the task for the LLM to perform, a robot API with function documentation and expressive function names, environment and task instruction inputs, and a set of in-context examples for inputs and outputs for the task (Liang et al., 2022). These methods are not trained in the domain of interest; rather they are prompt-engineered having in mind the domain at hand.</p>
<p>How can we extend LLM-prompting for semantic parsing and task planning to open-domain, freeform instructions, corrections, human-robot dialogue, and users' idiosyncratic routines, not known at prompt engineering time? The prompts used for the domain of tabletop rearrangement are already approaching the maximum context window of widely used LLMs (Singh et al., 2022a; Liang et al., 2022). Even as context window size grows, more prompt examples result in larger attention operations and cause an increase in both inference time and resource usage.</p>
<p>To this end, we introduce HELPER (Humaninstructable Embodied Language Parsing via Evolving Routines), a model that uses retrievalaugmented situated prompting of LLMs to parse free-form dialogue, instructions, and corrections from humans and vision-language models to programs over a set of parameterized visuomotor routines. HELPER is equipped with an external</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Open-ended instructable agents with retrieval-augmented LLMs. We equip LLMs with an external memory of language and program pairs to retrieve in-context examples for prompts during LLM querying for task plans. Our model takes as input instructions, dialogue segments, corrections and VLM environment descriptions, retrieves relevant memories to use as in-context examples, and prompts LLMs to predict task plans and plan adjustments. Our agent executes the predicted plans from visual input using occupancy and semantic map building, 3D object detection and state tracking, and active exploration using guidance from LLMs' common sense to locate objects not present in the maps. Successful programs are added to the memory paired with their language context, allowing for personalized subsequent interactions.</p>
<p>Non-parametric key-value memory of language-program pairs. HELPER uses its memory to retrieve relevant in-context language and action program examples, and generates prompts tailored to the current language input. HELPER expands its memory with successful executions of user specific procedures; it then recalls them and adapts them in future interactions with the user. HELPER uses pre-trained vision-language models (VLMs) to diagnose plan failures in language format, and uses these to retrieve similar failure cases with solutions from its memory to seed the prompt. To execute a program predicted by the LLM, HELPER combines successful practices of previous home embodied agents, such as semantic and occupancy map building (Chaplot et al., 2020a; Blukis et al., 2022; Min et al., 2021), LLM-based common sense object search (Inoue and Ohashi, 2022), object detection and tracking with off-the-shelf detectors (Chaplot et al., 2020b), object attribute detection with VLMs (Zhang et al., 2022), and verification of action preconditions during execution.</p>
<p>We test HELPER on the TEACh benchmark (Padmakumar et al., 2021), which evaluates agents in their ability to complete a variety of long-horizon household tasks from RGB input given natural language dialogue between a commander (the instruction-giving user) and a follower (the instruction-seeking user). We achieve a new state-of-the-art in the TEACh Execution from Dialog History and Trajectory-from-Dialogue settings, improving task success by 1.7x and goal-condition success by 2.1x compared to prior work in TfD. By further soliciting and incorporating user feedback, HELPER attains an additional 1.3x boost in task success. Our work is inspired by works in the language domain (Perez et al., 2021; Schick and Schütze, 2020; Gao et al., 2020; Liu et al., 2021) that retrieve in-context prompt examples based on the input language query for NLP tasks. HELPER extends this capability to the domain of instructable embodied agents, and demonstrates the potential of memory-augmented LLMs for semantic parsing of open-ended free-form instructive language into an expandable library of programs.</p>
<h2>2 Related Work</h2>
<p><strong>Instructable Embodied Agents</strong> Significant strides have been made by training large neural networks to jointly map instructions and their sen</p>
<p>sory contexts to agent actions or macro-actions using imitation learning (Anderson et al., 2018b; Ku et al., 2020; Anderson et al., 2018a; Savva et al., 2019; Gervet et al., 2022; Shridhar et al., 2020; Cao et al.; Suglia et al., 2021; Fan et al., 2018; Yu et al., 2020; Brohan et al., 2022; Stone et al., 2023; Yu et al., 2023). Existing approaches differ—among others—in the way the state of the environment is communicated to the model. Many methods map RGB image tokens and language inputs directly to actions or macro-actions (Pashevich et al., 2021; Wijmans et al., 2020; Suglia et al., 2021; Krantz et al., 2020). Other methods map language instructions and linguistic descriptions of the environment's state in terms of object lists or objects spatial coordinates to macro-actions, foregoing visual feature description of the scene, in an attempt to generalize better (Liang et al., 2022; Singh et al., 2022a; Chaplot et al., 2020a; Min et al., 2021; Liu et al., 2022a; Murray and Cakmak, 2022; Liu et al., 2022b; Inoue and Ohashi, 2022; Song et al., 2022; Zheng et al., 2022; Zhang et al., 2022; Huang et al., 2022b, 2023; Ahn et al., 2022; Zeng et al., 2022; Huang et al., 2022a). Some of these methods finetune language models to map language input to macro-actions, while others prompt frozen LLMs to predict action programs, relying on the emergent in-context learning property of LLMs to emulate novel tasks at test time. Some methods use natural language as the output format of the LLM (Wu et al., 2023a; Song et al., 2022; Blukis et al., 2022; Huang et al., 2022b), and others use code format (Singh et al., 2022a; Liang et al., 2022; Huang et al., 2023). HELPER prompts frozen LLMs to predict Python programs over visuo-motor functions for parsing dialogue, instructions and corrective human feedback.</p>
<p>The work closest to HELPER is LLM Planner (Song et al., 2022) which uses memoryaugmented prompting of pretrained LLMs for instruction following. However, it differs from HELPER in several areas such as plan memory expansion, VLM-guided correction, and usage of LLMs for object search. Furthermore, while Singh et al. (2022b) frequently seeks human feedback, HELPER requests feedback only post full task execution and employs Visual-Language Models (VLMs) for error feedback, reducing user interruptions.</p>
<p>Numerous simulation environments exist for evaluating home assistant frameworks, including</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: HELPER's architecture. The model uses memory-augmented LLM prompting for task planning from instructions, corrections and human-robot dialogue and for re-planning during failures given feedback from a VLM model. The generated program is executed the EXECUTOR module. The EXECUTOR builds semantic, occupancy and 3D object maps, tracks object states, verifies action preconditions, and queries LLMs for search locations for objects missing from the maps, using the LOCATOR module.</p>
<p>Habitat (Savva et al., 2019), GibsonWorld (Shen et al., 2021), ThreeDWorld (Gan et al., 2022), and AI2THOR (Kolve et al., 2017). ALFRED (Shridhar et al., 2020) and TEACh (Padmakumar et al., 2021) are benchmarks in the AI2THOR environment (Kolve et al., 2017), measuring agents' competence in household tasks through natural language. Our research focuses on the 'Trajectory from Dialogue' (TfD) evaluation in TEACh, mirroring ALFRED but with greater task and input complexity.</p>
<h3>Prompting LLMs for action prediction and visual reasoning</h3>
<p>Since the introduction of few-shot prompting by Brown et al. (2020), several approaches have improved the prompting ability of LLMs by automatically learning prompts (Lester et al. 2021), chain of thought prompting (Nye et al.; Gao et al. 2022; Wei et al. 2022; Wang et al. 2022; Chen et al. 2022; Yao et al. 2023) and retrieval-augmented LLM prompting (Nakano et al. 2021; Shi et al. 2023; Jiang et al. 2023) for language modeling, question answering, and long-form, multi-hop text generation. HELPER uses memory-augmented prompting by retrieving and integrating similar task plans into the prompt to facilitate language parsing to programs.</p>
<p>LLMs have been used as policies in Minecraft to predict actions (Wang et al. 2023b, a), error correction (Liu et al. 2023b), and for understanding instruction manuals for game play in some</p>
<p>Atari games (Wu et al., 2023b). They have also significantly improved text-based agents in textbased simulated worlds (Yao et al., 2022; Shinn et al., 2023; Wu et al., 2023c; Richards, 2023). ViperGPT (Surís et al., 2023), and CodeVQA (Subramanian et al., 2023) use LLM prompting to decompose referential expressions and questions to programs over simpler visual routines. Our work uses LLMs for planning from free-form dialogue and user corrective feedback for home task completion, a domain not addressed in previous works.</p>
<h2>3 Method</h2>
<p>HELPER is an embodied agent designed to map human-robot dialogue, corrections and VLM descriptions to actions programs over a fixed API of parameterized navigation and manipulation primitives. Its architecture is outlined in Figure 2. At its heart, it generates plans and plan adjustments by querying LLMs using retrieval of relevant language-program pairs to include as in-context examples in the LLM prompt. The generated programs are then sent to the EXECUTOR module, which translates each program step into specific navigation and manipulation action. Before executing each step in the program, the EXECUTOR verifies if the necessary preconditions for an action, such as the robot already holding an object, are met. If not, the plan is adjusted according to the current environmental and agent state. Should a step involve an undetected object, the EXECUTOR calls on the LOCATOR module to efficiently search for the required object by utilizing previous user instructions and LLMs’ common sense knowledge. If any action fails during execution, a VLM predicts the reason for this failure from pixel input and feeds this into the PLANNER for generating plan adjustments.</p>
<h3>3.1 PLANNER: Retrieval-Augmented LLM Planning</h3>
<p>Given an input $I$ consisting of a dialogue segment, instruction, or correction, HELPER uses memoryaugmented prompting of frozen LLMs to map the input into an executable Python program over a parametrized set of manipulation and navigation primitives $G \in\left{G_{\text {manipulation }} \cup G_{\text {navigation }}\right}$ that the EXECUTOR can perform (e.g., goto(X), pickup(X), slice(X), ...). Our action API can be found in Section D of the Appendix.</p>
<p>HELPER maintains a key-value memory of language - program pairs, as shown in Figure 3A. Each language key is mapped to a 1D vector using an LLM's frozen language encoder. Given current context $I$, the model retrieves the top- $K$ keys, i.e., the keys with the smallest $L_{2}$ distance with the embedding of the input context $I$, and adds the corresponding language - program pairs to the LLM prompt as in-context examples for parsing the current input $I$.</p>
<p>Figure 3B illustrates the prompt format for the Planner. It includes the API specifying the primitives $G$ parameterized as Python functions, the retrieved examples, and the language input $I$. The LLM is tasked to generate a Python program over parameterized primitives $G$. Examples of our prompts and LLM responses can be found in Section F of the Appendix.</p>
<h3>3.1.1 Memory Expansion</h3>
<p>The key-value memory of HELPER can be continually expanded with successful executions of instructions to adapt to a user's specific routines, as shown in Figure 1. An additional key-value pair is added with the language instruction paired with the execution plan if the user indicates the task was successful. Then, HELPER can recall this plan and adapt it in subsequent interactions with the user. For example, if a user instructs HELPER one day to "Perform the Mary cleaning. This involves cleaning two plates and two cups in the sink", the user need only say "Do the Mary cleaning" in future interactions, and HELPER will retrieve the previous plan, include it in the examples section of the prompt, and query the LLM to adapt it accordingly. The personalization capabilities of HELPER are evaluated in Section 4.4.</p>
<h3>3.1.2 Incorporating user feedback</h3>
<p>A user's feedback can improve a robot's performance, but requesting feedback frequently can deteriorate the overall user experience. Thus, we enable HELPER to elicit user feedback only when it has completed execution of the program. Specifically, it asks "Is the task completed to your satisfaction? Did I miss anything?" once it believes it has completed the task. The user responds either that the task has been completed (at which point HELPER stops acting) or points out problems and corrections in free-form natural language, such as, "You failed to cook a slice of potato. The potato slice needs to be cooked.". HELPER uses the language</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: HELPER parses dialogue segments, instructions, and corrections into visuomotor programs using retrieval-augmented LLM prompting. <strong>A.</strong> Illustration of the encoding and memory retrieval process. <strong>B.</strong> Prompt format and output of the PLANNER.</p>
<p>feedback to re-plan using the PLANNER. We evaluate HELPER in its ability to seek and utilize user feedback in Section 4.3.</p>
<h3>3.1.3 Visually-Grounded Plan Correction using Vision-Language Models</h3>
<p>Generated programs may fail for various reasons, such as when a step in the plan is missed or an object-of-interest is occluded. When the program fails, HELPER uses a vision-language model (VLM) pre-trained on web-scale data, specifically the ALIGN model (Jia et al., 2021), to match the current visual observation with a pre-defined list of textual failure cases, such as <em>an object is blocking you from interacting with the selected object</em>, as illustrated in Figure 4. The best match is taken to be the failure feedback <em>F</em>. The PLANNER module then retrieves the top-<em>K</em> most relevant error correction examples, each containing input dialogue, failure feedback, and the corresponding corrective program, from memory based on encodings of input <em>I</em> and failure feedback <em>F</em> from the VLM. The LLM is prompted with the the failed program step, the predicted failure description <em>F</em> from the VLM, the in-context examples, and the original dialogue segment <em>I</em>. The LLM outputs a self-reflection (Shinn et al., 2023) as to why the failure occurred, and generates a program over manipulation and navigation primitives <em>G</em>, and an additional set of corrective primitives <em>Gcorrective</em> (e.g., step-back(), move-to-an-alternate-viewpoint(), ...). This program is sent to the EXECUTOR for execution.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Inference of a failure feedback description by matching potential failure language descriptions with the current image using a vision-language model (VLM).</p>
<h3>3.2 EXECUTOR: Scene Perception, Pre-Condition Checks, Object Search and Action Execution</h3>
<p>The EXECUTOR module executes the predicted Python programs in the environment, converting the code into low-level manipulation and navigation actions, as shown in Figure 2. At each time step, the EXECUTOR receives an RGB image and obtains an estimated depth map via monocular depth estimation (Bhat et al., 2023) and object masks via an off-the-shelf object detector (Dong et al., 2021).</p>
<h3>3.2.1 Scene and object state perception</h3>
<p>Using the depth maps, object masks, and approximate egomotion of the agent at each time step, the EXECUTOR maintains a 3D occupancy map and object memory of the home environment to navigate around obstacles and keep track of previously seen objects, similar to previous works (Sarch et al., 2022). Objects are detected in every frame and are merged into object instances based on closeness of the predicted 3D centroids. Each object instance is initialized with a set of object state at-</p>
<p>tributes (cooked, sliced, dirty, ...) by matching the object crop against each attribute with the pretrained ALIGN model (Jia et al., 2021). Object attribute states are updated when an object is acted upon via a manipulation action.</p>
<h3>3.2.2 Manipulation and navigation pre-condition checks</h3>
<p>The EXECUTOR module verifies the pre-conditions of an action before the action is taken to ensure the action is likely to succeed. In our case, these constraints are predefined for each action (for example, the agent must first be holding a knife to slice an object). If any pre-conditions are not satisfied, the EXECUTOR adjusts the plan accordingly. In more open-ended action interfaces, an LLM's common sense knowledge can be used to infer the preconditions for an action, rather than pre-defining them.</p>
<h3>3.2.3 LOCATOR: LLM-based common sense object search</h3>
<p>When HELPER needs to find an object that has not been detected before, it calls on the LOCATOR module. The LOCATOR prompts an LLM to suggest potential object search location for the EXECUTOR to search nearby, e.g. "search near the sink" or "search in the cupboard". The LOCATOR prompt takes in the language $I$ (which may reference the object location, e.g., "take the mug from the cupboard" ) and queries the LLM to generate proposed locations by essentially parsing the instruction as well as using its common sense. Based on these predictions, HELPER will go to the suggested locations if they exist in the semantic map (e.g., to the cupboard) and search for the object-of-interest. The LOCATOR's prompt can be found in Section D of the Appendix.
Implementation details. We use OpenAI's gpt-4-0613 (gpt, 2023) API, except when mentioned otherwise. We resort to the text-embedding-ada-002 (ada, 2022) API to obtain text embeddings. Furthermore, we use the SOLQ object detector (Dong et al., 2021), which is pretrained on MSCOCO (Lin et al., 2014) and finetuned on the training rooms of TEACh. For monocular depth estimation, we use the ZoeDepth network (Bhat et al., 2023), pretrained on the NYU indoor dataset (Nathan Silberman and Fergus, 2012) and subsequently fine-tuned on the training rooms of TEACh. In the TEACh evaluations, we use $K=3$ for retrieval.</p>
<h2>4 Experiments</h2>
<p>We test HELPER in the TEACh benchmark (Padmakumar et al., 2021). Our experiments aim to answer the following questions:</p>
<ol>
<li>How does HELPER compare to the SOTA on task planning and execution from free-form dialogue?</li>
<li>How much do different components of HELPER contribute to performance?</li>
<li>How much does eliciting human feedback help task completion?</li>
<li>How effectively does HELPER adapt to a user's specific procedures?</li>
</ol>
<h3>4.1 Evaluation on the TEACh dataset</h3>
<p>Dataset The dataset comprises over 3,000 hu-man-human, interactive dialogues, geared towards completing household tasks within the AI2-THOR simulation environment (Kolve et al., 2017). We evaluate on the Trajectory from Dialogue (TfD) evaluation variant, where the agent is given a dialogue segment at the start of the episode. The model is then tasked to infer the sequence of actions to execute based on the user's intents in the dialogue segment, ranging from MAKE COFFEE to PREPARE BREAKFAST. We show examples of such dialogues in Figure 3. We also test on the Execution from Dialogue History (EDH) task in TEACh, where the TfD episodes are partitioned into "sessions". The agent is spawned at the start of one of the sessions and must predict the actions to reach the next session given the dialogue and action history of the previous sessions. The dataset is split into training and validation sets. The validation set is divided into 'seen' and 'unseen' rooms based on their presence in the training set. Validation 'seen' has the same room instances but different object locations and initial states than any episodes in the training set. At each time step, the agent obtains an egocentric RGB image and must choose an action from a specified set to transition to the next step, such as pickup(X), turn left(), etc. Please see Appendix Section G for more details on the simulation environment.</p>
<p>Evaluation metrics Following evaluation practises for the TEACh benchmark, we use the following two metrics: 1. Task success rate (SR), which refers to the fraction of task sessions in which the</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">No <br> In-Domain <br> LLM</th>
<th style="text-align: center;">Memory- <br> Augmented <br> LLM</th>
<th style="text-align: center;">User <br> Personal- <br> ization</th>
<th style="text-align: center;">Accepts <br> User <br> Feedback</th>
<th style="text-align: center;">VLM- <br> Guided <br> correction</th>
<th style="text-align: center;">LLM- <br> Guided <br> Search</th>
<th style="text-align: center;">Pre- <br> Condition <br> Check</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">E.T. ${ }^{\text {(Pashevich et al., 2021) }}$</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: left;">JARVIS ${ }^{\text {(Zheng et al., 2022) }}$</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: left;">FILM ${ }^{\text {(Min et al., 2021, 2022) }}$</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: left;">DANLI ${ }^{\text {(Zhang et al., 2022) }}$</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">LLM-Planner ${ }^{\text {(Song et al., 2022) }}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: left;">Code as Policies ${ }^{\text {(Liang et al., 2022) }}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td style="text-align: left;">HELPER (ours)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison of HELPER to previous work.
agent successfully fulfills all goal conditions. 2. Goal condition success rate (GC), which quantifies the proportion of achieved goal conditions across all sessions. Both of these metrics have corresponding path length weighted (PLW) variants. In these versions, the agent incurs penalties for executing a sequence of actions that surpasses the length of the reference path annotated by human experts.</p>
<p>Baselines We consider the following baselines: 1. Episodic Transformer (E.T.) (Pashevich et al., 2021) is an end-to-end multimodal transformer that encodes language inputs and a history of visual observations to predict actions, trained with imitation learning from human demonstrations.
2. Jarvis (Zheng et al., 2022) trains an LLM on the TEACh dialogue to generate high-level subgoals that mimic those performed by the human demonstrator. Jarvis uses a semantic map and the Episodic Transformer for object search.
3. FILM (Min et al., 2021, 2022) fine-tunes an LLM to produce parametrized plan templates. Similar to Jarvis, FILM uses a semantic map for carrying out subgoals and a semantic policy for object search.
4. DANLI (Zhang et al., 2022) fine-tunes an LLM to predict high-level subgoals, and uses symbolic planning over an object state and spatial map to create an execution plan. DANLI uses an object search module and manually-defined error correction.</p>
<p>HELPER differs from the baselines in its use of memory-augmented context-dependent prompting of pretrained LLMs and pretrained visual-language models for planning, failure diagnosis and recovery, and object search. We provide a more in-depth comparison of HELPER to previous work in Table 1.</p>
<p>Evaluation We show quantitative results for HELPER and the baselines on the TEACh Trajectory from Dialogue (TfD) and Execution from Di-
alogue History (EDH) validation split in Table 2. On the TfD validation unseen, HELPER achieves a $\mathbf{1 3 . 7 3 \%}$ task success rate and $\mathbf{1 4 . 1 7 \%}$ goalcondition success rate, a relative improvement of 1.7 x and 2.1 x , respectively, over DANLI, the prior SOTA in this setting. HELPER additionally sets a new SOTA in the EDH task, achieving a $\mathbf{1 7 . 4 0 \%}$ task success rate and $\mathbf{2 5 . 8 6 \%}$ goalcondition success rate on validation unseen.</p>
<h3>4.2 Ablations</h3>
<p>We ablate components of HELPER in order to quantify what matters for performance in Table $2 A b$ lations. We perform all ablations on the TEACh TfD validation unseen split. We draw the following conclusions:</p>
<ol>
<li>Retrieval-augmented prompting helps for planning, re-planning and failure recovery. Replacing the memory-augmented prompts with a fixed prompt (w/o Mem Aug; Table 2) led to a relative $18 \%$ reduction in success rate.</li>
<li>VLM error correction helps the agent recover from failures. Removal of the visually-grounded plan correction (w/o Correction; Table 2) led to a relative $6 \%$ reduction in success rate.</li>
<li>The pre-condition check and the LLM search help. Removal of the action pre-condition checks (w/o Pre Check; Table 2) led to a relative $16 \%$ reduction in success rate. Replacing the LOCATOR LLM-based search with a random search (w/o LOCATOR; Table 2) led to a relative $12 \%$ reduction in success rate.</li>
<li>Larger LLMs perform better. Using GPT-3.5 (w GPT-3.5; Table 2) exhibits a relative $31 \%$ reduction in success rate compared to using GPT-4. Our findings on GPT-4's superior planning abilities align with similar findings from recent studies of Wu et al. (2023d); Bubeck et al. (2023); Liu et al. (2023a); Wang et al. (2023a).</li>
<li>Perception is a bottleneck. Using GT depth</li>
</ol>
<p>Table 2: Trajectory from Dialogue (TfD) and Execution from Dialog History (EDH) evaluation on the TEACh validation set. Trajectory length weighted metrics are included in ( parentheses ). $\mathrm{SR}=$ success rate. $\mathrm{GC}=$ goal condition success rate.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">TfD</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">EDH</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SR</td>
<td style="text-align: center;">GC</td>
<td style="text-align: center;">SR</td>
<td style="text-align: center;">GC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">SR</td>
<td style="text-align: center;">GC</td>
<td style="text-align: center;">SR</td>
<td style="text-align: center;">GC</td>
</tr>
<tr>
<td style="text-align: center;">E.T.</td>
<td style="text-align: center;">0.48 (0.12)</td>
<td style="text-align: center;">0.35 (0.59)</td>
<td style="text-align: center;">1.02 (0.17)</td>
<td style="text-align: center;">1.42 (4.82)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">7.8 (0.9)</td>
<td style="text-align: center;">9.1 (1.7)</td>
<td style="text-align: center;">10.2 (1.7)</td>
<td style="text-align: center;">15.7 (4.1)</td>
</tr>
<tr>
<td style="text-align: center;">JARVIS</td>
<td style="text-align: center;">1.80 (0.30)</td>
<td style="text-align: center;">3.10 (1.60)</td>
<td style="text-align: center;">1.70 (0.20)</td>
<td style="text-align: center;">5.40 (4.50)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">15.80 (2.60)</td>
<td style="text-align: center;">16.60 (8.20)</td>
<td style="text-align: center;">15.10 (3.30)</td>
<td style="text-align: center;">22.60 (8.70)</td>
</tr>
<tr>
<td style="text-align: center;">FILM</td>
<td style="text-align: center;">2.9 (1.0)</td>
<td style="text-align: center;">6.1 (2.5)</td>
<td style="text-align: center;">5.5 (2.6)</td>
<td style="text-align: center;">5.8 (11.6)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10.2 (1.0)</td>
<td style="text-align: center;">18.3 (2.7)</td>
<td style="text-align: center;">14.3 (2.1)</td>
<td style="text-align: center;">26.4 (5.6)</td>
</tr>
<tr>
<td style="text-align: center;">DANLI</td>
<td style="text-align: center;">7.98 (3.20)</td>
<td style="text-align: center;">6.79 (6.57)</td>
<td style="text-align: center;">4.97 (1.86)</td>
<td style="text-align: center;">10.50 (10.27)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">16.98 (7.24)</td>
<td style="text-align: center;">23.44 (19.95)</td>
<td style="text-align: center;">17.76 (9.28)</td>
<td style="text-align: center;">24.93 (22.20)</td>
</tr>
<tr>
<td style="text-align: center;">HELPER (ours)</td>
<td style="text-align: center;">13.73 (1.61)</td>
<td style="text-align: center;">14.17 (4.56)</td>
<td style="text-align: center;">12.15 (1.79)</td>
<td style="text-align: center;">18.62 (9.28)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">17.40 (2.91)</td>
<td style="text-align: center;">25.86 (7.90)</td>
<td style="text-align: center;">18.59 (4.00)</td>
<td style="text-align: center;">32.09 (9.81)</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Ablations</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">w/o Mem Aug</td>
<td style="text-align: center;">11.27 (1.39)</td>
<td style="text-align: center;">11.09 (4.00)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">w/o Pre Check</td>
<td style="text-align: center;">11.6 (1.36)</td>
<td style="text-align: center;">11.32 (4.15)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">w/o Correction</td>
<td style="text-align: center;">12.9 (1.53)</td>
<td style="text-align: center;">12.45 (4.91)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">w/o Locator</td>
<td style="text-align: center;">12.09 (1.29)</td>
<td style="text-align: center;">10.89 (3.83)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">w/ GPT-3.5</td>
<td style="text-align: center;">9.48 (1.21)</td>
<td style="text-align: center;">10.05 (3.68)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">w/ GT depth</td>
<td style="text-align: center;">15.85 (2.85)</td>
<td style="text-align: center;">14.49 (6.89)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">w/ GT depth,seg</td>
<td style="text-align: center;">22.55 (6.39)</td>
<td style="text-align: center;">30.00 (14.56)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">w/ GT percept</td>
<td style="text-align: center;">30.23 (9.12)</td>
<td style="text-align: center;">50.46 (20.24)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">User Feedback</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">w/ Feedback 1</td>
<td style="text-align: center;">16.34 (1.67)</td>
<td style="text-align: center;">14.70 (4.69)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">w/ Feedback 2</td>
<td style="text-align: center;">17.48 (1.97)</td>
<td style="text-align: center;">14.93 (4.74)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">w/ GT percept, Feedback 2</td>
<td style="text-align: center;">37.75 (10.96)</td>
<td style="text-align: center;">56.77 (19.80)</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>(w/ GT depth; Table 2) led to an improvement of 1.15 x compared to using estimated depth from RGB. Notable is the 1.77 x improvement in pathlength weighted success when using GT depth. This change is due to lower accuracy for far depths in our depth estimation network lower, thereby causing the agent to spend more time mapping the environment and navigating noisy obstacle maps. Using lidar or better map estimation techniques could mitigate this issue.</p>
<p>Using ground truth segmentation masks and depth (w/ GT depth, seg; Table 2) improves task success and goal-conditioned task success by 1.64x and 2.11x, respectively. This shows the limitations of frame-based object detection and late fusion of detection responses over time. 3D scene representations that fuse features earlier across views may significantly improve 3D object detection. Using GT perception (w/ GT percept; Table 2), which includes depth, segmentation, action success, oracle failure feedback, and increased API failure limit (50), led to 2.20 x and 3.56 x improvement.</p>
<h3>4.3 Eliciting Users' Feedback</h3>
<p>We enable HELPER to elicit sparse user feedback by asking "Is the task completed to your satisfaction? Did I miss anything?" once it believes it has completed the task, as explained in Section 3.1.2. The user will then respond with steps missed by</p>
<p>HELPER, and HELPER will re-plan based on this feedback. As shown in in Table 2 User Feedback, asking for a user's feedback twice improves performance by 1.27x. Previous works do not explore this opportunity of eliciting human feedback partly due to the difficulty of interpreting it—being free-form language-which our work addresses.</p>
<h3>4.4 Personalization</h3>
<p>We evaluate HELPER's ability to retrieve userspecific routines, as well as on their ability to modify the retrieved routines, with one, two, or three modifications, as discussed in 3.1.1. For example, for three modifications we might instruct HELPER: "Make me a Dax sandwich with 1 slice of tomato, 2 lettuce leaves, and add a slice of bread".</p>
<p>Dataset The evaluation tests 10 user-specific plans for each modification category in five distinct tasks: Make a Sandwich; Prepare Breakfast; Make a Salad; Place X on Y; and Clean X. The evaluation contains 40 user requests. The complete list of user-specific plans and modification requests can be found in the Appendix, Section C.</p>
<p>Evaluation We report the success rate in Table 3. HELPER generates the correct personalized plan for all but three instances, out of 40 evaluation requests. This showcases the ability of HELPER to acquire, retrieve and adapt plans based on context and previous user interactions.</p>
<h2>5 Limitations</h2>
<p>Our model in its current form has the following limitations:</p>
<ol>
<li>Simplified failure detection. The AI2-THOR simulator much simplifies action failure detection which our work and previous works exploit (Min et al., 2021; Inoue and Ohashi, 2022). In a more general setting, continuous progress monitoring</li>
</ol>
<p>Table 3: Evaluation of HELPER for user personalization. Reported is success of generating the correct plan for 10 personalized plans for a request of the original plan without modifications, and one, two, or three modifications to the original plan. These experiments use the text-davinci-003 model as the prompted LLM.</p>
<table>
<thead>
<tr>
<th></th>
<th>Success</th>
</tr>
</thead>
<tbody>
<tr>
<td>Original Plan</td>
<td>$100 \%$</td>
</tr>
<tr>
<td>One Change</td>
<td>$100 \%$</td>
</tr>
<tr>
<td>Two Changes</td>
<td>$80 \%$</td>
</tr>
<tr>
<td>Three Changes</td>
<td>$90 \%$</td>
</tr>
</tbody>
</table>
<p>from pixels would be required for failure detection, which model VLMs can deliver and we will address in future work.
2. 3D perception bottleneck. HELPER relies on 2D object detectors and depth 3D lifting for 3D object localization. We observe a 2X boost in TEACh success rate from using ground truth segmentation in HELPER. In future work we plan to integrate early 2D features into persistent 3D scene feature representations for more accurate 3D object detection.
4. Cost from LLM querying. GPT-4 API is the most accurate LLM used in HELPER and incurs a significant cost. NLP research in model compression may help decreasing these costs, or finetuning smaller models with enough input-output pairs.</p>
<h2>3. Multimodal (vision and language) memory</h2>
<p>retrieval. Currently, we use a text bottleneck in our environment state descriptions. Exciting future directions include exploration of visual state incorporation to the language model and partial adaptation of its parameters. A multi-modal approach to the memory and plan generation would help contextualize the planning more with the visual state.</p>
<p>Last, to follow human instructions outside of simulation environments our model would need to interface with robot closed-loop policies instead of abstract manipulation primitives, following previous work (Liang et al., 2022).</p>
<h2>6 Conclusion</h2>
<p>We presented HELPER, an instructable embodied agent that uses memory-augmented prompting of pre-trained LLMs to parse dialogue segments, instructions and corrections to programs over action primitives, that it executes in household environments from visual input. HELPER updates its memory with user-instructed action programs after successful execution, allowing personalized interactions by recalling and adapting them. It sets a new state-of-the-art in the TEACh benchmark. Future research directions include extending the model to include a visual modality by encoding visual context during memory retrieval or as direct input to the LLM. We believe our work contributes towards exciting new capabilities for instructable and conversable systems, for assisting users and personalizing human-robot communication.</p>
<h2>7 Acknowledgements</h2>
<p>This material is based upon work supported by National Science Foundation grants GRF DGE1745016 \&amp; DGE2140739 (GS), a DARPA Young Investigator Award, a NSF CAREER award, an AFOSR Young Investigator Award, and DARPA Machine Common Sense, and an ONR award N000142312415. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Army, the National Science Foundation, or the United States Air Force.</p>
<p>This research project has benefitted from the Microsoft Accelerate Foundation Models Research (AFMR) grant program through which leading foundation models hosted by Microsoft Azure along with access to Azure credits were provided to conduct the research.</p>
<p>The authors thank William W. Cohen, Ayush Jain, Théophile Gervet, Nikolaos Gkanatsios, and Adam Harley for discussions and useful feedback over the course of this project.</p>
<h2>Ethics Statement</h2>
<p>The objective of this research is to construct autonomous agents. Despite the absence of human experimentation, practitioners could potentially implement this technology in human-inclusive environments. Therefore, applications of our research should appropriately address privacy considerations.</p>
<p>All the models developed in this study were trained using Ai2Thor (Kolve et al., 2017). Consequently, there might be an inherent bias towards North American homes. Additionally, we only consider English language inputs in this study.</p>
<h2>References</h2>
<ol>
<li>New and improved embedding model.</li>
<li>Openai. gpt-4 technical report. arXiv preprint arxiv:2303.08774.</li>
</ol>
<p>Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. 2022. Do as i can and not as i say: Grounding language in robotic affordances. In arXiv preprint arXiv:2204.01691.</p>
<p>Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, et al. 2018a. On evaluation of embodied navigation agents. arXiv preprint arXiv:1807.06757.</p>
<p>Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton Van Den Hengel. 2018b. Vision-and-language navigation: Interpreting visuallygrounded navigation instructions in real environments. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 36743683.</p>
<p>Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Müller. 2023. Zoedepth: Zeroshot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288.</p>
<p>Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, and Yoav Artzi. 2022. A persistent spatial semantic representation for high-level natural language instruction execution. In Conference on Robot Learning, pages 706-717. PMLR.</p>
<p>Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. 2022. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.</p>
<p>Yuchen Cao, Nilay Pande, Ayush Jain, Shikhar Sharma, Gabriel Sarch, Nikolaos Gkanatsios, Xian Zhou, and Katerina Fragkiadaki. Embodied symbiotic assistants that see, act, infer and chat.</p>
<p>Devendra Singh Chaplot, Dhiraj Gandhi, Saurabh Gupta, Abhinav Gupta, and Ruslan Salakhutdinov. 2020a. Learning to explore using active neural slam. arXiv preprint arXiv:2004.05155.</p>
<p>Devendra Singh Chaplot, Dhiraj Prakashchand Gandhi, Abhinav Gupta, and Russ R Salakhutdinov. 2020b. Object goal navigation using goal-oriented semantic exploration. Advances in Neural Information Processing Systems, 33.</p>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. 2022. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588.</p>
<p>Bin Dong, Fangao Zeng, Tiancai Wang, Xiangyu Zhang, and Yichen Wei. 2021. Solq: Segmenting objects by learning queries. Advances in Neural Information Processing Systems, 34:21898-21909.</p>
<p>Linxi Fan, Yuke Zhu, Jiren Zhu, Zihua Liu, Orien Zeng, Anchit Gupta, Joan Creus-Costa, Silvio Savarese, and Li Fei-Fei. 2018. Surreal: Open-source reinforcement learning framework and robot manipulation benchmark. In Conference on Robot Learning, pages 767-782. PMLR.</p>
<p>Chuang Gan, Siyuan Zhou, Jeremy Schwartz, Seth Alter, Abhishek Bhandwaldar, Dan Gutfreund, Daniel L.K. Yamins, James J. DiCarlo, Josh McDermott, Antonio Torralba, and Joshua B. Tenenbaum. 2022. The threedworld transport challenge: A visually guided task-and-motion planning benchmark towards physically realistic embodied ai. In 2022 International Conference on Robotics and Automation (ICRA), pages 8847-8854.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2022. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435.</p>
<p>Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723.</p>
<p>Theophile Gervet, Soumith Chintala, Dhruv Batra, Jitendra Malik, and Devendra Singh Chaplot. 2022. Navigating to objects in the real world. arXiv preprint arXiv:2212.00922.</p>
<p>Chenguang Huang, Oier Mees, Andy Zeng, and Wolfram Burgard. 2022a. Visual language maps for robot navigation. arXiv preprint arXiv:2210.05714.</p>
<p>Siyuan Huang, Zhengkai Jiang, Hao Dong, Yu Qiao, Peng Gao, and Hongsheng Li. 2023. Instruct2act: Mapping multi-modality instructions to robotic actions with large language model. arXiv preprint arXiv:2305.11176.</p>
<p>Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. 2022b. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608.</p>
<p>Yuki Inoue and Hiroki Ohashi. 2022. Prompter: Utilizing large language model prompting for a data efficient embodied instruction following. arXiv preprint arXiv:2211.03267.</p>
<p>Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 4904-4916. PMLR.</p>
<p>Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active retrieval augmented generation.</p>
<p>Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st annual meeting of the association for computational linguistics, pages 423-430.</p>
<p>Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. 2017. AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv.</p>
<p>Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra, and Stefan Lee. 2020. Beyond the nav-graph: Vision-and-language navigation in continuous environments. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXVIII 16, pages 104-120. Springer.</p>
<p>Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, and Jason Baldridge. 2020. Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding. arXiv preprint arXiv:2010.07954.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691.</p>
<p>Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. 2022. Code as policies: Language model
programs for embodied control. arXiv preprint arXiv:2209.07753.</p>
<p>Percy Liang. 2016. Learning executable semantic parsers for natural language understanding. Communications of the ACM, 59(9):68-76.</p>
<p>Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740-755. Springer.</p>
<p>Haoyu Liu, Yang Liu, Hongkai He, and Hangfang Yang. 2022a. Lebp-language expectation \&amp; binding policy: A two-stream framework for embodied vision-and-language interaction task learning agents. arXiv preprint arXiv:2203.04637.</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804.</p>
<p>Xiaotian Liu, Hector Palacios, and Christian Muise. 2022b. A planning based neural-symbolic approach for embodied instruction following. Interactions, $9(8): 17$.</p>
<p>Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li, Mengshen He, Zhengliang Liu, et al. 2023a. Summary of chatgpt/gpt-4 research and perspective towards the future of large language models. arXiv preprint arXiv:2304.01852.</p>
<p>Zeyi Liu, Arpit Bahety, and Shuran Song. 2023b. Reflect: Summarizing robot experiences for failure explanation and correction. arXiv preprint arXiv:2306.15724.</p>
<p>So Yeon Min, Devendra Singh Chaplot, Pradeep Ravikumar, Yonatan Bisk, and Ruslan Salakhutdinov. 2021. Film: Following instructions in language with modular methods.</p>
<p>So Yeon Min, Hao Zhu, Ruslan Salakhutdinov, and Yonatan Bisk. 2022. Don't copy the teacher: Data and model challenges in embodied dialogue. arXiv preprint arXiv:2210.04443.</p>
<p>Michael Murray and Maya Cakmak. 2022. Following natural language instructions for household tasks with landmark guided search and reinforced pose adjustment. IEEE Robotics and Automation Letters, 7(3):6870-6877.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted questionanswering with human feedback. arXiv preprint arXiv:2112.09332.</p>
<p>Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. 2012. Indoor segmentation and support inference from rgbd images. In ECCV.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy GurAri, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models, november 2021. URL http://arxiv. org/abs/2112.00114.</p>
<p>Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava, Patrick Lange, Anjali Narayan-Chen, Spandana Gella, Robinson Piramuthu, Gokhan Tur, and Dilek Hakkani-Tur. 2021. Teach: Task-driven embodied agents that chat.</p>
<p>Alexander Pashevich, Cordelia Schmid, and Chen Sun. 2021. Episodic transformer for vision-and-language navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1594215952.</p>
<p>Ethan Perez, Douwe Kiela, and Kyunghyun Cho. 2021. True few-shot learning with language models. Advances in neural information processing systems, 34:11054-11070.</p>
<p>Toran Bruce Richards. 2023. Auto-gpt: An autonomous gpt-4 experiment.</p>
<p>Gabriel Sarch, Zhaoyuan Fang, Adam W Harley, Paul Schydlo, Michael J Tarr, Saurabh Gupta, and Katerina Fragkiadaki. 2022. Tidee: Tidying up novel rooms using visuo-semantic commonsense priors. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXIX, pages 480-496. Springer.</p>
<p>Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. 2019. Habitat: A platform for embodied ai research. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9339-9347.</p>
<p>Timo Schick and Hinrich Schütze. 2020. It's not just size that matters: Small language models are also few-shot learners. arXiv preprint arXiv:2009.07118.</p>
<p>Bokui Shen, Fei Xia, Chengshu Li, Roberto MartínMartín, Linxi Fan, Guanzhi Wang, Claudia PérezD'Arpino, Shyamal Buch, Sanjana Srivastava, Lyne P. Tchapmi, Micael E. Tchapmi, Kent Vainio, Josiah Wong, Li Fei-Fei, and Silvio Savarese. 2021. igibson 1.0: a simulation environment for interactive tasks in large realistic scenes. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems, page accepted. IEEE.</p>
<p>Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. Replug: Retrievalaugmented black-box language models. arXiv preprint arXiv:2301.12652.</p>
<p>Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366.</p>
<p>Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. 2020. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10740-10749.</p>
<p>Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. 2022a. Progprompt: Generating situated robot task plans using large language models. arXiv preprint arXiv:2209.11302.</p>
<p>Kunal Pratap Singh, Luca Weihs, Alvaro Herrasti, Jonghyun Choi, Aniruddha Kembhavi, and Roozbeh Mottaghi. 2022b. Ask4help: Learning to leverage an expert for embodied tasks. Advances in Neural Information Processing Systems, 35:16221-16232.</p>
<p>Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M. Sadler, Wei-Lun Chao, and Yu Su. 2022. Llm-planner: Few-shot grounded planning for embodied agents with large language models. arXiv preprint arXiv:2212.04088.</p>
<p>Austin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrishnan, Kuang-Huei Lee, Quan Vuong, Paul Wohlhart, Sean Kirmani, Brianna Zitkovich, Fei Xia, Chelsea Finn, and Karol Hausman. 2023. Open-world object manipulation using pre-trained vision-language model. In arXiv preprint.</p>
<p>Sanjay Subramanian, Medhini Narasimhan, Kushal Khangaonkar, Kevin Yang, Arsha Nagrani, Cordelia Schmid, Andy Zeng, Trevor Darrell, and Dan Klein. 2023. Modular visual question answering via code generation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Alessandro Suglia, Qiaozi Gao, Jesse Thomason, Govind Thattai, and Gaurav S. Sukhatme. 2021. Embodied bert: A transformer model for embodied, language-guided visual task completion. In EMNLP 2021 Workshop on Novel Ideas in Learning-to-Learn through Interaction.</p>
<p>Dídac Surís, Sachit Menon, and Carl Vondrick. 2023. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128.</p>
<p>Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew Walter, Ashis Banerjee, Seth Teller, and Nicholas Roy. 2011. Understanding natural language commands for robotic navigation and mobile manipulation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 25, pages 1507-1514.</p>
<p>Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023a. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv: Arxiv-2305.16291.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. 2023b. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>
<p>Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, and Dhruv Batra. 2020. Decentralized distributed ppo: Solving pointgoal navigation. In International Conference on Lefoarning Representations (ICLR).</p>
<p>Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, and Thomas Funkhouser. 2023a. Tidybot: Personalized robot assistance with large language models. arXiv preprint arXiv:2305.05658.</p>
<p>Yue Wu, Yewen Fan, Paul Pu Liang, Amos Azaria, Yuanzhi Li, and Tom M Mitchell. 2023b. Read and reap the rewards: Learning to play atari with the help of instruction manuals. In NeurIPS.</p>
<p>Yue Wu, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Yuanzhi Li, Tom Mitchell, and Shrimai Prabhumoye. 2023c. Plan, eliminate, and track-language models are good teachers for embodied agents. arXiv preprint arXiv:2305.02412.</p>
<p>Yue Wu, So Yeon Min, Shrimai Prabhumoye, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Tom Mitchell, and Yuanzhi Li. 2023d. Spring: Gpt-4 out-performs rl algorithms by studying papers and reasoning. In NeurIPS.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629.</p>
<p>Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. 2020. Meta-world: A benchmark and evaluation
for multi-task and meta reinforcement learning. In Conference on Robot Learning, pages 1094-1100. PMLR.</p>
<p>Tianhe Yu, Ted Xiao, Austin Stone, Jonathan Tompson, Anthony Brohan, Su Wang, Jaspiar Singh, Clayton Tan, Dee M, Jodilyn Peralta, Brian Ichter, Karol Hausman, and Fei Xia. 2023. Scaling robot learning with semantically imagined experience. In arXiv preprint arXiv:2302.11550.</p>
<p>Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, et al. 2022. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598.</p>
<p>Yichi Zhang, Jianing Yang, Jiayi Pan, Shane Storks, Nikhil Devraj, Ziqiao Ma, Keunwoo Peter Yu, Yuwei Bao, and Joyce Chai. 2022. Danli: Deliberative agent for following natural language instructions. arXiv preprint arXiv:2210.12485.</p>
<p>Kaizhi Zheng, Kaiwen Zhou, Jing Gu, Yue Fan, Jialu Wang, Zonglin Li, Xuehai He, and Xin Eric Wang. 2022. Jarvis: A neuro-symbolic commonsense reasoning framework for conversational embodied agents.</p>
<p>A Analysis of User Personalization Failures</p>
<p>The three instances where the PLANNER made errors in the user personalization experiment (Section 4.4) involved logical mistakes or inappropriate alterations to parts of the original plan that were not requested for modification. For instance, in one case - involving a step to modify the plan from making one coffee to two coffees - the PLANNER includes placing two mugs in the coffee maker simultaneously, which is not a valid plan. In the other two instances, the PLANNER omits an object from the original plan that was not mentioned in the modification.</p>
<h2>B User Feedback Details</h2>
<p>In the user feedback evaluation, once the agent has indicated completion of the task from the original input dialogue, the agent will query feedback from the user. If the simulator indicates success of the task, the agent will end the episode. If the simulator indicates the task is not successful, feedback will be given to the agent for additional planning. This feedback is programatically generated from the TEACh simulator metadata, which gives us information about if the task is successful, and what object state changes are missing in order to complete the task (e.g., bread slice is not toasted, etc.). For each object state that is incorrect, we form a sentence of the following form: "You failed to complete the subtask: subtask. For the object object: description of desired object state." We combine all subtask sentences to create the feedback. HELPER follows the same pipeline (including examples, retrieval, planning, etc.) to process the feedback as with the input dialogue in the normal TfD evaluation. We show experiments with one and two user feedback requests in Section 4.3 of the main paper (a second request is queried if the first user feedback fails to produce task success).</p>
<h2>C User Personalization Inputs</h2>
<p>We provide a full list of the user personalization requests in Listing 1 for the user personalization experiments in Section 4.4.</p>
<h2>D Prompts</h2>
<p>We provide our full API (Listing 2), corrective API (Listing 3), PLANNER prompt (Listing 4), replanning prompt (Listing 5), and LOCATOR prompt
(Listing 6).</p>
<h2>E Pre-conditions</h2>
<p>An example of a pre-condition check for a macroaction is provided in Listing 7.</p>
<h2>F Example LLM inputs \&amp; Outputs</h2>
<p>We provide examples of dialogue input, retrieved examples, and LLM output for a TEACh sample in Listing 8, Listing 9, and Listing 10.</p>
<h2>G Simulation environment</h2>
<p>The TEACh dataset builds on the Ai2thor simulation environment (Kolve et al., 2017). At each time step the agent may choose from the following actions: Forward(), Backward(), Turn Left(), Turn Right(), Look Up(), Look Down(), Strafe Left(), Strafe Right(), Pickup(X), Place(X), Open(X), Close(X), ToggleOn(X), ToggleOff(X), $\operatorname{Slice}(\mathrm{X})$, and $\operatorname{Pour}(\mathrm{X})$, where X refers an object specified via a relative coordinate $(x, y)$ on the egocentric RGB frame. Navigation actions move the agent in discrete steps. We rotate in the yaw direction by 90 degrees, and rotate in the pitch direction by 30 degrees. The RGB and depth sensors are at a resolution of 480x480, a field of view of 90 degrees, and lie at a height of 0.9015 meters. The agent's coordinates are parameterized by a single $(x, y, z)$ coordinate triplet with $x$ and $z$ corresponding to movement in the horizontal plane and $y$ reserved for the vertical direction. The TEACh benchmark allows a maximum of 1000 steps and 30 API failures per episode.</p>
<h2>H Executor details</h2>
<h2>H. 1 Semantic mapping and planning</h2>
<p>Obstacle map HELPER maintains a 2D overhead occupancy map of its environment $\in \mathbb{R}^{H \times W}$ that it updates at each time step from the input RGBD stream. The map is used for exploration and navigation in the environment.</p>
<p>At every time step $t$, we unproject the input depth maps using intrinsic and extrinsic information of the camera to obtain a 3D occupancy map registered to the coordinate frame of the agent, similar to earlier navigation agents (Chaplot et al., 2020a). The 2D overhead maps of obstacles and free space are computed by projecting the 3D occupancy along the height direction at multiple height levels and summing. For each input RGB image,</p>
<p>we run a SOLQ object segmentor <em>Dong et al. (2021)</em> (pretrained on COCO <em>Lin et al. (2014)</em> then finetuned on TEACh rooms) to localize each of 116 semantic object categories. For failure detection, we use a simple matching approach from <em>Min et al. (2021)</em> to compare RGB pixel values before and after taking an action.</p>
<p>Object location and state tracking We maintain an object memory as a list of object detection 3D centroids and their predicted semantic labels $\left{\left[(X, Y, Z)<em i="i">{i}, \ell</em>$. We estimate the centroid by taking the 3D point corresponding to the median depth within the segmentation mask and bring it to a common coordinate frame. We do a simple form of non-maximum suppression on the object memory, by comparing the euclidean distance of centroids in the memory to new detected centroids of the same category, and keep the one with the highest score if they fall within a distance threshold.} \in{1 \ldots N}\right], i=1 . . K\right}$, where $K$ is the number of objects detected thus far. The object centroids are expressed with respect to the coordinate system of the agent, and, similar to the semantic maps, updated over time using egomotion. We track previously detected objects by their 3D centroid $C \in \mathbb{R}^{3</p>
<p>For each object in the object memory, we maintain an object state dictionary with a pre-defined list of attributes. These attributes include: category label, centroid location, holding, detection score, can use, sliced, toasted, clean, cooked. For the binary attributes, these are initialized by sending the object crop, defined by the detector mask, to the VLM model, and checking its match to each of [f"The {object_category} is {attribute}", f"The {object_category} is not {attribute}"]. We found that initializing these attributes with the VLM gave only a marginal difference to initializing them to default values in the TEACh benchmark, so we do not use it for the TEACh evaluations. However, we anticipate a general method beyond dataset biases of TEACh would much benefit from such visionbased attribute classification.</p>
<p>Exploration and path planning HELPER explores the scene using a classical mapping method. We take the initial position of the agent to be the center coordinate in the map. We rotate the agent in-place and use the observations to instantiate an initial map. Second, the agent incrementally completes the maps by randomly sampling an unexplored, traversible location based on the 2D occu- pancy map built so far, and then navigates to the sampled location, accumulating the new information into the maps at each time step. The number of observations collected at each point in the 2D occupancy map is thresholded to determine whether a given map location is explored or not. Unexplored positions are sampled until the environment has been fully explored, meaning that the number of unexplored points is fewer than a predefined threshold.</p>
<p>To navigate to a goal location, we compute the geodesic distance to the goal from all map locations using graph search <em>Inoue and Ohashi (2022)</em> given the top-down occupancy map and the goal location in the map. We then simulate action sequences and greedily take the action sequence which results in the largest reduction in geodesic distance.</p>
<h3>H. 2 2D-to-3D unprojection</h3>
<p>For the $i$-th view, a 2D pixel coordinate $(u, v)$ with depth $z$ is unprojected and transformed to its coordinate $(X, Y, Z)^{T}$ in the reference frame:</p>
<p>$$
(X, Y, Z, 1)=\mathbf{G}<em x="x">{i}^{-1}\left(z \frac{u-c</em>
$$}}{f_{x}}, z \frac{v-c_{y}}{f_{y}}, z, 1\right)^{T</p>
<p>where $\left(f_{x}, f_{y}\right)$ and $\left(c_{x}, c_{y}\right)$ are the focal lengths and center of the pinhole camera model and $\mathbf{G}<em i="i">{i} \in$ $S E(3)$ is the camera pose for view $i$ relative to the reference view. This module unprojects each depth image $I</em>$ being the number of pixels with an associated depth value.} \in \mathbb{R}^{H \times W \times 3}$ into a pointcloud in the reference frame $P_{i} \in \mathbb{R}^{M_{i} \times 3}$ with $M_{i</p>
<h2>I Additional Experiments</h2>
<h2>I. 1 Alternate EDH Evaluation Split</h2>
<p>Currently, the leaderboard for the TEACh EDH benchmark is not active. Thus, we are not able to evaluate on the true test set for TEACh. We used the original validation seen and unseen splits, which have been used in most previous works <em>Pashevich et al. (2021); Zheng et al. (2022); Min et al. (2022); Zhang et al. (2022)</em>. In Table 4 we report the alternative validation and test split as mentioned in the TEACh github README, and also reported by DANLI <em>Zhang et al. (2022)</em>.</p>
<p>Table 4: Alternative TEACh Execution from Dialog History (EDH) evaluation split. Trajectory length weighted metrics are included in ( parentheses ). $\mathrm{SR}=$ success rate. $\mathrm{GC}=$ goal condition success rate. Note that Test Seen and Unseen are not the true TEACh test sets, but an alternative split of the validation set used until the true test evaluation is released, as mentioned in the TEACh github README, and also reported by DANLI (Zhang et al., 2022).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Validation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Test</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SR</td>
<td style="text-align: center;">GC</td>
<td style="text-align: center;">SR</td>
<td style="text-align: center;">GC</td>
<td style="text-align: center;">SR</td>
<td style="text-align: center;">GC</td>
<td style="text-align: center;">SR</td>
<td style="text-align: center;">GC</td>
</tr>
<tr>
<td style="text-align: center;">E.T.</td>
<td style="text-align: center;">8.35 (0.86)</td>
<td style="text-align: center;">6.34 (3.69)</td>
<td style="text-align: center;">8.28 (1.13)</td>
<td style="text-align: center;">8.72 (3.82)</td>
<td style="text-align: center;">7.38 (0.97)</td>
<td style="text-align: center;">6.06 (3.17)</td>
<td style="text-align: center;">8.82 (0.29)</td>
<td style="text-align: center;">9.46 (3.03)</td>
</tr>
<tr>
<td style="text-align: center;">DANLI</td>
<td style="text-align: center;">17.25 (7.16)</td>
<td style="text-align: center;">23.88 (19.38)</td>
<td style="text-align: center;">16.89 (9.12)</td>
<td style="text-align: center;">25.10 (22.56)</td>
<td style="text-align: center;">16.71 (7.33)</td>
<td style="text-align: center;">23.00 (20.55)</td>
<td style="text-align: center;">18.63 (9.41)</td>
<td style="text-align: center;">24.77 (21.90)</td>
</tr>
<tr>
<td style="text-align: center;">HELPER</td>
<td style="text-align: center;">17.25 (3.22)</td>
<td style="text-align: center;">25.24 (8.12)</td>
<td style="text-align: center;">19.21 (4.72)</td>
<td style="text-align: center;">33.54 (10.95)</td>
<td style="text-align: center;">17.55 (2.59)</td>
<td style="text-align: center;">26.49 (7.67)</td>
<td style="text-align: center;">17.97 (3.44)</td>
<td style="text-align: center;">30.81 (8.93)</td>
</tr>
</tbody>
</table>
<p>Listing 1: Full list of user personalization requests for the user personalization evaluation.</p>
<p>original input to LLM:
[['Driver', 'What is my task?'], ['Commander', "Make me a sandwich. The name of this sandwich is called the Larry sandwich. The sandwich has two slices of toast, 3 slices of tomato, and 3 slice of lettuce on a clean plate."]]
[['Driver', 'What is my task?'], ['Commander', 'Make me a salad. The name of this salad is called the David salad. The salad has two slices of tomato and three slices of lettuce on a clean plate.']]
[['Driver', 'What is my task?'], ['Commander', "Make me a salad. The name of this salad is called the Dax salad. The salad has two slices of cooked potato. You'll need to cook the potato on the stove. The salad also has a slice of lettuce and a slice of tomato. Put all components on a clean plate."]]
[['Driver', 'What is my task?'], ['Commander', 'Make me breakfast. The name of this breakfast is called the Mary breakfast. The breakfast has a mug of coffee, and two slices of toast on a clean plate.']]
[['Driver', 'What is my task?'], ['Commander', 'Make me breakfast. The name of this breakfast is called the Lion breakfast. The breakfast has a mug of coffee, and four slices of tomato on a clean plate.']]
[['Driver', 'What is my task?'], ['Commander', 'Rearrange some objects. The name of this rearrangement is called the Lax rearrangement. Place three pillows on the sofa.']]
[['Driver', 'What is my task?'], ['Commander', 'Rearrange some objects. The name of this rearrangement is called the Pax rearrangement. Place two pencils and two pens on the desk.']]
[['Driver', 'What is my task?'], ['Commander', 'Clean some objects. The name of this cleaning is called the Gax cleaning. Clean two plates and two cups.']]
[['Driver', 'What is my task?'], ['Commander', "Make me a sandwich. The name of this sandwich is called the Gabe sandwich. The sandwich has two slices of toast, 2 slices of tomato, and 1 slice of lettuce on a clean plate."]]
[['Driver', 'What is my task?'], ['Commander', 'Clean some objects. The name of this cleaning is called the Kax cleaning. Clean a mug and a pan.']]
No change:
"Make me the Larry sandwich"
"Make me the David salad"
"Make me the Dax salad"
"Make me the Mary breakfast"
"Make me the Lion breakfast"
"Complete the Lax rearrangement"
"Complete the Pax rearrangement"
"Perform the Gax cleaning"
"Make me the Gabe sandwich"
"Perform the Kax cleaning"
One change:
"Make me the Larry sandwich with four slices of lettuce"
"Make me the David salad with a slice of potato"
"Make me the Dax salad without lettuce"
"Make me the Mary breakfast with no coffee"
"Make me the Lion breakfast with three slice of tomato"
"Complete the Lax rearrangement with two pillows"
"Complete the Pax rearrangement but use one pencil instead of the the two pencils"
"Perform the Gax cleaning with three plates instead of two"
"Make me the Gabe sandwich with only 1 slice of tomato"
"Perform the Kax cleaning with only a mug"</p>
<h1>Two changes:</h1>
<p>"Make me the Larry sandwich with four slices of lettuce and two slices of tomato"
"Make me the David salad but add a slice of potato and add one slice of egg"
"Make me the Dax salad without lettuce and without potato"
"Make me the Mary breakfast with no coffee and add an egg"
"Make me the Lion breakfast with three slice of tomato and two mugs of coffee"
"Complete the Lax rearrangement with two pillows and add a remote"
"Complete the Pax rearrangement but use one pencil instead of the two pencils and add a book"
"Perform the Gax cleaning with three plates instead of the two plates and include a fork"
"Make me the Gabe sandwich with only 1 slice of tomato and two slices of lettuce"
"Perform the Kax cleaning without the pan and include a spoon"
Three changes:
"Make me the Larry sandwich with four slices of lettuce, two slices of tomato, and place all components directly on the countertop"
"Make me the David salad and add a slice of potato, add one slice of egg, and bring a fork with it"
"Make me the Dax salad without lettuce, without potato, and add an extra slice of tomato"
"Make me the Mary breakfast with no coffee, add an egg, and add a cup filled with water"
"Make me the Lion breakfast with three slice of tomato, two mugs of coffee, and add a fork"
"Complete the Lax rearrangement with two pillows, a remote, and place it on the arm chair instead"
"Complete the Pax rearrangement but use one pencil instead of the two pencils and include a book and a baseball bat"
"Perform the Gax cleaning with three plates instead of the two plates, include a fork, and do not clean any cups"
"Make me the Gabe sandwich with only 1 slice of tomato, two slices of lettuce, and add a slice of egg"
"Perform the Kax cleaning without the pan, include a spoon, and include a pot"</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nl">InteractionObject</span><span class="p">:</span>
<span class="w">    </span><span class="ss">&quot; &quot;</span><span class="w"> </span><span class="ss">&quot;</span>
<span class="ss">    This class represents an expression that uniquely identifies an object in the house.</span>
<span class="ss">    &quot;&quot;&quot;</span>
<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="nl">object_class</span><span class="p">:</span><span class="w"> </span><span class="nf">str</span><span class="p">,</span><span class="w"> </span><span class="nl">landmark</span><span class="p">:</span><span class="w"> </span><span class="nf">str</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="p">,</span><span class="w"> </span><span class="nl">attributes</span><span class="p">:</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[]</span><span class="p">)</span><span class="err">:</span>
<span class="w">        </span><span class="p">...</span>
<span class="w">        </span><span class="nl">object_class</span><span class="p">:</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="n">category</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">interaction</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="p">(</span><span class="n">e</span><span class="p">.</span><span class="n">g</span><span class="p">.,</span><span class="w"> </span><span class="ss">&quot;Mug&quot;</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;Apple&quot;</span><span class="p">)</span>
<span class="w">        </span><span class="nl">landmark</span><span class="p">:</span><span class="w"> </span><span class="p">(</span><span class="n">optional</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">mentioned</span><span class="p">)</span><span class="w"> </span><span class="n">landmark</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="n">category</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">interaction</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="ow">in</span>
<span class="w">            </span><span class="n">relation</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="p">(</span><span class="n">e</span><span class="p">.</span><span class="n">g</span><span class="p">.,</span><span class="w"> </span><span class="ss">&quot;CounterTop&quot;</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="ss">&quot;apple is on the countertop&quot;</span><span class="p">)</span>
<span class="w">            </span><span class="nl">attributes</span><span class="p">:</span><span class="w"> </span><span class="p">(</span><span class="n">optional</span><span class="p">)</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">strings</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">desired</span><span class="w"> </span><span class="n">attributes</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">object</span><span class="p">.</span><span class="w"> </span><span class="n">These</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="ow">not</span>
<span class="w">                </span><span class="n">necessarily</span><span class="w"> </span><span class="n">attributes</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">currently</span><span class="w"> </span><span class="n">exist</span><span class="p">,</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="n">ones</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">eventually</span>
<span class="w">                </span><span class="n">have</span><span class="p">.</span><span class="w"> </span><span class="n">Attributes</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="nl">following</span><span class="p">:</span><span class="w"> </span><span class="ss">&quot;toasted&quot;</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;clean&quot;</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;cooked&quot;</span>
<span class="w">            </span><span class="p">...</span>
<span class="w">            </span><span class="n">self</span><span class="p">.</span><span class="n">object_class</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">object_class</span>
<span class="w">            </span><span class="n">self</span><span class="p">.</span><span class="n">landmark</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">landmark</span>
<span class="w">            </span><span class="n">self</span><span class="p">.</span><span class="n">attributes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">attributes</span>
<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">pickup</span><span class="p">(</span><span class="n">self</span><span class="p">)</span><span class="err">:</span>
<span class="w">        </span><span class="ss">&quot;&quot;&quot;pickup the object.</span>
<span class="ss">        This function assumes the object is in view.</span>
<span class="ss">        Example:</span>
<span class="ss">        dialogue: &lt;Commander&gt; Go get the lettuce on the kitchen counter.</span>
<span class="ss">        Python script:</span>
<span class="ss">        target_lettuce = InteractionObject(&quot;</span><span class="n">Lettuce</span><span class="ss">&quot;, landmark = &quot;</span><span class="n">CounterTop</span><span class="ss">&quot;)</span>
<span class="ss">        target_lettuce.go_to()</span>
<span class="ss">        target_lettuce.pickup()</span>
<span class="ss">        &quot;&quot;&quot;</span>
<span class="w">        </span><span class="n">pass</span>
<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">place</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">landmark_name</span><span class="p">)</span><span class="err">:</span>
<span class="w">        </span><span class="ss">&quot;&quot;&quot;put the interaction object on the landmark_name object.</span>
<span class="ss">        landmark_name must be a class InteractionObject instance</span>
<span class="ss">        This function assumes the robot has picked up an object and the landmark object is in view.</span>
<span class="ss">        Example:</span>
<span class="ss">        dialogue: &lt;Commander&gt; Put the lettuce on the kitchen counter.</span>
<span class="ss">        Python script:</span>
<span class="ss">        target_lettuce = InteractionObject(&quot;</span><span class="n">Lettuce</span><span class="ss">&quot;, landmark = &quot;</span><span class="n">CounterTop</span><span class="ss">&quot;)</span>
<span class="ss">        target_lettuce.go_to()</span>
<span class="ss">        target_lettuce.pickup()</span>
<span class="ss">        target_countertop = InteractionObject(&quot;</span><span class="n">CounterTop</span><span class="ss">&quot;)</span>
<span class="ss">        target_countertop.go_to()</span>
<span class="ss">        target_lettuce.place(target_countertop)</span>
<span class="ss">        &quot;&quot;&quot;</span>
<span class="w">        </span><span class="n">pass</span>
<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">slice</span><span class="p">(</span><span class="n">self</span><span class="p">)</span><span class="err">:</span>
<span class="w">        </span><span class="ss">&quot;&quot;&quot;slice the object into pieces.</span>
<span class="ss">        This function assumes the agent is holding a knife and the agent has navigated to the object</span>
<span class="ss">            using go_to().</span>
<span class="ss">        Example:</span>
<span class="ss">        dialogue: &lt;Commander&gt; Cut the apple on the kitchen counter.</span>
<span class="ss">        Python script:</span>
<span class="ss">        target_knife = InteractionObject(&quot;</span><span class="n">Knife</span><span class="ss">&quot;) # first we need a knife to slice the apple with</span>
<span class="ss">        target_knife.go_to()</span>
<span class="ss">        target_knife.pickup()</span>
<span class="ss">        target_apple = InteractionObject(&quot;</span><span class="n">Apple</span><span class="ss">&quot;, landmark = &quot;</span><span class="n">CounterTop</span><span class="ss">&quot;)</span>
<span class="ss">        target_apple.go_to()</span>
<span class="ss">        target_apple.slice()</span>
<span class="ss">        &quot;</span><span class="w"> </span><span class="ss">&quot; &quot;</span>
<span class="w">    </span><span class="n">pass</span>
</code></pre></div>

<p>def toggle_on(self):
"""toggles on the interaction object.
This function assumes the interaction object is already off and the agent has navigated to the object.
Only some landmark objects can be toggled on. Lamps, stoves, and microwaves are some examples of objects that can be toggled on.</p>
<p>Example:
dialogue: <Commander> Turn on the lamp.
Python script:
target_floorlamp = InteractionObject("FloorLamp")
target_floorlamp.go_to()
target_floorlamp.toggle_on()
"""
pass
def toggle_off(self):
"""toggles off the interaction object.
This function assumes the interaction object is already on and the agent has navigated to the object.
Only some objects can be toggled off. Lamps, stoves, and microwaves are some examples of objects that can be toggled off.</p>
<p>Example:
dialogue: <Commander> Turn off the lamp.
Python script:
target_floorlamp = InteractionObject("FloorLamp")
target_floorlamp.go_to()
target_floorlamp.toggle_off()
"""
pass
def go_to(self):
"""Navigate to the object
"""
pass
def open(self):
"""open the interaction object.
This function assumes the landmark object is already closed and the agent has already navigated to the object.
Only some objects can be opened. Fridges, cabinets, and drawers are some example of objects that can be closed.</p>
<p>Example:
dialogue: <Commander> Get the lettuce in the fridge.
Python script:
target_fridge = InteractionObject("Fridge")
target_lettuce = InteractionObject("Lettuce", landmark = "Fridge")
target_fridge.go_to()
target_fridge.open()
target_lettuce.pickup()
"""
pass
def close(self):
"""close the interaction object.
This function assumes the object is already open and the agent has already navigated to the object.
Only some objects can be closed. Fridges, cabinets, and drawers are some example of objects that can be closed.
"""
pass
def clean(self):</p>
<p>"""wash the interaction object to clean it in the sink.
This function assumes the object is already picked up.
Example:
dialogue: <Commander> Clean the bowl
Python script:
target_bowl = InteractionObject("Bowl", attributes = ["clean"])
target_bowl.clean()
"""
pass
def put_down(self):
"""puts the interaction object currently in the agent's hand on the nearest available receptacle</p>
<p>This function assumes the object is already picked up.
This function is most often used when the holding object is no longer needed, and the agent needs to pick up another object
"""
pass
def pour(self, landmark_name):
"""pours the contents of the interaction object into the landmark object specified by the landmark_name argument
landmark_name must be a class InteractionObject instance
This function assumes the object is already picked up and the object is filled with liquid.
"""
pass
def fill_up(self):
"""fill up the interaction object with water
This function assumes the object is already picked up. Note that only container objects can be filled with liquid.
"""
pass
def pickup_and_place(self, landmark_name):
"""go_to() and pickup() this interaction object, then go_to() and place() the interaction object on the landmark_name object.
landmark_name must be a class InteractionObject instance
"""
pass
def empty(self):
"""Empty the object of any other objects on/in it to clear it out.
Useful when the object is too full to place an object inside it.
Example:
dialogue: <Commander> Clear out the sink.
Python script:
target_sink = InteractionObject("Sink")
target_sink.empty()
"""
pass
def cook(self):
"""Cook the object
Example:
dialogue: <Commander> Cook the potato.
Python script:
target_potato = InteractionObject("Potato", attributes = ["cooked"])
target_potato.cook()
"""</p>            </div>
        </div>

    </div>
</body>
</html>
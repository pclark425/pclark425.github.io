<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6472 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6472</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6472</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-267750831</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.11626v1.pdf" target="_blank">Metacognitive Retrieval-Augmented Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Retrieval-augmented generation have become central in natural language processing due to their efficacy in generating factual content. While traditional methods employ single-time retrieval, more recent approaches have shifted towards multi-time retrieval for multi-hop reasoning tasks. However, these strategies are bound by predefined reasoning steps, potentially leading to inaccuracies in response generation. This paper introduces MetaRAG, an approach that combines the retrieval-augmented generation process with metacognition. Drawing from cognitive psychology, metacognition allows an entity to self-reflect and critically evaluate its cognitive processes. By integrating this, MetaRAG enables the model to monitor, evaluate, and plan its response strategies, enhancing its introspective reasoning abilities. Through a three-step metacognitive regulation pipeline, the model can identify inadequacies in initial cognitive responses and fixes them. Empirical evaluations show that MetaRAG significantly outperforms existing methods.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6472.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6472.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetaRAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Metacognitive Retrieval-Augmented Generation (MetaRAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented LLM framework that adds a metacognitive loop (monitoring, evaluating, planning) to iteratively assess and improve multi-hop QA answers by issuing extra retrievals, diagnosing reasoning errors, and instructing revision strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MetaRAG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A cognition-metacognition agent: the cognition component (gpt-35-turbo-16k in experiments) generates answers using retrieved documents; the metacognition component (finetuned T5-large as expert monitor and T5-XXL as NLI) monitors answer similarity to an expert, evaluates internal vs external knowledge sufficiency and common error patterns, then plans targeted actions (new queries, reliance policies, double-checking) and triggers multi-time retrievals and iterative re-answering.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>gpt-35-turbo-16k (cognition); T5-large (monitor); T5-XXL (NLI) — as reported in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented generation (external document memory; multi-time retrieval when triggered)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>raw text passages (Wikipedia articles segmented into ~100-token passages)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>similarity-based retrieval (BM25 + E5 retriever used in experiments; top-5 passages returned); metacognitive planner can generate new queries that are re-issued to the retriever and newly retrieved passages are appended to references</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotpotQA; 2WikiMultiHopQA (subsampled 500 validation questions each)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>multi-hop reasoning / question answering (retrieval-augmented)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>2WikiMultiHopQA: EM 42.8, F1 50.8 (reported in ablation/summary tables for MetaRAG configuration in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Ablation removing external-knowledge procedural assessment (reported as 'w/o. External' in paper): EM 37.4, F1 44.9 (this ablation indicates performance drop when external-knowledge handling is removed); the paper also contrasts to closebook baselines showing retrieval helps but these exact closebook numbers are in main tables.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM) and token-level F1 (plus precision and recall reported)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Latency and compute grow with proportion of questions sent to metacognition and with the number of metacognitive iterations; authors report inference time increases with higher monitor thresholds and higher max_iter (see Tables 4 and 5). Also there is a sweet spot (threshold ≈ 0.4 and max_iter ≈ 5) beyond which extra iterations yield diminishing returns or slight degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Dependence on quality of expert monitor and NLI: evaluating and planning can be affected if those models are inaccurate; over-triggering metacognition ('overthinking') can harm performance for simple queries; excessive iterations introduce noise and may reduce accuracy; conflicting internal vs external knowledge remains a primary challenge that requires careful planning (authors propose discard-or-rely strategies but note distractor effect).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Zhou, Y., Liu, Z., Jin, J., Nie, J.-Y., & Dou, Z. (2024). Metacognitive Retrieval-Augmented Large Language Models. (arXiv preprint / conference manuscript)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Metacognitive Retrieval-Augmented Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6472.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6472.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Standard RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (single-time retrieval baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline retrieval-augmented generation approach that retrieves documents for the input query and conditions the LLM on retrieved passages to produce answers (single-time retrieval in the canonical RAG setup).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Standard RAG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Single-time retrieval-augmented LLM: query used to fetch top-k passages which are concatenated to prompt the LLM; in this paper it serves as a baseline and is evaluated with the same retriever/corpus as MetaRAG for fairness.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in this paper for baseline LLMs (the paper uses gpt-35-turbo-16k for cognition experiments across methods)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented generation (single-time retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>raw text passages (Wikipedia passages used as external memory in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>BM25 + E5 retriever (top-5 passages) in the experimental setup of this paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotpotQA; 2WikiMultiHopQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>multi-hop reasoning / retrieval-augmented QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match and token-level F1 (used across comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Single-time retrieval is simpler and cheaper than multi-time retrieval but insufficient for some multi-hop queries; paper motivates multi-time or metacognitive retrieval to address such shortcomings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Single-time retrieval can fail on complex multi-hop questions that require iterative decomposition; susceptible to insufficient or conflicting external evidence because retrieval happens only once.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Metacognitive Retrieval-Augmented Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6472.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6472.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (reasoning + acting loop)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A language-agent paradigm that interleaves chain-of-thought reasoning with environment actions (e.g., retrieval) forming a thought-action-observation loop to iteratively gather information and reason.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent that alternates reasoning steps and actions (actions include retrieval or API calls) so the model can generate intermediate search queries / interactions and use observations to refine reasoning; in this paper used as a multi-time retrieval baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in this paper (baseline behavior evaluated using gpt-35-turbo-16k in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>multi-time retrieval / interaction history (treated as external evidence gathered during the loop)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>observations and retrieved passages (raw text) appended to context</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>uses generated intermediate content as dynamic queries; in the paper's experiments retrieval is done via BM25 + E5 retriever</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotpotQA; 2WikiMultiHopQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>multi-hop reasoning / iterative retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match and token-level F1</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Multi-time retrieval approaches like ReAct can improve on ambiguous or multi-hop queries but add complexity and potential extra latency compared to single-time retrieval; paper notes gains but exact tradeoffs depend on trigger policy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Improvements are limited when knowledge conflicts or when reasoning errors persist despite additional retrieval; paper argues a metacognitive critic can better diagnose error causes than plain ReAct.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Metacognitive Retrieval-Augmented Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6472.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6472.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flare</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flare (active retrieval during generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that actively retrieves information during answer generation using forward-looking sentences as dynamic search triggers to support multi-hop reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Flare</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Performs active retrieval while generating answers, using prospective sentences as retrieval triggers so the model can iteratively gather relevant evidence during multi-step generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in this paper (evaluated as baseline using the experimental setup with gpt-35-turbo-16k)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>multi-time retrieval (active retrieval during generation)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>raw text passages from Wikipedia</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>dynamic queries formed during generation; in this paper, retrieval is instantiated via BM25 + E5 for comparability</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotpotQA; 2WikiMultiHopQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>multi-hop reasoning / retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match and token-level F1</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Active retrieval increases retrieval calls and latency but can provide missing facts for multi-hop chains; the paper positions MetaRAG as improving on active retrieval by diagnosing when further retrieval is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>May still follow pre-defined retrieval schedules and lacks the metacognitive diagnosis of why answers fail; vulnerable to retrieval of conflicting or distracting documents.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Metacognitive Retrieval-Augmented Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6472.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6472.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IR‑CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interleaved Retrieval with Chain-of-Thought (IR-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that interleaves retrieval operations with Chain-of-Thought reasoning to support multi-hop question answering by retrieving when intermediate steps require evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>IR-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Interleaves retrieval and CoT reasoning: at certain reasoning steps the agent issues retrievals to obtain evidence that supports the chain of thought, evaluated here as a baseline under the same retriever/corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in this paper (evaluated using gpt-35-turbo-16k as the cognition model)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>interleaved multi-time retrieval (external document memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>raw text passages</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>retrieval triggered during reasoning steps; in experiments retrieval implemented by BM25 + E5</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotpotQA; 2WikiMultiHopQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>multi-hop reasoning / retrieval-augmented QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match and token-level F1</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Interleaving retrieval and CoT can increase retrieval calls and inference cost but helps incorporate evidence into multi-step chains; still can miss diagnosing knowledge conflicts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The paper reports gains from interleaved retrieval but notes remaining failure modes (insufficient/conflicting knowledge, erroneous reasoning) which MetaRAG targets with metacognitive evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Metacognitive Retrieval-Augmented Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6472.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6472.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self‑Ask</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Ask (with intermediate question decomposition)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that decomposes complex questions into intermediate sub-questions and issues retrievals for those sub-questions to enable multi-step answering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Self-Ask</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Generates intermediate sub-questions during reasoning and uses retrieved answers to these sub-questions to compose the final answer; used as a baseline in the paper's experiments with the shared retrieval/corpus setup.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in this paper (baseline behavior evaluated with gpt-35-turbo-16k as cognition LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>multi-step retrieval (decomposition-driven retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>raw text passages (Wikipedia)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>retrieval calls issued for each sub-question; in experiments retrieval done via BM25 + E5</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotpotQA; 2WikiMultiHopQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>multi-hop reasoning / question decomposition + retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match and token-level F1</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Decomposition increases number of retrievals and latency but can make multi-hop chains more explicit; still susceptible to errors from incomplete decomposition or wrong retrieved evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance depends on quality of decomposition and retrieved evidence; paper argues metacognitive planning helps identify missing sub-questions or insufficient evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Metacognitive Retrieval-Augmented Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6472.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6472.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion (language agents with verbal reinforcement learning / self-critic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A language-agent approach that uses a critic/evaluator and past linguistic feedback to iteratively improve agent outputs; included here as a baseline that uses a self-critic mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An agent that records past attempts and employs a self-evaluator to provide feedback and guide subsequent attempts; in this paper Reflexion is a baseline that includes a critic component but lacks MetaRAG's explicit metacognitive diagnosis and planning for retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in this paper (baseline evaluated using gpt-35-turbo-16k for cognition)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic/logged attempts (verbal memory of past attempts and feedback) and optionally retrieval when configured</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>stored previous attempts and textual feedback (logged text); in paper used as a baseline with retrieval enabled in some comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>read/write textual logs of past attempts; in the paper the baseline was run with retrieval and a critic component to compare with MetaRAG</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotpotQA; 2WikiMultiHopQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>multi-hop reasoning / iterative refinement</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match and token-level F1</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Reflexion's self-critic improves over non-critic baselines but MetaRAG shows larger gains by diagnosing causes (insufficient/conflicting knowledge vs reasoning errors); Reflexion's approach may be less targeted than metacognitive planning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Self-criticism alone can detect errors but not always provide targeted retrieval or correction strategy; paper reports MetaRAG outperforms Reflexion substantially on both datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Metacognitive Retrieval-Augmented Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6472.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6472.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Training LM w/ Memory Aug.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Training language models with memory augmentation (referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced prior work (Zhong et al., EMNLP 2022) that studies augmenting language models with explicit memory mechanisms during training to improve long-range dependencies and retrieval of past tokens/contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Memory-augmented LMs (Zhong et al. 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Prior research that trains language models augmented with an explicit memory component (paper referenced in related work); mentioned as background on memory augmentation techniques for LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>memory augmentation (training-time memory mechanisms) — referenced (not implemented) in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>memory-augmented LMs (general)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Referenced as prior work; this paper does not reproduce or report detailed tradeoffs from that work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not detailed in this paper (only cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Metacognitive Retrieval-Augmented Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Training language models with memory augmentation <em>(Rating: 2)</em></li>
                <li>REALM: Retrieval-Augmented Language Model Pre-Training <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Improving Language Models by Retrieving from Trillions of Tokens <em>(Rating: 2)</em></li>
                <li>REPLUG: Retrieval-Augmented Black-Box Language Models <em>(Rating: 2)</em></li>
                <li>Generalization through Memorization: Nearest Neighbor Language Models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6472",
    "paper_id": "paper-267750831",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "MetaRAG",
            "name_full": "Metacognitive Retrieval-Augmented Generation (MetaRAG)",
            "brief_description": "A retrieval-augmented LLM framework that adds a metacognitive loop (monitoring, evaluating, planning) to iteratively assess and improve multi-hop QA answers by issuing extra retrievals, diagnosing reasoning errors, and instructing revision strategies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MetaRAG",
            "agent_description": "A cognition-metacognition agent: the cognition component (gpt-35-turbo-16k in experiments) generates answers using retrieved documents; the metacognition component (finetuned T5-large as expert monitor and T5-XXL as NLI) monitors answer similarity to an expert, evaluates internal vs external knowledge sufficiency and common error patterns, then plans targeted actions (new queries, reliance policies, double-checking) and triggers multi-time retrievals and iterative re-answering.",
            "model_size": "gpt-35-turbo-16k (cognition); T5-large (monitor); T5-XXL (NLI) — as reported in the paper",
            "memory_used": true,
            "memory_type": "retrieval-augmented generation (external document memory; multi-time retrieval when triggered)",
            "memory_representation": "raw text passages (Wikipedia articles segmented into ~100-token passages)",
            "memory_access_mechanism": "similarity-based retrieval (BM25 + E5 retriever used in experiments; top-5 passages returned); metacognitive planner can generate new queries that are re-issued to the retriever and newly retrieved passages are appended to references",
            "task_name": "HotpotQA; 2WikiMultiHopQA (subsampled 500 validation questions each)",
            "task_category": "multi-hop reasoning / question answering (retrieval-augmented)",
            "performance_with_memory": "2WikiMultiHopQA: EM 42.8, F1 50.8 (reported in ablation/summary tables for MetaRAG configuration in paper)",
            "performance_without_memory": "Ablation removing external-knowledge procedural assessment (reported as 'w/o. External' in paper): EM 37.4, F1 44.9 (this ablation indicates performance drop when external-knowledge handling is removed); the paper also contrasts to closebook baselines showing retrieval helps but these exact closebook numbers are in main tables.",
            "has_comparative_results": true,
            "performance_metric": "Exact Match (EM) and token-level F1 (plus precision and recall reported)",
            "tradeoffs_reported": "Latency and compute grow with proportion of questions sent to metacognition and with the number of metacognitive iterations; authors report inference time increases with higher monitor thresholds and higher max_iter (see Tables 4 and 5). Also there is a sweet spot (threshold ≈ 0.4 and max_iter ≈ 5) beyond which extra iterations yield diminishing returns or slight degradation.",
            "limitations_or_failure_cases": "Dependence on quality of expert monitor and NLI: evaluating and planning can be affected if those models are inaccurate; over-triggering metacognition ('overthinking') can harm performance for simple queries; excessive iterations introduce noise and may reduce accuracy; conflicting internal vs external knowledge remains a primary challenge that requires careful planning (authors propose discard-or-rely strategies but note distractor effect).",
            "citation": "Zhou, Y., Liu, Z., Jin, J., Nie, J.-Y., & Dou, Z. (2024). Metacognitive Retrieval-Augmented Large Language Models. (arXiv preprint / conference manuscript)",
            "uuid": "e6472.0",
            "source_info": {
                "paper_title": "Metacognitive Retrieval-Augmented Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Standard RAG",
            "name_full": "Retrieval-Augmented Generation (single-time retrieval baseline)",
            "brief_description": "A baseline retrieval-augmented generation approach that retrieves documents for the input query and conditions the LLM on retrieved passages to produce answers (single-time retrieval in the canonical RAG setup).",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Standard RAG",
            "agent_description": "Single-time retrieval-augmented LLM: query used to fetch top-k passages which are concatenated to prompt the LLM; in this paper it serves as a baseline and is evaluated with the same retriever/corpus as MetaRAG for fairness.",
            "model_size": "Not specified in this paper for baseline LLMs (the paper uses gpt-35-turbo-16k for cognition experiments across methods)",
            "memory_used": true,
            "memory_type": "retrieval-augmented generation (single-time retrieval)",
            "memory_representation": "raw text passages (Wikipedia passages used as external memory in experiments)",
            "memory_access_mechanism": "BM25 + E5 retriever (top-5 passages) in the experimental setup of this paper",
            "task_name": "HotpotQA; 2WikiMultiHopQA",
            "task_category": "multi-hop reasoning / retrieval-augmented QA",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": true,
            "performance_metric": "Exact Match and token-level F1 (used across comparisons)",
            "tradeoffs_reported": "Single-time retrieval is simpler and cheaper than multi-time retrieval but insufficient for some multi-hop queries; paper motivates multi-time or metacognitive retrieval to address such shortcomings.",
            "limitations_or_failure_cases": "Single-time retrieval can fail on complex multi-hop questions that require iterative decomposition; susceptible to insufficient or conflicting external evidence because retrieval happens only once.",
            "citation": "",
            "uuid": "e6472.1",
            "source_info": {
                "paper_title": "Metacognitive Retrieval-Augmented Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct (reasoning + acting loop)",
            "brief_description": "A language-agent paradigm that interleaves chain-of-thought reasoning with environment actions (e.g., retrieval) forming a thought-action-observation loop to iteratively gather information and reason.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "ReAct",
            "agent_description": "Agent that alternates reasoning steps and actions (actions include retrieval or API calls) so the model can generate intermediate search queries / interactions and use observations to refine reasoning; in this paper used as a multi-time retrieval baseline.",
            "model_size": "Not specified in this paper (baseline behavior evaluated using gpt-35-turbo-16k in experiments)",
            "memory_used": true,
            "memory_type": "multi-time retrieval / interaction history (treated as external evidence gathered during the loop)",
            "memory_representation": "observations and retrieved passages (raw text) appended to context",
            "memory_access_mechanism": "uses generated intermediate content as dynamic queries; in the paper's experiments retrieval is done via BM25 + E5 retriever",
            "task_name": "HotpotQA; 2WikiMultiHopQA",
            "task_category": "multi-hop reasoning / iterative retrieval",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": true,
            "performance_metric": "Exact Match and token-level F1",
            "tradeoffs_reported": "Multi-time retrieval approaches like ReAct can improve on ambiguous or multi-hop queries but add complexity and potential extra latency compared to single-time retrieval; paper notes gains but exact tradeoffs depend on trigger policy.",
            "limitations_or_failure_cases": "Improvements are limited when knowledge conflicts or when reasoning errors persist despite additional retrieval; paper argues a metacognitive critic can better diagnose error causes than plain ReAct.",
            "uuid": "e6472.2",
            "source_info": {
                "paper_title": "Metacognitive Retrieval-Augmented Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Flare",
            "name_full": "Flare (active retrieval during generation)",
            "brief_description": "A method that actively retrieves information during answer generation using forward-looking sentences as dynamic search triggers to support multi-hop reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Flare",
            "agent_description": "Performs active retrieval while generating answers, using prospective sentences as retrieval triggers so the model can iteratively gather relevant evidence during multi-step generation.",
            "model_size": "Not specified in this paper (evaluated as baseline using the experimental setup with gpt-35-turbo-16k)",
            "memory_used": true,
            "memory_type": "multi-time retrieval (active retrieval during generation)",
            "memory_representation": "raw text passages from Wikipedia",
            "memory_access_mechanism": "dynamic queries formed during generation; in this paper, retrieval is instantiated via BM25 + E5 for comparability",
            "task_name": "HotpotQA; 2WikiMultiHopQA",
            "task_category": "multi-hop reasoning / retrieval",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": true,
            "performance_metric": "Exact Match and token-level F1",
            "tradeoffs_reported": "Active retrieval increases retrieval calls and latency but can provide missing facts for multi-hop chains; the paper positions MetaRAG as improving on active retrieval by diagnosing when further retrieval is needed.",
            "limitations_or_failure_cases": "May still follow pre-defined retrieval schedules and lacks the metacognitive diagnosis of why answers fail; vulnerable to retrieval of conflicting or distracting documents.",
            "uuid": "e6472.3",
            "source_info": {
                "paper_title": "Metacognitive Retrieval-Augmented Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "IR‑CoT",
            "name_full": "Interleaved Retrieval with Chain-of-Thought (IR-CoT)",
            "brief_description": "A baseline that interleaves retrieval operations with Chain-of-Thought reasoning to support multi-hop question answering by retrieving when intermediate steps require evidence.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "IR-CoT",
            "agent_description": "Interleaves retrieval and CoT reasoning: at certain reasoning steps the agent issues retrievals to obtain evidence that supports the chain of thought, evaluated here as a baseline under the same retriever/corpus.",
            "model_size": "Not specified in this paper (evaluated using gpt-35-turbo-16k as the cognition model)",
            "memory_used": true,
            "memory_type": "interleaved multi-time retrieval (external document memory)",
            "memory_representation": "raw text passages",
            "memory_access_mechanism": "retrieval triggered during reasoning steps; in experiments retrieval implemented by BM25 + E5",
            "task_name": "HotpotQA; 2WikiMultiHopQA",
            "task_category": "multi-hop reasoning / retrieval-augmented QA",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": true,
            "performance_metric": "Exact Match and token-level F1",
            "tradeoffs_reported": "Interleaving retrieval and CoT can increase retrieval calls and inference cost but helps incorporate evidence into multi-step chains; still can miss diagnosing knowledge conflicts.",
            "limitations_or_failure_cases": "The paper reports gains from interleaved retrieval but notes remaining failure modes (insufficient/conflicting knowledge, erroneous reasoning) which MetaRAG targets with metacognitive evaluation.",
            "uuid": "e6472.4",
            "source_info": {
                "paper_title": "Metacognitive Retrieval-Augmented Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Self‑Ask",
            "name_full": "Self-Ask (with intermediate question decomposition)",
            "brief_description": "An approach that decomposes complex questions into intermediate sub-questions and issues retrievals for those sub-questions to enable multi-step answering.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Self-Ask",
            "agent_description": "Generates intermediate sub-questions during reasoning and uses retrieved answers to these sub-questions to compose the final answer; used as a baseline in the paper's experiments with the shared retrieval/corpus setup.",
            "model_size": "Not specified in this paper (baseline behavior evaluated with gpt-35-turbo-16k as cognition LLM)",
            "memory_used": true,
            "memory_type": "multi-step retrieval (decomposition-driven retrieval)",
            "memory_representation": "raw text passages (Wikipedia)",
            "memory_access_mechanism": "retrieval calls issued for each sub-question; in experiments retrieval done via BM25 + E5",
            "task_name": "HotpotQA; 2WikiMultiHopQA",
            "task_category": "multi-hop reasoning / question decomposition + retrieval",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": true,
            "performance_metric": "Exact Match and token-level F1",
            "tradeoffs_reported": "Decomposition increases number of retrievals and latency but can make multi-hop chains more explicit; still susceptible to errors from incomplete decomposition or wrong retrieved evidence.",
            "limitations_or_failure_cases": "Performance depends on quality of decomposition and retrieved evidence; paper argues metacognitive planning helps identify missing sub-questions or insufficient evidence.",
            "uuid": "e6472.5",
            "source_info": {
                "paper_title": "Metacognitive Retrieval-Augmented Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion (language agents with verbal reinforcement learning / self-critic)",
            "brief_description": "A language-agent approach that uses a critic/evaluator and past linguistic feedback to iteratively improve agent outputs; included here as a baseline that uses a self-critic mechanism.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Reflexion",
            "agent_description": "An agent that records past attempts and employs a self-evaluator to provide feedback and guide subsequent attempts; in this paper Reflexion is a baseline that includes a critic component but lacks MetaRAG's explicit metacognitive diagnosis and planning for retrieval.",
            "model_size": "Not specified in this paper (baseline evaluated using gpt-35-turbo-16k for cognition)",
            "memory_used": true,
            "memory_type": "episodic/logged attempts (verbal memory of past attempts and feedback) and optionally retrieval when configured",
            "memory_representation": "stored previous attempts and textual feedback (logged text); in paper used as a baseline with retrieval enabled in some comparisons",
            "memory_access_mechanism": "read/write textual logs of past attempts; in the paper the baseline was run with retrieval and a critic component to compare with MetaRAG",
            "task_name": "HotpotQA; 2WikiMultiHopQA",
            "task_category": "multi-hop reasoning / iterative refinement",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": true,
            "performance_metric": "Exact Match and token-level F1",
            "tradeoffs_reported": "Reflexion's self-critic improves over non-critic baselines but MetaRAG shows larger gains by diagnosing causes (insufficient/conflicting knowledge vs reasoning errors); Reflexion's approach may be less targeted than metacognitive planning.",
            "limitations_or_failure_cases": "Self-criticism alone can detect errors but not always provide targeted retrieval or correction strategy; paper reports MetaRAG outperforms Reflexion substantially on both datasets.",
            "uuid": "e6472.6",
            "source_info": {
                "paper_title": "Metacognitive Retrieval-Augmented Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Training LM w/ Memory Aug.",
            "name_full": "Training language models with memory augmentation (referenced work)",
            "brief_description": "A referenced prior work (Zhong et al., EMNLP 2022) that studies augmenting language models with explicit memory mechanisms during training to improve long-range dependencies and retrieval of past tokens/contexts.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Memory-augmented LMs (Zhong et al. 2022)",
            "agent_description": "Prior research that trains language models augmented with an explicit memory component (paper referenced in related work); mentioned as background on memory augmentation techniques for LMs.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "memory augmentation (training-time memory mechanisms) — referenced (not implemented) in this paper",
            "memory_representation": null,
            "memory_access_mechanism": null,
            "task_name": null,
            "task_category": "memory-augmented LMs (general)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Referenced as prior work; this paper does not reproduce or report detailed tradeoffs from that work.",
            "limitations_or_failure_cases": "Not detailed in this paper (only cited in related work).",
            "citation": "",
            "uuid": "e6472.7",
            "source_info": {
                "paper_title": "Metacognitive Retrieval-Augmented Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Training language models with memory augmentation",
            "rating": 2,
            "sanitized_title": "training_language_models_with_memory_augmentation"
        },
        {
            "paper_title": "REALM: Retrieval-Augmented Language Model Pre-Training",
            "rating": 2,
            "sanitized_title": "realm_retrievalaugmented_language_model_pretraining"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Improving Language Models by Retrieving from Trillions of Tokens",
            "rating": 2,
            "sanitized_title": "improving_language_models_by_retrieving_from_trillions_of_tokens"
        },
        {
            "paper_title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
            "rating": 2,
            "sanitized_title": "replug_retrievalaugmented_blackbox_language_models"
        },
        {
            "paper_title": "Generalization through Memorization: Nearest Neighbor Language Models",
            "rating": 2,
            "sanitized_title": "generalization_through_memorization_nearest_neighbor_language_models"
        }
    ],
    "cost": 0.017474999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Metacognitive Retrieval-Augmented Large Language Models
18 Feb 2024</p>
<p>Yujia Zhou zhouyujia@ruc.edu.cn 
Zheng Liu zhengliu1026@gmail.com 
Jiajie Jin jinjiajie@ruc.edu.cn 
Jian-Yun Nie nie@iro.umontreal.ca 
Zhicheng Dou dou@ruc.edu.cn </p>
<p>School of Information
Renmin University of China
BeijingChina</p>
<p>Beijing Academy of Artificial Intelligence
BeijingChina</p>
<p>Gaoling School of Artificial Intelligence
Renmin University of China
BeijingChina</p>
<p>University de Montreal Quebec
Canada</p>
<p>Gaoling School of Artificial Intelligence
Renmin University of China
BeijingChina</p>
<p>Metacognitive Retrieval-Augmented Large Language Models
18 Feb 20242A3CDE507F4D28BCB0F6749858B2549710.1145/3589334.3645481arXiv:2402.11626v1[cs.CL]Retrieval-Augmented GenerationLLMsMetacognition
Retrieval-augmented generation have become central in natural language processing due to their efficacy in generating factual content.While traditional methods employ single-time retrieval, more recent approaches have shifted towards multi-time retrieval for multi-hop reasoning tasks.However, these strategies are bound by predefined reasoning steps, potentially leading to inaccuracies in response generation.This paper introduces MetaRAG, an approach that combines the retrieval-augmented generation process with metacognition.Drawing from cognitive psychology, metacognition allows an entity to self-reflect and critically evaluate its cognitive processes.By integrating this, MetaRAG enables the model to monitor, evaluate, and plan its response strategies, enhancing its introspective reasoning abilities.Through a three-step metacognitive regulation pipeline, the model can identify inadequacies in initial cognitive responses and fixes them.Empirical evaluations show that MetaRAG significantly outperforms existing methods.CCS CONCEPTS• Information systems → Question answering.</p>
<p>INTRODUCTION</p>
<p>Recently, large language models (LLMs) have emerged as a foundational component in various natural language processing tasks, attributed to their remarkable capability to comprehend and generate human-like language [24].While these models are endowed with vast repositories of knowledge learned during training, they exhibit the propensity to generate hallucinated content [22,41].To address this issue, researchers have introduced the idea of integrating retrieval systems into LLMs.By doing so, LLMs can look up relevance information from external knowledge bases, ensuring a more reliable and precise content generation.</p>
<p>Historically, retrieval-augmented language models [9][10][11]20] employed single-time retrieval, extracting knowledge once based on an initial query.This method, while effective for tasks with straightforward informational needs, falls short when faced with complex tasks demanding multi-step reasoning.Recognizing this limitation, recent researches have shifted towards a multi-time retrieval framework [1,15,21,26,34].This method doesn't confine knowledge retrieval to one instance but revisits it iteratively during the generation process, by decomposing the primary question into sub-questions [16], or leveraging partially generated content [25,39] and forward-looking sentences [12] as dynamic search queries.</p>
<p>Although previous methods have made strides in improving the quality of generated answers, they strictly adhere to predefined reasoning steps over all questions.Such inflexible approaches lack the ability to diagnose specific errors in their responses and consequently don't possess mechanisms to enhance their performance.We argue that this limitation might stem from the model's lack of awareness regarding its own reasoning processes.When humans confront complex issues, they often reflect on their thought patterns, gradually adjusting and optimizing their strategies.This ability comes from our innate metacognition, which enables introspection, self-assessment, and self-regulation.Inspired by it, we aim to integrate metacognitive ability into LLMs to enhance retrievalaugmented generation (RAG).By adopting this approach, the model is able to identify its own inaccuracies and dynamically adjust their reasoning strategies, leading to more precise answer generation.</p>
<p>Derived from the field of cognitive psychology [19,28], metacognition concerns an individual's capacity to self-reflect and critically The correspondence between the metacognitive processes in humans and retrieval-augmented LLMs evaluate their cognitive processes [36].As shown in the Figure 1(a), it can be classified into two integral components: metacognitive knowledge and metacognitive regulation.The former refers to an individual's self-awareness of their cognitive strengths, limitations, and mechanisms.On the other hand, metacognitive regulation [4] involves the active management and control of one's cognitive processes.Empowered by metacognitive capabilities, the human brain possesses the capacity to discern the underlying rationale behind responses and acquire the means for self-improvement.Drawing inspiration from human metacognitive processes, we introduce the Metacognitive Retrieval-Augmented Generation framework (MetaRAG).As illustrated in Figure 1(b), MetaRAG features a "cognition-metacognition" collaborative framework.The cognition component is responsible for deriving answers from the provided question and references, while the metacognitive component, acting as a critic model, delves deep into potential mistakes during reasoning.Upon conducting an analysis of model performance under different conditions of knowledge (as detailed in Sec.3.2), it has been observed that there are three main reasons causing the model fails to infer the correct answer: insufficient knowledge, conflicting knowledge, and erroneous reasoning.Endowed with the benefit of metacognitive mechanism, we expect the model to be aware of its own cognitive process in RAG tasks from two aspects: (1) The sufficiency and harmonization of external retrieved knowledge and LLM's intrinsic knowledge.(2) The reliability and accuracy of multi-hop reasoning.By doing so, the model is capable of identifying potential issues present in knowledge integration and answer reasoning, thereby enabling targeted improvements.</p>
<p>Specifically, we delineate the metacognitive process into three distinct steps in the context of retrieval-augmented LLMs.(1) Monitoring assesses the quality of the current response to determine whether there's a need to invoke the metacognitive evaluating.(2) Evaluating is to identify the reasons why the current answer may not meet the requirements.During this phase, the model leverages metacognitive knowledge to analyze the flaws in the response.This knowledge encompasses two main areas: declarative knowledge, which involves recognizing prevalent error patterns, and procedural knowledge, focusing on the utilization of methods to assess the sufficiency and harmonization of both internal and external knowledge.Based on this evaluation, results are categorized into four distinct scenarios.(3) Planning offers tailored suggestions for the cognitive component on potential improvements.For each of the aforementioned scenarios in the evaluating stage, distinct planning strategies are designed to enhance the original cognitive process.Collectively, these steps ensure that the model not only identifies inadequacies in its initial cognitive responses but also fixes them based on the metacognitive evaluating and planning.The experimental results on two multi-hop question answering (QA) datasets indicate that MetaRAG gains higher capabilities of reasoning and outperforms existing baselines significantly.</p>
<p>The contributions of this paper are summarized as: (1) We introduce a metacognitive retrieval-augmented generation framework that integrates LLMs with human introspective reasoning for multihop QA tasks.(2) Through empirical analysis, we summarize three primary challenges in multi-hop QA causing wrong answers: insufficient knowledge, conflicting knowledge, and erroneous reasoning.</p>
<p>(3) We devise a three-step metacognitive regulation pipeline tailored for retrieval-augmented LLMs, offering a systematic way for models to assess, diagnose, and refine the original cognitive process.</p>
<p>RELATED WORK</p>
<p>The development of retrieval-augmented language models has been a central theme in recent research endeavors.Their aim is to harmoniously marry the static knowledge encapsulated within the language model to the dynamic wealth of information on the web.The development of these models can be bifurcated into two primary phases: single-time retrieval and multi-time retrieval.</p>
<p>Single-time Retrieval.Initial endeavors in retrieval-augmented language models predominantly embraced the single-time retrieval strategy [6,9,20,40].In this framework, a single-time extraction of knowledge was performed in response to the user's initial query.Various methods were conceived to incorporate this external knowledge retrieval.For instance, Guu et al. [5] introduced a language model that incorporated latent knowledge retrieval during pretraining, whereas Ram et al. [26] chose to keep the core LM architecture untouched, simply appending grounding documents to its input.Meanwhile, Shi et al. [30] perceived the language model as an inscrutable entity, complementing it with an externally trainable retrieval module.Such strategies showcased remarkable efficacy for tasks that demanded straightforward information, like factoid question answering [18] and fact verification [32].However, their applicability waned for intricate tasks demanding multi-hop reasoning, as the single-time retrieval lacked the depth to decode the subtleties embedded in complex inquiries.</p>
<p>Multi-time Retrieval.To counter the shortcomings of the single-time retrieval paradigm, the spotlight shifted towards the development of multi-time retrieval models.This paradigm champions an iterative knowledge extraction process throughout content generation.Some approaches [15,23,34] are designed to passively harness past contexts, conducting retrievals at predetermined intervals.Others [17,39] deconstruct a multifaceted query into a series of simpler sub-queries, each necessitating its distinct retrieval operation.Furthermore, the intrinsic capabilities of the latest LLMs have been harnessed to autonomously dictate the timing and content of retrievals.For instance, Press et al. [25] leverages the model's partially generated content as evolving search queries, allowing it to iteratively refine its search.Meanwhile, Jiang et al. [12] strategically uses prospective sentences as dynamic search triggers.The ReAct model [39] ingeniously fuses a Chain-of-Thought (CoT) rationale with action in a seamless thought-action-observation loop.Other innovative approaches [29,31] embed introspective mechanisms that iteratively refine the model's outputs.This iterative retrieval approach proves effective for queries with inherent ambiguities [42] or those demanding a synthesis of diverse information sources.In contrast to the aforementioned studies, this paper conducts an exploration of The fundamental reasons for causing the model to answer incorrectly in RAG.Drawing inspiration from the domain of cognitive psychology, we integrate metacognitive ability into LLMs to enable the model to be aware of its reasoning process, thus enhancing the quality of answer generation.</p>
<p>PRELIMINARY</p>
<p>In this section, we formulate the task of retrieval-augmented generation and investigate its limitations on multi-hop QA.</p>
<p>Task Definition</p>
<p>Given a question  and a retrieval corpus  = {  } | | =1 (with Wikipedia articles serving as the primary data source in this study), the goal of retrieval-augmented LLMs is to generate an answer  based on the question as well as the documents retrieved in relation to it.This can be represented as:
𝑦 = LLM QA ([𝐷 𝑞 , 𝑞], Prompt QA ),(1)
Prompt QA :</p>
<p>Please act as a question-answering system, answer the {question} based on the {retrieved documents} where   is the retrieved documents for the query .[•, •] is concatenation following designated prompts.LLM QA is the role of the LLM, which concentrates on question answering tasks.</p>
<p>Task Exploration</p>
<p>We conduct an empirical study to evaluate the effectiveness of retrieval-augmented LLMs under various knowledge conditions.Our main aim is to ascertain whether a question can be answered utilizing either the intrinsic knowledge within the LLM or via externally retrieved documents.Through human annotation of (a) the quality of closebook answers from the LLM and (b) the knowledge completeness of retrieved documents, we are able to categorize questions into four distinct conditions:</p>
<p>• No knowledge.Neither the LLM nor retrieved documents can provide an answer correctly.• Only external.Answers can be found in retrieved documents, but not directly from the LLM.• Only internal.The LLM can answer the question directly, but external documents cannot provide the solution.• Both internal and external.The question can be addressed either directly by the LLM or through retrieved documents.</p>
<p>For comparison, both standard RAG [2] and ReAct [39] are put to the test on sampled 100 questions from HotpotQA dataset.</p>
<p>From Figure 2, Our study yields several insights: (1) When the model operates without any knowledge, it faces difficulty in generating accurate responses.(2) When the model is based only on either internal or external knowledge, there's a noticeable accuracy improvement.However, conflicts in the knowledge limit the model's ability to answer questions correctly.(3) In situations where both internal and external knowledge sources can tackle the question at hand, there's a marked improvement in the model's accuracy.Yet, it still isn't flawless.This suggests that even with complete knowledge, the model can err due to incorrect reasoning.These insights highlight three primary challenges in multi-hop QA when the model fails to answer correctly: insufficient knowledge, conflicting knowledge, and erroneous reasoning.In subsequent sections, we will delve deeper into how metacognitive strategies can help overcome these challenges.</p>
<p>METACOGNITIVE RAG</p>
<p>Retrieval augmentation has become one of the primary methods to mitigate the hallucination issues in LLMs.However, existing research on retrieval-augmented LLMs are bound by predefined reasoning steps, lacking the ability to diagnose specific errors in their responses.Motivated by this observation, in this section, we introduces a metacognitive retrieval-augmented generation framework.This approach taps into the principles of metacognition, allowing for introspection of the cognitive process.By doing so, it identifies shortcomings in the reasoning process and aims to enhance the accuracy of answer derivation.</p>
<p>The overall framework of MetaRAG is depicted in Figure 3. MetaRAG comprises two spaces: the cognition space and the metacognition space.The former functions as a QA system, while the latter serves as both an evaluator and critic, introspecting the reasoning process.This metacognition space primarily encompasses three main phases: (1) Monitoring; (2) Evaluating; (3) Planning.The following sections introduce the details of these three steps.</p>
<p>Monitoring: Assessing Answer Satisfaction</p>
<p>The primary function of monitoring is to keep track of one's cognitive processes.In human brain, not all cognitive activities necessarily trigger metacognitive evaluating [19].Typically, only when the problem becomes so complex that the correctness of the cognitive process cannot be guaranteed, it becomes necessary to "think thrice before answering".In multi-hop QA tasks, due to the complexity of the task or insufficient knowledge, retrieval-augmented LLMs sometimes fail to reason out the correct answer.The role of monitoring The green part is the cognition space, focusing on reasoning the answer for a given question.In contrast, the blue part is the metacognition space, responsible for monitoring, evaluating, and planning.</p>
<p>is to assess the satisfaction of the answer, which then determines whether to activate the metacognitive evaluating phase.To investigate the conditions under which an answer is deemed satisfactory, we hypothesize that an answer is highly plausible when the cognition of the LLM aligns with the cognition of an expert model.Conversely, certain deviation necessitates the intervention of metacognition.With this in mind, we select an expert model on QA tasks to evaluate the satisfaction level of the answers produced.Specifically, given a question , retrieved documents   , we first prompt the expert model  to generate an answer:
𝑦 ′ = 𝑀 𝜙 ([𝐷 𝑞 , 𝑞]), (2)
where  is the parameters of the model .Next, we decide on the model's subsequent action by computing the similarity between the LLM outputs  and the expert model's outputs  ′ .The decision of the next action is defined as:
Action = Activate evaluating stage if ⟨ ì 𝑋 𝑦 , ì 𝑋 𝑦 ′ ⟩ &lt; k, Output the answer otherwise.
Here,  serves as a threshold value governing the model's behavior.A higher value of  implies that a greater number of reasoning processes require metacognitive evaluation.ì  represents embeddings encoded by an encoder (e.g.BERT Encoder), and ⟨, ⟩ is the similarity function, implemented by cosine similarity.In cases where the similarity between the LLM output and the expert model output falls below a certain threshold, the expert model triggers metacognitive process, including metacognitive evaluating and planning.</p>
<p>Evaluating: Identifying Answer Limitations</p>
<p>When a monitor discerns that an answer fails to fully address a question, it triggers the metacognitive process of evaluating.This introspective exercise is geared towards identifying the shortcomings of the provided response and discerning why the model may have faltered in its reasoning.Central to this introspection are two pivotal questions: (a) Are both internal and external sources of knowledge sufficient to tackle the posed question?and (b) Is the reasoning process of the QA LLM susceptible to common issues often encountered in multi-hop QA?</p>
<p>To address these concerns, the evaluating step employs two types of metacognitive knowledge: procedural knowledge and declarative knowledge.Within cognitive psychology [19,28], procedural knowledge embodies the grasp of methodologies and strategies essential for confronting specific tasks, while declarative knowledge is anchored in specific facts or content-based information, covering facts and concepts associated with problem solving.For RAG task, we convert the role of LLM from the original question answering system to an evaluator-critic system.Different from the question-answering perspective which forces the model to generate an answer, evaluator-critic perspective can more objectively judge the limitations of its reasoning process towards the answer.</p>
<p>Procedural Knowledge.This domain of knowledge is crucial for examining the sufficiency of both the internal and external knowledge for a given question.To address question (a), we propose model-based methods to evaluating the answer automatically, simulating human annotators in Sec.3.2.We leverage the evaluator-critic LLM to gauge the adequacy of its internal knowledge.Meanwhile, a natural language inference (NLI) model is employed to measure the sufficiency of external knowledge.Note that this process may be affected by the accuracy of the LLM and the NLI model, but can be replaced by any better model in the future.</p>
<p>• Internal Knowledge Evaluating: We capitalize on the inherent capacity of the LLM to determine if a question can be aptly answered using its built-in knowledge.To do this, we present the question  to the evaluator-critic LLM, which functions as an evaluator here and offers a binary outcome based on:
LLM Eval-Critic (𝑞, Prompt Eval ),(3)
Prompt Eval :</p>
<p>Please act as an evaluator-critic system, determine if you can provide a reliable answer to the {question} based on your own knowledge?</p>
<p>• External Knowledge Evaluating: To gauge the adequacy of external knowledge sources, we deploy an advanced NLI model TRUE [8] to examine if the retrieved documents, represented as   , provide enough information to answer the question.The process is formulated as:
𝑓 {𝑑 𝑖 } |𝐷 | 𝑖=1 , 𝑞 ,(4)
with  (premise, hypothesis) being the function of the NLI model.It returns a value of 1 if the premise entails the hypothesis, otherwise, it returns 0. Upon evaluating through the aforementioned model-based evaluating methods, we can classify the situation into four categories (as in Sec.3.2) in an automatic manner.Each situation highlights specific potential sources of errors, leading to varying strategies employed for future planning depending on the identified category.</p>
<p>Declarative Knowledge.Addressing question (b), declarative knowledge within MetaRAG is directed towards identifying prevalent error patterns.This aids in identifying possible pitfalls in the reasoning process.We categorize typical mistakes into three types:</p>
<p>• Incomplete Reasoning: This error is the most prevalent in multihop QA.It arises when the model fails to utilize all relevant fragments from the given context or does not follow a comprehensive chain of thought to arrive at the correct answer.• Answer Redundance: This pertains to instances where the model delivers an overly verbose or repetitious answer.Such redundancy can arise when the model identifies multiple analogous data points but cannot consolidate them effectively.• Ambiguity Understanding: This error manifests when the model misunderstands the nuances within a query, leading it to generate answers based on related but incorrect references.</p>
<p>Relying on declarative knowledge (DK), we invoke the critic functionality of the evaluator-critic LLM to determine if the proposed answer falls prey to any of these errors.For each error type, we furnish a description and several examples in the format {Error name -Error description -Examples}.Subsequently, the question , documents   , and answer  are fed into the LLM, functioning as a critic in this context: LLM Eval-Critic ([DK, ,   , ], Prompt Critic ).</p>
<p>(
)5
Prompt Critic :</p>
<p>Please act as an evaluator-critic system, assess whether the {response} based on {references} for the {question} contains any {error types}?</p>
<p>Through the evaluating phase, the model gains an understanding of potential issues within the current answer, which may stem from gaps in knowledge or deficiencies in the reasoning process.Once these challenges are identified, the model can then develop customized solutions to enhance the precision of its reasoning in the context of question-answering.We detail this as follows.</p>
<p>Planning: Strategizing Answer Refinement</p>
<p>In metacognition, the concept of planning refers to the effective regulation of the original cognitive process, guided by the results obtained from the evaluation stage.Empirical studies in Section 3.2 have illuminated three primary challenges in multi-hop QA when the model fails to provide accurate answers: insufficient knowledge, conflicting knowledge, and erroneous reasoning.After identifying the issues during the evaluating stage, in this section, we will introduce planning strategies to address each of these challenges.</p>
<p>Insufficient Knowledge.In the first condition, there is a lack of both internal and external knowledge to answer the current question.When the evaluator-critic LLM recognizes this situation, it's prompted to generate a new query to further retrieve information from the corpus.A well-formulated follow-up query should have two characteristics: (1) It should differ from the original inquiry to specifically target missing information.(2) It should break down the original question into a more specific sub-question.Specifically, given a question , the existing retrieved documents   , and an answer , we utilize the evaluator-critic LLM's introspective ability to deduce what external knowledge is still lacking:
𝑞 ′ = LLM Eval-Critic ([𝑞, 𝐷 𝑞 , 𝑦], Prompt QG ),(6)
where Prompt QG is an instruction that encourages the LLM to ask a new query with "To answer this question, I further need to search { ′ }".With this new query  ′ , the model conducts another search to obtain additional documents.These newly retrieved documents are then incorporated into the reference list as new external knowledge.</p>
<p>Conflicting Knowledge.Another situation that can result in inaccurate responses is when there's a disparity between internal and external knowledge.When one subset of knowledge is sufficient to answer a question, but another isn't, the model might become confused due to the inconsistency between the two.This scenario can be classified into following two cases: • Only Internal Knowledge Available.When the model is capable of providing the correct answer directly, external references may serve as distractors.To mitigate this, it's advisable for the model to discard external references and rely on its intrinsic knowledge.We achieve this by altering the question-answering prompt, guiding the model to rely solely on its internal knowledge.• Only External Knowledge Available.Conversely, in situations where only external knowledge is present, LLMs can be prone to hallucinations if they mistakenly believe they know the answer.To circumvent this, we ask the LLM to only rely on the provided references for its response.</p>
<p>Erroneous Reasoning.Even if a model can answer questions consistently using both internal and external knowledge, errors may still occur during multi-step reasoning.To address the issue of faulty reasoning, we propose improvements from two perspectives: • Double-Checking the Reasoning Process.First, we aim to verify that each statement in our reasoning process is backed by evidence.To achieve this, we invoke the NLI model  to assess the groundedness of each statement   in the LLM output .This will help determine which statements are supported by external references   = { 1 ,  2 , ...} and which ones aren't.Finally, the statements need to be double-checked are:
𝑆 DC = 𝑠 𝑖 |𝑓 {𝑑 𝑖 } |𝐷 | 𝑖=1 , 𝑠 𝑖 = 0 .(7)
For any statement that lacks evidence in  DC , we request the LLM to re-evaluate its correctness, ensuring that the LLM excludes any statement that doesn't meet its confidence threshold.</p>
<p>• Providing Suggestions.In response to the common errors identified during the evaluating phase, we ask the evaluator-critic LLM to provide specific suggestions for the question-answering LLM based on the specific error type by " Please generate a statement that offers suggestions to prevent the occurrence of the {error type} in future reasoning processes".These suggestions serve as guidance during the next round of answer reasoning.It's worth noting that if no common errors are detected, we set a default suggestion "Please think step by step." to guide its reasoning process.</p>
<p>The planning at this stage primarily focuses on systematically reducing the model's error rate when operating under conditions of comprehensive and consistent knowledge.</p>
<p>EXPERIMENTAL SETUP 5.1 Datasets and Evaluation Metrics</p>
<p>To test the ability of our proposed method on multi-hop reasoning, we conduct experiments on two multi-hop question answering datasets: HotpotQA [38] and 2WikiMultiHopQA [7].These two datasets are all constructed based on Wikipedia documents, allowing us to use the consistent document corpus and retrievers to provide external references for LLMs.Considering the constraints of experimental costs, following [12], we sub-sample 500 questions from the validation set of each dataset for experiments.For evaluation metrics, at answer-level, we use exact match (EM) to test whether the prediction is consistent with the reference answer.At token-level, following [12], we use token-level F1, precision (Prec.)and recall (Rec.) for comprehensive evaluation.</p>
<p>Baselines</p>
<p>For comparison, we choose two closebook models and six retrievalaugmented models as baselines.Standard Prompting [2] directs the LLM to respond to queries.Chain-of-Thought [37] furnishes LLM with examples inclusive of reasoning processes to encourage more thoughtful reasoning.Standard RAG [20] employs the query to retrieve multiple documents, and inputs them into LLM for deriving answers.ReAct [39] proposes synergizing reasoning and acting in language models.Flare [12] employs active retrieval during the answer generation process.IR-CoT [34] Interleaves retrieval with CoT Reasoning for multi-hop QA.Self-Ask [25] integrates intermediate steps to assist in deliberating on complex issues.Reflexion [31] incorporates an evaluator to reinforce language agents through linguistic feedback.To ensure a balanced comparison among all baselines, uniform settings are maintained across all models, including identical in-context demonstrations, prompt formats, retrievers, and document corpora.</p>
<p>Implementation Details</p>
<p>In cognition process, we choose the cutting-edge gpt-35-turbo-16k LLM by querying its API iteratively with a temperature setting of 0. Since both datasets predominantly depend on knowledge from Wikipedia, we utilize the Wikipedia dump [14] to serve as the document corpus, where articles are segmented into passages of 100 tokens.The retrieval of relevant documents from this corpus employs the BM25 algorithm [27] and E5 retriever [35], selecting the top 5 passages to serve as the external knowledge.</p>
<p>Transitioning to the metacognition process, we leverage a finetuned T5-large model which acts as our expert monitoring model.The efficacy of similarity calculations is based on a repository of sentence transformers.We set a default judgment threshold for our monitoring mechanism at 0.4 to ensure precision.The maximum number of iterations is set to 5. The NLI model used in evaluating and planning is entrusted to a T5-XXL model.The code of MetaRAG is available on https://github.com/ignorejjj/MetaRAG.</p>
<p>RESULTS AND ANALYSIS 6.1 Main Results</p>
<p>The main results are shown in Table 1.It can be observed that:</p>
<p>(1) Our proposed MetaRAG consistently surpasses all baseline methods across two datasets.When compared to Reflexion, which also integrates a self-critic mechanism in its reasoning process, MetaRAG demonstrates a substantial improvement on all metrics.This suggests that using a metacognitive strategy is more beneficial than merely relying on self-criticism.By leveraging metacognitive knowledge and regulation, our method aligns better with human thought.This allows for a better identification of errors or gaps in knowledge during reasoning, leading to enhanced answer accuracy.</p>
<p>(2) Models equipped with a self-critic mechanism demonstrate superior performance compared to those without it.This indicates that by assigning a critic role to LLMs, they gain the ability to assess the quality of their own responses from a different perspective.MetaRAG further considers the conditions of knowledge and the accuracy of multi-hop reasoning, allowing it not only to pinpoint mistakes but also to identify the cause of these mistakes.</p>
<p>(3) Upon comparing two datasets, we observe a more significant improvement with MetaRAG on 2WikiMultihopQA than on HotpotQA, boosting performance by 34.6% and 26.0%respectively when compared to the baseline model Reflexion.Upon closer examination of the datasets, we note that the 2WikiMultihopQA set exhibits a higher proportion of conflicting knowledge, meaning there is a higher incidence where the retriever retrieves information that is inconsistent with the knowledge contained within the LLM.MetaRAG adeptly addresses this by meticulously formulating planning strategies based on varying conditions of knowledge, enhancing the precision of reasoning in a targeted manner.</p>
<p>The Study of Monitoring Phase</p>
<p>In order to understand the impact of monitor on the overall framework, we conduct two experiments by comparing different monitoring models and the similarity threshold .</p>
<p>Monitoring Models Variation.We evaluate the impact of using various expert models, which determine whether activating metacognition, as monitors.We primarily focus on two categories of models for monitoring: large language models and fine-tuned QA models.LLMs, like LLaMA2-chat [33] and ChatGLM2 [3], offer notable zero-shot capabilities, allowing them to assess answer quality.The fine-tuned QA models SpanBERT-large [13] and T5-large are smaller but have been specifically trained on particular datasets.We compare their parameter sizes and performance.</p>
<p>As illustrated in Table 2, utilizing fine-tuned QA models as the expert model during the monitoring phase yields superior performance compared to large language models.This suggests that Table 1: Evaluation results on two multi-hop question answering datasets.✓ and − indicates reasoning with and without the retrieval (Retr.),multi-time retrieval (Multi.),and critic component." †" denotes the result outperforms baseline models in t-test at &lt;0.05 level.The best results are in bold and the second best results are underlined.fine-tuned QA models can offer more precise feedback with fewer parameters, meeting the efficiency and effectiveness requirements of the monitoring stage.The performance of LLaMA2-chat and ChatGLM2 indicates that using LLMs to self-supervise is a feasible approach, setting higher standards for model capabilities.Moreover, the T5-large slightly outperforms SpanBERT-large, which might be attributed to the fact that generative models with larger parameter capacities are more apt for this task than extraction-based models.Different similarity thresholds.Secondly, we focus on the relationship between the similarity threshold  in the monitor and overall performance.This threshold dictates the ease of triggering metacognitive processes.A higher threshold implies a higher likelihood for activating metacognitive evaluating.We test the range of  from 0.2 to 0.8, incrementing by 0.1, and report the metacognitive proportion and answer quality on 2WikiMultihopQA.</p>
<p>Method</p>
<p>As depicted in Figure 4(a), when the threshold is set to 0.2, roughly 15% of questions are directed to the evaluator-critic LLM for metacognitive reasoning (green bars).At this point, there is approximately a 20% improvement compared to Reflexion.As the threshold increases, the proportion of questions requiring metacognitive reasoning steadily increases.By the time the threshold reaches 0.8, this arrives to 84%.Interestingly, the performance doesn't increase linearly.The model performs best when the threshold is set at 0.4.This suggests that not all questions benefit from metacognitive reasoning.For some straightforward inquiries, overthinking can be counterproductive.This mirrors human tendencies: going with one's intuition can be more effective than over-analyzing.</p>
<p>Ablation Studies on Meta-knowledge</p>
<p>The metacognitive evaluating employs metacognitive knowledge (declarative and procedural knowledge) to pinpoint potential mistakes in the reasoning and assessing the completeness of internal and external knowledge.To explore the necessity of these two categories of metacognitive knowledge, we conduct ablation studies by eliminating the assessment of internal or external knowledge for procedural knowledge or exclude a type of common error assessment linked to declarative knowledge.The findings, depicted in Table 3, highlight that stripping away any facet of metacognitive knowledge detrimentally impacts performance across all evaluation metrics.Among these, the omission of procedural knowledge results in the most pronounced decline in model efficiency.This suggests a heightened importance of understanding the interplay between internal and external knowledge,   as this understanding is crucial for the model's strategic planning.Within the procedural knowledge category, it's evident that recognizing external knowledge stands out in terms of importance.This underscores the idea that many questions arise from an insufficiency in external knowledge.Thanks to the architecture of MetaRAG, this deficiency can be alleviated, leading the model to generate new queries for knowledge acquisition.When we turn our focus to declarative knowledge, each type of common error bears significance to the overall model efficacy.Notably, errors stemming from incomplete reasoning seem to be most impactful.This implies that conventional QA prompts cannot effectively harness the multihop reasoning abilities of LLMs.However, with the inclusion of metacognitive knowledge, this latent potential can be harnessed, thereby refining the model's reasoning precision.</p>
<p>Performance of Each Planning Strategies</p>
<p>During the planning phase, we employ improvement strategies for three distinct scenarios: insufficient knowledge, conflicting knowledge, and erroneous reasoning.To validate the effectiveness of these strategies, we conduct experiments to measure the enhancements each scenario could offer.We categorize all questions into three scenarios based on the conditions of knowledge, and examine the impact of the planning approach on each scenario.Figure 5 shows the performance of various models in the three scenarios.Generally, as the richness of knowledge increases, the accuracy of each model improves.The ReAct and Reflexion models enhance the performance in situations of insufficient knowledge through employing multi-time retrieval and critic mechanisms.However, the improvement in scenarios of conflicting knowledge and complete knowledge is relatively marginal.In contrast to these two methods, our proposed MetaRAG significantly boosts the accuracy of reasoning in these two scenarios.This achievement is primarily attributed to MetaRAG's meticulous analysis of conflicts between internal and external knowledge and common error types through a metacognitive process, thereby optimizing the model's reasoning process in a targeted manner.</p>
<p>Exploration of the Number of Iterations</p>
<p>In the context of MetaRAG, monitoring is crucial in determining whether to proceed to the next stage of the metacognitive process.It's important to emphasize that the results are significantly influenced by the maximum number of iterations.To identify the ideal number, we systematically increase the maximum iteration count from 1 to 6, while closely observing how the accuracy changes.As depicted in Figure 4(b), the accuracy of MetaRAG improves progressively as the maximum iteration count increases.However, once the iteration count reaches 5, the performance peaks, indicating that deeper metacognitive reflection can indeed enhance inference accuracy.Nevertheless, excessively increasing the number of reflection rounds leads to a slight decline in results.This could be attributed to the model's diminishing ability to extract more useful information or suggestions through the metacognitive mechanism.An intriguing observation is a minor accuracy peak at an iteration count of 2. This phenomenon primarily arises from the characteristics of the 2WikiMultihopQA dataset, where the majority of questions require references from two sources.Two rounds of metacognitive reflection prove sufficient to gather the necessary knowledge for these questions.Beyond that, additional rounds of reflection tend to introduce noise, resulting in fluctuating results.</p>
<p>CONCLUSION</p>
<p>In this paper, we proposed MetaRAG, a novel framework combining the retrieval-augmented LLMs process with human-inspired metacognition to enhance multi-hop reasoning.Through a structured metacognitive process involving monitoring, evaluating, and planning stages, MetaRAG facilitates model awareness on its own reasoning process.This empowers the model to identify the sufficiency of knowledge and potential mistakes during reasoning.Experimental results on two multi-hop QA datasets demonstrated the superior performance of MetaRAG over existing baselines.In the future, we aspire to incorporate more human cognitive approaches, including emotional understanding, intuition, and cultural awareness, into the reasoning process of LLM.</p>
<p>APPENDIX A IMPLEMENTATION DETAILS A.1 Model Selection</p>
<p>A.2 Categorize the Question</p>
<p>During the task exploration phase, we employed a human annotation approach to categorize the questions.We recruited 10 graduate students specializing in information retrieval.For each question, we provided five retrieved documents and one closebook answer generated by an LLM.The annotators were tasked with assessing:</p>
<p>• Quality of Closebook Answers from the LLM: They determined whether the LLM's closebook answer could satisfactorily address the question (can/cannot).• Knowledge Completeness of Retrieved Documents: They evaluated whether the retrieved documents contained sufficient information to answer the question (can/cannot).Based on these assessments, we used a voting mechanism among the annotators to categorize each question into one of the four types outlined in Section 3.2.In the actual reasoning phase, the categorization is supported by the evaluator LLM and NLI model.The evaluator LLM assesses the internal knowledge (i.e., the LLM's own understanding and the quality of its closebook answer), while the NLI model evaluates the external knowledge.This dual evaluation helps determine whether the LLM has the necessary internal knowledge and whether the external information is adequate to formulate a correct response.</p>
<p>B THE ROLE OF EXPERT MODEL</p>
<p>• Role of the Expert Model: The expert model in our system is not used as a provider of ground-truth answers but rather as a benchmark for monitoring the consistency and potential correctness of answers.In our experiments, we found that no single model, including the expert model, can consistently deliver perfect answers.However, we observed that when the responses of ChatGPT and the fine-tuned expert model align closely, the likelihood of the answer being correct increases.This alignment serves as a crucial indicator to assess whether an answer is likely to be accurate or requires further refinement.• MetaRAG's Performance Relative to the Expert Model: In Section 6.2, we conducted tests to evaluate the impact of using expert models of varying quality.Our findings indicate that the performance of MetaRAG is indeed influenced by the quality of the expert model.A higher-quality expert model tends to enhance the effectiveness of MetaRAG.However, it's important to note that MetaRAG's functionality extends beyond merely replicating the expert model's answers.Instead, MetaRAG leverages the expert model as part of its metacognitive process to evaluate and potentially correct its responses, thereby adding a layer of self-awareness and adaptability to the answer generation process.</p>
<p>C PROMPT DESIGNING</p>
<p>In designing prompts for our study, we adhered to the following three principles to ensure both effectiveness and fairness:</p>
<p>• Fairness: We maintained consistency in prompts for similar components across baselines.This approach ensures that any observed differences in performance are attributable to the models themselves rather than the prompts.• Robustness: We implemented demonstrations and a Chain of Thought (CoT) voting strategy.Demonstrations ensure that the output format aligns with our expectations, while the CoT voting strategy enhances the stability and consistency of model outputs.• Simplicity: In designing prompts for the Evaluator-critic LLM, we aimed for brevity to avoid biases introduced by intricate prompt engineering.Excluding demonstrations, our prompts have an average length of 23 words.This focus on simplicity is to highlight the effectiveness of the metacognitive mechanism independently of detailed prompt design.</p>
<p>D COMPLEXITY AND COST ANALYSIS</p>
<p>We have conducted an in-depth analysis focusing on two critical hyperparameters that influence the balance between performance and computational cost in our proposed MetaRAG method: the Threshold for Metacognitive Evaluation, which determines whether to initiate metacognitive reasoning based on a certain confidence level, and the Maximum Iteration Rounds, which controls the maximum number of metacognitive reasoning iterations the model can perform.We conducted experiments on the 2WikiMultiHop dataset to assess the tradeoffs.The results are as follows: The results demonstrate that MetaRAG achieves superior performance within a comparable inference time.When higher performance is required, adjusting the threshold and the maximum iteration rounds can effectively enhance the response quality.</p>
<p>Figure 1 :
1
Figure 1: The correspondence between the metacognitive processes in humans and retrieval-augmented LLMs</p>
<p>Figure 2 :
2
Figure 2: Comparisons of single-and multi-time retrievalaugmented LLMs under different conditions of knowledge.</p>
<p>Figure 3 :
3
Figure3: The overall architecture of MetaRAG.The green part is the cognition space, focusing on reasoning the answer for a given question.In contrast, the blue part is the metacognition space, responsible for monitoring, evaluating, and planning.</p>
<p>Figure 4 :
4
Figure 4: Performance with different similarity thresholds and the number of iterations on 2WikiMultihopQA.</p>
<p>Figure 5 :
5
Figure 5: Performance of the planning strategy under each knowledge conditions: insufficient, conflicting, sufficient.</p>
<p>Table 2 :
2
† 49.9 † 52.1 † 50.9 † 42.8 † 50.8 † 50.7 † 52.2 † The comparison of various monitoring expert models with different parameter size (Param.)on 2WikiMultihopQA.
Retr.Multi.CriticHotpotQA2WikiMultihopQAEMF1Prec.Rec.EMF1Prec.Rec.Without retrieval (Closebook)Standard Prompting---20.025.826.428.921.625.724.531.8Chain-of-Thought---22.434.233.946.027.637.435.844.3With retrieval (BM25+E5)Standard RAG✓--24.633.034.134.518.825.225.626.2ReAct✓✓-24.841.742.644.721.028.027.630.0Flare✓✓-29.242.442.843.028.239.840.040.8IR-CoT✓✓-31.440.341.641.230.842.642.340.9Self-Ask✓✓-28.243.143.444.828.637.536.542.8Reflexion✓✓✓30.043.443.244.331.841.740.644.2MetaRAG (ours) 37.8  Expert model ✓ ✓ ✓ Param. EM F1 Prec. Rec.Large Language ModelsLLaMA2-chat13B40.447.647.648.8ChatGLM26B39.848.848.550.5Fine-tuned QA ModelsSpanBert-large0.34B42.050.450.351.8T5-large0.77B42.850.850.752.2</p>
<p>Table 3 :
3
Ablation Studies on Meta-knowledge.Co. is the consistency between model evaluating and human annotation.
Expert modelEMF1Prec.Rec.Procedural Knowledge (Internal Co.=0.76; External Co.=0.84)w/o. Internal41.449.549.650.5w/o. External37.444.945.145.9w/o. All30.636.837.237.3Declarative Knowledgew/o. Incomplete41.249.749.851.0w/o. Redundance41.649.349.350.6w/o. Ambiguity41.250.951.051.9w/o. All40.649.249.350.5MetaRAG42.850.850.752.2</p>
<p>Table 4 :
4
Results Based on Threshold for Metacognition.
ModelsEM F1 Prec. Rec. Threshold Time(s)ReAct21.0 28.0 27.630.0-5.36Self-Ask28.6 37.5 36.542.8-6.34MetaRAG 38.0 45.0 45.245.80.25.37MetaRAG 42.8 50.8 50.752.20.48.06MetaRAG 42.0 50.0 49.951.60.68.65MetaRAG 41.4 49.3 49.450.60.811.81</p>
<p>Table 5 :
5
Results Based on Maximum Iteration Rounds.
ModelsEM F1 Prec. Rec. Max_iter Time(s)ReAct21.0 28.0 27.630.0-5.36Self-Ask28.6 37.5 36.542.8-6.34MetaRAG 42.8 50.8 50.752.228.06MetaRAG 42.6 50.3 50.251.438.73MetaRAG 43.2 50.7 50.751.6411.27MetaRAG 43.4 51.7 51.853.0512.92
ACKNOWLEDGMENTSZheng Liu and Zhicheng Dou are the corresponding authors.This work was supported by National Key R&amp;D Program of China No. 2022ZD0120103, National Natural Science Foundation of China No. 62272467, Beijing Natural Science Foundation No. L233008, the fund for building world-class universities (disciplines) of Renmin University of China, Public Computing Cloud, Renmin University of China.The work was partially done at the Engineering Research Center of Next-Generation Intelligent Search and Recommendation, Ministry of Education, and Beijing Key Laboratory of Big Data Management and Analysis Methods.Case StudyQuestion: Which film has the director who died later, Say It With Flowers or Boot Hill Bandits?Gold Answer: Boot Hill Bandits
Improving Language Models by Retrieving from Trillions of Tokens. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las, Aurelia Casas, Jacob Guy, Roman Menick, Tom Ring, Saffron Hennigan, Loren Huang, Chris Maggiore, Albin Jones, Andy Cassirer, Michela Brock, Geoffrey Paganini, Oriol Irving, Simon Vinyals, Osindero, ICML (Proceedings of Machine Learning Research). Karen Simonyan, Jack W Rae, Erich Elsen, Laurent Sifre, 2022162</p>
<p>Language Models are Few-Shot Learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Ilya Sutskever, and Dario Amodei. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford2020NeurIPS</p>
<p>GLM: General Language Model Pretraining with Autoregressive Blank Infilling. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Development of Critical Thinking with Metacognitive Regulation. Yasushi Gotoh, 2016. 2016International Association for Development of the Information Society</p>
<p>REALM: Retrieval-Augmented Language Model Pre-Training. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang, CoRR abs/2002.089092020. 2020</p>
<p>Efficient Nearest Neighbor Language Models. Junxian He, Graham Neubig, Taylor Berg-Kirkpatrick, EMNLP (1). Association for Computational Linguistics2021</p>
<p>Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, Akiko Aizawa, COLING. International Committee on Computational Linguistics. 2020</p>
<p>TRUE: Re-evaluating Factual Consistency Evaluation. Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, Yossi Matias, 10.18653/v1/2022.naacl-main.287Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational LinguisticsSeattle, United States2022</p>
<p>Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering. Gautier Izacard, Edouard Grave, EACL. Association for Computational Linguistics2021</p>
<p>Few-shot Learning with Retrieval Augmented Language Models. Gautier Izacard, S H Patrick, Maria Lewis, Lucas Lomeli, Fabio Hosseini, Timo Petroni, Jane Schick, Armand Dwivedi-Yu, Sebastian Joulin, Edouard Riedel, Grave, CoRR abs/2208.032992022. 2022</p>
<p>Retrieval as Attention: End-to-end Learning of Retrieval and Reading within a Single Transformer. Zhengbao Jiang, Luyu Gao, Zhiruo Wang, Jun Araki, Haibo Ding, Jamie Callan, Graham Neubig, EMNLP. Association for Computational Linguistics2022</p>
<p>Active Retrieval Augmented Generation. Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig, CoRR abs/2305.069832023. 2023</p>
<p>SpanBERT: Improving Pre-training by Representing and Predicting Spans. Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, Omer Levy, Trans. Assoc. Comput. Linguistics. 82020. 2020</p>
<p>Dense Passage Retrieval for Open-Domain Question Answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, S H Patrick, Ledell Lewis, Sergey Wu, Danqi Edunov, Wen-Tau Chen, Yih, EMNLP (1). Association for Computational Linguistics2020</p>
<p>Generalization through Memorization: Nearest Neighbor Language Models. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis, ICLR. OpenReview.net2020</p>
<p>Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP. Omar Khattab, Keshav Santhanam, Lisa Xiang, David Li, Percy Hall, Christopher Liang, Matei Potts, Zaharia, CoRR abs/2212.140242022. 2022</p>
<p>Decomposed Prompting: A Modular Approach for Solving Complex Tasks. Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal, 2023In ICLR. OpenReview.net</p>
<p>Natural Questions: a Benchmark for Question Answering Research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M Dai, Jakob Uszkoreit, Quoc Le, Slav Petrov, Trans. Assoc. Comput. Linguistics. 72019. 2019</p>
<p>Metacognition: A literature review. Emily R Lai, Always learning: Pearson research report. 242011. 2011</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 332020. 2020</p>
<p>UniGen: A Unified Generative Framework for Retrieval and Question Answering with Large Language Models. Xiaoxi Li, Yujia Zhou, Zhicheng Dou, CoRR abs/2312.110362023. 2023</p>
<p>On Faithfulness and Factuality in Abstractive Summarization. Joshua Maynez, Shashi Narayan, Bernd Bohnet, Ryan T Mcdonald, ACL. Association for Computational Linguistics2020</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, arXiv:2112.09332Webgpt: Browser-assisted question-answering with human feedback. 2021. 2021arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, Ryan Lowe, 2022In NeurIPS</p>
<p>Measuring and Narrowing the Compositionality Gap in Language Models. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, Mike Lewis, CoRR abs/2210.033502022. 2022</p>
<p>Kevin Leyton-Brown, and Yoav Shoham. 2023. In-Context Retrieval-Augmented Language Models. Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, CoRR abs/2302.000832023</p>
<p>The Probabilistic Relevance Framework: BM25 and Beyond. Stephen E Robertson, Hugo Zaragoza, Found. Trends Inf. Retr. 32009. 2009</p>
<p>. Gregory Schraw, David Moshman, Metacognitive theories. Educational psychology review. 71995. 1995</p>
<p>Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy. Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, Weizhu Chen, CoRR abs/2305.152942023. 2023</p>
<p>REPLUG: Retrieval-Augmented Black-Box Language Models. Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, Wen-Tau Yih, CoRR abs/2301.126522023. 2023</p>
<p>Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, arXiv:2303.11366Reflexion: Language agents with verbal reinforcement learning. 2023. 2023arXiv preprint</p>
<p>FEVER: a Large-scale Dataset for Fact Extraction and VERification. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit Mittal, NAACL-HLT. Association for Computational Linguistics2018</p>
<p>Llama 2: Open Foundation and Fine-Tuned Chat Models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad . Almahairi, Thomas Scialom, CoRR abs/2307.092882023. 2023</p>
<p>Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal, ACL (1). 2023Association for Computational Linguistics</p>
<p>Text Embeddings by Weakly-Supervised Contrastive Pre-training. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, CoRR abs/2212.035332022. 2022</p>
<p>Metacognitive Prompting Improves Understanding in Large Language Models. Yuqing Wang, Yun Zhao, CoRR abs/2308.053422023. 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, arXiv:1809.096002018. 2018arXiv preprint</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022. 2022arXiv preprint</p>
<p>Training Language Models with Memory Augmentation. Zexuan Zhong, Tao Lei, Danqi Chen, EMNLP. Association for Computational Linguistics2022</p>
<p>Detecting Hallucinated Content in Conditional Neural Sequence Generation. Chunting Zhou, Graham Neubig, Jiatao Gu, Mona T Diab, Francisco Guzmán, Luke Zettlemoyer, Marjan Ghazvininejad, ACL/IJCNLP 2021. Association for Computational Linguistics. 2021ACL/IJCNLP (Findings) (Findings of ACL)</p>
<p>Encoding History with Context-aware Representation Learning for Personalized Search. Yujia Zhou, Zhicheng Dou, Ji-Rong Wen, SIGIR. ACM2020</p>            </div>
        </div>

    </div>
</body>
</html>
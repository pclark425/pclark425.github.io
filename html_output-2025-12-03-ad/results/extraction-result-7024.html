<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7024 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7024</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7024</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-277043897</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.11086v1.pdf" target="_blank">A Survey of Cross-domain Graph Learning: Progress and Future Directions</a></p>
                <p><strong>Paper Abstract:</strong> Graph learning plays a vital role in mining and analyzing complex relationships involved in graph data, which is widely used in many real-world applications like transaction networks and communication networks. Foundation models in CV and NLP have shown powerful cross-domain capabilities that are also significant in graph domains. However, existing graph learning approaches struggle with cross-domain tasks. Inspired by successes in CV and NLP, cross-domain graph learning has once again become a focal point of attention to realizing true graph foundation models. In this survey, we present a comprehensive review and analysis of existing works on cross-domain graph learning. Concretely, we first propose a new taxonomy, categorizing existing approaches based on the learned cross-domain information: structure, feature, and structure-feature mixture. Next, we systematically survey representative methods in these categories. Finally, we discuss the remaining limitations of existing studies and highlight promising avenues for future research. Relevant papers are summarized and will be consistently updated at: https://github.com/cshhzhao/Awesome-Cross-Domain-Graph-Learning.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7024.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7024.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GIMLET</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GIMLET (Generalized Position Embedding for Unified Graph-Text Modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A flatten-based unified graph-text approach that transforms graphs into token sequences via a generalized position embedding and combines graph structure with LLM-driven text processing for instruction-style downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gimlet: A unified graph-text model for instruction-based molecule zeroshot learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>flattened node/token sequence (Generalized Position Embedding)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes graphs as linear token sequences where nodes/tokens are assigned generalized position embeddings that capture local structure; sequences are fed to LLMs to jointly process structural and textual information.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based (implicitly lossy)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>flattening into token sequence with generalized position embeddings (sequence order encodes positional/structural info)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>instruction-based molecule zero-shot learning / graph-to-text for LLM downstream tasks</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>pre-trained LLM (unspecified) used for instruction-style processing</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large pre-trained language model used to consume flattened graph-token sequences and produce instruction-style outputs; specific backbone not reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Reported to enable LLM-driven processing of graph-structured inputs and support instruction-based zero-shot transfer in molecular tasks (survey claim).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Survey notes typical flattening approaches can compress structural detail (potentially lossy) and depend on careful encoding to preserve graph relations; dataset/metric specifics not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Placed among flatten-based unified mixture methods â€” enables LLM use on graphs but may trade off fine-grained structural fidelity compared to GNN-based unified approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Cross-domain Graph Learning: Progress and Future Directions', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7024.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7024.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphTranslator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphTranslator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A translator module that converts node embeddings into token sequences to align pre-trained graph models with LLMs, enabling LLMs to handle both pre-defined and open-ended graph tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graphtranslator: Aligning graph model to large language model for open-ended tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Translator-based token sequence</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Transforms node embeddings (or graph representations) into linear textual token sequences via a Translator module so that LLMs can directly ingest graph-derived tokens for open-ended generation or reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based (lossy/abstraction-oriented)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>embedding-to-token translation (module serializes node/embedding information into a token sequence representation)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>open-ended graph tasks / LLM-compatible graph-to-text conversion</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>pre-trained LLM integrated with a Translator module</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An LLM (unspecified) augmented by a Translator that maps graph embeddings to token sequences; enables LLM usage for graph tasks without changing the LLM internals.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Reported to allow reuse of pre-trained graph models and LLMs for open-ended tasks, improving flexibility of model application across graph tasks (survey statement).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Survey highlights potential loss of structural detail during translation and lack of reported canonicalization; empirical performance and tokenization cost not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared with GNN-based unified methods, GraphTranslator emphasizes interoperability with LLMs and open-ended tasks at the possible expense of preserving fine-grained graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Cross-domain Graph Learning: Progress and Future Directions', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7024.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7024.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphGPT (Graph instruction tuning for large language models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dual-stage instruction tuning method that aligns graph structural and semantic information with LLMs, improving cross-domain generalization via instruction-style fine-tuning on graph tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graphgpt: Graph instruction tuning for large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>instruction-style flattened graph text</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represents graphs as instruction-like textual sequences (verbalized graph structure and node features) for LLM instruction tuning; two-stage tuning aligns structural/semantic signals to LLM input format.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based (lossy/semantic-focused)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>instruction-verbalization: convert graph elements and tasks into natural-language instruction sequences for the LLM</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>instruction-following graph tasks / graph-to-text for LLM instruction tuning</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>large language model (instruction-tuned, unspecified backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An LLM that undergoes dual-stage instruction tuning to better accept graph-verbalized inputs and output graph task solutions or reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Survey reports improved cross-domain generalization through instruction tuning and alignment of graph semantics to LLM representations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Potential for structural information loss in flattened instruction representation; exact performance metrics not provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Stronger at producing instruction-aligned outputs for graphs than raw flattening; still may be less structure-faithful than specialized GNN encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Cross-domain Graph Learning: Progress and Future Directions', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7024.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7024.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GITA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GITA (Graph to visual and textual integration)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Combines graph visualization with textual conversion, using a vision-language model (VLM) to jointly reason over visual and textual graph data for vision-language graph reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GITA: Graph to visual and textual integration for vision-language graph reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>visual+textual flattened representation (visualized graph + textual tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Converts graphs to visual representations (images/visualizations) and textual descriptions; both modalities are input to a VLM that jointly reasons over the visual and textual encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>multimodal flattening (visual + sequential token-based), hierarchical multimodal</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>graph visualization to image + textual conversion of node/edge information; VLM ingests combined modalities</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>vision-language graph reasoning / multimodal graph-to-text</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vision-Language Model (VLM) (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A vision-language model that takes paired visualized graph images and textual sequences describing graph components, enabling joint multimodal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enables joint visual-textual reasoning over graphs by LLM/VLM architectures and broader integration of graph semantics via visual cues.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires an additional visual-rendering step; survey does not report quantitative gains and notes potential scaling/efficiency concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Provides richer multimodal cues compared with text-only flattening, but introduces modality alignment complexity absent in pure text representations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Cross-domain Graph Learning: Progress and Future Directions', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7024.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7024.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructGraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructGraph</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Uses a structured format verbalizer to convert graph data into text sequences and employs instruction tuning and preference optimization to improve LLM graph reasoning and reduce hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Instructgraph: Boosting large language models via graph-centric instruction tuning and preference alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>structured format verbalization (graph-to-instruction text)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Serializes graph structure and features into a structured textual verbalization (a schema-like text format) suitable for instruction tuning of LLMs; includes instruction and preference signals to guide outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based (structured verbalization; lossy to some extent)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>structured verbalizer that maps nodes, edges, and attributes into text blocks/instructions fed to LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph reasoning and instruction-following tasks; reducing LLM hallucination on graph tasks</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM (instruction-tuned, unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An LLM tuned with graph-centric instruction data and preference alignment to better answer graph queries and produce faithful outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Reported to reduce hallucinations and improve graph reasoning by combining instruction tuning with structured graph verbalizations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Primarily demonstrated for text-attributed graphs in survey; may be limited for non-TAG domains and challenges around preserving full structural fidelity remain.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Targets hallucination reduction more explicitly than simpler flattening schemes; compared to GNN-based methods, emphasizes LLM-native instruction formats.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Cross-domain Graph Learning: Progress and Future Directions', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7024.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7024.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TEA-GLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TEA-GLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A flatten-based approach that employs a linear projection module to align pre-trained GNN representations with LLM token embeddings, enabling zero-shot predictions across multiple graph tasks without modifying the LLM itself.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>linear-projected token-aligned sequence</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Projects GNN node/graph representations via a linear projection into the token embedding space of an LLM so that the LLM can accept graph-derived tokens and perform zero-shot tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based (projection-aligned; lossy/approximate)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>linear projection of GNN representations into LLM token embedding space followed by token-sequence construction</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>zero-shot graph task prediction using LLMs (graph-to-text alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM + linear projection module (specific LLM not reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A pre-trained LLM left unchanged, with a linear projection layer that maps graph encoder outputs into LLM token embedding space for direct LLM consumption.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Survey claims this enables zero-shot predictions across multiple graph tasks without modifying the LLM; practical gains or numbers not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Alignment via a simple linear projection may not fully capture structural nuances; evaluation details and datasets not provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Simpler and less invasive than full fine-tuning of LLMs, but likely less expressive than richer Translator or instruction-tuning approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Cross-domain Graph Learning: Progress and Future Directions', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7024.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7024.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GOFA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GOFA (Generative One-For-All for Joint Graph Language Modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative model that jointly models graph and language tokens for unified graph-language modeling, enabling joint graph language modeling and generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GOFA: A generative one-for-all model for joint graph language modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>joint graph-language token sequence</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Interleaves or jointly models graph tokens (nodes/edges/features) and language tokens in a single generative sequence to allow a single model to produce and consume both graph and textual content.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based, generative (hybrid graph+language)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>joint tokenization of graph elements with language tokens into a single autoregressive sequence suitable for a generative model</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>joint graph-language modeling / graph-to-text generation and related tasks</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>generative graph-language model (one-for-all, unspecified architecture/size)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A generative model trained to model sequences that combine graph and natural language tokens to support a range of graph and text tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Survey positions GOFA as a unifying generative approach that can handle multiple graph-language tasks in a single model, potentially simplifying multi-task training.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Complexity of joint tokenization and ensuring structural fidelity are potential challenges; survey does not report quantitative evaluations here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>More unified than Translator/flattening approaches by jointly modeling both modalities, but complexity and token budget may be higher.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Cross-domain Graph Learning: Progress and Future Directions', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7024.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7024.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HiGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HiGPT (Heterogeneous Graph Language Model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A heterogeneous graph tokenizer converts complex graph structures into textual representations and applies instruction-tuning to LLMs for enhanced understanding of intra- and inter-node relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Higpt: Heterogeneous graph language model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>heterogeneous graph tokenization (token sequence)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Uses a heterogeneous graph tokenizer that serializes multi-typed nodes/edges into a token sequence that captures heterogeneity, then applies instruction tuning for LLM consumption.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based (heterogeneity-aware; lossy to varying degrees)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>heterogeneous graph tokenizer that encodes node/edge types and attributes into a linear token sequence</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>heterogeneous graph language modeling / instruction-following graph tasks</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>instruction-tuned LLM (unspecified backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An LLM augmented by a tokenizer specialized for heterogeneous graph elements to better incorporate typed relations into language model inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Survey suggests improved handling of heterogeneous graphs by LLMs through dedicated tokenization and instruction tuning; specific gains not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Tokenization design must handle heterogeneity carefully; survey lacks quantitative comparisons or datasets used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>More explicitly handles heterogeneity than generic flattening approaches; compared to GNN encoders, focuses on LLM-native tokenization strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Cross-domain Graph Learning: Progress and Future Directions', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7024.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7024.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphCLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphCLIP</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generates textual descriptions of subgraphs via an LLM and uses graph-summary contrastive learning to improve zero-shot and few-shot transfer in text-attributed graph settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GraphCLIP: Enhancing transferability in graph foundation models for text-attributed graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>subgraph textual summary sequences</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Uses an LLM to produce textual summaries/descriptions of subgraphs, then aligns these text summaries with graph representations via contrastive learning to enable transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential token-based (summary-focused, lossy abstraction)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>LLM-generated textual summarization of subgraphs (subgraph -> natural-language summary), used in contrastive alignment with graph encoders</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>zero-shot and few-shot transfer for text-attributed graph tasks via contrastive alignment</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM for summary generation + graph encoder for contrastive alignment (unspecified backbones)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A pipeline where an LLM creates textual subgraph summaries and a contrastive pretraining objective aligns these with graph encoder representations to improve transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Reported to improve zero-shot and few-shot transferability in text-attributed graphs through text-summary contrastive objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Depends on quality of LLM-generated summaries; may be limited to TAGs and subject to LLM data-leakage issues noted in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Combines benefits of textualization (enabling LLMs) with representation alignment; compared to raw flattening, uses contrastive learning to better align modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Cross-domain Graph Learning: Progress and Future Directions', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7024.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7024.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphWiz</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphWiz (instruction-following language model for graph computational problems)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Uses the GraphInstruct dataset to train LLMs in solving graph computational problems via instruction-following; it transforms graphs into textual sequences and trains models to produce explicit reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graphwiz: An instruction-following language model for graph computational problems.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>instruction-style graph textual sequences (GraphInstruct format)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Converts graphs into instruction-following textual sequences based on the GraphInstruct dataset format; LLMs are trained to produce stepwise reasoning and computational outputs for graph problems.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based (instructional; lossy abstraction of computation steps)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>graph -> textual instruction conversion following GraphInstruct schema; sequences include problem specification and expected reasoning trace</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>GraphInstruct</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph computational problems / instruction-following reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>instruction-following language model (LLM, unspecified architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An LLM trained on the GraphInstruct instruction-following dataset to solve algorithmic/graph computation exercises and output explicit reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enables models to produce explicit reasoning paths for graph computation tasks and improves instruction-following capability on graph problems (survey claim).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Survey notes dependency on the curated GraphInstruct dataset and the general risks of LLM data leakage and hallucination; quantitative performance not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Stronger than simple flattening for algorithmic reasoning because it trains models on explicit reasoning traces; compared to GNNs, focuses on language-model-based problem solving rather than embedding-driven prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Cross-domain Graph Learning: Progress and Future Directions', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gimlet: A unified graph-text model for instruction-based molecule zeroshot learning. <em>(Rating: 2)</em></li>
                <li>Graphtranslator: Aligning graph model to large language model for open-ended tasks. <em>(Rating: 2)</em></li>
                <li>Graphgpt: Graph instruction tuning for large language models. <em>(Rating: 2)</em></li>
                <li>GITA: Graph to visual and textual integration for vision-language graph reasoning. <em>(Rating: 2)</em></li>
                <li>Instructgraph: Boosting large language models via graph-centric instruction tuning and preference alignment. <em>(Rating: 2)</em></li>
                <li>GOFA: A generative one-for-all model for joint graph language modeling. <em>(Rating: 2)</em></li>
                <li>Higpt: Heterogeneous graph language model. <em>(Rating: 2)</em></li>
                <li>GraphCLIP: Enhancing transferability in graph foundation models for text-attributed graphs. <em>(Rating: 2)</em></li>
                <li>Graphwiz: An instruction-following language model for graph computational problems. <em>(Rating: 2)</em></li>
                <li>TEA-GLM <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7024",
    "paper_id": "paper-277043897",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "GIMLET",
            "name_full": "GIMLET (Generalized Position Embedding for Unified Graph-Text Modeling)",
            "brief_description": "A flatten-based unified graph-text approach that transforms graphs into token sequences via a generalized position embedding and combines graph structure with LLM-driven text processing for instruction-style downstream tasks.",
            "citation_title": "Gimlet: A unified graph-text model for instruction-based molecule zeroshot learning.",
            "mention_or_use": "mention",
            "representation_name": "flattened node/token sequence (Generalized Position Embedding)",
            "representation_description": "Encodes graphs as linear token sequences where nodes/tokens are assigned generalized position embeddings that capture local structure; sequences are fed to LLMs to jointly process structural and textual information.",
            "representation_type": "sequential, token-based (implicitly lossy)",
            "encoding_method": "flattening into token sequence with generalized position embeddings (sequence order encodes positional/structural info)",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "instruction-based molecule zero-shot learning / graph-to-text for LLM downstream tasks",
            "model_name": "pre-trained LLM (unspecified) used for instruction-style processing",
            "model_description": "A large pre-trained language model used to consume flattened graph-token sequences and produce instruction-style outputs; specific backbone not reported in survey.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Reported to enable LLM-driven processing of graph-structured inputs and support instruction-based zero-shot transfer in molecular tasks (survey claim).",
            "limitations": "Survey notes typical flattening approaches can compress structural detail (potentially lossy) and depend on careful encoding to preserve graph relations; dataset/metric specifics not reported.",
            "comparison_with_other": "Placed among flatten-based unified mixture methods â€” enables LLM use on graphs but may trade off fine-grained structural fidelity compared to GNN-based unified approaches.",
            "uuid": "e7024.0",
            "source_info": {
                "paper_title": "A Survey of Cross-domain Graph Learning: Progress and Future Directions",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "GraphTranslator",
            "name_full": "GraphTranslator",
            "brief_description": "A translator module that converts node embeddings into token sequences to align pre-trained graph models with LLMs, enabling LLMs to handle both pre-defined and open-ended graph tasks.",
            "citation_title": "Graphtranslator: Aligning graph model to large language model for open-ended tasks.",
            "mention_or_use": "mention",
            "representation_name": "Translator-based token sequence",
            "representation_description": "Transforms node embeddings (or graph representations) into linear textual token sequences via a Translator module so that LLMs can directly ingest graph-derived tokens for open-ended generation or reasoning.",
            "representation_type": "sequential, token-based (lossy/abstraction-oriented)",
            "encoding_method": "embedding-to-token translation (module serializes node/embedding information into a token sequence representation)",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "open-ended graph tasks / LLM-compatible graph-to-text conversion",
            "model_name": "pre-trained LLM integrated with a Translator module",
            "model_description": "An LLM (unspecified) augmented by a Translator that maps graph embeddings to token sequences; enables LLM usage for graph tasks without changing the LLM internals.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Reported to allow reuse of pre-trained graph models and LLMs for open-ended tasks, improving flexibility of model application across graph tasks (survey statement).",
            "limitations": "Survey highlights potential loss of structural detail during translation and lack of reported canonicalization; empirical performance and tokenization cost not provided.",
            "comparison_with_other": "Compared with GNN-based unified methods, GraphTranslator emphasizes interoperability with LLMs and open-ended tasks at the possible expense of preserving fine-grained graph structure.",
            "uuid": "e7024.1",
            "source_info": {
                "paper_title": "A Survey of Cross-domain Graph Learning: Progress and Future Directions",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "GraphGPT",
            "name_full": "GraphGPT (Graph instruction tuning for large language models)",
            "brief_description": "A dual-stage instruction tuning method that aligns graph structural and semantic information with LLMs, improving cross-domain generalization via instruction-style fine-tuning on graph tasks.",
            "citation_title": "Graphgpt: Graph instruction tuning for large language models.",
            "mention_or_use": "mention",
            "representation_name": "instruction-style flattened graph text",
            "representation_description": "Represents graphs as instruction-like textual sequences (verbalized graph structure and node features) for LLM instruction tuning; two-stage tuning aligns structural/semantic signals to LLM input format.",
            "representation_type": "sequential, token-based (lossy/semantic-focused)",
            "encoding_method": "instruction-verbalization: convert graph elements and tasks into natural-language instruction sequences for the LLM",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "instruction-following graph tasks / graph-to-text for LLM instruction tuning",
            "model_name": "large language model (instruction-tuned, unspecified backbone)",
            "model_description": "An LLM that undergoes dual-stage instruction tuning to better accept graph-verbalized inputs and output graph task solutions or reasoning traces.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Survey reports improved cross-domain generalization through instruction tuning and alignment of graph semantics to LLM representations.",
            "limitations": "Potential for structural information loss in flattened instruction representation; exact performance metrics not provided in survey.",
            "comparison_with_other": "Stronger at producing instruction-aligned outputs for graphs than raw flattening; still may be less structure-faithful than specialized GNN encoders.",
            "uuid": "e7024.2",
            "source_info": {
                "paper_title": "A Survey of Cross-domain Graph Learning: Progress and Future Directions",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "GITA",
            "name_full": "GITA (Graph to visual and textual integration)",
            "brief_description": "Combines graph visualization with textual conversion, using a vision-language model (VLM) to jointly reason over visual and textual graph data for vision-language graph reasoning tasks.",
            "citation_title": "GITA: Graph to visual and textual integration for vision-language graph reasoning.",
            "mention_or_use": "mention",
            "representation_name": "visual+textual flattened representation (visualized graph + textual tokens)",
            "representation_description": "Converts graphs to visual representations (images/visualizations) and textual descriptions; both modalities are input to a VLM that jointly reasons over the visual and textual encodings.",
            "representation_type": "multimodal flattening (visual + sequential token-based), hierarchical multimodal",
            "encoding_method": "graph visualization to image + textual conversion of node/edge information; VLM ingests combined modalities",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "vision-language graph reasoning / multimodal graph-to-text",
            "model_name": "Vision-Language Model (VLM) (unspecified)",
            "model_description": "A vision-language model that takes paired visualized graph images and textual sequences describing graph components, enabling joint multimodal reasoning.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Enables joint visual-textual reasoning over graphs by LLM/VLM architectures and broader integration of graph semantics via visual cues.",
            "limitations": "Requires an additional visual-rendering step; survey does not report quantitative gains and notes potential scaling/efficiency concerns.",
            "comparison_with_other": "Provides richer multimodal cues compared with text-only flattening, but introduces modality alignment complexity absent in pure text representations.",
            "uuid": "e7024.3",
            "source_info": {
                "paper_title": "A Survey of Cross-domain Graph Learning: Progress and Future Directions",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "InstructGraph",
            "name_full": "InstructGraph",
            "brief_description": "Uses a structured format verbalizer to convert graph data into text sequences and employs instruction tuning and preference optimization to improve LLM graph reasoning and reduce hallucinations.",
            "citation_title": "Instructgraph: Boosting large language models via graph-centric instruction tuning and preference alignment.",
            "mention_or_use": "mention",
            "representation_name": "structured format verbalization (graph-to-instruction text)",
            "representation_description": "Serializes graph structure and features into a structured textual verbalization (a schema-like text format) suitable for instruction tuning of LLMs; includes instruction and preference signals to guide outputs.",
            "representation_type": "sequential, token-based (structured verbalization; lossy to some extent)",
            "encoding_method": "structured verbalizer that maps nodes, edges, and attributes into text blocks/instructions fed to LLMs",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "graph reasoning and instruction-following tasks; reducing LLM hallucination on graph tasks",
            "model_name": "LLM (instruction-tuned, unspecified)",
            "model_description": "An LLM tuned with graph-centric instruction data and preference alignment to better answer graph queries and produce faithful outputs.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Reported to reduce hallucinations and improve graph reasoning by combining instruction tuning with structured graph verbalizations.",
            "limitations": "Primarily demonstrated for text-attributed graphs in survey; may be limited for non-TAG domains and challenges around preserving full structural fidelity remain.",
            "comparison_with_other": "Targets hallucination reduction more explicitly than simpler flattening schemes; compared to GNN-based methods, emphasizes LLM-native instruction formats.",
            "uuid": "e7024.4",
            "source_info": {
                "paper_title": "A Survey of Cross-domain Graph Learning: Progress and Future Directions",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "TEA-GLM",
            "name_full": "TEA-GLM",
            "brief_description": "A flatten-based approach that employs a linear projection module to align pre-trained GNN representations with LLM token embeddings, enabling zero-shot predictions across multiple graph tasks without modifying the LLM itself.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "linear-projected token-aligned sequence",
            "representation_description": "Projects GNN node/graph representations via a linear projection into the token embedding space of an LLM so that the LLM can accept graph-derived tokens and perform zero-shot tasks.",
            "representation_type": "sequential, token-based (projection-aligned; lossy/approximate)",
            "encoding_method": "linear projection of GNN representations into LLM token embedding space followed by token-sequence construction",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "zero-shot graph task prediction using LLMs (graph-to-text alignment)",
            "model_name": "LLM + linear projection module (specific LLM not reported)",
            "model_description": "A pre-trained LLM left unchanged, with a linear projection layer that maps graph encoder outputs into LLM token embedding space for direct LLM consumption.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Survey claims this enables zero-shot predictions across multiple graph tasks without modifying the LLM; practical gains or numbers not reported.",
            "limitations": "Alignment via a simple linear projection may not fully capture structural nuances; evaluation details and datasets not provided in survey.",
            "comparison_with_other": "Simpler and less invasive than full fine-tuning of LLMs, but likely less expressive than richer Translator or instruction-tuning approaches.",
            "uuid": "e7024.5",
            "source_info": {
                "paper_title": "A Survey of Cross-domain Graph Learning: Progress and Future Directions",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "GOFA",
            "name_full": "GOFA (Generative One-For-All for Joint Graph Language Modeling)",
            "brief_description": "A generative model that jointly models graph and language tokens for unified graph-language modeling, enabling joint graph language modeling and generation.",
            "citation_title": "GOFA: A generative one-for-all model for joint graph language modeling.",
            "mention_or_use": "mention",
            "representation_name": "joint graph-language token sequence",
            "representation_description": "Interleaves or jointly models graph tokens (nodes/edges/features) and language tokens in a single generative sequence to allow a single model to produce and consume both graph and textual content.",
            "representation_type": "sequential, token-based, generative (hybrid graph+language)",
            "encoding_method": "joint tokenization of graph elements with language tokens into a single autoregressive sequence suitable for a generative model",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "joint graph-language modeling / graph-to-text generation and related tasks",
            "model_name": "generative graph-language model (one-for-all, unspecified architecture/size)",
            "model_description": "A generative model trained to model sequences that combine graph and natural language tokens to support a range of graph and text tasks.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Survey positions GOFA as a unifying generative approach that can handle multiple graph-language tasks in a single model, potentially simplifying multi-task training.",
            "limitations": "Complexity of joint tokenization and ensuring structural fidelity are potential challenges; survey does not report quantitative evaluations here.",
            "comparison_with_other": "More unified than Translator/flattening approaches by jointly modeling both modalities, but complexity and token budget may be higher.",
            "uuid": "e7024.6",
            "source_info": {
                "paper_title": "A Survey of Cross-domain Graph Learning: Progress and Future Directions",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "HiGPT",
            "name_full": "HiGPT (Heterogeneous Graph Language Model)",
            "brief_description": "A heterogeneous graph tokenizer converts complex graph structures into textual representations and applies instruction-tuning to LLMs for enhanced understanding of intra- and inter-node relationships.",
            "citation_title": "Higpt: Heterogeneous graph language model.",
            "mention_or_use": "mention",
            "representation_name": "heterogeneous graph tokenization (token sequence)",
            "representation_description": "Uses a heterogeneous graph tokenizer that serializes multi-typed nodes/edges into a token sequence that captures heterogeneity, then applies instruction tuning for LLM consumption.",
            "representation_type": "sequential, token-based (heterogeneity-aware; lossy to varying degrees)",
            "encoding_method": "heterogeneous graph tokenizer that encodes node/edge types and attributes into a linear token sequence",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "heterogeneous graph language modeling / instruction-following graph tasks",
            "model_name": "instruction-tuned LLM (unspecified backbone)",
            "model_description": "An LLM augmented by a tokenizer specialized for heterogeneous graph elements to better incorporate typed relations into language model inputs.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Survey suggests improved handling of heterogeneous graphs by LLMs through dedicated tokenization and instruction tuning; specific gains not provided.",
            "limitations": "Tokenization design must handle heterogeneity carefully; survey lacks quantitative comparisons or datasets used.",
            "comparison_with_other": "More explicitly handles heterogeneity than generic flattening approaches; compared to GNN encoders, focuses on LLM-native tokenization strategies.",
            "uuid": "e7024.7",
            "source_info": {
                "paper_title": "A Survey of Cross-domain Graph Learning: Progress and Future Directions",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "GraphCLIP",
            "name_full": "GraphCLIP",
            "brief_description": "Generates textual descriptions of subgraphs via an LLM and uses graph-summary contrastive learning to improve zero-shot and few-shot transfer in text-attributed graph settings.",
            "citation_title": "GraphCLIP: Enhancing transferability in graph foundation models for text-attributed graphs.",
            "mention_or_use": "mention",
            "representation_name": "subgraph textual summary sequences",
            "representation_description": "Uses an LLM to produce textual summaries/descriptions of subgraphs, then aligns these text summaries with graph representations via contrastive learning to enable transfer.",
            "representation_type": "sequential token-based (summary-focused, lossy abstraction)",
            "encoding_method": "LLM-generated textual summarization of subgraphs (subgraph -&gt; natural-language summary), used in contrastive alignment with graph encoders",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "zero-shot and few-shot transfer for text-attributed graph tasks via contrastive alignment",
            "model_name": "LLM for summary generation + graph encoder for contrastive alignment (unspecified backbones)",
            "model_description": "A pipeline where an LLM creates textual subgraph summaries and a contrastive pretraining objective aligns these with graph encoder representations to improve transfer.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Reported to improve zero-shot and few-shot transferability in text-attributed graphs through text-summary contrastive objectives.",
            "limitations": "Depends on quality of LLM-generated summaries; may be limited to TAGs and subject to LLM data-leakage issues noted in survey.",
            "comparison_with_other": "Combines benefits of textualization (enabling LLMs) with representation alignment; compared to raw flattening, uses contrastive learning to better align modalities.",
            "uuid": "e7024.8",
            "source_info": {
                "paper_title": "A Survey of Cross-domain Graph Learning: Progress and Future Directions",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "GraphWiz",
            "name_full": "GraphWiz (instruction-following language model for graph computational problems)",
            "brief_description": "Uses the GraphInstruct dataset to train LLMs in solving graph computational problems via instruction-following; it transforms graphs into textual sequences and trains models to produce explicit reasoning paths.",
            "citation_title": "Graphwiz: An instruction-following language model for graph computational problems.",
            "mention_or_use": "mention",
            "representation_name": "instruction-style graph textual sequences (GraphInstruct format)",
            "representation_description": "Converts graphs into instruction-following textual sequences based on the GraphInstruct dataset format; LLMs are trained to produce stepwise reasoning and computational outputs for graph problems.",
            "representation_type": "sequential, token-based (instructional; lossy abstraction of computation steps)",
            "encoding_method": "graph -&gt; textual instruction conversion following GraphInstruct schema; sequences include problem specification and expected reasoning trace",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "GraphInstruct",
            "task_name": "graph computational problems / instruction-following reasoning tasks",
            "model_name": "instruction-following language model (LLM, unspecified architecture)",
            "model_description": "An LLM trained on the GraphInstruct instruction-following dataset to solve algorithmic/graph computation exercises and output explicit reasoning steps.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Enables models to produce explicit reasoning paths for graph computation tasks and improves instruction-following capability on graph problems (survey claim).",
            "limitations": "Survey notes dependency on the curated GraphInstruct dataset and the general risks of LLM data leakage and hallucination; quantitative performance not provided.",
            "comparison_with_other": "Stronger than simple flattening for algorithmic reasoning because it trains models on explicit reasoning traces; compared to GNNs, focuses on language-model-based problem solving rather than embedding-driven prediction.",
            "uuid": "e7024.9",
            "source_info": {
                "paper_title": "A Survey of Cross-domain Graph Learning: Progress and Future Directions",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gimlet: A unified graph-text model for instruction-based molecule zeroshot learning.",
            "rating": 2,
            "sanitized_title": "gimlet_a_unified_graphtext_model_for_instructionbased_molecule_zeroshot_learning"
        },
        {
            "paper_title": "Graphtranslator: Aligning graph model to large language model for open-ended tasks.",
            "rating": 2,
            "sanitized_title": "graphtranslator_aligning_graph_model_to_large_language_model_for_openended_tasks"
        },
        {
            "paper_title": "Graphgpt: Graph instruction tuning for large language models.",
            "rating": 2,
            "sanitized_title": "graphgpt_graph_instruction_tuning_for_large_language_models"
        },
        {
            "paper_title": "GITA: Graph to visual and textual integration for vision-language graph reasoning.",
            "rating": 2,
            "sanitized_title": "gita_graph_to_visual_and_textual_integration_for_visionlanguage_graph_reasoning"
        },
        {
            "paper_title": "Instructgraph: Boosting large language models via graph-centric instruction tuning and preference alignment.",
            "rating": 2,
            "sanitized_title": "instructgraph_boosting_large_language_models_via_graphcentric_instruction_tuning_and_preference_alignment"
        },
        {
            "paper_title": "GOFA: A generative one-for-all model for joint graph language modeling.",
            "rating": 2,
            "sanitized_title": "gofa_a_generative_oneforall_model_for_joint_graph_language_modeling"
        },
        {
            "paper_title": "Higpt: Heterogeneous graph language model.",
            "rating": 2,
            "sanitized_title": "higpt_heterogeneous_graph_language_model"
        },
        {
            "paper_title": "GraphCLIP: Enhancing transferability in graph foundation models for text-attributed graphs.",
            "rating": 2,
            "sanitized_title": "graphclip_enhancing_transferability_in_graph_foundation_models_for_textattributed_graphs"
        },
        {
            "paper_title": "Graphwiz: An instruction-following language model for graph computational problems.",
            "rating": 2,
            "sanitized_title": "graphwiz_an_instructionfollowing_language_model_for_graph_computational_problems"
        },
        {
            "paper_title": "TEA-GLM",
            "rating": 1
        }
    ],
    "cost": 0.01829925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey of Cross-domain Graph Learning: Progress and Future Directions</p>
<p>Haihong Zhao 
Hong Kong University of Science and Technology (Guangzhou)
511453GuangzhouGuangdongChina</p>
<p>Chenyi Zi 
Hong Kong University of Science and Technology (Guangzhou)
511453GuangzhouGuangdongChina</p>
<p>Hong Kong University of Science and Technology (Guangzhou)
511453GuangzhouGuangdongChina</p>
<p>Aochuan Chen 
Hong Kong University of Science and Technology (Guangzhou)
511453GuangzhouGuangdongChina</p>
<p>Hong Kong University of Science and Technology (Guangzhou)
511453GuangzhouGuangdongChina</p>
<p>Jia Li 
Hong Kong University of Science and Technology (Guangzhou)
511453GuangzhouGuangdongChina</p>
<p>Hong Kong University of Science and Technology (Guangzhou)
511453GuangzhouGuangdongChina</p>
<p>Hong Kong University of Science and Technology (Guangzhou)
511453GuangzhouGuangdongChina</p>
<p>) H Zhao 
Hong Kong University of Science and Technology (Guangzhou)
511453GuangzhouGuangdongChina</p>
<p>A Survey of Cross-domain Graph Learning: Progress and Future Directions
0A4E9708F673E2CC77044A38B93B68D8Graph LearningCross-domainStructureFeatureStructure-featureTaxonomy Cross-domain Graph Learning Structure-oriented Structure Generation GraphControl [22]GA 2 E [23]OpenGraph [24]UniAug [25] Structure Contrast GCC [26]PCRec [27]GRADE [28]APT [29]FedStar [30]BooG [31]ProCom [32]RiemannGFM [33] Feature-oriented Dimension Align KTN [34]NaP [35]GPF-plus [36]RELIEF [37] Dimension Misalign OFA [38]GraphAlign [39]CDFS-GAD [40]ZeroG [41]DAGPrompT [42] Mixture-oriented Sequential Mixture PTGB [43]SDGA [44]TAPE [45]SOGA [46]GSPT [47]ARC [48]UniGraph [49]AnyGraph [50]GraphLoRA [51]SAMGPT [52]MDGFM [53] Unified Mixture Graph-based UDA-GCN [54]DA-GCN [55]COMMDER [56]GCFL [57]CDGEncoder [58]CrossHG-Meta [59]METABrainC [60]CCDR [61]GAST [62]DH-GAT [63]ALEX [64]ACT [65]CDTC [66]DGASN [67]STGP [68]GCOPE [21]ALCDR [69]PGPRec [70]CrossLink [71]MDGPT [72]Uni-GLM [73] Flatten-based GIMLET [74]GraphTranslator [75]GraphGPT [76]GITA [77]InstructGraph [78]TEA-GLM [79]GOFA [80]HiGPT [81]GraphCLIP [82]GraphWiz [83]
Graph learning plays a vital role in mining and analyzing complex relationships involved in graph data, which is widely used in many real-world applications like transaction networks and communication networks.Foundation models in CV and NLP have shown powerful cross-domain capabilities that are also significant in graph domains.However, existing graph learning approaches struggle with cross-domain tasks.Inspired by successes in CV and NLP, cross-domain graph learning has once again become a focal point of attention to realizing true graph foundation models.In this survey, we present a comprehensive review and analysis of existing works on cross-domain graph learning.Concretely, we first propose a new taxonomy, categorizing existing approaches based on the learned cross-domain information: structure, feature, and structure-feature mixture.Next, we systematically survey representative methods in these categories.Finally, we discuss the remaining limitations of existing studies and highlight promising avenues for future research.Relevant papers are summarized and will be consistently updated at: https: //github.com/cshhzhao/Awesome-Cross-Domain-Graph-Learning.</p>
<p>I. INTRODUCTION</p>
<p>G RAPH, or graph theory, has been playing an increasingly crucial role across numerous fields, actively contributing to various industries in the modern world, including recommendation [1], [2], finance [3], [4], weather [5], pharmaceuticals [6], [7], and communications [8], [9].Graph data can model the structural characteristics between nodes, offering significant advantages in capturing interactions and dependencies across structures and features.Many real-world datasets-such as citation networks [10], molecular [7], and transportation networks [11]-are naturally represented as graphs.To efficiently process and analyze these complex graph data, Graph Neural Networks (GNNs) [12]- [14] have emerged as a leading technique.The primary goal of GNNs is to learn the relationships between nodes and their neighbors through recursive message passing and aggregation mechanisms, thereby obtaining expressive representations at the node, edge, or graph level for various downstream tasks.</p>
<p>In recent years, the rapid development of foundation models, especially the large language models (LLMs), has significantly revolutionized the way people live and work, such as ChatGPT in natural language processing (NLP) and DALL-E 2 in computer vision (CV).Compared to previous deep learning models, the core advantage of foundation models lies in their ability to train on data from diverse domains, subsequently transferring the learned knowledge across different domains, encompassing three key capabilities: cross-modal, cross-domain, and crosstask [15], [16].These three capabilities are also crucial in graph domains.For instance, cross-modal capabilities allow social networks to combine user relationships (graph structure) with textual content (text modality) to more accurately predict user interests, thus enhancing recommendation systems.Crossdomain capabilities facilitate knowledge transfer between different domains-such as leveraging insights from transportation networks to optimize power load management in power grid networks, or using rich user interaction data from social networks to improve anomaly detection in banking transaction networks with limited data.Cross-task capabilities enable models trained for one task (e.g., node classification to predict a paper's research field) to be directly applied to another task (e.g., link prediction to identify paper citations), thus enhancing adaptability and efficiency across tasks.</p>
<p>The importance of cross-domain graph learning (CDGL).In graph domains, the key to achieving this vision lies in finding an effective way to integrate cross-domain knowledge that allows for straightforward generalization across domains.In detail, single-domain cross-modal problems (e.g., integrating images, text, and graphs) can benefit from advancements in CV and NLP, where many methods have successfully leveraged multimodal information [17].Additionally, single-domain cross-task problems can be addressed using classical graph pretraining models [18]- [20].However, when dealing with cross-domain problems, the heterogeneity and diversity of graph data make it challenging for data from a single domain to capture a arXiv:2503.11086v1[cs.LG] 14 Mar 2025 universal graph representation.Figure 1 shows the overview of CDGL, which can help better understand the differences across multi-domain graph data [21]:</p>
<p>(a) Structural differences: Graphs from different domains vary in node connectivity, with some being densely connected and others sparse.For instance, the social network among hotel staff forms a structured and topdown hierarchy with sparse connections, and the citation network among academic papers is densely interconnected.(b) Feature differences: The dimensions and semantics of node features also differ across domains, such as social networks and molecular networks.</p>
<p>This diversity poses challenges for effectively integrating cross-domain graph information and producing universal crossdomain graph representations.To this end, CDGL should be considered the crucial step towards graph foundation models.Through CDGL, commonalities can be extracted from graphs across different domains, enabling the learning of graph representations with broad transferability.</p>
<p>Motivations.Although CDGL has increasingly received attention in graph domains-especially due to its role in constructing graph foundation models-this rapidly expanding field still lacks a systematic review, which hinders researchers from gaining a comprehensive understanding of the field and exploring future directions.Concretely, previous graph-based crossdomain methods were usually designed for specific scenarios, with limited cross-domain scope and lacking universality.Therefore, they are mainly included as subcategories in surveys within particular domains, such as graph neural network-based methods included in recommendation system surveys [1], [84].</p>
<p>To fill this gap, in this work, we aim to provide a systematic survey on CDGL, helping researchers better understand the current state of research and its challenges.Specifically, we go through existing representative CDGL works to organize our first-level taxonomy, as shown in Figure 2, categorizing based on the information captured by graph neural networks used for cross-domain transfer (i.e., structure-oriented, feature-oriented, and mixture-orineted) throughout the entire model pipeline.We further refine our taxonomy and introduce more granularity to the initial categories.Contributions.The contributions of this work can be summarized from the following three aspects.(1) A structured taxonomy.A broad overview of the field is presented with a structured taxonomy that categorizes existing works into three categories (Figure 2).(2) A comprehensive review.Based on the proposed taxonomy, the current research progress of cross-domain techniques for graph learning is systematically delineated.To the best of our knowledge, this is the first comprehensive survey for CDGL.(3) Some future directions.We discuss the remaining limitations of existing works and point out possible future directions.</p>
<p>II. PRELIMINARY</p>
<p>In this section, we first give the related notations and then introduce the basic concepts of two key areas related to this survey, i.e., graph representation learning and cross-domain learning.Finally, we give a brief introduction to the newly proposed taxonomy for CDGL.</p>
<p>A. Notations</p>
<p>Consider a graph instance represented as G = {V, E}, where
V = {v 1 , v 2 , . . . , v N } is the set of nodes with N elements.
The set E âŠ† V Ã— V defines the connections between nodes.Associated with each node v i is a feature vector x i âˆˆ R D .To describe the connectivity within the graph, we use the adjacency matrix A âˆˆ {0, 1} N Ã—N , where the matrix entry A i,j is defined as 1 if and only if there is an edge (v i , v j ) âˆˆ E. A graph data space, including a node space and an edge space in a graph space, can be denoted as G = {V, E}, where V and E are the set of nodes and edges in the corresponding graph space.</p>
<p>B. Graph Representation Learning</p>
<p>Definitions.Graph representation learning is a powerful technique for managing complex and heterogeneous non-Euclidean graph data.It focuses on learning meaningful representations for nodes, edges, or entire graphs by mapping them into continuous vector spaces.This approach preserves the intrinsic structural and feature information of the graph, which can facilitate various tasks such as node classification, link prediction, and graph classification.These methods can be broadly categorized into two main branches: shallow embedding methods and deep graph neural networks (GNNs).Shallow Embedding Methods.Shallow embedding methods, such as node2vec [85] and DeepWalk [86], aim to map nodes into low-dimensional embeddings by preserving the network's similarity structure.These methods utilize techniques like factorization-based methods or random walks to achieve this goal.While providing flexibility for various downstream tasks, shallow embedding methods face limitations: they cannot generate embeddings for unseen nodes and lack the capability to incorporate node features effectively.</p>
<p>Deep Graph Neural Networks (GNNs).Deep GNNs, including Graph Convolutional Networks (GCN) [12] and Graph Attention Networks (GAT) [13], maintain input node features as constants and optimize model parameters for specific tasks.These networks typically employ a message-passing mechanism, where nodes iteratively exchange and aggregate information with their neighbors.This process leads to more expressive graph representations that can effectively handle more complex graph-based tasks.In detail, the forward process of the message-passing mechanism can be defined as:
h (l) i = U h (lâˆ’1) i , M({h (lâˆ’1) i , h (lâˆ’1) j | v j âˆˆ N i })
where h (l) i is the feature vector of node v i in the l-th layer, and N i is a set of neighbor nodes of node v i .M denotes the message passing function of aggregating neighbor information, U denotes the update function with central node feature and neighbor node features as input.By stacking multiple layers, GNNs can aggregate messages from higher-order neighbors.</p>
<p>The prevalent methods in graph representation learning are currently based on GNN architectures due to their enhanced ability to capture complex relational patterns in graph data.However, both shallow embedding methods and GNNs are primarily designed to learn representations within a specific graph domain, limiting their effectiveness across different domains.This issue underscores the need for research into methods that can generalize across diverse graph-based applications, aiming to enhance the learning capacity of graph representation techniques to operate effectively across varying domains.</p>
<p>C. Cross-domain Learning</p>
<p>Before formally introducing CDGL, we first provide an overview of cross-domain learning, a pivotal research direction in deep learning that plays a crucial role in the development of foundational models such as LLMs.Specifically, we present a unified definition of cross-domain learning and summarize common cross-domain strategies, laying the groundwork for a better understanding of CDGL.</p>
<p>Definitions.Given the source domains D S = {D S1 , . . ., D Sm |D Si = {X Si , Y Si }, i = 1, . . ., m} and target domains D T = {D T1 , . . ., D Tn |D Tj = {X Tj , Y Tj }, j = 1, . . ., n}, where for any i and j, |D Tj âˆ© D Si | â‰¥ 0, the objective of general cross-domain learning is to map the data from source domains to a shared embedding space and then learn a set of task functions based on this shared space for the target domains.Each dataset is referred to as a distinct domain, where a domain is characterized by a unique data distribution and task context.</p>
<p>First, we define the model M as a source encoder that maps all X Si âˆˆ D S to a unified embedding space E, which can be also applied for target domains:
M : X Si â†’ E, for i = 1, . . . , m.(1)
Next, we define the set of task functions for the target domains F = {f T1 , . . ., f Tn }, where each task function f Tj aims to map the embeddings M (X Tj ) to the corresponding task result Y Tj :
F = {f Tj : M (X Tj ) â†’ Y Tj , for j = 1, . . . , n}.(2)
In this framework, the objective is to learn the model M and the task functions f Tj such that the features across all domains are unified into a shared embedding space, allowing effective knowledge transfer across domains, which in turn improves generalization and performance on the target tasks.</p>
<p>The objective function for training M can be defined as:
min M m i=1 E xâˆ¼P S i (x) [L M (M (x), Y Si )] ,(3)
where L M can be an unsupervised or supervised loss function (e.g., contrastive loss or classification loss), and P Si (x) represents the distribution of the source domain data.The objective function for learning the set of task functions F can be defined as:
min f T j n j=1 E xâˆ¼P T j (x) L T (f Tj (M (x)), Y Tj ) ,(4)
where L T is the appropriate loss function for the target domain tasks.</p>
<p>Note that these two processes can be conducted In contrast, graph data presents unique challenges for cross-domain learning.Unlike text or images, graphs are not inherently structured and require manual definition of their structures, leading to complexity in graph construction and a lack of uniformity in graph representations.Additionally, graph features often vary widely across domains, and some features may not even carry semantic information, making them difficult to align.This inherent complexity and diversity in graph data contribute to the slower development of cross-domain learning in the graph domain, which remains an area of active research.</p>
<p>D. Cross-domain graph learning</p>
<p>CDGL is a key yet underexplored area within cross-domain learning.Building on the previous definition, CDGL can be described by specifying the X Ti using graph data G Ti .</p>
<p>Definitions.Derived from cross-domain learning, CDGL aims to capture cross-domain knowledge from given source-domain graph data G = {G S1 , . . ., G Si |i = 1, . . ., m} and then transfer the captured knowledge to learn a set of task functions (e.g., node classification, graph classification, etc.) on target-domain graph data G = {G T1 , . . ., G Ti |i = 1, . . ., n}.</p>
<p>1) The Refinement of Cross-domain Scale: The source and target domains in cross-domain graph learning (CDGL) are typically defined manually, and this definition plays a crucial role in determining the scope of cross-domain learning.The manner in which these domains are defined influences the scale, which in turn reflects the complexity and diversity of the domains involved.In this survey, we categorize CDGL into three distinct scales, as described below:</p>
<p>1 Limited cross-domain graph learning refers to scenarios where graph samples from source and target domains share a set of similar or consistent attributes (i.e., from the same field).For example, NaP [35] divides the Facebook100 dataset-which contains social networks from 100 U.S. universities-into different domains, treating the social network of each university as a unique domain.The data from each domain share similar attributes, except for the university types.CDGL is then conducted across different universities.Similarly, SDGA [44] treats ACMv9, Citationv1, and DBLPv7-datasets constructed from ArnetMiner [87]-as distinct domains for cross-domain learning.2 Conditional cross-domain graph learning refers to situations where the boundaries between source domains and graph domains are clearly defined, but their attributes can be transformed into a common format (e.g., text) and covered by a specific high-level field.For example, OFA [38] can convert cross-domain graph data into a text-attribute graph (TAG) format.However, it faces difficulties when applied to non-TAG domains, such as brain networks [60].3 Open cross-domain graph learning refers to scenarios where graph data from both source and target domains can be arbitrary and do not need to satisfy specific conditions.For example, GCC [26] focuses on leveraging only the structural information from datasets across domains, using it to pretrain models and learn common structural patterns in graph data.GCOPE [21] presents a general framework for crossdomain graph pre-training that unifies the feature dimensions of graph data with distinct boundaries, enabling pre-training across domains for better generalization.Importantly, these unified graph features can represent data from any domain.</p>
<p>In general, from the perspective of node features, if a CDGL method requires that the feature dimensions across datasets remain consistent, it typically falls under the limited scale; if it relies on mapping features into an existing shared representation space (e.g., a text space), it is conditional; and if it can align inherently diverse feature dimensions for cross-domain knowledge transfer, it belongs to the open scale.</p>
<p>2) The Refinement of Cross-domain Difficulty: Beyond crossdomain scales, we further discuss the difficulty of learning cross-domain knowledge during training (i.e., cross-domain difficulty) to better understand CDGL.Within the same crossdomain scale, different methods encounter varying levels of difficulty due to differences in the overlap between source and target domains.Consequently, we delineate cross-domain difficulty into three distinct levels based on the magnitude of the intersection between the source and target domains: By addressing these levels, we can better evaluate and develop models for diverse scenarios.Note that some approaches (e.g., domain adaptation approaches), which explicitly delineate source and target domains and ensure that the datasets do not overlap, actually involve both labeled source and unlabeled target samples in training.In such cases, we should consider the source and target domains as completely overlapping, representing a low-difficulty scenario.Ultimately, achieving robust performance in high-difficulty settings is key to advancing open cross-domain graph learning.</p>
<p>3) Proposed Taxonomy.: Considering the key elements of graph data (including nodes, edges, adjacency matrix, and node features, which collectively encompass both structural and feature information) and referring to the definition of the given CDGL, we propose a taxonomy (as illustrated in Figure 2) that organizes representative techniques focused on capturing generalizable cross-domain information into three main categories: (1) Structure-oriented approach, where the graph structural information from various domains is utilized as the basis for cross-domain learning.(2) Feature-oriented approach, where the graph feature information from various domains is specifically learned and leveraged.(3) Mixtureoriented approach, where both the graph structural and feature information from various domains are organically integrated and analyzed from a unified perspective.</p>
<p>In the following sections, we present a comprehensive survey along the three main categories of our taxonomy for learning cross-domain graph knowledge in graph-related tasks, respectively.</p>
<p>III. STRUCTURE-ORIENTED APPROACH</p>
<p>Structural information, as the distinctive and core characteristic of graph data, is essential for addressing real-world graphrelated problems.Existing graph models have demonstrated the ability to effectively capture structural information in singledomain scenarios.However, in cross-domain scenarios, the graphs from different domains exhibit varying node connectivity patterns, with some being densely connected and others more sparsely connected.These structural differences pose significant challenges to current graph models.To address this, structureoriented approaches aim to improve the performance of existing models by extracting commonalities in structural information across domains.Based on how structural information is utilized, these approaches can be broadly categorized into three main branches: Structure Generation and Structure Contrast.</p>
<p>A. Structure Generation</p>
<p>To effectively capture structural information, a useful approach is to establish a unified node connection strategy for graphs, ensuring that the graph data in both source domains and target domains follow the same connection rules.This allows the graph data from different domains to share common structural characteristics.The CDGL method based on structure generation is proposed with this idea in mind.It employs a shared module to generate new and distinct structures for graph data from both source and target domains.These newly generated structures enable the graph model to adopt a unified perspective for analyzing structural information across various domains, thereby enhancing the model's performance on different graph-related tasks, as shown in Figure 3.For instance, GraphControl [22] introduces ControlNet, designed specifically for graph data, to extract downstream-specific graph structures from target domain graphs.Finally, by incorporating relevant feature information into the extracted subgraph data, crossdomain learning is achieved through fine-tuning or prompting.UniAug [25] is the first to introduce diffusion models for CDGL.Training a graph diffusion model on multi-source graph structure data enables the generation of transferable graph structures tailored to various downstream domains.GA 2 E [23] is the first to reformulate graph data from different domains into subgraphs.In the pretraining phase, it leverages an autoencoder architecture to embed and reconstruct these subgraphs, enabling the graph model to utilize diverse structural information.OpenGraph [24] carefully a structure-based node tokenization scheme for graph domains, then introduces a scalable graph transformer to learn the node token embeddings and subsequently employs the LLM techniques to generate different graph structures, finally tackle various downstream graphs with variations in node sets and feature semantics.</p>
<p>B. Structure Contrast</p>
<p>In contrast to structure generation, the goal of structure contrast is to effectively compare the structures of different graph datasets and identify common structural patterns across domains, as shown in Figure 3.This approach centers on designing negative and positive sample pairs to enable meaningful comparisons between graph structures, or on learning a set of weights to compare different structure-related model parameters.For instance, GCC [26] employs structural contrastive learning to pre-train graph neural networks in a cross-domain manner, enabling models to learn transferable structural representations across diverse domains.PCRec [27] pre-trains a graph neural network using self-supervised contrastive learning on source domain graphs to capture transferable structural information, then transfers the pre-trained encoder to initialize node embeddings on the target domain, enhancing recommendation performance by balancing source and target information while reducing bias from source domains.GRADE [28] introduces a graph adaptive network for CDGL by measuring and minimizing structural distribution shifts between source and target graphs using a Graph Subtree Discrepancy based on the Weisfeiler-Lehman graph isomorphism test, effectively comparing structural information for improved transferability.FedStar [30] leverages federated learning to train structure-only graph models on different clients, compares their parameters, and integrates them via weighted averaging to create a global model that merges cross-domain structural information.APT [29] leverages structural properties to contrast graph data during cross-domain pretraining, developing graph selection strategies that help the pretrained model more effectively capture cross-domain structural patterns.BooG [31] boosts the cross-domain learning of graph foundation models by unifying structural characteristics through virtual super nodes and edges, and employs a novel contrastive learning pretraining objective to learn expressive, generalizable graph representations.ProCom [32] enhances cross-domain targeted community detection by capturing structural information of latent communities through a dual-level context-aware pretraining method and a targeted community-guided prompting mechanism.RiemannGFM [33] captures cross-domain knowledge by learning a universal structural vocabulary of graph substructures-such as trees and cycles-embedding them into Riemannian manifolds and leveraging geometric contrastive learning for robust and transferable representations.</p>
<p>C. Discussions</p>
<p>Structure-oriented approaches have demonstrated superior performance on realizing CDGL, being able to handle various complex graph structural information.Moreover, they, specifically the structure generation approaches, also exhibit strong flexibility, as the unified node connection rules can generate meaningful structures for the graphs from unseen domains.Another advantage of these methods is their ability to explore the potential for uncovering commonalities across graph data from different domains, with structural information serving as the foundation of such data.For graph data, while structural information forms the foundation, it is the feature information that plays a crucial role in conveying substantive meaning.However, structure-oriented approaches mainly focus on capturing structural information but sometimes ignore the feature information.Taking structure generation approaches as an example, they need to consider the coherence between the generated structure and feature information to ensure effective communication across graph data from different domains.</p>
<p>D. Significance and Future Implications</p>
<p>Existing structure-oriented CDGL methods hold significant potential for bridging diverse domains by capturing universal structural insights.They effectively uncover structural commonalities across domains and achieve generalization at an open scale.Although relying solely on structural information may limit performance, these approaches provide critical insights into transferring structural patterns, paving the way for open CDGL and graph foundation models that seamlessly integrate structural and feature information.</p>
<p>IV. FEATURE-ORIENTED APPROACH</p>
<p>As discussed in Section III-C, feature information plays a vital role in conveying substantive meaning in graph data, helping to differentiate between domains.The essence of featureoriented approaches is to integrate feature information from various domains to discover commonalities across cross-domain features.However, training a general graph model on data from multiple domains presents unique challenges, primarily due to the feature differences across domains, particularly when feature dimensions do not align.For example, in most cases, if the feature dimensions of source domain A and target domain B differ, their semantic meanings will inevitably differ as well, since it is highly unlikely that domains with different dimensions share the same semantics.On the other hand, when A and B have the same feature dimensions, the semantics may either be identical or distinct.If the semantics are aligned, no additional adjustments are needed, and the domains can essentially be treated as one.However, when the semantics differ, further steps are required to bridge the semantic gap.Therefore, in this section, we broadly categorize models into Dimension Align and Dimension Misalign approaches, based on whether they require feature dimension alignment.</p>
<p>A. Dimension Align</p>
<p>The earliest feature-oriented approaches are Dimension Align, meaning they perform CDGL when the feature dimensions of the graph data in the source and target domains are consistent.Typically, when the feature dimensions align, the source and target domains share a high-level domain.Dimension Align approaches are generally considered part of limited CDGL.For instance, KTN [34] introduces different feature extractors for different node types in heterogeneous graph neural networks, and then uses domain adaptation strategies to integrate cross-domain feature information.NaP [35] improves out-of-distribution generalization in graph contrastive learning by dynamically treating semantically similar cross-domain negative pairs as positive pairs based on embedding similarity, reducing the distribution gap between domains, achieving CDGL to some extent.GPF-plus [36] and RELIEF [37] are two newly proposed feature-based graph prompt techniques to align the pre-training knowledge to different downstream datasets, achieving limited CDGL across molecular domains, where the dimension of different molecular datasets can be represented in the same feature space [88].</p>
<p>B. Dimension Misalign</p>
<p>Since Dimension Align approaches are significantly limited in cross-domain scale compared to open CDGL, which has already been widely achieved in NLP and CV thanks to the rapid development of LLMs, Dimension Misalign approaches have been proposed in recent years.These methods can map graph features from different domains into a shared embedding space with the same dimension.By aligning graph feature dimensions, they relax the limitation of cross-domain scale.Depending on the feature mapping strategy used, these approaches are classified as either conditional or open CDGL.For example, OFA [38] and ZeroG [41] both use LLMs to align text-attributed features from different domains.OFA freezes LLMs and directly trains a Multi-layer Perceptron (MLP) on aligned node features to achieve zero-shot node classification, while ZeroG [41] further utilizes a LoRA fine-tuning strategy to align semantic information across different domains and then leverages a text similarity task to conduct node classification results, achieving CDGL.Similar to the former approaches, GraphAlign [39] also uses LLMs to algin features, while it additionally designs alignment strategies of feature encoding, normalization, alongside a mixture-of-feature-expert module.Besides, CDFS-GAD [40] employs a domain-adaptive graph contrastive learning module that enhances cross-domain feature alignment by optimizing contrastive loss to bring similar nodes closer and push dissimilar ones apart.Additionally, a domainspecific prompt-tuning module customizes the encoder to effectively capture and retain both common and unique features across domains.Furthermore, DAGPrompT [42] proposes GLoRA to adapt feature projection through low-rank matrix updates, aligning pre-trained GNNs with downstream task distributions.</p>
<p>C. Discussions</p>
<p>Compared with structure-oriented approaches, focusing on learning cross-domain feature commonalities to build graph models also demonstrates its own distinct advantages in handling CDGL especially when the feature information between the source domains and target domains differs significantly.However, because feature-oriented approaches often skip the dedicated extraction of structural information, they may encounter conflicts between structural and feature patterns when training a cross-domain graph model, limiting the performance upper bound.</p>
<p>D. Significance and Future Implications</p>
<p>If structure-oriented CDGL research serves as the basis for achieving open CDGL or graph foundation models, then featureoriented CDGL represents a critical breakthrough toward realizing true open cross-domain scalability.As mentioned earlier, the feature dimensions of graph data from different domains are often inconsistent, and even when they align, their semantic meanings may differ.This makes it exceedingly challenging to learn commonalities from cross-domain features.Existing feature-oriented CDGL works extensively explore how to comprehend feature information across domains and effectively capture their shared semantics.These insights are pivotal for advancing toward open CDGL, making featureoriented approaches a key enabler of this vision.</p>
<p>V. MIXTURE-ORIENTED APPROACH</p>
<p>Mixture-oriented approaches are capable of performing crossdomain learning on both structural and feature information simultaneously, exploring the commonalities of structure and feature either individually or in pairs.This has been the most mainstream research direction in CDGL.Based on the different ways of combining structure and information, we categorize Mixture-oriented approaches into two types: Sequential Mixture and Unified Mixture.</p>
<p>A. Sequential Mixture</p>
<p>Similar to sequential ensemble learning techniques [89], [90],</p>
<p>Sequential Mixture approach typically processes structural information and feature information in a sequential manner.This can involve processing the structural information, followed by the introduction of feature information.For instance, [44] first leverages a graph convolutional network (GCN) to extract high-order structural information, followed by an adversarial transformation module with shift parameters to align the source graph feature distribution with that of the target domain, thereby achieving sequential mixture.TAPE [45] leverages LLMs to generate textual explanations enriched features, which are then used by GNNs to integrate structural information, ultimately enhancing the performance of node classification on text-attributed graphs.GSPT [47] proposes to construct context using random walks first, which is used to represent structural information of the graph, and then feed the context into a transformer to perform masked feature reconstruction, which means the feature information is reconstructed afterward.SOGA [46] first leverages the structural properties of the target graph through Structure Consistency optimization before enhancing feature information through Information Maximization, enabling effective adaptation to the target domain without access to source data.Alternatively, the method can prioritize the feature information before incorporating the structural information.An example of this is UniGraph [49] starts with textual features (raw text features associated with nodes) being fed to the language model encoder (DeBERTa, E5).After processing the textual features, the resulting embeddings are then passed to the GNN encoder for further propagation across the graph.PTGB [43] prioritizes feature transformation and alignment through a data-driven atlas mapping approach before capturing structural properties during GNN pre-training.ARC [48] aligns features across different graph datasets into a common anomaly-sensitive space before utilizing residual-based graph encoding to capture high-order structural information for anomaly detection.AnyGraph [50] employs a feature-centric pre-processing approach using SVD and feature mapping before handling graph structures through an expert routing mechanism and high-order connectivity encoding, allowing for effective cross-domain adaptation.GraphLoRA [51] leverages a structure-aware contrastive lowrank adaptation module that integrates structural information from the target graph, mitigating structural discrepancies, while its Structure-aware Maximum Mean Discrepancy (SMMD) aligns node feature distributions across graphs, ensuring both structural and feature-level adaptation.SAMGPT [52] first aligns and adapts features across domains, then learns universal structure tokens during multi-domain pre-training, and finally utilizes holistic and specific prompts to transfer both unified and domain-specific structural knowledge to downstream tasks.MDGFM [53] first projects domain-specific features into a unified semantic space using unique domain tokens, and then aligns graph topologies through a graph topology-aware alignment mechanism that, via contrastive learning, extracts universal structure tokens for effective prompt-based adaptation.</p>
<p>B. Unified Mixture</p>
<p>In contrast to the Sequential Mixture approach, the Unified Mixture approach typically considers structural and feature information from a unified perspective during the training process, with the goal of achieving an organic mixture.As shown in Figure 5, unified mixture approaches typically involve two steps: (1) employing a unify function U nif y(â€¢) to extract graph structures and features simultaneously to generate node/edge/graph-level representations H rep , and (2) a task head function T askH(â€¢) is then leveraged to mapping the representations and output the predicted results, as illustrated below:</p>
<p>Mixture Unifying:
H rep = U nif y(V, E, A, X ), (5) Prediction: Å¶ = T askH(H rep ),(6)
where V, E, X denotes the set of nodes, edges, and node features, respectively.A is the adjacency matrix of the given graph.</p>
<p>Traditionally, the strategies for U nif y(â€¢) and T askH(â€¢) are part of a cohesive framework.The U nif y(â€¢) strategy is typically built upon the message-passing mechanism, where the differences lie in the ways of passing or the applied information encoding techniques.Subsequently, T askH(â€¢) is realized through a GNN-based task head, such as fully connected layers, which then outputs the task prediction results based on the H rep from U nif y(â€¢).However, with the advent of the powerful representation and reasoning capabilities of LLMs in NLP, flatten-based unifying strategies have been introduced into graph learning.These strategies can fundamentally transform graph data into semantically rich node or token sequences.LLMs then leverage these sequences to perform reasoning and generate corresponding task results [91].To help comprehensively understand these unified mixture approaches, the following sections will illustrate these two sets of unify-task strategies in detail.</p>
<p>GNN-based Unified</p>
<p>In general, GNN-based unified mixture approaches usually adopt similar selection strategies of task head functions while differing according to the passing ways or the applied information encoding techniques.For example, DA-GCN [54], UDA-GCN [54], COMMANDER [56], CCDR [61], GAST [62], DH-GAT [63], ALEX [64], ACT [65], ALCDR [69], and DGASN [67] all adopt domain adaptation training frameworks, differing mainly in their approaches to unifying data from source and target domains.Specifically, DA-GCN and UDA-GCN utilize graph convolutional networks (GCNs) with domain adaptation, which explores the initial possibilities for Unified Mixture CDGL by employing GCNs to aggregate structural and feature information together in a preliminary unified manner.To better handle the unified structure and feature information, more advanced methods have been introduced, such as graph attention mechanisms, hyperbolic representations, and contrastive learning.COMMANDER employs a graph-attentive encoder for domain alignment, providing more flexibility and dynamic focus during feature aggregation.CCDR proposes a unified contrastive learning framework for cross-domain recommendation, utilizing both intra-domain and inter-domain contrastive learning to enhance representation learning and effectively transfer user preferences between domains.GAST uses a hybrid graph attention mechanism along with a POS-Transformer to integrate syntactic and sequential semantics, enhancing both structural and semantic representation.DH-GAT applies hyperbolic graph attention networks to model hierarchical structures effectively, leveraging the power of hyperbolic space to represent latent relationships.ALEX leverages singular value decomposition (SVD) and domain discriminators to unify cross-domain representations, facilitating effective feature alignment.ACT employs contrastive learning with domain alignment for anomaly detection, focusing on learning similarities and differences between domains to generate cohesive representations.ALCDR uses an optimal transportbased anchor link learning mechanism to adaptively transfer knowledge across domains, providing a robust means of crossdomain unification.DGASN employs a graph attention network (GAT) to simultaneously learn node and edge embeddings, and using adversarial domain adaptation to align the source and target distributions, thus achieving domain-invariant graph representations.</p>
<p>Additionally, CDGEncoder [58], PGPRec [70], STGP [68], GCOPE [21], UniGLM [73], CrossLink [71], and MDGPT [72] all involve a pretraining phase, but they differ in their information encoding or propagation methods during pretraining.Specifically, the CDGEncoder uses a multi-view GNN encoder combined with attention mechanisms to aggregate contextual and topological views.PGPRec introduces personalized graph prompts and uses contrastive learning during pretraining to enhance user/item representations.STGP utilizes a two-stage prompting strategy to adapt both domain and task-specific properties across spatio-temporal tasks.GCOPE employs graph coordinators to align diverse graph datasets during pretraining, mitigating negative transfer.UniGLM considers introducing LLMs as the encoder for text attributes in the traditional pretraining paradigm with a tailored contrastive learning approach based on the adaptive positive sample selection to align features and structures.CrossLink leverages a decode-only transformer to model the temporal evolution of dynamic graphs for link prediction.Finally, MDGPT uses domain tokens and dual prompts to unify and adapt multi-domain graph features, enhancing cross-domain generalization.</p>
<p>In addition to the above two mainstream paradigms, domain generalization, multi-task learning, and federated learning have also been explored.For example, CrossHG-Meta [59] proposes to process structural and feature information simultaneously by aggregating heterogeneous information from multiple semantic contexts and then designs a unified domain generalization framework for CDGL, especially for heterogeneous graphs.METABrainC [60] proposes a pre-training cross-domain framework via meta-learning techniques using multiple selfsupervised tasks on source datasets and adapts it to target tasks via fine-tuning, enhancing cross-domain brain connectome analysis.GCFL [57] employs a clustering algorithm to group graph models trained on clients based on parameter similarities, thereby achieving a unified representation of structural and feature information within a federated learning framework, ultimately enhancing cross-domain generalization.</p>
<p>Flatten-based Unified</p>
<p>Referring to the input of LLMs, flatten-based Unified Mixture approaches focus on converting graphs into textual descriptions by transforming them into sequences of nodes or tokens.These approaches then develop tailored learning strategies to allow LLMs to process these sequences and generate task-specific outputs effectively.For instance, GIMLET [74] introduces a unified graph-text approach called Generalized Position Embedding, which transforms graphs into token sequences and seamlessly combines graph structure with LLM-driven text processing.GraphTranslator [75] employs a Translator module to convert node embeddings into token sequences, enabling effective integration of pre-trained graph models and LLMs, allowing LLMs to handle both pre-defined and open-ended graph tasks.GraphGPT [76] leverages a dual-stage instruction tuning mechanism to align graph structural and semantic information with LLMs, enhancing cross-domain generalization through both graph structural alignment and node feature transfer.GITA [77] combines graph visualization with textual conversion, using a Vision-Language Model (VLM) to jointly reason over visual and textual graph data, providing a comprehensive integration of graph semantics and structure.InstructGraph [78] uses a structured format verbalizer to convert graph data into text sequences, combined with instruction tuning and preference optimization to improve graph reasoning and reduce hallucinations, making LLMs suitable for graphcentric tasks.TEA-GLM [79] employs a linear projection module to align pre-trained Graph Neural Network (GNN) representations with LLM token embeddings, enabling zeroshot predictions across multiple graph tasks without modifying the LLM itself.GOFA [80] introduces a hybrid architecture that interleaves GNN layers with a pre-trained LLM, combining structural and semantic modeling abilities while transforming graph data into flattened text descriptions for LLM-based text generation.HiGPT [81] utilizes a heterogeneous graph tokenizer to transform complex graph structures into textual representations, employing a novel instruction-tuning framework to enhance LLM's understanding of intra-and internode relationships.GraphCLIP [82] uses a graph-summary contrastive learning approach by generating textual descriptions of subgraphs with an LLM, which improves zero-shot and fewshot transfer capabilities through graph prompt tuning and invariant learning.GraphWiz [83] leverages the GraphInstruct dataset to train LLMs in solving graph computational problems, using an instruction-following framework that transforms graphs into textual sequences, allowing explicit reasoning paths for improved task performance.</p>
<p>C. Discussions</p>
<p>Mixture-oriented approaches mainly focus on simultaneously integrate structure and feature information across domains, enabling deep learning models to more comprehensively capture commonalities across domains.Nevertheless, the absence of a powerful feature alignment technique that effectively maps datasets from different domains into a shared feature space remains a significant limitation for feature-oriented approaches.As shown in Table I, earlier mixture-oriented approaches were only able to scale to limited cross-domain settings.Recently, some feature alignment techniques, such as large language models (LLMs), have been introduced, but they have only managed to achieve performance at a conditional scale.Indeed, a few newly proposed methods have demonstrated the potential to achieve open cross-domain scale, but their primary focus has been on exploring the feasibility of true open CDGL.For example, some approaches utilize domain-specific linear projection layers to align feature dimensions across domains, yet the resulting performance improvements have been limited.</p>
<p>D. Significance and Future Implications</p>
<p>Mixture-oriented approaches are fundamentally designed to effectively integrate structure-oriented and feature-oriented methods, aiming to achieve a synergistic effect where the whole is greater than the sum of its parts (i.e., 1 + 1 &gt; 2).For instance, since the cross-domain representation spaces derived from structural and feature information are inherently distinct, finding an organic way for these spaces to adapt to each other is both challenging and crucial.Current explorations in mixtureoriented approaches not only offer valuable insights but also pave the way for achieving truly open CDGL and even the development of graph foundation models.</p>
<p>VI. FUTURE DIRECTIONS</p>
<p>Table I summarizes the graph models that transfer particular graph information to achieve CDGL according to the proposed taxonomy.Based on the review and analysis illustrated above, we believe that there is still much space for further enhancement in this field, especially for realizing the true graph foundation models.In this section, we discuss the remaining limitations of capturing graph information across domains used for achieving effective CDGL and list some directions for further exploration in subsequent research.</p>
<p>A. Feature Alignment.</p>
<p>The correlation between feature dimensions and semantics across graph data from different domains is a crucial aspect to consider in CDGL.Misaligned feature dimensions will hinder the training of a general graph model capable of handling graph data from various domains, while misaligned semantics will prevent the trained graph model from capturing shared characteristics across domains, thereby limiting the performance of CDGL.</p>
<p>In existing CDGL methods, those targeting limited-scale scenarios can often skip this issue, as the graph data from source and target domains typically share identical feature dimensions by nature, such as in molecular networks [17], [36].For conditional scenarios, which mainly focus on graph data in recommendation systems [70] and text-attributed graphs (TAGs) [38], [41], feature alignment challenges are circumvented in different ways: Graph data in recommendation systems may come from different domains, but they are all built on the basis of users and items, thus avoiding feature alignment problems.For TAGs, features can be easily aligned using embeddings generated by large language models (LLMs).</p>
<p>However, these methods are insufficient for addressing open CDGL, where feature dimensions and semantics are highly complex and heterogeneous across domains.Although some methods have begun exploring feature alignment solutions for more open scenarios, the research is still in its early stages.For instance, methods like ProG [93], MDGPT [72], and CDFS-GAD [40] have attempted to align feature dimensions across different datasets using techniques such as SVD or MLP.However, even after aligning feature dimensions, the aligned features may still lack semantic equivalence.Therefore, proposing a universal feature alignment approach to handle datasets from different domains is a key research direction towards achieving effective open CDGL.TABLE I: A summary of models that leverage cross-domain knowledge to enhance graph-related tasks in the literature, ordered by their release time.Paradigm denotes the core technique used to achieve Cross-Domain Graph Learning.Domain indicates the domains across which knowledge is transferred.Scale and Difficulty refer to the cross-domain scales and the level of difficulty.Fine-tuning denotes whether it is necessary to fine-tune the parameters of Graph Neural Networks (GNNs).Prompting indicates the use of graph-or text-formatted prompts.Acronyms in Task: Node refers to node-level tasks; Link refers to link-level tasks; Graph refers to graph-level tasks; Reasoning refers to Graph Reasoning.</p>
<p>B. Dealing with the Scale of Cross-domain Datasets.</p>
<p>In NLP and CV, the cross-domain generalization ability of foundation models follows the Scaling Law, which suggests that generalization improves with an increase in model size and the number of training samples.Typically, training an effective foundation model requires a large volume of samples, often in the order of billions, whereas traditional CDGL models generally rely on much smaller training datasets.This discrepancy in data volume limits the cross-domain performance and generalization capabilities of these models.While structureoriented methods can process large-scale graph-structured data and achieve open CDGL at the structural level, the lack of feature information restricts their performance.However, when both structural and feature information are considered, traditional CDGL methods usually achieve only limited or conditional cross-domain learning, inherently constraining the diversity and scale of the datasets used for training.Although certain models, such as GCOPE, can alleviate some limitations on dataset diversity and scale to achieve open CDGL, the scale and diversity of the datasets remain limited.Therefore, to significantly improve the performance of graph models in open cross-domain scenarios and contribute to true graph foundation models, large-scale and diverse datasets are essential.</p>
<p>C. Overcoming data leakage.</p>
<p>Table I demonstrates that LLMs are receiving increasing attention in CDGL, exploring their potential capabilities in this area.However, the introduction of LLMs inevitably brings about the issue of data leakage, which has become a focal point of discussion [94].Specifically, since LLMs are pretrained on extensive text corpora, it's likely that they may have seen and memorized at least part of the test data from common benchmark datasets, especially in citation networks.This undermines the reliability of current studies that rely on earlier benchmark datasets.Moreover, [95] demonstrates that specific prompts could potentially enhance the "activation" of LLMs' corresponding memory, thereby influencing evaluation outcomes.Both [96] and [45] have attempted to avoid the data leakage issue by collecting a new citation dataset, ensuring that the test papers are sampled from time periods after ChatGPT's data cut-off.However, these efforts remain limited to the citation domain, and the impact of graph structures in their datasets is not significant.Therefore, it's crucial to reconsider the methods employed to accurately evaluate the performance of LLMs on graph-related tasks.A fair, systematic, and comprehensive benchmark is also needed.</p>
<p>D. Improving interpretability.</p>
<p>Interpretability, also known as explainability, refers to the ability to explain or present a model's behavior in terms that humans can understand [91].In CDGL, most methods utilize Graph Neural Networks (GNNs).However, GNNs are typically treated as black boxes [97] and lack interpretability.While these methods consider the relationships among various pieces of information in the graph during their design and have experimentally demonstrated effectiveness in CDGL, their improvements are often based on intuition and a high-level understanding of graph data.Consequently, their interpretability remains poor.To enhance interpretability, CDGL approaches that integrate Large Language Models (LLMs) have been proposed.This is primarily due to LLMs' reasoning and explanatory abilities, which enable them to produce user-friendly explanations for graph reasoning tasks [83], [91].However, most of these approaches are limited by their reliance on Text-Attributed Graphs (TAGs); they can only provide explanations for conditional CDGL scenarios.Extending the powerful reasoning and explanatory abilities of LLMs from TAGs to non-TAGs is also an important direction.This expansion directly affects the use of CDGL models in applications with strict security requirements, such as network fault diagnosis [8], [9].</p>
<p>E. Unified Cross-domain Evaluation Metrics.</p>
<p>As discussed in Sections II-C and II-D, the ultimate goal of CDGL is to develop a foundational model capable of generalizing across any domain.However, most existing approaches evaluate model performance using classical graph-related task metrics, assessing cross-domain generalization ability based on performance across different datasets.This approach may not fully capture the model's ability to generalize from a global perspective.Consequently, designing a unified cross-domain evaluation metric to holistically assess generalization ability represents a promising direction for future research.</p>
<p>F. Compatibility Analysis and Improvement across Domains.</p>
<p>Existing CDGL approaches often treat data from different domains uniformly, aiming to uncover correlations in structure, features, or structural-feature patterns to enhance cross-domain graph representation learning for various tasks.However, in practice, the compatibility of information between domains varies significantly.Similar to how humans find it easier to transfer knowledge between closely related fields (e.g., applying mathematical principles to physics) than between unrelated fields (e.g., applying musical theory to physics), analyzing domain compatibility and making targeted adjustments during cross-domain learning is a critical area for further investigation.</p>
<p>VII. CONCLUSION</p>
<p>The mining of commonalities in data from different domains has led CDGL to emerge as a prominent area of research in recent years, especially as it is a key point for achieving true graph foundation models.In this survey, we aim to provide an in-depth overview of existing strategies for transferring knowledge across domains.First, we introduce a novel taxonomy that categorizes techniques involving various crossdomain scales into three categories based on the different graph information transferred by specific models: structure, features, and a mixture of both.Next, we systematically review representative studies according to this taxonomy.Finally, we discuss some limitations and highlight several future research directions.Through this comprehensive review, we aspire to shed light on the advancements and challenges in the field of CDGL, thereby encouraging further progress in this domain.</p>
<p>Fig. 1 :
1
Fig. 1: The purpose of cross-domain graph learning aims to integrate graph knowledge across a myriad of graph domains and transfer the knowledge to other domains.</p>
<p>Fig. 2 :
2
Fig. 2: A taxonomy of graph models for solving cross-domain graph learning with representative examples.</p>
<p>1
1
High-Difficulty (When |D Tj âˆ© D Si | = 0): In this case, the source and target domains share no common data or features, indicating a complete lack of overlap.This extreme training setting is often adopted when the target domain has very limited data.It also aligns with the ultimate goal of CDGL-achieving open cross-domain graph learning under challenging conditions.A model that performs well here typically demonstrates strong generalizability.2 Moderate-Difficulty (When 0 &lt; |D Tj | âˆ’ |D Tj âˆ© D Si | &lt; |D Tj |): In this scenario, the target domain shares either partial or complete overlap with the source domain.The difficulty is moderate because CDGL models can learn shared cross-domain knowledge in advance, reducing the overall challenge.However, unique domain characteristics may still require fine-tuning or domain-specific adjustments to address discrepancies. 3 Low-Difficulty (When |D Tj | âˆ’ |D Tj âˆ© D Si | = 0): This scenario indicates that the target domain is entirely equal to the source domain, with both domains sharing the same representation space.Cross-domain learning in this setting is relatively easy because the model does not need to transfer knowledge from source to target domains but only needs to capture cross-domain knowledge directly.This situation is more suitable for cases where both domains have abundant data, and the goal is to use a single model to handle tasks across different domains.Note that, in general, high overlap does not necessarily guarantee good generalization.</p>
<p>Fig. 3 :
3
Fig.3: The core ideas behind structure-oriented approaches for capturing cross-domain knowledge consist of two main strategies: a) structure generation approaches, which employ a shared generator module to generate new and distinct structures for graph data from both source and target domains; b) structure contrast approaches, which construct positive and negative sample pairs for various graph data or leverage graph structure information to train GNNs and then make model parameter contrast across domains.</p>
<p>Fig. 4 :
4
Fig.4: The core ideas of feature-oriented approaches: a) feature align approaches, which directly leverage a trainable feature extractor to mine the commonalities among source and target domains; b) feature misalign approaches, which need to align the graph data with different feature dimensions across domains for training a general graph models.Note that both approaches can be further enhanced by prompting techniques.</p>
<p>Fig. 5 :
5
Fig. 5: The core ideas of mixture-oriented approaches.a) Feature-Structure Mixture: Feature information is prioritized, and structure information is integrated afterward.b) Structure-Feature Mixture: Structure information is prioritized, and feature information is integrated afterward.c) GNN-based Unified Mixture: Structural and feature information are jointly extracted and processed using a unified extractor during GNN training.d) Flatten-based Unified Mixture: Graph data is transformed into textual sequences through a flattening process, enabling large language models (LLMs) to handle both structural and feature information for downstream tasks.This categorization details two distinct integration philosophies: Sequential-Mixture (a-b) and Unified-Mixture (c-d).</p>
<p>either through joint training or separate training.The conditions |D Tj âˆ©D Si | reflect, to some extent, the difficulty of transferring cross-domain knowledge during the training process.
of pre-trained language models (PLMs), like BERT andGPT, enabled generalization across domains by leveragingcontextual embeddings and task-specific fine-tuning. Morerecently, large language models (LLMs), such as GPT-3 andGPT-4, have demonstrated strong cross-domain capabilitiesthrough massive pretraining on diverse corpora, excelling inzero-shot and few-shot learning. Furthermore, multi-modaltechniques have extended cross-domain learning beyondtext, allowing models to integrate information from othermodalities, such as images and structured data, to addresscomplex tasks like visual question answering and multi-modal dialogue.2 CV. The development of cross-domain learning in computervision (CV) follows a similar trajectory to that of NLP. Earlyapproaches rely on domain adaptation techniques such asadversarial training, feature alignment, and style transfer toaddress challenges posed by domains with different visualcharacteristics. As the field advances, domain generalizationmethods, including invariant feature learning and meta-learning, emerge to improve model robustness to unseendomains. The introduction of pre-trained vision models likeResNet and Vision Transformers (ViTs) further the progressby enabling models to generalize across domains through thereuse of learned features, which can then be fine-tuned fordownstream tasks. Recently, multi-modal models like CLIPand Sora demonstrate exceptional cross-domain capabilitiesby integrating visual and language understanding. For in-stance, Sora excels in tasks like image captioning, visualquestion answering, and zero-shot reasoning, benefiting fromlarge-scale pretraining on multi-modal datasets. Just as inNLP, the combination of visual and textual data allows thesemodels to achieve superior cross-domain performance.3 Summary. The rapid development of cross-domain learningin NLP and CV can be attributed to the relatively straight-forward alignment of features across domains, such as textand images. These fields benefit from shared representationformats-text and images-where semantic or visual featuresare easier to align across different domains. As a result, whenthe data scale increases, models can more easily learn sharedcross-domain patterns, enabling better generalization.The development of cross-domain learning in various fields.To better understand this research direction, we introducethe development of cross-domain learning in the fields ofNatural Language Processing (NLP) and Computer Vision (CV),focusing on how cross-domain learning is achieved within thesedomains.1 NLP. The development of cross-domain learning in NLPhas progressed significantly, driven by innovations in modelarchitectures, training strategies, and data availability. Earlyefforts relied on domain adaptation techniques, such as fea-ture alignment and adversarial learning, to transfer knowledgefrom resource-rich domains to low-resource ones. Alongsidethese, domain generalization techniques, which aim toimprove model robustness by learning domain-invariantrepresentations, have also been widely explored. The rise
ACKNOWLEDGMENTSThis should be a simple paragraph before the References to thank those individuals and institutions who have supported your work on this article.
A survey on crossdomain recommendation: taxonomies, methods, and future directions. T Zang, Y Zhu, H Liu, R Zhang, J Yu, ACM Transactions on Information Systems. 4122022</p>
<p>Modelling high-order social relations for item recommendation. Y Liu, L Chen, X He, J Peng, Z Zheng, J Tang, IEEE Transactions on Knowledge and Data Engineering. 3492020</p>
<p>Diga: guided diffusion model for graph recovery in anti-money laundering. X Li, Y Li, X Mo, H Xiao, Y Shen, L Chen, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023</p>
<p>Network analytics for anti-money laundering-a systematic literature review and experimental evaluation. B Deprez, T Vanderschueren, W Verbeke, B Baesens, T Verdonck, arXiv:2405.193832024arXiv preprint</p>
<p>Cirt: Global subseasonal-to-seasonal forecasting with geometry-inspired transformer. Y Liu, Z Zheng, J Cheng, F Tsung, D Zhao, Y Rong, J Li, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Improving molecular property prediction through a task similarity enhanced transfer learning strategy. H Li, X Zhao, S Li, F Wan, D Zhao, J Zeng, Iscience. 25102022</p>
<p>A knowledgeguided pre-training framework for improving molecular representation learning. H Li, R Zhang, Y Min, D Ma, D Zhao, J Zeng, Nature Communications. 14175682023</p>
<p>A dual-system method for intelligent fault localization in communication networks. J Ji, F Zhu, J Cui, H Zhao, B Yang, ICC 2022-IEEE International Conference on Communications. IEEE2022</p>
<p>Effective fault scenario identification for communication networks via knowledge-enhanced graph neural networks. H Zhao, B Yang, J Cui, Q Xing, J Shen, F Zhu, J Cao, IEEE Transactions on Mobile Computing. 2342024</p>
<p>Collective classification in network data. P Sen, G Namata, M Bilgic, L Getoor, B Galligher, T Eliassi-Rad, AI magazine. 2932008</p>
<p>Urban region pre-training and prompting: A graph-based approach. J Jin, Y Song, D Kan, H Zhu, X Sun, Z Li, X Sun, J Zhang, arXiv:2408.059202024arXiv preprint</p>
<p>Semi-supervised classification with graph convolutional networks. T N Kipf, M Welling, arXiv:1609.029072016arXiv preprint</p>
<p>Graph attention networks. P VeliÄkoviÄ‡, G Cucurull, A Casanova, A Romero, P LiÃ², Y Bengio, International Conference on Learning Representations. 2018</p>
<p>Segno: Generalizing equivariant graph neural networks with physical inductive biases. Y Liu, J Cheng, H Zhao, T Xu, P Zhao, F Tsung, J Li, Y Rong, The Twelfth International Conference on Learning Representations. </p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>Graph prompt learning: A comprehensive survey and beyond. X Sun, J Zhang, X Wu, H Cheng, Y Xiong, J Li, arXiv:2311.165342023arXiv preprint</p>
<p>Large language models on graphs: A comprehensive survey. B Jin, G Liu, C Han, M Jiang, H Ji, J Han, IEEE Transactions on Knowledge and Data Engineering. 2024</p>
<p>Graph contrastive learning with augmentations. Y You, T Chen, Y Sui, T Chen, Z Wang, Y Shen, Advances in neural information processing systems. 202033</p>
<p>All in one: Multi-task prompting for graph neural networks. X Sun, H Cheng, J Li, B Liu, J Guan, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023</p>
<p>A survey on self-supervised pre-training of graph foundation models: A knowledge-based perspective. Z Zhao, Y Li, Y Zou, R Li, R Zhang, arXiv:2403.161372024arXiv preprint</p>
<p>All in one and one for all: A simple yet effective method towards cross-domain graph pretraining. H Zhao, A Chen, X Sun, H Cheng, J Li, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>Graphcontrol: Adding conditional control to universal graph pre-trained models for graph domain transfer learning. Y Zhu, Y Wang, H Shi, Z Zhang, D Jiao, S Tang, Proceedings of the ACM on Web Conference 2024. the ACM on Web Conference 20242024</p>
<p>Exploring task unification in graph representation learning via generative approach. Y Hu, S Ouyang, Z Yang, G Chen, J Wan, X Wang, Y Liu, arXiv:2403.143402024arXiv preprint</p>
<p>Opengraph: Towards open graph foundation models. L Xia, B Kao, C Huang, arXiv:2403.011212024arXiv preprint</p>
<p>Cross-domain graph data scaling: A showcase with diffusion models. W Tang, H Mao, D Dervovic, I Brugere, S Mishra, Y Xie, J Tang, arXiv:2406.018992024arXiv preprint</p>
<p>Gcc: Graph contrastive coding for graph neural network pre-training. J Qiu, Q Chen, Y Dong, J Zhang, H Yang, M Ding, K Wang, J Tang, Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining. the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining2020</p>
<p>Pre-training graph neural network for cross domain recommendation. C Wang, Y Liang, Z Liu, T Zhang, S Y Philip, 2021 IEEE Third International Conference on Cognitive Machine Intelligence (CogMI). </p>
<p>Non-iid transfer learning on graphs. J Wu, J He, E Ainsworth, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337350</p>
<p>Better with less: A data-active perspective on pre-training graph neural networks. J Xu, R Huang, X Jiang, Y Cao, C Yang, C Wang, Y Yang, Advances in Neural Information Processing Systems. 202336</p>
<p>Federated learning on non-iid graphs via structural knowledge sharing. Y Tan, Y Liu, G Long, J Jiang, Q Lu, C Zhang, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202337</p>
<p>Boosting graph foundation model from structural perspective. Y Cheng, Y Zhao, J Yu, X Li, arXiv:2407.199412024arXiv preprint</p>
<p>Procom: A few-shot targeted community detection algorithm. X Wu, K Xiong, Y Xiong, X He, Y Zhang, Y Jiao, J Zhang, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>Riemanngfm: Learning a graph foundation model from riemannian geometry. L Sun, Z Huang, S Zhou, Q Wan, H Peng, P Yu, arXiv:2502.032512025arXiv preprint</p>
<p>Zero-shot transfer learning within a heterogeneous graph via knowledge transfer networks. M Yoon, J Palowitch, D Zelle, Z Hu, R Salakhutdinov, B Perozzi, Advances in Neural Information Processing Systems. 202235</p>
<p>Negative as positive: Enhancing out-of-distribution generalization for graph contrastive learning. Z Wang, B Xu, Y Yuan, H Shen, X Cheng, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>
<p>Universal prompt tuning for graph neural networks. T Fang, Y Zhang, Y Yang, C Wang, L Chen, Advances in Neural Information Processing Systems. 202436</p>
<p>Relief: Reinforcement learning empowered graph feature prompt tuning. J Zhu, Z Ding, J Yu, J Tan, X Li, W Qian, arXiv:2408.031952024arXiv preprint</p>
<p>One for all: Towards training one graph model for all classification tasks. H Liu, J Feng, L Kong, N Liang, D Tao, Y Chen, M Zhang, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Graphalign: Pretraining one graph neural network on multiple graphs via feature alignment. Z Hou, H Li, Y Cen, J Tang, Y Dong, arXiv:2406.029532024arXiv preprint</p>
<p>Towards cross-domain few-shot graph anomaly detection. J Chen, S Fu, Z Zhang, Z Ma, M Feng, T S Wirjanto, Q Peng, arXiv:2410.086292024arXiv preprint</p>
<p>Zerog: Investigating crossdataset zero-shot transferability in graphs. Y Li, P Wang, Z Li, J X Yu, J Li, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>Dagprompt: Pushing the limits of graph prompting with a distribution-aware graph prompt tuning approach. Q Chen, L Wang, B Zheng, G Song, arXiv:2501.151422025arXiv preprint</p>
<p>Ptgb: Pre-train graph neural networks for brain network analysis. Y Yang, H Cui, C Yang, arXiv:2305.143762023arXiv preprint</p>
<p>Semisupervised domain adaptation in graph transfer learning. Z Qiao, X Luo, M Xiao, H Dong, Y Zhou, H Xiong, 10.24963/ijcai.2023/253Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23, E. Elkind. the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23, E. Elkindmain Track20238</p>
<p>Harnessing explanations: LLM-to-LM interpreter for enhanced textattributed graph representation learning. X He, X Bresson, T Laurent, A Perold, Y Lecun, B Hooi, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Source free graph unsupervised domain adaptation. H Mao, L Du, Y Zheng, Q Fu, Z Li, X Chen, S Han, D Zhang, Proceedings of the 17th ACM International Conference on Web Search and Data Mining. the 17th ACM International Conference on Web Search and Data Mining2024</p>
<p>A pure transformer pretraining framework on text-attributed graphs. Y Song, H Mao, J Xiao, J Liu, Z Chen, W Jin, C Yang, J Tang, H Liu, arXiv:2406.138732024arXiv preprint</p>
<p>Arc: A generalist graph anomaly detector with in-context learning. Y Liu, S Li, Y Zheng, Q Chen, C Zhang, S Pan, arXiv:2405.167712024arXiv preprint</p>
<p>Unigraph: Learning a cross-domain graph foundation model from natural language. Y He, B Hooi, arXiv:2402.136302024arXiv preprint</p>
<p>Anygraph: Graph foundation model in the wild. L Xia, C Huang, arXiv:2408.107002024arXiv preprint</p>
<p>Graphlora: Structure-aware contrastive low-rank adaptation for cross-graph transfer learning. Z.-R Yang, J Han, C.-D Wang, H Liu, arXiv:2409.166702024arXiv preprint</p>
<p>Samgpt: Text-free graph foundation model for multi-domain pre-training and cross-domain adaptation. X Yu, Z Gong, C Zhou, Y Fang, H Zhang, arXiv:2502.054242025arXiv preprint</p>
<p>Multi-domain graph foundation models: Robust knowledge transfer via topology alignment. S Wang, B Wang, Z Shen, B Deng, Z Kang, arXiv:2502.020172025arXiv preprint</p>
<p>Unsupervised domain adaptive graph convolutional networks. M Wu, S Pan, C Zhou, X Chang, X Zhu, Proceedings of the web conference 2020. the web conference 20202020</p>
<p>Da-gcn: A domain-aware attentive graph convolution network for shared-account cross-domain sequential recommendation. L Guo, L Tang, T Chen, L Zhu, Q V H Nguyen, H Yin, arXiv:2105.033002021arXiv preprint</p>
<p>Cross-domain graph anomaly detection. K Ding, K Shu, X Shan, J Li, H Liu, IEEE Transactions on Neural Networks and Learning Systems. 3362021</p>
<p>Federated graph classification over non-iid graphs. H Xie, J Ma, L Xiong, C Yang, Advances in neural information processing systems. 202134</p>
<p>Cross-domain few-shot graph classification. K Hassani, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Few-shot heterogeneous graph learning via cross-domain knowledge transfer. Q Zhang, X Wu, Q Yang, C Zhang, X Zhang, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2022</p>
<p>Dataefficient brain connectome analysis via multi-task meta-learning. Y Yang, Y Zhu, H Cui, X Kan, L He, Y Guo, C Yang, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2022</p>
<p>Contrastive cross-domain recommendation in matching. R Xie, Q Liu, L Wang, S Liu, B Zhang, L Lin, Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining. the 28th ACM SIGKDD conference on knowledge discovery and data mining2022</p>
<p>Graph adaptive semantic transfer for cross-domain sentiment classification. K Zhang, Q Liu, Z Huang, M Cheng, K Zhang, M Zhang, W Wu, E Chen, Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval2022</p>
<p>Decoupled hyperbolic graph attention network for cross-domain named entity recognition. J Xu, Y Cai, Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval2023</p>
<p>Alex: Towards effective graph transfer learning with noisy labels. J Yuan, X Luo, Y Qin, Z Mao, W Ju, M Zhang, Proceedings of the 31st ACM international conference on multimedia. the 31st ACM international conference on multimedia2023</p>
<p>Cross-domain graph anomaly detection via anomaly-aware contrastive alignment. Q Wang, G Pang, M Salehi, W Buntine, C Leckie, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Crossdomain few-shot graph classification with a reinforced task coordinator. Q Zhang, S Pei, Q Yang, C Zhang, N V Chawla, X Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Domain-adaptive graph attention-supervised network for cross-network edge classification. X Shen, M Shao, S Pan, L T Yang, X Zhou, IEEE Transactions on Neural Networks and Learning Systems. 2023</p>
<p>Prompt-enhanced spatio-temporal graph transfer learning. J Hu, X Liu, Z Fan, Y Yin, S Xiang, S Ramasamy, R Zimmermann, arXiv:2405.124522024arXiv preprint</p>
<p>Beyond the overlapping users: Cross-domain recommendation via adaptive anchor link learning. Y Zhao, C Li, J Peng, X Fang, F Huang, S Wang, X Xie, J Gong, Proceedings of the 46th international ACM SIGIR conference on research and development in information retrieval. the 46th international ACM SIGIR conference on research and development in information retrieval2023</p>
<p>Contrastive graph prompt-tuning for cross-domain recommendation. Z Yi, I Ounis, C Macdonald, ACM Transactions on Information Systems. 4222023</p>
<p>Enhancing cross-domain link prediction via evolution process modeling. X Huang, W Chow, Y Zhu, Y Wang, Z Chai, C Wang, C Lei, Y Yang, 2025in THE WEB CONFERENCE 2025</p>
<p>Text-free multi-domain graph pre-training: Toward graph foundation models. X Yu, C Zhou, Y Fang, X Zhang, arXiv:2405.139342024arXiv preprint</p>
<p>Uniglm: Training one unified language model for text-attributed graphs. Y Fang, D Fan, S Ding, N Liu, Q Tan, arXiv:2406.120522024arXiv preprint</p>
<p>Gimlet: A unified graph-text model for instruction-based molecule zeroshot learning. H Zhao, S Liu, M Chang, H Xu, J Fu, Z Deng, L Kong, Q Liu, Advances in Neural Information Processing Systems. 202336</p>
<p>Graphtranslator: Aligning graph model to large language model for open-ended tasks. M Zhang, M Sun, P Wang, S Fan, Y Mo, X Xu, H Liu, C Yang, C Shi, Proceedings of the ACM on Web Conference 2024. the ACM on Web Conference 20242024</p>
<p>Graphgpt: Graph instruction tuning for large language models. J Tang, Y Yang, W Wei, L Shi, L Su, S Cheng, D Yin, C Huang, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>
<p>GITA: Graph to visual and textual integration for vision-language graph reasoning. Y Wei, S Fu, W Jiang, Z Zhang, Z Zeng, Q Wu, J Kwok, Y Zhang, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>Instructgraph: Boosting large language models via graph-centric instruction tuning and preference alignment. J Wang, J Wu, Y Hou, Y Liu, M Gao, J Mcauley, arXiv:2402.087852024arXiv preprint</p>
<p>Llms as zero-shot graph learners: Alignment of gnn representations with llm token embeddings. D Wang, Y Zuo, F Li, J Wu, arXiv:2408.145122024arXiv preprint</p>
<p>GOFA: A generative one-for-all model for joint graph language modeling. L Kong, J Feng, H Liu, C Huang, J Huang, Y Chen, M Zhang, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Higpt: Heterogeneous graph language model. J Tang, Y Yang, W Wei, L Shi, L Xia, D Yin, C Huang, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>Graphclip: Enhancing transferability in graph foundation models for text-attributed graphs. Y Zhu, H Shi, X Wang, Y Liu, Y Wang, B Peng, C Hong, S Tang, arXiv:2410.103292024arXiv preprint</p>
<p>Graphwiz: An instruction-following language model for graph computational problems. N Chen, Y Li, J Tang, J Li, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>A survey on crossdomain sequential recommendation. S Chen, Z Xu, W Pan, Q Yang, Z Ming, arXiv:2401.049712024arXiv preprint</p>
<p>node2vec: Scalable feature learning for networks. A Grover, J Leskovec, Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining2016</p>
<p>Deepwalk: Online learning of social representations. B Perozzi, R Al-Rfou, S Skiena, Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. the 20th ACM SIGKDD international conference on Knowledge discovery and data mining2014</p>
<p>Arnetminer: extraction and mining of academic social networks. J Tang, J Zhang, L Yao, J Li, L Zhang, Z Su, Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. the 14th ACM SIGKDD international conference on Knowledge discovery and data mining2008</p>
<p>A knowledgeguided pre-training framework for improving molecular representation learning. H Li, R Zhang, Y Min, D Ma, D Zhao, J Zeng, Nature Communications. 14175682023</p>
<p>Sequential ensemble learning for outlier detection: A bias-variance perspective. S Rayana, W Zhong, L Akoglu, 2016 IEEE 16th international conference on data mining (ICDM. IEEE2016</p>
<p>Image denoising via sequential ensemble learning. X Yang, Y Xu, Y Quan, H Ji, IEEE Transactions on Image Processing. 292020</p>
<p>A survey of graph meets large language model: Progress and future directions. Y Li, Z Li, P Wang, J Li, X Sun, H Cheng, J X Yu, 10.24963/ijcai.2024/898Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24. K Larson, the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-2420248survey Track</p>
<p>Da-gcn: A domain-aware attentive graph convolution network for shared-account cross-domain sequential recommendation. L Guo, L Tang, T Chen, L Zhu, Q V H Nguyen, H Yin, arXiv:2105.033002021arXiv preprint</p>
<p>All in one: Multi-task prompting for graph neural networks. X Sun, H Cheng, J Li, B Liu, J Guan, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023</p>
<p>Can we trust the evaluation on chatgpt. R Aiyappa, J An, H Kwak, Y.-Y Ahna, The Third Workshop on Trustworthy Natural Language Processing. 202347</p>
<p>Exploring the potential of large language models (llms) in learning on graphs. Z Chen, H Mao, H Li, W Jin, H Wen, X Wei, S Wang, D Yin, W Fan, H Liu, ACM SIGKDD Explorations Newsletter. 2522024</p>
<p>Can LLMs effectively leverage graph structural information: When and why. J Huang, X Zhang, Q Mei, J Ma, NeurIPS 2023 Workshop: New Frontiers in Graph Learning. 2023</p>
<p>Zi received his bachelor's degree from Computer Science department, the South University of Technology. He is currently a fall 2023 MPhil. student at The Hong Kong University of Science and Technology (Guangzhou). H Yuan, H Yu, S Gui, S Ji, Tmc, Www Kdd, Iclr Neurips, Chenyi, He has published several papers as the leading author in top conferences such as CVPR, ICLR, KDD, NeurIPS, and ICML. Jia Li received the PhD degree from the Chinese University of Hong Kong, in 2021. He is an assistant professor with the Hong Kong University of Science and Technology (Guangzhou) and an affiliated assistant professor with the Hong Kong University of Science and Technology. Changchun, China; Changchun, China; Peking, ChinaWWW and NeurIPS2022. 2019. 2022. 202245Jilin University ; and his M.sc degree from The School of Artificial Intelligence, Jilin University ; Tsinghua UniversityIEEE transactions on pattern analysis and machine intelligence. His research interests include machine learning, data mining, and deep graph learning. He has published several papers as the leading author in top conferences such as KDD</p>            </div>
        </div>

    </div>
</body>
</html>
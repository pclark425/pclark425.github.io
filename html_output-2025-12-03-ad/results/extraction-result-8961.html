<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8961 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8961</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8961</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-a2f36bbef278ffd53212b7e3c4cb7bb9c92e7f3d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a2f36bbef278ffd53212b7e3c4cb7bb9c92e7f3d" target="_blank">Self-Judge: Selective Instruction Following with Alignment Self-Evaluation</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Self-J, a novel self-training framework for developing judge models without needing human-annotated quality scores, is introduced, which leverages the model's inherent self-evaluation capability to extract information about response quality from labeled instruction-tuning data.</p>
                <p><strong>Paper Abstract:</strong> Pre-trained large language models (LLMs) can be tailored to adhere to human instructions through instruction tuning. However, due to shifts in the distribution of test-time data, they may not always execute instructions accurately, potentially generating factual errors or misaligned content when acting as chat assistants. To enhance the reliability of LLMs in following instructions, we propose the study of selective instruction following, whereby the system declines to execute instructions if the anticipated response quality is low. We train judge models that can predict numerical quality scores for model responses. To address data scarcity, we introduce Self-J, a novel self-training framework for developing judge models without needing human-annotated quality scores. Our method leverages the model's inherent self-evaluation capability to extract information about response quality from labeled instruction-tuning data. It incorporates a gold reference answer to facilitate self-evaluation and recalibrates by assessing the semantic similarity between the response sample and the gold reference. During the training phase, we implement self-distillation as a regularization technique to enhance the capability of reference-free estimation. To validate alignment evaluation on general instruction-following tasks, we collect large-scale high-quality instructions from Hugging Face for model training and evaluation. Extensive experiments on five open-source models show that our method correlates much more with GPT-4 than strong baselines, e.g., supervised models distilled from GPT-4 and GPT-3.5-turbo. Our analysis shows our model's strong generalization across domains. Additionally, our judge models serve as good reward models, e.g., boosting WizardLM-13B-V1.2 from 89.17 to 92.48 and from 12.03 to 15.90 in version v1 and v2 of AlpacaEval respectively using best-of-32 sampling with our judge models.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8961.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8961.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-J</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELF-J (Alignment Self-Evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-training framework that extracts numerical quality scores from an instruction-tuned LLM's own self-evaluations (with reference) recalibrated by semantic similarity, then trains a reference-free judge model via self-distillation to predict quality scores for selective instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Instruction-tuned LLMs (Vicuna-13b, WizardLM-13b, Llama-2-13b-chat, Llama-2-70b-chat, Ours-13b); judge model architecture: Llama-2-13b (13B) fine-tuned via LoRA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Judge models: Llama-2-13b base fine-tuned with LoRA (tuned modules: q,k,v,out), batch size 128, 2 epochs, LR=3e-4. Instruction-tuned models evaluated include Vicuna-13b-v1.5, WizardLM-13b-v1.2, Llama-2-13b-chat, Llama-2-70b-chat and Ours-13b (Llama-2-13b LoRA fine-tuned on 87k instructions).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-evaluation with reference + cosine recalibration + self-distillation (SELF-J)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Prompt the instruction-tuned model with (instruction x, model response y', reference y) to produce an integer self-evaluation score z (1-10). Compute cosine similarity between embeddings of y' and y as a second score. Combine the two scores via a weighted average z = alpha * z^(1) + (1-alpha) * z^(2) with alpha selected on a small dev set, discretize and uniformize resulting scores across 1-10. Use these generated labels to fine-tune a judge model; apply self-distillation by jointly optimizing a teacher objective p(z|x,y',y) and a student objective p(z|x,y') with a KL regularizer so the student learns reference-free estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General instruction-following alignment evaluation (collected Hugging Face instructions + AlpacaEval)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Judge models predict a numerical alignment score (1-10) for model-generated responses across diverse instruction tasks (common, coding, academic). Evaluation measures correlation of judge scores with GPT-4 evaluations (Pearson) and system-level ranking correlation (Kendall's tau); used for selective abstention and best-of-N selection on AlpacaEval.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reference-free Pearson correlation with GPT-4: SELF-J (avg over tested models) = 58.65% on 850-sample eval set (without reference) and 55.02% on combined 1655-sample set. Reference-based (with reference) average Pearson = 70.34% on 850-sample set and 65.42% on combined set. As a reward model for best-of-32 sampling with WizardLM-13b on AlpacaEval: V1 win-rate improved from 89.17% to 92.48%; V2 score improved from 12.03 to 15.90 (percentage win metric reported in Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baselines: VRO (sampling-variance) avg Pearson ~41.12% (850 samples); Auto-J-13b (GPT-4 distilled supervised judge) avg ~43.54%; UltraRM-13b (GPT-4 preference-trained reward model) avg ~52.98%. Ablation -self-distil (SELF-J trained without self-distillation) yields reference-free avg Pearson 54.56% (850 samples) and 48.99% (1655 samples), lower than full SELF-J.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering to elicit scalar self-evaluations from the instruction-tuned model (teacher uses reference), embedding-based cosine similarity recalibration, and training-time self-distillation (teacher with reference -> student without) implemented via joint NLL + KL losses.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: SELF-J yields substantially higher Pearson correlations with GPT-4 than training-free baselines (VRO, PPL) and competitive to/distinct from GPT-3.5-turbo; self-distillation ablation shows a drop (e.g., avg Pearson 58.65% -> 54.56% on 850 samples) demonstrating the benefit of the self-distillation step. As a reward model, using SELF-J in best-of-32 improved WizardLM's reported win-rate/score on AlpacaEval (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Self-evaluation accuracy depends on reference availability — self-eval without reference is poor and much worse than VRO in some cases. SELF-J requires reference answers during training (they used GPT-4 references for 30k judge-training datapoints, which is costly). The self-evaluation and cosine signals are noisy; small dev set (150 samples) used to tune alpha. Models sometimes do not follow the exact response format for ratings, requiring an extra extractor (GPT-3.5) to parse ratings. Auto-J performed poorly at high abstention rates; Judge (cosine) baseline is weak. The paper reports that PPL is ineffective and that performance gains are modest in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Directly compared against Cosine-only, Self-eval-only, Self-eval+cosine, GPT-3.5-turbo scoring, Auto-J-13b (GPT-4-distilled judge), UltraRM-13b (GPT-4 preference-trained reward model), and training-free baselines PPL and VRO. SELF-J outperforms these baselines on reference-free evaluation and is competitive on reference-based estimation. Ablations (Judge(cosine), Judge(self-eval), -self-distil) show SELF-J's components contribute to improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Removing self-distillation (-self-distil) reduces reference-free Pearson correlation: average 58.65% -> 54.56% (850-sample eval). Judge(cosine) and Judge(self-eval) (trained from simpler generated labels without self-distillation) perform worse than full SELF-J (reported in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Judge: Selective Instruction Following with Alignment Self-Evaluation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8961.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8961.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Selective refinement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Selective refinement (model-generated feedback + one-step refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative answer-improvement procedure where responses rated below a threshold by the judge model are passed back to the same model to (1) generate feedback f and (2) produce a refined response conditioned on original input, original response, and the feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Ours-13b (Llama-2-13b LoRA-tuned 13B) and WizardLM-13b-v1.2 (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned 13B models: Ours-13b is Llama-2-13b fine-tuned via LoRA on 87k Hugging Face instructions (ChatGPT-generated references); WizardLM-13b-v1.2 is an existing instruction-tuned 13B model.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-refinement (generate feedback -> refine)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Two-stage process triggered when judge rates answer below a threshold: (1) model generates feedback f from (x + y1' + z) -> f, where x is the instruction, y1' the initial response, and z the judge score; (2) model generates a refined response y2' conditioning on (x + y1' + f) -> y2'. This implements a single generate-then-reflect iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Instruction-following evaluation on collected eval set (common, coding, academic categories) with GPT-4 scoring</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Diverse instruction-following tasks sampled from the collected Hugging Face instruction pool and evaluated with GPT-4 using the reference-based prompt; responses before and after refinement are scored by GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Small but consistent improvements: Ours-13b overall average GPT-4 score improved from 6.17 (1st round) to 6.24 (2nd round). WizardLM-13b overall improved from 6.69 to 6.85. Category examples: WizardLM academic 7.91 -> 8.01; coding 5.67 -> 5.84.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>First-round (no refinement) scores: Ours-13b overall 6.17; WizardLM-13b overall 6.69; category-specific numbers in Table 3.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering: the same LLM generates feedback and then conditions generation of refined answers on that feedback; judge model threshold determines which responses are refined.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative GPT-4 evaluations show consistent small gains in average score after one refinement pass across models and categories (Table 3 and Figure 7). Coverage vs. average-score curves show the second-round responses lie above the first-round curve indicating net improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Improvements are relatively small in magnitude (e.g., +0.07 to +0.16 on a 1-10 scale). The method was evaluated only with a single refinement iteration; multi-step iterative improvement was not explored. Effectiveness depends on judge-model accuracy and quality of generated feedback; no detailed failure-case analysis provided beyond noting modest gains. Additional compute cost and latency are incurred.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared only to the baseline of no refinement (first-round responses). Not directly compared to other iterative methods (e.g., Reflexion, chain-of-thought self-critique) in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>No multi-step ablation; the reported comparison is first-round vs second-round only (Table 3 and Figure 7).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Judge: Selective Instruction Following with Alignment Self-Evaluation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8961.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8961.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-distillation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-distillation (teacher-with-reference -> student-reference-free)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training-time distillation technique where the judge model is trained simultaneously on a teacher objective that conditions on the reference answer and a student objective that is reference-free, with a KL loss to transfer teacher behavior to the student.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Judge model architecture: Llama-2-13b (13B) fine-tuned with LoRA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base Llama-2-13b judge model is fine-tuned with LoRA (q,k,v,out), batch size 128, 2 epochs, LR=3e-4. During training, each batch runs two forward passes: teacher (inputs x, y', y) and student (inputs x, y'), with KL loss; teacher gradients are not backpropagated for KL term.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-distillation for reference-free evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Jointly optimize negative log-likelihood for teacher p(z|x,y',y) and student p(z|x,y') plus a KL divergence term KL[p(student) || p(teacher)], with hyperparameters beta (weight for student NLL) and gamma (weight for KL); values used: beta=2, gamma=0.3. This allows the model to internalize reference-based judgments and perform better when references are unavailable at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Judge model training for alignment evaluation (predicting numerical scores 1-10)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Train a judge to predict alignment quality scores for model-generated responses (labelled via SELF-J's generated labels). Evaluation by Pearson correlation with GPT-4 and system-level Kendall's tau for ranking models (AlpacaEval).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Including self-distillation yields higher reference-free correlations: SELF-J (with self-distillation) reference-free avg Pearson 58.65% (850 samples) vs -self-distil 54.56% on same set; on combined 1655 samples SELF-J 55.02% vs -self-distil 48.99%. System-level Kendall's tau average (across 5 judge models) for SELF-J = 63.53% (AlpacaEval v1 ranking).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Without self-distillation (-self-distil) reference-free Pearson correlations drop (e.g., 58.65% -> 54.56% on 850-sample eval; 55.02% -> 48.99% on 1655-sample eval), indicating weaker reference-free estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Training-time teacher/student distillation using reference-based teacher signals (prompted LLM with reference) and a student that must perform without references; implemented via prompt templates and KL regularization; requires reference answers during training.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>2</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Ablation: removing the self-distillation component reduces reference-free Pearson correlations by multiple points (reported numbers above). The drop is consistent across evaluation splits, demonstrating that self-distillation materially improved performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires reference answers for teacher inputs during training (authors used GPT-4 to generate references for 30k judge-training points, which is costly). Teacher signals and cosine recalibration remain noisy; improvement depends on quality and coverage of references. No detailed failure-mode analysis beyond numeric ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Ablated against judge models trained without self-distillation and against supervised models distilled from GPT-4 (Auto-J) and GPT-4 preference-trained reward models (UltraRM); SELF-J with self-distillation outperformed these baselines on reference-free estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Removing self-distillation reduces reference-free Pearson correlation: example decreases from 58.65% -> 54.56% (850-sample) and 55.02% -> 48.99% (1655-sample), demonstrating contribution of self-distillation (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Judge: Selective Instruction Following with Alignment Self-Evaluation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8961.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8961.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-eval (model self-evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model self-evaluation (reference-based and reference-free prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using the instruction-tuned model itself to produce a numerical quality rating for its own responses when prompted with formats like Fig. 2; used both directly and as a component of SELF-J.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Instruction-tuned LLMs (the same models evaluated: Vicuna-13b, WizardLM-13b, Llama-2-chat variants, Ours-13b)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned models prompted to output a scalar rating for (x, y') or (x, y', y) according to the reference-based prompt in Fig. 2; sometimes augmented with cosine similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-evaluation (prompted scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Direct prompting of the same model that produced y' to output a 1-10 integer quality score; used both with and without reference answer. Scores optionally combined with cosine similarity of embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Quality estimation for instruction-following responses</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Prompt-based generation of numeric quality labels used either as final estimates (self-eval baseline) or as components in SELF-J training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reference-based self-eval performance varies by model: e.g., reference-based Pearson correlations per Table 2 (self-eval+cosine average ~59.98% on 850-sample set). Self-eval (reference-based) alone shows varying results across models (e.g., 44.66% to 55.13% per-model entries in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Self-eval without reference performs poorly (e.g., average ~11.46% on 850-sample set), often much worse than sampling-variance baselines (VRO).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineered scoring prompts (Fig. 2) that ask the model to judge its own output; optionally combined with cosine similarity of embeddings as a recalibration.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>When provided the reference, self-eval improves markedly compared to reference-free self-eval; combining self-eval with cosine (self-eval+cosine) further improves correlation with GPT-4 compared to either alone (reported in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Self-eval without reference is unreliable and may be worse than simple unsupervised variance measures; effectiveness tightly correlated with the underlying model capability — weaker instruction-tuned models produce poor self-eval scores. Formatting/response extraction issues occurred when models did not adhere to the required output format.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against cosine-only, self-eval+cosine, GPT-3.5-turbo scoring, and judge models; self-eval+cosine is a component of SELF-J and is outperformed by full SELF-J after training.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Reported comparisons show self-eval+cosine outperforms self-eval and cosine alone in reference-based settings; however, self-eval alone (reference-free) performs poorly compared to VRO.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Judge: Selective Instruction Following with Alignment Self-Evaluation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 2)</em></li>
                <li>Language Models are Few-Shot Learners <em>(Rating: 2)</em></li>
                <li>AlpacaEval <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8961",
    "paper_id": "paper-a2f36bbef278ffd53212b7e3c4cb7bb9c92e7f3d",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "SELF-J",
            "name_full": "SELF-J (Alignment Self-Evaluation)",
            "brief_description": "A self-training framework that extracts numerical quality scores from an instruction-tuned LLM's own self-evaluations (with reference) recalibrated by semantic similarity, then trains a reference-free judge model via self-distillation to predict quality scores for selective instruction following.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Instruction-tuned LLMs (Vicuna-13b, WizardLM-13b, Llama-2-13b-chat, Llama-2-70b-chat, Ours-13b); judge model architecture: Llama-2-13b (13B) fine-tuned via LoRA",
            "model_description": "Judge models: Llama-2-13b base fine-tuned with LoRA (tuned modules: q,k,v,out), batch size 128, 2 epochs, LR=3e-4. Instruction-tuned models evaluated include Vicuna-13b-v1.5, WizardLM-13b-v1.2, Llama-2-13b-chat, Llama-2-70b-chat and Ours-13b (Llama-2-13b LoRA fine-tuned on 87k instructions).",
            "reflection_method_name": "Self-evaluation with reference + cosine recalibration + self-distillation (SELF-J)",
            "reflection_method_description": "Prompt the instruction-tuned model with (instruction x, model response y', reference y) to produce an integer self-evaluation score z (1-10). Compute cosine similarity between embeddings of y' and y as a second score. Combine the two scores via a weighted average z = alpha * z^(1) + (1-alpha) * z^(2) with alpha selected on a small dev set, discretize and uniformize resulting scores across 1-10. Use these generated labels to fine-tune a judge model; apply self-distillation by jointly optimizing a teacher objective p(z|x,y',y) and a student objective p(z|x,y') with a KL regularizer so the student learns reference-free estimation.",
            "task_name": "General instruction-following alignment evaluation (collected Hugging Face instructions + AlpacaEval)",
            "task_description": "Judge models predict a numerical alignment score (1-10) for model-generated responses across diverse instruction tasks (common, coding, academic). Evaluation measures correlation of judge scores with GPT-4 evaluations (Pearson) and system-level ranking correlation (Kendall's tau); used for selective abstention and best-of-N selection on AlpacaEval.",
            "performance_with_reflection": "Reference-free Pearson correlation with GPT-4: SELF-J (avg over tested models) = 58.65% on 850-sample eval set (without reference) and 55.02% on combined 1655-sample set. Reference-based (with reference) average Pearson = 70.34% on 850-sample set and 65.42% on combined set. As a reward model for best-of-32 sampling with WizardLM-13b on AlpacaEval: V1 win-rate improved from 89.17% to 92.48%; V2 score improved from 12.03 to 15.90 (percentage win metric reported in Table 5).",
            "performance_without_reflection": "Baselines: VRO (sampling-variance) avg Pearson ~41.12% (850 samples); Auto-J-13b (GPT-4 distilled supervised judge) avg ~43.54%; UltraRM-13b (GPT-4 preference-trained reward model) avg ~52.98%. Ablation -self-distil (SELF-J trained without self-distillation) yields reference-free avg Pearson 54.56% (850 samples) and 48.99% (1655 samples), lower than full SELF-J.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering to elicit scalar self-evaluations from the instruction-tuned model (teacher uses reference), embedding-based cosine similarity recalibration, and training-time self-distillation (teacher with reference -&gt; student without) implemented via joint NLL + KL losses.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Quantitative: SELF-J yields substantially higher Pearson correlations with GPT-4 than training-free baselines (VRO, PPL) and competitive to/distinct from GPT-3.5-turbo; self-distillation ablation shows a drop (e.g., avg Pearson 58.65% -&gt; 54.56% on 850 samples) demonstrating the benefit of the self-distillation step. As a reward model, using SELF-J in best-of-32 improved WizardLM's reported win-rate/score on AlpacaEval (Table 5).",
            "limitations_or_failure_cases": "Self-evaluation accuracy depends on reference availability — self-eval without reference is poor and much worse than VRO in some cases. SELF-J requires reference answers during training (they used GPT-4 references for 30k judge-training datapoints, which is costly). The self-evaluation and cosine signals are noisy; small dev set (150 samples) used to tune alpha. Models sometimes do not follow the exact response format for ratings, requiring an extra extractor (GPT-3.5) to parse ratings. Auto-J performed poorly at high abstention rates; Judge (cosine) baseline is weak. The paper reports that PPL is ineffective and that performance gains are modest in some settings.",
            "comparison_to_other_methods": "Directly compared against Cosine-only, Self-eval-only, Self-eval+cosine, GPT-3.5-turbo scoring, Auto-J-13b (GPT-4-distilled judge), UltraRM-13b (GPT-4 preference-trained reward model), and training-free baselines PPL and VRO. SELF-J outperforms these baselines on reference-free evaluation and is competitive on reference-based estimation. Ablations (Judge(cosine), Judge(self-eval), -self-distil) show SELF-J's components contribute to improvements.",
            "ablation_study_results": "Removing self-distillation (-self-distil) reduces reference-free Pearson correlation: average 58.65% -&gt; 54.56% (850-sample eval). Judge(cosine) and Judge(self-eval) (trained from simpler generated labels without self-distillation) perform worse than full SELF-J (reported in Table 2).",
            "uuid": "e8961.0",
            "source_info": {
                "paper_title": "Self-Judge: Selective Instruction Following with Alignment Self-Evaluation",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Selective refinement",
            "name_full": "Selective refinement (model-generated feedback + one-step refinement)",
            "brief_description": "An iterative answer-improvement procedure where responses rated below a threshold by the judge model are passed back to the same model to (1) generate feedback f and (2) produce a refined response conditioned on original input, original response, and the feedback.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Ours-13b (Llama-2-13b LoRA-tuned 13B) and WizardLM-13b-v1.2 (13B)",
            "model_description": "Instruction-tuned 13B models: Ours-13b is Llama-2-13b fine-tuned via LoRA on 87k Hugging Face instructions (ChatGPT-generated references); WizardLM-13b-v1.2 is an existing instruction-tuned 13B model.",
            "reflection_method_name": "Self-refinement (generate feedback -&gt; refine)",
            "reflection_method_description": "Two-stage process triggered when judge rates answer below a threshold: (1) model generates feedback f from (x + y1' + z) -&gt; f, where x is the instruction, y1' the initial response, and z the judge score; (2) model generates a refined response y2' conditioning on (x + y1' + f) -&gt; y2'. This implements a single generate-then-reflect iteration.",
            "task_name": "Instruction-following evaluation on collected eval set (common, coding, academic categories) with GPT-4 scoring",
            "task_description": "Diverse instruction-following tasks sampled from the collected Hugging Face instruction pool and evaluated with GPT-4 using the reference-based prompt; responses before and after refinement are scored by GPT-4.",
            "performance_with_reflection": "Small but consistent improvements: Ours-13b overall average GPT-4 score improved from 6.17 (1st round) to 6.24 (2nd round). WizardLM-13b overall improved from 6.69 to 6.85. Category examples: WizardLM academic 7.91 -&gt; 8.01; coding 5.67 -&gt; 5.84.",
            "performance_without_reflection": "First-round (no refinement) scores: Ours-13b overall 6.17; WizardLM-13b overall 6.69; category-specific numbers in Table 3.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering: the same LLM generates feedback and then conditions generation of refined answers on that feedback; judge model threshold determines which responses are refined.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Quantitative GPT-4 evaluations show consistent small gains in average score after one refinement pass across models and categories (Table 3 and Figure 7). Coverage vs. average-score curves show the second-round responses lie above the first-round curve indicating net improvement.",
            "limitations_or_failure_cases": "Improvements are relatively small in magnitude (e.g., +0.07 to +0.16 on a 1-10 scale). The method was evaluated only with a single refinement iteration; multi-step iterative improvement was not explored. Effectiveness depends on judge-model accuracy and quality of generated feedback; no detailed failure-case analysis provided beyond noting modest gains. Additional compute cost and latency are incurred.",
            "comparison_to_other_methods": "Compared only to the baseline of no refinement (first-round responses). Not directly compared to other iterative methods (e.g., Reflexion, chain-of-thought self-critique) in this paper.",
            "ablation_study_results": "No multi-step ablation; the reported comparison is first-round vs second-round only (Table 3 and Figure 7).",
            "uuid": "e8961.1",
            "source_info": {
                "paper_title": "Self-Judge: Selective Instruction Following with Alignment Self-Evaluation",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Self-distillation",
            "name_full": "Self-distillation (teacher-with-reference -&gt; student-reference-free)",
            "brief_description": "A training-time distillation technique where the judge model is trained simultaneously on a teacher objective that conditions on the reference answer and a student objective that is reference-free, with a KL loss to transfer teacher behavior to the student.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Judge model architecture: Llama-2-13b (13B) fine-tuned with LoRA",
            "model_description": "Base Llama-2-13b judge model is fine-tuned with LoRA (q,k,v,out), batch size 128, 2 epochs, LR=3e-4. During training, each batch runs two forward passes: teacher (inputs x, y', y) and student (inputs x, y'), with KL loss; teacher gradients are not backpropagated for KL term.",
            "reflection_method_name": "Self-distillation for reference-free evaluation",
            "reflection_method_description": "Jointly optimize negative log-likelihood for teacher p(z|x,y',y) and student p(z|x,y') plus a KL divergence term KL[p(student) || p(teacher)], with hyperparameters beta (weight for student NLL) and gamma (weight for KL); values used: beta=2, gamma=0.3. This allows the model to internalize reference-based judgments and perform better when references are unavailable at test time.",
            "task_name": "Judge model training for alignment evaluation (predicting numerical scores 1-10)",
            "task_description": "Train a judge to predict alignment quality scores for model-generated responses (labelled via SELF-J's generated labels). Evaluation by Pearson correlation with GPT-4 and system-level Kendall's tau for ranking models (AlpacaEval).",
            "performance_with_reflection": "Including self-distillation yields higher reference-free correlations: SELF-J (with self-distillation) reference-free avg Pearson 58.65% (850 samples) vs -self-distil 54.56% on same set; on combined 1655 samples SELF-J 55.02% vs -self-distil 48.99%. System-level Kendall's tau average (across 5 judge models) for SELF-J = 63.53% (AlpacaEval v1 ranking).",
            "performance_without_reflection": "Without self-distillation (-self-distil) reference-free Pearson correlations drop (e.g., 58.65% -&gt; 54.56% on 850-sample eval; 55.02% -&gt; 48.99% on 1655-sample eval), indicating weaker reference-free estimation.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Training-time teacher/student distillation using reference-based teacher signals (prompted LLM with reference) and a student that must perform without references; implemented via prompt templates and KL regularization; requires reference answers during training.",
            "number_of_iterations": 2,
            "evidence_for_improvement": "Ablation: removing the self-distillation component reduces reference-free Pearson correlations by multiple points (reported numbers above). The drop is consistent across evaluation splits, demonstrating that self-distillation materially improved performance.",
            "limitations_or_failure_cases": "Requires reference answers for teacher inputs during training (authors used GPT-4 to generate references for 30k judge-training points, which is costly). Teacher signals and cosine recalibration remain noisy; improvement depends on quality and coverage of references. No detailed failure-mode analysis beyond numeric ablation.",
            "comparison_to_other_methods": "Ablated against judge models trained without self-distillation and against supervised models distilled from GPT-4 (Auto-J) and GPT-4 preference-trained reward models (UltraRM); SELF-J with self-distillation outperformed these baselines on reference-free estimation.",
            "ablation_study_results": "Removing self-distillation reduces reference-free Pearson correlation: example decreases from 58.65% -&gt; 54.56% (850-sample) and 55.02% -&gt; 48.99% (1655-sample), demonstrating contribution of self-distillation (Table 2).",
            "uuid": "e8961.2",
            "source_info": {
                "paper_title": "Self-Judge: Selective Instruction Following with Alignment Self-Evaluation",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Self-eval (model self-evaluation)",
            "name_full": "Model self-evaluation (reference-based and reference-free prompting)",
            "brief_description": "Using the instruction-tuned model itself to produce a numerical quality rating for its own responses when prompted with formats like Fig. 2; used both directly and as a component of SELF-J.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Instruction-tuned LLMs (the same models evaluated: Vicuna-13b, WizardLM-13b, Llama-2-chat variants, Ours-13b)",
            "model_description": "Instruction-tuned models prompted to output a scalar rating for (x, y') or (x, y', y) according to the reference-based prompt in Fig. 2; sometimes augmented with cosine similarity.",
            "reflection_method_name": "Self-evaluation (prompted scoring)",
            "reflection_method_description": "Direct prompting of the same model that produced y' to output a 1-10 integer quality score; used both with and without reference answer. Scores optionally combined with cosine similarity of embeddings.",
            "task_name": "Quality estimation for instruction-following responses",
            "task_description": "Prompt-based generation of numeric quality labels used either as final estimates (self-eval baseline) or as components in SELF-J training.",
            "performance_with_reflection": "Reference-based self-eval performance varies by model: e.g., reference-based Pearson correlations per Table 2 (self-eval+cosine average ~59.98% on 850-sample set). Self-eval (reference-based) alone shows varying results across models (e.g., 44.66% to 55.13% per-model entries in Table 2).",
            "performance_without_reflection": "Self-eval without reference performs poorly (e.g., average ~11.46% on 850-sample set), often much worse than sampling-variance baselines (VRO).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-engineered scoring prompts (Fig. 2) that ask the model to judge its own output; optionally combined with cosine similarity of embeddings as a recalibration.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "When provided the reference, self-eval improves markedly compared to reference-free self-eval; combining self-eval with cosine (self-eval+cosine) further improves correlation with GPT-4 compared to either alone (reported in Table 2).",
            "limitations_or_failure_cases": "Self-eval without reference is unreliable and may be worse than simple unsupervised variance measures; effectiveness tightly correlated with the underlying model capability — weaker instruction-tuned models produce poor self-eval scores. Formatting/response extraction issues occurred when models did not adhere to the required output format.",
            "comparison_to_other_methods": "Compared against cosine-only, self-eval+cosine, GPT-3.5-turbo scoring, and judge models; self-eval+cosine is a component of SELF-J and is outperformed by full SELF-J after training.",
            "ablation_study_results": "Reported comparisons show self-eval+cosine outperforms self-eval and cosine alone in reference-based settings; however, self-eval alone (reference-free) performs poorly compared to VRO.",
            "uuid": "e8961.3",
            "source_info": {
                "paper_title": "Self-Judge: Selective Instruction Following with Alignment Self-Evaluation",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 2
        },
        {
            "paper_title": "Language Models are Few-Shot Learners",
            "rating": 2
        },
        {
            "paper_title": "AlpacaEval",
            "rating": 2
        }
    ],
    "cost": 0.022158,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Short Paper</h1>
<h2>Self-Judge: Selective Instruction Following with Alignment Self-Evaluation</h2>
<p>Hai Ye<br>Department of Computer Science<br>National University of Singapore<br>yehai@comp.nus.edu.sg</p>
<p>Hwee Tou Ng<br>Department of Computer Science<br>National University of Singapore<br>nght@comp.nus.edu.sg</p>
<p>Pre-trained large language models (LLMs) can be tailored to adhere to human instructions through instruction tuning. However, due to shifts in the distribution of test-time data, they may not always execute instructions accurately, potentially generating factual errors or misaligned content when acting as chat assistants. To enhance the reliability of LLMs in following instructions, we propose the study of selective instruction following, whereby the system declines to execute instructions if the anticipated response quality is low. We train judge models that can predict numerical quality scores for model responses. To address data scarcity, we introduce SELF-J, a novel self-training framework for developing judge models without needing humanannotated quality scores. Our method leverages the model's inherent self-evaluation capability to extract information about response quality from labeled instruction-tuning data. It incorporates a gold reference answer to facilitate self-evaluation and recalibrates by assessing the semantic similarity between the response sample and the gold reference. During the training phase, we implement self-distillation as a regularization technique to enhance the capability of referencefree estimation. To validate alignment evaluation on general instruction-following tasks, we collect large-scale high-quality instructions from Hugging Face for model training and evaluation. Extensive experiments on five open-source models show that our method correlates much more with GPT-4 than strong baselines, e.g., supervised models distilled from GPT-4 and GPT-3.5turbo. Our analysis shows our model's strong generalization across domains. Additionally, our judge models serve as good reward models, e.g., boosting WizardLM-13B-V1.2 from 89.17 to 92.48 and from 12.03 to 15.90 in version v1 and v2 of AlpacaEval respectively using best-of32 sampling with our judge models. Ranking 95 models from AlpacaEval, our judges show a high Kendall's $\tau$ correlation coefficient (0.63) with GPT-4. Our work underscores the potential of alignment self-evaluation in large language models ${ }^{1}$.</p>
<h2>1. Introduction</h2>
<p>By building generative transformers with larger number of model parameters and training on larger pre-training corpora, we can effectively create powerful large language models (LLMs) (Radford et al. 2019; Brown et al. 2020). LLMs have demonstrated strong generalization capabilities, enabling them to generalize to any task through in-context learning (Brown et al. 2020). To make large language models more user-friendly, it is</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>essential to further align them with human preferences, ensuring they generate content that is beneficial, safe, and follows user intentions (Ouyang et al. 2022). Instruction tuning is an effective technique to align LLMs to human preferences. The models are fine-tuned to follow instructions described in natural language. Aiming to have a good generalization ability and to solve tasks across domains, instruction tuning usually involves diverse tasks. Instruction-following models have evolved into highly effective chat assistants for daily tasks, fulfilling various functions across different fields, e.g., content creation and customer support (OpenAI 2022).</p>
<p>Due to shifts in test-time distribution, models refined through instruction tuning may still produce outputs that are not aligned with human preferences, such as hallucinated content, unhelpful material, and irrelevant responses. To develop LLMs that adhere to human instructions, we propose studying selective instruction following where the system can decline to execute an instruction when it finds the response to be of low quality. We achieve this goal by exploring alignment evaluation that quantifies how well a model's output adheres to human preference. Alignment evaluation aims to measure the quality of model outputs on aspects such as helpfulness, correctness, relevance, etc (Zheng et al. 2023). This process is challenging since it involves diverse tasks and the model's outputs are expressed in natural language. The advanced capabilities of leading models like GPT-4 Turbo have increasingly been used to assess the performance of less powerful models. Multiple studies have shown a strong correlation between evaluations by these models and those conducted by humans (Zheng et al. 2023; Dubois et al. 2023; Li et al. 2023c). Crowdsourced human evaluation also plays a significant role. As seen through platforms like Chatbot Arena (Zheng et al. 2023), users provide preference feedback for two compared models, which is then used to calculate Elo ratings for gauging model performance. However, collecting human feedback is much slower and more costly.</p>
<p>In this work, we train judge models that can predict numerical quality scores for model outputs as a measure of alignment (see Fig. 1). Independent of input from other LLMs or human annotations, we propose a self-training-based method named SELF-J by utilizing the model's self-evaluation capabilities. It aims to extract quality scores from labeled instruction-tuning data, and subsequently train the judge model using the generated scores. Alignment evaluation does not have a definitive metric (e.g., semantic similarity) that can automatically calculate quality scores by comparing the model response with a gold standard refer-
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1
Selective instruction following with alignment evaluation. We train a judge model to rate an LLM's response with a numerical score.
ence (Zheng et al. 2023). We rely on the instructiontuned model itself to infer the quality scores for the model's own generated responses. To derive the score more accurately, we introduce the reference answer from the labeled instruction-tuning set to facilitate the model's self-evaluation. The reference answer offers a valuable supervision signal that facilitates self-evaluation, enabling the model to simply compare the sampled answer with the reference. To further reduce the noise in self-evaluation ratings, these ratings are recalibrated based on the semantic similarity between the model-generated samples and the gold-standard reference. This integration of semantic understanding and similarity metrics enables a more precise inference of quality scores. During the training phase, we introduce a self-distillation</p>
<p>technique to regularize the training of judge models. Here, a teacher model, enhanced with additional reference answers, directs the training of the student model. It can enhance the estimation of reference-free quality during tests where reference answers are unavailable.</p>
<p>We evaluate SELF-J on open-source models, e.g., Llama-2-Chat (Touvron et al. 2023). To validate alignment evaluation on general instruction-following tasks, such as coding, writing, etc, we collect a large number of high-quality instructions that cover practical questions from Hugging Face ${ }^{2}$, which is used for instruction tuning, judge modeling, and evaluation. From the collected instructions, we randomly sample 30k instructions for judge model tuning and another 1 k instructions for alignment evaluation. We further expand the test set by including AlpacaEval (Li et al. 2023c), a widely recognized benchmark for instruction-following tasks. AlpacaEval is considered a cross-domain evaluation set, making it suitable for testing the adaptability of our models across different contexts. In our evaluation, we report the correlation between measures with GPT-4's evaluation. We further evaluate the generalization ability of trained judge models, by first probing domain transfer from a source model to tested target models. On AlpacaEval, we augment the model with best-of- $N$ sampling by using our judge model as the reward model.</p>
<p>Our contributions in this paper can be summarized as follows:</p>
<ol>
<li>We introduce SELF-J, a novel self-training-based framework for judge model learning without human annotated scores. It is independent of GPT-4 without distilling the estimation scores.</li>
<li>We conduct extensive experiments with five open-source models for evaluation, which are Vicuna-13b, WizardLM-13b, Llama-2-chat-13b, Llama-2-chat-70b, and our instruction-tuned model using 87 k of our collected instructions. The experimental results validate the effectiveness of our approach.</li>
<li>SELF-J surpasses GPT-3.5-turbo and GPT-4 distilled models on reference-free evaluation, and matches GPT-3.5-turbo on reference-based estimation.</li>
<li>Serving as a reward model, SELF-J empowers WizardLM-13B-V1.2 with best-of-32 sampling on AlpacaEval, by improving the performance from 89.17 to 92.48 and from 12.03 to 15.90 on v1 and v2 of AlpacaEval respectively. It even outperforms GPT-4-0613 on v2 evaluation.</li>
<li>The tuned judge models achieve a high system-level Kendall's $\tau$ correlation coefficient ( 0.63 ) with GPT-4 for ranking 95 models submitted to AlpacaEval.</li>
<li>A collection of high-quality instructions on a large scale has been compiled for analysis. By fine-tuning Llama-2-13b with randomly chosen 87 k instructions and GPT-3.5 Turbo responses, we can match Llama-2-13b-Chat's performance on AlpacaEval.</li>
</ol>
<h1>2. Related Work</h1>
<h3>2.1 Instruction Tuning</h3>
<p>Aligning large language models to human preference is important since it stops the generation of harmful and useless content. Reinforcement learning from human feedback (RLHF) has become a standard approach to align LLMs. This involves initial instruction fine-tuning followed by reinforcement learning, as detailed by (Ouyang</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>et al. 2022). The focus has recently shifted towards self-alignment strategies, which aim to align LLMs more efficiently and cost-effectively. Key areas of exploration include instruction generation and reward modeling. Wang et al. (2023c) demonstrate the potential of generating instructions and responses using in-context learning with just 175 human-labeled examples. Following this, research by (Sun et al. 2023b) has shown that model alignment can be achieved with as few as five labeled examples. Another study from (Li et al. 2023b) introduces the concept of back-translating responses into instructions as a novel alignment technique. Furthermore, Zhou et al. (2023) highlight the significance of instruction diversity over quantity for effective alignment. For reward modeling, recent works have leveraged large language models to provide feedback. A study by (Sun et al. 2023a) utilizes an LLM to offer preference feedback on model responses based on a set of principles. Bai et al. (2022) focus on generating non-harmful prompts and employ LLMs to provide feedback, aiming to identify less harmful responses.</p>
<h1>2.2 Alignment Evaluation</h1>
<p>There has been a growing interest in the automatic evaluation of chat assistant performance. This trend is driven by the absence of reliable evaluation metrics and the prohibitive costs of human evaluation. Recent studies have leveraged state-of-the-art models, such as GPT-4, to assess the capabilities of less sophisticated models (Zheng et al. 2023; Li et al. 2023c). These investigations have uncovered a strong correlation between evaluations conducted by GPT-4 and those performed by humans (Zheng et al. 2023; Li et al. 2023c; Dubois et al. 2023). Nevertheless, the proprietary nature and the substantial expense linked to the use of GPT-4 have spurred initiatives aimed at creating open-source judge models. These models are designed to offer consistent and cost-effective evaluations (Li et al. 2023a; Wang et al. 2023a,b). Li et al. (2023a) develop an open-source model, auto-j, which is distilled from GPT-4. This model is designed to assess alignments by furnishing both a critique and a score. Similarly, Wang et al. (2023b) introduce a model also distilled from GPT-4, designed to express a preference between pairs of responses to a given question, which is particularly useful for optimizing hyper-parameters during instruction tuning. Cui et al. (2023) focus on constructing an open-source reward model that leverages preference feedback data from GPT-4 for reward modeling. This model is intended to support the community in developing more effective alignment algorithms. Another contribution by (Wang et al. 2023a) involves the creation of a model capable of generating critiques for model responses, which is achieved by aggregating critique data from the Internet. To the best of our knowledge, our method represents the first attempt to train a judge model that does not depend on demonstration scores from GPT-4 for its training.</p>
<h3>2.3 AI Feedback</h3>
<p>Reinforcement learning from human feedback (RLHF) has traditionally been a resourceintensive process, which needs significant human effort to collect feedback. Recent work has introduced a new approach, reinforcement learning from AI feedback (RLAIF), which leverages large language models to provide feedback, thereby replacing the need for human input. Bai et al. (2022) utilize LLMs to offer preference feedback on the outcomes of harmful prompts. This feedback is then used to train a reward model aimed at aligning with harmless content. Yuan et al. (2024) prompt LLMs to assign numerical scores to model responses. These scored responses are subsequently paired and used</p>
<p>in iterative training of the model, employing direct preference optimization (Rafailov et al. 2023). Another study investigates self-alignment by converting responses back into instructions, with LLMs filtering out instructions of lower quality (Li et al. 2023b). Lee et al. (2023) delve into the utilization of AI feedback specifically for the task of summarization. Furthermore, the embedding of principles within reward modeling through feedback from large language models has been proposed as a novel strategy for enhancing alignment (Sun et al. 2023a). Our research investigates self-alignment evaluation, which is closely linked to the concept of AI feedback.</p>
<h1>2.4 Open-Source Instructions</h1>
<p>The pursuit of open-source advancements within the community extends beyond just models to include data as well. Wang et al. (2023c) and Taori et al. (2023) have leveraged in-context learning to generate a large set of instructions. Similarly, Conover et al. (2023) have opted for a human-centric approach, gathering instructions and responses to compile an open-source dataset featuring 15k instructions. Longpre et al. (2023) have embarked on a mission to amass a large collection of instructions with a focus on traditional NLP tasks, such as natural language understanding and question answering. Xu et al. (2023) have utilized ChatGPT to generate challenging instructions, discovering that fine-tuning LLMs with difficult prompts can notably enhance model performance. Similarly, Ding et al. (2023) have employed ChatGPT for generating a vast array of dialogue data, further illustrating the versatility of LLMs in simulating complex conversational scenarios. The instruction set of ShareGPT has been widely used for instructiontuning (Chiang et al. 2023). In our work, we compile a comprehensive collection of practical and high-quality instructions from Hugging Face, contributing to the pool of resources available for enhancing the effectiveness and efficiency of instruction-tuning LLMs.</p>
<h3>2.5 Uncertainty Estimation for Language Generation</h3>
<p>There is a growing interest in measuring uncertainty in conditional language generation. Ren et al. (2023) introduce an approach that involves training a lightweight model, incorporating both encoder and decoder embeddings derived from transformers. This model is tailored for selective instruction following as well as identification of out-of-distribution (OOD) samples. Crucially, their research highlights the inadequacy of perplexity as a reliable measure for gauging a model's confidence level in generated text, suggesting the need for alternative metrics. Building on this quest for better uncertainty metrics, Kuhn, Gal, and Farquhar (2023) have proposed the concept of semantic entropy. This method employs a natural language inference (NLI) model to estimate the semantic discrepancies among several model responses to an input, providing a more nuanced understanding of model uncertainty. Similarly, the notion of sampling variance has been explored by (Huang et al. 2023), which quantifies the semantic variability across multiple samples generated by a model. This technique offers another layer of insights into a model's performance by assessing the consistency of its outputs. In the field of machine translation (MT), efforts by (Guerreiro, Voita, and Martins 2023) and (Rei et al. 2020) have delved into the training of classifiers dedicated to the estimation of translation quality. The work of (Chollampatt and Ng 2018) and (Qorib and Ng 2023) concerns quality estimation of grammatical error correction.</p>
<p>Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. You will be given a reference answer and the assistant's answer. Begin your evaluation by comparing the assistant's answer with the reference answer. Be as objective as possible. After providing your explanation, in the last new line, you must rate the response on a scale of 1 to 10 by strictly following this format: " ${$ "rating": your rating)". For example, " ${$ "rating": 5}".</p>
<div class="codehilite"><pre><span></span><code><span class="k">[Question]</span>
<span class="na">{question}</span>
<span class="k">[The Start of Reference Answer]</span>
<span class="na">{reference answer}</span>
<span class="k">[The End of Reference Answer]</span>
<span class="k">[The Start of Assistant&#39;s Answer]</span>
<span class="na">{answer}</span>
<span class="k">[The End of Assistant&#39;s Answer]</span>
</code></pre></div>

<p>Figure 2
The prompt used for alignment evaluation. It is used by GPT-4 and other open-source models studied in this work. We use the version of reference-based evaluation, where a reference answer is required for evaluation. The prompt is adopted from Zheng et al. (2023) and Dettmers et al. (2023). As indicated by Zheng et al. (2023), reference answers can improve the performance of GPT-4's evaluations on reasoning tasks, such as coding problems and mathematical questions.</p>
<h1>3. Preliminary</h1>
<h3>3.1 Instruction Fine-Tuning</h3>
<p>To align a large language model with instruction tuning, we need a labeled instruction set that involves diverse tasks, such as coding, logical reasoning, knowledge verification, strategic planning, creative ideation, etc. We denote the labeled set as $\mathcal{D}_{\text {train }}=$ ${x, y}$, and the LLM is fine-tuned to generate the response $y$ conditioned on the input instruction $x$. After fine-tuning, a reinforcement learning stage can be applied to further align the model to human preference (Bai et al. 2022). At inference time, the instructiontuned LLM $\mathcal{F}$ samples a response $y^{\prime}$ conditioned on the instruction $x$, i.e., $y^{\prime} \sim \mathcal{F}\left(y^{\prime} \mid \boldsymbol{x}\right)$.</p>
<h3>3.2 Alignment Evaluation</h3>
<p>Evaluation Criteria. For instruction-following LLMs, alignment evaluation aims to measure how well the model outputs align with human preference. Preference for desired outputs may vary among different users. In this work, we follow previous work that focuses on evaluating the preferences shared by most users which are helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response (Zheng et al. 2023). As demonstrated by Zheng et al. (2023), when prompting GPT-4 as the evaluator</p>
<p>using these aspects of preference, it can have a high correlation with human evaluations. Below is a breakdown of each aspect:</p>
<ul>
<li>Helpfulness measures whether the model response effectively addresses the instruction. A helpful response should provide valuable solutions to solve the problems of the user.</li>
<li>Relevance indicates how closely the model response aligns with the user question or the topic of the question. A highly relevant response should directly address the user's problem without discussion about unrelated areas.</li>
<li>Accuracy evaluates the correctness of the information provided in the response. An accurate response should be free from errors and based on knowledge that can be traced to some sources.</li>
<li>Depth focuses on the thoroughness and comprehensiveness of the response. An answer with depth should go beyond a superficial response, offering a detailed understanding or insight.</li>
<li>Creativity is about the novelty of the response. Creative responses should introduce new ideas, solutions, or perspectives that are not conventional.</li>
<li>Level of detail assesses the amount of information contained in the response. A detailed response should contain specific examples, data, or explanations to support the main arguments.</li>
</ul>
<p>By focusing on the six aspects during evaluation, a comprehensive score will be induced as a measure of the level of alignment. As shown in the prompt in Fig. 2, the alignment score is an integer from 1 to 10 .
Types of Alignment Evaluation. By evaluating a response $\boldsymbol{y}^{\prime} \sim \mathcal{F}\left(\boldsymbol{y}^{\prime} \mid \boldsymbol{x}\right)$, an estimator $\mathcal{J}<em _theta="\theta">{\theta}$ parameterized by $\theta$ produces a quality score $z$. There are two types of alignment evaluation: reference-free and reference-based estimation.
$\triangleright$ Reference-free: It considers the scenario that the reference answer is usually not available at test time. The estimator takes in $\boldsymbol{x}$ and $\boldsymbol{y}^{\prime}$ for evaluation, i.e., $z \sim \mathcal{J}</em>\right)$.
$\triangleright$ Reference-based: It is an oracle setting where the reference answer $\boldsymbol{y}$ is available, i.e., $z \sim \mathcal{J}_{\theta}\left(z \mid \boldsymbol{x}, \boldsymbol{y}^{\prime}, \boldsymbol{y}\right)$. As pointed out by Zheng et al. (2023), LLMs are limited in their reasoning capability. They fall short in grading reasoning tasks since they are not aware of the correct answer to the question during evaluation. Providing a reference answer can enhance the ability of the LLMs to assess such questions.}\left(z \mid \boldsymbol{x}, \boldsymbol{y}^{\prime</p>
<p>In this work, we are interested in reference-free estimation since reference answers are not available at test time. We also aim to benefit reference-free estimation from reference-based evaluation through self-distillation.</p>
<h1>3.3 Selective Instruction Following</h1>
<p>For selective instruction following, the system can refuse to execute the instruction (or display the answer) when the generated answer is of low quality. The alignment score represents the quality of a model's generated response. We follow the setting of selective prediction (Kamath, Jia, and Liang 2020) to formulate selective instruction following. At test time, given an instruction $\boldsymbol{x}$ without reference answer, the quality score $z$ is generated conditioned on the instruction $\boldsymbol{x}$ and model response $\boldsymbol{y}^{\prime}$, i.e., $z \sim \mathcal{J}_{\theta}\left(z \mid \boldsymbol{x}, \boldsymbol{y}^{\prime}\right)$. With a threshold $\eta \in \mathbb{R}$, if $z \geq \eta$, the response $\boldsymbol{y}^{\prime}$ is accepted; otherwise, the response is discarded.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3
Illustration of SELF-J (see also the pseudocode of Algorithm 1). (a) We first conduct instruction tuning on a pre-trained LLM (or directly using an existing instruction-tuned model, e.g., Vicuna). (b) We generate quality scores with model self-evaluation recalibrated by a semantic similarity score. (c) With the generated quality scores, we train a judge model through self-distillation.</p>
<h1>4. Uncertainty Measurement</h1>
<p>Uncertainty represents the confidence level of a model's output. It has been widely studied and used for selective prediction (or generation) and out-of-distribution (OOD) detection (Ren et al. 2023).
PPL. For language generation, perplexity is a simple method to measure the uncertainty of a model, which is monotonically related to the average of negative log-likelihood over output tokens, i.e., $-\frac{1}{\left|\boldsymbol{y}^{\prime}\right|} \sum_{t=1}^{\left|\boldsymbol{y}^{\prime}\right|} \log p\left(y_{t}^{\prime} \mid y_{&lt;t}^{\prime}, \boldsymbol{x}\right)$. However, recent work points out that it is not effective for selective language generation tasks (Ren et al. 2023).
Sampling Variance. A better way to estimate the uncertainty of language generation is the variance ratio for the original output (VRO) with extra sampled responses (Huang et al. 2023). Except for the original response $\boldsymbol{y}^{\prime}$, we further sample extra $K$ responses $\left{\boldsymbol{y}<em k="1">{k}^{\prime}\right}</em>$. The variance over the sampled responses is calculated as follows:}^{K</p>
<p>$$
-\frac{1}{K} \sum_{k=1}^{K} S\left(\boldsymbol{y}^{\prime}, \boldsymbol{y}_{k}^{\prime}\right)
$$</p>
<p>where $S(\cdot, \cdot)$ measures the semantic similarity between the original prediction and another sampled response. More similar responses will have a smaller variance, and the model is expected to be more certain about its generation. We calculate cosine similarity over the response embeddings.</p>
<h2>5. Method</h2>
<p>Here, we introduce our method SELF-J for judge model training, which aims to utilize self-training to train judge models without human annotations. Fig. 3 is an overview of our method. Briefly speaking, SELF-J begins by instruction-tuning an LLM (or using an existing instruction-tuned LLM). It then self-generates quality scores for model responses (§5.1), using these to fine-tune a judge model through self-distillation (§5.2).</p>
<p>Tasks in instruction tuning, unlike more established natural language generation (NLG) tasks such as machine translation, summarization, or grammatical error correction, lack a standard evaluation metric, which complicates automatic quality scoring using metrics like BLEU (Papineni et al. 2002), Rouge (Lin 2004), or $M^{2}$ (Dahlmeier and Ng 2012). We therefore present a more effective way to assess the quality scores. On the labeled instruction set $\mathcal{D}<em _text="\text" _train="{train">{\text {train }}={\boldsymbol{x}, \boldsymbol{y}}$, we first sample model responses $\boldsymbol{y}^{\prime}$ from an instruction-tuned LLM $\mathcal{F}$, i.e., $\boldsymbol{y}^{\prime} \sim \mathcal{F}\left(\boldsymbol{y}^{\prime} \mid \boldsymbol{x}\right)$. After sampling, we obtain the training set $\mathcal{D}</em>\right}$ with model responses.}}^{\prime}=\left{\boldsymbol{x}, \boldsymbol{y}, \boldsymbol{y}^{\prime</p>
<p>Since there are no labeled quality scores, we aim to train the judge model with selftraining, where we first generate quality scores and then train the judge model with the generated scores. Our goal is to tune an LLM to generate an accurate quality score $z \sim \mathcal{J}<em _theta="\theta">{\theta}\left(\cdot \mid \boldsymbol{x}, \boldsymbol{y}^{\prime}\right)$ given $\left(\boldsymbol{x}, \boldsymbol{y}^{\prime}\right) \sim p\left(\boldsymbol{x}, \boldsymbol{y}^{\prime}\right)$, where the judge model $\mathcal{J}</em>$ with cross entropy minimization:}$ is parameterized by $\theta$. Suppose we have the true distribution $q\left(z \mid \boldsymbol{x}, \boldsymbol{y}^{\prime}\right)$ that generates the quality score, we can tune the judge model $\mathcal{J}_{\theta</p>
<p>$$
\mathcal{L}<em z="z">{\theta}\left(\boldsymbol{x}, \boldsymbol{y}^{\prime}\right)=-\sum</em>\right)
$$} q\left(z \mid \boldsymbol{x}, \boldsymbol{y}^{\prime}\right) \log \mathcal{J}_{\theta}\left(z \mid \boldsymbol{x}, \boldsymbol{y}^{\prime</p>
<p>which aims to minimize the difference between the two distributions $q\left(z \mid \boldsymbol{x}, \boldsymbol{y}^{\prime}\right)$ and $\mathcal{J}_{\theta}\left(z \mid \boldsymbol{x}, \boldsymbol{y}^{\prime}\right)$.</p>
<p>We can approximate the true distribution with the instruction-tuned model $\mathcal{F}$ where we can prompt the model $\mathcal{F}$ to generate the quality score $z \sim \mathcal{F}\left(\cdot \mid \boldsymbol{x}, \boldsymbol{y}^{\prime}\right)$ for the data $\left(\boldsymbol{x}, \boldsymbol{y}^{\prime}\right)$. However, the instruction-tuned model may not be good at estimating the quality of self-generated responses, especially for questions that need strong reasoning abilities (Zheng et al. 2023). As pointed out by Zheng et al. (2023) and demonstrated by our experiments, providing a reference answer to the model can enhance the model's performance on response quality estimation. To better approximate the true distribution, we further introduce the reference answers $\boldsymbol{y}_{i}$, and $q\left(z \mid \boldsymbol{x}, \boldsymbol{y}^{\prime}\right)$ can be further derived as follows:</p>
<p>$$
\begin{aligned}
q\left(z \mid \boldsymbol{x}, \boldsymbol{y}^{\prime}\right) &amp; =\sum_{\boldsymbol{y}<em i="i">{i}} q\left(z, \boldsymbol{y}</em>\right) \
&amp; =\sum_{\boldsymbol{y}} \mid \boldsymbol{x}, \boldsymbol{y}^{\prime<em i="i">{i}} q\left(z \mid \boldsymbol{x}, \boldsymbol{y}</em>}, \boldsymbol{y}^{\prime}\right) q\left(\boldsymbol{y<em _boldsymbol_y="\boldsymbol{y">{i} \mid \boldsymbol{x}, \boldsymbol{y}^{\prime}\right) \
&amp; =\underbrace{\sum</em><em i="i">{i}} q\left(z \mid \boldsymbol{x}, \boldsymbol{y}</em>}, \boldsymbol{y}^{\prime}\right)<em i="i">{w</em>}} \underbrace{q\left(\boldsymbol{y<em w__i="w_{i">{i}, \boldsymbol{y}^{\prime} \mid \boldsymbol{x}\right)}</em>
\end{aligned}
$$}</p>
<p>In the given equation, the distribution of $q\left(z \mid \boldsymbol{x}, \boldsymbol{y}^{\prime}\right)$ represents the weighted sum of $q\left(z \mid \boldsymbol{x}, \boldsymbol{y}<em i="i">{i}, \boldsymbol{y}^{\prime}\right)$ across different reference answers. Here, the weight $w</em>}$ is $\frac{q\left(\boldsymbol{y<em i="i">{i}, \boldsymbol{y}^{\prime} \mid \boldsymbol{x}\right)}{q\left(\boldsymbol{y}^{\prime} \mid \boldsymbol{x}\right)}$. Since $q\left(\boldsymbol{y}^{\prime} \mid \boldsymbol{x}\right)$ is constant for varying references, it can be omitted, allowing us to use $q\left(\boldsymbol{y}</em>}, \boldsymbol{y}^{\prime} \mid \boldsymbol{x}\right)$ directly as the weight. Assuming $z_{i} \sim q\left(\cdot \mid \boldsymbol{x}, \boldsymbol{y<em i="i">{i}, \boldsymbol{y}^{\prime}\right)$, the aggregated score $z$ is computed as $\sum</em>$ are of comparable quality and exert a uniform influence on quality assessment. This assumption enables us to reduce the number of reference answers required when applying Eq. 3 to compute quality scores.} w_{i} \cdot z_{i}$. We hypothesize that all reference answers $\boldsymbol{y}_{i</p>
<h1>5.1 Quality Score Generation</h1>
<p>In this section, we generate the quality scores for training based on Eq. 3, utilizing the training dataset $\mathcal{D}_{\text {train }}=\left{\boldsymbol{x}, \boldsymbol{y}, \boldsymbol{y}^{\prime}\right}$. According to Eq. 3, the score $z$ is a recalibrated score of $q\left(z \mid \boldsymbol{x}, \boldsymbol{y}, \boldsymbol{y}^{\prime}\right)$ by using $q\left(\boldsymbol{y}, \boldsymbol{y}^{\prime} \mid \boldsymbol{x}\right)$. This process ensures that both factors are considered when determining the quality of the scores.</p>
<p>We employ the instruction-tuned model $\mathcal{F}$ to initialize $q\left(z \mid \boldsymbol{x}, \boldsymbol{y}, \boldsymbol{y}^{\prime}\right)$, capitalizing on the robust generalization capabilities of LLMs for self-assessment. By referencing the prompt illustrated in Fig. 2, we input the tuple $\left(\boldsymbol{x}, \boldsymbol{y}^{\prime}, \boldsymbol{y}\right)$ into the instruction-tuned model $\mathcal{F}$. This setup prompts the model to generate an integer quality score $z$ for the response, where $z$ ranges from 1 to $10(1 \leq z \leq 10)$.</p>
<p>In our framework, $q\left(\boldsymbol{y}, \boldsymbol{y}^{\prime} \mid \boldsymbol{x}\right)$ reflects the semantic similarity between the model's response $\boldsymbol{y}^{\prime}$ and the gold standard reference $\boldsymbol{y}$. A response that exhibits a high degree of semantic agreement with the gold reference typically indicates superior quality; on the other hand, lower similarity often points to reduced quality. When the selfevaluation score produced by the model $\mathcal{F}$ proves to be inaccurate, leveraging the semantic similarity can serve to recalibrate the score, effectively acting as an ensemble mechanism. To assess this consistency, we utilize the cosine similarity between the embedded representations of the model response and the gold reference.</p>
<p>Given that both self-evaluation by the instruction-tuned model $\mathcal{F}$ and calculation of cosine similarity are susceptible to noise, we derive the score $z$ by taking the weighted average of the self-evaluation score $z^{(1)}$ and the cosine similarity score $z^{(2) 3}$ :</p>
<p>$$
z=\alpha z^{(1)}+(1-\alpha) z^{(2)}
$$</p>
<p>This approach helps to mitigate the impact of any inaccuracies that might arise from either individual metric. Since $z^{(1)}$ is an integer ranging from 1 to 10 , and $z^{(2)}$ is a real number in the interval $[-1,1]$, we discretize $z^{(2)}$ by evenly distributing its values across 1 to 10 so that $z^{(1)}$ and $z^{(2)}$ are on the same scale. After combination, over the training set, we further adjust the score $z$ to conform to a uniform distribution across the range from 1 to $10 ; z$ is also an integer and $1 \leq z \leq 10$.
Search for optimal $\alpha^{<em>}$. To combine the two scores more effectively, we use a small development set $\mathcal{Q}_{\text {dev }}=\left{\boldsymbol{x}, \boldsymbol{y}^{\prime}, \boldsymbol{y}, z^{</em>}\right}$ ( 150 samples in our experiments) with human-labeled scores $z^{<em>}$ to find the optimal $\alpha^{</em>}$. To effectively combine the self-evaluation and cosine similarity scores, given the small size of the development set, we standardized each set of scores using Z-score normalization. Then we iteratively search for the optimal value of $\alpha$ within the range from 0 to 1 , using a step size of 0.1 . For each candidate $\alpha$, we compute the correlation between the scores of $z$ and the labeled scores $z^{<em>}$. The $\alpha$ that yields the highest correlation is selected as the optimal value, denoted as $\alpha^{</em>}$.</p>
<h3>5.2 Self-Distillation for Judge Model Training</h3>
<p>After creating a training set $\mathcal{Q}<em _theta="\theta">{\text {train }}=\left{\boldsymbol{x}, \boldsymbol{y}^{\prime}, \boldsymbol{y}, z\right}$ with quality scores of model responses, we proceed to fine-tune a pre-trained LLM to develop the judge model $\mathcal{J}</em>\right)$. Using the}$. For reference-free estimation, the judge model, taking the input instruction $\boldsymbol{x}$ and the model response $\boldsymbol{y}^{\prime}$, is trained to predict a numerical score $z$, where $z \sim \mathcal{J}_{\theta}\left(z \mid \boldsymbol{x}, \boldsymbol{y}^{\prime</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Algorithm 1 Pseudocode for SELF-J</h1>
<p>1: Input: Instruction set $\mathcal{D}<em _dev="{dev" _text="\text">{\text {train }}={\boldsymbol{x}, \boldsymbol{y}}$; Dev set $\mathcal{Q}</em>$.
2: Sample model responses with model $\mathcal{F}$ on $\mathcal{D}_{\text {train }}$ :}}=\left{\boldsymbol{x}, \boldsymbol{y}^{\prime}, \boldsymbol{y}, z^{*}\right}$ with labeled quality scores; Instruction-tuned LLM $\mathcal{F</p>
<p>$$
\mathcal{D}<em i="i">{\text {train }}^{\prime}=\left{\left(\boldsymbol{x}</em>}, \boldsymbol{y<em i="i">{i}^{\prime}, \boldsymbol{y}</em>}\right) \mid \boldsymbol{y<em i="i">{i}^{\prime} \sim \mathcal{F}\left(\boldsymbol{y}^{\prime} \mid \boldsymbol{x}</em>}\right), \forall \boldsymbol{x<em _text="\text" _train="{train">{i} \in \mathcal{D}</em>\right}
$$}</p>
<p>3: Generate self-evaluation ratings with model $\mathcal{F}$ on $\mathcal{D}_{\text {train }}^{\prime}$ :</p>
<p>$$
\mathcal{Z}^{(1)}=\left{z_{i}^{(1)} \mid z_{i}^{(1)} \sim \mathcal{F}\left(z \mid \boldsymbol{x}<em i="i">{i}, \boldsymbol{y}</em>}^{\prime}, \boldsymbol{y<em i="i">{i}\right), \forall\left(\boldsymbol{x}</em>}, \boldsymbol{y<em i="i">{i}^{\prime}, \boldsymbol{y}</em>\right}
$$}\right) \in \mathcal{D}_{\text {train }}^{\prime</p>
<p>4: Calculate cosine similarities on $\mathcal{D}_{\text {train }}^{\prime}$ :</p>
<p>$$
\mathcal{Z}^{(2)}=\left{z^{(2)} \mid z^{(2)} \sim S\left(\boldsymbol{y}<em i="i">{i}^{\prime}, \boldsymbol{y}</em>}\right), \forall\left(\boldsymbol{y<em i="i">{i}^{\prime}, \boldsymbol{y}</em>\right}
$$}\right) \in \mathcal{D}_{\text {train }}^{\prime</p>
<p>5: Search for an optimal $\alpha^{*}$ on the dev set $\mathcal{Q}_{\text {dev }}$.
6: Combine self-evaluation ratings and cosine similarities:</p>
<p>$$
\mathcal{Q}<em i="i">{\text {train }}=\left{\left(\boldsymbol{x}</em>}, \boldsymbol{y<em i="i">{i}^{\prime}, \boldsymbol{y}</em>=\alpha^{}, z_{i}\right) \mid z_{i<em>} z_{i}^{(1)}+\left(1-\alpha^{</em>}\right) z_{i}^{(2)}, \forall z_{i}^{(1)} \in \mathcal{Z}^{(1)}, z_{i}^{(2)} \in \mathcal{Z}^{(2)}\right}
$$</p>
<p>7: Adjust scores $z$ to a uniform distribution; $z$ is an integer and $1 \leq z \leq 10$.
8: Train a language model on $\mathcal{Q}<em _theta="\theta">{\text {train }}$ with self-distillation loss in Equation 6.
9: Output: Judge model $\mathcal{J}</em>$.
training set $\mathcal{Q}_{\text {train }}$, we fine-tune the LLM to minimize the negative log-likelihood:</p>
<p>$$
\mathcal{L}<em _text="\text" _train="{train">{N L L}(\theta)=-\frac{1}{\left|\mathcal{Q}</em>}}\right|} \sum_{i=1}^{\left|\mathcal{Q<em i="i">{\text {train }}\right|} \log p\left(z</em>} \mid \boldsymbol{x<em i="i">{i}, \boldsymbol{y}</em>\right)
$$}^{\prime</p>
<p>where we use the template shown in Fig. 12 to include $\boldsymbol{x}$ and $\boldsymbol{y}^{\prime}$ as the input context. For implementation convenience, we subtract 1 from the scores in the training set so that during training, the range of $z$ is from 0 to 9 .</p>
<p>During testing, the judge model estimates the quality score $z$ using only the context of the input instruction and model response, since usually, no reference response is available to the judge model. Ideally, if the reference answer is available, it would significantly aid the model in making more precise estimation of the score $z$. However, the reference answer is typically absent during testing. To address this problem, we introduce a self-distillation approach to train the judge model. It involves optimizing a teacher objective that incorporates the reference answer for quality estimation, i.e., $p\left(z \mid \boldsymbol{x}, \boldsymbol{y}^{\prime}, \boldsymbol{y}\right)$, and then distilling the ability of the teacher into a student model that performs reference-free estimation, i.e., $p\left(z \mid \boldsymbol{x}, \boldsymbol{y}^{\prime}\right)$.</p>
<p>We optimize a KL-divergence loss for self-distillation. By considering the selfdistillation objective, our final training loss is defined as:</p>
<p>$$
\begin{aligned}
\mathcal{L}<em _text="\text" _train="{train">{S D}(\theta)=-\frac{1}{\left|\mathcal{Q}</em>}}\right|} \sum_{i=1}^{\left|\mathcal{Q<em i="i">{\text {train }}\right|} \quad &amp; \log p\left(z</em>} \mid \boldsymbol{x<em i="i">{i}, \boldsymbol{y}</em>}^{\prime}, \boldsymbol{y<em i="i">{i}\right) \
&amp; +\beta \log p\left(z</em>} \mid \boldsymbol{x<em i="i">{i}, \boldsymbol{y}</em>\right) \
&amp; -\gamma \mathrm{KL}\left[p\left(z_{i} \mid \boldsymbol{x}}^{\prime<em i="i">{i}, \boldsymbol{y}</em>}^{\prime}\right) | p\left(z_{i} \mid \boldsymbol{x<em i="i">{i}, \boldsymbol{y}</em>\right)\right])
\end{aligned}
$$}^{\prime}, \boldsymbol{y}_{i</p>
<p>Here, "SD" stands for "self-distillation", with $\beta$ and $\gamma$ serving as hyper-parameters. We fine-tune the same model using both teacher and student objectives, which is why this process is termed self-distillation. For each batch of training data, the model undergoes two forward passes-one for the teacher objective and one for the student objective. During the optimization of the KL loss, gradient back-propagation in the teacher is omitted. For the teacher objective, we use the template in Fig. 13 to include $\boldsymbol{x}, \boldsymbol{y}^{\prime}$, and $\boldsymbol{y}$ as the input context. The pseudocode in Algorithm 1 displays the detailed training procedure of SELF-J.</p>
<p>After training, our model-referred to as the judge model-can perform both reference-free and reference-based evaluations. Focusing on the reference-free method, to determine the quality $z$ of a response $\boldsymbol{y}^{\prime}$, we compute the expected score $z$ as follows:</p>
<p>$$
z=\sum_{c_{i}=0}^{C} c_{i} \cdot p\left(c_{i} \mid \boldsymbol{x}, \boldsymbol{y}^{\prime}\right)
$$</p>
<p>In this equation, $c_{i}$ represents each possible score class, ranging from 0 to $C$ (with $C=9$ in our case). The model predicts the probability $p\left(c_{i} \mid \boldsymbol{x}, \boldsymbol{y}^{\prime}\right)$ for each score class. Thus, $z$ is calculated as a weighted average, integrating the probabilities of each score class effectively.</p>
<h1>6. Instruction Collection</h1>
<p>We aim to study alignment evaluation on generation tasks, such as coding, writing, etc. To better validate our method, we collected a large number of input instructions. We manually filtered datasets from Hugging Face as of June 2023, particularly those in the NLP category. We post-processed the datasets to filter out low-quality instructions as much as possible. We retained all good-quality instructions. We removed instructions that were either too short or too long. We also used the original instructions without tokenization, paraphrasing, etc, to maintain the real distribution of the instructions. After sorting, we keep 37 datasets in total as indicated in Table 6 of the Appendix. The training sets of these datasets will be used as instructions. We manually categorized the datasets into three main categories: common, coding, and academic. Common instructions mainly concern everyday matters, such as seeking advice and solving technical problems. All instructions involving coding such as code generation and debugging are classified under the coding category. Lastly, subject-specific instructions, such as science and medicine, are categorized as academic.</p>
<p>Our cleaned collection consists of around 5.7 million instructions (see Table 6). We show the diversity of our collection in Fig. 4, covering different topics. Fig. 5</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4
Diversity of our instruction set (with 30k random samples for judge model training), showing root verbs in the inner circle and their first nouns in the outer circle.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5
The proportions of different categories in our whole instruction set (about 5.7 million instructions). Common topics have the highest number of questions, followed by coding questions, with academic questions being the least frequent.
shows the proportion of each category. The largest category is common instructions, followed by coding instructions, and then academic instructions. We further calculate the distribution of token counts of the instructions in Fig. 14. Most of our instructions are not too long or too short, with the number of tokens mostly between 10 and 20. Some of the instructions are long with a token count larger than 150. We also show some sample instructions in Fig. 15. We keep the original instructions from the source datasets without paraphrasing, to maintain the real distribution of the instructions.</p>
<h1>7. Experiments</h1>
<h3>7.1 Datasets</h3>
<p>We sample a part of the data from our collected instructions to conduct our experiments. Table 1 shows the data statistics used in our experiments. Since our collected instructions do not have high-quality answers from the original datasets, and given the prohibitive cost of human labeling, we follow the practice of previous work by using ChatGPT and GPT-4 to simulate human expertise (Chiang et al. 2023; Xu et al. 2023), which means we use ChatGPT and GPT-4 to generate reference answers for our instructions. We also use GPT-4 to generate quality scores on dev and eval sets to simulate human ratings.
Instruction Fine-Tuning From our collected instructions, we sample 87 k instructions for training our instruction-following model, where the reference outputs are generated by ChatGPT. Choosing 87 k instructions for training follows the setting of Vicuna (Chiang et al. 2023).</p>
<p>Table 1
Datasets used in our experiments. For data used for instruction tuning, we use GPT-3.5-turbo to generate the reference answers, and GPT-4 is called to provide responses for instructions used in training judge models. Dev set is used to search for the optimal $\alpha$ to combine self-evaluation ratings and cosine similarities. For evaluation, we first evaluate methods on our collected instructions, and then AlpacaEval is combined for generalization demonstration. On dev and test sets, we use GPT-4 to generate quality scores for model responses to simulate human ratings.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Datasets</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">Source of References</th>
<th style="text-align: center;">Source of Ratings</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Instruction tuning</td>
<td style="text-align: center;">87 k</td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Judge model training</td>
<td style="text-align: center;">30 k</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Dev set</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">GPT-4</td>
</tr>
<tr>
<td style="text-align: left;">Eval set 1</td>
<td style="text-align: center;">850</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">GPT-4</td>
</tr>
<tr>
<td style="text-align: left;">Eval set 2 (AlpacaEval)</td>
<td style="text-align: center;">805</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">GPT-4</td>
</tr>
</tbody>
</table>
<p>Jugde Model Training We further sample another 30k instructions from our collection for judge model learning, without overlapping with the instruction-tuning set. Different from the instruction-tuning set, for judge model training, the reference answers are from GPT-4, as GPT-4 is substantially better than ChatGPT, which allows us to more accurately validate our algorithm. We set the size of the training set to 30 k because calling GPT-4 is too expensive to generate outputs for many more instructions. 30k is a number that we can afford and is a reasonable amount for judge model training (as demonstrated by our experiments).</p>
<p>We allocate one dataset for instruction tuning and another one for training the judge model, and the two sets do not overlap. However, practically, the two sets can be shared. The rationale behind segregating the datasets for instruction tuning and judge model training is primarily due to the high cost associated with calling GPT-4 for generating responses, limiting us to only 30k instructions for judge modeling, but we may need more data to conduct instruction fine-tuning. Additionally, for existing instruction-tuned LLMs, we only need the data for judge model training.
Dev and Eval Sets We randomly sample 1k for judge model evaluation, where we randomly split the 1 k samples into the development set (150) and test set (850). Subsequently, for generalization testing, we expand the evaluation by integrating our instructions with AlpacaEval (Li et al. 2023c). AlpacaEval is regarded as a cross-domain benchmark that may have a different domain from our collected eval set. AlpacaEval includes 805 instructions consisting of diverse tasks, such as coding, writing, reasoning, role-playing, advising, etc. For the dev and eval sets, the reference answers are also from GPT-4.</p>
<p>We use GPT-4 to generate quality scores for model responses, where we utilize the reference-based estimation since as indicated by Zheng et al. (2023), incorporating reference answers can improve the quality of GPT-4's evaluation on reasoning tasks, such as coding problems and mathematical questions. As shown in Fig. 2, we adopt the template from Zheng et al. 2023 and Dettmers et al. 2023 for GPT-4 evaluation.</p>
<h1>7.2 Setup</h1>
<p>Instruction Tuning We use our collected 87 k instructions with outputs generated by ChatGPT to fine-tune the Llama-2-13b model (Touvron et al. 2023). We use LoRA fine-</p>
<p>tuning (Hu et al. 2022). We set the batch size to 128 . We train the model for 3 epochs with a learning rate of $3 \mathrm{e}-4$. For LoRA fine-tuning, we tune the modules of query, key, value, and output projection. The tuned model is named Ours-13b.
Tested Models. Except for the model Ours-13b, we further study the following opensource models: Vicuna-13b-v1.5 (Chiang et al. 2023), WizardLM-13b-v1.2 (Xu et al. 2023), Llama-2-13b-Chat, and Llama-2-70b-Chat (Touvron et al. 2023). These models are directly used as instruction-tuned models. In total, we have 5 models as the tested models for evaluation.
Judge Models. We train a judge model for each of the tested models. We use 30k instructions for judge model tuning, where the reference answer is from GPT-4. Each tested model samples one response $\boldsymbol{y}^{\prime}$ for each instruction, which creates 30 k data points, i.e., $\left(\boldsymbol{x}, \boldsymbol{y}^{\prime}, \boldsymbol{y}\right)$, for training. Then the model generates the self-rating scores for the sampled responses $\boldsymbol{y}^{\prime}$ by also using the template as in Fig. 2. To calculate cosine similarity, we use text-embedding-ada-002 from OpenAI to obtain the embeddings of the answer texts.</p>
<p>We tune Llama-2-13b as the judge models, where LoRA (Hu et al. 2022) is applied for parameter-efficient tuning. We set the batch size to 128 . We train the model for 2 epochs with a learning rate of $3 \mathrm{e}-4$. For LoRA fine-tuning, we tune the modules of query, key, value, and output projection. During self-distillation tuning, the teacher and student losses are jointly optimized on a shared base model. $\beta$ is set to 2 and $\gamma$ to 0.3 after parameter selection. The gradient is cut off to back-propagate to the teacher when applying the KL-divergence loss. Experiments were conducted on Nvidia A100 GPUs.</p>
<p>When using the instruction-tuned models to perform self-evaluation, we find that the models may not generate a response that strictly follows the format specified by the prompt in Fig. 2. The rating score is required to be in the format of " ${$ "rating": model rating $]^{\prime \prime}$. To deal with this problem, we prompt GPT-3.5-turbo to extract the rating score from a response. In practice, we can employ humans or other automatic methods to extract the rating scores.</p>
<h1>7.3 Baselines</h1>
<p>In our evaluation, we consider reference-based and reference-free evaluation. In reference-based evaluation, we compare the following baselines:</p>
<ul>
<li>Cosine We calculate the cosine similarity between the embeddings of the model's response and the reference answer, utilizing OpenAI's text-embedding-ada-002 to encode the texts.</li>
<li>Self-eval We initiate self-evaluation by the instruction-tuned model itself using the prompt depicted in Fig. 2.</li>
<li>Self-eval+cosine We merge the scores from self-eval and cosine similarity as outlined in Eq. 4, utilizing a development set to determine the optimal hyperparameter $\alpha^{*}$ for this combination.</li>
<li>GPT-3.5-turbo Similar to the self-eval method, we employ the prompt detailed in Fig. 2 to instruct GPT-3.5-turbo to generate quality scores. Additionally, we enhance the scoring by integrating GPT-3.5-turbo's scores with cosine similarities (denoted as " + cosine"), applying the optimal value of $\alpha^{*}$ identified on the dev set.</li>
</ul>
<p>Then we further experiment with reference-free evaluation by comparing to the baselines as follows:</p>
<p>Table 2
Given GPT-4's status as the leading model, we first rely on GPT-4 to assess the model's response (using the template in Fig. 2), and then calculate the Pearson correlation coefficients (in $\%$ ) between various measures with GPT-4's scores. The combined set consists of our eval set and AlpacaEval. Auto-J-13b and UltraRM-13b are supervised models distilled from GPT-4, which are both fine-tuned on Llama-2-13b. PPL and VRO are training-free methods. Each tested model has trained a judge model.</p>
<p>Our 850 test samples</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Ours-13b</th>
<th style="text-align: center;">Vicuna-13b</th>
<th style="text-align: center;">WizardLM-13b</th>
<th style="text-align: center;">Llm2-13b-chat</th>
<th style="text-align: center;">Llm2-70b-chat</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">with reference</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Cosine</td>
<td style="text-align: center;">39.75</td>
<td style="text-align: center;">42.81</td>
<td style="text-align: center;">40.81</td>
<td style="text-align: center;">59.04</td>
<td style="text-align: center;">58.82</td>
<td style="text-align: center;">48.25</td>
</tr>
<tr>
<td style="text-align: center;">Self-eval</td>
<td style="text-align: center;">44.66</td>
<td style="text-align: center;">55.13</td>
<td style="text-align: center;">48.52</td>
<td style="text-align: center;">40.26</td>
<td style="text-align: center;">50.70</td>
<td style="text-align: center;">47.85</td>
</tr>
<tr>
<td style="text-align: center;">Self-eval+cosine</td>
<td style="text-align: center;">53.19</td>
<td style="text-align: center;">60.77</td>
<td style="text-align: center;">55.69</td>
<td style="text-align: center;">64.72</td>
<td style="text-align: center;">65.51</td>
<td style="text-align: center;">59.98</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-turbo</td>
<td style="text-align: center;">66.41</td>
<td style="text-align: center;">66.58</td>
<td style="text-align: center;">69.96</td>
<td style="text-align: center;">73.35</td>
<td style="text-align: center;">75.81</td>
<td style="text-align: center;">70.42</td>
</tr>
<tr>
<td style="text-align: center;">+ cosine</td>
<td style="text-align: center;">68.33</td>
<td style="text-align: center;">69.99</td>
<td style="text-align: center;">70.90</td>
<td style="text-align: center;">78.13</td>
<td style="text-align: center;">78.12</td>
<td style="text-align: center;">73.09</td>
</tr>
<tr>
<td style="text-align: center;">SELF-J (ours)</td>
<td style="text-align: center;">66.75</td>
<td style="text-align: center;">70.95</td>
<td style="text-align: center;">69.56</td>
<td style="text-align: center;">72.76</td>
<td style="text-align: center;">71.70</td>
<td style="text-align: center;">70.34</td>
</tr>
<tr>
<td style="text-align: center;">without reference</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PPL</td>
<td style="text-align: center;">13.22</td>
<td style="text-align: center;">13.46</td>
<td style="text-align: center;">6.47</td>
<td style="text-align: center;">29.25</td>
<td style="text-align: center;">$-3.99$</td>
<td style="text-align: center;">11.68</td>
</tr>
<tr>
<td style="text-align: center;">VRO</td>
<td style="text-align: center;">45.20</td>
<td style="text-align: center;">40.03</td>
<td style="text-align: center;">38.24</td>
<td style="text-align: center;">40.66</td>
<td style="text-align: center;">41.47</td>
<td style="text-align: center;">41.12</td>
</tr>
<tr>
<td style="text-align: center;">Self-eval</td>
<td style="text-align: center;">1.23</td>
<td style="text-align: center;">15.19</td>
<td style="text-align: center;">12.75</td>
<td style="text-align: center;">12.13</td>
<td style="text-align: center;">15.99</td>
<td style="text-align: center;">11.46</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-turbo</td>
<td style="text-align: center;">15.21</td>
<td style="text-align: center;">25.98</td>
<td style="text-align: center;">19.07</td>
<td style="text-align: center;">20.05</td>
<td style="text-align: center;">22.78</td>
<td style="text-align: center;">20.62</td>
</tr>
<tr>
<td style="text-align: center;">Auto-J-13b</td>
<td style="text-align: center;">37.02</td>
<td style="text-align: center;">39.68</td>
<td style="text-align: center;">37.88</td>
<td style="text-align: center;">53.71</td>
<td style="text-align: center;">49.43</td>
<td style="text-align: center;">43.54</td>
</tr>
<tr>
<td style="text-align: center;">UltraRM-13b</td>
<td style="text-align: center;">43.50</td>
<td style="text-align: center;">44.18</td>
<td style="text-align: center;">50.68</td>
<td style="text-align: center;">63.83</td>
<td style="text-align: center;">62.69</td>
<td style="text-align: center;">52.98</td>
</tr>
<tr>
<td style="text-align: center;">Judge models-13b</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Judge (cosine)</td>
<td style="text-align: center;">39.73</td>
<td style="text-align: center;">38.78</td>
<td style="text-align: center;">39.21</td>
<td style="text-align: center;">61.20</td>
<td style="text-align: center;">58.06</td>
<td style="text-align: center;">47.40</td>
</tr>
<tr>
<td style="text-align: center;">Judge (self-eval)</td>
<td style="text-align: center;">45.02</td>
<td style="text-align: center;">45.14</td>
<td style="text-align: center;">43.61</td>
<td style="text-align: center;">48.13</td>
<td style="text-align: center;">44.57</td>
<td style="text-align: center;">45.29</td>
</tr>
<tr>
<td style="text-align: center;">SELF-J (ours)</td>
<td style="text-align: center;">56.94</td>
<td style="text-align: center;">56.67</td>
<td style="text-align: center;">53.10</td>
<td style="text-align: center;">64.87</td>
<td style="text-align: center;">61.65</td>
<td style="text-align: center;">58.65</td>
</tr>
<tr>
<td style="text-align: center;">- self-distil</td>
<td style="text-align: center;">50.35</td>
<td style="text-align: center;">50.75</td>
<td style="text-align: center;">49.76</td>
<td style="text-align: center;">62.02</td>
<td style="text-align: center;">59.91</td>
<td style="text-align: center;">54.56</td>
</tr>
<tr>
<td style="text-align: center;">All 1655 test samples (Our collections + AlpacaEval)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">Ours-13b</td>
<td style="text-align: center;">Vicuna-13b</td>
<td style="text-align: center;">WizardLM-13b</td>
<td style="text-align: center;">Llm2-13b-chat</td>
<td style="text-align: center;">Llm2-70b-chat</td>
<td style="text-align: center;">Avg.</td>
</tr>
<tr>
<td style="text-align: center;">with reference</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Cosine</td>
<td style="text-align: center;">35.76</td>
<td style="text-align: center;">41.43</td>
<td style="text-align: center;">41.41</td>
<td style="text-align: center;">54.05</td>
<td style="text-align: center;">54.11</td>
<td style="text-align: center;">45.35</td>
</tr>
<tr>
<td style="text-align: center;">Self-eval</td>
<td style="text-align: center;">37.87</td>
<td style="text-align: center;">52.21</td>
<td style="text-align: center;">42.13</td>
<td style="text-align: center;">39.96</td>
<td style="text-align: center;">46.35</td>
<td style="text-align: center;">43.70</td>
</tr>
<tr>
<td style="text-align: center;">Self-eval+cosine</td>
<td style="text-align: center;">46.68</td>
<td style="text-align: center;">56.44</td>
<td style="text-align: center;">48.28</td>
<td style="text-align: center;">57.65</td>
<td style="text-align: center;">57.41</td>
<td style="text-align: center;">53.29</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-turbo</td>
<td style="text-align: center;">63.21</td>
<td style="text-align: center;">65.63</td>
<td style="text-align: center;">65.56</td>
<td style="text-align: center;">70.69</td>
<td style="text-align: center;">71.90</td>
<td style="text-align: center;">67.40</td>
</tr>
<tr>
<td style="text-align: center;">+ cosine</td>
<td style="text-align: center;">63.70</td>
<td style="text-align: center;">66.53</td>
<td style="text-align: center;">63.48</td>
<td style="text-align: center;">73.46</td>
<td style="text-align: center;">70.89</td>
<td style="text-align: center;">67.61</td>
</tr>
<tr>
<td style="text-align: center;">SELF-J (ours)</td>
<td style="text-align: center;">58.84</td>
<td style="text-align: center;">66.54</td>
<td style="text-align: center;">65.16</td>
<td style="text-align: center;">69.39</td>
<td style="text-align: center;">67.17</td>
<td style="text-align: center;">65.42</td>
</tr>
<tr>
<td style="text-align: center;">without reference</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PPL</td>
<td style="text-align: center;">2.31</td>
<td style="text-align: center;">$-3.30$</td>
<td style="text-align: center;">$-2.22$</td>
<td style="text-align: center;">6.41</td>
<td style="text-align: center;">$-2.78$</td>
<td style="text-align: center;">0.08</td>
</tr>
<tr>
<td style="text-align: center;">VRO</td>
<td style="text-align: center;">39.49</td>
<td style="text-align: center;">36.58</td>
<td style="text-align: center;">35.57</td>
<td style="text-align: center;">45.24</td>
<td style="text-align: center;">44.05</td>
<td style="text-align: center;">40.19</td>
</tr>
<tr>
<td style="text-align: center;">Self-eval</td>
<td style="text-align: center;">1.41</td>
<td style="text-align: center;">11.94</td>
<td style="text-align: center;">14.98</td>
<td style="text-align: center;">15.05</td>
<td style="text-align: center;">11.62</td>
<td style="text-align: center;">11.00</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-turbo</td>
<td style="text-align: center;">18.03</td>
<td style="text-align: center;">17.21</td>
<td style="text-align: center;">17.32</td>
<td style="text-align: center;">18.79</td>
<td style="text-align: center;">19.11</td>
<td style="text-align: center;">18.09</td>
</tr>
<tr>
<td style="text-align: center;">Auto-J-13b</td>
<td style="text-align: center;">43.96</td>
<td style="text-align: center;">40.20</td>
<td style="text-align: center;">39.36</td>
<td style="text-align: center;">52.49</td>
<td style="text-align: center;">48.77</td>
<td style="text-align: center;">44.96</td>
</tr>
<tr>
<td style="text-align: center;">UltraRM-13b</td>
<td style="text-align: center;">44.87</td>
<td style="text-align: center;">44.50</td>
<td style="text-align: center;">49.80</td>
<td style="text-align: center;">64.54</td>
<td style="text-align: center;">62.72</td>
<td style="text-align: center;">53.29</td>
</tr>
<tr>
<td style="text-align: center;">Judge models-13b</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Judge (cosine)</td>
<td style="text-align: center;">31.10</td>
<td style="text-align: center;">29.86</td>
<td style="text-align: center;">36.83</td>
<td style="text-align: center;">58.85</td>
<td style="text-align: center;">52.14</td>
<td style="text-align: center;">41.76</td>
</tr>
<tr>
<td style="text-align: center;">Judge (self-eval)</td>
<td style="text-align: center;">44.02</td>
<td style="text-align: center;">42.39</td>
<td style="text-align: center;">42.44</td>
<td style="text-align: center;">53.26</td>
<td style="text-align: center;">41.92</td>
<td style="text-align: center;">44.81</td>
</tr>
<tr>
<td style="text-align: center;">SELF-J (ours)</td>
<td style="text-align: center;">48.95</td>
<td style="text-align: center;">51.10</td>
<td style="text-align: center;">50.72</td>
<td style="text-align: center;">64.54</td>
<td style="text-align: center;">59.78</td>
<td style="text-align: center;">55.02</td>
</tr>
<tr>
<td style="text-align: center;">- self-distil</td>
<td style="text-align: center;">41.07</td>
<td style="text-align: center;">43.16</td>
<td style="text-align: center;">45.08</td>
<td style="text-align: center;">60.59</td>
<td style="text-align: center;">55.06</td>
<td style="text-align: center;">48.99</td>
</tr>
</tbody>
</table>
<ul>
<li>PPL We compute the average negative log-likelihood over output tokens.</li>
<li>VRO For the model's output, we generate three additional outputs and compute the sampling variance between the original output and these extra outputs, as elaborated in Eq. 1. We employ OpenAI's text-embedding-ada-002 for text encoding.</li>
<li>Auto-J-13b This is an open-source model designed to predict numerical quality scores for alignment evaluation, but unlike ours, it is distilled from GPT-4's evalu-</li>
</ul>
<p>ation scores (Li et al. 2023a). This model is also fine-tuned from Llama-2-13b with full parameter fine-tuning.</p>
<ul>
<li>UltraRM-13b This is a reward model tuned with preference feedback data generated by GPT-4. The model is trained to assign a higher reward to the preferred answer but a lower reward to the rejected answer. The model is also fine-tuned from Llama-2-13b with full-parameter fine-tuning.</li>
</ul>
<p>Lastly, we report the results of trained judge models. By ablating SELF-J, we further train Judge (cosine) and Judge (self-eval). Similar to SELF-J, both ablated baselines first generate quality scores and then train the judge models with the generated scores.</p>
<ul>
<li>Judge (cosine) relies on the cosine response-reference similarity scores.</li>
<li>Jugde (self-eval) uses self-evaluation that generates quality scores with reference.</li>
</ul>
<p>For the two ablated baselines, we do not use self-distillation during judge model training. We directly use the generated scores for training. For SELF-J, we report the results of reference-based and reference-free evaluation.</p>
<h1>7.4 Main Results</h1>
<p>We present our main results, the Pearson correlation coefficients with GPT-4, in Table 2. By analyzing these results, we made the following observations:
PPL is ineffective for uncertainty estimation in instruction tuning. Initially, we explore baselines for uncertainty estimation that do not require training. From the results, we can see that PPL is especially poor, exhibiting a low or even negative correlation with GPT-4's evaluation.
VRO is much better than PPL. We find that sampling variance reliably estimates alignment, correlating with GPT-4's evaluation. However, it is less effective than tuningbased methods such as Auto-J-13b, UltraRM-13b, and our tuned judge models. Sampling variance is a useful alternative when training a judge model is impractical.
Cosine similarity and self-evaluation perform well with references. The methods of cosine and self-eval both have merits. In reference-based evaluation, we can see that self-eval on Vicuna and WizarLM excels, but not on our tuned model or Llama-2Chat. However, without a reference, self-eval becomes much worse than VRO since the effectiveness of self-evaluation is closely related to the model's capabilities.
Integrating both methods improves outcomes. Our method Self-eval+cosine outperforms both cosine and self-eval. It raises correlation by up to 7 points in models like Vicuna and WizardLM. This validates the effectiveness of integrating cosine similarity and self-evaluation.
Training judge models proves to be an effective method. Based on the performance of various judge models, such as Auto-J-13b, UltraRM-13b, and three versions employing different quality scores during training, it is evident that the trained judge models surpass the training-free baseline, $V R O$, in performance.
SELF-J stands out as the top-performing judge model. On reference-free evaluation, our approach SELF-J outperforms other trained judge models, including those models distilled from GPT-4. It is also much better than GPT-3.5-turbo, where even GPT-3.5turbo cannot perform well without the reference answers. On reference-based evaluation, we find SELF-J to be competitive with GPT-3.5-turbo. SELF-J proves to be superior to the method of Self-eval+cosine, despite its training data being generated by Selfeval+cosine, which is likely attributable to the strong generalization capability of large language models even with noise in the training data.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6
Results for selective instruction following on our collected eval set: average GPT-4 evaluation score versus abstention rate. If the judge model rates a model response below a certain threshold, that response is discarded. By adjusting this threshold, we can generate various combinations of abstention rate and average GPT-4 evaluation score for the model responses that are kept.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7
Results for selective refinement on our collected eval set: average GPT-4 evaluation score versus coverage. If a response is rated below a certain threshold by the judge model, it undergoes self-refinement. The refinement process has two stages: first, the model creates feedback $\boldsymbol{f}$ : $\boldsymbol{x}+\boldsymbol{y}<em 1="1">{1}^{\prime}+z \rightarrow \boldsymbol{f}$. Then, it uses this feedback to refine the initial response $\boldsymbol{y}</em>}^{\prime}$ into a new response $\boldsymbol{y<em 1="1">{2}^{\prime}$, following $\boldsymbol{x}+\boldsymbol{y}</em>$. We plot and compare two curves, one for the first-round response, and one for the second-round response with self-refinement.}^{\prime}+\boldsymbol{f} \rightarrow \boldsymbol{y}_{2}^{\prime</p>
<p>Table 3
Refinement results on our collected test set with GPT-4 evaluation scores (using the template in Fig. 2 for evaluation). A model refines initial responses using generated feedback for improved second-round outputs. Refinement improves the quality of model responses in each category.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Ours-13b</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">WizardLM-13b</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">1st</td>
<td style="text-align: left;">2nd</td>
<td style="text-align: left;">1st</td>
<td style="text-align: left;">2nd</td>
</tr>
<tr>
<td style="text-align: left;">Common</td>
<td style="text-align: left;">6.42</td>
<td style="text-align: left;">$\mathbf{6 . 4 5}$</td>
<td style="text-align: left;">7.08</td>
<td style="text-align: left;">$\mathbf{7 . 1 5}$</td>
</tr>
<tr>
<td style="text-align: left;">Coding</td>
<td style="text-align: left;">5.32</td>
<td style="text-align: left;">$\mathbf{5 . 3 3}$</td>
<td style="text-align: left;">5.67</td>
<td style="text-align: left;">$\mathbf{5 . 8 4}$</td>
</tr>
<tr>
<td style="text-align: left;">Academic</td>
<td style="text-align: left;">7.45</td>
<td style="text-align: left;">$\mathbf{7 . 5 3}$</td>
<td style="text-align: left;">7.91</td>
<td style="text-align: left;">$\mathbf{8 . 0 1}$</td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: left;">6.17</td>
<td style="text-align: left;">$\mathbf{6 . 2 4}$</td>
<td style="text-align: left;">6.69</td>
<td style="text-align: left;">$\mathbf{6 . 8 5}$</td>
</tr>
</tbody>
</table>
<h1>7.5 Selective Instruction Following and Refinement</h1>
<p>We report more results for selective instruction following. We plot the curve of average GPT-4 evaluation score versus abstention rate, where a model response is discarded if its score rated by the judge model is lower than a threshold, and we vary the threshold to obtain different pairs of abstention rate and average GPT-4 evaluation score of maintained model responses. We show the results of selective instruction following in Fig. 6. For selective instruction following, as expected, the overall performance improves with the exclusion of more responses with lower rating scores from the generation process. Our method SELF-J outperforms other baselines by achieving consistently higher GPT4 scores at different abstention rates. Two ablated models which are Judge (cosine) and Judge (self-eval) perform worse than SELF-J. We observe that VRO is a strong baseline that can compete with training methods such as UltraRM-13b, Judge (cosine), and Judge (self-eval). We find that Auto-J-13b does not perform well when the abstention rates are high.</p>
<p>We further study selective refinement, where a model response with a judge model's rating score lower than the threshold will be refined by the model itself. For refinement, the model follows $\boldsymbol{x}+\boldsymbol{y}<em 1="1">{1}^{\prime}+z \rightarrow \boldsymbol{f}$ and $\boldsymbol{x}+\boldsymbol{y}</em>}^{\prime}+\boldsymbol{f} \rightarrow \boldsymbol{y<em 1="1">{2}^{\prime}$, where the model needs to generate feedback $f$ first, then incorporate the feedback to refine the firstround response $\boldsymbol{y}</em>$. To evaluate the effectiveness of refinement, we plot two curves of average GPT-4 score versus coverage, one for the first-round response and one for the second-round response refined by the model itself. From the results shown in Fig. 7, we can see an improvement by selective refinement. We also present the results of refinement on all responses in Table 3 and the refinement process enhances the quality of the model's responses on each category of instructions. Overall, on our 13b model, the average score is improved from 6.17 to 6.24 , and on WizardLM-13b, the score is improved from 6.69 to 6.85 . Hence, selective refinement enhances model performance.}^{\prime}$ to get the second-round response $\boldsymbol{y}_{2}^{\prime</p>
<h3>7.6 Results of Domain Transfer</h3>
<p>Domain Transfer. In our study, each judge model is associated with a specific instruction-following model. We examine the judge model's capacity to generalize across various models, which is testing a judge model trained for one source model to evaluate other target models. The results of this generalization test are shown in Fig. 8. Compared to the VRO baseline, which is a strong baseline without requiring model</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8
Generalization test on our eval set: judge models from one source assess different target models; SELF-J's Pearson correlation coefficient with GPT-4 minus that of the VRO baseline. VRO is the unsupervised baseline that is free of model training and shows a strong performance. The positive values, i.e., better than VRO, indicate the good generalization ability of judge models.</p>
<h1>Table 4</h1>
<p>System-level Kendall's $\tau$ correlation (in \%) with GPT-4 on AlpacaEval (version 1), assessed using judge models (w/o ref.) across 95 models. For each tested model from the leaderboard of AlpacaEval, we use the judge model (which has been trained for Ours-13b, Vicuna-13b, Wizardlm-13b, Llm2-13b-chat, or Llm2-70b-chat) to rate each response and calculate the average performance on the test set. Using 95 models, we assess the system-level ranking by comparing the ranking derived from the scores of the judge models with that of GPT-4, thereby measuring the correlation between the judge model and GPT-4. See also Fig. 16.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Kendall (\%)</th>
<th style="text-align: center;">SELF-J</th>
<th style="text-align: center;">Judge (cosine)</th>
<th style="text-align: center;">Judge (self-eval)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ours-13b</td>
<td style="text-align: center;">50.77</td>
<td style="text-align: center;">41.05</td>
<td style="text-align: center;">$\mathbf{6 8 . 5 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-13b</td>
<td style="text-align: center;">69.09</td>
<td style="text-align: center;">48.04</td>
<td style="text-align: center;">$\mathbf{7 0 . 5 7}$</td>
</tr>
<tr>
<td style="text-align: left;">Wizardlm-13b</td>
<td style="text-align: center;">$\mathbf{6 8 . 4 2}$</td>
<td style="text-align: center;">38.37</td>
<td style="text-align: center;">59.87</td>
</tr>
<tr>
<td style="text-align: left;">Llm2-13b-chat</td>
<td style="text-align: center;">$\mathbf{6 6 . 4 9}$</td>
<td style="text-align: center;">52.47</td>
<td style="text-align: center;">59.37</td>
</tr>
<tr>
<td style="text-align: left;">Llm2-70b-chat</td>
<td style="text-align: center;">62.87</td>
<td style="text-align: center;">52.92</td>
<td style="text-align: center;">$\mathbf{6 4 . 2 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Avg.</td>
<td style="text-align: center;">63.53</td>
<td style="text-align: center;">46.57</td>
<td style="text-align: center;">64.51</td>
</tr>
</tbody>
</table>
<p>training, our trained judge models generally exhibit notable improved performance. This highlights the strong generalization ability of the trained judge models. We also find that the judge model trained for Vicuna exhibits the best performance across target models.</p>
<h3>7.7 Results on AlpacaEval</h3>
<p>The AlpacaEval benchmark has two versions. V1 compares the model with text-davinci003, but V2 uses the baseline of GPT-4-turbo. Here we apply the trained judge models</p>
<p>Table 5
Best-of-32 sampling results with WizardLM on AlpacaEval using judge models as the reward model. Here, we use the judge models trained for Vicuna-13b since it shows the best performance in Fig. 8 and Table 4. Our instruction-following model tuned with our collected instructions is comparable to Llama-2-13b-Chat. WizardLM-13b + best-of-32 (SELF-J and Judge (self-eval)) outperforms GPT-4 0613 on V2 evaluation.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">AlpacaEval <br> Models</th>
<th style="text-align: center;">V1 <br> $(\%)$ Win</th>
<th style="text-align: center;">V2 <br> $(\%)$ Win</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">gpt4_turbo</td>
<td style="text-align: center;">97.70</td>
<td style="text-align: center;">50.00</td>
</tr>
<tr>
<td style="text-align: left;">Yi-34B-Chat</td>
<td style="text-align: center;">94.08</td>
<td style="text-align: center;">29.66</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 0613</td>
<td style="text-align: center;">93.78</td>
<td style="text-align: center;">15.76</td>
</tr>
<tr>
<td style="text-align: left;">GPT 3.5 Turbo 0613</td>
<td style="text-align: center;">93.42</td>
<td style="text-align: center;">14.13</td>
</tr>
<tr>
<td style="text-align: left;">Claude 2</td>
<td style="text-align: center;">91.36</td>
<td style="text-align: center;">17.19</td>
</tr>
<tr>
<td style="text-align: left;">Claude</td>
<td style="text-align: center;">88.39</td>
<td style="text-align: center;">16.99</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA2 Chat 70B</td>
<td style="text-align: center;">92.66</td>
<td style="text-align: center;">13.87</td>
</tr>
<tr>
<td style="text-align: left;">UltraLM 13B V2.0 (best-of-16)</td>
<td style="text-align: center;">92.30</td>
<td style="text-align: center;">13.85</td>
</tr>
<tr>
<td style="text-align: left;">PairRM 0.4B+Tulu 2+DPO 13B (best-of-16)</td>
<td style="text-align: center;">91.06</td>
<td style="text-align: center;">13.83</td>
</tr>
<tr>
<td style="text-align: left;">Tulu 2+DPO 13B</td>
<td style="text-align: center;">88.12</td>
<td style="text-align: center;">10.12</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna 13B v1.3</td>
<td style="text-align: center;">82.11</td>
<td style="text-align: center;">7.14</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna 13B v1.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">6.70</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA2 Chat 13B</td>
<td style="text-align: center;">81.09</td>
<td style="text-align: center;">7.70</td>
</tr>
<tr>
<td style="text-align: left;">Ours-13b</td>
<td style="text-align: center;">79.13</td>
<td style="text-align: center;">7.33</td>
</tr>
<tr>
<td style="text-align: left;">WizardLM-13B-V1.2</td>
<td style="text-align: center;">89.17</td>
<td style="text-align: center;">12.03</td>
</tr>
<tr>
<td style="text-align: left;">w/ best-of-32 SELF-J</td>
<td style="text-align: center;">92.48</td>
<td style="text-align: center;">15.90</td>
</tr>
<tr>
<td style="text-align: left;">w/ best-of-32 Judge (cosine)</td>
<td style="text-align: center;">90.87</td>
<td style="text-align: center;">14.47</td>
</tr>
<tr>
<td style="text-align: left;">w/ best-of-32 Judge (self-eval)</td>
<td style="text-align: center;">93.11</td>
<td style="text-align: center;">17.18</td>
</tr>
</tbody>
</table>
<p>on AlpacaEval by ranking tested models of the leaderboard using the judge models and enhancing the models with best-of- $N$ sampling.
Jugde models correlate well with GPT-4 in system-level ranking. On AlpacaEval v1, we calculate the system-level correlation between judge models and GPT-4. On the 95 evaluated models from the leaderboard, we use judge models to rate the model responses and obtain the average score on the test set. We use the scores measured by judge models to obtain the system-level ranking of the 95 models and then measure the correlation with GPT-4's ranking. As shown in Table 4, of the average result of the five judge models, both SELF-J and Judge (self-eval) show very high correlation with GPT-4. However, Judge (cosine) is significantly worse than them. Cosine similarity cannot truly understand semantic differences; it is merely a simple measure of similarity between embedded vectors. The judge models trained for different instruction-tuned models do not significantly differ with SELF-J or Judge (self-eval). In Table 7, we present all results including the scores measured by the judge model and GPT-4 and the corresponding ranking. Fig. 16 plots the linear fit for win rates by GPT-4 versus SELF-J scores.
Judge models serve as good reward models. Additionally, we further use judge models as reward models to enhance the performance of WizardLM-13b with best-of-32 sampling. On the test set of AlpacaEval, for each test prompt, we sample 32 responses with WizardLM-13b, each scored by the judge model, with the highest-scoring response selected. Results in Table 5 indicate that all versions of the judge model improve performance. We find that the judge model (self-eval) surpasses SELF-J. To explain this, in best-</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>3 However, other methods can also be applied to integrate these two scores such as multiplying the two values.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
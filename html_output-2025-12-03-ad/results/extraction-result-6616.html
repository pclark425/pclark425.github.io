<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6616 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6616</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6616</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-016368185723d0ec99aafa4b5927300590d0647f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/016368185723d0ec99aafa4b5927300590d0647f" target="_blank">Entities as Experts: Sparse Memory Access with Entity Supervision</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> A new model, Entities as Experts (EaE), that can access distinct memories of the entities mentioned in a piece of text that is more modular and interpretable than the Transformer architecture on which it is based is introduced.</p>
                <p><strong>Paper Abstract:</strong> We focus on the problem of capturing declarative knowledge in the learned parameters of a language model. We introduce a new model, Entities as Experts (EaE), that can access distinct memories of the entities mentioned in a piece of text. Unlike previous efforts to integrate entity knowledge into sequence models, EaE's entity representations are learned directly from text. These representations capture sufficient knowledge to answer TriviaQA questions such as "Which Dr. Who villain has been played by Roger Delgado, Anthony Ainley, Eric Roberts?". EaE outperforms a Transformer model with $30\times$ the parameters on this task. According to the Lama knowledge probes, EaE also contains more factual knowledge than a similar sized Bert. We show that associating parameters with specific entities means that EaE only needs to access a fraction of its parameters at inference time, and we show that the correct identification, and representation, of entities is essential to EaE's performance. We also argue that the discrete and independent entity representations in EaE make it more modular and interpretable than the Transformer architecture on which it is based.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6616.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6616.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Entities as Experts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer-based language model that learns a large, explicit entity memory (one embedding per entity) and sparsely retrieves and reintegrates entity representations for detected mention spans to improve masked-language and QA performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>EAE (Entities as Experts)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Adds an Entity Memory layer between Transformer blocks that constructs a pseudo-entity vector from a mention span, retrieves the top-K nearest learned entity embeddings (by dot-product) from an entity embedding matrix, forms a weighted sum to produce an entity representation, projects that representation into the sequence and continues processing with later Transformer layers. Entity retrieval during pre-training is supervised with entity linking; mention detection is learned.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈367M parameters (base EAE); EAE-emb-512 variant ≈623M</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Learned large vector entity memory (learned entity embedding matrix indexed by entity id)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Fixed-size learned entity embeddings (vectors) for a pre-defined vocabulary of entities (1M in pretraining experiments; some comparisons used 200k subset)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Read: dot-product similarity between pseudo-entity vector (from mention span) and entity embeddings; select top-K (K=100 at inference by default) and compute softmax-weighted sum of those embeddings. Access is gated by detected mention spans (sparse access). Write/update: entity embeddings are learned/updated during training by gradient descent; inference uses retrieval-only (no online writes). Entity-linking supervision encourages selecting the correct entity during training.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Masked Wikipedia hyperlink prediction; LAMA knowledge probes (ConceptNet, RE, SQuAD, T-REx); Open-domain QA (TriviaQA, WebQuestions); TACRED relation extraction; Ultra-fine entity typing</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Knowledge probing (cloze), open-domain question answering, relation extraction, entity typing</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Masked hyperlink prediction: Entity prediction accuracy 61.8%, token prediction accuracy 56.9%, perplexity 11.0 (Table 1). LAMA (avg): 20.0% (subtask T-REx 37.4%, SQuAD 22.4%). TriviaQA (closed-book, dev): Exact Match 43.2%; TriviaQA Wiki-test (closed-book entity prediction): 53.4% Exact Match; WebQuestions: 39.0% Exact Match. TACRED (revisited splits): EAE outperforms KnowBERT on revised/weighted splits (see text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>No-EAE (same total params but no entity memory layer): Masked hyperlink prediction: Entity prediction acc 58.6%, token prediction acc 45.0%, PPL 19.3 (Table 1). LAMA (avg): 17.4%. TriviaQA (dev) Exact Match 37.7%; WebQuestions 33.4% (Table 3). EAE-unsup (entity memory layer without entity-link supervision) performs worse than EAE: token acc 46.9%, PPL 16.9 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (entity prediction / token prediction), Perplexity (PPL), Exact Match (EM) for QA, Micro-F1 for typing / relation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Sparsity: only mention spans access memory, so only a fraction of parameters are touched at inference (potential compute savings); naive implementation does not yet realize runtime gains, requiring fast MIPS / ANN methods for sub-linear retrieval. Increasing entity embedding dimensionality (to 512) increases total model size but not proportion of parameters accessed at inference. EAE shows higher token prediction accuracy but can be overconfident when wrong (marginally worse perplexity vs larger baseline). Memory requires a pre-fixed entity vocabulary (cannot handle unseen entities); large entity embedding matrix is memory-intensive. Supervision (entity linking) is important: unsupervised memory (EAE-unsup) degrades performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performs poorly on non-entity cloze tasks (ConceptNet) and date answers; struggles when question entities are linked incorrectly—linking errors significantly reduce QA performance; entities seen very infrequently in training (<100 mentions) have much lower QA accuracy; cannot handle unseen entities (fixed vocabulary); 88% of mentions lack hyperlinks and thus memory access is unsupervised for them during pretraining; naive top-K retrieval scales poorly without approximate nearest-neighbor search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, Tom Kwiatkowski. Entities as Experts: Sparse Memory Access with Entity Supervision. Google Research.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Entities as Experts: Sparse Memory Access with Entity Supervision', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6616.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6616.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EAE-unsup</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Entities as Experts (unsupervised entity-linking ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation of EAE where the entity memory layer exists but is not supervised by entity-linking during training; memory accesses are therefore not guided to specific entity ids.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>EAE-unsup</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same architecture as EAE with an entity embedding matrix and retrieval mechanism, but entity memory access is not supervised with entity-linking loss during pretraining (the ELLoss is disabled).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈366M parameters (as reported)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Learned entity embedding matrix (but access unsupervised during training)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Learned entity embeddings (vectors) that are not explicitly tied to training entity-link targets</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Same top-K dot-product retrieval as EAE, but without supervision nudging the pseudo-entity vector to match the correct entity embedding during training; at inference full attention used for memory access (per paper's ablation description).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Masked hyperlink prediction; LAMA; other probing tasks (evaluated as ablation in same suites as EAE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Knowledge probing, cloze tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Masked hyperlink/token prediction: token accuracy 46.9%, perplexity 16.9 (Table 1). LAMA average: 18.0% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Compared to EAE (with supervised memory): EAE token acc 56.9% and PPL 11.0 — EAE-unsup is substantially worse, indicating supervision is important.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Token prediction accuracy, Perplexity, LAMA accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Without entity-link supervision, the memory layer is not effectively allocated and performance degrades; unsupervised memory access leads to worse token prediction and perplexity. Paper emphasizes need for supervision to teach the model how to allocate entity-memory parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Significantly worse performance than supervised EAE on token prediction and knowledge probes; indicates unsupervised memory is insufficient for these tasks in current setup.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, Tom Kwiatkowski. Entities as Experts: Sparse Memory Access with Entity Supervision. Google Research.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Entities as Experts: Sparse Memory Access with Entity Supervision', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6616.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6616.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>No-EAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>No-Entities-as-Experts (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline architecture with the same number of parameters as EAE but without the entity memory layer; entity embeddings are present only for the EntityPred head and not used inside the Transformer stack.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>No-EAE</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Transformer encoder (12 layers) with TokenPred and EntityPred heads; an entity embedding matrix exists but is NOT integrated into intermediate Transformer layers, so the model cannot re-integrate entity knowledge into token predictions via a memory layer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈366M parameters (as reported)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>None (entity embeddings only used at output head; no integrated memory access inside model)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>N/A for integrated memory (entity embeddings only used by prediction head, not reintegrated into representations)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>N/A (no intermediate retrieval and reintegration mechanism)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Masked hyperlink prediction; LAMA; TriviaQA; WebQuestions (used as comparative baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Knowledge probing, open-domain QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>N/A (this is the without-memory baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Masked hyperlink prediction: Entity acc 58.6%, token acc 45.0%, PPL 19.3 (Table 1). LAMA average 17.4% (Table 2). TriviaQA dev EM 37.7%; WebQuestions 33.4% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy, Perplexity, Exact Match</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>No internal entity memory means model must encode entity knowledge entirely in the dense Transformer parameters; lower token accuracy and worse QA performance compared to EAE despite similar total parameter count; demonstrates benefit of allocating parameters as entity-specific memory.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Cannot model interactions between entity embeddings and token representations within encoder layers, reducing efficacy for masked-language and QA tasks that benefit from explicit entity retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, Tom Kwiatkowski. Entities as Experts: Sparse Memory Access with Entity Supervision. Google Research.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Entities as Experts: Sparse Memory Access with Entity Supervision', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6616.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6616.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KnowBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KnowBERT (Knowledge enhanced contextual word representations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-based model that integrates precomputed entity representations and WordNet synsets via a Knowledge Attention and Recontextualization layer to inject external knowledge into contextual representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge enhanced contextual word representations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KnowBERT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Incorporates pre-existing (externally trained) entity embeddings and a Knowledge Attention & Recontextualization component that pools mention representations, attends to candidate entities, and re-injects knowledge into contextual representations; relies on an externally-provided candidate detector (alias table) rather than learning mention detection end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>BERT-base backbone (≈110M) in the described variant</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>External precomputed entity representations (fixed during some variants) integrated via a KB-attention module</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Pretrained entity/contextual embeddings (e.g., Deep-Ed) and WordNet synset embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Candidate generation via alias table; Knowledge Attention and Recontextualization over pooled mention representations to attend to entity candidates and re-contextualize token embeddings; entity embeddings typically frozen in some setups or fine-tuned in others.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Compared on TACRED relation extraction (reported via external scores) and discussed relative to knowledge probing tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Relation extraction, knowledge-enhanced language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1 for TACRED (reported in cited evaluations); knowledge probe accuracies in original KnowBERT work (not enumerated here)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Uses precomputed entity representations and an external alias table / candidate detector, making the pipeline dependent on external resources; does not train mention detection end-to-end; integration method differs (no learned large separate entity-memory inside Transformer as in EAE).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Relies on fixed candidate detector and precomputed embeddings; less end-to-end (can't learn mention detection and memory allocation from scratch). In this paper, KnowBERT slightly underperforms EAE on revised TACRED splits (text reports EAE outperforms KnowBERT on revised/weighted splits but slightly underperforms on original split).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2019. Knowledge enhanced contextual word representations. arXiv preprint.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Entities as Experts: Sparse Memory Access with Entity Supervision', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6616.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6616.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ERNIE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ERNIE (Enhanced Representation through kNowledge Integration)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-based model that augments token representations with entity inputs (pre-supplied entity list) and performs multi-head attention over those entities before integrating them into token embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ernie: Enhanced language representation with informative entities</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ERNIE</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Takes as additional input the list of entities in the sentence and performs multi-head attention over those entity vectors which are then aggregated with token representations; ERNIE also masks entities during pretraining and trains the model to predict them. Entity representations are precomputed and kept fixed during some variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>BERT-base backbone (≈110M) in the described variant</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>External precomputed entity embeddings provided as inputs</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Pretrained entity embeddings (from external training on entity contexts)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Multi-head attention over provided entity vectors (entities are given by preprocessing / linking); no learned sparse top-K retrieval inside Transformer unlike EAE</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Knowledge-enhanced pretraining tasks (entity masking) and evaluated on similar knowledge/NER/QA tasks in original ERNIE work (paper references ERNIE as related work here)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Knowledge-augmented language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Relies on pre-existing entity links provided as input (not learned mention detection); uses precomputed entity representations (may be frozen), which reduces end-to-end learning flexibility compared to methods that learn entity embeddings jointly.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Depends on external linking preprocessing; fixed representations may limit adaptability; not trained end-to-end to learn memory allocation from text as in EAE.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. Ernie: Enhanced language representation with informative entities. arXiv preprint.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Entities as Experts: Sparse Memory Access with Entity Supervision', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6616.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6616.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemoryNetworks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of neural architectures that store a set of memory slots and perform multiple rounds of attention-based reading over them to answer queries; proposed as general memory-augmented neural models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memory networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Memory Networks (Weston et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>End-to-end memory architectures that maintain an explicit memory module (slots of vectors) and use attention-based mechanisms to read (and in some variants write) memories to perform tasks such as question answering; inspired EAE's conceptual framing of an entity memory where each slot corresponds to an entity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Explicit memory slots (learned vector memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Memory slots holding vector representations (facts, contexts, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Attention-based reading (soft-attention over memory slots), sometimes multiple hops; writes via supervised update / learned mechanisms depending on variant</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General QA/reading tasks in original Memory Networks work (cited as inspiration here)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Question answering, reasoning with memories</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Unconstrained memory networks can be computationally expensive when the memory is large; scaling requires indexing or sparse access methods (motivates supervised access in EAE).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Scalability issues when memory is large; naive access is costly; requires mechanisms to ensure efficient retrieval for large memory sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Jason Weston, Sumit Chopra, and Antoine Bordes. 2014. Memory networks. arXiv preprint.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Entities as Experts: Sparse Memory Access with Entity Supervision', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6616.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6616.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProductKeyMem</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Product-Key Memory (Large memory layers with product keys)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method to scale very large memory layers by using product quantization / product-key indexing to efficiently index and retrieve values from a very large memory store.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large memory layers with product keys</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Product-Key Memory (Lample et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Efficient large memory layer design that uses product-key (compositional) indexing to enable sub-linear-time retrieval from very large key-value stores; proposed as a scalable alternative to naive memory networks for large external memory stores.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Large external key-value memory with product-key indexing</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Keys and values stored in compositional/product-key format enabling compact indexing of large stores</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Approximate nearest-neighbor retrieval via product-key lookup; sub-linear retrieval methods to scale large memories</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Memory-augmented modeling tasks (cited as scalable memory technique; not directly evaluated in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Scalable memory retrieval / conditional computation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Enables scaling of memory size with feasible retrieval cost; implementation complexity and engineering required for integration with Transformer stacks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not evaluated in this paper; listed as a possible scalable retrieval approach to realize EAE's sparse top-K routing efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Guillaume Lample, Alexandre Sablayrolles, Marc'Aurelio Ranzato, Ludovic Denoyer, and Hervé Jégou. 2019. Large memory layers with product keys. NeurIPS.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Entities as Experts: Sparse Memory Access with Entity Supervision', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6616.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6616.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SparseMoE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparsely-Gated Mixture-of-Experts (MoE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conditional computation technique that routes inputs to a small subset of expert sub-networks (experts) so the model can scale capacity while keeping per-example compute small.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Sparsely-Gated Mixture-of-Experts (Shazeer et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Conditional computation approach that uses a learned router to select a sparse subset of expert parameter blocks per input; conceptually related to EAE's idea of associating parameters with entities so only a fraction are accessed per example.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Expert parameter shards (conditional parameter bank, conceptually analogous to a memory of experts)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Parameters of expert networks (not an explicit vector store of facts but conditional parameter sets)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Learned routing / gating to select top-K experts for each input; sparsely-gated activation</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General large-scale modeling tasks (cited as conditional computation inspiration for EAE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Conditional computation, model scaling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Allows large capacity with limited per-example compute; requires effective routing to avoid load imbalance and can introduce implementation complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Routing and load balancing challenges; engineering complexity for efficient sparse execution — cited as related conceptual prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Entities as Experts: Sparse Memory Access with Entity Supervision', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Memory networks <em>(Rating: 2)</em></li>
                <li>Large memory layers with product keys <em>(Rating: 2)</em></li>
                <li>Outrageously large neural networks: The sparsely-gated mixture-of-experts layer <em>(Rating: 2)</em></li>
                <li>Knowledge enhanced contextual word representations <em>(Rating: 2)</em></li>
                <li>Ernie: Enhanced language representation with informative entities <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6616",
    "paper_id": "paper-016368185723d0ec99aafa4b5927300590d0647f",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "EAE",
            "name_full": "Entities as Experts",
            "brief_description": "A Transformer-based language model that learns a large, explicit entity memory (one embedding per entity) and sparsely retrieves and reintegrates entity representations for detected mention spans to improve masked-language and QA performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "EAE (Entities as Experts)",
            "agent_description": "Adds an Entity Memory layer between Transformer blocks that constructs a pseudo-entity vector from a mention span, retrieves the top-K nearest learned entity embeddings (by dot-product) from an entity embedding matrix, forms a weighted sum to produce an entity representation, projects that representation into the sequence and continues processing with later Transformer layers. Entity retrieval during pre-training is supervised with entity linking; mention detection is learned.",
            "model_size": "≈367M parameters (base EAE); EAE-emb-512 variant ≈623M",
            "memory_used": true,
            "memory_type": "Learned large vector entity memory (learned entity embedding matrix indexed by entity id)",
            "memory_representation": "Fixed-size learned entity embeddings (vectors) for a pre-defined vocabulary of entities (1M in pretraining experiments; some comparisons used 200k subset)",
            "memory_access_mechanism": "Read: dot-product similarity between pseudo-entity vector (from mention span) and entity embeddings; select top-K (K=100 at inference by default) and compute softmax-weighted sum of those embeddings. Access is gated by detected mention spans (sparse access). Write/update: entity embeddings are learned/updated during training by gradient descent; inference uses retrieval-only (no online writes). Entity-linking supervision encourages selecting the correct entity during training.",
            "task_name": "Masked Wikipedia hyperlink prediction; LAMA knowledge probes (ConceptNet, RE, SQuAD, T-REx); Open-domain QA (TriviaQA, WebQuestions); TACRED relation extraction; Ultra-fine entity typing",
            "task_category": "Knowledge probing (cloze), open-domain question answering, relation extraction, entity typing",
            "performance_with_memory": "Masked hyperlink prediction: Entity prediction accuracy 61.8%, token prediction accuracy 56.9%, perplexity 11.0 (Table 1). LAMA (avg): 20.0% (subtask T-REx 37.4%, SQuAD 22.4%). TriviaQA (closed-book, dev): Exact Match 43.2%; TriviaQA Wiki-test (closed-book entity prediction): 53.4% Exact Match; WebQuestions: 39.0% Exact Match. TACRED (revisited splits): EAE outperforms KnowBERT on revised/weighted splits (see text).",
            "performance_without_memory": "No-EAE (same total params but no entity memory layer): Masked hyperlink prediction: Entity prediction acc 58.6%, token prediction acc 45.0%, PPL 19.3 (Table 1). LAMA (avg): 17.4%. TriviaQA (dev) Exact Match 37.7%; WebQuestions 33.4% (Table 3). EAE-unsup (entity memory layer without entity-link supervision) performs worse than EAE: token acc 46.9%, PPL 16.9 (Table 1).",
            "has_comparative_results": true,
            "performance_metric": "Accuracy (entity prediction / token prediction), Perplexity (PPL), Exact Match (EM) for QA, Micro-F1 for typing / relation tasks",
            "tradeoffs_reported": "Sparsity: only mention spans access memory, so only a fraction of parameters are touched at inference (potential compute savings); naive implementation does not yet realize runtime gains, requiring fast MIPS / ANN methods for sub-linear retrieval. Increasing entity embedding dimensionality (to 512) increases total model size but not proportion of parameters accessed at inference. EAE shows higher token prediction accuracy but can be overconfident when wrong (marginally worse perplexity vs larger baseline). Memory requires a pre-fixed entity vocabulary (cannot handle unseen entities); large entity embedding matrix is memory-intensive. Supervision (entity linking) is important: unsupervised memory (EAE-unsup) degrades performance.",
            "limitations_or_failure_cases": "Performs poorly on non-entity cloze tasks (ConceptNet) and date answers; struggles when question entities are linked incorrectly—linking errors significantly reduce QA performance; entities seen very infrequently in training (&lt;100 mentions) have much lower QA accuracy; cannot handle unseen entities (fixed vocabulary); 88% of mentions lack hyperlinks and thus memory access is unsupervised for them during pretraining; naive top-K retrieval scales poorly without approximate nearest-neighbor search.",
            "citation": "Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, Tom Kwiatkowski. Entities as Experts: Sparse Memory Access with Entity Supervision. Google Research.",
            "uuid": "e6616.0",
            "source_info": {
                "paper_title": "Entities as Experts: Sparse Memory Access with Entity Supervision",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "EAE-unsup",
            "name_full": "Entities as Experts (unsupervised entity-linking ablation)",
            "brief_description": "An ablation of EAE where the entity memory layer exists but is not supervised by entity-linking during training; memory accesses are therefore not guided to specific entity ids.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "EAE-unsup",
            "agent_description": "Same architecture as EAE with an entity embedding matrix and retrieval mechanism, but entity memory access is not supervised with entity-linking loss during pretraining (the ELLoss is disabled).",
            "model_size": "≈366M parameters (as reported)",
            "memory_used": true,
            "memory_type": "Learned entity embedding matrix (but access unsupervised during training)",
            "memory_representation": "Learned entity embeddings (vectors) that are not explicitly tied to training entity-link targets",
            "memory_access_mechanism": "Same top-K dot-product retrieval as EAE, but without supervision nudging the pseudo-entity vector to match the correct entity embedding during training; at inference full attention used for memory access (per paper's ablation description).",
            "task_name": "Masked hyperlink prediction; LAMA; other probing tasks (evaluated as ablation in same suites as EAE)",
            "task_category": "Knowledge probing, cloze tasks",
            "performance_with_memory": "Masked hyperlink/token prediction: token accuracy 46.9%, perplexity 16.9 (Table 1). LAMA average: 18.0% (Table 2).",
            "performance_without_memory": "Compared to EAE (with supervised memory): EAE token acc 56.9% and PPL 11.0 — EAE-unsup is substantially worse, indicating supervision is important.",
            "has_comparative_results": true,
            "performance_metric": "Token prediction accuracy, Perplexity, LAMA accuracy",
            "tradeoffs_reported": "Without entity-link supervision, the memory layer is not effectively allocated and performance degrades; unsupervised memory access leads to worse token prediction and perplexity. Paper emphasizes need for supervision to teach the model how to allocate entity-memory parameters.",
            "limitations_or_failure_cases": "Significantly worse performance than supervised EAE on token prediction and knowledge probes; indicates unsupervised memory is insufficient for these tasks in current setup.",
            "citation": "Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, Tom Kwiatkowski. Entities as Experts: Sparse Memory Access with Entity Supervision. Google Research.",
            "uuid": "e6616.1",
            "source_info": {
                "paper_title": "Entities as Experts: Sparse Memory Access with Entity Supervision",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "No-EAE",
            "name_full": "No-Entities-as-Experts (ablation)",
            "brief_description": "A baseline architecture with the same number of parameters as EAE but without the entity memory layer; entity embeddings are present only for the EntityPred head and not used inside the Transformer stack.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "No-EAE",
            "agent_description": "Transformer encoder (12 layers) with TokenPred and EntityPred heads; an entity embedding matrix exists but is NOT integrated into intermediate Transformer layers, so the model cannot re-integrate entity knowledge into token predictions via a memory layer.",
            "model_size": "≈366M parameters (as reported)",
            "memory_used": false,
            "memory_type": "None (entity embeddings only used at output head; no integrated memory access inside model)",
            "memory_representation": "N/A for integrated memory (entity embeddings only used by prediction head, not reintegrated into representations)",
            "memory_access_mechanism": "N/A (no intermediate retrieval and reintegration mechanism)",
            "task_name": "Masked hyperlink prediction; LAMA; TriviaQA; WebQuestions (used as comparative baseline)",
            "task_category": "Knowledge probing, open-domain QA",
            "performance_with_memory": "N/A (this is the without-memory baseline)",
            "performance_without_memory": "Masked hyperlink prediction: Entity acc 58.6%, token acc 45.0%, PPL 19.3 (Table 1). LAMA average 17.4% (Table 2). TriviaQA dev EM 37.7%; WebQuestions 33.4% (Table 3).",
            "has_comparative_results": true,
            "performance_metric": "Accuracy, Perplexity, Exact Match",
            "tradeoffs_reported": "No internal entity memory means model must encode entity knowledge entirely in the dense Transformer parameters; lower token accuracy and worse QA performance compared to EAE despite similar total parameter count; demonstrates benefit of allocating parameters as entity-specific memory.",
            "limitations_or_failure_cases": "Cannot model interactions between entity embeddings and token representations within encoder layers, reducing efficacy for masked-language and QA tasks that benefit from explicit entity retrieval.",
            "citation": "Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, Tom Kwiatkowski. Entities as Experts: Sparse Memory Access with Entity Supervision. Google Research.",
            "uuid": "e6616.2",
            "source_info": {
                "paper_title": "Entities as Experts: Sparse Memory Access with Entity Supervision",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "KnowBERT",
            "name_full": "KnowBERT (Knowledge enhanced contextual word representations)",
            "brief_description": "A BERT-based model that integrates precomputed entity representations and WordNet synsets via a Knowledge Attention and Recontextualization layer to inject external knowledge into contextual representations.",
            "citation_title": "Knowledge enhanced contextual word representations",
            "mention_or_use": "mention",
            "agent_name": "KnowBERT",
            "agent_description": "Incorporates pre-existing (externally trained) entity embeddings and a Knowledge Attention & Recontextualization component that pools mention representations, attends to candidate entities, and re-injects knowledge into contextual representations; relies on an externally-provided candidate detector (alias table) rather than learning mention detection end-to-end.",
            "model_size": "BERT-base backbone (≈110M) in the described variant",
            "memory_used": true,
            "memory_type": "External precomputed entity representations (fixed during some variants) integrated via a KB-attention module",
            "memory_representation": "Pretrained entity/contextual embeddings (e.g., Deep-Ed) and WordNet synset embeddings",
            "memory_access_mechanism": "Candidate generation via alias table; Knowledge Attention and Recontextualization over pooled mention representations to attend to entity candidates and re-contextualize token embeddings; entity embeddings typically frozen in some setups or fine-tuned in others.",
            "task_name": "Compared on TACRED relation extraction (reported via external scores) and discussed relative to knowledge probing tasks",
            "task_category": "Relation extraction, knowledge-enhanced language modeling",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": true,
            "performance_metric": "F1 for TACRED (reported in cited evaluations); knowledge probe accuracies in original KnowBERT work (not enumerated here)",
            "tradeoffs_reported": "Uses precomputed entity representations and an external alias table / candidate detector, making the pipeline dependent on external resources; does not train mention detection end-to-end; integration method differs (no learned large separate entity-memory inside Transformer as in EAE).",
            "limitations_or_failure_cases": "Relies on fixed candidate detector and precomputed embeddings; less end-to-end (can't learn mention detection and memory allocation from scratch). In this paper, KnowBERT slightly underperforms EAE on revised TACRED splits (text reports EAE outperforms KnowBERT on revised/weighted splits but slightly underperforms on original split).",
            "citation": "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2019. Knowledge enhanced contextual word representations. arXiv preprint.",
            "uuid": "e6616.3",
            "source_info": {
                "paper_title": "Entities as Experts: Sparse Memory Access with Entity Supervision",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "ERNIE",
            "name_full": "ERNIE (Enhanced Representation through kNowledge Integration)",
            "brief_description": "A BERT-based model that augments token representations with entity inputs (pre-supplied entity list) and performs multi-head attention over those entities before integrating them into token embeddings.",
            "citation_title": "Ernie: Enhanced language representation with informative entities",
            "mention_or_use": "mention",
            "agent_name": "ERNIE",
            "agent_description": "Takes as additional input the list of entities in the sentence and performs multi-head attention over those entity vectors which are then aggregated with token representations; ERNIE also masks entities during pretraining and trains the model to predict them. Entity representations are precomputed and kept fixed during some variants.",
            "model_size": "BERT-base backbone (≈110M) in the described variant",
            "memory_used": true,
            "memory_type": "External precomputed entity embeddings provided as inputs",
            "memory_representation": "Pretrained entity embeddings (from external training on entity contexts)",
            "memory_access_mechanism": "Multi-head attention over provided entity vectors (entities are given by preprocessing / linking); no learned sparse top-K retrieval inside Transformer unlike EAE",
            "task_name": "Knowledge-enhanced pretraining tasks (entity masking) and evaluated on similar knowledge/NER/QA tasks in original ERNIE work (paper references ERNIE as related work here)",
            "task_category": "Knowledge-augmented language modeling",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Relies on pre-existing entity links provided as input (not learned mention detection); uses precomputed entity representations (may be frozen), which reduces end-to-end learning flexibility compared to methods that learn entity embeddings jointly.",
            "limitations_or_failure_cases": "Depends on external linking preprocessing; fixed representations may limit adaptability; not trained end-to-end to learn memory allocation from text as in EAE.",
            "citation": "Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. Ernie: Enhanced language representation with informative entities. arXiv preprint.",
            "uuid": "e6616.4",
            "source_info": {
                "paper_title": "Entities as Experts: Sparse Memory Access with Entity Supervision",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "MemoryNetworks",
            "name_full": "Memory Networks",
            "brief_description": "A family of neural architectures that store a set of memory slots and perform multiple rounds of attention-based reading over them to answer queries; proposed as general memory-augmented neural models.",
            "citation_title": "Memory networks",
            "mention_or_use": "mention",
            "agent_name": "Memory Networks (Weston et al.)",
            "agent_description": "End-to-end memory architectures that maintain an explicit memory module (slots of vectors) and use attention-based mechanisms to read (and in some variants write) memories to perform tasks such as question answering; inspired EAE's conceptual framing of an entity memory where each slot corresponds to an entity.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "Explicit memory slots (learned vector memory)",
            "memory_representation": "Memory slots holding vector representations (facts, contexts, etc.)",
            "memory_access_mechanism": "Attention-based reading (soft-attention over memory slots), sometimes multiple hops; writes via supervised update / learned mechanisms depending on variant",
            "task_name": "General QA/reading tasks in original Memory Networks work (cited as inspiration here)",
            "task_category": "Question answering, reasoning with memories",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Unconstrained memory networks can be computationally expensive when the memory is large; scaling requires indexing or sparse access methods (motivates supervised access in EAE).",
            "limitations_or_failure_cases": "Scalability issues when memory is large; naive access is costly; requires mechanisms to ensure efficient retrieval for large memory sizes.",
            "citation": "Jason Weston, Sumit Chopra, and Antoine Bordes. 2014. Memory networks. arXiv preprint.",
            "uuid": "e6616.5",
            "source_info": {
                "paper_title": "Entities as Experts: Sparse Memory Access with Entity Supervision",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "ProductKeyMem",
            "name_full": "Product-Key Memory (Large memory layers with product keys)",
            "brief_description": "A method to scale very large memory layers by using product quantization / product-key indexing to efficiently index and retrieve values from a very large memory store.",
            "citation_title": "Large memory layers with product keys",
            "mention_or_use": "mention",
            "agent_name": "Product-Key Memory (Lample et al.)",
            "agent_description": "Efficient large memory layer design that uses product-key (compositional) indexing to enable sub-linear-time retrieval from very large key-value stores; proposed as a scalable alternative to naive memory networks for large external memory stores.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "Large external key-value memory with product-key indexing",
            "memory_representation": "Keys and values stored in compositional/product-key format enabling compact indexing of large stores",
            "memory_access_mechanism": "Approximate nearest-neighbor retrieval via product-key lookup; sub-linear retrieval methods to scale large memories",
            "task_name": "Memory-augmented modeling tasks (cited as scalable memory technique; not directly evaluated in this paper)",
            "task_category": "Scalable memory retrieval / conditional computation",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Enables scaling of memory size with feasible retrieval cost; implementation complexity and engineering required for integration with Transformer stacks.",
            "limitations_or_failure_cases": "Not evaluated in this paper; listed as a possible scalable retrieval approach to realize EAE's sparse top-K routing efficiently.",
            "citation": "Guillaume Lample, Alexandre Sablayrolles, Marc'Aurelio Ranzato, Ludovic Denoyer, and Hervé Jégou. 2019. Large memory layers with product keys. NeurIPS.",
            "uuid": "e6616.6",
            "source_info": {
                "paper_title": "Entities as Experts: Sparse Memory Access with Entity Supervision",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "SparseMoE",
            "name_full": "Sparsely-Gated Mixture-of-Experts (MoE)",
            "brief_description": "A conditional computation technique that routes inputs to a small subset of expert sub-networks (experts) so the model can scale capacity while keeping per-example compute small.",
            "citation_title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
            "mention_or_use": "mention",
            "agent_name": "Sparsely-Gated Mixture-of-Experts (Shazeer et al.)",
            "agent_description": "Conditional computation approach that uses a learned router to select a sparse subset of expert parameter blocks per input; conceptually related to EAE's idea of associating parameters with entities so only a fraction are accessed per example.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "Expert parameter shards (conditional parameter bank, conceptually analogous to a memory of experts)",
            "memory_representation": "Parameters of expert networks (not an explicit vector store of facts but conditional parameter sets)",
            "memory_access_mechanism": "Learned routing / gating to select top-K experts for each input; sparsely-gated activation",
            "task_name": "General large-scale modeling tasks (cited as conditional computation inspiration for EAE)",
            "task_category": "Conditional computation, model scaling",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Allows large capacity with limited per-example compute; requires effective routing to avoid load imbalance and can introduce implementation complexity.",
            "limitations_or_failure_cases": "Routing and load balancing challenges; engineering complexity for efficient sparse execution — cited as related conceptual prior work.",
            "citation": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint.",
            "uuid": "e6616.7",
            "source_info": {
                "paper_title": "Entities as Experts: Sparse Memory Access with Entity Supervision",
                "publication_date_yy_mm": "2020-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Memory networks",
            "rating": 2
        },
        {
            "paper_title": "Large memory layers with product keys",
            "rating": 2
        },
        {
            "paper_title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
            "rating": 2
        },
        {
            "paper_title": "Knowledge enhanced contextual word representations",
            "rating": 2
        },
        {
            "paper_title": "Ernie: Enhanced language representation with informative entities",
            "rating": 2
        }
    ],
    "cost": 0.02053275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Entities as Experts: Sparse Memory Access with Entity Supervision</h1>
<p>Thibault Févry * Livio Baldini Soares<br>thibaultfevry@gmail.com liviobs@google.com<br>Nicholas FitzGerald<br>nfitz@google.com<br>Eunsol Choi ${ }^{\dagger}$<br>The University of Austin at Texas<br>eunsol@cs.utexas.edu<br>Tom Kwiatkowski<br>tomkwiat@google.com</p>
<h2>Google Research</h2>
<h4>Abstract</h4>
<p>We focus on the problem of capturing declarative knowledge about entities in the learned parameters of a language model. We introduce a new model-Entities as Experts (EAE)that can access distinct memories of the entities mentioned in a piece of text. Unlike previous efforts to integrate entity knowledge into sequence models, EAE's entity representations are learned directly from text. We show that EAE's learned representations capture sufficient knowledge to answer TriviaQA questions such as "Which Dr. Who villain has been played by Roger Delgado, Anthony Ainley, Eric Roberts?", outperforming an encodergenerator Transformer model with $10 \times$ the parameters. According to the LAMA knowledge probes, EAE contains more factual knowledge than a similarly sized BERT, as well as previous approaches that integrate external sources of entity knowledge. Because EAE associates parameters with specific entities, it only needs to access a fraction of its parameters at inference time, and we show that the correct identification and representation of entities is essential to EAE's performance.</p>
<h2>1 Introduction</h2>
<p>Neural network sequence models, pre-trained as language models, have recently revolutionized text understanding (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2018), and recent work has suggested that they could take the place of curated knowledge bases or textual corpora for tasks such as question answering (Petroni et al., 2019; Roberts et al., 2020).</p>
<p>In this paper, we focus on developing neural sequence models that capture the knowledge required to answer questions about real world entities. To this end, we introduce a new model architecture</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Our model with an entity memory, applied to the open domain QA task. The red arrows shows the integration of the entity and token representations.
that can access distinct and independent representations of the entities mentioned in text. Unlike other efforts to inject entity specific knowledge into sequence models (Peters et al., 2019; Zhang et al., 2019; Poerner et al., 2019) our model learns entity representations from text along with all the other model parameters. We call our model Entities as Experts (EAE), since it divides the parameter space according to entity identity. This name also reflects EAE's similarities with the Massive Mixture of Experts (Shazeer et al., 2017), as well as other work that integrates learned memory stores into sequence models (Weston et al., 2014; Lample et al., 2019).</p>
<p>To understand the motivation for distinct and independent entity representations, consider Figure 1. A traditional Transformer (Vaswani et al., 2017) needs to build an internal representation of Charles Darwin from the words "Charles" and "Darwin", both of which can also refer to different entities such as the Charles River, or Darwin City. Conversely, EAE can access a dedicated representation of "Charles Darwin", which is a memory of all of the contexts in which this entity has previously been mentioned. This memory can also be accessed for other mentions of Darwin, such as "Charles</p>
<p>Robert Darwin" or "the father of natural selection". Retrieving and re-integrating this memory makes it easier for EA E to find the answer.</p>
<p>We train EAE to predict masked-out spans in English Wikipedia text (Devlin et al., 2018); to only access memories for entity mention spans; and to access the correct memory for each entity mention. Mention span supervision comes from an existing mention detector, and entity identity supervision comes from Wikipedia hyperlinks. By associating memories with specific entities, EAE can learn to access them sparsely. The memory is only accessed for spans that mention entities, and only the mentioned memories need to be retrieved.</p>
<p>We evaluate EAE's ability to capture declarative knowledge using the LAMA knowledge probes introduced by Petroni et al. (2019), as well as the open-domain variants of the TriviaQA and WebQuestions question answering tasks (Joshi et al., 2017; Berant et al., 2013). On both tasks, EAE outperforms related approaches with many more parameters. An in-depth analysis of EAE's predictions on TriviaQA shows that the correct identification and reintegration of entity representations is essential for EAE's performance.</p>
<p>We further demonstrate that EAE's learned entity representations are better than the pre-trained embeddings used by Zhang et al. (2019); Peters et al. (2019) at knowledge probing tasks and the TACRED relation extraction task (Zhang et al., 2017; Alt et al., 2020). We show that training EAE to focus on entities is better than imbuing a similarsized network with an unconstrained memory store, and explain how EAE can outperform much larger sequence models while only accessing a small proportion of its parameters at inference time.</p>
<h2>2 Approach</h2>
<p>Let $\mathcal{E}=\left{e_{1} \ldots e_{N}\right}$ be a predefined set of entities, and let $\mathcal{V}=\left{\left[\operatorname{MASK}\right], w_{1} \ldots w_{M}\right}$ be a vocabulary of tokens. A context $\mathbf{x}=\left[x_{0} \ldots x_{L}\right]$ is a sequence of tokens $x_{i} \in \mathcal{V}$. Each context comes with the list of the mentions it contains, $\mathbf{m}=\left[m_{0} \ldots m_{M}\right]$, where each mention $m_{i}=\left(e_{m_{i}}, s_{m_{i}}, t_{m_{i}}\right)$ is defined by its linked entity $e_{m_{i}}$, start token index $s_{m_{i}}$ and end token index $t_{m_{i}}$. Entity mentions might not be linked to a specific entity in $\mathcal{E}$, thus $e_{m_{i}} \in \mathcal{E} \cup e_{\varnothing}$, where $e_{\varnothing}$ refers to the null entity.</p>
<h3>2.1 Model Architecture</h3>
<p>The basic model architecture follows the Transformer (Vaswani et al., 2017), interleaved with our entity memory layer. Our model has two embedding matrices - token and entity embeddings. Figure 2 illustrates our model. Our model is:</p>
<p>$$
\begin{aligned}
&amp; \mathbf{X}^{0}=\text { TokenEmbed }(\mathbf{x}) \
&amp; \mathbf{X}^{1}=\text { Transformer }\left(\mathbf{X}^{0}, \text { num_layers }=l_{0}\right) \
&amp; \mathbf{X}^{2}=\text { EntityMemory }\left(\mathbf{X}^{1}\right) \
&amp; \mathbf{X}^{3}=\text { LayerNorm }\left(\mathbf{X}^{2}+\mathbf{X}^{1}\right) \
&amp; \mathbf{X}^{4}=\text { Transformer }\left(\mathbf{X}^{3}, \text { num_layers }=l_{1}\right) \
&amp; \mathbf{X}^{5}=\text { TaskSpecificHeads }\left(\mathbf{X}^{4}\right)
\end{aligned}
$$</p>
<p>The entity memory layer constructs an entity embedding $E_{m_{i}}$ for each mention $m_{i}$. The output of the entity memory layer and preceding transformer layer are summed, normalized, then processed by additional transformer layers. Throughout this work we use $l_{0}=4$ and $l_{1}=8$.</p>
<p>Entity Memory Layer Let $\mathbf{E}$ be a matrix of learned entity embeddings of shape $\left(N, d_{\text {ent }}\right)$. $\operatorname{EntEmbed}\left(e_{i}\right)$ maps an entity $e_{i}$ to its row in $\mathbf{E}$. The entity memory layer takes the output sequence from the preceding transformer layer ( $\mathbf{X}^{l}$ ) and outputs a new sequence ( $\mathbf{X}^{l+1}$ ), sparsely populated with entity representations. For each entity mention $m_{i}$, the output sequence has an entity representation, a projection of the weighted sum of entity embeddings in $\mathbf{E}$, at position $s_{m_{i}}$.</p>
<p>$$
x_{i}^{l+1}=\mathbf{W}<em m__k="m_{k">{b} E</em>
$$}} \quad \text { if } \quad i=s_{m_{k}</p>
<p>where $\mathbf{W}<em m__k="m_{k">{b}$ maps the entity representation $E</em>$.}}$ to the dimension of $x_{i}^{l</p>
<p>We now describe how to generate $E_{m_{i}}$ for each mention $m_{i}$. First, we generate a pseudo entity embedding $h_{m_{i}}$ based on the mention's span representation $\left[x_{s_{m_{i}}}^{l} | x_{t_{m_{i}}}^{l}\right]$, a concatenation of its start and tail representations.</p>
<p>$$
h_{m_{i}}=\mathbf{W}<em s__m__i="s_{m_{i">{\mathbf{f}}\left[x</em>\right]
$$}}}^{l} | x_{t_{m_{i}}}^{l</p>
<p>where $\mathbf{W}<em _ent="{ent" _text="\text">{\mathbf{f}}$ is of shape $\left(d</em>$.}}, 2 \cdot d_{\text {emb }}\right)$, where $d_{\text {emb }}$ is the dimension of $\mathbf{X}^{\mathbf{1}</p>
<p>We find the $k$ nearest entity embeddings of $h_{m_{i}}$ from $\mathbf{E}$ by computing the dot product, and $E_{m_{i}}$ is a weighted sum of them. More formally:</p>
<p>$$
\begin{aligned}
E_{m_{i}} &amp; =\sum_{e_{j} \in \operatorname{topK}\left(\mathcal{E}, h_{m_{i}}, k\right)} \alpha_{j} \cdot\left(\operatorname{EntEmbed}\left(e_{j}\right)\right) \
\alpha_{j} &amp; =\frac{\exp \left(\operatorname{EntEmbed}\left(e_{j}\right) \cdot h_{m_{i}}\right)}{\sum_{e \in \operatorname{topK}\left(\mathcal{E}, h_{m_{i}}, k\right)} \exp \left(\operatorname{EntEmbed}(e) \cdot h_{m_{i}}\right)}
\end{aligned}
$$</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The Entities as Experts model: the initial transformer layer output is used (i) to predict mention boundaries, (ii) to retrieve entity embeddings from entity memory, and (iii) to construct input to the next transformer layer, augmented with the retrieved entity embeddings of (ii). The final transformer block output is connected to task specific heads: token prediction and entity prediction. The entity retrieval after the first transformer layer (ii) is also supervised with an entity linking objective during pre-training.</p>
<p>Where topK(£, h_{m_{i}}, k) returns the k entities that yield the highest score EntEmbed(e_{j}) · h_{m_{i}}. We use k = N to train and use k = 100 at inference (see Section 4.1 and 6.3).</p>
<p>The entity memory layer can be applied to any sequence output without loss of generality. We apply it to the output of the first Transformer.</p>
<p><strong>Task-Specific Heads</strong> The final transformer layer can be connected to multiple task specific heads. In our experiments, we introduce two heads: TokenPred and EntityPred.</p>
<p>The TokenPred head predicts masked tokens for a cloze task. Each masked token's final representation x^{k}_{i} is fed to an output softmax over the token vocabulary, as in BERT.</p>
<p>The EntityPred head predicts entity ids for each entity mention span (i.e., entity linking). We build the pseudo entity embedding (h_{m_{i}}) from the last sequence output (X^{4}). Then, the model predicts the entity whose embedding in E is the closest to the pseudo entity embedding.</p>
<p><strong>Inference-time Mention Detection</strong> We introduce a mention detection layer to avoid dependence at inference on an external mention detector. The mention detection layer applies a BIO^{1} classifier to the first transformer block's output. We decode the entire BIO sequence, ensuring that inconsistent sequences are disallowed. We use inferred mention spans at inference for all our experiments.</p>
<h3>2.2 Training</h3>
<h4>2.2.1 Data and Preprocessing</h4>
<p>We assume access to a corpus D = {(x_{i}, m_{i})}, where all entity mentions are detected but not necessarily all linked to entities. We use English Wikipedia as our corpus, with a vocabulary of 1m entities. Entity links come from hyperlinks, leading to 32m 128 byte contexts containing 17m entity links. Non-overlapping entity mention boundaries come from hyperlinks and the Google Cloud Natural Language API^{2} leading to 140m mentions.</p>
<p>We remove 20% of randomly chosen entity mentions (all tokens in the mention boundaries are replaced with [MASK]) to support a masked language modeling objective. See Appendix B for full details of our pre-processing and pre-training hyperparameters.</p>
<h4>2.2.2 Learning Objective</h4>
<p>The pre-training objective is the sum of (1) a mention boundary detection loss, (2) an entity linking loss, and (3) a masked language modeling loss.</p>
<p><strong>Mention Detection</strong> The BIO classification of tokens is supervised with a cross-entropy loss over the labels. Assuming the mention boundaries are complete, we apply this supervision to all tokens.</p>
<p><strong>Entity Linking</strong> We use the hyperlinked entities e_{m_{i}} to supervise entity memory assess. For each hyperlinked mention m_{i} = (e_{m_{i}}, s_{m_{i}}, t_{m_{i}}), where</p>
<p>In a BIO encoding, each token is classified as being the beginning, Inside, or Outside of a mention.</p>
<p><sup>1</sup>In a BIO encoding, each token is classified as being the Beginning, Inside, or Outside of a mention.</p>
<p>$e_{i} \neq e_{\varnothing}$, the pseudo embedding $h_{m_{i}}$ (Equation 2) should be close to the entity embedding of the annotated entity, $\operatorname{EntEmbed}\left(e_{m_{i}}\right)$.</p>
<p>$$
\begin{aligned}
\text { ELLoss } &amp; =\sum_{m_{i}} \alpha_{i} \cdot \mathbb{1}<em m__i="m_{i">{e</em> \
\alpha_{i} &amp; =\frac{\exp \left(\operatorname{EntEmbed}\left(e_{m_{i}}\right) \cdot h_{m_{i}}\right)}{\sum_{e \in \mathcal{E}} \exp \left(\operatorname{EntEmbed}(e) \cdot h_{m_{i}}\right)}
\end{aligned}
$$}} \neq e_{\varnothing}</p>
<p>Note that this loss is not applied to $88 \%$ of mentions, which do not have hyperlinks. Memory access for those mentions is unsupervised. The same loss is used for the EntityPred head.</p>
<p>Masked Language Modelling We follow BERT, and train the TokenPred head to independently predict each of the masked out tokens in an input context, using a cross-entropy loss over $\mathcal{V}$.</p>
<h2>3 Related Work</h2>
<p>Knowledge-augmented language models Our work shares inspiration with other approaches seeking to inject knowledge into language models (Ahn et al., 2016; Yang et al., 2016; Logan et al., 2019; Zhang et al., 2019; Levine et al., 2019; Xiong et al., 2019a; Peters et al., 2019; Poerner et al., 2019; Wang et al., 2020). These have used a variety of knowledge sources (WikiData, WordNet relations, outputs from dependency parsers) and additional training objectives (synonym and hyponym-hypernym prediction, word-supersense prediction, replaced entity detection, predication prediction, dependency relation prediction, entity linking). ${ }^{3}$ Our focus is on adding knowledge about entities, so our work is closer to Zhang et al. (2019); Peters et al. (2019); Xiong et al. (2019b); Wang et al. (2020); Poerner et al. (2019) than to the linguistically-augmented approaches of Levine et al. (2019); Lauscher et al. (2019). Closest to our work, KnOwBERT (Peters et al., 2019) introduce an entity memory layer that is similar to the one in EAE. In contrast with our work, KnowBERT starts from the BERT checkpoint, does not train with a knowledge-focused objective such as our mention-masking input function and uses precomputed entity representations when integrating the information from knowledge bases. In addition, KnowBERT relies on a fixed, pre-existing candidate detector (alias table) to identify potential candidates and entities for a span while our model</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>learns to detect mentions. We compare to their approach in Section 7.</p>
<p>Memory Augmented Neural Networks Our entity memory layer is closely tied to memory-based neural layers (Weston et al., 2014; Sukhbaatar et al., 2015). In particular, it can be seen as a memory network where memory access is supervised through entity linking, and memory slots each correspond to a learned entity representation. When unconstrained, these memory networks can be computationally expensive and supervising access through entity linking limits this issue. Another approach to scale memory networks is given by Lample et al. (2019) who introduce product-key memories to efficiently index a large store of values.</p>
<p>Conditional Computation Conditional computation models seek to increase model capacity without a proportional increase in computational complexity. This is usually done through routing, where only a subset of the network is used to process each input. To facilitate such routing, approaches such as large mixture of experts (Shazeer et al., 2017) or gating (Eigen et al., 2013; Cho and Bengio, 2014) have been used. Our method proposes entities as experts, which allows us to supervise memory access at two levels. We only access memories for entity mentions, and we only need to access memories for the entities that were mentioned.</p>
<h2>4 Models Evaluated</h2>
<p>We evaluate EAE on cloze knowledge probes, opendomain question answering and relation extraction. Here, we describe baselines from previous work and ablations.</p>
<h3>4.1 The Entities as Experts models</h3>
<p>Our primary model, Entities As Experts embeds the input using the token embedding layer, then passes it through 4 transformer layers. This output is used for the entity memory layer, which uses embeddings of size 256, then passed through an additional 8 transformer layers. Finally, the model has a TokenPred head and an EntityPred head. In EAE, hyper-parameters for the transformer layers are identical to those in BERT-base (Devlin et al., 2018).</p>
<p>Sparse Activation in EaE EAE only accesses the entity embedding matrix for mention spans, and we only retrieve $k=100$ entity memories for each mention (see Appendix 6.3 for analysis of this</p>
<p>choice). This type of conditional computation can facilitate massive model scaling with fixed computational resources (Shazeer et al., 2017; Lample et al., 2019) and, while our current implementation does not yet include an efficient implementation of top-k routing (Section 2), we note that it is possible with fast maximum inner product search (Ram and Gray, 2012; Shen et al., 2015; Shrivastava and Li, 2014; Johnson et al., 2017). We leave the implementation and investigation of this to future work.</p>
<h3>4.2 Ablations</h3>
<p>EaE-unsup In EAE-unsup, the entity memory is not supervised to isolate the usefulness of supervising memory slots with entity linking. We use full attention at inference when doing memory access.</p>
<p>No-EaE This ablation seeks to isolate the impact of the entity-memory layer. No EAE has a token embedding layer, twelve transformer layers, and an EntityPred and a TokenPred head. This approach has similar number of parameters as EAE, ${ }^{4}$ but only uses the entity embedding matrix at the EntityPred head. In contrast with EAE, this baseline cannot model interactions between the entity representations in the entity embedding matrix. Also, the entity embeddings cannot be directly used to inform masked language modelling predictions.</p>
<p>BERT / MM We compare to the BERT model. To ascertain which changes are due to EAE's data and masking function (Section 2.2.2), and which are due to the entity-specific modeling, we report performance for BERT-MM which uses BERT's architecture with EAE's masking function and data. We present results for Base and Large model sizes.</p>
<h3>4.3 Question Answering Models</h3>
<p>RELIC learns entity embeddings that match BERT's encoding of the contexts in which those entities were mentioned (Ling et al., 2020). T5 is an encoder-decoder trained on an enormous web corpus. We compare to the version fine-tuned for opendomain question answering (Roberts et al., 2020). We also compare to the nearest neighbour baselines introduced by Lewis et al. 2020; and we compare to three recent QA models that use a retriever and a reader to extract answers: BM25+BERT and ORQA from Lee et al. 2019 and GraphRetriever (GR) is introduced by Min et al. 2019b. All are described fully in Appendix A.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>5 Knowledge Probing Tasks</h2>
<p>We follow previous work in using cloze tests and question answering tasks to quantify the declarative knowledge captured in the parameters of our model (Petroni et al., 2019; Roberts et al., 2020).</p>
<h3>5.1 Predicting Wikipedia Hyperlinks</h3>
<p>We explore the ability of our model to predict masked out hyperlink mentions from Wikipedia, similar to the pre-training task ${ }^{5}$ (Section 2.2). We calculate accuracy on a 32 k test examples separate from the training data (Appendix B).</p>
<p>Table 1 shows the results for all our models. The MM-base and No-EAE models perform similarly on the token prediction task. These two models have the same architecture up until the point of token prediction. This indicates that the signal coming from the entity linking loss (Section 2.2.2) does not benefit language modeling when it is applied at the top of the transformer stack only.</p>
<p>Introducing the entity memory layer in the middle of the transformer stack (EAE) improves performance on both language modeling and entity linking, compared to the No-EAE model. This indicates that the entity representations are being used effectively, and that the model is learning inter-entity relations, using the output from the entity memory layer to improve predictions at the downstream entity and token prediction layers.</p>
<p>If the memory layer is not supervised (EAEunsup), performance on token prediction accuracy and perplexity is significantly worse than for EAE. This underlines the importance of entity linking supervision in teaching EAE how to best allocate the parameters in the entity memory layer.</p>
<p>Finally, EAE performs better at predicting mention tokens than the 24-layer MM-large, but does marginally worse in terms of perplexity. We believe that EAE is overconfident in its token predictions when wrong, and we leave investigation of this phenomenon to future work.</p>
<h3>5.2 LAMA</h3>
<p>LAMA (Petroni et al., 2019) contains cloze tasks from three different knowledge base sources, and one QA dataset. LAMA aims to probe the knowledge contained in a language model, with a focus on the type of knowledge that has traditionally been manually encoded in knowledge bases. As a zero-shot probing task, it does not involve any</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Params</th>
<th style="text-align: center;">Entity Acc</th>
<th style="text-align: center;">Token</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">PPL</td>
</tr>
<tr>
<td style="text-align: center;">MM-base</td>
<td style="text-align: center;">110 m</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">19.6</td>
</tr>
<tr>
<td style="text-align: center;">MM-large</td>
<td style="text-align: center;">340 m</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">10.3</td>
</tr>
<tr>
<td style="text-align: center;">EAE-unsup</td>
<td style="text-align: center;">366 m</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">16.9</td>
</tr>
<tr>
<td style="text-align: center;">No EAE</td>
<td style="text-align: center;">366 m</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">19.3</td>
</tr>
<tr>
<td style="text-align: center;">EAE</td>
<td style="text-align: center;">367 m</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">56.9</td>
<td style="text-align: center;">11.0</td>
</tr>
</tbody>
</table>
<p>Table 1: Results on cloze-style entity prediction accuracy, token prediction accuracy and perplexity on the test set of our masked hyperlink data.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Concept <br> Net</th>
<th style="text-align: center;">RE</th>
<th style="text-align: center;">SQuAD</th>
<th style="text-align: center;">T <br> -REx</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BERT-base</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">17.7</td>
</tr>
<tr>
<td style="text-align: left;">BERT-large</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">$\mathbf{1 0 . 5}$</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">19.9</td>
</tr>
<tr>
<td style="text-align: left;">MM-base</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">29.7</td>
<td style="text-align: center;">16.3</td>
</tr>
<tr>
<td style="text-align: left;">MM-large</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">$\mathbf{2 4 . 4}$</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">18.7</td>
</tr>
<tr>
<td style="text-align: left;">EAE-unsup</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">18.0</td>
</tr>
<tr>
<td style="text-align: left;">No EAE</td>
<td style="text-align: center;">10.3</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">18.5</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">17.4</td>
</tr>
<tr>
<td style="text-align: left;">EAE</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">$\mathbf{3 7 . 4}$</td>
<td style="text-align: center;">$\mathbf{2 0 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Results on the LAMA probe. Adding entity memory improves performance for the probes that focus on entities. Mention masking reduces performance on ConceptNet sub-task which requires prediction of non-mention terms such as "happy".
task specific model training, and we do not apply any LAMA-specific modeling. Table 2 shows that adding the entity memory layer to the MM-base model improves performance across the board on this task. Not supervising this layer with entity linking (EAE-unsup) is worse overall.</p>
<p>EAE's average accuracy is similar to BERTlarge. However, the LAMA sub-task accuracies show that the two models are complementary. Mention focused approaches are much better than the BERT baselines at predicting the mention like words in the SQuAD and T-REx probes, but they are marginally worse for the RE probe and very significantly worse for the ConceptNet probe. This is because the ConceptNet sub-task mostly includes non-entity answers such as "fly", "cry", and "happy", and a third of the answers in the GoogleRE sub-task are dates. We leave modeling of nonentity concepts and dates to future work. Finally, the entity specific memory in EAE is most beneficial for T-Rex, which focuses on common entities that are likely in our 1 m entity vocabulary.</p>
<h3>5.3 Open domain Question Answering</h3>
<p>Setup TriviaQA and WebQuestions were introduced as reading comprehension and semantic parsing tasks, respectively (Joshi et al., 2017; Berant
et al., 2013). More recently, these datasets have been used to assess the performance of QA systems in the open domain setting where no evidence documents or database is given. In this setup, TriviaQA contains 79 k train examples and WebQuestions 3.1 k . Most approaches rely on a text corpus at test time, extracting answers from evidence passages returned by a retrieval system. However, T5 and RELIC used neural networks to answer questions directly. We follow Roberts et al. 2020 in describing approaches as open-book (test time access to corpus) and closed-book (no test time access to corpus), and we report the nearest neighbour results from Lewis et al. 2020.</p>
<p>We follow RELIC and resolve answer strings to Wikipedia entity identifiers. ${ }^{6}$ We follow the training procedure from Section 2.2, with a round of task specific training that applies the entity linking and mention detection losses to the question answering data. Each question is appended with a special 'answer position' token and EAE is trained to predict the correct answer entity in this position, using the entity linking loss from Section 2.2.2. Mention spans are identified within the question (Section 2.2.1) and the mention detection loss from Section 2.2.2 is applied to encourage EAE to access the entity memory for entities in the question. See Appendix C for additional information on task setup and fine-tuning hyper-parameters.</p>
<p>Results Table 3 shows results on two open domain QA datasets. Entity prediction methods, RELIC and EAE, significantly outperform nearest neighbor baselines, showing that model generalizes beyond train / development overlap and entity representations contains information about answer entities. No-EAE and RELIC both encode text with a transformer and retrieve answer entities. Compared to RELIC, No-EAE is trained to identify all entities in a piece of text, instead of just one. This leads to small but significant gains on TriviaQA. A much larger gain (almost 6 points on both datasets) comes from adding an entity memory inside the transformer encoder (EAE). We also show that it is possible to improve performance on TriviaQA by doubling the size of the entity embeddings to 512d (EAE-emb-512). While this almost doubles the model size, it does not significantly increase</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"># <br> Params</th>
<th style="text-align: center;">TQA <br> Dev</th>
<th style="text-align: center;">TQA <br> Wiki Test</th>
<th style="text-align: center;">Web <br> Q</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Open-Book: Span Selection - Oracle 100\%</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BM25+BERT</td>
<td style="text-align: center;">110 m</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">17.7</td>
</tr>
<tr>
<td style="text-align: center;">ORQA</td>
<td style="text-align: center;">330 m</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">36.4</td>
</tr>
<tr>
<td style="text-align: center;">GR</td>
<td style="text-align: center;">110 m</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">31.6</td>
</tr>
<tr>
<td style="text-align: center;">Closed-Book: Nearest Neighbor</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ORACLE</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">TFIDF</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BERT-base</td>
<td style="text-align: center;">110 m</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Closed-Book: Generation - Oracle 100\%</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">T5-Base</td>
<td style="text-align: center;">220 m</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">29.1</td>
<td style="text-align: center;">29.1</td>
</tr>
<tr>
<td style="text-align: center;">T5-Large</td>
<td style="text-align: center;">770 m</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">32.2</td>
</tr>
<tr>
<td style="text-align: center;">T5-3B</td>
<td style="text-align: center;">3B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">34.4</td>
</tr>
<tr>
<td style="text-align: center;">T5-11B</td>
<td style="text-align: center;">11B</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">37.4</td>
</tr>
<tr>
<td style="text-align: center;">T5-11B+SSM</td>
<td style="text-align: center;">11B</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">61.6</td>
<td style="text-align: center;">43.5</td>
</tr>
<tr>
<td style="text-align: center;">Closed-Book: Entity Prediction</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ORACLE</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">91.0</td>
</tr>
<tr>
<td style="text-align: center;">RELIC</td>
<td style="text-align: center;">3B</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">No EAE</td>
<td style="text-align: center;">366 m</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">33.4</td>
</tr>
<tr>
<td style="text-align: center;">EAE</td>
<td style="text-align: center;">367 m</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">39.0</td>
</tr>
<tr>
<td style="text-align: center;">EAE, emb 512</td>
<td style="text-align: center;">623 m</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">38.7</td>
</tr>
</tbody>
</table>
<p>Table 3: Exact Match accuracy on TriviaQA and WebQuestions. Open-book approaches reserve $10 \%$ of the training data for development; entity prediction approaches only train on linked entity answers; and T5 merges Unfiltered-Dev into training for Wiki-Test. For more description of these choices, see Appendix E.
the number of parameters that need to be accessed at inference time. See Section 4.1 for a discussion of how this could be beneficial, and Appendix 6.3 for a preliminary investigation of conditional activation in EAE.</p>
<p>Even though entity prediction approach cannot answer $15 \%$ of the data with unlinked answers for TriviaQA, and $9 \%$ for WebQuestions, it outperforms all of the standard T5 models including one that has $30 \times$ the parameters. This indicates that the entity specific model architecture is more efficient in capturing the sort of information required for this knowledge probing task than the general encoderdecoder architecture used by T5. However, when T5 is enhanced with an extra pre-training steps focusing on likely answer spans from Wikipedia (T5-11B + SSM) its performance leapfrogs that of EAE. We note that the 'salient spans' included in the SSM objective are likely to be entities (Guu et al., 2020), and believe that there is significant future work to be done in combining methods of entity prediction and text generation.</p>
<p>Though closed-book approaches are still behind open-book approaches on TriviaQA, we be-
lieve even higher performances could be attained by ensembling diverse approaches and a preliminary study (Appendix F), indicates that ensembling open-book with closed-book approaches is preferable to ensembling within a single paradigm.</p>
<h2>6 Analysis of TriviaQA Results</h2>
<h3>6.1 Entity-Based Analysis</h3>
<p>We compare the performances of retrieval-based GR, generation-based T5-11B, and our EAE model on the TriviaQA Unfiltered-dev set. Figure 3 (a) shows that all models perform better on frequent entities. EAE's performance is much lower for entities seen fewer than one hundred times, likely because their entity embedding do not contain enough information. Figure 3 (b) shows that as the number of named entity mentions grows in the question, T5 performance decreases whereas EAE and GR performance increases. We presume more entity mentions makes it easier to retrieve relevant documents, thus contributing to better performance for GR. For EAE, having more entity mentions allows the model to access more entity knowledge. We conduct further qualitative analysis below. Figure 3 (c) shows that closed-book models under-perform on long questions. The trend disappears for openbook models.</p>
<h3>6.2 Manual Analysis</h3>
<p>We randomly sampled 100 examples from unfiltered-dev set, and analysed the ability of EAE's to correctly identify and link the question's entities. We find that $87 \%$ of questions do not have any incorrectly predicted entities. ${ }^{7}$</p>
<p>We find that when there is at least one incorrectly linked entity, the performance of EAE is considerably reduced. On this small sample, the performance is even lower than for examples in which there are no named entities in the question.</p>
<p>Table 4 illustrates three representative examples from EAE and T5. In the first example, the question contains a date but no proper names. Since EAE does not have a representation for dates in the entity memory, this question is challenging and the model predicts an incorrect answer of the correct type (annual event). The second example demonstrates EAE's ability to model connections between entities. In this case, 'The Master' only occurs 38</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Performance on TriviaQA by: answer frequency in our Wikipedia training corpus (NA if not linked); proper names in the question; tokens in the question. Standard deviations obtained through bootstrapping.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Question</th>
<th style="text-align: left;">Answer</th>
<th style="text-align: left;">T5 Prediction</th>
<th style="text-align: left;">EaE Prediction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Next Sunday, Sept 19, is International what day?</td>
<td style="text-align: left;">Talk like a pirate day</td>
<td style="text-align: left;">talk like a pirate day</td>
<td style="text-align: left;">Pearl Harbor Re- <br> membrance Day</td>
</tr>
<tr>
<td style="text-align: left;">Which [Dr. Who] villain has been played by [Roger <br> Delgado], [Anthony Ainley], [Eric Roberts], etc?</td>
<td style="text-align: left;">The Master</td>
<td style="text-align: left;">mr. daleks</td>
<td style="text-align: left;">The Master</td>
</tr>
<tr>
<td style="text-align: left;">Which early aviator flew in a plane christened ${$ Jason $}$ ?</td>
<td style="text-align: left;">Amy Johnston</td>
<td style="text-align: left;">jean batten</td>
<td style="text-align: left;">Icarus</td>
</tr>
<tr>
<td style="text-align: left;">Jason $\rightarrow$ Jason (Greek Mythology) Q176758</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 4: Illustrative examples of predictions for the TriviaQA dev set. Questions are annotated with [correct] and {incorrect} entity predictions from EaE, which is most successful when question entities are linked successfully.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$K$</th>
<th style="text-align: right;">Entity acc</th>
<th style="text-align: right;">Tok acc</th>
<th style="text-align: right;">Tok PPL</th>
<th style="text-align: right;">TQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: right;">59.2</td>
<td style="text-align: right;">56.7</td>
<td style="text-align: right;">18.0</td>
<td style="text-align: right;">40.1</td>
</tr>
<tr>
<td style="text-align: left;">10</td>
<td style="text-align: right;">61.7</td>
<td style="text-align: right;">57.2</td>
<td style="text-align: right;">11.1</td>
<td style="text-align: right;">43.1</td>
</tr>
<tr>
<td style="text-align: left;">100</td>
<td style="text-align: right;">61.8</td>
<td style="text-align: right;">57.1</td>
<td style="text-align: right;">11.0</td>
<td style="text-align: right;">43.2</td>
</tr>
<tr>
<td style="text-align: left;">Full $\left(10^{6}\right)$</td>
<td style="text-align: right;">61.8</td>
<td style="text-align: right;">56.9</td>
<td style="text-align: right;">11.0</td>
<td style="text-align: right;">43.4</td>
</tr>
</tbody>
</table>
<p>Table 5: Impact of varying the number of retrieved entity embeddings $(K)$ in the Entity Memory layer at inference on the entity prediction and TriviaQA tasks.
times in our training data, but in each of those occurrences it is likely that at least one of the relevant actors is also mentioned. EaE learns the characteractor relationship, while T5 makes up an incorrect character name based on a common category of Dr. Who villain. The final example highlights the sensitivity of EaE to incorrectly linked question entities. Here, the name 'Jason' has been incorrectly linked to the Greek mythological with that name, which causes EaE to predict another Greek mythological figure, Icarus, as the answer. This is particularly interesting because Icarus is strongly associated with human flight-EAE is still trying to find an aviator, albeit one from Greek mythology. Additional examples can be found in Appendix G.</p>
<h3>6.3 Top-K over Entity Embeddings</h3>
<p>As described in Section 2, EaE uses the top 100 entity memories during retrieval for each mention.</p>
<p>Here, we empirically analyse the influence of this choice. Table 5 shows how varying the number of retrieved entity embeddings in the entity memory layer at inference time impacts accuracy of entity prediction and TrviaQA. Even for $K=10$, performance does not deteriorate meaningfully.</p>
<p>This is a key advantage of our modular approach, where entity information is organized in the Entity Memory layer. Despite only accessing as many parameters as BERT Base, our model outperforms BERT Large on token prediction. Similarly in TriviaQA, we outperform T5-3B model, while accessing about $3 \%$ of the parameters. While naive implementation would not bring significant computational gain, fast nearest neighbor methods methods on entity embeddings enable retrieving the top $K$ entities in sub-linear time and storage, enabling efficient model deployment.</p>
<h2>7 Comparison to Alternative Entity Representations</h2>
<p>EAE is novel in learning entity representations along with the parameters of a Transformer model. Here, we provide a direct comparison between EAE's learned representations and the externally trained, fixed representations used by ERNIE (Zhang et al., 2019) and KnOWBERT (Peters et al., 2019). Table 6 presents results for the EaE architecture trained with entity memories initialized</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Initialization</th>
<th style="text-align: center;">Dim</th>
<th style="text-align: center;">Learned</th>
<th style="text-align: center;">Typing</th>
<th style="text-align: center;">PPL</th>
<th style="text-align: center;">Entity Acc.</th>
<th style="text-align: center;">LAMA-SquAD</th>
<th style="text-align: center;">T-REx</th>
<th style="text-align: center;">TQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">63.0</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">30.9</td>
</tr>
<tr>
<td style="text-align: left;">TransE</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">74.1</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">24.2</td>
</tr>
<tr>
<td style="text-align: left;">TransE</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\mathbf{7 5 . 1}$</td>
<td style="text-align: center;">10.5</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">30.6</td>
</tr>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">300</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">$\mathbf{1 9 . 3}$</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">33.2</td>
</tr>
<tr>
<td style="text-align: left;">Deep-Ed</td>
<td style="text-align: center;">300</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">30.1</td>
</tr>
<tr>
<td style="text-align: left;">Deep-Ed</td>
<td style="text-align: center;">300</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">$\mathbf{8 . 9}$</td>
<td style="text-align: center;">$\mathbf{6 5 . 1}$</td>
<td style="text-align: center;">15.7</td>
<td style="text-align: center;">$\mathbf{3 3 . 5}$</td>
<td style="text-align: center;">$\mathbf{3 3 . 9}$</td>
</tr>
</tbody>
</table>
<p>Table 6: Comparison of learned representations to knowledge graph embeddings (TransE) and pre-trained representations of entity descriptions and contexts (Deep-Ed). All experiments use same 200k entities.
from three different sources and two different training scenarios. The initial embedddings are either the TransE-Wikidata embeddings used by ERNIE (Bordes et al., 2013); the Deep-Ed embeddings used by KnowBert (Ganea and Hofmann, 2017); or the random embeddings used by EaE. The embeddings are either frozen, following ERNIE and KNOWBERT, or trained along with all other network parameters. ${ }^{8}$ Along with the knowledge probing tasks from Section 5, we report performance on the on the 9-way entity typing task from Choi et al. 2018 (Appendix D).</p>
<p>It is clear that learning entity representations is beneficial. However, initializing the entity memory with Deep-Ed embeddings leads to gains on most of the knowledge probing tasks, suggesting that there are potential benefits from combining training regimens. Meanwhile, the best entity typing results come from initializing with Wikidata embeddings, possibly because Wikidata has high quality coverage of the types ('person', 'organization', 'location', etc.) used. Finally, we point out that both ERNIE and KNOWBERT differ from EaE in other ways (see Appendix A). KnowBert in particular incorporates WordNet synset embeddings as well as entity embeddings, leading to entity typing accuracy of 76.1. Future work will explore different ways of combining learned embeddings with knowledge from diverse external sources.</p>
<p>As discussed in Section 3, there are significant differences between EaE and KNOWBERT other than the choice of entity representation. In particular, KnowBert has an explicit entity-entity attention mechanism. To determine whether this has a significant effect on a model's ability to model entity-entity relations, we evaluate EaE on TACRED (Zhang et al., 2017) using the cleaned</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 7: $\mathrm{F}_{1}$ scores on original and revisited versions of TACRED test sets. KnowBert scores are reported as in Alt et al. (2020), corresponding to the best performing variant (KnowBert-W+W).
dataset introduced by (Alt et al., 2020). ${ }^{9}$ Table 7 shows that EaE outperforms KNOWBERT on the revised and weighted splits introduced by (Alt et al., 2020), although it slightly under-performs on the original setting. ${ }^{10}$ This result indicates that EAE, without explicitly entity-entity attention, can capture relations between entities effectively.</p>
<h2>8 Conclusion</h2>
<p>We introduced a new transformer architecture, EaE, which learns entity representations from text along with other model parameters. Our evaluation shows that EAE is effective at capturing declarative knowledge and can be used for a wide variety of tasks - including open domain question answering, relation extraction, entity typing and knowledge probing tasks. Our entity representations influence the answer predictions for open-domain question answering system and are of high quality, compared to prior work such as KnowBert.</p>
<p>Our model learns representations for a pre-fixed vocabulary of entities, and cannot handle unseen entities. Future work can explore representations for rare or unseen entities, as well as developing less memory-intensive ways to learn and integrate entity representations. Furthermore, integrating information from knowledge-bases can further improve the quality of entity representation.</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<h2>Acknowledgments</h2>
<p>We thank many members of Google Research for helpful feedback and discussions, especially Kenton Lee, Dipanjan Das, Pat Verga and William Cohen. We also thank Adam Roberts and Sewon Min for sharing their system outputs.</p>
<h2>References</h2>
<p>Sungjin Ahn, Heeyoul Choi, Tanel Pärnamaa, and Yoshua Bengio. 2016. A neural knowledge language model. arXiv preprint 1608.00318.</p>
<p>Christoph Alt, Aleksandra Gabryszak, and Leonhard Hennig. 2020. TACRED revisited: A thorough evaluation of the TACRED relation extraction task. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 15581569, Online. Association for Computational Linguistics.</p>
<p>Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski. 2019. Matching the blanks: Distributional similarity for relation learning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2895-2905, Florence, Italy. Association for Computational Linguistics.</p>
<p>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533-1544.</p>
<p>Antoine Bordes, Nicolas Usunier, Alberto GarciaDuran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multirelational data. In NeurIPS.</p>
<p>Kyunghyun Cho and Yoshua Bengio. 2014. Exponentially increasing the capacity-to-computation ratio for conditional computation in deep learning. arXiv preprint 1406.7362.</p>
<p>Eunsol Choi, Omer Levy, Yejin Choi, and Luke Zettlemoyer. 2018. Ultra-fine entity typing. In ACL.</p>
<p>Andrew M Dai and Quoc V Le. 2015. Semi-supervised sequence learning. In NeurIPS.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint 1810.04805.</p>
<p>David Eigen, Marc'Aurelio Ranzato, and Ilya Sutskever. 2013. Learning factored representations in a deep mixture of experts. arXiv preprint 1312.4314.</p>
<p>Octavian-Eugen Ganea and Thomas Hofmann. 2017. Deep joint entity disambiguation with local neural attention. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2619-2629, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Realm: Retrievalaugmented language model pre-training. arXiv preprint 2002.08909.</p>
<p>Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In $A C L$.</p>
<p>Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017. Billion-scale similarity search with gpus. arXiv preprint 1702.08734.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint 1705.03551.</p>
<p>Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint 1412.6980.</p>
<p>Guillaume Lample, Alexandre Sablayrolles, Marc'Aurelio Ranzato, Ludovic Denoyer, and Hervé Jégou. 2019. Large memory layers with product keys. In NeurIPS.</p>
<p>Anne Lauscher, Ivan Vulić, Edoardo Maria Ponti, Anna Korhonen, and Goran Glavaš. 2019. Informing unsupervised pretraining with external linguistic knowledge. arXiv preprint 1909.02339.</p>
<p>Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. arXiv preprint 1906.00300.</p>
<p>Yoav Levine, Barak Lenz, Or Dagan, Dan Padnos, Or Sharir, Shai Shalev-Shwartz, Amnon Shashua, and Yoav Shoham. 2019. Sensebert: Driving some sense into bert. arXiv preprint 1908.05646.</p>
<p>Patrick Lewis, Pontus Stenetorp, and Sebastian Riedel. 2020. Question and answer test-train overlap in open-domain question answering datasets.</p>
<p>Jeffrey Ling, Nicholas FitzGerald, Zifei Shan, Livio Baldini Soares, Thibault Févry, David Weiss, and Tom Kwiatkowski. 2020. Learning crosscontext entity representations from text. arXiv preprint 2001.03765.</p>
<p>Robert Logan, Nelson F Liu, Matthew E Peters, Matt Gardner, and Sameer Singh. 2019. Baracks wife hillary: Using knowledge graphs for fact-aware language modeling. In $A C L$.</p>
<p>Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019a. A discrete hard em approach for weakly supervised question answering. arXiv preprint 1909.04849.</p>
<p>Sewon Min, Danqi Chen, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2019b. Knowledge guided text retrieval and reading for open domain question answering. arXiv preprint 1911.03868.</p>
<p>Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. arXiv preprint 1802.05365.</p>
<p>Matthew E Peters, Mark Neumann, Robert L Logan, IV, Roy Schwartz, Vidur Joshi, Sameer Singh, and Noah A Smith. 2019. Knowledge enhanced contextual word representations. arXiv.</p>
<p>Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. 2019. Language models as knowledge bases? arXiv preprint 1909.01066.</p>
<p>Nina Poerner, Ulli Waltinger, and Hinrich Schütze. 2019. Bert is not a knowledge base (yet): Factual knowledge vs. name-based reasoning in unsupervised qa. arXiv preprint 1911.03681.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint 1910.10683.</p>
<p>Parikshit Ram and Alexander G Gray. 2012. Maximum inner-product search using cone trees. In International conference on Knowledge discovery and data mining.</p>
<p>Michael Ringgaard, Rahul Gupta, and Fernando CN Pereira. 2017. Sling: A framework for frame semantic parsing. arXiv preprint 1710.07032.</p>
<p>Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? ArXiv, abs/2002.08910.</p>
<p>Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint 1701.06538.</p>
<p>Fumin Shen, Wei Liu, Shaoting Zhang, Yang Yang, and Heng Tao Shen. 2015. Learning binary codes for maximum inner product search. In ICCV.</p>
<p>Anshumali Shrivastava and Ping Li. 2014. Asymmetric lsh (alsh) for sublinear time maximum inner product search (mips). In NeurIPS.</p>
<p>Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. 2015. End-to-end memory networks. In NeurIPS.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Neurips.</p>
<p>Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Cuihong Cao, Daxin Jiang, Ming Zhou, et al. 2020. K-adapter: Infusing knowledge into pre-trained models with adapters. arXiv preprint 2002.01808.</p>
<p>Jason Weston, Sumit Chopra, and Antoine Bordes. 2014. Memory networks. arXiv preprint 1410.3916.</p>
<p>Wenhan Xiong, Jingfei Du, William Yang Wang, and Veselin Stoyanov. 2019a. Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model. arXiv preprint 1912.09637.</p>
<p>Wenhan Xiong, Jingfei Du, William Yang Wang, and Veselin Stoyanov. 2019b. Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model. arXiv preprint 1912.09637.</p>
<p>Zichao Yang, Phil Blunsom, Chris Dyer, and Wang Ling. 2016. Reference-aware language models. arXiv preprint 1611.01628.</p>
<p>Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and Christopher D Manning. 2017. Positionaware attention and supervised data improve slot filling. In EMNLP.</p>
<p>Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. Ernie: Enhanced language representation with informative entities. arXiv preprint 1905.07129.</p>
<h2>A Other Models Evaluated</h2>
<p>In this section we describe the models mentioned in Section 4.3 and compared to in Section sections 5 and 7 .</p>
<h2>A. 1 Open-Book Question Answering Systems</h2>
<p>Open-Book Open Domain Question Answering Systems are usually comprised of two components: a retriever and a reader. The retriever reads a set of documents from a corpus or facts from a knowledge base. Top retrievals are then fed to the reader which predicts an answer, often through span selection.</p>
<p>BM25+BERT (Lee et al., 2019) uses the nonparametric BM25 as its retriever and a BERT-base reader to predict an answer through span selection.</p>
<p>ORQA (Lee et al., 2019) uses two BERT-base models to retrieve relevant passages and a BERTbase reader to predict the answer span.</p>
<p>Graph Retriever (Min et al., 2019b)'s retriever can perform Graph-based, Entity-based and Textmatch retrieval. These different retrieval modalities are then fused in representations that serve as input to a span-selection reader.</p>
<h2>A. 2 Other Models</h2>
<p>BERT (Devlin et al., 2018) is a transformer, pretrained using masked language modelling. We report results for BERT-base, which has 110 m parameters, and BERT-large, which has 340 m parameters. The transformer architecture used by BERT-base is identical to the 12 transformer layers in EAE. BERT-large uses a much larger transformer, and has a similar number of parameters overall to EAE.</p>
<p>RELIC (Ling et al., 2020) is a dual encoder with a BERT-base architecture that compares a representation of a mention to an entity representation. It is similar to our No-EAE architecture. Its training is however different, as only linked mentions are masked and only one mention is masked at a time. In addition, RELIC does not have mention detection or masked language modelling losses. Finally, it is also initialized with the BERT checkpoint whereas we train our models from scratch.</p>
<p>T5 T5 is an encoder-decoder transformer introduced in Raffel et al. (2019). It has been fine-tuned for open domain question answering in Roberts et al. (2020). In that setup, the model is trained to generate the answer to a question without any
context. T5 does not explicitly model entities or have any form of memory. We compare to models of different sizes, from 220 m parameters to 11 B . 'SSM' refers to salient span masking, indicating that prior to fine-tuning on open domain question answering the model was fine-tuned using salient span masking, which bears resemblances to our mention masking.</p>
<p>KnowBERT (Peters et al., 2019) KnowBERT is a BERT-base transformer that embeds multiple knowledge bases to improve performance in a variety of tasks. The integration of this information is done through a Knowledge Attention and Recontextualization component, which can be seen as a small transformer that is run on the pooled mention representations of potential entities. KnowBERT uses this layer to embed entity representations from Wikipedia as well as Wordnet graph information. In contrast with our work, KnowBERT starts from the BERTcheckpoint, does not train with a knowledge-focused objective such as our mention-masking input function and uses preexisting representations when integrating the information from knowledge bases. In addition, KnowBERT relies on a fixed, pre-existing candidate detector (alias table) to identify potential candidates and entities for a span while our model learns mention detection.</p>
<p>ErniE (Zhang et al., 2019) ErniE is a BERTbase transformer that takes as additional input the list of entities in the sentence. Multi-head attention is performed on those entities before they are introduced in they are aggregated with the token representations. In addition to BERT's pre-training objective, ERNIE also masks entities and trains the model to predict them. In contrast with both KnowBERT and EAE, ERNIE takes entity linking as an input rather than learning to do it inside the model. In contrast with our approach, ERNIE uses pre-existing entity representations that are fixed during training.</p>
<h2>B Wikipedia Pre-training</h2>
<p>Wikipedia Processing We build our training corpus of contexts paired with entity mention labels from the 2019-04-14 dump of English Wikipedia. We first divide each article into chunks of 500 bytes, resulting in a corpus of 32 million contexts with over 17 million entity mentions. We restrict ourselves to the one million most frequent entities</p>
<p>( $86 \%$ of the linked mentions). These are processed with the BERT tokenizer using the lowercase vocabulary, limited to 128 word-piece tokens. In addition to the Wikipedia links, we annotate each sentence with unlinked mention spans using the mention detector from Section 2.2. These are used as additional signal for our mention detection component and allow the model to perform retrieval even for mentions that are not linked in Wikipedia. We set aside $0.2 \%$ of the data for development and $0.2 \%$ for test and use the rest to pre-train our model.</p>
<p>Pre-training hyper-parameters We pre-train our model from scratch. We use ADAM (Kingma and $\mathrm{Ba}, 2014$ ) with a learning rate of $1 \mathrm{e}-4$. We apply warmup for the first $5 \%$ of training, decaying the learning rate afterwards. We also apply gradient clipping with a norm of 1.0. We train for one million steps using a large batch size of 4096. We use a TokenPrediction head for all our models and an EntityPrediction head for the EAE and No-EAE models. We did not run extensive hyper-parameter tuning for pre-training due to computational cost. We train on 64 Google Cloud TPUs for all our pre-training experiments. All pre-training experiments took between 2 days and a week to complete.</p>
<h2>C Open Domain Question Answering</h2>
<p>Open Domain QA Preprocessing We annotate each question with proper-name mentions ${ }^{11}$ using the mention detector from Section 2.2.</p>
<p>When the answer is an entity, in our entity vocabulary, we link the answer string to an entity ID using the SLING phrase table ${ }^{12}$ (Ringgaard et al., 2017). If the answer is not an entity in our vocabulary, we discard the question from the training set, though we keep it in the development and test set to ensure fair comparisons with prior work. Table 8 shows the share of answers that were linked using our procedure. This means that Oracle performance for our model on the TriviaQA development set is only $85 \%$, which is due to non entity-answers and entities not in our vocabulary.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 8: Share of the answers that are linked by our linking procedure for the TriviaQA and WebQuestion datasets. The test set for TriviaQA is not public, hence the missing number.</p>
<p>Hyper-parameters For TriviaQA, we fine-tune the entire model using a learning rate of $5 \mathrm{e}-6$, a batch size of 64 and performing 50,000 training steps. For WebQuestions, we set the learning rate to 3e-5, the batch size to 32 and train for 700 steps. Fine-tuning was done on 4 Google Cloud TPUs. In both cases, we searched the learning rate over $\left{5 \times 10^{-5}, 3 \times 10^{-5}, 10^{-5}, 5 \times 10^{-6}, 3 \times 10^{-6}\right}$ and the batch size in ${32,64,128}$ and selected the model based on validation performance.</p>
<h2>D Entity Typing</h2>
<p>We describe the procedure to obtain the Typing results of Table 6.</p>
<p>Open Entity Processing We use the Ultra-fine entity typing dataset introduced in (Choi et al., 2018). As is done in Zhang et al. (2019); Peters et al. (2019) we limit the task to the 9 generic types ('person', 'group', 'organization', 'location', 'entity', 'time', 'object', 'event' and 'place') and use the Micro-F1 metric. The dataset is comprised of 5994 examples, equally divided between train, development and test. We pass the sentences to our model without extra annotations for mention annotations or entity linking, relying instead on the model's own predictions. To predict a type for the span, we take the span representation and project it to the 9 classes.</p>
<p>Hyper-parameters Since we have no mention boundary or linking information, we freeze the entity embeddings and the mention detection parameters in fine-tuning. We used a learning rate of 3e-5, a batch size of 32 and trained for 700 steps. We also used label smoothing with a value of 0.1 . We searched the learning rate and the batch size between the same bounds as for the open-domain question answering tasks. For every model, we ran with five different seeds and selected the best model based on validation performance before running on test. We selected the threshold to compute F1 based on validation scores.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Systems</th>
<th style="text-align: center;">Oracle Acc.</th>
<th style="text-align: center;">Pred Overlap (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">T5 \&amp; EAE</td>
<td style="text-align: center;">55.9</td>
<td style="text-align: center;">29.3</td>
</tr>
<tr>
<td style="text-align: left;">T5 \&amp; GR</td>
<td style="text-align: center;">$\mathbf{6 6 . 4}$</td>
<td style="text-align: center;">30.1</td>
</tr>
<tr>
<td style="text-align: left;">EAE \&amp; GR</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">33.6</td>
</tr>
<tr>
<td style="text-align: left;">ORQA \&amp; GR</td>
<td style="text-align: center;">63.8</td>
<td style="text-align: center;">$\mathbf{3 9 . 6}$</td>
</tr>
</tbody>
</table>
<p>Table 9: Comparing prediction overlap and oracle accuracy on TriviaQA. Oracle accuracy considers a prediction correct if at least one of the model prediction is correct. While ORQA and GR outperform EAE and T5, their predictions overlap more with GR and offer less complementary value.</p>
<h2>E TriviaQA Evaluation Data Configuration</h2>
<p>As the TriviaQA dataset was originally introduced for reading comprehension task, prior work adapted the initial splits and evaluation to reflect the open domain setting. We describe the setup used by prior approaches and introduce ours, to enable fair comparison. The dataset comes in two blends, Wikipedia and Web documents. While TriviaQA's official web evaluation uses (question, document, answer) triplets, all open domain approaches average over (question, answer) pairs when using the Web data.</p>
<p>Open-book Approaches: Lee et al. (2019); Min et al. (2019a,b) use only the web splits. They use $90 \%$ of the original train data for training, performing model selection on the remaining $10 \%$ and reporting test numbers on the original development set since the original test set is hidden.</p>
<p>Previous Closed-book Approaches: Roberts et al. (2020) also use the web data to train and validate their model. However, they use the Wikipedia ${ }^{13}$ test split as a test set. After hyperparameter tuning, they re-train a model on both the training and development sets of the web data. Ling et al. (2020) uses the original web splits and reports performance on the development set.</p>
<p>Our approach: To compare our approach more closely to T5, we follow their setup, with the exception that we do not re-train a model on both the train and development splits after hyper-parameter selection.</p>
<h2>F Comparing QA paradigms</h2>
<p>We compare the performance of four systems (ORQA, GraphRetriever (GR), T5, EAE) on the TriviaQA Unfiltered-Dev set. GR achieves an accuracy of 55.4, ORQA 45.1, EAE 43.2 and T5 42.3. As the open-book paradigm differs significantly from the closed-book one, we intuit they might complement each other.</p>
<p>To test this hypothesis, we measure prediction overlap and oracle accuracy. Oracle accuracy considers a prediction correct if either system is correct (see Table 9). Unsurprisingly, the two open book approaches show the most similar predictions, overlapping in nearly $40 \%$ of examples. While ORQA outperforms T5 and EAE, the oracle accuracy of ORQA \&amp; GR is lower than ORQA \&amp; T5 or ORQA \&amp; EAE. This suggests some questions might be better suited to the closed book paradigm. In addition, the oracle accuracy of the two closed book systems is higher than that of the best performing open book system. We leave designing approaches that better combine these paradigms to future work.</p>
<h2>G Additional Examples of TriviaQA Predictions</h2>
<p>Table 10 illustrates additional representative sample of questions and predictions from EAE and T5. We break this sample down into questions that contain no named entities, questions that contain only correctly linked named entities, and questions that contain incorrectly linked named entities.</p>
<p>Example (i) shows another case where our model fails to handle dates. While T5 also has no distinct representation for dates, its 11B parameters managed to memorize the esoteric connection between the phrases 'Sept 19' and 'talk like a pirate day'. In the second example without named entities, the answer entity is sufficiently frequent in Wikipedia (top 50k entities), and EAE seems to have learned its connection with the specific categorical information in the question.</p>
<p>Example (iii) shows another case where the correctly linked entities enable EAE to correctly answer the prompt while T5 predicts a different director's name. Example (vi) shows how EAE fails at predicting date answers, even when there is abundant information in the question, predicting the entity representing the 1990's.</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Questions with no proper name mentions</th>
<th style="text-align: center;">Answer</th>
<th style="text-align: center;">T5 Prediction</th>
<th style="text-align: center;">EaE Prediction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">i</td>
<td style="text-align: center;">What links 1st January 1660 and 31st May 1669?</td>
<td style="text-align: center;">First and last entries in Samuel Pepys's diaries</td>
<td style="text-align: center;">they were the dates of the henry viii's last bath</td>
<td style="text-align: center;">Anglo-Dutch Wars</td>
</tr>
<tr>
<td style="text-align: center;">ii</td>
<td style="text-align: center;">Which radioactive substance sometimes occurs naturally in spring water?</td>
<td style="text-align: center;">radon</td>
<td style="text-align: center;">radon</td>
<td style="text-align: center;">radon</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Questions with only [correctly] linked entities</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">iii</td>
<td style="text-align: center;">Who directed the 2011 [Palme d'Or] winning film '[The Tree Of Life]'?</td>
<td style="text-align: center;">Terence Malick</td>
<td style="text-align: center;">ang lee</td>
<td style="text-align: center;">Terrence Malick</td>
</tr>
<tr>
<td style="text-align: center;">iv</td>
<td style="text-align: center;">Name the year: [Hirohito] dies; The [Exxon Valdez] runs aground; [San Francisco] suffers its worst earthquake since 1906.</td>
<td style="text-align: center;">1989</td>
<td style="text-align: center;">1989</td>
<td style="text-align: center;">1990s</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Questions with {incorrectly} linked entities</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">v</td>
<td style="text-align: center;">Which car manufacturer produces the ${$ Jimmy $}$ model?</td>
<td style="text-align: center;">Suzuki</td>
<td style="text-align: center;">suzuki</td>
<td style="text-align: center;">Brixham</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Jimmy $\rightarrow$ Marcos Engineering Q1637323</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">vi</td>
<td style="text-align: center;">Where do you find the ${$ Bridal Veil $}$, [American], and [Horseshoe Falls]?</td>
<td style="text-align: center;">Niagara falls</td>
<td style="text-align: center;">niagra Falls</td>
<td style="text-align: center;">Niagara Falls</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Bridal Veil $\rightarrow$ Veil (Garment) Q6497446</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 10: Additional examples of question, answer and model predictions for the TriviaQA Unfiltered dev set.</p>
<p>Examples (v) and (vi) shows other failure modes when EAE fails to correctly predict the entities in the question. In example (vi) the entity is not available, though this not cause the model to err, likely thanks to the two other correctly predicted question entities. Example's (vi) typo (from 'Jimny' to 'Jimmy') is particularly interesting: when fixed, EAE links 'Jimny' correctly and predicts the right answer.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{13}$ https://competitions.codalab.org/ competitions/17208&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{9}$ Our method follows (Baldini Soares et al., 2019) in using special tokens to mark subject and object, and concatenating their representations to model the relation.
${ }^{10}$ The weighted split weights examples by its difficulty, focusing on correctly predicting difficult examples.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
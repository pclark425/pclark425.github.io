<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5198 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5198</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5198</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-270371247</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.05365v2.pdf" target="_blank">CaLM: Contrasting Large and Small Language Models to Verify Grounded Generation</a></p>
                <p><strong>Paper Abstract:</strong> Grounded generation aims to equip language models (LMs) with the ability to produce more credible and accountable responses by accurately citing verifiable sources. However, existing methods, by either feeding LMs with raw or preprocessed materials, remain prone to errors. To address this, we introduce CaLM, a novel verification framework. CaLM leverages the insight that a robust grounded response should be consistent with information derived solely from its cited sources. Our framework empowers smaller LMs, which rely less on parametric memory and excel at processing relevant information given a query, to validate the output of larger LMs. Larger LM responses that closely align with the smaller LMs' output, which relies exclusively on cited documents, are verified. Responses showing discrepancies are iteratively refined through a feedback loop. Experiments on three open-domain question-answering datasets demonstrate significant performance gains of 1.5% to 7% absolute average without any required model fine-tuning.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5198.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5198.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CaLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrasting Large and Small Language Models to Verify Grounded Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative post-verification framework that uses a large 'main' LM to generate answers with citations and a smaller 'verifier' LM that sees only the cited documents to check consistency; inconsistent segments are removed and the main LM is prompted to correct them using additional retrieved documents in subsequent iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo-1106 / text-unicorn2 (main); tulu-2-dpo (verifier, 13B used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Main LMs experimented with: GPT-3.5-Turbo-1106 (OpenAI API model) and a PaLM-based model (text-unicorn2). Verifier LM: tulu-2-dpo (13B) and other sizes of Tulu family were tested as verifier variants.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Iterative verification with small-LM verifier (CaLM iterative verification)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Process: (1) retrieve top-k documents, (2) main LM generates answer A with citations from those k docs, (3) verifier (smaller) LM receives only the cited evidence set G and re-generates an answer A', (4) compare A and A' via ROUGE-2; if ROUGE-2 >= threshold θ accept A, otherwise extract the intersection Ā = A ∩ A' and corresponding verified citations Ḡ, augment Ḡ with additional retrieved passages and re-prompt the main LM to correct the remaining content; iterate up to a max iteration T. Key design choices: verifier only sees cited docs (not the full retrieval pool); consistency measured by ROUGE-2 with θ typically tuned (experimentally θ values between 0.2–0.5; θ=0.25 used for ELI5, θ=0.5 for ASQA in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ASQA, QAMPARI, ELI5 (open-domain grounded QA / long-form / multi-answer benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-domain question answering with grounded generation: ASQA (ambiguous long-form QA requiring multiple short answers), QAMPARI (multiple-entity answers from Wikipedia tables/graph), and ELI5 (long-form explanations requiring claim-level correctness).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>QAMPARI: With CaLM (main=GPT-3.5-Turbo-1106) average = 34.30 (Table 3). With CaLM (main=text-unicorn2) average = 34.72 (Table 3). ASQA: paper reports CaLM yields an average improvement of over 6% when using text-unicorn (exact table values in paper). ELI5: paper reports CaLM obtains the best average performance across main LMs and notably improved citation quality (exact per-model numbers are reported in Table 4 of the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>QAMPARI baselines: ICLCite average = 33.05 (GPT-3.5-Turbo-1106) and 28.86 (text-unicorn2) (Table 3). Other baselines (Summ+ICLCite, Snippet+ICLCite, ICLCite+USC) are reported in Tables 2–4 for the datasets; CaLM improvements are reported relative to these baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: On QAMPARI CaLM improves average metric from 33.05 to 34.30 (+1.25) using GPT-3.5 and from 28.86 to 34.72 (+5.86) using text-unicorn (Table 3). On ASQA CaLM reports >6% average improvement with text-unicorn. Ablations: Table 5 shows verifier choice increases average performance (e.g., average from 25.58 to 27.49 when using 'only cited documents' verifier design); Table 6 ablation shows removing the constraint that the verifier only sees cited documents causes a notable drop in performance, especially citation quality. Iteration study (Fig.4): precision for correctness rises across iterations and plateaus after ~3 iterations; extending iterations from 3 to 6 yields marginal average gains (~0.4) but higher cost. Qualitative case studies (Figs.5–6) show verifier can catch citation errors and find better evidence in later rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations: added latency and higher token usage due to post-verification and re-processing of documents; diminishing returns after ~2–3 iterations (plateau); sensitivity to choice of ROUGE-2 threshold θ; verifier and main LM can both hallucinate leading to jointly-verified but incorrect outputs; smaller verifier LMs are more sensitive to irrelevant evidence and can produce erroneous results when given noisy citations (paper motivates small-LM sensitivity but notes susceptibility to errors when evidence is irrelevant). The authors also note that maximum iteration T is required to avoid infinite loops and that when T is reached the last candidate is output (no guarantee of full verification).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CaLM: Contrasting Large and Small Language Models to Verify Grounded Generation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5198.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5198.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-reflective Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that augments retrieval-augmented generation with self-reflection mechanisms by finetuning models to emit special tokens that trigger additional retrieval and fact-checking steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-reflective retrieval augmented generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Finetuned LM (as used in Asai et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Self-RAG requires finetuning LMs to produce special control tokens that cause the model to initiate further retrieval/fact-checking; the exact backbone model depends on the original Self-RAG experiments (not finetuned in this CaLM paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-reflection via finetuned retrieval triggers</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The method trains the LM to generate control tokens that indicate uncertain or check-worthy content; these tokens trigger extra retrieval and verification rounds, effectively performing model-internal iterative checks. Requires model finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ASQA (reported in paper via original Self-RAG numbers)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Self-RAG was evaluated on grounded QA tasks in its original work; in this paper Self-RAG is referenced as a related method and original results are quoted for ASQA only.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported in the original Self-RAG paper; this CaLM paper does not re-run Self-RAG except to report original ASQA numbers (exact numeric values from Self-RAG are not reproduced in full within this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Mentioned as prior work that improves factuality via self-reflection plus retrieval; CaLM claims to outperform Self-RAG without requiring finetuning, but the CaLM paper does not provide a direct head-to-head reimplementation comparison (it reports Self-RAG numbers from the original paper for ASQA only).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires model finetuning and associated training infrastructure; may have generalization issues and training cost (noted in the CaLM paper's related-work discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CaLM: Contrasting Large and Small Language Models to Verify Grounded Generation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5198.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5198.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-reflection (Ji et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Towards mitigating LLM hallucination via self reflection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting-based approach where LMs are asked to self-reflect on their outputs to detect and mitigate hallucinations, reported to improve factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards mitigating LLM hallucination via self reflection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting/self-reflection techniques applied to existing LMs (various backbones in original work).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-reflection prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The LM is prompted to analyze or critique its own answer (self-critique) and then revise the output; typically single-model, generate-then-reflect style prompting without requiring a separate verifier model.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>General factuality/hallucination mitigation tasks (paper cited as prior work improving factuality).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Cited in related work: the CaLM paper notes that prompting LLMs to self-reflect has been shown to improve factuality (Ji et al., 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not detailed in CaLM beyond the citation; general limitations of self-reflection prompting in prior work include dependence on prompt design and limited ability to recover from incorrect external evidence, as discussed in literature.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CaLM: Contrasting Large and Small Language Models to Verify Grounded Generation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5198.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5198.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICLCite + USC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-Context Learning with Citation (ICLCite) plus Universal Self-Consistency (USC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that first performs in-context citation-based generation and then applies the Universal Self-Consistency aggregation (generate many outputs and pick consensus) as a post-processing step.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Universal self-consistency for large language model generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo-1106 / text-unicorn2 (as used in baseline runs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ICLCite uses LLMs prompted with retrieved documents (five provided) to produce cited answers; USC is the sampling-and-vote aggregation technique of Chen et al. (2023).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Universal self-consistency (sampling and consensus)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate multiple answer samples (e.g., n=9), then aggregate via self-consistency voting to obtain a final output. This is a post-processing ensembling-style approach rather than a document-grounded verifier loop.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ASQA, QAMPARI, ELI5 (used as baseline comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same grounded QA benchmarks; USC aims to improve robustness by majority consensus across multiple LLM samples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported in paper as baseline (ICLCite + USC): QAMPARI average for GPT-3.5-Turbo-1106 = 27.92 (Table 3), for text-unicorn2 = 27.57 (Table 3). These numbers are lower than the plain ICLCite baseline in some settings (the paper reports ICLCite baseline values).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>ICLCite (no USC) reported averages: QAMPARI GPT-3.5-Turbo-1106 = 33.05; text-unicorn2 = 28.86 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>In this paper USC did not consistently improve the ICLCite baseline; the CaLM paper reports that ICLCite+USC sometimes underperforms ICLCite alone on these grounded-generation tasks (see Table 3), indicating self-consistency does not always help when grounding/citation quality is the main challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>CaLM authors note that self-consistency aggregation can fail to improve (or can hurt) performance in grounded generation settings, likely because sampling consensus does not correct citation errors or hallucinations induced by noisy retrieval inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CaLM: Contrasting Large and Small Language Models to Verify Grounded Generation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-reflective retrieval augmented generation <em>(Rating: 2)</em></li>
                <li>Towards mitigating LLM hallucination via self reflection <em>(Rating: 2)</em></li>
                <li>Universal self-consistency for large language model generation <em>(Rating: 2)</em></li>
                <li>Enabling large language models to generate text with citations <em>(Rating: 2)</em></li>
                <li>RARR: Researching and revising what language models say, using language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5198",
    "paper_id": "paper-270371247",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "CaLM",
            "name_full": "Contrasting Large and Small Language Models to Verify Grounded Generation",
            "brief_description": "An iterative post-verification framework that uses a large 'main' LM to generate answers with citations and a smaller 'verifier' LM that sees only the cited documents to check consistency; inconsistent segments are removed and the main LM is prompted to correct them using additional retrieved documents in subsequent iterations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo-1106 / text-unicorn2 (main); tulu-2-dpo (verifier, 13B used in experiments)",
            "model_description": "Main LMs experimented with: GPT-3.5-Turbo-1106 (OpenAI API model) and a PaLM-based model (text-unicorn2). Verifier LM: tulu-2-dpo (13B) and other sizes of Tulu family were tested as verifier variants.",
            "reflection_method_name": "Iterative verification with small-LM verifier (CaLM iterative verification)",
            "reflection_method_description": "Process: (1) retrieve top-k documents, (2) main LM generates answer A with citations from those k docs, (3) verifier (smaller) LM receives only the cited evidence set G and re-generates an answer A', (4) compare A and A' via ROUGE-2; if ROUGE-2 &gt;= threshold θ accept A, otherwise extract the intersection Ā = A ∩ A' and corresponding verified citations Ḡ, augment Ḡ with additional retrieved passages and re-prompt the main LM to correct the remaining content; iterate up to a max iteration T. Key design choices: verifier only sees cited docs (not the full retrieval pool); consistency measured by ROUGE-2 with θ typically tuned (experimentally θ values between 0.2–0.5; θ=0.25 used for ELI5, θ=0.5 for ASQA in experiments).",
            "num_iterations": 4,
            "task_name": "ASQA, QAMPARI, ELI5 (open-domain grounded QA / long-form / multi-answer benchmarks)",
            "task_description": "Open-domain question answering with grounded generation: ASQA (ambiguous long-form QA requiring multiple short answers), QAMPARI (multiple-entity answers from Wikipedia tables/graph), and ELI5 (long-form explanations requiring claim-level correctness).",
            "performance_with_reflection": "QAMPARI: With CaLM (main=GPT-3.5-Turbo-1106) average = 34.30 (Table 3). With CaLM (main=text-unicorn2) average = 34.72 (Table 3). ASQA: paper reports CaLM yields an average improvement of over 6% when using text-unicorn (exact table values in paper). ELI5: paper reports CaLM obtains the best average performance across main LMs and notably improved citation quality (exact per-model numbers are reported in Table 4 of the paper).",
            "performance_without_reflection": "QAMPARI baselines: ICLCite average = 33.05 (GPT-3.5-Turbo-1106) and 28.86 (text-unicorn2) (Table 3). Other baselines (Summ+ICLCite, Snippet+ICLCite, ICLCite+USC) are reported in Tables 2–4 for the datasets; CaLM improvements are reported relative to these baselines.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: On QAMPARI CaLM improves average metric from 33.05 to 34.30 (+1.25) using GPT-3.5 and from 28.86 to 34.72 (+5.86) using text-unicorn (Table 3). On ASQA CaLM reports &gt;6% average improvement with text-unicorn. Ablations: Table 5 shows verifier choice increases average performance (e.g., average from 25.58 to 27.49 when using 'only cited documents' verifier design); Table 6 ablation shows removing the constraint that the verifier only sees cited documents causes a notable drop in performance, especially citation quality. Iteration study (Fig.4): precision for correctness rises across iterations and plateaus after ~3 iterations; extending iterations from 3 to 6 yields marginal average gains (~0.4) but higher cost. Qualitative case studies (Figs.5–6) show verifier can catch citation errors and find better evidence in later rounds.",
            "limitations_or_failure_cases": "Reported limitations: added latency and higher token usage due to post-verification and re-processing of documents; diminishing returns after ~2–3 iterations (plateau); sensitivity to choice of ROUGE-2 threshold θ; verifier and main LM can both hallucinate leading to jointly-verified but incorrect outputs; smaller verifier LMs are more sensitive to irrelevant evidence and can produce erroneous results when given noisy citations (paper motivates small-LM sensitivity but notes susceptibility to errors when evidence is irrelevant). The authors also note that maximum iteration T is required to avoid infinite loops and that when T is reached the last candidate is output (no guarantee of full verification).",
            "uuid": "e5198.0",
            "source_info": {
                "paper_title": "CaLM: Contrasting Large and Small Language Models to Verify Grounded Generation",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Self-RAG",
            "name_full": "Self-reflective Retrieval-Augmented Generation",
            "brief_description": "A method that augments retrieval-augmented generation with self-reflection mechanisms by finetuning models to emit special tokens that trigger additional retrieval and fact-checking steps.",
            "citation_title": "Self-reflective retrieval augmented generation",
            "mention_or_use": "mention",
            "model_name": "Finetuned LM (as used in Asai et al., 2023)",
            "model_description": "Self-RAG requires finetuning LMs to produce special control tokens that cause the model to initiate further retrieval/fact-checking; the exact backbone model depends on the original Self-RAG experiments (not finetuned in this CaLM paper).",
            "reflection_method_name": "Self-reflection via finetuned retrieval triggers",
            "reflection_method_description": "The method trains the LM to generate control tokens that indicate uncertain or check-worthy content; these tokens trigger extra retrieval and verification rounds, effectively performing model-internal iterative checks. Requires model finetuning.",
            "num_iterations": null,
            "task_name": "ASQA (reported in paper via original Self-RAG numbers)",
            "task_description": "Self-RAG was evaluated on grounded QA tasks in its original work; in this paper Self-RAG is referenced as a related method and original results are quoted for ASQA only.",
            "performance_with_reflection": "Reported in the original Self-RAG paper; this CaLM paper does not re-run Self-RAG except to report original ASQA numbers (exact numeric values from Self-RAG are not reproduced in full within this paper).",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "evidence_of_improvement": "Mentioned as prior work that improves factuality via self-reflection plus retrieval; CaLM claims to outperform Self-RAG without requiring finetuning, but the CaLM paper does not provide a direct head-to-head reimplementation comparison (it reports Self-RAG numbers from the original paper for ASQA only).",
            "limitations_or_failure_cases": "Requires model finetuning and associated training infrastructure; may have generalization issues and training cost (noted in the CaLM paper's related-work discussion).",
            "uuid": "e5198.1",
            "source_info": {
                "paper_title": "CaLM: Contrasting Large and Small Language Models to Verify Grounded Generation",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Self-reflection (Ji et al., 2023)",
            "name_full": "Towards mitigating LLM hallucination via self reflection",
            "brief_description": "A prompting-based approach where LMs are asked to self-reflect on their outputs to detect and mitigate hallucinations, reported to improve factuality.",
            "citation_title": "Towards mitigating LLM hallucination via self reflection",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "Prompting/self-reflection techniques applied to existing LMs (various backbones in original work).",
            "reflection_method_name": "Self-reflection prompting",
            "reflection_method_description": "The LM is prompted to analyze or critique its own answer (self-critique) and then revise the output; typically single-model, generate-then-reflect style prompting without requiring a separate verifier model.",
            "num_iterations": null,
            "task_name": "",
            "task_description": "General factuality/hallucination mitigation tasks (paper cited as prior work improving factuality).",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "evidence_of_improvement": "Cited in related work: the CaLM paper notes that prompting LLMs to self-reflect has been shown to improve factuality (Ji et al., 2023).",
            "limitations_or_failure_cases": "Not detailed in CaLM beyond the citation; general limitations of self-reflection prompting in prior work include dependence on prompt design and limited ability to recover from incorrect external evidence, as discussed in literature.",
            "uuid": "e5198.2",
            "source_info": {
                "paper_title": "CaLM: Contrasting Large and Small Language Models to Verify Grounded Generation",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ICLCite + USC",
            "name_full": "In-Context Learning with Citation (ICLCite) plus Universal Self-Consistency (USC)",
            "brief_description": "A baseline that first performs in-context citation-based generation and then applies the Universal Self-Consistency aggregation (generate many outputs and pick consensus) as a post-processing step.",
            "citation_title": "Universal self-consistency for large language model generation",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo-1106 / text-unicorn2 (as used in baseline runs)",
            "model_description": "ICLCite uses LLMs prompted with retrieved documents (five provided) to produce cited answers; USC is the sampling-and-vote aggregation technique of Chen et al. (2023).",
            "reflection_method_name": "Universal self-consistency (sampling and consensus)",
            "reflection_method_description": "Generate multiple answer samples (e.g., n=9), then aggregate via self-consistency voting to obtain a final output. This is a post-processing ensembling-style approach rather than a document-grounded verifier loop.",
            "num_iterations": null,
            "task_name": "ASQA, QAMPARI, ELI5 (used as baseline comparisons)",
            "task_description": "Same grounded QA benchmarks; USC aims to improve robustness by majority consensus across multiple LLM samples.",
            "performance_with_reflection": "Reported in paper as baseline (ICLCite + USC): QAMPARI average for GPT-3.5-Turbo-1106 = 27.92 (Table 3), for text-unicorn2 = 27.57 (Table 3). These numbers are lower than the plain ICLCite baseline in some settings (the paper reports ICLCite baseline values).",
            "performance_without_reflection": "ICLCite (no USC) reported averages: QAMPARI GPT-3.5-Turbo-1106 = 33.05; text-unicorn2 = 28.86 (Table 3).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "In this paper USC did not consistently improve the ICLCite baseline; the CaLM paper reports that ICLCite+USC sometimes underperforms ICLCite alone on these grounded-generation tasks (see Table 3), indicating self-consistency does not always help when grounding/citation quality is the main challenge.",
            "limitations_or_failure_cases": "CaLM authors note that self-consistency aggregation can fail to improve (or can hurt) performance in grounded generation settings, likely because sampling consensus does not correct citation errors or hallucinations induced by noisy retrieval inputs.",
            "uuid": "e5198.3",
            "source_info": {
                "paper_title": "CaLM: Contrasting Large and Small Language Models to Verify Grounded Generation",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-reflective retrieval augmented generation",
            "rating": 2,
            "sanitized_title": "selfreflective_retrieval_augmented_generation"
        },
        {
            "paper_title": "Towards mitigating LLM hallucination via self reflection",
            "rating": 2,
            "sanitized_title": "towards_mitigating_llm_hallucination_via_self_reflection"
        },
        {
            "paper_title": "Universal self-consistency for large language model generation",
            "rating": 2,
            "sanitized_title": "universal_selfconsistency_for_large_language_model_generation"
        },
        {
            "paper_title": "Enabling large language models to generate text with citations",
            "rating": 2,
            "sanitized_title": "enabling_large_language_models_to_generate_text_with_citations"
        },
        {
            "paper_title": "RARR: Researching and revising what language models say, using language models",
            "rating": 1,
            "sanitized_title": "rarr_researching_and_revising_what_language_models_say_using_language_models"
        }
    ],
    "cost": 0.016253749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CaLM: Contrasting Large and Small Language Models to Verify Grounded Generation</p>
<p>I-Hung Hsu <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#105;&#104;&#117;&#110;&#103;&#104;&#115;&#117;&#64;&#117;&#115;&#99;&#46;&#101;&#100;&#117;">&#105;&#104;&#117;&#110;&#103;&#104;&#115;&#117;&#64;&#117;&#115;&#99;&#46;&#101;&#100;&#117;</a> 
University of Southern California</p>
<p>Zifeng Wang 
Google Cloud AI Research</p>
<p>Long T Le 
Google Cloud AI Research</p>
<p>Lesly Miculicich 
Google Cloud AI Research</p>
<p>Nanyun Peng 
University of California
Los Angeles</p>
<p>Chen-Yu Lee <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#99;&#104;&#101;&#110;&#121;&#117;&#108;&#101;&#101;&#64;&#103;&#111;&#111;&#103;&#108;&#101;&#46;&#99;&#111;&#109;">&#99;&#104;&#101;&#110;&#121;&#117;&#108;&#101;&#101;&#64;&#103;&#111;&#111;&#103;&#108;&#101;&#46;&#99;&#111;&#109;</a> 
Google Cloud AI Research</p>
<p>Tomas Pfister 
Google Cloud AI Research</p>
<p>Google Cloud AI Research</p>
<p>Chen-Yu Lee</p>
<p>CaLM: Contrasting Large and Small Language Models to Verify Grounded Generation
1BB7444D8B26C3557EA39AC4782B10A3
Grounded generation aims to equip language models (LMs) with the ability to produce more credible and accountable responses by accurately citing verifiable sources.However, existing methods, by either feeding LMs with raw or preprocessed materials, remain prone to errors.To address this, we introduce CaLM, a novel verification framework.CaLM leverages the insight that a robust grounded response should be consistent with information derived solely from its cited sources.Our framework empowers smaller LMs, which rely less on parametric memory and excel at processing relevant information given a query, to validate the output of larger LMs.Larger LM responses that closely align with the smaller LMs' output, which relies exclusively on cited documents, are verified.Responses showing discrepancies are iteratively refined through a feedback loop.Experiments on three open-domain questionanswering datasets demonstrate significant performance gains of 1.5% to 7% absolute average without any required model fine-tuning.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) are increasingly popular tools for information seeking.A recent trend emphasizes integrating citations from verifiable sources to boost LLM credibility and enable user verification.This aims to reduce hallucinations and increase accountability (Gao et al., 2023b;Huang and Chang, 2023;Liu et al., 2023b).To achieve this, LLMs must not only identify relevant documents within vast retrieved collections but also accurately ground their responses in these sources and effectively generate their responses.</p>
<p>Achieving accurate responses and precise citations in a single attempt can be challenging due to the potential irrelevance of input documents.</p>
<p>Error and hallucination can propagate.This significantly increases the complexity of LLM operations (Gao et al., 2023b).</p>
<p>A standard approach to achieve this grounded generation is by retrieval-augmented generation with instructions to guide LLMs to generate responses along with their corresponding sources in one single LLM inference run (see Fig. 1 (a)).More recently, more sophisticated approaches utilize LLMs to first summarize relevant documents (Gao et al., 2023b) or use key information extraction and algorithms that explore different relevant document combinations by asking LLMs to enrich the original input query with additional information arXiv:2406.05365v2[cs.CL] 24 Jun 2024 (Li et al., 2023) (see Fig. 1 (b)).</p>
<p>However, both single-run (Fig. 1 (a)) and preprocessing (Fig. 1 (b)) strategies face challenges for accurate generation and citation.Single-run approaches require LLMs to process the input query and a potential large volume of retrieved documents in one forward pass, which can strain their capabilities.Preprocessing approaches, while more focused, risk error propagation or loss of information.Additionally, both strategies limit the LLM's ability to iterate, refine, and verify responses, impacting citation accuracy and answer correctness.</p>
<p>In contrast to single-run and preprocessing strategies, we propose a novel post-verification approach that enables LLMs to fact-check and ground their responses.Our design leverages the complementary strengths of larger and smaller LMs.We observe that larger LMs excel at identifying relevant information within a vast corpus but can rely excessively on internal parametric memory during generation.Smaller LMs, however, are adept at processing retrieved relevant information but less capable of identifying it from large collections (see § 3.3 for details).</p>
<p>Building on these observations, we propose CaLM (Contrasting Large and sMall language models to verify grounded generation).CaLM validates the large LLM's response by crossreferencing it with output from a smaller LM.The smaller LM scrutinizes the cited documents to confirm the large LLM's citation accuracy.If the responses align, the large LLM's answer is verified.If not, CaLM extracts useful statements and evidence from the large LLM's response and seeks additional supporting information to improve the query response.Importantly, CaLM requires no model fine-tuning, allowing smaller LMs to significantly enhance the grounded generation capabilities of large LMs.Fig. 3 illustrates this process.</p>
<p>We conduct experiments on three open-domain question answering datasets (QAMPARI, ASQA, and ELI5), which require consulting multiple sources for comprehensive answers.Our method demonstrates significant improvements in both answer accuracy and citation quality, outperforming state-of-the-art methods by an average of 1.5% to 7%.Crucially, our method remains robust even in challenging scenarios with less powerful retrieval systems, while other baselines often struggle.</p>
<p>Problem Statement</p>
<p>Task Setup.</p>
<p>We cope with the problem of grounded generation (Gao et al., 2023b).Given a query q and a corpus of trustworthy text passages D, the model needs to generate an answer response A, which consists of n statements s 1 , s 2 , ...s n , based on the knowledge in D. Each statement s i cites a list of passages C i = {c 1 i , c 2 i , ...}, ∀c j i ∈ D. The collective sets C i for i = 1, 2, . . ., n constitute the grounded evidence G, from which A is derived.Our goal is to jointly optimize the usefulness of A to q, the preciseness of C i for statement s i , and the integrity of G to adequately support A.</p>
<p>Evaluation of Response.The task involves measuring three dimensions of system responses, following the setup from Gao et al. (2023b).</p>
<p>• Fluency: Determining whether the model's generated text A is fluent and coherent.• Correctness: Assessing if A is accurate and covers all relevant aspects of query q. • Citation Quality: Evaluating whether cited passages directly support the answer and avoids irrelevant citations.This is achieved by evaluating both citation recall and citation precision.Gao et al. (2023b) propose measuring citation quality by averaging scores for each statement s i .Citation recall ensures there is at least one supporting citation c j i for s i .Citation precision measures whether all the citations are "relevant".Specifically, a citation c j i is considered "irrelevant" to statement s i if c j i cannot support s i , and removing c j i from C i would not impact the overall support for s i from the remaining citations.</p>
<p>Automated Verification for Grounded Generation</p>
<p>Although LLMs have demonstrated proficiency in a wide range of tasks, they remain susceptible to generating hallucinations (Huang et al., 2023;Zhang et al., 2023).These hallucinations could occur in both answers and citations within grounded generations due to the high complexity of the entire working pipeline, which includes noise from the retrievers and the limited ability of LLMs to handle long contexts (Liu et al., 2023a).This issue underscores the critical need for verification mechanisms to ensure the quality of the generated output and to leverage the interplay between verification and generation systems to improve the final output A.</p>
<p>In this section, we first analyzes key factors to verify grounded generation ( § 3.1).Subsequently, we introduce an automated and unsupervised verification method for grounded generation using a small LM as a verifier and contrasting results from large and small LMs to verify large LMs' response ( § 3.2 &amp; 3.3).</p>
<p>Key Factors for Automated Verification</p>
<p>Automated verification, unlike the task evaluation in § 2, operates without a ground-truth reference and should be efficient for real-time system feedback.Here, our focus lies on assessing answer correctness and citation quality.To evaluate the correctness of a generated grounded response, we must ensure that generated responses (A) faithfully leverage information from the knowledge base D, avoiding hallucinations or model biases.Additionally, correct reasoning in deriving the answer is also crucial.For the automatic evaluation of citation quality, a trained Natural Language Inference (NLI) model can assess each citation and statement pair iteratively to measure the citation's fidelity (Gao et al., 2023b).However, this process can be computationally expensive for lengthy generated answers with numerous citations.Efficient automated verification for grounded generation must consider these factors.</p>
<p>Contrasting Large and Small LMs for Automated Verification</p>
<p>We propose a verification method using a smaller LM to assess the quality of a larger LM's grounded generation.The small LM receives only the large LM's cited documents (G) as input to answer the same query q.Consistency between their responses indicates the quality of G and the grounded generation from the large LM.</p>
<p>Our design exploits the inherent characteristics of smaller LMs.We posit that a robust G should enable even small LMs to deduce the correct answer.Notably, small LMs, having fewer parameters, are demonstrably more receptive to integrating external knowledge (Xie et al., 2023).Reaching consistent results from both LMs indicate high answer fidelity.1 Leveraging different LMs as support also reduce the reasoning error risks as the different LMs exhibit diverse strengths and reasoning mechanisms (Jiang et al., 2023).These characteristics of small LMs makes our design effective for verifying the answer correctness.</p>
<p>Furthermore, as will detailed in § 3.3, smaller LMs are more sensitive to the relevance of input evidence.Irrelevant documents in the evidence set G can easily mislead small LMs, while missing crucial citations hinder their ability to reach the correct answer independently, due to their limited parametric knowledge.This sensitivity allows us to utilize small LMs for assessing the quality of G.</p>
<p>Analyzing Model Size Impact on LMs'</p>
<p>Sensitivity to Input Document Relevance</p>
<p>Our automated verification method exploits the high sensitivity of small LMs.This section empirically demonstrates that, within the same model family, smaller LMs exhibit greater sensitivity to the relevance of input documents to a given query q compared to their larger counterparts.</p>
<p>We investigate the sensitivity of LMs to the relevance of input documents by modeling the performance of an LM as a function of the input document's relevance.To mitigate the effects of the divergent abilities for different LMs to follow instructions, we examine the relative performance improvement of the model response as the relevance of the input document increases.The larger this value is, the more sensitive the LM is.</p>
<p>Yet, the complexity of this function is beyond simple linearity.As depicted in Fig. 2(a), this function typically exhibits a monotonic increase and hence we intend to additionally study second-order improvements gain analysis to further study the curvature of the function.This analysis use three anchoring points x low , x med , x high , and the corresponding performance P (x low ), P (x med ), P (x high ) to study the much incremental performance gain the LM can get when the input document's relevance keep increasing.Specifically, we use the ratio P (x high )−P (x med ) P (x med )−P (x low ) to represent the incremental performance gain.A higher ratio indicates the LM is more sensitive to input document's relevance, as illustrated by the orange line in Fig. 2(a).</p>
<p>We conduct empirical studies using a retrievalaugmented generation setting on the ASQA dataset (Stelmakh et al., 2022).Each instance in the dataset contains multiple answers and requires reading multiple documents to make correct prediction.In this experiment, a LM utilizes five input documents d to answer queries q, with model per-   Function with larger second order relative improvement rate
(𝑥!"#, 𝑃 𝑥!"# ) (𝑥 $%&amp; , 𝑃 𝑥 $%&amp; ) (𝑥 '()' , 𝑃 𝑥 '()' )
Function with lower second order relative improvement rate</p>
<p>Relative Improvement:
! " <em>+,</em> #!(" -./ ) !("-./)
2 nd Order Relative Improvement:</p>
<p>! " <em>+,</em> #!("012)</p>
<p>! "012 #!("-./)</p>
<p>Figure 2: The studies of the performance of an LM as a function of the input document's relevance score using the ASQA dataset.We show that, within the same LM family, smaller LMs demonstrate higher sensitivity to the relevance of the input document, when anchored to the largest model in the family.(a) The illustration of the function.This function is a monotonic increase function as the accuracy always increase when input document's relevance score increase.Hence, studying the second order relative improvements can help us know the incremental performance gain for the LM when the input document's relevance keep increasing.(b) The result of relative improvement.(c) The result of the second order relative improvement analysis.From (b)(c), we can observe that smaller models tend to exhibit greater relative improvements and achieve larger incremental performance gains compared to their larger counterparts.</p>
<p>formance evaluated based on answer accuracy, and the relevance of the input documents d is assessed by their average recall rate in containing answers.We prepare three different set of d with approximately 27%, 56%, and 78% recall rates, respectively, to represents x low , x med , x high in Fig. 2(a).Fig. 2(b) presents the results of relative performance improvement, while Fig. 2(c) shows the second-order analysis.Our study encompasses LM families such as Yi-LM, Vicuna (Zheng et al., 2023), WizardLM (Xu et al., 2023), and Tulu-2dpo (Ivison et al., 2023).The findings indicate that within the same LM family, compared to the largest model, smaller LMs usually achieve higher relative improvement and receive a greater incremental performance gain, suggesting their higher sensitivity to the relevance of input documents to the query q.On the other hand, smaller LMs are more prone to erroneous results and suboptimal performance with irrelevant documents, which motivates our automated verification method design.</p>
<p>CaLM Framework</p>
<p>Building on the automated verification design from § 3.2, we introduce CaLM, an inference framework that leverages the synergy between verification and generation systems to enhance the system's final grounded generation.</p>
<p>Fig. 3 depicts the iterative five-step algorithm of our CaLM framework.Steps three and four correspond to the automated verification method detailed in § 3.2.The remaining steps involve the large LM making predictions.</p>
<p>We now present a detailed breakdown of each step below.We differentiate between the "main LM," whose responses are verified, and the "verifier LM," an auxiliary LM assisting the verification process.As described in § 3.2, we recommend using a larger LM as the main LM and a smaller LM for verification to achieve optimal performance.</p>
<p>Step (1) Context Retrieval.CaLM starts by selecting a ranked pool of trustworthy passages p for a given query q using a retriever.</p>
<p>Step (2) Main LM Generation.We select the top-k documents from the reference pool p and feed them into the main LM.This value of k is a hyperparameter constrained by the maximum input capacity of the main LM.The LM then analyzes these k passages to generate an answer candidate A for the query q.Our findings in § 3.3 suggest employing a large-scale LM at this stage.This is because larger LMs exhibit greater robustness in accurately identifying useful information and filtering out noise within the retrieved passages.</p>
<p>Step (1): Retrieve document pool p given query.</p>
<p>Step (2): Main LM generate response with citations based on input documents.</p>
<p>Step (3) Verifier LM takes the cited documents from Main LM to generate another response.</p>
<p>Step (4) Contrast Main LM and Verifier LM's responses to produce verified statements and documents, discarding unverified ones.</p>
<p>Output if the answers are consistent.</p>
<p>Step ( 5 Step (5)</p>
<p>New input for next step 2</p>
<p>Write an accurate and concise answer using the provided search results and cite them properly.</p>
<p>Prompt Prompt Verifier Output</p>
<p>The Brotherhood of the Bell [1]</p>
<p>[Doc 1]</p>
<p>Write an accurate and … Additionally, you will be provided with an incomplete draft solution, which is based on the first search results… New prompt for next step 2  1)).Then, the main language model (LM) takes the first batch of documents and employs retrieval-augmented generation to produce an answer candidate, which cites relevant supporting documents (Step (2)).Subsequently, this candidate is validated by contrasting it with the verifier output from the verifier LM (Steps ( 3) &amp; ( 4)).Our verifier LM evaluates citation quality by accessing only the documents cited by the main LM's response, rather than the same input documents.For responses with sufficient consistency, we accept the answer candidate directly.If inconsistent, we break down the answer candidate into individual statements, retaining only those corroborated by similar arguments in the verifier output for further correction in next iteration (Step ( 5)).</p>
<p>Step (3) Verifier LM Generation.Building upon the automated verification design outlined in § 3.2, this step leverages a grounded evidence set G extracted from the main LM's answer candidate A. A smaller, dedicated verification LM then re-attempt the grounded generation process and obtain verifier output A ′ for query q by solely providing the model with the evidence set, G, rather than the full top-k documents.</p>
<p>Step (4) Contrasting Answer Candidate and Verifier Output: A strong evidence G should enable small LMs to deduce correct answers reliably.Our goal in this step is to verify consistency (Wang et al., 2023) of A ′ against the answer candidate A.</p>
<p>If the comparison shows enough consistency, we accept the answer A and stop the iterative process.Otherwise, we dismiss the inconsistent segments from the answer and citations, and continue with the next iteration.More technically, we extract Ā = A ∩ A ′ and the corresponding Ḡ within Ā for</p>
<p>Step (5) usage.</p>
<p>The realization of consistency measurement is done via calculating ROUGE-2 score between the A ′ and A. If the ROUGE-2 score exceeds a threshold θ, the answer candidate is considered acceptable.This threshold can be tuned using a small development set.Empirically, by our observation, setting θ = 0.2 to 0.5 yields satisfactory results by small set of qualitative examination.</p>
<p>Step (5) Input Preparation for Next Iteration:</p>
<p>In this final step, we prepare input for the next iteration, including input reference document lists and the draft for correction.The new input reference document lists is initialized by Ḡ and is supplemented more passages from the next batch of passages from pool p until complete the budget k.This process boosts the likelihood of finding relevant documents while maintaining useful documents that have been verified.Then, we create a new prompt for large LM focusing on leveraging the new input reference document lists to correct the incomplete answer response Ā.</p>
<p>The full prompt we use for each step are detailed in Appx.§B.In practice, we should set a maximum iteration T to halt the whole process to prevent the verification condition cannot be satisfied.If this maximum iteration is reached, we will output the last answer candidate we get from the main LM.</p>
<p>Experimental Setup</p>
<p>We consider three factoid question-answering (QA) datasets.For each dataset, we present an illustrative example in Tab. 1 for better understanding.We prepare the trustworthy text passages D for each dataset accordingly following (Gao et al., 2023b).Each entry in D is a 100-word passage following previous works on open-domain QA (Karpukhin et al., 2020;Petroni et al., 2021;Liu et al., 2023a).</p>
<p>The ASQA dataset</p>
<p>Basic Introduction.ASQA (Stelmakh et al., 2022) is a long-form generation QA dataset derived from the AmbigQA dataset (Min et al., 2020).It comprises questions characterized by their ambiguity, necessitating multiple short answers to address various aspects.Each entry in the dataset is accompanied by a comprehensive long-form answer that covers all the corresponding short answers.948 samples are tested for our experiment.</p>
<p>Experimental Setting Since most questions can be answered by Wikipedia, prior works usually use 2018-12-20 Wikipedia snapshot as D.</p>
<p>For retriever utilization, we examine the application of both DPR (Karpukhin et al., 2020) and GTRlarge (Ni et al., 2022).DPR introduces marginally more complex challenges for the LLM, achieving a recall rate of 51.5%, whereas GTR achieves 56.8% when considering the top-5 retrieved documents.</p>
<p>Evaluation</p>
<p>• Fluency: We use MAUVE score (Pillutla et al., 2021) to evaluate the corpus-wise similarity of the machine generated text and the long answers generated by systems.• Correctness: We follow (Stelmakh et al., 2022) to calculate the exact matching recall (EM recall) of the presents of correct short answers.• Citation quality: As mentioned in § 2, we calculate citation recall and citation precision using the scripts provided by (Gao et al., 2023b).</p>
<p>The QAMPARI dataset</p>
<p>Basic Introduction.QAMPARI (Amouyal et al., 2022) is created from Wikipedia, pairing questions with multiple answers derived from its knowledge graph and tables.These answers, comprised of entities, describe simple relationships to the entities in the query q.As a result, this dataset focuses on testing systems' abilities on entity identification within questions and accurately pinpointing the relevant entities.We use the same 1000 testing samples used in (Gao et al., 2023b) for experiments.</p>
<p>Experimental Setting Similar to the ASQA case, we employ the Wikipedia snapshot from 2018-12-20, as our D.For retrieval, we again use both DPR and GTR-large, achieving recall rates of approximately 17.6% and 31.6%,respectively, for the top five retrieved documents for each query.</p>
<p>Evaluation Metrics</p>
<p>In QAMPARI, we only considering the correctness and the citation quality since the output of the dataset is a list of entities.</p>
<p>• Correctness: We follow (Stelmakh et al., 2022) to to calculate the entity precision and recall of the model prediction using exact string match.</p>
<p>When calculate recall, the evaluation considers recall to be 100% if the prediction includes at least 5 correct answer, denoted as recall-5.• Citation quality: We use the same way as the ASQA dataset to evaluate the citation quality.</p>
<p>The ELI5 dataset</p>
<p>Basic Introduction.The ELI5 dataset, introduced by (Fan et al.), primarily features "How," "Why," and 'What" questions.It tests a system's ability to summarize complex information into clear and insightful answers.We use the 1000 samples used in (Gao et al., 2023b) for our experiment.</p>
<p>Experimental Setting Unlike ASQA and QAM-PARI, the ELI5 dataset covers diverse topics and hence, documents in Sphere corpus are treated as D (Piktus et al., 2021).Given the large size of the Sphere corpus, BM25 is used for efficient retrieval.</p>
<p>Evaluation Metrics</p>
<p>• Correctness: ELI5 dataset does not provide short entity answers.We follow (Gao et al., 2023b) to calculate claim recall for correctness.</p>
<p>For each reference answer in the dataset, three "sub-claims" are first extracted, and we will test whether the machine's answer A can entail these sub-claims using a TRUE NLI model (Honovich et al., 2022) • Fluency &amp; Citation quality: We use the metrics as the ASQA dataset for evaluation.</p>
<p>Compared Methods</p>
<p>We compare the following methods, all of which we have independently rerun, except Self-RAG.</p>
<p>For our own rerun results, the reported results are the average of three random runs. 1. In-Context Learning (ICLCite): LLMs are invoked once for grounded generation through instruction-based in-context learning.Following (Gao et al., 2023b), we provide five documents to the LLM.They suggest that increasing the number of input document lists does not significantly improve performance when using GPT-3.5-turbo.2. Summary then In-Context Learning (Summ + ICLCite) (Gao et al., 2023b): This method follows the preprocessing paradigm shown in Fig. 1.Initially, LLMs generate summaries for each document based on the query q.Then, these summaries are fed into the LLM to execute ICLCite.In our experiments, we generate summaries for the top-9 documents retrieved for each instance.samples and then use a LLM to determine the most consistent result among them.5. Self-RAG (Asai et al., 2023): This method finetunes LMs to generate special tokens to trigger additional fact checks and retrieval.Since this method requires model finetuning, we only report results on ASQA dataset only.6. CaLM: We use verification and an iterative refinement design to ensure the output quality.We follow the setting of ICLCite to set the our main LM's reading budget k = 5.We set the consistency threshold θ = 0.25 for the ELI5 dataset and θ = 0.5 for the ASQA dataset.This θ is decided by manual qualitative examination on a small set of development data.Besides, we set the maximum iteration to be 4 for budget concern, and use the 13B version of tulu-2-dpo (Ivison et al., 2023) as the verifier LM.</p>
<p>6 Experimental Results</p>
<p>Main Results</p>
<p>For the main experiment, we consider two different large LMs as the backbone: GPT-3.5-Turbo-1106(Ouyang et al., 2022) and the PaLM-based LLMs (Anil et al., 2023), text-unicorn2 .Tab. 2, Tab. 3, and Tab. 4 present the results on ASQA, QAMPARI, and ELI5, respectively.We have three discovery across the three datasets: 1. Snippet+ICLCite performs as the strongest baseline.We hypothesize that for tasks involving grounded generation, preserving original evidence within documents is crucial for citation quality.Abstractive summarization can result in the loss of significant evidence crucial for solving the task.Table 2: The experimental results on ASQA.CaLM achieves an average improvement of over 6% when using text-unicorn.When using GPT-3.5-Turbo-1106as the main LM, CaLM is the only method that outperforms the ICLCite baseline while making the fewest total LM API calls.The best results are bold, while the second best are underlined.* USC stands for Universal Self Consistency (Chen et al., 2023).† We report Self-RAG's numbers using the results from their original paper, where they retrieve up to ten documents per input using Contriever as the retriever (Izacard et al., 2022).We observe that with a weaker retriever, Snip-pet+ICLCite often fails to enhance performance.We conjecture that this is attributed to increased noise in these scenarios.More noisy input lists can lead to a higher likelihood of hallucinations and errors during the preprocessing steps, resulting in degraded performance.3. CaLM effectively improves the performance in both answer correctness and citation quality, and the improvement is robust against the choice of retriever.We attribute this robustness to our approach of releasing only high-quality responses in each iteration while continuously exploring new batches of documents.</p>
<p>Method</p>
<p>Analysis</p>
<p>We conduct analysis of CaLM on the QAMPARI dataset.All the studies are conducted using GTR as the retriever and text-unicorn as the main LM, except where noted in the table.</p>
<p>What if we use larger LM as the verifier LM?  Tab. 5 shows how the size of the verifier impacts the task performance.We can see that all verifiers improve the performance, demonstrating the robustness of our framework.Additionally, the smaller LM variants outperforms the largest LM (tulu-2-70b) in citation quality, and the medium-sized LM achieves the best performance.A medium-sized LM combines external knowledge integration and information discernment, resulting in a better verifier performance.Comparing with using the large LM itself as verifier, our choice of a smaller LM as the verifier performs better.</p>
<p>What if the verifier LM could access more than just the cited documents?In our automated verification method, the verifier LM is limited to accessing only the cited documents.Removing this constraint simplifies our verification algorithm to re-sampling with a different LM.The results, shown in Tab.6, demonstrate a notable decline in performance when such verification design is removed, especially in citation quality.This highlights the essential role and effectiveness of our design of automated verification.</p>
<p>Further Analysis Appendix A details additional studies on model performance and prediction changes across iterations.</p>
<p>Related Work</p>
<p>Evaluation Early research focused on evaluating attribution in text generation.Rashkin et al. (2023) introduced the "Attributable to Identified Sources" (AIS) framework for assessing faithfulness.Subsequent studies developed automatic (Honovich et al., 2022;Yue et al., 2023) and human (Bohnet et al., 2022) (2023b) proposed ALCE, an automatic benchmark for text generation with citations.In this study, we assess our approach using ALCE.</p>
<p>Finetuned LMs Some studies have investigated fine-tuning language models (LMs) for generating cited answers (Menick et al., 2022;Nakano et al., 2021).Similarly, Ye et al. ( 2023) employed adaptation approach for fine-tuning.These methods required training and could be be susceptible to generalization issues.2023) utilized an LLM as a verifier for document relevance but didn't employ the answer to verify the correct grounding.</p>
<p>Retrieval-based Methods</p>
<p>Self-reflection Prompting LLMs to self-reflect on their answers has been shown to improve factuality (Ji et al., 2023).Asai et al. (2023) employed this concept, enhancing LM quality and factuality via retrieval and self-reflection by training special tokens.CaLM outperforms this method without the need of training.</p>
<p>Conclusion</p>
<p>In this paper, we introduce CaLM, a novel verification approach for grounded generation.We observe that while larger LMs excel at identifying relevant materials, they tend to rely excessively on internal parametric memory.Conversely, smaller LMs are adept at processing focused information.CaLM leverages these complementary strengths to offer a fresh perpective on robust and scalable solution for verification in grounded generation.</p>
<p>Limitation</p>
<p>We acknowledge the limitations of CaLM from the following aspects to inspire future research opportunities in the field of grounded generation.Firstly, as a postprocessing technique, our method introduces additional latency in generating the final answer.As a remedy, we can set the maximum iteration T smaller.From Fig. 4 in the appendix, we have shown that only even a single iteration of our correction process significantly enhances performance, yet latency remains an unavoidable factor.</p>
<p>Moreover, unlike preprocessing approaches depicted in Fig. 1(b), which can reduce input token consumption for the final LLM, our method necessitates that the LLM initially processes all documents, leading to a higher cost for token usage.</p>
<p>Lastly, despite the considerable advancements CaLM has made across datasets, the instances that pass our verification process are still not flawless.Given that both the answer candidate and the verifier output are outcomes from LMs, there is an inevitable risk of both models producing hallucinations simultaneously.</p>
<p>We hope future works can leverage the idea and insights from CaLM to advance the development of more robust grounded generation with low latency and reduced token costs.</p>
<p>Broader Considerations</p>
<p>As a method that directly apply LLMs, CaLM inherits all potential risks associated with LLMs, including but not limited to unethical outputs, toxicity, and biases (Bender et al., 2021;Yuan et al., 2024;Gallegos et al., 2023).Our qualitative assessment of CaLM, conducted across several samples from three datasets, indicates that LLMs generally adhere to instructions and generate responses relevant to the content of provided documents.However, we strongly advise conducting a comprehensive evaluation of these potential issues before deploying CaLM in practical settings.</p>
<p>A Further Analysis</p>
<p>In this section, we focus on analysis when iteration of CaLM proceed.</p>
<p>How does the model's performance improve as iterations proceed?Fig. 4 illustrates the outcomes of terminating the process from iteration 0 through 6.As demonstrated in the figure, the precision for correctness consistently improve with each iteration of CaLM as our framework only allows high quality final output.On the other hand, our design of updating the input document list contributes to a consistent rise in correctness recall.</p>
<p>However, we observe that performance tends to plateau after the third iteration, with subsequent iterations yielding diminishing returns.Extending the maximum iterations to six produced only a marginal average improvement of 0.4 compared to the third iteration, while significantly increasing the computational cost in terms of API calls.</p>
<p>We believe CaLM's iterative nature is a key strength, allowing for continuous improvement.However, our findings suggest that two to three iterations offer substantial gains with minimal computational overhead.This demonstrates the framework's efficiency and practicality for real-world applications.</p>
<p>Case study on CaLM's correction.We present two case studies to demonstrate that by looping through our verification design, more accurate evidence can be found and more accurate responses can be generated.Fig. 5 is based on the ASQA dataset, and Fig. 6 utilizes the QAMPARI dataset.</p>
<p>From the example of Fig. 5, we can see that the small LM, serving as a verifier, when given access to cleaner input document sets, is capable of identifying overlooked information by the main LM.This detection initiates iterative correction processes in subsequent rounds.</p>
<p>The example in Fig. 6 demonstrates that (1) CaLM finds more convincing evidence documents in later rounds, and (2) CaLM catches citation errors through verification.</p>
<p>B Used prompts</p>
<p>In this section, we list the prompts we use for our experiment.</p>
<p>B.1 Prompts for ASQA</p>
<p>Two different prompt sets are used for the ASQA dataset.Fig. 7 shows the prompts we used for the LM to conduct grounded generation, which mainly follow the prompt used in (Gao et al., 2023b) with two shot examples.We design our own prompt for the main model to perform correction.The prompt is detailed in Fig. 8, which use 1-shot example.</p>
<p>B.2 Prompts for QAMPARI</p>
<p>Two different prompt sets are used for the QAMPRI dataset.Fig. 9 shows the prompts we used for the LM to conduct grounded generation, which mainly follow the prompt used in (Gao et al., 2023b) with two shot examples.We design our own prompt for the main model to perform correction.The prompt is detailed in Fig. 10, which use 1-shot example.</p>
<p>B.3 Prompts for ELI5</p>
<p>Two different prompt sets are used for the ELI5 dataset.Fig. 11 shows the prompts we used for the LM to conduct grounded generation, which mainly follow the prompt used in (Gao et al., 2023b) with two shot examples.We design our own prompt for the main model to perform correction.The prompt is detailed in Fig. 12, which use 1-shot example.</p>
<p>C Implementation Details</p>
<p>For all experiments with public available models, we use vLLM framework for inference (Kwon et al., 2023).We operate vLLM on our machine with 16 NVIDIA-A100-40GB GPU.For experiments with GPT-3.5-Turbo-1106,we use the official API3 .For experiment with text-unicorn, we use Google-Cloud vertex API4 .</p>
<p>D Dataset and Evaluation Tool</p>
<p>We use the artifacts provided by Gao et al. (2023b).The dataset and corresponding evaluation code is under MIT licence5 .We do not change any of the provided data and maintain consistent with their intended use.</p>
<p>Figure 4: The study examines the iterative performance improvements on the QAMPARI dataset.We use GPT-3.5-Turbo as the main LM for this running study.</p>
<p>Figure 5: A case study of CaLM on ASQA dataset.The question is "Who sings don't tell me what to do?" and all reference short answers are "Pam Tillis", "Marty Stuart", and "Baby Animals".</p>
<p>Figure 6: A case study of CaLM on QAMPARI dataset.The question is "Which movie did John Carpenter direct for which he also composed the music?" and all reference answers are "Vampires", "In the Mouth of Madness", "Assault on Precinct 13", "Dark Star", "Big Trouble in Little China", "They Live", "Halloween", "Escape from New York", "Prince of Darkness", "Ghosts of Mars", "The Fog", "Chevil", "Village of the Damned".Instruction : Provide a concise response to the question by analyzing relevant search results ( some of which might be irrelevant ) , and cite useful resources using [1][2][3] format .Use an unbiased and journalistic tone , ensuring facts are presented clearly based on the search documents .Cite at least one and no more than three documents per sentence .Additionally , you will be provided with an incomplete draft solution , which is based on the first { SIZE_OF_VERIFIED_DOCS } search results and might contain citation inaccuracies .Therefore , it might not capture all conceivable responses to the question .Your role is to assess the draft ' s comprehensiveness as well as its correctness , and then update the solution to encapsulate all possible answers according to the search documents .Provide your answer after " Corrected Answer :" , and ensure each sentence is supported by citations from one to three sources .Instruction : Provide a concise response to the question by analyzing relevant search results ( some of which might be irrelevant ) , and cite useful resources using [1][2][3] format .Use an unbiased and journalistic tone , ensuring facts are presented clearly based on the search documents .Cite at least one and no more than three documents per sentence .Additionally , you will be provided with an incomplete draft solution , which is based on the first { SIZE_OF_VERIFIED_DOCS } search results and might contain citation inaccuracies .Therefore , it might not capture all conceivable responses to the question .Your role is to assess the draft ' s comprehensiveness as well as its correctness , and then update the solution to encapsulate all possible answers according to the search documents .Provide your answer after " Corrected Answer :" , and ensure each sentence is supported by citations from one to three sources .Instruction : Provide a succint response to the question by analyzing relevant search results ( some of which might be irrelevant ) , and cite these using [1][2][3] format .Limit citations to one to three per sentence , and only cite a necessary subset if multiple sources corroborate a point .Additionally , you will be provided with an incomplete draft solution , which is based on the first { } search results and might contain citation inaccuracies .Therefore , it might not capture all conceivable responses to the question .Your role is to assess the draft 's comprehensiveness as well as its correctness of citations , and then update the solution to encapsulate all possible answers according to the search documents .Provide your answer after " Corrected Answer :" , and ensure each sentence is supported by citations from one to three sources .Instruction : Provide a succint response to the question by analyzing relevant search results ( some of which might be irrelevant ) , and cite these using [1][2][3] format .Limit citations to one to three per sentence , and only cite a necessary subset if multiple sources corroborate a point .Additionally , you will be provided with an incomplete draft solution , which is based on the first { SIZE_OF_VERIFIED_DOCS } search results and might contain citation inaccuracies .Therefore , it might not capture all conceivable responses to the question .Your role is to assess the draft 's comprehensiveness as well as its correctness of citations , and then update the solution to encapsulate all possible answers according to the search documents .Provide your answer after " Corrected Answer :" , and ensure each sentence is supported by citations from one to three sources .</p>
<p>Figure 1 :
1
Figure 1: Comparison between different categories of existing inference methods for grounded generation.(a) LLM with single-run can hallucinate easily due to the high complexity of the task.(b) Preprocessing methods reduce task complexity but the hallucination issues can propagate from preprocessing steps.(c) We propose using verification and rectification to ensure LLMs generate outputs with complete citations and accurate answers, maintaining quality.</p>
<p>Fig</p>
<p>Fig. (b): Relative Improvement in QA accuracy when input high relevant documents compared to input noisy ones.</p>
<p>Fig</p>
<p>Fig. (a): The illustration of describing the accuracy of LM as a function of the input document relevancy to the query Fig. (c): 2 nd Order Relative Improvement Analysis in QA accuracy when adjusting input document relevancy.</p>
<p>Figure 3 :
3
Figure 3: Overview of CaLM: Top: The flow diagram of our method.Bottom: A detailed depiction of each step's operation.The algorithm starts with a retriever extract a relevant document pool p for the input query (Step (1)).Then, the main language model (LM) takes the first batch of documents and employs retrieval-augmented generation to produce an answer candidate, which cites relevant supporting documents (Step (2)).Subsequently, this candidate is validated by contrasting it with the verifier output from the verifier LM (Steps (3) &amp; (4)).Our verifier LM evaluates citation quality by accessing only the documents cited by the main LM's response, rather than the same input documents.For responses with sufficient consistency, we accept the answer candidate directly.If inconsistent, we break down the answer candidate into individual statements, retaining only those corroborated by similar arguments in the verifier output for further correction in next iteration (Step (5)).</p>
<p>He et al. (2022);Gao et al. (2023a) used post-editing to ensure text consistency by retrieving relevant documents.Gao et al. (2023b) explored methods like document summarization and LLM-enabled searches for citation improvement, yet lacked verification for answer validation.Li et al. (</p>
<p>Instruction : Write an accurate , engaging , and concise answer for the given question using only the provided search results ( some of which might be irrelevant ) and cite them properly .Use an unbiased and journalistic tone .Always cite for factual claims .When citing several search results , use[1][2][3].Cite at least one and no more than three documents per sentence .If multiple documents support the sentence , only cite a minimum sufficient subset .Question : { EXAMPLE 1} Document <a href="Title : { TITLE_FOR_DOC_1 }">1</a> : { DOC_1 } Document <a href="Title : { TITLE_FOR_DOC_2 }">2</a> : { DOC_2 } Document <a href="Title : { TITLE_FOR_DOC_3 }">3</a> : { DOC_3 } Document <a href="Title : { TITLE_FOR_DOC_4 }">4</a> : { DOC_4 } Document <a href="Title : { TITLE_FOR_DOC_5 }">5</a> : { DOC_5 } Answer : { ANSWER_FOR_EXAMPLE 1} Instruction : Write an accurate , engaging , and concise answer for the given question using only the provided search results ( some of which might be irrelevant ) and cite them properly .Use an unbiased and journalistic tone .Always cite for factual claims .When citing several search results , use [1][2][3].Cite at least one and no more than three documents per sentence .If multiple documents support the sentence , only cite a minimum sufficient subset .Question : { EXAMPLE 2} Document <a href="Title : { TITLE_FOR_DOC_1 }">1</a> : { DOC_1 } Document <a href="Title : { TITLE_FOR_DOC_2 }">2</a> : { DOC_2 } Document <a href="Title : { TITLE_FOR_DOC_3 }">3</a> : { DOC_3 } Document : { TITLE_FOR_DOC_4 }) : { DOC_4 } Document <a href="Title : { TITLE_FOR_DOC_5 }">5</a> : { DOC_5 } Answer : { ANSWER_FOR_EXAMPLE 2} Instruction : Write an accurate , engaging , and concise answer for the given question using only the provided search results ( some of which might be irrelevant ) and cite them properly .Use an unbiased and journalistic tone .Always cite for factual claims .When citing several search results , use [1][2][3].Cite at least one and no more than three documents per sentence .If multiple documents support the sentence , only cite a minimum sufficient subset .Question : { REAL QUERY } Document <a href="Title : { TITLE_FOR_DOC_1 }">1</a> : { DOC_1 } Document <a href="Title : { TITLE_FOR_DOC_2 }">2</a> : { DOC_2 } Document <a href="Title : { TITLE_FOR_DOC_3 }">3</a> : { DOC_3 } Document <a href="Title : { TITLE_FOR_DOC_4 }">4</a> : { DOC_4 } Document <a href="Title : { TITLE_FOR_DOC_5 }">5</a> : { DOC_5 } Answer :</p>
<p>Figure 7 :
7
Figure 7: Prompt for the LM to conduct the grounded generation on ASQA dataset.</p>
<p>Question : { EXAMPLE 1} Document <a href="Title : { TITLE_FOR_DOC_1 }">1</a> : { DOC_1 } Document <a href="Title : { TITLE_FOR_DOC_2 }">2</a> : { DOC_2 } Document <a href="Title : { TITLE_FOR_DOC_3 }">3</a> : { DOC_3 } Document <a href="Title : { TITLE_FOR_DOC_4 }">4</a> : { DOC_4 } Document <a href="Title : { TITLE_FOR_DOC_5 }">5</a> : { DOC_5 } Drafted Solution : { DRAFT SOLUTION FOR EXAMPLE 1} Corrected Answer : { ANSWER_FOR_EXAMPLE 1}</p>
<p>Question : { QUERY } Document <a href="Title : { TITLE_FOR_DOC_1 }">1</a> : { DOC_1 } Document <a href="Title : { TITLE_FOR_DOC_2 }">2</a> : { DOC_2 } Document <a href="Title : { TITLE_FOR_DOC_3 }">3</a> : { DOC_3 } Document <a href="Title : { TITLE_FOR_DOC_4 }">4</a> : { DOC_4 } Document <a href="Title : { TITLE_FOR_DOC_5 }">5</a> : { DOC_5 } Drafted Solution : { DRAFT SOLUTION FOR QUERY } Corrected Answer :</p>
<p>Figure 8 :
8
Figure 8: Prompt for the main LM to conduct correction on ASQA dataset.</p>
<p>Figure 9 :
9
Figure 9: Prompt for the LM to conduct the grounded generation on QAMPARI dataset.</p>
<p>Figure 10 :
10
Figure 10: Prompt for the main LM to conduct correction on QAMPARI dataset.</p>
<p>Figure 11 :
11
Figure 11: Prompt for the LM to conduct the grounded generation on ELI5 dataset.</p>
<p>Question : { EXAMPLE 1} Document <a href="Title : { TITLE_FOR_DOC_1 }">1</a> : { DOC_1 } Document <a href="Title : { TITLE_FOR_DOC_2 }">2</a> : { DOC_2 } Document <a href="Title : { TITLE_FOR_DOC_3 }">3</a> : { DOC_3 } Document <a href="Title : { TITLE_FOR_DOC_4 }">4</a> : { DOC_4 } Document <a href="Title : { TITLE_FOR_DOC_5 }">5</a> : { DOC_5 } Drafted Solution : { DRAFT SOLUTION FOR EXAMPLE 1} Corrected Answer : { ANSWER_FOR_EXAMPLE 1}</p>
<p>Question : { QUERY } Document <a href="Title : { TITLE_FOR_DOC_1 }">1</a> : { DOC_1 } Document <a href="Title : { TITLE_FOR_DOC_2 }">2</a> : { DOC_2 } Document <a href="Title : { TITLE_FOR_DOC_3 }">3</a> : { DOC_3 } Document <a href="Title : { TITLE_FOR_DOC_4 }">4</a> : { DOC_4 } Document <a href="Title : { TITLE_FOR_DOC_5 }">5</a> : { DOC_5 } Drafted Solution : { DRAFT SOLUTION FOR QUERY } Corrected Answer :</p>
<p>Figure 12 :
12
Figure 12: Prompt for the main LM to conduct correction on ELI5 dataset.</p>
<p>Table 1 :
1
Who sings don't tell me what to do? Reference Answer: Marty Stuart recorded the song Don't Tell Me What to Do, recorded as I'll Love You Forever (If I Want To) in 1988.Pam Tillis sang Don't Tell Me What To Do in 1990, and in 1993, the Baby Animals recorded the song.QAMPARI Question: Which movie did John Carpenter direct for which he also composed the music?Reference Answer: Vampires, In the Mouth of Madness, Assault on Precinct 13, Dark Star, Big Trouble in Little China, They Live, Halloween, Escape from New York, Prince of Darkness, Ghosts of Mars, The Fog, Chevil, Village of the Damned ELI5 Question: Why are fruit in Chinese supermarkets so much bigger than western chains like loblaws or food basics?Reference Answer: There are a lot less restrictions on using chemicals in agriculture here, so some farmers use 'growth accelerators' to make their fruit (and other products) bigger.Last May, overuse of the chemicals resulted in a spate of [exploding watermelons] Illustrative examples from each experiment's dataset.
DatasetExampleASQAQuestion:</p>
<p>(Gao et al., 2023b)Context Learning (Snippet + ICLCite)(Gao et al., 2023b): Similar to Summ + ICLCite, but LLM are guided to generate extractive summaries during preprocessing steps.4. In-Context Learning with Self-Consistency (ICLCite + USC): This post-processing method that employs ICLCite to initially generate various output samples.Then, it applies universal self-consistency Chen et al. (2023) to obtain the results.For a fair comparison with other baselines, like Summ+ICLCite, we first generate 9</p>
<p>Table 3 :
3
The experimental results on QAMPARI.Compared to all the preprocess and postprocess baselines, our method obtains the best average performance across different settings and use the fewest LM API calls.
MethodFluency Correct. mauve Claim. Prec. Rec. CitationAverageGPT-3.5-Turbo-1106 as Main LMICLCite25.3312.90 44.63 49.3433.05Summ + ICLCite17.8610.67 38.81 43.26 26.40 (-6.65)Snippet + ICLCite 25.3012.06 35.49 41.35 28.55 (-4.50)ICLCite + USC25.0412.16 34.87 39.60 27.92 (-5.13)CaLM (ours)25.8412.92 46.55 51.90 34.30 (+1.25)text-unicorn as Main LMICLCite35.8212.21 35.35 32.0528.86Summ + ICLCite34.5711.74 35.54 35.02 29.22 (+0.36)Snippet + ICLCite 47.4313.51 34.37 33.00 32.08 (+3.22)ICLCite + USC31.4212.29 34.90 31.66 27.57 (-1.29)CaLM (ours)37.0611.95 46.26 43.60 34.72 (+5.86)</p>
<p>Table 4 :
4
The experimental results on ELI5.CaLM obtains the best average performance regardless of the used main LM.The improvements are especially significant in the citation quality.</p>
<p>Table 5 :
5
Evaluating the choice of verifier LM.Our investigation focuses on comparing the performance of models of varying sizes and using the main LM itself as the verifier.Compared to the largest LM in the same family, a smaller-sized LM yields better performance.Additionally, our choice of verifier can largely outperform using the main LM itself as verifier.
Verifier LM's input documentCorrectness Prec. Rec.-5 Prec. Rec. CitationAverageOnly cited documents (Ours.)30.28 22.79 28.53 28.34 27.49Same as the main LM28.19 23.26 25.22 25.65 25.58</p>
<p>Table 6 :
6
Ablating our design of using only cited documents for verification.We can observe a significant performance drop if we remove the design.</p>
<p>evaluation methods based on AIS.Recent work by Liu et al. (2023b) evaluated generative search engines that provide citations, and Gao et al.</p>
<p>In this paper, we differentiate LMs by size, labeling them as large or small. However, a more accurate categorization would be strong versus weak LMs, reflecting their varying performance levels across different LM families.
https://cloud.google.com/vertex-ai/docs/generativeai/model-reference/text
https://platform.openai.com/
https://cloud.google.com/vertex-ai/docs/reference/rest
https://github.com/princeton-nlp/ALCE/tree/main</p>
<p>QAMPARI: : An open-domain question answering benchmark for questions with many answers from multiple paragraphs. Samuel Joseph Amouyal, Ohad Rubin, Ori Yoran, Tomer Wolfson, Jonathan Herzig, Jonathan Berant, Andrew M Rohan Anil, Orhan Dai, Melvin Firat, Dmitry Johnson, Alexandre Lepikhin, Siamak Passos, Emanuel Shakeri, Paige Taropa, Zhifeng Bailey, Eric Chen, Jonathan H Chu, Laurent El Clark, Yanping Shafey, Kathy Huang, Gaurav Meier-Hellstern, Sebastian Mishra, Yi Ruder, Kefan Tay, Yuanzhong Xiao, Yujing Xu, Gustavo Zhang, Junwhan Hernández Ábrego, Jacob Ahn, Paul Austin, Jan A Barham, James Botha, Siddhartha Bradbury, Brahma, Markus Vlad Fienber, Xavier Freitag, Sebastian Garcia, Lucas Gehrmann, Gonzalez, arXiv:2205.12665Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz2022. 2023Fangxiaoyu FengErica Moreira, Mark Omernick, Kevin Robinson,; Kevin Brooks, Michele Catasta; Nan Du, Ethan Dyer, Vladimir FeinbergarXiv preprintPalm 2 technical report. CoRR, abs/2305.10403</p>
<p>Self-rag: Self-reflective retrieval augmented generation. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi, NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. 2023</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, FAccT '21: 2021 ACM Conference on Fairness, Accountability, and Transparency, Virtual Event. Toronto, Canada2021. March 3-10, 2021</p>
<p>Attributed question answering: Evaluation and modeling for attributed large language models. Bernd Bohnet, Pat Vinh Q Tran, Roee Verga, Daniel Aharoni, Andor, Baldini Livio, Massimiliano Soares, Jacob Ciaramita, Kuzman Eisenstein, Jonathan Ganchev, Herzig, arXiv:2212.080372022arXiv preprint</p>
<p>Universal self-consistency for large language model generation. Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, Denny Zhou, arXiv:2311.173112023arXiv preprint</p>
<p>ELI5: long form question answering. Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, Michael Auli, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. Long Papers. the 57th Conference of the Association for Computational Linguistics, ACL 2019Florence, ItalyJuly 28-August 2, 20191</p>
<p>Bias and fairness in large language models: A survey. Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, Nesreen K Ahmed, CoRR, abs/2309.007702023</p>
<p>RARR: Researching and revising what language models say, using language models. Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, Kelvin Guu, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics2023a1</p>
<p>Enabling large language models to generate text with citations. Tianyu Gao, Howard Yen, Jiatong Yu, Danqi Chen, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023b</p>
<p>Hangfeng He, Hongming Zhang, Dan Roth, arXiv:2301.00303Rethinking with retrieval: Faithful large language model inference. 2022arXiv preprint</p>
<p>TRUE: Re-evaluating factual consistency evaluation. Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, Yossi Matias, Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterHuman Language Technologies2022</p>
<p>Jie Huang, Kevin Chen, -Chuan Chang, arXiv:2307.02185Citation: A key to building responsible and accountable large language models. 2023arXiv preprint</p>
<p>Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, Ting Liu, arXiv:2311.05232A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. 2023arXiv preprint</p>
<p>Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A Smith, Iz Beltagy, Hannaneh Hajishirzi, arXiv:2311.10702Camels in a changing climate: Enhancing LM adaptation with tulu 2. 2023arXiv preprint</p>
<p>Unsupervised dense information retrieval with contrastive learning. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, Edouard Grave, Trans. Mach. Learn. Res. 2022</p>
<p>Towards mitigating LLM hallucination via self reflection. Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, Pascale Fung, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. Dongfu Jiang, Xiang Ren, Bill Yuchen, Lin , Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Vladimir Karpukhin, Barlas Oguz, Sewon Min, S H Patrick, Ledell Lewis, Sergey Wu, Danqi Edunov, Wen-Tau Chen, Yih, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, Canada; Online2023. July 9-14, 2023. 2020. 2020. November 16-20, 20201Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. the ACM SIGOPS 29th Symposium on Operating Systems Principles2023</p>
<p>Xiaonan Li, Changtai Zhu, Linyang Li, Zhangyue Yin, Tianxiang Sun, Xipeng Qiu, arXiv:2311.07838Llatrieval: Llm-verified retrieval for verifiable generation. 2023arXiv preprint</p>
<p>Lost in the middle: How language models use long contexts. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang, ArXiv:2307.031722023a</p>
<p>Evaluating verifiability in generative search engines. Nelson F Liu, Tianyi Zhang, Percy Liang, Findings of the Association for Computational Linguistics: EMNLP 2023. Singapore2023b. December 6-10, 2023</p>
<p>Teaching language models to support answers with verified quotes. Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, 2022. 2022</p>
<p>Ambigqa: Answering ambiguous open-domain questions. Sewon Min, Julian Michael, Hannaneh Hajishirzi, Luke Zettlemoyer, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingOnline2020. 2020. November 16-20, 2020</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, arXiv:2112.09332Webgpt: Browser-assisted questionanswering with human feedback. 2021arXiv preprint</p>
<p>Large dual encoders are generalizable retrievers. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Ábrego, Ji Ma, Vincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, Yinfei Yang, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab Emirates2022. December 7-11, 20222022</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, Ryan Lowe, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021. Fabio Petroni, Aleksandra Piktus, Angela Fan, S H Patrick, Majid Lewis, Nicola De Yazdani, James Cao, Yacine Thorne, Vladimir Jernite, Jean Karpukhin, Vassilis Maillard, Tim Plachouras, Sebastian Rocktäschel, Riedel, the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021New Orleans, LA, USA2022. 2022. 2022. November 28 -December 9, 2022. 2021. June 6-11, 2021Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems</p>
<p>Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Dmytro Okhonko, Samuel Broscheit, Gautier Izacard, S H Patrick, Barlas Lewis, Edouard Oguz, Wen-Tau Grave, Sebastian Yih, Riedel, The web is your oyster -knowledge-intensive NLP against a very large web corpus. 2021</p>
<p>MAUVE: measuring the gap between neural text and human text using divergence frontiers. Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, Zaïd Harchaoui, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021. NeurIPS2021. 2021. December 6-14, 2021</p>
<p>Measuring Attribution in Natural Language Generation Models. Vitaly Hannah Rashkin, Matthew Nikolaev, Lora Lamm, Michael Aroyo, Dipanjan Collins, Slav Das, Gaurav Petrov, Iulia Singh Tomar, David Turc, Reitter, 2023Computational Linguistics</p>
<p>ASQA: factoid questions meet long-form answers. Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, Ming-Wei Chang, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab Emirates2022. December 7-11, 20222022</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023. May 1-5, 2023</p>
<p>Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, Yu Su, arXiv:2305.13300Adaptive chameleon or stubborn sloth: Unraveling the behavior of large language models in knowledge clashes. 2023arXiv preprint</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang, arXiv:2304.12244arXiv:2311.09533Ruoxi Sun, Sercan Ö Arik, and Tomas Pfister. 2023. Effective large language model adaptation for improved grounding. Xi Ye2023arXiv preprintWizardlm: Empowering large language models to follow complex instructions</p>
<p>R-judge: Benchmarking safety risk awareness for LLM agents. Tongxin Yuan, Zhiwei He, Lingzhong Dong, Yiming Wang, Ruijie Zhao, Tian Xia, Lizhen Xu, Binglin Zhou, Fangqi Li, Zhuosheng Zhang, Rui Wang, Gongshen Liu, CoRR, abs/2401.100192024</p>
<p>Automatic evaluation of attribution by large language models. Xiang Yue, Boshi Wang, Ziru Chen, Kai Zhang, Yu Su, Huan Sun, ; Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, Shuming Shi, arXiv:2309.01219Siren's song in the AI ocean: A survey on hallucination in large language models. 2023. 2023arXiv preprintFindings of the Association for Computational Linguistics: EMNLP 2023</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, Hao Zhang, Joseph E Gonzalez, arXiv:2306.05685and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint</p>
<p>Don 't Tell Me What to Do " is a song written by Harlan Howard and Max D. Barnes , and recorded by American country music artist Pam Tillis . It was released in December 1990 as the first single from the album " Put Yourself in My Place. The song reached number 5 on the " Billboard " Hot Country Singles &amp; Tracks chart. 1988Round 1: &gt;&gt; Main LM : &gt;&gt;&gt; Access to : Document <a href="Title : Don 't Tell Me What to Do. Marty Stuart recorded this song under the title &quot;I ' ll Love You Forever ( If I Want">1</a>o</p>
<p>Let There Be Country " album , although the album was not released until 1992. Don 't Tell Me What to Do " for Baby Animals ' song of the same name , see " Don 't Tell Me What to Do ( Baby Animals song ). American country music artist Pam Tillis. Harlan Howard and Max D. BarnesDon 't Tell Me What to Do " is a song written by. It was released in December 1990 as the first single from the album ...( Omitted</p>
<p>By 1991 , she had signed to Arista Nashville , entering Top 40 on Hot Country Songs for the first time with " Don 't Tell Me What to Do " , the first of five &gt;&gt;&gt; Response : The song " Don ' t Tell Me What to Do " was written by Harlan Howard and Max D . Barnes and recorded by American country music artist Pam Tillis. Pam Tillis Pamela Yvonne Tillis. born July 24 , 1957) is an American country music singer -songwriter and actress . She is the daughter of the late country music singer Mel. reaching number 5 on the " Billboard " Hot Country Singles &amp; Tracks chart</p>
<p>Don ' t Tell Me What to Do. &gt;&gt;, L M Verifier, Access to : Document 1 ,4 ,5 &gt;&gt;&gt; Response : Pam Tillis recorded the song. Harlan Howard, Max D Barnes, Marty Stuart, which reached the Top 40 of Hot Country Songs in 1991 [5]. This song was</p>
<p>Don 't Tell Me What to Do " is a song written by Harlan Howard and Max D. Barnes , and recorded by American country music artist Pam Tillis . It was released in December 1990 as the first single from the album " Put Yourself in My Place. The song reached number 5 on the " Billboard " Hot Country Singles &amp; Tracks chart. 1988Round 2: &gt;&gt; Main LM : &gt;&gt;&gt; Access to : Document <a href="Title : Don 't Tell Me What to Do. Marty Stuart recorded this song under the title &quot;I ' ll Love You Forever ( If I Want">1</a>o</p>
<p>Let There Be Country " album , although the album was not released until 1992. Don 't Tell Me What to Do " for Baby Animals ' song of the same name , see " Don 't Tell Me What to Do ( Baby Animals song ). American country music artist Pam Tillis. Harlan Howard and Max D. BarnesDon 't Tell Me What to Do " is a song written by. It was released in December 1990 as the first single from the album ...( Omitted</p>
<p>By 1991 , she had signed to Arista Nashville , entering Top 40 on Hot Country Songs for the first time with " Don 't Tell Me What to Do. Pam Tillis Pamela Yvonne Tillis. Harlan Howard and Max D . Barnes and recorded by American country music artist Pam Tillisborn July 24 , 1957) ) : ( Omitted ) &gt;&gt;&gt; Response : The song " Don ' t Tell Me What to Do. reaching number 5 on the " Billboard " Hot Country Singles &amp; Tracks chart</p>
<p>Marty Stuart also recorded this song under the title "I ' ll Love You Forever ( If I Want To ). Additionally, in 1988 [1</p>
<p>Omit the rest of the steps for clarity. </p>
<p>The film acts as a direct sequel to Carpenter 's original film , ignoring the continuity of all other previous films . It is his first direct involvement with the franchise since 1982 ' s ". Carpenter 's films are characterized by minimalist lighting and photography , static cameras , use of steadicam , and distinctive synthesized scores ( usually self -composed ). October. 201813The Ward ", he has scored all of his films ( though some are collaborations ) , most famously the themes from. His music is generally synthesized Document [3]( Title : Thomas Newman. Omitted</p>
<p>a score by Ennio Morricone and a cast including young actor Kurt Russell and respected character actors such as. Wilford Brimley, Richard Dysart, Charles Hallahan, Keith David, Richard Masur, ; John, W CampbellJr, Novella, The Thing. The Thing " was part of what Carpenter later called his " Apocalypse Trilogy. a trio of films. Prince Document <a href="Title : Alan Howarth ( composer )">5</a> : ( Omitted ) &gt;&gt;&gt; Response : Assault on Precinct 13 [2] , Halloween [2] , The Thing [4</p>
<blockquote>
<blockquote>
<p>, L M Verifier, Access to : Document 2 ,4 &gt;&gt;&gt; Response : Assault on Precinct 13. 2</p>
</blockquote>
</blockquote>
<p>Carpenter is also notable for having composed or co -composed most of his films ' music ; some of them are now well -known , with the main theme of " Halloween " being considered a part of popular culture . He won a Saturn Award for Best Music for the Document. Wilford Brimley, Richard Dysart, Charles Hallahan, Keith David, Richard Masur, ; John, W CampbellJr, Novella, classics that Carpenter has directed include " Dark Star. Donald Rubinstein, Christine2018. 1974. 1976. 1982. 1983. 1986. 1995. 201813It is his first direct involvement with the franchise since 1982 ' s ". Carpenter 's films are characterized by minimalist lighting and photography , static cameras , use of steadicam , and distinctive synthesized scores ( usually self -composed ). Omitted ) Document [8]( Title : Nellee Hooper. Assault on Precinct 13 [6] , The Thing [6] , Christine [6] , Big Trouble in Little China [6] , Prince of Darkness [6] , They Live [6] , In the Mouth of Madness [6] &gt;&gt; Verifier LM : &gt;&gt;&gt; Access to : Document 6 &gt;&gt;&gt; Response : Halloween [6] , Dark Star [6] , The Thing [6] , Christine [6] , Big Trouble in Little China [6] , Prince of Darkness [6] , They Live [6] , In the Mouth of Madness [6] ( Omit the rest of the steps for clarity</p>            </div>
        </div>

    </div>
</body>
</html>
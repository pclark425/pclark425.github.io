<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6154 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6154</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6154</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-121.html">extraction-schema-121</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-4fb0e1283194c0d20ac0396bc375fdf91c14da1e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4fb0e1283194c0d20ac0396bc375fdf91c14da1e" target="_blank">Complex Claim Verification with Evidence Retrieved in the Wild</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work presents the first realistic pipeline to check real-world claims by retrieving raw evidence from the web by restricting the retriever to only search documents available prior to the claim’s making, modeling the realistic scenario of emerging claims.</p>
                <p><strong>Paper Abstract:</strong> Retrieving evidence to support or refute claims is a core part of automatic fact-checking. Prior work makes simplifying assumptions in retrieval that depart from real-world use cases: either no access to evidence, access to evidence curated by a human fact-checker, or access to evidence published after a claim was made. In this work, we present the first realistic pipeline to check real-world claims by retrieving raw evidence from the web. We restrict our retriever to only search documents available prior to the claim’s making, modeling the realistic scenario of emerging claims. Our pipeline includes five components: claim decomposition, raw document retrieval, fine-grained evidence retrieval, claim-focused summarization, and veracity judgment. We conduct experiments on complex political claims in the ClaimDecomp dataset and show that the aggregated evidence produced by our pipeline improves veracity judgments. Human evaluation finds the evidence summary produced by our system is reliable (it does not hallucinate information) and relevant to answering key questions about a claim, suggesting that it can assist fact-checkers even when it does not reflect a complete evidence set.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6154.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6154.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VeracityEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic veracity classification evaluation (DeBERTa-large)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automatic evaluation of system outputs by training a DeBERTa-large classifier to predict PolitiFact-style six-way veracity labels from (claim + claim-focused summaries), reporting accuracy, soft-accuracy (off-by-one allowed), Macro-F1, and mean absolute error (MAE) on the ClaimDECOMP test set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Supervised classification compared to professional fact-checker labels (PolitiFact) using a fine-tuned DeBERTa-large model; report standard classification metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy (Acc), Soft Accuracy (counts off-by-one on 6-point scale as correct), Macro-F1, Mean Absolute Error (MAE) on 6-point veracity scale.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>DeBERTa-large (as downstream classifier); text-davinci-003 used earlier in pipeline for decomposition and summarization</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>political fact-checking / social sciences (claim verification)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not a scientific theory per se: LLM-produced claim-focused evidence summaries are used as inputs to a classifier to produce veracity judgments; evaluation treats veracity judgments as a proxy for system effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Default system (subquestion-driven retrieval + zero-shot summarization) on test: Acc=33.0, Soft Acc=74.5, Macro-F1=34.5, MAE=0.99 (Table 3). Unconstrained retrieval (no time/site constraints) yields much higher Acc (Dev 50.5, Test 49.0) showing dependence on retrieved fact-check pages.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>ClaIMDECOMP (ClaimDECOMP) dataset: 1,200 complex political claims with six-way labels from PolitiFact (train 800 / dev 200 / test 200).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Model predictions are compared directly to professional fact-checker labels (PolitiFact). An oracle upper bound (Claim + Justification) achieves much higher scores (Test Acc 57.5), indicating room between automated and human-justified performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Veracity labels are subjective; classification accuracy is only a proxy for system utility. Performance strongly depends on retrieval setting (temporal/site constraints) and available evidence; many claims are uncheckable from web evidence available before claim date.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complex Claim Verification with Evidence Retrieved in the Wild', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6154.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6154.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FaithfulnessEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human faithfulness evaluation of claim-focused summaries</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Crowdsourced human evaluation to measure how faithfully LLM-generated summaries represent the source documents, using a four-way label set and numeric scoring to quantify hallucination rates and factual errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Crowdsourced annotation (Amazon Mechanical Turk) of document-summary pairs; annotators label each summary as Faithful / Minor Factual Error / Major Factual Error / Completely Wrong and provide rationales; consensus by majority vote; inter-annotator agreement reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Four discrete labels mapped to numeric scores (Faithful=4, Minor=3, Major=2, Completely Wrong=1); report percentage in each category and average score; Fleiss' Kappa for annotator agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>text-davinci-003 for summarization (zero-shot and few-shot prompts); also compared to davinci-001 for baseline zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>political fact-checking / summarization quality</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Evaluation of LLM-generated evidence summaries (not scientific theories) for factual fidelity to retrieved documents, to assess risk of hallucination when LLMs compress evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Few-shot prompt on text-davinci-003: Faithful 82.5%, Minor 6.5%, Major 8.5%, Completely Wrong 2.5%, average score 3.69 (N=200). Zero-shot-003: Faithful ~66%, higher major/complete errors. Fleiss' Kappa = 0.30 (annotator agreement).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>200 document-summary pairs randomly sampled from ClaimDECOMP development set (50 claims / 200 pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Summaries judged against the original documents by human annotators; few-shot prompting markedly reduced hallucination relative to zero-shot and earlier models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Subjectivity in error severity (moderate inter-annotator agreement). Faithful summaries can still omit needed evidence; evaluation only checks fidelity to individual documents, not coverage across full evidence set.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complex Claim Verification with Evidence Retrieved in the Wild', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6154.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6154.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ComprehensivenessEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human comprehensiveness evaluation (answerability of subquestions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Crowdsourced assessment of whether claim-focused summaries enable answering annotated yes/no subquestions derived from claims, providing a proxy for how comprehensive the returned evidence is.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Annotators label each (summary, subquestion) pair as Answerable / Partially Answerable / Unanswerable and provide yes/no when answerable; consensus via qualified workers; Fleiss' Kappa reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Fraction of subquestions labeled Answerable / Partially / Unanswerable; claim-level aggregation (answerable if all subquestions answerable), and cross-tabulation with faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>text-davinci-003 for summarization; questions are gold from ClaimDECOMP or generated by the pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>political fact-checking / evidence coverage</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Measure of whether LLM-produced summaries contain (or imply) the information needed to resolve each subaspect of a complex claim (proxy for completeness of evidence).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>On 161 subquestions (50 claims): zero-shot-003 -> Answerable 47.8%, Partially 22.4%, Unanswerable 29.8%; few-shot-003 -> Answerable 42.9%, Partially 21.1%, Unanswerable 36.0% (Table 6). Claim-level: 36/50 claims had at least one unanswerable question; among claims fully addressable, only 1/14 had a major factual error in summaries (Table 7). Fleiss' Kappa = 0.32.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Subset from ClaimDECOMP (50 claims, 161 subquestions).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Evaluates whether LLM summaries give humans sufficient information to answer subquestions that experts annotated; shows LLM summaries help but often fail to cover all aspects.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Comprehensiveness is subjective and can be inflated by hallucinations (zero-shot summaries more 'answerable' but less faithful). Many subquestions remain unanswerable due to retrieval limits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complex Claim Verification with Evidence Retrieved in the Wild', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6154.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6154.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RetrievalStability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stability evaluation of first-stage web retrieval (Jaccard and downstream impact)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Measurement of how search-engine retrieval results change over time and how that affects downstream veracity classification, using Jaccard overlap of URL sets and downstream metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Repeated runs of first-stage retrieval at different times (T=0, T=1 week, T=2 months); compute Jaccard overlap of retrieved URL sets and measure veracity classifier metrics on outputs using each retrieval round.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Jaccard similarity of URL sets; downstream Acc, Soft-Acc, Macro-F1, MAE; statistical significance testing of changes.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Not an LLM per se for retrieval, but pipeline uses text-davinci-003 downstream; Bing Search API for retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>information retrieval robustness for fact-checking systems</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Evaluate temporal reproducibility and sensitivity of web-based evidence retrieval feeding into LLM summarization and veracity classification.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Jaccard overlap: 1 week = 0.48, 2 months = 0.30 (relative to original). Downstream veracity metrics changed modestly and not statistically significantly: e.g., Acc 33.0 (orig) -> 33.5 (1 week) -> 29.5 (2 months) (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Queries derived from ClaimDECOMP; retrieval via Bing Search API with temporal/site constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Shows that while raw retrieved URLs drift substantially, aggregated downstream veracity judgments are relatively stable (small, non-significant changes).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Commercial search API results fluctuate; makes reproducible benchmarking difficult. Jaccard on URLs underestimates semantic overlap if content moves or is mirrored.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complex Claim Verification with Evidence Retrieved in the Wild', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6154.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6154.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SummPromptAblation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation study on summarization prompting (zero-shot vs few-shot vs no-summary)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Controlled experiments comparing summarization strategies for claim-focused summaries (zero-shot and few-shot prompts using text-davinci-003) and the effect of skipping summarization on downstream veracity classification and human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Ablation runs of full pipeline varying the summarization prompt: zero-shot-003, few-shot-003, and no summary (feed text spans directly to classifier); measure downstream classifier metrics and run human faithfulness/comprehensiveness studies.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Downstream Acc, Soft Acc, Macro-F1, MAE; human faithfulness percentages and average scores; comprehensiveness answerable rates.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>text-davinci-003 for summarization (zero-shot and few-shot); davinci-001 compared as weaker baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>LM summarization quality impacting fact verification</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Investigate whether and how LLM prompting strategies affect hallucination rates and usefulness of summaries as evidence for claim verification.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Downstream (test set): few-shot-003 yields Acc=35.0, Soft Acc=76.5, Macro-F1=36.2, MAE=0.94; zero-shot (default row in Table 3) Acc=33.0; no-summary (raw text spans) performs worse: Acc=29.0, Soft Acc=66.0, Macro-F1=26.3, MAE=1.18. Human faithfulness: few-shot-003 has substantially fewer hallucinations (Faithful 82.5%) vs zero-shot variants (≈66%).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>ClaimDECOMP (dev/test splits) for automatic metrics; sampled document-summary pairs for human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Few-shot summaries are judged more faithful by humans and yield better downstream classifier metrics than no-summary; zero-shot summaries appear to hallucinate more, inflating 'answerable' rates but harming faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Prompt design impacts hallucination vs coverage trade-offs; stronger models and better few-shot exemplars reduce but do not eliminate errors; classifier and summarizer interactions are complex.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complex Claim Verification with Evidence Retrieved in the Wild', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6154.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6154.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BM25SSR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Second-stage retrieval with BM25 (chunking, expansion, top-K selection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deterministic, scalable second-stage retrieval using BM25 over overlapping k1-word chunks (stride k1/2) with ±k2-word context expansion, selecting top-K1 spans and merging overlaps to produce top-K2 document spans fed to summarizer/classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Engineered retrieval pipeline component evaluated via downstream end-to-end metrics and ablations (compare using subquestions vs raw claim for first-stage queries, and different inputs for second-stage retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Downstream veracity classification metrics; ablation effects on accuracy and Macro-F1; also computational efficiency (reduction in words/context).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>BM25 is a classical IR method; downstream uses text-davinci-003 and DeBERTa-large.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>information retrieval for evidence selection in fact-checking</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Procedure to compress and select the most relevant textual spans from many retrieved web pages before LLM summarization, enabling feasible LLM processing and improving relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Hyperparameters used: k1=30 (chunk size), k2=150 (expansion), top-K1=10 (spans), top-K2=4 (documents). Ablations show second-stage retrieval input choice changes performance slightly; retrieval and expansion help capture needed context. Pipeline reduces average words from ~70k (first-stage) to ~2.7k after second-stage and to 251 words after summarization (Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Applied to documents retrieved for ClaimDECOMP queries (first-stage via Bing).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Human evaluations indicated that compressed, claim-focused summaries are useful; second-stage retrieval helps produce focused inputs for summarizers evaluated by humans.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>BM25-based span selection may miss nuanced evidence; coarse chunking/expansion parameters are heuristic; retrieval failures (missing web evidence) are a primary bottleneck for end-to-end performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complex Claim Verification with Evidence Retrieved in the Wild', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generating literal and implied subquestions to fact-check complex claims <em>(Rating: 2)</em></li>
                <li>WebGPT: Browser-assisted question-answering with human feedback <em>(Rating: 2)</em></li>
                <li>AVeriTeC: A dataset for real-world claim verification with evidence from the web <em>(Rating: 2)</em></li>
                <li>Generating fact checking briefs <em>(Rating: 1)</em></li>
                <li>MultiFC: A real-world multi-domain dataset for evidence-based fact checking of claims <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6154",
    "paper_id": "paper-4fb0e1283194c0d20ac0396bc375fdf91c14da1e",
    "extraction_schema_id": "extraction-schema-121",
    "extracted_data": [
        {
            "name_short": "VeracityEval",
            "name_full": "Automatic veracity classification evaluation (DeBERTa-large)",
            "brief_description": "Automatic evaluation of system outputs by training a DeBERTa-large classifier to predict PolitiFact-style six-way veracity labels from (claim + claim-focused summaries), reporting accuracy, soft-accuracy (off-by-one allowed), Macro-F1, and mean absolute error (MAE) on the ClaimDECOMP test set.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Supervised classification compared to professional fact-checker labels (PolitiFact) using a fine-tuned DeBERTa-large model; report standard classification metrics.",
            "evaluation_criteria": "Accuracy (Acc), Soft Accuracy (counts off-by-one on 6-point scale as correct), Macro-F1, Mean Absolute Error (MAE) on 6-point veracity scale.",
            "llm_model_name": "DeBERTa-large (as downstream classifier); text-davinci-003 used earlier in pipeline for decomposition and summarization",
            "theory_domain": "political fact-checking / social sciences (claim verification)",
            "theory_description": "Not a scientific theory per se: LLM-produced claim-focused evidence summaries are used as inputs to a classifier to produce veracity judgments; evaluation treats veracity judgments as a proxy for system effectiveness.",
            "evaluation_results": "Default system (subquestion-driven retrieval + zero-shot summarization) on test: Acc=33.0, Soft Acc=74.5, Macro-F1=34.5, MAE=0.99 (Table 3). Unconstrained retrieval (no time/site constraints) yields much higher Acc (Dev 50.5, Test 49.0) showing dependence on retrieved fact-check pages.",
            "benchmarks_or_datasets": "ClaIMDECOMP (ClaimDECOMP) dataset: 1,200 complex political claims with six-way labels from PolitiFact (train 800 / dev 200 / test 200).",
            "comparison_to_human": "Model predictions are compared directly to professional fact-checker labels (PolitiFact). An oracle upper bound (Claim + Justification) achieves much higher scores (Test Acc 57.5), indicating room between automated and human-justified performance.",
            "limitations_or_challenges": "Veracity labels are subjective; classification accuracy is only a proxy for system utility. Performance strongly depends on retrieval setting (temporal/site constraints) and available evidence; many claims are uncheckable from web evidence available before claim date.",
            "uuid": "e6154.0",
            "source_info": {
                "paper_title": "Complex Claim Verification with Evidence Retrieved in the Wild",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "FaithfulnessEval",
            "name_full": "Human faithfulness evaluation of claim-focused summaries",
            "brief_description": "Crowdsourced human evaluation to measure how faithfully LLM-generated summaries represent the source documents, using a four-way label set and numeric scoring to quantify hallucination rates and factual errors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Crowdsourced annotation (Amazon Mechanical Turk) of document-summary pairs; annotators label each summary as Faithful / Minor Factual Error / Major Factual Error / Completely Wrong and provide rationales; consensus by majority vote; inter-annotator agreement reported.",
            "evaluation_criteria": "Four discrete labels mapped to numeric scores (Faithful=4, Minor=3, Major=2, Completely Wrong=1); report percentage in each category and average score; Fleiss' Kappa for annotator agreement.",
            "llm_model_name": "text-davinci-003 for summarization (zero-shot and few-shot prompts); also compared to davinci-001 for baseline zero-shot.",
            "theory_domain": "political fact-checking / summarization quality",
            "theory_description": "Evaluation of LLM-generated evidence summaries (not scientific theories) for factual fidelity to retrieved documents, to assess risk of hallucination when LLMs compress evidence.",
            "evaluation_results": "Few-shot prompt on text-davinci-003: Faithful 82.5%, Minor 6.5%, Major 8.5%, Completely Wrong 2.5%, average score 3.69 (N=200). Zero-shot-003: Faithful ~66%, higher major/complete errors. Fleiss' Kappa = 0.30 (annotator agreement).",
            "benchmarks_or_datasets": "200 document-summary pairs randomly sampled from ClaimDECOMP development set (50 claims / 200 pairs).",
            "comparison_to_human": "Summaries judged against the original documents by human annotators; few-shot prompting markedly reduced hallucination relative to zero-shot and earlier models.",
            "limitations_or_challenges": "Subjectivity in error severity (moderate inter-annotator agreement). Faithful summaries can still omit needed evidence; evaluation only checks fidelity to individual documents, not coverage across full evidence set.",
            "uuid": "e6154.1",
            "source_info": {
                "paper_title": "Complex Claim Verification with Evidence Retrieved in the Wild",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ComprehensivenessEval",
            "name_full": "Human comprehensiveness evaluation (answerability of subquestions)",
            "brief_description": "Crowdsourced assessment of whether claim-focused summaries enable answering annotated yes/no subquestions derived from claims, providing a proxy for how comprehensive the returned evidence is.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Annotators label each (summary, subquestion) pair as Answerable / Partially Answerable / Unanswerable and provide yes/no when answerable; consensus via qualified workers; Fleiss' Kappa reported.",
            "evaluation_criteria": "Fraction of subquestions labeled Answerable / Partially / Unanswerable; claim-level aggregation (answerable if all subquestions answerable), and cross-tabulation with faithfulness.",
            "llm_model_name": "text-davinci-003 for summarization; questions are gold from ClaimDECOMP or generated by the pipeline.",
            "theory_domain": "political fact-checking / evidence coverage",
            "theory_description": "Measure of whether LLM-produced summaries contain (or imply) the information needed to resolve each subaspect of a complex claim (proxy for completeness of evidence).",
            "evaluation_results": "On 161 subquestions (50 claims): zero-shot-003 -&gt; Answerable 47.8%, Partially 22.4%, Unanswerable 29.8%; few-shot-003 -&gt; Answerable 42.9%, Partially 21.1%, Unanswerable 36.0% (Table 6). Claim-level: 36/50 claims had at least one unanswerable question; among claims fully addressable, only 1/14 had a major factual error in summaries (Table 7). Fleiss' Kappa = 0.32.",
            "benchmarks_or_datasets": "Subset from ClaimDECOMP (50 claims, 161 subquestions).",
            "comparison_to_human": "Evaluates whether LLM summaries give humans sufficient information to answer subquestions that experts annotated; shows LLM summaries help but often fail to cover all aspects.",
            "limitations_or_challenges": "Comprehensiveness is subjective and can be inflated by hallucinations (zero-shot summaries more 'answerable' but less faithful). Many subquestions remain unanswerable due to retrieval limits.",
            "uuid": "e6154.2",
            "source_info": {
                "paper_title": "Complex Claim Verification with Evidence Retrieved in the Wild",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "RetrievalStability",
            "name_full": "Stability evaluation of first-stage web retrieval (Jaccard and downstream impact)",
            "brief_description": "Measurement of how search-engine retrieval results change over time and how that affects downstream veracity classification, using Jaccard overlap of URL sets and downstream metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Repeated runs of first-stage retrieval at different times (T=0, T=1 week, T=2 months); compute Jaccard overlap of retrieved URL sets and measure veracity classifier metrics on outputs using each retrieval round.",
            "evaluation_criteria": "Jaccard similarity of URL sets; downstream Acc, Soft-Acc, Macro-F1, MAE; statistical significance testing of changes.",
            "llm_model_name": "Not an LLM per se for retrieval, but pipeline uses text-davinci-003 downstream; Bing Search API for retrieval.",
            "theory_domain": "information retrieval robustness for fact-checking systems",
            "theory_description": "Evaluate temporal reproducibility and sensitivity of web-based evidence retrieval feeding into LLM summarization and veracity classification.",
            "evaluation_results": "Jaccard overlap: 1 week = 0.48, 2 months = 0.30 (relative to original). Downstream veracity metrics changed modestly and not statistically significantly: e.g., Acc 33.0 (orig) -&gt; 33.5 (1 week) -&gt; 29.5 (2 months) (Table 4).",
            "benchmarks_or_datasets": "Queries derived from ClaimDECOMP; retrieval via Bing Search API with temporal/site constraints.",
            "comparison_to_human": "Shows that while raw retrieved URLs drift substantially, aggregated downstream veracity judgments are relatively stable (small, non-significant changes).",
            "limitations_or_challenges": "Commercial search API results fluctuate; makes reproducible benchmarking difficult. Jaccard on URLs underestimates semantic overlap if content moves or is mirrored.",
            "uuid": "e6154.3",
            "source_info": {
                "paper_title": "Complex Claim Verification with Evidence Retrieved in the Wild",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SummPromptAblation",
            "name_full": "Ablation study on summarization prompting (zero-shot vs few-shot vs no-summary)",
            "brief_description": "Controlled experiments comparing summarization strategies for claim-focused summaries (zero-shot and few-shot prompts using text-davinci-003) and the effect of skipping summarization on downstream veracity classification and human judgments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Ablation runs of full pipeline varying the summarization prompt: zero-shot-003, few-shot-003, and no summary (feed text spans directly to classifier); measure downstream classifier metrics and run human faithfulness/comprehensiveness studies.",
            "evaluation_criteria": "Downstream Acc, Soft Acc, Macro-F1, MAE; human faithfulness percentages and average scores; comprehensiveness answerable rates.",
            "llm_model_name": "text-davinci-003 for summarization (zero-shot and few-shot); davinci-001 compared as weaker baseline.",
            "theory_domain": "LM summarization quality impacting fact verification",
            "theory_description": "Investigate whether and how LLM prompting strategies affect hallucination rates and usefulness of summaries as evidence for claim verification.",
            "evaluation_results": "Downstream (test set): few-shot-003 yields Acc=35.0, Soft Acc=76.5, Macro-F1=36.2, MAE=0.94; zero-shot (default row in Table 3) Acc=33.0; no-summary (raw text spans) performs worse: Acc=29.0, Soft Acc=66.0, Macro-F1=26.3, MAE=1.18. Human faithfulness: few-shot-003 has substantially fewer hallucinations (Faithful 82.5%) vs zero-shot variants (≈66%).",
            "benchmarks_or_datasets": "ClaimDECOMP (dev/test splits) for automatic metrics; sampled document-summary pairs for human evaluation.",
            "comparison_to_human": "Few-shot summaries are judged more faithful by humans and yield better downstream classifier metrics than no-summary; zero-shot summaries appear to hallucinate more, inflating 'answerable' rates but harming faithfulness.",
            "limitations_or_challenges": "Prompt design impacts hallucination vs coverage trade-offs; stronger models and better few-shot exemplars reduce but do not eliminate errors; classifier and summarizer interactions are complex.",
            "uuid": "e6154.4",
            "source_info": {
                "paper_title": "Complex Claim Verification with Evidence Retrieved in the Wild",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "BM25SSR",
            "name_full": "Second-stage retrieval with BM25 (chunking, expansion, top-K selection)",
            "brief_description": "A deterministic, scalable second-stage retrieval using BM25 over overlapping k1-word chunks (stride k1/2) with ±k2-word context expansion, selecting top-K1 spans and merging overlaps to produce top-K2 document spans fed to summarizer/classifier.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Engineered retrieval pipeline component evaluated via downstream end-to-end metrics and ablations (compare using subquestions vs raw claim for first-stage queries, and different inputs for second-stage retrieval).",
            "evaluation_criteria": "Downstream veracity classification metrics; ablation effects on accuracy and Macro-F1; also computational efficiency (reduction in words/context).",
            "llm_model_name": "BM25 is a classical IR method; downstream uses text-davinci-003 and DeBERTa-large.",
            "theory_domain": "information retrieval for evidence selection in fact-checking",
            "theory_description": "Procedure to compress and select the most relevant textual spans from many retrieved web pages before LLM summarization, enabling feasible LLM processing and improving relevance.",
            "evaluation_results": "Hyperparameters used: k1=30 (chunk size), k2=150 (expansion), top-K1=10 (spans), top-K2=4 (documents). Ablations show second-stage retrieval input choice changes performance slightly; retrieval and expansion help capture needed context. Pipeline reduces average words from ~70k (first-stage) to ~2.7k after second-stage and to 251 words after summarization (Table 8).",
            "benchmarks_or_datasets": "Applied to documents retrieved for ClaimDECOMP queries (first-stage via Bing).",
            "comparison_to_human": "Human evaluations indicated that compressed, claim-focused summaries are useful; second-stage retrieval helps produce focused inputs for summarizers evaluated by humans.",
            "limitations_or_challenges": "BM25-based span selection may miss nuanced evidence; coarse chunking/expansion parameters are heuristic; retrieval failures (missing web evidence) are a primary bottleneck for end-to-end performance.",
            "uuid": "e6154.5",
            "source_info": {
                "paper_title": "Complex Claim Verification with Evidence Retrieved in the Wild",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generating literal and implied subquestions to fact-check complex claims",
            "rating": 2
        },
        {
            "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
            "rating": 2
        },
        {
            "paper_title": "AVeriTeC: A dataset for real-world claim verification with evidence from the web",
            "rating": 2
        },
        {
            "paper_title": "Generating fact checking briefs",
            "rating": 1
        },
        {
            "paper_title": "MultiFC: A real-world multi-domain dataset for evidence-based fact checking of claims",
            "rating": 1
        }
    ],
    "cost": 0.01578725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Complex Claim Verification with Evidence Retrieved in the Wild</h1>
<p>Jifan Chen Grace Kim Aniruddh Sriram Greg Durrett Eunsol Choi<br>Department of Computer Science<br>The University of Texas at Austin<br>jf_chen@utexas.edu</p>
<h4>Abstract</h4>
<p>Retrieving evidence to support or refute claims is a core part of automatic fact-checking. Prior work makes simplifying assumptions in retrieval that depart from real-world use cases: either no access to evidence, access to evidence curated by a human fact-checker, or access to evidence published after a claim was made. In this work, we present the first realistic pipeline to check real-world claims by retrieving raw evidence from the web. We restrict our retriever to only search documents available prior to the claim's making, modeling the realistic scenario of emerging claims. Our pipeline includes five components: claim decomposition, raw document retrieval, fine-grained evidence retrieval, claim-focused summarization, and veracity judgment. We conduct experiments on complex political claims in the ClaIMDECOMP dataset and show that the aggregated evidence produced by our pipeline improves veracity judgments. Human evaluation finds the evidence summary produced by our system is reliable (it does not hallucinate information) and relevant to answering key questions about a claim, suggesting that it can assist fact-checkers even when it does not reflect a complete evidence set. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>To combat the rise of misinformation, the NLP community has developed automatic fact-checking tools. However, these automated systems are not ready for wide adoption at real fact-checking organizations. Prior work handling real claims either relies on access to a document set which contains the "gold" evidence (Ferreira and Vlachos, 2016; Alhindi et al., 2018; Hanselowski et al., 2019; Atanasova et al., 2020) or conducts unconstrained retrieval (Augenstein et al., 2019), which may retrieve articles written by fact-checkers about the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Our fact-checking setting addresses realistic claims using evidence retrieved prior to when the claim was made.
claim (example in Figure 1).
We present the first study of fact-checking political claims under a realistic retrieval setting. Our retrieval over the web is restricted to documents authored before the time of the claim and not sourced from fact-checking websites, as shown by the left side of Figure 1. We propose a pipeline (illustrated in Figure 2) that builds upon prior work in fact checking as well as large language models (Brown et al., 2020) to handle the complexity of this setting. Our system first decomposes a claim into a series of subquestions (Chen et al., 2022a; Ousidhoum et al., 2022), targeting both explicit and implicit aspects of the claim. Each subquestion is fed into a commercial search engine to retrieve relevant documents, with the restrictions described above. Then, we conduct a second stage of fine-grained retrieval to isolate the most relevant portions of the documents. Finally, we use state-of-the-art language models (Brown et al., 2020; Ouyang et al., 2022) to generate claim-focused summaries from the retrieved content. These summaries can serve both as explanations for users as well as inputs to a classifier to determine the veracity based on these summaries.</p>
<p>Evaluating individual components of our</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of our pipeline: a claim is decomposed into yes/no subquestions (Sec. 3.1), then we use the questions in two stages of retrieval (Sec. 3.2 and Sec. 3.3) to select the most relevant paragraphs. Finally, we generate a claim-focused summary (Sec. 3.4) and train a veracity classifier to get the veracity label (Sec. 3.5). This filters contents irrelevant to the claim (see Appendix B for details and an example in Figure 5).</p>
<p>pipeline is challenging due to the absence of gold annotations at each stage. We use automatic evaluation on the veracity classification performance, comparing to labels given by professional fact-checkers. We supplement this with a human study evaluating the claim-focused summaries for comprehensiveness and faithfulness. This evaluation counterbalances the subjectivity of the veracity judgments (Lim, 2018) while shedding light on intermediate stages of the process.</p>
<p>We apply our pipeline to ClaIMDECOMP (Chen et al., 2022a), a dataset containing 1,200 real-world complex political claims with veracity labels. Performance on veracity classification shows that: (1) our retrieval setting is indeed much harder than "unrestricted" retrieval settings; (2) using web evidence leads to performance gains compared to automatic fact-checking without evidence; (3) the decomposition is crucial for obtaining high-quality raw documents from the web compared to using the original claim alone. Our human study further indicates that: (4) claim-focused summaries are mostly faithful and helpful for both machines and humans to fact-check a claim; (5) the retrieved evidence is often relevant to some aspects of the claim, but can rarely cover all aspects, suggesting that finding sufficient raw evidence in the wild is the core challenge in building automatic fact-checking systems.</p>
<h2>2 Background and Motivation</h2>
<p>Early NLP research on fact-checking political claims (Vlachos and Riedel, 2014; Wang, 2017; Rashkin et al., 2017; Volkova et al., 2017; Pérez-Rosas et al., 2018; Dungs et al., 2018) typically considered using the claim alone as an input to an automated system. By not seeking evidence, systems judge the veracity of a claim mostly based on surface-level linguistic patterns rather than based on factual errors. Research that incorporates evidence either assumes access to justifications provided by fact-checkers (Vlachos and Riedel, 2014; Alhindi et al., 2018; Hanselowski et al., 2019; Atanasova et al., 2020) or evidence from <em>unconstrained</em> retrieval (Popat et al., 2017, 2018; Augenstein et al., 2019), which frequently yields evidence sets containing pages from fact-checking websites (Glockner et al., 2022). This does not reflect the difficulties in real-world evidence retrieval. Fan et al. (2020) explore generating questions to retrieve evidence from the web, but only evaluate their system with humans in the loop, who can aggressively filter irrelevant retrieval results. Contemporaneous to this work, Schlichtkrull et al. (2023) construct a dataset, AVeriTeC, using real-world claims and evidence retrieved from the web. Our method uses binary subquestions designed to target all needed aspects of factuality for a claim, whereas their questions are wh-questions optimized around retrieval, similar to QABriefs (Fan et al., 2020).</p>
<p>To our knowledge, we present the first automatic fact-checking system with a realistic retrieval pipeline using evidence available at the time a claim was made. This presents a very challenging setting where many claims are not checkable. We therefore emphasize the evidence our system returns as a way of assisting human fact-checkers; we believe this realistic task setting and corresponding evaluation should be reused in future work.</p>
<p>Our work shifts the focus away from the evaluation on classification accuracy alone. Accuracy on truth labels assigned by fact-checkers is a proxy metric we use to evaluate our systems. However, fact-checking experts argue that the task is too subjective and complex to be automated in the near term (Graves, 2018; Nakov et al., 2021). Part of this arises from the fact that information needed to check claims is not always available on the web (Singh et al., 2021). Our approach of returning</p>
<p>Claim: James Quintero stated on October 10, 2016 in a panel discussion at SXSW Eco: When San Francisco banned plastic grocery bags, "you saw the number of instances of people going to the ER with things like salmonella and other related illnesses" spike.</p>
<h2>text-davinci-003</h2>
<p>$\nabla$
Q1: Did San Francisco ban plastic grocery bags in the past?
Q2: Did the number of people going to the ER for salmonella and other related illnesses increase after the plastic bag ban in San Francisco?
Q10: Was the plastic bag ban directly caused the increase of the number of people going to the ER for salmonella and other related illnesses in San Fransisco?</p>
<p>Figure 3: An example of our claim decomposition process: each claim is decomposed into ten subquestions.
information on a best-effort basis and providing evidence to enable humans to assist in the judgment can help overcome issues with returning judgments from error-prone AI systems (Bansal et al., 2021; Brand et al., 2022).</p>
<h2>3 Methodology</h2>
<p>Our pipeline, shown in Figure 3, consists of five parts: claim decomposition, raw document retrieval, fine-grained retrieval, claim-focused summarization, and veracity classification. We describe each part below.</p>
<h3>3.1 Claim Decomposition</h3>
<p>Given a real-world complex claim, we first decompose it into a set of yes/no questions for which the answers are useful to fact-check the claim. Chen et al. (2022a); Ousidhoum et al. (2022) show that such decompositions are both helpful to retrieve relevant evidence and make veracity judgments.</p>
<p>For decomposition, we prompt a large-scale language model, text-davinci-003, with incontext examples. ${ }^{2}$ We carefully choose four inputdecomposition pairs from the human annotations of Chen et al. (2022a) to form a few-shot prompt. We generate a set of questions through multiple rounds of sampling until we gather 10 different questions. An example decomposition is shown in Figure 3. For the full prompt, see Appendix A.2.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Q2: Did the number of people going to the ER for salmonella and other related illnesses increase after the plastic bag ban in San Francisco? (Claim date: October 10, 2016)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Bing Search</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Plastic Bag Ban Responsible</td>
</tr>
<tr>
<td style="text-align: left;">For Spike In E. Coll Infections,</td>
</tr>
<tr>
<td style="text-align: left;">Study Says ...</td>
</tr>
<tr>
<td style="text-align: left;">a 46 percent increase in</td>
</tr>
<tr>
<td style="text-align: left;">deaths from foodborne illness</td>
</tr>
<tr>
<td style="text-align: left;">in the three months after the</td>
</tr>
<tr>
<td style="text-align: left;">bag ban went into effect in</td>
</tr>
<tr>
<td style="text-align: left;">2007 ...</td>
</tr>
<tr>
<td style="text-align: left;">- HuffPost (Feb. 7, 2013)</td>
</tr>
</tbody>
</table>
<h4>Abstract</h4>
<p>Did bag ban cause disease? Evidence is shaky ... This declaration relied on a study that has numerous questions about its methodology and conclusions. We rate this Mostly False. - Austin American-statesman (Nov. 25, 2016)</p>
<p>Figure 4: Two documents returned by searching Q2 (generated in step 1). The right page post-dates the claim by one month and directly cites a PolitiFact article, making it problematic to use as raw evidence.</p>
<h3>3.2 First-stage Retrieval</h3>
<p>For each question generated in the previous step, we feed it to a commercial search engine API to collect the relevant documents.</p>
<p>Temporal and Site Constraints We assume that a system should not be able to access pages published after the claim was made. This condition matches real-time fact-checking scenario during a political speech. We place a temporal constraint on the system to reflect this. Next, to investigate how the presence of fact-checking websites affects the veracity judgment of a claim, we also place a site constraint to filter out the documents from fact-checking websites. Our list of fact-checking websites can be found in Appendix A.1. An example of the retrieved documents is shown in Figure 4.</p>
<p>We use the Bing Search API, ${ }^{3}$ and retrieve 10 documents per subquestion after filtering by the constraints. We extract the actual content from the page URLs using two tools: html2text ${ }^{4}$ and readability-lxml. ${ }^{5}$ Approximately one-third of the URLs are protected ${ }^{6}$ and cannot be scraped.</p>
<p>Table 1 contains the raw counts from web retrieval with and without the timestamp of a claim. These results underscore the importance of temporal filtering: we find little overlap between the two document sets by comparing the Jaccard distance between two sets of the retrieved URLs.</p>
<p>One challenge for the reproducibility of our work</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>Claim: Melissa Agard stated on September 2, 2021 in News release: "No other country on the planet witnesses the number of gun deaths that we do here in the United States, and it's not even close."</p>
<h1>Decomposed subquestions:</h1>
<p>(1) Is the United States the country with the highest rate of gun deaths?
(2) Does the claim account for population size (i.e., per capita rates), or is it based on total numbers?
(3) Does the statement consider gun deaths relative to the total number of guns in the country?
(4) Is the number of gun deaths in the United States substantially higher when compared to countries of similar economic and political stability?
(5) Do gun deaths account for a large portion of deaths in the U.S.?
...
(10) Are there any mitigating factors that affect the gun death rate in the United States?</p>
<h2>Retrieved documents and summaries:</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Doc title</th>
<th style="text-align: center;">URL</th>
<th style="text-align: center;">Summary</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Firearm-related deaths rate U.S. by gender 1970-2016 { Statista (Nov 7, 2019)</td>
<td style="text-align: center;">https://www.statista.com/statistics/ 186951/deaths-by-firearm-related-injuries-in-the-us-by-gender-once-1970/</td>
<td style="text-align: center;">The death rate from firearm-related injuries in the United States is 19.4 per 100,000 population among males, and homicides from firearms account for 72.6 percent of all homicides in the U.S. The ownership of legal firearms is widespread, with around 43 percent of households having at least one firearm. (Faithful)</td>
</tr>
<tr>
<td style="text-align: center;">Do Gun Laws Affect the Rate of Shooting Deaths? (Oct 12, 2018)</td>
<td style="text-align: center;">https://www.thetrace.org/2018/10/ do-gun-laws-affect-the-rate-of-shooting-deaths/</td>
<td style="text-align: center;">The Centers for Disease Control and Prevention tracks gun deaths in all 50 states and the national rate of gun deaths in 2016 was 11.8. No additional evidence is provided to compare the number of gun deaths in the US to other countries. (Faithful)</td>
</tr>
<tr>
<td style="text-align: center;">Gun Violence Deaths: How The U.S. Compares With The Rest Of The World (Mar 24, 2021)</td>
<td style="text-align: center;">https://www.kuow.org/stories/gun-violence-deaths-how-the-u-s-compares-with-the-rest-of-the-world</td>
<td style="text-align: center;">The United States has the 32nd highest rate of deaths from gun violence with 3.96 deaths per 100,000 people compared to countries such as Canada with 0.47 deaths per 100,000 people and the United Kingdom with 0.04 deaths per 100,000 people. (Faithful)</td>
</tr>
<tr>
<td style="text-align: center;">A Doctor's Insights Into Gun Violence And Gun Laws Around The World (Aug 6, 2019)</td>
<td style="text-align: center;">https://www.kuer.org/2019-08-06/a-doctors-insights-into-gun-violence-and-gun-laws-around-the-world</td>
<td style="text-align: center;">The US rate of deaths from gun violence is 4.43 deaths per 100,000 people and is four times higher than the rates in war-torn Syria and Yemen. This places is among the top 30 countries in the world with the highest rates of deaths from gun violence. (Faithful)</td>
</tr>
</tbody>
</table>
<h2>Annotated subquestions:</h2>
<p>Does the U.S. have the highest number of gun deaths out of all the countries on the planet? No (annotator judgment based on summaries) Does the U.S. have a high number of gun deaths? Yes (annotator judgment based on summaries)
Does the U.S. have a high number of gun deaths when looking at deaths as a share of the population? Yes (annotator judgment based on summaries)
Model Prediction: Half-True
Label: Mostly-False
Figure 5: System outputs for an example picked from the dev set of ClaIMDECOMP: the claim is first decomposed into a set of yes/no questions and then the top four retrieved documents (through first and second stage retrieval) are summarized. Finally, a trained DeBERTa model makes a prediction regarding the four summarized documents.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"># retrieved</th>
<th style="text-align: center;"># scraped</th>
<th style="text-align: center;"># words</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">w/ timestamp</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">1,561</td>
</tr>
<tr>
<td style="text-align: left;">w/o timestamp</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">1,660</td>
</tr>
<tr>
<td style="text-align: left;">Jaccard score</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 1: The statistics for the retrieved documents obtained through the first-stage retrieval after filtering the documents from fact-checking websites. Jaccard between these two sets show that incorporating the timestamp in retrieval makes a substantial difference.
is that commercial search engines may return different results over time. In Section 5.3, we experiment with the same query set at different times. We find that the search results change over time: only $30 \%$ of search output URLs overlap when queried two months apart. However, the veracity judgment classification result is not impacted much.</p>
<h3>3.3 Second-stage Retrieval</h3>
<p>Most of the documents collected from the previous step contain at most a few snippets relevant to the
claim. However, as can be seen from Figure 2, firststage retrieval can easily result in tens of thousands of words of retrieved documents, which are costly to process with an LLM. Furthermore, even with state-of-the-art language models, it is hard to do complex reasoning over such long context (Liu et al., 2024; Levy et al., 2024). Thus, we conduct a second-stage retrieval to pick the most relevant text spans to the claim from the retrieved documents. Specifically, we segment the documents into text spans containing $k_{1}$ words with a stride of $\frac{1}{2} k_{1}$ words. Following Chen et al. (2022a), we employ $\mathrm{BM}-25$ to retrieve the top- $K_{1}$ highest-scored text spans, expanding these spans with a $\pm k_{2}$-word context. If two text spans overlap, they are merged to form a larger span. This process yields a set of "documents" ranked by the highest-scored text spans, of which we pick the top- $K_{2}$.</p>
<h3>3.4 Claim-Focused Summarization</h3>
<p>Since the documents retrieved in the previous step can contain up to several thousand words, it be-</p>
<p>comes cumbersome for both humans and models to make a judgment based on them (Stammbach and Ash, 2020). Consequently, we prompt a large language model, specifically text-davinci-003, to summarize each retrieved document separately with respect to the claim. ${ }^{7}$ Such single-document summarization has been shown to be robust on news articles (Goyal et al., 2022; Zhang et al., 2023).</p>
<p>We investigate two types of prompts. For a zeroshot prompt, we instruct the model not to make any judgments about the stance of the given document. For a few-shot prompt, we select four documents and carefully write desired summaries. For documents that are not relevant to the claim, we write "the document is not relevant to checking the claim" as its desired output. We conduct human evaluation of the summary quality of different prompts in Section 6.1, where we find that few-shot prompting works better. See Appendix A. 3 for full prompts.</p>
<h3>3.5 Veracity Classification</h3>
<p>The final stage of our pipeline involves making a judgment based on the summaries generated in the previous stage. Unlike previous stages which use off-the-shelf tools, here we train a DeBERTalarge (He et al., 2020) model ${ }^{8}$ to perform a six-way veracity classification (true, mostly true, half true, barely true, false, and pants-on-fire).</p>
<p>Training We run our pipeline over the training, development, and test data of ClaIMDECOMP and train on pairs of the form (claim+summary, label). Since the dataset is small, we train the classifier five times with different random seeds and report the test set performance using the model that achieves the best performance on the development set.</p>
<h3>3.6 Final Pipeline</h3>
<p>Our complete pipeline's results when executed on an example are shown in Figure 5. We note that the question decomposition phase yields an overcomplete set of questions, including redundant ones. However, the final retrieved and summarized documents are able to shed light on the claim from</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>several complementary perspectives. While the final veracity judgment does not exactly match the judgment from PolitiFact, reading the documents still gives an informed picture of the situation.</p>
<h2>4 Experimental Setup</h2>
<p>Our main automatic evaluation is on claim veracity prediction (Wang, 2017), evaluating our entire pipeline end-to-end. We will describe the human evaluation setup in Section 6.</p>
<p>Data We use the data from ClaimDECOMP (Chen et al., 2022a) which contains 1,200 complex claims from PolitiFact (train: 800, dev: 200, test: 200). Each claim is labeled with one of the six veracity labels, a justification paragraph written by expert fact-checkers, and subquestions annotated by prior work.</p>
<p>Hyperparameters For the second-stage retrieval, we set top- $K_{1}=10$ (highest-scored text spans), top- $K_{2}=4$ (highest-scored documents), $k_{1}=30$ (chunk size), and $k_{2}=150$ (expansion parameter). See appendix A. 4 for all hyperparameters.</p>
<p>Evaluation Metric We report accuracy (Acc), mean absolute error (MAE, on our 6-point scale), and Macro-F1. We also introduce soft accuracy (soft Acc), which is calculated by counting off-byone errors on the six-point veracity scale (e.g., half true instead of mostly true) as correct, as veracity judgments are subjective.</p>
<p>Comparison Systems For our Claim-only system, we concatenate the metadata, including the speaker and the venue of the claim, with the claim itself, and feed the resulting text into the classifier (Wang, 2017). This approach serves as a lower bound for the veracity classification.</p>
<p>We extend the Claim-only baseline to Claim+Justification by appending the humanwritten justification paragraph, excluding the sentence containing the label, to the claim. This is an oracle setting to establish an upper bound for veracity classification.</p>
<h2>5 Automatic Evaluation: Claim Veracity</h2>
<h3>5.1 Constrained vs. Unconstrained Search</h3>
<p>We first situate our work with respect to baselines and past systems by varying the retrieval condition. We experiment with a temporal constraint, where pages must originate before the date of the claim, and a site constraint, where sites must be</p>
<table>
<thead>
<tr>
<th>Retrieval Constraint</th>
<th></th>
<th>Dev (N=200)</th>
<th></th>
<th></th>
<th></th>
<th>Test (N=200)</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Temporal</td>
<td>Site</td>
<td>Acc</td>
<td>Soft Acc</td>
<td>Macro-F1</td>
<td>MAE</td>
<td>Acc</td>
<td>Soft Acc</td>
<td>Macro-F1</td>
<td>MAE</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>50.5</td>
<td>88.5</td>
<td>47.5</td>
<td>0.62</td>
<td>$49.0^{+}$</td>
<td>$86.0^{+}$</td>
<td>$48.5^{+}$</td>
<td>$0.68^{+}$</td>
</tr>
<tr>
<td>-</td>
<td>Non-FC</td>
<td>37.5</td>
<td>76.5</td>
<td>38.6</td>
<td>0.94</td>
<td>$33.5^{+}$</td>
<td>$75.0^{+}$</td>
<td>$33.9^{+}$</td>
<td>$0.95^{+}$</td>
</tr>
<tr>
<td>Before</td>
<td>-</td>
<td>42.5</td>
<td>75.0</td>
<td>41.7</td>
<td>0.87</td>
<td>$33.5^{+}$</td>
<td>72.0</td>
<td>$38.0^{+}$</td>
<td>$0.98^{+}$</td>
</tr>
<tr>
<td>Before</td>
<td>Non-FC</td>
<td>40.5</td>
<td>76.5</td>
<td>41.4</td>
<td>0.87</td>
<td>$33.0^{+}$</td>
<td>$74.5^{+}$</td>
<td>$34.5^{+}$</td>
<td>$0.99^{+}$</td>
</tr>
<tr>
<td>Claim only</td>
<td></td>
<td>37.0</td>
<td>71.0</td>
<td>34.6</td>
<td>0.98</td>
<td>25.5</td>
<td>68.0</td>
<td>27.5</td>
<td>1.12</td>
</tr>
<tr>
<td>Claim + Justification (oracle)</td>
<td></td>
<td>52.5</td>
<td>88.5</td>
<td>54.5</td>
<td>0.64</td>
<td>57.5</td>
<td>93.0</td>
<td>57.8</td>
<td>0.50</td>
</tr>
</tbody>
</table>
<p>Table 2: Veracity classification performance with different retrieval constraints. The top block is our full system (\$) setting in Table 3) with constraints over what is retrieved. Red indicates using oracle information. " + " denotes that the results are statistically significant improvements ( $p&lt;0.05$ ) compared to the results of Claim only on the test set.
non-fact-checking (non-FC) sites. Even in the unconstrained setting, we exclude pages from PolitiFact (our dataset's source) to prevent label leakage.</p>
<p>The unconstrained setting corresponds to that used in MultiFC (Augenstein et al., 2019). MultiFC includes numerous documents that are filtered out by our constrained settings. For each claim, they extract the top 10 pages from the Google search API. We find that 12,721 out of 15,379 claims ( $82.7 \%$ ) contain at least one page from our excluded website list and $24.4 \%$ of the retrieved web pages are from fact-checking websites.</p>
<p>Table 2 reports the performance of our system with various retrieval constraints. Comparing the performance of claim-only and other models that use retrieval, we see a statistically significant ${ }^{9}$ improvement over all four of our metrics in nearly all settings, showing that retrieving and summarizing evidence is helpful to predict the veracity label, even with constraints.</p>
<p>Second, we see adding either temporal or site constraints dramatically reduces the performance. This implies that retrieval over the web works largely because it retrieves fact-checks that were published after the claim was released, with synthesized evidence. We believe that future work on retrieval should use a constrained setting.</p>
<h3>5.2 Stage Ablations</h3>
<p>We evaluate design choices in each stage of the pipeline to understand how each individual component contributes to the final performance. The results are shown in Table 3.</p>
<p>First-stage Retrieval: subquestions vs. original claim Using the original claim instead of the generated subquestions as an input to web search (\$) vs. (1) results in a notable decrease in performance.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>The subquestion set encompasses multiple aspects of the claim, enabling the search engine to locate relevant information more easily across separate search queries. Comparing (8) and (2), we see using the gold subquestions actually yields worse performance than our predicted subquestions. This could be because we predict 10 subquestions, potentially garnering more relevant data than the 3 (on average) gold subquestions (Chen et al., 2022a).</p>
<p>Second-stage Retrieval Rather than retrieving with subquestions (subQs), we instead perform our search with the raw Claim (3), Gold subQs from ClaimDecomp (4), or Justification (5), which uses oracle information. Different queries yield only slight differences in performance and none of them is statistically significant, even when (5) uses the human-written justification. We believe this is because we expand the retrieved text span by a context window ( $\pm 150$ words). As a result, this retrieval step does not need to be very precise to capture the relevant information.</p>
<p>Claim-focused Summarization We compare zero-shot (8) and few-shot (6) prompts for generating the summary; no summary (7) directly feeds the text spans from second-stage retrieval to the veracity classifier. System (7) shows the worst performance across all metrics, suggesting that summarization matters. This may result from two primary factors: (1) The document length exceeds the context window capacity of DeBERTa, causing crucial information to be truncated. (2) our veracity classifier cannot easily discern the most relevant information given a large amount of context. Differences in the prompt (8) and (6) do not impact veracity classification results much but have differences under human inspection, which we discuss in the next section.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Evidence Generation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Performance</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">FSR</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">SSR</td>
<td style="text-align: center;">Summary</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Soft Acc</td>
<td style="text-align: center;">Macro-F1 MAE</td>
</tr>
<tr>
<td style="text-align: center;">Claim only</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$25.5^{+}$</td>
<td style="text-align: center;">$68.0^{+}$</td>
<td style="text-align: center;">$27.5^{+}$</td>
</tr>
<tr>
<td style="text-align: center;">Claim + Justification</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$57.5^{+}$</td>
<td style="text-align: center;">$93.0^{+}$</td>
<td style="text-align: center;">$57.8^{+}$</td>
</tr>
<tr>
<td style="text-align: center;">Our Default System</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(1) subQs</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">subQs</td>
<td style="text-align: center;">zero-shot-003</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">34.5</td>
</tr>
<tr>
<td style="text-align: center;">Ablation on first-stage retrieval</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(1) Claim</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$24.5^{+}$</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">$18.0^{+}$</td>
</tr>
<tr>
<td style="text-align: center;">(2) Gold subQs</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">$28.1^{+}$</td>
</tr>
<tr>
<td style="text-align: center;">Ablation on second-stage retrieval</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(3)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Claim</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">35.6</td>
</tr>
<tr>
<td style="text-align: center;">(4)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Gold subQs</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">35.4</td>
</tr>
<tr>
<td style="text-align: center;">(5)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Justification</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">37.2</td>
</tr>
<tr>
<td style="text-align: center;">Ablation on summarization</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(6)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">few-shot-003</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">36.2</td>
</tr>
<tr>
<td style="text-align: center;">(7)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">no summary (raw doc)</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">$66.0^{+}$</td>
<td style="text-align: center;">$26.3^{+}$</td>
</tr>
</tbody>
</table>
<p>Table 3: End-to-end fact-checking performance on the test set of ClaimDECOMP. We ablate various stages of the model (FSR: first-stage retrieval; SSR: second-stage retrieval). Red indicates using oracle information. " + " denotes the result changes are statistically significant $(p&lt;0.05)$ with respect to our default system.</p>
<h3>5.3 Stability of First-stage Retrieval</h3>
<p>As commercial search engines evolve over time, we conduct experiments to explore the reproducibility of our first-stage retrieval step. We use the default system setting in Table 3 and conducted three rounds of retrieval at $T=0, T=1$ week, and $T=2$ months. We evaluate the Jaccard similarity of the sets of URLs retrieved from our queries to understand how much changes in the Bing API and the broader web change our results. We also evaluate the veracity of our system. Note that this Jaccard similarity is between the members of the URL sets (i.e., the URLs themselves), not capturing any lexical or domain similarity of the URLs.</p>
<p>Results are shown in Table 4. A noticeable trend is a decline in the Jaccard score between varying retrieval rounds over time. However, this decrease does not significantly impact the models' efficacy in the veracity assessment.</p>
<p>We caution that as the time gap increases, the set of documents retrieved from the Bing Search API could become considerably different, posing a challenge to consistently benchmark retrieval performance using commercial search engines. Therefore, we advocate for future research to focus on developing a comprehensive yet challenging document set that could be publicly released as a benchmark to spur research.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Overlap</th>
<th style="text-align: center;">Acc</th>
<th style="text-align: center;">Soft-Acc</th>
<th style="text-align: center;">Ma-F1</th>
<th style="text-align: center;">MAE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ours</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">0.99</td>
</tr>
<tr>
<td style="text-align: left;">1 week</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">36.8</td>
<td style="text-align: center;">0.98</td>
</tr>
<tr>
<td style="text-align: left;">2 months</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">1.03</td>
</tr>
</tbody>
</table>
<p>Table 4: Model performance with respect to different rounds of retrieval at intervals of one week and two months. The overlap between "Ours" and subsequent document sets, measured with Jaccard score, decreases as the time gap increases. However, none of the changes in our downstream metrics is statistically significant.</p>
<h2>6 Human Evaluation of Summaries</h2>
<p>Summarizing documents from web search with large language models improves the performance of our fact-checking pipeline. However, these models can generate untruthful content (Bommasani et al., 2021; Chowdhery et al., 2022; Ouyang et al., 2022). Furthermore, as pointed out by Lim (2018), the accuracy of veracity classification alone does not entirely reflect the system's overall effectiveness, as certain labels such as "false" and "barelytrue" may be ambiguous. We believe the true measure of our system's utility lies in the full package of summarized evidence it returns rather than just the accuracy of the veracity label. Therefore, we carry out two human studies, on comprehensiveness and faithfulness, to better understand intermediate outputs of the system.</p>
<p>Setting We randomly pick 50 claims which contain 200 document-summary pairs from the devel-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Summ-type</th>
<th style="text-align: center;">F</th>
<th style="text-align: center;">Minor</th>
<th style="text-align: center;">Major</th>
<th style="text-align: center;">NF</th>
<th style="text-align: center;">Avg score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">zero-shot-001</td>
<td style="text-align: center;">$65.8 \%$</td>
<td style="text-align: center;">$9.2 \%$</td>
<td style="text-align: center;">$20.0 \%$</td>
<td style="text-align: center;">$5.0 \%$</td>
<td style="text-align: center;">3.45</td>
</tr>
<tr>
<td style="text-align: left;">zero-shot-003</td>
<td style="text-align: center;">$66.0 \%$</td>
<td style="text-align: center;">$18.0 \%$</td>
<td style="text-align: center;">$16.0 \%$</td>
<td style="text-align: center;">$0.0 \%$</td>
<td style="text-align: center;">3.50</td>
</tr>
<tr>
<td style="text-align: left;">few-shot-003</td>
<td style="text-align: center;">$82.5 \%$</td>
<td style="text-align: center;">$6.5 \%$</td>
<td style="text-align: center;">$8.5 \%$</td>
<td style="text-align: center;">$2.5 \%$</td>
<td style="text-align: center;">3.69</td>
</tr>
</tbody>
</table>
<p>Table 5: Faithfulness Human Evaluation $(N=200)$. " $F$ " denotes that the summary is factual and "NF" denotes that the summary is completely wrong. Few-shot prompting helps the model make fewer factual errors.
opment set of ClaIMDECOMP and run two human evaluation studies on this set. For each task, we recruited annotators from Amazon Mechanical Turk with a qualification test. In total, we recruited 17 worker for the faithfulness study and 15 workers for the comprehensiveness study. The details about crowdsourcing can be found in Appendix C.</p>
<p>Comparison Systems We compare the summaries generated from two prompts, zero-shot-003 and few-shot-003, on GPT-3.5 (davinci-003). For the faithfulness study, we also compare the summaries generated through with zero-shot prompt on an earlier GPT model (davinci-001) (zero-shot-001) to see how the faithfulness varies for different models.</p>
<h3>6.1 Faithfulness Evaluation</h3>
<p>Goal We assess the frequency and degree to which the language model generates untruthful content during query-focused summarization. For each document and summary pair, annotators choose one of four labels below (see appendix C. 1 for examples):</p>
<ul>
<li>Faithful: the summary accurately represents the meaning and details of the original document.</li>
<li>Minor Factual Error: some details are not aligned with the original document, but the overall message remains intact.</li>
<li>Major Factual Error: there are factual errors that result in the summary misrepresenting the original document.</li>
<li>Completely Wrong: the language model hallucinates content that completely alters the meaning of the original document.
In addition to selecting a label, we ask annotators to provide a natural language justification for their choices. The annotations agree with a Fleiss Kappa score of 0.30 . While this number is somewhat low, when we evaluated their justifications and we find many of the disagreements are because of subjectivity on the extent of factual error. We compute a consensus annotation via majority vote. We assign
numerical scores to each label, where "Faithful", "Minor", "Major", and "Completely Wrong" correspond to $4,3,2$, and 1 respectively and report average values. If all annotators disagree, we compute the average score and return the label that is nearest to the average score as a consensus.</li>
</ul>
<p>Results The results are shown in Table 5. We see that few-shot prompting substantially decreases the chance of hallucinations in the summaries. When combining "Factual" and "Minor", we see $89 \%$ of the summaries are good enough to be used as evidence for the classifier. Additionally, by checking the unfaithful summaries, we find that they do not consist of useful hallucination like making a veracity judgment based on the parametric knowledge. Comparing the performance of zero-shot-001 and zero-shot-003, we find that the weaker model makes more major factual errors. Together, they indicate that with stronger models and better prompts, we may expect these summarization models to improve further.</p>
<h3>6.2 Comprehensiveness Evaluation</h3>
<p>Goal We aim to measure the extent to which the claim-focused summaries are able to address the claim. This is subjective and difficult task to evaluate. Here, we leverage the human-annotated yes/no subquestions from ClaIMDECOMP as a proxy for evaluating the comprehensiveness of our summaries: if provided summary can help humans to answer more of these yes/no questions, we deem the summary to be more comprehensive.</p>
<p>In this task, annotators are given a summary / subquestion pair and label subquestion as "answerable", "partially answerable", ${ }^{10}$ or "unanswerable", and additionally provide yes/no answer if the question is labeled as "answerable". Annotators were also asked to provide natural language justification for their answers. We collect this annotation on 161 questions associated with 50 claims. The annotations agree with a Fleiss Kappa score of 0.32 .</p>
<p>Results The results are presented in Table 6. We see that zero-shot summaries yield more answerable questions than few-shot summaries. However, faithfulness evaluation hints that this is caused by hallucinations in zero-shot summaries; the system</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Summ-type</th>
<th style="text-align: center;">Ans</th>
<th style="text-align: center;">Partially Ans</th>
<th style="text-align: center;">UnAns</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">zero-shot-003</td>
<td style="text-align: center;">$47.8 \%$</td>
<td style="text-align: center;">$22.4 \%$</td>
<td style="text-align: center;">$29.8 \%$</td>
</tr>
<tr>
<td style="text-align: left;">few-shot-003</td>
<td style="text-align: center;">$42.9 \%$</td>
<td style="text-align: center;">$21.1 \%$</td>
<td style="text-align: center;">$36.0 \%$</td>
</tr>
</tbody>
</table>
<p>Table 6: Human evaluation results on 161 subquestions from the same 50 claims we picked for the human study on faithfulness. "Ans", "Partially Ans", and "UnAns" denote the number of questions that are answerable, partially answerable, and unanswerable.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Faithful</th>
<th style="text-align: center;">Minor</th>
<th style="text-align: center;">Unfaithful</th>
<th style="text-align: right;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ans</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: right;">6</td>
</tr>
<tr>
<td style="text-align: left;">Partially Ans</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: right;">8</td>
</tr>
<tr>
<td style="text-align: left;">Partially UnAns</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">11</td>
<td style="text-align: right;">30</td>
</tr>
<tr>
<td style="text-align: left;">UnAns</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: right;">6</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">12</td>
<td style="text-align: right;">50</td>
</tr>
</tbody>
</table>
<p>Table 7: Claim-level statistics of few-shot-003 taking both faithfulness and comprehensiveness into consideration. "Unfaithful" label aggregates "Major Error" and "Completely Wrong" labels. The claim-level labels are derived from the sub-parts as defined in section 6.3.
imputes information that seems to help, but which is not supported by the document.</p>
<p>Nevertheless, the few-shot summaries allow us to partially address over $60 \%$ of the gold annotated subquestions derived from the PolitiFact justification. We find this result encouraging: even though the system does not have access to these (often subtle) factors, it can retrieve information to enable a human annotator to make a judgment about them.</p>
<h3>6.3 Combined Evaluation</h3>
<p>While in previous section we evaluated faithfulness and comprehensiveness separately, here we conduct a claim-level evaluation: how many claims can be comprehensively addressed with a set of faithful summaries? We label a claim as answerable if all of its subquestions are answerable. If all subquestions are unanswerable the claim is unanswerable. Otherwise, we label claim as partially unanswerable. For claim-level faithfulness, we apply the same principles: a claim is faithful is all summaries are faithful, otherwise it is either unfaithful or contains minor factual errors. Table 7 shows the results by combining the two factors. We see that addressing every aspect of complex claims is still challenging: 36 out of 50 claims contain at least one unanswerable question. For claims that can be fully addressed (all questions are either answerable or partially answerable), only 1 out of 14 contains a major factual error in the summary.</p>
<h2>7 Related Work</h2>
<p>Retrieval augmented models Prior work has shown that a variety of NLP tasks could benefit from incorporating a retrieval component. Such tasks mainly include question answering (Chen et al., 2017; Kwiatkowski et al., 2019; Karpukhin et al., 2020; Khattab et al., 2021; Nakano et al., 2021), text generation (Lewis et al., 2020; Shi et al., 2023; Ram et al., 2023), language modeling (Guu et al., 2020; Khandelwal et al., 2020; Zhong et al., 2022), and dialog (Moghe et al., 2018; Fan et al., 2021; Thoppilan et al., 2022).</p>
<p>Most of these work assume having access to a fixed corpus, however, for the task of real-world fact-checking, no such corpus exists. In this work, we follow WebGPT (Nakano et al., 2021) and use Bing Search API to retrieve evidence from the wild web. Recent LLM agents such as Bing Chat and Google Bard follow this paradigm, so we believe these directions will be relevant for future work.</p>
<p>Question decomposition has been shown to be effective in evidence retrieval and question understanding for complex question answering (Talmor and Berant, 2018; Min et al., 2019; Qi et al., 2019; Perez et al., 2020; Wolfson et al., 2020; Geva et al., 2021). Question generation has also been shown to play a useful role in retrieval pipelines in opendomain QA (Sachan et al., 2022). In more recent research, it was demonstrated by Chen et al. (2022a) that such decompositions can also aid in retrieving evidence to assess complex claims and make veracity judgment. This observation is consistent with concurrent studies on fact-checking text generation outputs (Gao et al., 2022; Chen et al., 2022b; Liu et al., 2022) and Wikipedia (Kamoi et al., 2023).</p>
<h2>8 Conclusion</h2>
<p>We introduce a pipeline for realistic, automated fact-checking of complex political claims by retrieving raw evidence from web documents, improving final fact checking accuracy by integrating retrieved evidence. Our pipeline show promising results on the ClaimDECOMP dataset. Yet, web search often cannot surface all the pieces of information necessary to verify a given claim. This work emphasizes the challenges of evidence retrieval in real-world scenarios and underscores the need for a human-in-the-loop fact-checking system.</p>
<h2>Limitations and Future Directions</h2>
<p>Performance is bottlenecked by the first-stage retrieval. The results in the last section show that $36.0 \%$ of questions are unanswerable using our most faithful claim-focused summaries. By investigating the unanswerable cases, we see that the following cases lead to retrieval failure: (1) no relevant information is available on the web except the fact-checking websites. These claims can be onerous to check, such as requiring talking to or emailing specific people to check facts. Those cases are beyond the scope of this work and we think a system doing triage for the claims, would be promising for future work. (2) No relevant subquestions are generated or the subquestions are not well decontextualized (Choi et al., 2021). In such cases, a stronger question generation model or decontextualization model can help further.</p>
<p>The need of human-in-the-loop fact-checking. To address the failures in the first-stage retrieval and the potential errors in the summarization stage, we envision a human-in-the-loop fact-checking system. This system begins with the automated pipeline presented in this paper, which provides fact-checkers with summarized documents and judgments. If the fact-checkers deem these documents unsatisfactory, the system reveals the subquestions used for evidence retrieval, allowing factcheckers to rerun the search. The system then retrieves additional documents and generates updated summaries. This iterative process continues until the fact-checkers are satisfied with the retrieved evidence. Moreover, the system could further learn from the fact-check feedback to improve itself: for example, the system could learn what questions are important to retrieve good evidence and what questions are not according to the fact-checker. In general, we believe such systems will be necessary, but developing them is outside of the scope of this work.</p>
<p>Scope of facts checked. Our work only addresses English-language political claims. Misinformation in other languages is a crucial problem that we believe future work should address. Moreover, even within English, there is a strong need for factchecking systems that can address other kinds of claims that have a different distribution; for example, claims from social media, which are often embedded in images or memes. Nevertheless, we believe the decomposition and retrieval approach
here can play a role in such systems as well.</p>
<h2>Acknowledgments</h2>
<p>This work was partially supported by NSF CAREER Award IIS-2145280, by Good Systems, ${ }^{11}$ a UT Austin Grand Challenge to develop responsible AI technologies, and by grants from Salesforce Inc. and Open Philanthropy. We thank the UT Austin NLP community for feedback on the earlier drafts of the paper.</p>
<h2>References</h2>
<p>Tariq Alhindi, Savvas Petridis, and Smaranda Muresan. 2018. Where is your evidence: Improving factchecking by justification modeling. In Proceedings of the First Workshop on Fact Extraction and VERification (FEVER), pages 85-90, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, and Isabelle Augenstein. 2020. Generating fact checking explanations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7352-7364, Online. Association for Computational Linguistics.</p>
<p>Isabelle Augenstein, Christina Lioma, Dongsheng Wang, Lucas Chaves Lima, Casper Hansen, Christian Hansen, and Jakob Grue Simonsen. 2019. MultiFC: A real-world multi-domain dataset for evidencebased fact checking of claims. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4685-4697, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco Tulio Ribeiro, and Daniel Weld. 2021. Does the Whole Exceed Its Parts? The Effect of AI Explanations on Complementary Team Performance. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, CHI '21, New York, NY, USA. Association for Computing Machinery.</p>
<p>Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.</p>
<p>Erik Brand, Kevin Roitero, Michael Soprano, Afshin Rahimi, and Gianluca Demartini. 2022. A neural model to jointly predict and explain truthfulness of statements. ACM Journal of Data and Information Quality, 15(1):1-19.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer opendomain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870-1879, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Jifan Chen, Aniruddh Sriram, Eunsol Choi, and Greg Durrett. 2022a. Generating literal and implied subquestions to fact-check complex claims. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3495-3516, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Sihao Chen, Senaka Buthpitiya, Alex Fabrikant, Dan Roth, and Tal Schuster. 2022b. PropSegmEnt: A Large-Scale Corpus for Proposition-Level Segmentation and Entailment Recognition. arXiv eprint arxiv:2212.10750.</p>
<p>Eunsol Choi, Jennimaria Palomaki, Matthew Lamm, Tom Kwiatkowski, Dipanjan Das, and Michael Collins. 2021. Decontextualization: Making sentences stand-alone. Transactions of the Association for Computational Linguistics, 9:447-461.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Sebastian Dungs, Ahmet Aker, Norbert Fuhr, and Kalina Bontcheva. 2018. Can rumour stance alone predict veracity? In Proceedings of the 27th International Conference on Computational Linguistics, pages 3360-3370, Santa Fe, New Mexico, USA. Association for Computational Linguistics.</p>
<p>Angela Fan, Claire Gardent, Chloé Braud, and Antoine Bordes. 2021. Augmenting transformers with KNNbased composite memory for dialog. Transactions of the Association for Computational Linguistics, 9:8299.</p>
<p>Angela Fan, Aleksandra Piktus, Fabio Petroni, Guillaume Wenzek, Marzieh Saeidi, Andreas Vlachos, Antoine Bordes, and Sebastian Riedel. 2020. Generating fact checking briefs. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7147-7161, Online. Association for Computational Linguistics.</p>
<p>William Ferreira and Andreas Vlachos. 2016. Emergent: a novel data-set for stance classification. In Proceedings of the 2016 conference of the North American
chapter of the association for computational linguistics: Human language technologies. ACL.</p>
<p>Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, N. Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin Guu. 2022. Rarr: Researching and revising what language models say, using language models.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346361.</p>
<p>Max Glockner, Yufang Hou, and Iryna Gurevych. 2022. Missing counter-evidence renders NLP fact-checking unrealistic for misinformation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5916-5936, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Tanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022. News Summarization and Evaluation in the Era of GPT-3. arXiv eprint arxiv:2209.12356.</p>
<p>Lucas Graves. 2018. Understanding the Promise and Limits of Automated Fact-Checking. Technical report, Reuters Institute, University of Oxford.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In International conference on machine learning, pages 3929-3938. PMLR.</p>
<p>Andreas Hanselowski, Christian Stab, Claudia Schulz, Zile Li, and Iryna Gurevych. 2019. A richly annotated corpus for different tasks in automated factchecking. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 493-503, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. DeBERTa: Decoding-enhanced BERT with Disentangled Attention. In International Conference on Learning Representations.</p>
<p>Ryo Kamoi, Tanya Goyal, Juan Diego Rodriguez, and Greg Durrett. 2023. WiCE: Real-World Entailment for Claims in Wikipedia. arXiv eprint arxiv:2303.01432.</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769-6781, Online. Association for Computational Linguistics.</p>
<p>Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language</p>
<p>models. In International Conference on Learning Representations.</p>
<p>Omar Khattab, Christopher Potts, and Matei Zaharia. 2021. Relevance-guided supervision for OpenQA with ColBERT. Transactions of the Association for Computational Linguistics, 9:929-944.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466.</p>
<p>Mosh Levy, Alon Jacoby, and Yoav Goldberg. 2024. Same task, more tokens: the impact of input length on the reasoning performance of large language models. arXiv preprint arXiv:2402.14848.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459-9474.</p>
<p>Chloe Lim. 2018. Checking how fact-checkers check. Research \&amp; Politics, 5(3):2053168018786848.</p>
<p>Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157-173.</p>
<p>Yixin Liu, Alexander R. Fabbri, Pengfei Liu, Yilun Zhao, Linyong Nan, Ruilin Han, Simeng Han, Shafiq Joty, Chien-Sheng Wu, Caiming Xiong, and Dragomir Radev. 2022. Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation. arXiv eprint arxiv:2212.07981.</p>
<p>Sewon Min, Victor Zhong, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2019. Multi-hop reading comprehension through question decomposition and rescoring. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6097-6109, Florence, Italy. Association for Computational Linguistics.</p>
<p>Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. 2018. Towards exploiting background knowledge for building conversation systems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2322-2332, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders,
et al. 2021. WebGPT: Browser-assisted questionanswering with human feedback. arXiv preprint arXiv:2112.09332.</p>
<p>Preslav Nakov, David Corney, Maram Hasanain, Firoj Alam, Tamer Elsayed, Alberto Barr'on-Cedeno, Paolo Papotti, Shaden Shaar, and Giovanni Da San Martino. 2021. Automated fact-checking for assisting human fact-checkers. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI).</p>
<p>Nedjma Ousidhoum, Zhangdie Yuan, and Andreas Vlachos. 2022. Varifocal question generation for factchecking. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2532-2544, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.</p>
<p>Ethan Perez, Patrick Lewis, Wen-tau Yih, Kyunghyun Cho, and Douwe Kiela. 2020. Unsupervised question decomposition for question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8864-8880, Online. Association for Computational Linguistics.</p>
<p>Verónica Pérez-Rosas, Bennett Kleinberg, Alexandra Lefevre, and Rada Mihalcea. 2018. Automatic detection of fake news. In Proceedings of the 27th International Conference on Computational Linguistics, pages 3391-3401, Santa Fe, New Mexico, USA. Association for Computational Linguistics.</p>
<p>Kashyap Popat, Subhabrata Mukherjee, Jannik Strötgen, and Gerhard Weikum. 2017. Where the truth lies: Explaining the credibility of emerging claims on the web and social media. In Proceedings of the 26th International Conference on World Wide Web Companion, pages 1003-1012. International World Wide Web Conferences Steering Committee.</p>
<p>Kashyap Popat, Subhabrata Mukherjee, Andrew Yates, and Gerhard Weikum. 2018. DeClarE: Debunking fake news and false claims using evidence-aware deep learning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 22-32, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Peng Qi, Xiaowen Lin, Leo Mehr, Zijian Wang, and Christopher D. Manning. 2019. Answering complex open-domain questions through iterative query generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2590-2602, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented language models. arXiv preprint arXiv:2302.00083.</p>
<p>Hannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana Volkova, and Yejin Choi. 2017. Truth of varying shades: Analyzing language in fake news and political fact-checking. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2931-2937, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Devendra Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau Yih, Joelle Pineau, and Luke Zettlemoyer. 2022. Improving passage retrieval with zero-shot question generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3781-3797, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Michael Schlichtkrull, Zhijiang Guo, and Andreas Vlachos. 2023. Averitec: A dataset for real-world claim verification with evidence from the web. Advances in Neural Information Processing Systems, Datasets and Benchmarks Track, 36.</p>
<p>Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. REPLUG: RetrievalAugmented Black-Box Language Models. arXiv preprint arXiv:2301.12652.</p>
<p>Prakhar Singh, Anubrata Das, Junyi Jessy Li, and Matthew Lease. 2021. The case for claim difficulty assessment in automatic fact checking. arXiv preprint arXiv:2109.09689.</p>
<p>Dominik Stammbach and Elliott Ash. 2020. e-fever: Explanations and summaries for automated fact checking. Proceedings of the 2020 Truth and Trust Online (TTO 2020), pages 32-43.</p>
<p>Alon Talmor and Jonathan Berant. 2018. The web as a knowledge-base for answering complex questions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 641-651, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. LaMDA: Language Models for Dialog Applications. arXiv preprint arXiv:2201.08239.</p>
<p>Andreas Vlachos and Sebastian Riedel. 2014. Fact checking: Task definition and dataset construction. In Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 18-22, Baltimore, MD, USA. Association for Computational Linguistics.</p>
<p>Svitlana Volkova, Kyle Shaffer, Jin Yea Jang, and Nathan Hodas. 2017. Separating facts from fiction: Linguistic models to classify suspicious and trusted news posts on Twitter. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 647-653, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>William Yang Wang. 2017. "liar, liar pants on fire": A new benchmark dataset for fake news detection. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 422-426, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav Goldberg, Daniel Deutch, and Jonathan Berant. 2020. Break it down: A question understanding benchmark. Transactions of the Association for Computational Linguistics, 8:183-198.</p>
<p>Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B. Hashimoto. 2023. Benchmarking Large Language Models for News Summarization. arXiv eprint arxiv:2301.13848.</p>
<p>Zexuan Zhong, Tao Lei, and Danqi Chen. 2022. Training language models with memory augmentation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5657-5673, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<h2>Appendix</h2>
<h2>A Experimental Details</h2>
<h2>A. 1 List of Websites being Filtered</h2>
<ul>
<li>www.politifact.com</li>
<li>www.snopes.com</li>
<li>www.factcheck.org</li>
<li>www.washingtonpost.com/news/ fact-checker/</li>
<li>www.apnews.com/hub/ap-fact-check</li>
<li>www.fullfact.org</li>
<li>www.reuters.com/fact-check</li>
</ul>
<p>We also filter the URLs that contain "fact-check" or "factcheck"; we also filter any PDF files and videos.</p>
<p>Claim: Viral image stated on June 8, 2020 in post on Facebook: Cops in Norway: require 3 years of training, 4 people killed since 2002. Cops in Finland: require 2 years of training, 7 people killed since 2000. Cops in Iceland: require 2 years of training, 1 person killed since ever. Cops in the U.S.: require 21 weeks of training, 8,000+ people killed since 2001.</p>
<p>Suppose you are a fact-checker, generate several yes or no questions to help me answer if this claim is true or false.</p>
<p>Questions:
Does Norway require 3 years of training for cops?
Have Norwegian cops killed 4 people since the early 2000's?
Does Finland require only 2 years of training for police?
Have Finnish police killed 7 people since 2000?
Does Iceland only require 2 years of training for cops?
Have Iceland cops only killed 1 person ever?
Does the U.S. require only 21 weeks of training for cops?
Have U.S. cops killed more than 8,000 people since 2001?
Do experts associate only training time with police-related shooting fatalities?
Claim: Barry DuVal stated on September 25, 2015 in an interview: We're the only major oil-producing nation in the world with a self-imposed ban on exporting our crude oil to other nations.</p>
<p>Suppose you are a fact-checker, generate several yes or no questions to help me answer if this claim is true or false.</p>
<p>Questions:
Is the U.S. the only major oil-producing nation to ban exports of crude oil? Is the self-imposed ban on crude oil export of U.S a complete ban?</p>
<p>Claim: William Barr stated on September 2, 2020 in a CNN interview: We indicted someone in Texas, 1,700 ballots collected from people who could vote, he made them out and voted for the person he wanted to.</p>
<p>Suppose you are a fact-checker, generate several yes or no questions to help me answer if this claim is true or false.</p>
<p>Questions:
Were 1700 mail-in ballots investigated for fraud in Texas during the 2020 election? Did the Justice Department indict someone in Texas for voter fraud? Did widespread mail-in order fraud happen in Texas during the 2020 election? Did voter disenfranchisement happen in Texas during the 2020 election?</p>
<p>Claim: INPUT CLAIM
Suppose you are a fact-checker, generate several yes or no questions to help me answer if this claim is true or false.</p>
<p>Questions:
MODEL OUTPUT
Figure 6: Few-shot prompt we used to generate subquestions in this paper.</p>
<h2>A. 2 Question Generation Prompt and Deduplication</h2>
<p>The prompt we used to generate the questions is shown in Figure 6. Since the generated question set sometimes contains duplicates, we delete the duplicated questions according to the exact string match.</p>
<h2>A. 3 Question-focused Summarization Prompt</h2>
<p>The zero-shot and few-shot prompts we used to generate the claim-focused summaries are shown in Figure 7 and Figure 8 respectively.</p>
<h2>A. 4 Hyperparameters of Veracity Classifier</h2>
<ul>
<li>Model: DeBERTa-large</li>
<li>Batch size: 32</li>
</ul>
<p>Suppose you are assisting a fact-checker to fact-check the claim: INPUT CLAIM</p>
<p>Summarize the relevant information from the document in 1-2 sentences. Your response should provide a clear and concise summary of the relevant information contained in the document. Do not include a judgment about the claim and do not repeat any information from the claim that is not supported by the document.</p>
<p>Summarization:
MODEL OUTPUT
Figure 7: Zero-shot prompt we used to generate the claim-focused summaries in this paper.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">First-stage</th>
<th style="text-align: center;">Second-stage</th>
<th style="text-align: center;">Summ</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"># documents</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">4.0</td>
</tr>
<tr>
<td style="text-align: left;"># words</td>
<td style="text-align: center;">70,245</td>
<td style="text-align: center;">2,710</td>
<td style="text-align: center;">251</td>
</tr>
</tbody>
</table>
<p>Table 8: Average number of unique documents and average number of words in total from those documents after each stage of our pipeline.</p>
<ul>
<li>Max sequence length: 512</li>
<li>Epochs: 25</li>
<li>Initial learning rate: 3e-5</li>
<li>Optimizer: Adam with linear decay</li>
<li>Metric for selecting best dev model: MAE</li>
<li>Random seed of 5 runs: 290032, 33432, 7876, 366, 77</li>
<li>Training device: NVIDIA-A6000</li>
</ul>
<h2>B Information Compression through the Pipeline</h2>
<p>Our pipeline progressively refines the crucial data needed to validate a claim. Table 8 demonstrates the average count of unique documents and the total word count in these documents after each phase of our pipeline under both temporal and site constraints.</p>
<h2>C Human Study</h2>
<h2>C. 1 Examples of Unfaithful Summaries</h2>
<p>Figure 12 shows three examples containing unfaithful content. We see that the "Minor" error does not affect the interpretation of the original document while "Major" and "Completely Wrong" errors alter the view.</p>
<p>Document: Vote by mail: Which states allow absentee voting - Washington Post
Content: excuse to vote absentee and states that will allow fear of the coronavirus as an excuse. In response to the coronavirus, nearly half of all states expanded access to mail ballots for their primaries, either by allowing fear of the coronavirus as a reason or proactively sending an application or ballot to every registered voter. Fewer have taken action for the general election, as the move has become increasingly partisan and subject to litigation. President Trump has made numerous unfounded claims that mail-in voting will create widespread abuse and fraud. His suspicions are out of step with the views of election experts and many within his own party, who are building large-scale vote-by-mail programs. A recent analysis by The Washington Post found only 372 cases of potential fraud out of roughly 14.6 million ballots cast by mail in 2016 and 2018. (Examining the arguments against voting by mail: Does it really lead to fraud or benefit only Democrats ?) Only a quarter of voters used mailed ballots in 2018, and they mostly resided in a handful of states. Nearly everyone who voted in Oregon, the first state to issue all ballots by mail in 2000, did so by mail. But in most states, fewer than 10 percent of voters did. # # # # Most places expanding vote-by-mail in November had limited mail-in voting in 2018 Percentage of votes cast by mail in 2018 midterm elections Even in states that have not made absentee voting easier, the number of ballot requests is still expected to spike. To meet this challenge, local election officials will have to overcome numerous hurdles with little time and money to spare. They must acquire large volumes of specialized envelopes and paper. Additional staff, and in some cases machines, are necessary to open, sort and tabulate postal ballots and verify</p>
<p>Suppose you are assisting a fact-checker to fact-check the claim:
"Donald Trump stated on April 7, 2020 in a press briefing: With voting by mail, "you get thousands and thousands of people sitting in somebody's living room, signing ballots all over the place." Summarize the relevant information from the document in 1-2 sentences. Your response should provide a clear and concise summary of the relevant information contained in the document. Do not include a judgment about the claim and do not repeat any information from the claim that is not supported by the document:</p>
<p>Trump's suspicions are out of step with the views of election experts and many within his own party, who are building large-scale vote-by-mail programs. The Washington Post conducted an analysis that found only 372 cases of potential fraud out of approximately 14.6 million ballots cast by mail in the 2016 and 2018 elections.</p>
<p>Document: Governor Ron DeSantis' State of the State Address - CBS Miami
Content: hard and one thing is clear: we have a great First Lady. I'm convinced that this whole Governor thing is just a way for the people of Florida to have Casey as their First Lady. Our kids Madison and Mason have taken over the Governor's mansion but the baby-proofing has been successful - I can report that no artifacts of Florida history have yet to be destroyed. But stay tuned Mason will be walking very soon. Just last week, Casey traveled to NW Florida to survey the Hurricane Michael recovery efforts. We have both been to the region several times over the past few months. To the people of NW Florida: we pledged to stand with you as you work to rebuild your communities stronger than before - and we have followed through on that pledge. I've already traveled to Washington, DC and secured a historic commitment from the Trump administration to provide assistance to the communities that Michael battered. My administration has worked to cut through red tape to expedite relief efforts and, thanks to the leadership of your former colleague Jared Moskowitz, we are making great progress. Here with us today is Mayor Al Cathey and Administrator Tanja Castro from Mexico Beach, which was ground zero for Hurricane Michael 's wrath. They and many others affected by the storm have displayed remarkable resilience in the face of huge obstacles. They deserve our admiration - and our support. These communities will not be rebuilt in days, weeks or months, but they will be rebuilt. They will be rebuilt because we as Christians will answer the call as we always do. God bless the good people of NW Florida. Executive energy and leadership are necessary to meet fully the challenges that are before us - but they are not sufficient. In a</p>
<p>Suppose you are assisting a fact-checker to fact-check the claim:
"Ron DeSantis stated on March 5, 2019 in his State of the State address: "I've already traveled to Washington, D.C., and secured a historic commitment from the Trump administration to provide assistance to the communities that Michael battered."
Summarize the relevant information from the document in 1-2 sentences. Your response should provide a clear and concise summary of the relevant information contained in the document. Do not include a judgment about the claim and do not repeat any information from the claim that is not supported by the document:
Ron DeSantis did state in his address that he traveled to Washington, D.C. and secured a commitment from the Trump administration. However, no additional evidence is provided about this claim.
Document: Cuban government unveils controversial cybersecurity regulation
Content: others. Activists, opponents, unofficial organizations and Internet users fear that the new regulations will be applied at the discretion of the government and will serve to silence dissident voices on the networks and prosecute those who criticize the Cuban system and its leaders. In this sense, complaints and criticisms of the island "s authorities due to the new regulation have already started being published on the social networks. In addition, in this Tuesday's Gaceta Oficial other regulations were published that, according to the Ministry of Communications, "update" and "strengthen" Cuba 's legal body on these issues. Among these is Decree Law No. 35 on "Telecommunications, Information and Communication Technologies and the Use of the Radioelectric Spectrum, "which establishes that "the Cuban state is the owner of public telecommunications services and has the power to regulate, order, control and supervise the different telecommunications/ICT services and networks ; as well as granting the right to its exploitation, " Cubadebate pointed out. Finally, the package includes a group of regulations on Information and Communication Technologies, the Radioelectric Spectrum, the use of Radiocommunication Services by Satellites and Interconnection, Access and Essential Installations of Telecommunication Networks. OnCuba Staff</p>
<p>Suppose you are assisting a fact-checker to fact-check the claim:
"Marco Rubio stated on December 17, 2014: "The reason why Cubans don't have access to 21st century telecommunications - like smart phones, like access to the Internet - is because it is illegal in Cuba."
Summarize the relevant information from the document in 1-2 sentences. Your response should provide a clear and concise summary of the relevant information contained in the document. Do not include a judgment about the claim and do not repeat any information from the claim that is not supported by the document:
Cuba is passing new regulations on telecommunication, establishing that the Cuban state can regulate and supervise different telecommunication services and networks. This includes Decree-Law No. 35 on "Telecommunications, Information and Communication Technologies and the Use of the Radioelectric Spectrum."</p>
<p>Document: FBI Chief Claims 'Gaps' Remain In Syrian Refugee Screening Process
Content: # # # # Published: Fri., Oct 9th 2015 @ 1:13 pm EST FBI Director James Comey voiced his concern during a Senate Homeland Security Committee hearing about the lack of information available to screen Syrian refugees coming into the United States. "My concern there is there are certain gaps ... in the data available to us, " Comey said. "There is risk associated of bringing anybody in from the outside, but specifically from a conflict zone like that, " he added. This hearing comes after the Obama administration 's announcement to allow 85,000 refugees into the U.S. next year and 100,000 in 2017. Those are huge increases from the current level of 70,000. The Obama administration has said that at least 33,000 of the refugees next year will come from Syria. Many lawmakers are concerned that ISIS extremists or other terrorists could make it into the U.S. as a refugee due to the lack of intelligence and information available about the area. "The intelligence that we have of this particular conflict zone is not as rich as we would like it to be, " admitted Nicholas Rasmussen, the head of the National Counterterrorism Center during the hearing. Even Homeland Security Secretary Jels Johnson expressed concern saying, "We should do the right thing by accepting more, but we should be careful in doing it. "Currently the U.S. has taken in around 2,000 Syrian refugees and has contributed about $\$ 4$ billion in foreign aid to the Syrian refugee crisis, more than any other country. Read more at The Hill. refugees</p>
<p>Suppose you are assisting a fact-checker to fact-check the claim:
"Rob Portman stated on October 20, 2016 in an Ohio Senate debate in Cleveland: "The director of the FBI said that we cannot figure out who (Syrian refugees) are, what their intentions are, because we have no information on them, because we don't have any contact with the Syrian government or any people on the ground to be able to determine that."
Summarize the relevant information from the document in 1-2 sentences. Your response should provide a clear and concise summary of the relevant information contained in the document. Do not include a judgment about the claim and do not repeat any information from the claim that is not supported by the document:
FBI Director James Comey voiced concern during a Senate Homeland Security Committee hearing about the lack of information available to screen Syrian refugees coming into the United States. Nicholas Rasmussen says we do not have as much information about Syria as we would like and Homeland Security Secretary Jels Johnson says we need to be careful.</p>
<p>Figure 8: Few-shot prompt we used to generate the claim-focused summaries in this paper.</p>
<h2>C. 2 Recruiting Process</h2>
<p>Faithfulness study We set up a qualification test that consists of 5 examples. We selected workers from MTurk if they get more than $3 / 5$ examples correct according to our curated labels and if they write reasonable rationales. In total, there are 31 workers who took the qualification test and we selected 15 of them for the task. We pay $\$ 3$ for the qualification test and $\$ 2$ dollars for one HIT that contains 4 document-summary pairs in the actual
task. The detailed instructions and the annotation interface is shown in Figure 10.</p>
<p>Comprehensiveness study We set up a qualification test that consists of 10 examples. We selected workers from MTurk if they got more than 7/10 questions right according to our curated labels and if they write reasonable rationales. In total, there are 28 workers who took the qualification test and we selected 17 of them for the task. We pay $\$ 3$ for the qualification test and $\$ 0.3$ dollars for one</p>
<p>Use summary of documents to score how likely this claim is true at the scale of 0 to 100, 0 being pants-on-fire and 100 being true.</p>
<p>Explain your reasoning first and output your predicted score.
Claim to fact-check: INPUT CLAIM
Summary to use:
Document 0: INPUT SUMMARY 0
Document 1: INPUT SUMMARY 1
Document 2: INPUT SUMMARY 2
Document 3: INPUT SUMMARY 3
Format your output like this
Explanation: Your explanation
Score: Your prediction
Figure 9: Zero-shot prompt for Claim + summary
question in the actual task.
The detailed instructions and the annotation interface is shown in Figure 11.</p>
<h2>D Using LLMs as a Veracity Classifier</h2>
<p>We experiment with using ChatGPT (gpt-3.5-turbo) as the classifier in the final stage. Since ChatGPT is not trained on our training set, it does not have access to the label distribution of the dataset. To make a fair comparison with the DeBERTa model, instead of directly predicting a discrete label (one out of the six labels), we prompt the model to explain its reasoning process and predict a truthfulness score on a scale of 0 to 100,0 for the claim being false and 100 for true. We then rank the examples according to the predicted scores and map the scores to discrete labels to the label distribution of the training set. To be specific, we rank the examples in the training set by their labels, assigning the lowest rank to pants-on-fire and the highest to true. Each label, denoted as $l_{i}$, corresponds to a percentile $p_{i}$. We then map the predicted score falling between $p_{i}$ and $p_{i+1}$ to the label $l_{i}$. We use a zero-shot prompt ${ }^{12}$ to produce the score and the prompt is shown in Figure 9.</p>
<p>The results are shown in Table 9. Comparing the claim-only results from the two models, we see that ChatGPT achieves slightly better performance than DeBERTa. However, unlike the DeBERTa model, when adding the summary, we see a notable performance drop for ChatGPT. We argue that this</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>might be because ChatGPT relies heavily on prior knowledge and it is not able to use the provided summary effectively. We believe improving this is a promising direction for future work.</p>
<h4><strong>Instructions:</strong></h4>
<p>Thank you for participating in this task! This task aims to determine how trustable an AI system is at automatically gathering the most relevant information from a document to verify a political claim.</p>
<ul>
<li>You are given <strong>Q</strong> a <strong>political claim</strong>, <strong>Q</strong> a <strong>wisdom</strong> of a document that is potentially relevant to check the political claim, and <strong>Q</strong> a <strong>summary</strong> of the wisdom generated by an AI system.</li>
<li>The search is already whether the summary is <strong>faithful</strong> or past document. For the summary of an <strong>faithful</strong>, the summary should avoid shifting any new information back in an amount of the original document or misrepresenting the information presented to other documents.</li>
<li>Note that your job is not to evaluate whether the document/summary is relevant or not to the claim. The claim is not meant to be used to judge whether the summary is faithful or not. A just provokes you some context that may be helpful.</li>
<li><strong>This document is traceable</strong>, you can make sure that opens up to the context. Do not penalize the summary if it includes context you think would reasonably occur in the way of the document if not traceable.</li>
<li>Major factual errors should be errors that cause the summary to actually give a different impression than the original document. Minor factual errors are those where, even though some details may not align, they don't change the overall message of the document.</li>
<li>Translate all the summary to cite the claim. However, if the summary contains an assessment regarding whether the document is relevant to the claim or not, it's your best to evaluate whether the assessment made by the machine is accurate or not based on our criteria (correct, minor, major, etc.).</li>
</ul>
<h4><strong>Examples:</strong></h4>
<p>Highlights are added by us for illustration but not present in real examples you will see.</p>
<h4><strong>Example of faithful summary</strong></h4>
<p><strong>Claim</strong></p>
<p>Significant and: "You know what the biggest lie is, is that restaurants are spreaders of COVID. There's no science for that." In fact, plenty of evidence suggests restaurant dining has helped spread the coronavirus. Places that allow indoor dining and don't follow safety protocols are considered especially unsafe.</p>
<p><strong>Document</strong></p>
<p>Document title: What are the main modes of transmission for COVID-19? - Lost Systems</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Document title: What are the main modes of transmission for COVID-19? - Lost Systems</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Thank you for participating in this task! The goal of this task is to determine how good an AI system is at finding information to help check political claims. You are going to see whether some information the AI system produces contains the answers to questions that are important to fact-checkers.</p>
<h1>Task: Comprehensiveness</h1>
<ul>
<li>You are given a political claim, a set of AI system-generated sentences based on web searches, and a set of yes/no questions that are related to checking the claim.</li>
<li>In this task, you should determine whether the questions are answerable based on the AI-generated sentences. For each question, you should choose from the following three labels:</li>
</ul>
<h2>1. Answerable: The question is fully answered by the rationale.</h2>
<ol>
<li>Partially Answerable: Only part of the question could be addressed by the rationale or question is addressed but it's not clear whether there's evidence for it.</li>
<li>
<p>Unanswerable: The question cannot be answered by the rationale.</p>
</li>
<li>
<p>If you think the question is answerable from the rationale, you should also give your answer. If the answer is partially answerable, use your best guess.</p>
</li>
</ol>
<p>We provide two examples below for you to better understand the task.</p>
<h2>Example 1</h2>
<h2>Claim:</h2>
<p>Donald Trump stated on February 5, 2018 in a speech near Cincinnati. At the State of the Union address, Democrats, "even on positive news ... were like death and un-American. Un-American. Somebody said, 'treasomous.' I mean, yeah, I guess, why not? Can we call that treason? Why not?"</p>
<h2>AI-Generated Sentences:</h2>
<ol>
<li>In 1976, Donald Ford (R) became the only president to ever declare the state of the union to be not good. Since 1981, every State of the Union address from George W. Bush (R) and Barack Obama (D) has declared that the state of the union is strong, to some extent. The White House has already announced that President Donald Trump will declare on February 5, 2018 that the state of the union is "strong".</li>
<li>In a speech near Cincinnati on February 5, 2018, Donald Trump criticized Democrats for not stopping at the State of the Union address. He also discussed the midterm elections and supra popular with his base, such as the lack of players traveling during the national anthem at the Super Bowl. He suggested that Democrats were "very selfish" and asked if their lack of enthusiasm could be called "treasomous."</li>
<li>In a speech on February 5, 2018 near Cincinnati, President Trump accused Democrats of being "un-American and potentially treasomous for not applauding during his State of the Union address when he mentioned good news such as rising wages and two African-American unemployment. He accused the Democrats of being selfish and suggested the lack of applause was an indication of their lack of patriotism.</li>
<li>On February 5, 2018, President Trump made a speech near Cincinnati where he accused Democratic congressional members of being un-American and potentially treasomous for not applauding positive news during his State of the Union address. He also criticized Nancy Pelosi for describing lax cut bonuses of $\$ 1,000$ or more as "crumbs". Additionally, he noted that Republicans were "going totally crazy wild" during his speech.</li>
</ol>
<h2>Questions:</h2>
<ul>
<li>Q1: Does declining to applaud the president qualify as treason?</li>
<li>Q2: Can the crime of treason be charged outside of wartime?</li>
<li>Q3: Is declining to applaud the president protected under U.S. law?</li>
</ul>
<h2>Annotation:</h2>
<ul>
<li>Q1: Partially answerable. Answer: Yes. Explanation: Sentences 2 and 3 provide evidence that Trump may think declining to applaud is treasomous, but it is unclear how the question defines treason.</li>
<li>Q2: Unanswerable. Answer: N/A. Explanation: No document talks about wartime.</li>
<li>Q3: Unanswerable. Answer: N/A. Explanation: No document provides evidence about U.S. laws on treason.</li>
</ul>
<h2>Example 2</h2>
<h2>Claim:</h2>
<p>Andrew Yang stated on September 21, 2019 in Iowa Steak Fry speech: Says about universal basic income, "Thomas Paine was for it at our founding ... Martin Luther King, champion in the '60s, called it a guaranteed minimum income for all Americans ... It passed the U.S. House of Representatives twice in 1971."</p>
<h2>AI-Generated Sentences:</h2>
<ol>
<li>This document discusses the concept of universal basic income (UBI), which is a policy idea that proposes every citizen should receive a regular stipend from the government to cover their basic needs. UBI has been discussed since the founding of the United States, most recently in the civil-rights era, Martin Luther King Jr. and Thomas Paine were both proponents of the idea.</li>
<li>This document discusses five reasons why Martin Luther King Jr. supported a guaranteed income, which is now referred to as Universal Basic Income. These reasons include automation and the dissolution of jobs that he saw occurring in 1991, advocating for a guaranteed income in his last book, and the passing of the U.S. House of Representatives in 1971.</li>
<li>The 120th Congress passed a number of laws related to basic, criminal justice reform, the opioid crisis, and the Music Modernization Act. It also failed to pass funding for large parts of the federal government in the current fiscal year. However, it did pass the Tax Cuts and Jobs Act and the First Step Act.</li>
<li>In his September 21, 2018 Iowa Steak Fry speech, Andrew Yang stated that Thomas Paine and Martin Luther King Jr. have both advocated for a Universal Basic Income (UBI). In addition, Yang noted that the U.S. House of Representatives passed the measure twice in 1971. He also gave the example of the state of Alaska successfully implementing a basic income.</li>
</ol>
<h2>Questions:</h2>
<ul>
<li>Q1: Was Thomas Paine for universal basic income?</li>
<li>Q2: Was Martin Luther King Jr in support of a minimum basic income for all Americans?</li>
<li>Q3: Did the House pass twice a bill supporting minimum basic income in 1971?</li>
<li>Q4: Did the House pass twice a bill for minimum basic income in the 1970s?"</li>
</ul>
<h2>Annotation:</h2>
<ul>
<li>Q1: Answerable. Answer: Yes. Explanation: It is supported by paragraph 1.</li>
<li>Q2: Answerable. Answer: Yes. Explanation: It is supported by paragraphs 1 and 2.</li>
<li>Q3: Partially answerable. Answer: No. Explanation: Paragraph 2 discussed this passing of the bill in 1971, but no evidence showing it passed twice. Paragraph 4 says it passed twice but it's just a restatement of the claim. In such a case, you just use your best guess.</li>
<li>Q4: Partially answerable. Answer: Yes. Explanation: Similar to Q3, use your best guess.</li>
</ul>
<p>Claim
$\$ 2.50+1$
AI-Generated Sentences:</p>
<ol>
<li>$\$ 20 \mathrm{PT}$-rationale)</li>
</ol>
<h2>ATTENTION</h2>
<p>If there is no evidence that DIRECTLY addresses the question, you should consider it as Unanswerable, rather than select answerable with NO as the answer.
To prevent arbitrary selection, you should also write a brief sentence to explain your decision.</p>
<h2>Questions</h2>
<p>$\square$
$\square$
Explanation</p>
<p>Claim: Facebook posts stated on September 29, 2021: The White House "created a fake set for [President Joe] Biden to get his booster shot. The entire Biden presidency is one giant charade."
Document Title: President Joe Biden Gets COVID Booster Vaccine Shot - PEOPLE.com
Content: President Joe Biden received his COVID-19 booster shot ... Biden received his first dose of Pfizer-BioNTech novel coronavirus vaccine in December at Delaware 's ChristianaCare Hospital. His second shot was administered there three weeks later. Both shots were televised ...
Summary: On Monday, President Joe Biden received his COVID-19 booster shot on camera, administered by a nurse with the White House Medical Unit. Biden has previously received both of his initial doses of the Pfizer-BioNTech novel coronavirus vaccine in December at Delaware's ChristianaCare Hospital.
Minor Error: The summaries says both shots are received in December while the document says the first one was in December and the second one was three weeks later. The summary slightly changes the information in the original document.
Claim: Elizabeth Warren stated on November 20, 2019 in a debate: "Today in America - a new study came out - 20 years out, whites who borrowed money, $94 \%$ of them have paid off their student loan debt; 5\% of African Americans have paid it off."
Document Title: More Evidence Student Borrowers Prepay Their Loans - Forbes
Content: Most borrowers will still fully pay off their loans before they are required to do so ... Borrowers pay off almost all loans ( $94 \%$ ) with a final payment larger than the scheduled monthly payments they are required to make ...
Summary: A new report from the Consumer Financial Protection Bureau found that $94 \%$ of borrowers pay off their student loans before the scheduled payoff date, typically doing so in five years or less ...
Major Error: The document says borrowers pay off $94 \%$ of the loan not $94 \%$ of borrowers pay off their loans
Claim: Andrew Giuliani stated on May 18, 2021 in a news conference: "The one good thing about the antibodies if you've had it, is it actually is even better than the vaccine, and here's why. With the vaccine you can still transmit, with the antibodies you can't transmit."
Document Title: COVID-19: Long-term effects - Mayo Clinic
Content: It involves extreme fatigue that worsens with physical or mental activity, but doesn't improve with rest ... What should you do if you have post-COVID-19 syndrome symptoms ? If you 're having symptoms of post-COVID-19 syndrome, talk to your health care provider ...
Summary: The Centers for Disease Control and Prevention states that there is no evidence to suggest that people who have recovered from COVID-19 and have antibodies are not able to transmit the virus.
Completely Wrong: The document is about the long-term effects of COVID-19. However, model is likely utilizing its parameterized knowledge and draws the conclusion directly.</p>
<p>Figure 12: Three examples from the faithfulness evaluation (Section 6.1), showing the cases of minor error, major error, and completely wrong, respectively. Red text denotes the mismatches between the summary and the document.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{12}$ We also experimented with few-shot prompts. However, these did not yield better performance than the zero-shot prompt.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ http://www.microsoft.com/en-us/bing/apis/ bing-web-search-api
${ }^{4}$ https://github.com/Alir3z4/html2text/
${ }^{5}$ https://github.com/buriy/python-readability
${ }^{6}$ Paywall, PDFs, and anti-scraping measures.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7767 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7767</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7767</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-266273016</p>
                <p><strong>Paper Title:</strong> Human-Written vs AI-Generated Texts in Orthopedic Academic Literature: Comparative Qualitative Analysis</p>
                <p><strong>Paper Abstract:</strong> Background As large language models (LLMs) are becoming increasingly integrated into different aspects of health care, questions about the implications for medical academic literature have begun to emerge. Key aspects such as authenticity in academic writing are at stake with artificial intelligence (AI) generating highly linguistically accurate and grammatically sound texts. Objective The objective of this study is to compare human-written with AI-generated scientific literature in orthopedics and sports medicine. Methods Five original abstracts were selected from the PubMed database. These abstracts were subsequently rewritten with the assistance of 2 LLMs with different degrees of proficiency. Subsequently, researchers with varying degrees of expertise and with different areas of specialization were asked to rank the abstracts according to linguistic and methodological parameters. Finally, researchers had to classify the articles as AI generated or human written. Results Neither the researchers nor the AI-detection software could successfully identify the AI-generated texts. Furthermore, the criteria previously suggested in the literature did not correlate with whether the researchers deemed a text to be AI generated or whether they judged the article correctly based on these parameters. Conclusions The primary finding of this study was that researchers were unable to distinguish between LLM-generated and human-written texts. However, due to the small sample size, it is not possible to generalize the results of this study. As is the case with any tool used in academic research, the potential to cause harm can be mitigated by relying on the transparency and integrity of the researchers. With scientific integrity at stake, further research with a similar study design should be conducted to determine the magnitude of this issue.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7767.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7767.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Expert Rating</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human expert peer-review style rating of abstracts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human reviewers (2 senior, 2 junior) evaluated each abstract on peer-review-style parameters (nuance, style, originality, objectivity, grammatical soundness, comprehension) and classified origin (human vs AI) using a 1–5 scale and categorical labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human-Written vs AI-Generated Texts in Orthopedic Academic Literature: Comparative Qualitative Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT 3.4; You.com</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>orthopedics / scientific publishing / NLP evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation of generated scientific text (plausibility/attribution)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human expert rating (peer-review parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Four human researchers rated each abstract on pre-specified peer-review-like parameters (nuance, style, originality, objectivity, grammatical soundness, comprehension) using an ordinal 1–5 scale and additionally classified each abstract as generated by a newer-generation AI, a more-developed AI, or a human; short textual explanations were collected.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Per-parameter Likert ratings (1–5); categorical classification labels (AI vs human; newer vs advanced AI); interrater/intrarater reliability assessed</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Likert scale: 1 = very bad through 5 = very good for each parameter; categorical labels for origin (human, newer AI, advanced AI); reliability measured by comparing assessments across/within raters</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>PubMed-selected meniscal injury abstracts and AI-generated/rewritten variants (total N=25)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>4 participants (2 senior researchers with >10 years and doctoral degrees; 2 junior researchers within first 2 years); each rated 25 randomized abstracts on multiple parameters (1–5) and classified origin; interrater and intrarater reliability procedures were described (comparisons across/within evaluators).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Neither human reviewers nor AI-detection software could reliably distinguish AI-generated texts from human-written texts; suggested criteria (originality, style, nuance) did not correlate with correct identification; one junior rater showed correct identification by objectivity parameter but overall results were not conclusive.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>LLM-generated abstracts were largely indistinguishable from human-written abstracts to human raters; criterion scores did not reliably predict correct classification.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Small sample size (5 source articles, 25 abstracts total), raters were non-native English speakers (possible effect), only 4 human evaluators, limited diversity of article topics (meniscal injuries).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human-Written vs AI-Generated Texts in Orthopedic Academic Literature: Comparative Qualitative Analysis', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7767.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7767.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Likert 1–5 Rating</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>1–5 Likert rating scale for evaluation parameters</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ordinal rating scale used to quantify multiple qualitative dimensions (originality, nuance, style, objectivity, grammatical soundness, comprehension) for each abstract, later used in descriptive and inferential analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human-Written vs AI-Generated Texts in Orthopedic Academic Literature: Comparative Qualitative Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT 3.4; You.com</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>evaluation / psychometrics / NLP assessment</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Likert 1–5 scoring</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Raters assigned an ordinal score from 1 to 5 to each abstract for each pre-specified parameter; scores were used to test correlations with correct identification and to compare groups.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Ordinal scores per parameter (1–5)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>1 = very bad, 2 = bad, 3 = neutral/average, 4 = good, 5 = very good; applied independently to each qualitative parameter</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>25 abstracts (5 original PubMed abstracts + 20 AI rewritten/generated variants)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Scores provided by 4 human raters across all abstracts; ratings used for interrater/intrarater comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>No correlation found between these 1–5 parameter scores and correct identification of AI-generated texts according to the paper's analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Parameter scores did not reliably differentiate human-written and AI-generated abstracts in this sample.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Ordinal scale may miss subtle stylistic signals; very small sample size; rater subjectivity and language proficiency could bias scores.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human-Written vs AI-Generated Texts in Orthopedic Academic Literature: Comparative Qualitative Analysis', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7767.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7767.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-detection software</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unspecified automated AI-detection platform(s) (tool not named in methods; Turnitin referenced in discussion)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automated classifier(s) used to predict whether each abstract was human-written or AI-generated; outputs were aggregated and compared to ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human-Written vs AI-Generated Texts in Orthopedic Academic Literature: Comparative Qualitative Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT 3.4; You.com (texts evaluated by the detector)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / AI-detection</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>binary classification</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Automated AI-detection classification</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>A detection platform automatically labeled each abstract as 'human written' or 'AI generated'; labeled counts were tabulated for each abstract provenance group and compared to actual provenance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Counts (n) of abstracts predicted as human-written vs AI-generated per provenance group</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>For each provenance group (original human; rewritten by advanced AI; rewritten by newer AI; completely generated by advanced AI; completely generated by newer AI), the detector outputs number of abstracts labeled human vs AI (integer counts).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>25 abstracts (5 human originals, 20 AI rewritten/generated)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Not applicable (automated tool); human raters evaluated separately.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Table 3 counts reported: Written by humans: predicted human = 4, predicted AI = 1; Rewritten by advanced AI: predicted human = 1, predicted AI = 3; Rewritten by newer unadvanced AI: predicted human = 3, predicted AI = 2; Completely generated by advanced AI: predicted human = 1, predicted AI = 4; Completely generated by newer unadvanced AI: predicted human = 2, predicted AI = 3.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Automated detector misclassified a substantial fraction of AI-generated abstracts as human-written and did not reliably separate human and AI provenance in this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Detection tool is unnamed and not characterized; small sample; potential for adversarially modified texts to evade detection; no tool calibration or thresholds reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human-Written vs AI-Generated Texts in Orthopedic Academic Literature: Comparative Qualitative Analysis', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7767.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7767.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Statistical Tests</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mann-Whitney U, Wilcoxon W, Z test, asymptotic two-tailed P-value</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Nonparametric and significance tests used to analyze relationships between rater experience, parameter scores, and correct identification outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human-Written vs AI-Generated Texts in Orthopedic Academic Literature: Comparative Qualitative Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT 3.4; You.com</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>statistics / hypothesis testing</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>statistical evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Mann-Whitney U, Wilcoxon W, Z test with asymptotic two-tailed P-values</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Nonparametric tests were applied to compare distributions (eg, rating scores) and to assess correlations between rater experience and correct identification or between criterion scores and identification; p-values reported via asymptotic two-tailed significance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Test statistics (U, W, Z) and corresponding two-tailed P-values</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>P-value indicates probability of observing data under the null hypothesis; significance threshold not explicitly stated in paper (commonly alpha = 0.05).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Ratings and classification outcomes from 4 human raters across 25 abstracts</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Used to compare groups (senior vs junior) and correlations between rating parameters and correct identification; details of exact test outputs not fully reported.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>The paper states these tests were used and that no correlations were found between suggested criteria and correct identification; specific U/W/Z statistics or P-values are not provided in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Statistical analyses did not reveal significant differences or correlations that allowed reliable identification of AI-generated vs human abstracts in this sample.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Small sample size reduces statistical power; detailed numerical results (test statistics, exact P-values) not reported, limiting reproducibility and secondary analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human-Written vs AI-Generated Texts in Orthopedic Academic Literature: Comparative Qualitative Analysis', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7767.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7767.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PubMed Meniscal Abstracts (corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PubMed-selected meniscal injury abstracts and AI-generated/rewritten variants (total N=25)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Corpus assembled for the study: searched PubMed for 'meniscus', selected first 5 Q1/Q2 journal abstracts on meniscal injuries, then generated AI-rewrites and AI-original abstracts using two LLMs to create a 25-abstract test set for human and automated evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human-Written vs AI-Generated Texts in Orthopedic Academic Literature: Comparative Qualitative Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT 3.4; You.com</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>orthopedics / NLP evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>dataset / corpus</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Manual PubMed selection + LLM generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Search PubMed with term 'meniscus' and choose first 5 articles from Q1/Q2 journals; produce rewrites and novel abstracts using ChatGPT 3.4 and You.com with prompts 'rewrite the following in perfect academic English' and 'write five abstracts on meniscal injuries'; randomize numbering of the 25 resulting abstracts for blinded evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Number of abstracts per provenance group</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Composition: 5 original human abstracts; 5 rewritten by ChatGPT 3.4; 5 rewritten by You.com; 5 newly generated by ChatGPT 3.4; 5 newly generated by You.com = total 25 abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>PubMed (search term 'meniscus') plus two LLM-generated sets</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Randomized 25-abstract bundle evaluated by 4 human raters and by AI-detection software.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Dataset used as basis for human and automated evaluations reported in tables; outcomes showed poor discrimination between human and AI texts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Mixing human and AI abstracts in the corpus resulted in humans and detection software failing to reliably identify provenance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Corpus topic limited to meniscal injuries (a 'hot topic' subset); small number of source abstracts; potential undisclosed AI use in original PubMed abstracts not traceable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human-Written vs AI-Generated Texts in Orthopedic Academic Literature: Comparative Qualitative Analysis', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7767.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7767.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation Criteria (textual)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Set of textual evaluation criteria: originality, nuance, style, subtle phrasing, word choice, objectivity, grammatical soundness, comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Qualitative attributes proposed and used as parameters to help differentiate human-written from AI-generated academic texts; each criterion was scored by raters and tested for correlation with correct identification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human-Written vs AI-Generated Texts in Orthopedic Academic Literature: Comparative Qualitative Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT 3.4; You.com</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>textual analysis / peer review / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>qualitative evaluation criteria</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Qualitative criteria scoring and correlation analysis</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Raters scored each abstract on the listed qualitative criteria with a 1–5 scale; the study tested whether these criterion scores correlated with correct human identification of AI-generated abstracts and whether they predicted judgements of AI vs human.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Per-criterion Likert scores and correlation analyses (statistical tests stated: Mann-Whitney U, Wilcoxon W, Z test, asymptotic P)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Each criterion scored 1 = very bad to 5 = very good; correlations between these scores and identification outcomes assessed using nonparametric tests and significance testing.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>25 randomized abstracts from PubMed and LLM generation</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>4 raters provided scores and short textual explanations; analyses compared criterion scores to correct/wrong classifications.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>The criteria (originality, style, nuance, etc.) did not correlate with correct identification and did not predict whether raters judged texts as AI-generated or human-written.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>No reliable differences or predictive power of these criteria between AI and human texts were found in this sample.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>These qualitative criteria (originating in prior literature) failed in this small sample; subjective scoring and evaluator language proficiency may confound results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human-Written vs AI-Generated Texts in Orthopedic Academic Literature: Comparative Qualitative Analysis', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Comparing scientific abstracts generated by ChatGPT to real abstracts with detectors and blinded human reviewers <em>(Rating: 2)</em></li>
                <li>Fighting fire with fire: can ChatGPT detect AI-generated text? <em>(Rating: 2)</em></li>
                <li>AI vs human --differentiation analysis of scientific content generation <em>(Rating: 2)</em></li>
                <li>Differentiate ChatGPT-generated and human-written medical texts <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7767",
    "paper_id": "paper-266273016",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "Human Expert Rating",
            "name_full": "Human expert peer-review style rating of abstracts",
            "brief_description": "Human reviewers (2 senior, 2 junior) evaluated each abstract on peer-review-style parameters (nuance, style, originality, objectivity, grammatical soundness, comprehension) and classified origin (human vs AI) using a 1–5 scale and categorical labels.",
            "citation_title": "Human-Written vs AI-Generated Texts in Orthopedic Academic Literature: Comparative Qualitative Analysis",
            "mention_or_use": "use",
            "model_name": "ChatGPT 3.4; You.com",
            "model_size": null,
            "scientific_domain": "orthopedics / scientific publishing / NLP evaluation",
            "theory_type": "evaluation of generated scientific text (plausibility/attribution)",
            "evaluation_method_name": "Human expert rating (peer-review parameters)",
            "evaluation_method_description": "Four human researchers rated each abstract on pre-specified peer-review-like parameters (nuance, style, originality, objectivity, grammatical soundness, comprehension) using an ordinal 1–5 scale and additionally classified each abstract as generated by a newer-generation AI, a more-developed AI, or a human; short textual explanations were collected.",
            "evaluation_metric": "Per-parameter Likert ratings (1–5); categorical classification labels (AI vs human; newer vs advanced AI); interrater/intrarater reliability assessed",
            "metric_definition": "Likert scale: 1 = very bad through 5 = very good for each parameter; categorical labels for origin (human, newer AI, advanced AI); reliability measured by comparing assessments across/within raters",
            "dataset_or_benchmark": "PubMed-selected meniscal injury abstracts and AI-generated/rewritten variants (total N=25)",
            "human_evaluation_details": "4 participants (2 senior researchers with &gt;10 years and doctoral degrees; 2 junior researchers within first 2 years); each rated 25 randomized abstracts on multiple parameters (1–5) and classified origin; interrater and intrarater reliability procedures were described (comparisons across/within evaluators).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Neither human reviewers nor AI-detection software could reliably distinguish AI-generated texts from human-written texts; suggested criteria (originality, style, nuance) did not correlate with correct identification; one junior rater showed correct identification by objectivity parameter but overall results were not conclusive.",
            "comparison_to_human_generated": true,
            "comparison_results": "LLM-generated abstracts were largely indistinguishable from human-written abstracts to human raters; criterion scores did not reliably predict correct classification.",
            "limitations_noted": "Small sample size (5 source articles, 25 abstracts total), raters were non-native English speakers (possible effect), only 4 human evaluators, limited diversity of article topics (meniscal injuries).",
            "uuid": "e7767.0",
            "source_info": {
                "paper_title": "Human-Written vs AI-Generated Texts in Orthopedic Academic Literature: Comparative Qualitative Analysis",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Likert 1–5 Rating",
            "name_full": "1–5 Likert rating scale for evaluation parameters",
            "brief_description": "Ordinal rating scale used to quantify multiple qualitative dimensions (originality, nuance, style, objectivity, grammatical soundness, comprehension) for each abstract, later used in descriptive and inferential analyses.",
            "citation_title": "Human-Written vs AI-Generated Texts in Orthopedic Academic Literature: Comparative Qualitative Analysis",
            "mention_or_use": "use",
            "model_name": "ChatGPT 3.4; You.com",
            "model_size": null,
            "scientific_domain": "evaluation / psychometrics / NLP assessment",
            "theory_type": "metric",
            "evaluation_method_name": "Likert 1–5 scoring",
            "evaluation_method_description": "Raters assigned an ordinal score from 1 to 5 to each abstract for each pre-specified parameter; scores were used to test correlations with correct identification and to compare groups.",
            "evaluation_metric": "Ordinal scores per parameter (1–5)",
            "metric_definition": "1 = very bad, 2 = bad, 3 = neutral/average, 4 = good, 5 = very good; applied independently to each qualitative parameter",
            "dataset_or_benchmark": "25 abstracts (5 original PubMed abstracts + 20 AI rewritten/generated variants)",
            "human_evaluation_details": "Scores provided by 4 human raters across all abstracts; ratings used for interrater/intrarater comparisons",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "No correlation found between these 1–5 parameter scores and correct identification of AI-generated texts according to the paper's analysis.",
            "comparison_to_human_generated": true,
            "comparison_results": "Parameter scores did not reliably differentiate human-written and AI-generated abstracts in this sample.",
            "limitations_noted": "Ordinal scale may miss subtle stylistic signals; very small sample size; rater subjectivity and language proficiency could bias scores.",
            "uuid": "e7767.1",
            "source_info": {
                "paper_title": "Human-Written vs AI-Generated Texts in Orthopedic Academic Literature: Comparative Qualitative Analysis",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "AI-detection software",
            "name_full": "Unspecified automated AI-detection platform(s) (tool not named in methods; Turnitin referenced in discussion)",
            "brief_description": "Automated classifier(s) used to predict whether each abstract was human-written or AI-generated; outputs were aggregated and compared to ground truth.",
            "citation_title": "Human-Written vs AI-Generated Texts in Orthopedic Academic Literature: Comparative Qualitative Analysis",
            "mention_or_use": "use",
            "model_name": "ChatGPT 3.4; You.com (texts evaluated by the detector)",
            "model_size": null,
            "scientific_domain": "NLP / AI-detection",
            "theory_type": "binary classification",
            "evaluation_method_name": "Automated AI-detection classification",
            "evaluation_method_description": "A detection platform automatically labeled each abstract as 'human written' or 'AI generated'; labeled counts were tabulated for each abstract provenance group and compared to actual provenance.",
            "evaluation_metric": "Counts (n) of abstracts predicted as human-written vs AI-generated per provenance group",
            "metric_definition": "For each provenance group (original human; rewritten by advanced AI; rewritten by newer AI; completely generated by advanced AI; completely generated by newer AI), the detector outputs number of abstracts labeled human vs AI (integer counts).",
            "dataset_or_benchmark": "25 abstracts (5 human originals, 20 AI rewritten/generated)",
            "human_evaluation_details": "Not applicable (automated tool); human raters evaluated separately.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Table 3 counts reported: Written by humans: predicted human = 4, predicted AI = 1; Rewritten by advanced AI: predicted human = 1, predicted AI = 3; Rewritten by newer unadvanced AI: predicted human = 3, predicted AI = 2; Completely generated by advanced AI: predicted human = 1, predicted AI = 4; Completely generated by newer unadvanced AI: predicted human = 2, predicted AI = 3.",
            "comparison_to_human_generated": true,
            "comparison_results": "Automated detector misclassified a substantial fraction of AI-generated abstracts as human-written and did not reliably separate human and AI provenance in this dataset.",
            "limitations_noted": "Detection tool is unnamed and not characterized; small sample; potential for adversarially modified texts to evade detection; no tool calibration or thresholds reported.",
            "uuid": "e7767.2",
            "source_info": {
                "paper_title": "Human-Written vs AI-Generated Texts in Orthopedic Academic Literature: Comparative Qualitative Analysis",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Statistical Tests",
            "name_full": "Mann-Whitney U, Wilcoxon W, Z test, asymptotic two-tailed P-value",
            "brief_description": "Nonparametric and significance tests used to analyze relationships between rater experience, parameter scores, and correct identification outcomes.",
            "citation_title": "Human-Written vs AI-Generated Texts in Orthopedic Academic Literature: Comparative Qualitative Analysis",
            "mention_or_use": "use",
            "model_name": "ChatGPT 3.4; You.com",
            "model_size": null,
            "scientific_domain": "statistics / hypothesis testing",
            "theory_type": "statistical evaluation",
            "evaluation_method_name": "Mann-Whitney U, Wilcoxon W, Z test with asymptotic two-tailed P-values",
            "evaluation_method_description": "Nonparametric tests were applied to compare distributions (eg, rating scores) and to assess correlations between rater experience and correct identification or between criterion scores and identification; p-values reported via asymptotic two-tailed significance.",
            "evaluation_metric": "Test statistics (U, W, Z) and corresponding two-tailed P-values",
            "metric_definition": "P-value indicates probability of observing data under the null hypothesis; significance threshold not explicitly stated in paper (commonly alpha = 0.05).",
            "dataset_or_benchmark": "Ratings and classification outcomes from 4 human raters across 25 abstracts",
            "human_evaluation_details": "Used to compare groups (senior vs junior) and correlations between rating parameters and correct identification; details of exact test outputs not fully reported.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "The paper states these tests were used and that no correlations were found between suggested criteria and correct identification; specific U/W/Z statistics or P-values are not provided in the text.",
            "comparison_to_human_generated": true,
            "comparison_results": "Statistical analyses did not reveal significant differences or correlations that allowed reliable identification of AI-generated vs human abstracts in this sample.",
            "limitations_noted": "Small sample size reduces statistical power; detailed numerical results (test statistics, exact P-values) not reported, limiting reproducibility and secondary analysis.",
            "uuid": "e7767.3",
            "source_info": {
                "paper_title": "Human-Written vs AI-Generated Texts in Orthopedic Academic Literature: Comparative Qualitative Analysis",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "PubMed Meniscal Abstracts (corpus)",
            "name_full": "PubMed-selected meniscal injury abstracts and AI-generated/rewritten variants (total N=25)",
            "brief_description": "Corpus assembled for the study: searched PubMed for 'meniscus', selected first 5 Q1/Q2 journal abstracts on meniscal injuries, then generated AI-rewrites and AI-original abstracts using two LLMs to create a 25-abstract test set for human and automated evaluation.",
            "citation_title": "Human-Written vs AI-Generated Texts in Orthopedic Academic Literature: Comparative Qualitative Analysis",
            "mention_or_use": "use",
            "model_name": "ChatGPT 3.4; You.com",
            "model_size": null,
            "scientific_domain": "orthopedics / NLP evaluation",
            "theory_type": "dataset / corpus",
            "evaluation_method_name": "Manual PubMed selection + LLM generation",
            "evaluation_method_description": "Search PubMed with term 'meniscus' and choose first 5 articles from Q1/Q2 journals; produce rewrites and novel abstracts using ChatGPT 3.4 and You.com with prompts 'rewrite the following in perfect academic English' and 'write five abstracts on meniscal injuries'; randomize numbering of the 25 resulting abstracts for blinded evaluation.",
            "evaluation_metric": "Number of abstracts per provenance group",
            "metric_definition": "Composition: 5 original human abstracts; 5 rewritten by ChatGPT 3.4; 5 rewritten by You.com; 5 newly generated by ChatGPT 3.4; 5 newly generated by You.com = total 25 abstracts.",
            "dataset_or_benchmark": "PubMed (search term 'meniscus') plus two LLM-generated sets",
            "human_evaluation_details": "Randomized 25-abstract bundle evaluated by 4 human raters and by AI-detection software.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Dataset used as basis for human and automated evaluations reported in tables; outcomes showed poor discrimination between human and AI texts.",
            "comparison_to_human_generated": true,
            "comparison_results": "Mixing human and AI abstracts in the corpus resulted in humans and detection software failing to reliably identify provenance.",
            "limitations_noted": "Corpus topic limited to meniscal injuries (a 'hot topic' subset); small number of source abstracts; potential undisclosed AI use in original PubMed abstracts not traceable.",
            "uuid": "e7767.4",
            "source_info": {
                "paper_title": "Human-Written vs AI-Generated Texts in Orthopedic Academic Literature: Comparative Qualitative Analysis",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Evaluation Criteria (textual)",
            "name_full": "Set of textual evaluation criteria: originality, nuance, style, subtle phrasing, word choice, objectivity, grammatical soundness, comprehension",
            "brief_description": "Qualitative attributes proposed and used as parameters to help differentiate human-written from AI-generated academic texts; each criterion was scored by raters and tested for correlation with correct identification.",
            "citation_title": "Human-Written vs AI-Generated Texts in Orthopedic Academic Literature: Comparative Qualitative Analysis",
            "mention_or_use": "use",
            "model_name": "ChatGPT 3.4; You.com",
            "model_size": null,
            "scientific_domain": "textual analysis / peer review / NLP",
            "theory_type": "qualitative evaluation criteria",
            "evaluation_method_name": "Qualitative criteria scoring and correlation analysis",
            "evaluation_method_description": "Raters scored each abstract on the listed qualitative criteria with a 1–5 scale; the study tested whether these criterion scores correlated with correct human identification of AI-generated abstracts and whether they predicted judgements of AI vs human.",
            "evaluation_metric": "Per-criterion Likert scores and correlation analyses (statistical tests stated: Mann-Whitney U, Wilcoxon W, Z test, asymptotic P)",
            "metric_definition": "Each criterion scored 1 = very bad to 5 = very good; correlations between these scores and identification outcomes assessed using nonparametric tests and significance testing.",
            "dataset_or_benchmark": "25 randomized abstracts from PubMed and LLM generation",
            "human_evaluation_details": "4 raters provided scores and short textual explanations; analyses compared criterion scores to correct/wrong classifications.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "The criteria (originality, style, nuance, etc.) did not correlate with correct identification and did not predict whether raters judged texts as AI-generated or human-written.",
            "comparison_to_human_generated": true,
            "comparison_results": "No reliable differences or predictive power of these criteria between AI and human texts were found in this sample.",
            "limitations_noted": "These qualitative criteria (originating in prior literature) failed in this small sample; subjective scoring and evaluator language proficiency may confound results.",
            "uuid": "e7767.5",
            "source_info": {
                "paper_title": "Human-Written vs AI-Generated Texts in Orthopedic Academic Literature: Comparative Qualitative Analysis",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Comparing scientific abstracts generated by ChatGPT to real abstracts with detectors and blinded human reviewers",
            "rating": 2,
            "sanitized_title": "comparing_scientific_abstracts_generated_by_chatgpt_to_real_abstracts_with_detectors_and_blinded_human_reviewers"
        },
        {
            "paper_title": "Fighting fire with fire: can ChatGPT detect AI-generated text?",
            "rating": 2,
            "sanitized_title": "fighting_fire_with_fire_can_chatgpt_detect_aigenerated_text"
        },
        {
            "paper_title": "AI vs human --differentiation analysis of scientific content generation",
            "rating": 2,
            "sanitized_title": "ai_vs_human_differentiation_analysis_of_scientific_content_generation"
        },
        {
            "paper_title": "Differentiate ChatGPT-generated and human-written medical texts",
            "rating": 2,
            "sanitized_title": "differentiate_chatgptgenerated_and_humanwritten_medical_texts"
        }
    ],
    "cost": 0.01732575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Human-Written vs AI-Generated Texts in Orthopedic Academic Literature: Comparative Qualitative Analysis</p>
<p>BScHassan Tarek Hakam hassantarek.hakam@mhb-fontane.de 
Center of Orthopaedics and Trauma Surgery
Brandenburg Medical School
University Clinic of Brandenburg
Brandenburg an der HavelGermany</p>
<p>Faculty of Health Sciences
University Clinic of Brandenburg
Brandenburg an der HavelGermany</p>
<p>Center of Evidence Based Practice
Brandenburg</p>
<p>MDRobert Prill 
Faculty of Health Sciences
University Clinic of Brandenburg
Brandenburg an der HavelGermany</p>
<p>Center of Evidence Based Practice
Brandenburg</p>
<p>PhDLisa Korte 
Center of Health Services Research
Faculty of Health Sciences
University Clinic of Brandenburg
Rüdersdorf bei BerlinGermany</p>
<p>PhD;Bruno Lovreković 
Faculty of Orthopaedics
University Hospital Merkur
ZagrebCroatia</p>
<p>MDMarko Ostojić 
Departement of Orthopaedics
University Hospital Mostar
MostarBosnia and Herzegovina</p>
<p>MDNikolai Ramadanov 
Center of Orthopaedics and Trauma Surgery
Brandenburg Medical School
University Clinic of Brandenburg
Brandenburg an der HavelGermany</p>
<p>Faculty of Health Sciences
University Clinic of Brandenburg
Brandenburg an der HavelGermany</p>
<p>MDFelix Muehlensiepen 
Center of Evidence Based Practice
Brandenburg</p>
<p>Center of Health Services Research
Faculty of Health Sciences
University Clinic of Brandenburg
Rüdersdorf bei BerlinGermany</p>
<p>JBI Affiliated Group
Brandenburg an der HavelGermany</p>
<p>Center of Orthopaedics and Trauma Surgery
University Clinic of Brandenburg Brandenburg Medical School Hochstr 29 Brandenburg an der Havel
14770, 03381 411940Germany Phone</p>
<p>Human-Written vs AI-Generated Texts in Orthopedic Academic Literature: Comparative Qualitative Analysis
87AEBB9EB408BAB36AF189AE4FB8C8D810.2196/52164artificial intelligenceAIlarge language modelLLMresearchorthopedic surgerysports medicineorthopedicssurgeryorthopedicqualitative studymedical databasefeedbackdetectiontoolscientific integritystudy design
Background: As large language models (LLMs) are becoming increasingly integrated into different aspects of health care, questions about the implications for medical academic literature have begun to emerge.Key aspects such as authenticity in academic writing are at stake with artificial intelligence (AI) generating highly linguistically accurate and grammatically sound texts.Objective: The objective of this study is to compare human-written with AI-generated scientific literature in orthopedics and sports medicine.Methods: Five original abstracts were selected from the PubMed database.These abstracts were subsequently rewritten with the assistance of 2 LLMs with different degrees of proficiency.Subsequently, researchers with varying degrees of expertise and with different areas of specialization were asked to rank the abstracts according to linguistic and methodological parameters.Finally, researchers had to classify the articles as AI generated or human written.Results: Neither the researchers nor the AI-detection software could successfully identify the AI-generated texts.Furthermore, the criteria previously suggested in the literature did not correlate with whether the researchers deemed a text to be AI generated or whether they judged the article correctly based on these parameters.Conclusions:The primary finding of this study was that researchers were unable to distinguish between LLM-generated and human-written texts.However, due to the small sample size, it is not possible to generalize the results of this study.As is the case with any tool used in academic research, the potential to cause harm can be mitigated by relying on the transparency and integrity of the researchers.With scientific integrity at stake, further research with a similar study design should be conducted to determine the magnitude of this issue.</p>
<p>Introduction</p>
<p>Artificial intelligence (AI) is perhaps best defined as an algorithmic mechanism applied to machines, whereby solving challenges requires little to no human interaction [1].Differentiating human-made and AI-generated work is becoming increasingly difficult with the rapid technological advancement of deep learning [2].Deep learning is based on the replication of human thinking and the brain's structure [3].With the vast potential benefit that AI might bring to the table, extensive research has been conducted in the last decade with the purpose of finding potential solutions for health care-related problems [4].The field of orthopedics, for example, might greatly benefit from AI image recognition capabilities to assist in the diagnosis of fractures or skin lesions.Other benefits can be drawn from AI's capacity to analyze massive amounts of clinical information, which in turn presents benefits in clinical decision-making, risk assessment, and the generation of individualized care plans [5].That is why an exponential increase in research on the topic of AI in the field of orthopedics has been noted, which has led to a subsequent increase in reviews trying to summarize the findings and give out recommendations [4].</p>
<p>Orthopedic sports medicine is the subspecialty of orthopedics that deals with pathologic conditions of the musculoskeletal system that arise from the practice of sports.This includes the prevention, diagnosis, and treatment of diseases.A particular challenge of sports medicine lies in the willingness of athletes to return to performance in a timely manner [4].Through the use of deep neural networks, AI can assist specialists in various aspects of management.AI has shown to be especially advantageous for the diagnosis of fractures based on plain radiographs and computed tomography, with reviews reporting high accuracy, sensitivity, and specificity for the evaluation of plain radiographs [6] and computed tomography images [7].With the evolution of convolutional neural networks and the increased capacity to integrate large amounts of written information, the patient's medical records could serve as a basis for determining an individualized care plan as well as for making predictions for the best future course of treatment [8].</p>
<p>The influence of large language models (LLMs) on research in the field of orthopedics and sports medicine has not yet been well studied.AI is commonly used by researchers to help organize thought processes, obtain feedback, edit their work, and present their citations in the requested format.Consequently, AI has made academic work much more efficient [9].However, considering that some of the most impactful journals allow the use of AI in composing or editing scientific texts, there are some ethical reservations regarding the authenticity and credibility of academic work [2].Furthermore, some journals are actively involved in the development of tools to spot AI-generated texts [10].In the light of this, the line where scientific research becomes fraudulent with regards to the use of AI must be determined.Different journals have adopted different guidelines for the use of AI.</p>
<p>The aim of this qualitative analysis is to determine the possibility that human researchers and AI-detection platforms can detect AI-generated texts.For this purpose, 4 researchers were recruited to participate in this study.As well as this, an AI-detection platform was used to assist in this endeavor.</p>
<p>Methods</p>
<p>This study adopted a similar method to previously conducted research on the matter [10].</p>
<p>Recruitment</p>
<p>For the purposes of the study, 4 participants were recruited.Two senior researchers in the fields of orthopedics and qualitative research, as well as 2 junior researchers in the same fields, expressed their interest in the subject at hand.All researchers were informed about the study's objectives.The inclusion criteria for senior researchers were more than 10 years of research experience and having a doctoral degree in their field.Junior researchers were defined as students or physicians who had commenced their first project in the last 2 years.</p>
<p>Ethical Considerations</p>
<p>Due to the noninterventional nature of this study, as well as the anonymization of the included participants, local institutional and regulatory bodies did not require ethical approval.The methodology of the study and data collection were in line with the Geneva conventions.Informed consent was obtained from all participants involved in this study.The privacy and confidentiality of the involved participants has been protected by anonymizing their responses.No compensation was given to the participating individuals.</p>
<p>Selection of Literature</p>
<p>After searching PubMed for relevant material, 5 abstracts about meniscal injuries were selected for inclusion in the study [11][12][13][14][15].The search strategy included the word "meniscus."Subsequently, the first 5 articles published in reputable first quartile (Q1) or second quartile (Q2) journals were chosen to ensure the high quality of the articles.Abstracts that did not meet the criteria were excluded.This choice was made based on the fact that abstracts usually present a general overview of the topic at hand and communicate the main objectives of the paper.Although some treatment modalities are commonly applied to meniscal injuries, it is often impossible to completely restore the meniscal architecture, especially when the injury occurs in the middle, less vascularized portion [16].Selecting meniscal injuries as a topic was, therefore, agreed upon by the research team as it is a common pathologic condition [17] and an area of extensive research [18].</p>
<p>Involving AI</p>
<p>Abstracts selected in the previous step were then rewritten by 2 AI platforms.One platform was the commonly used and extensively developed ChatGPT 3.4 (OpenAI) and the other was You.com.Using the instruction "rewrite the following in perfect academic English," 5 new abstracts were generated by each AI.In the subsequent step, the command "write five abstracts on meniscal injuries" was used and 10 further abstracts were generated.</p>
<p>XSL • FO</p>
<p>RenderX</p>
<p>Randomization</p>
<p>The 25 resulting abstracts included the 5 original versions that were written by humans, the 5 rewritten versions that were generated by each AI, and the 5 newly generated versions that were composed by each AI platform.The abstracts were numbered from 1 to 25.These numbers were subsequently randomized using Microsoft Excel and the assigned abstracts were presented as a sheaf in the resulting order.</p>
<p>Evaluation</p>
<p>Evaluation of the abstracts was carried out using 2 methods.The first method of evaluation involved researchers with varying specialties and at different stages of their academic careers, while the second was based on the use of AI-detection software.</p>
<p>Participants were then asked to evaluate all the resulting abstracts using parameters that are commonly used for peer review.Suggested criteria that might aid in differentiating human-written from AI-generated literature included nuance, style, and originality [10].Subtle phrasing and word choice might also be giveaways.A rating scale from 1 (very bad) to 5 (very good) was used for each parameter.</p>
<p>Participants were additionally asked whether they thought that the abstract was generated by a newer-generation AI, a more-developed AI, or a human.A short explanation was provided by each participant.</p>
<p>User Statistics</p>
<p>Descriptive statistics were used to investigate the correlation between the degree of academic experience and the number of correctly identified abstracts on one hand and between the previously mentioned parameters (eg, originality, grammatical soundness) and the correct identification of abstracts on the other.Furthermore, the correlation between the parameters and a researcher's classification of an abstract was investigated.Interrater reliability was assessed by comparing the assessment of different articles by the same researcher, on the levels of both correct identification and assessed parameters.Intrarater reliability was assessed by comparing the assessments of different evaluators for both previously mentioned parameters.</p>
<p>The Mann-Whitney U test, the Wilcoxon W test, the Z test, and the asymptotic significance (2-tailed) P value were determined.</p>
<p>Results</p>
<p>The results of the analysis are presented in Tables 1-3.Further descriptive statistics are presented in Multimedia Appendix 1.</p>
<p>Discussion</p>
<p>Principal Results</p>
<p>The primary results of the study indicate that neither AI-detection software nor human critical appraisal can reliably distinguish AI-generated texts from human-written work.Regarding human detection of AI-generated texts, neither clinical experience nor area of expertise played a role in the evaluation of the presented material.The secondary results of the study indicate that criteria suggested by prior research, such as originality, style, and nuance, did not correlate with whether the researchers identified a text correctly or not.Furthermore, none of the criteria correlated with whether researchers judged a text as human written or AI generated.The qualitative analysis of the written answers did not provide any new insights on the subject in question.However, the junior orthopedic researcher was able to correctly identify texts according to the objectivity parameter.Whether this was due to correct interpretation or chance is unclear.Perhaps future studies with larger sample sizes can help in shedding light on this matter.Selecting the evaluators might have impacted the results of the study.Although the researchers were proficient published authors, English was not their primary language and this might have led to the inability to correctly identify the abstracts.However, the impact of this study is not reduced, as one might argue that scientific literature consumption is not restricted to researchers with English as their mother tongue.Furthermore, reading and publishing in English is becoming common practice, especially if research is considered to be relevant on the international level.</p>
<p>Comparison With Prior Work</p>
<p>Although AI is an evolutionary technology that presents an enormous potential for future research applications, the results of this study and previous studies with similar methodologies [10] are alarming.AI seems to have reached human-level writing skills, which in combination with its easy accessibility is able to threaten academic integrity.The findings of this analysis contradict previous claims for the ability to detect manuscripts generated by AI through model-agnostic and distribution-agnostic features [19].Even though nonmalicious applications of AI, including grammatical corrections, reference style adjustment, and thought-process organization, represent plausible uses of AI models, potential fraudulent uses include the generation of complete texts from a simple command.Examples of malicious AI use might also include the rewriting of entire texts [20,21], as shown in this study.AI-generated texts can also be passed through AI-detection software by malicious users, who would then use the texts that passed the examination, making it even more difficult to subsequently detect fraudulent use.</p>
<p>Besides the ability to falsify results, AI presents researchers with the capacity to present false results in a plausible manner [22,23].This also applies to inaccurate findings being reported confidently, which may be a misrepresentation that could lead to confusion, especially if the results are presented to unexperienced peers.Therefore, fact-checking the AI-generated statements and references will be essential when relying on such tools.AI also the capacity to generate images that can be used in the presentation of results [24].In the area of orthopedic surgery, AI has already been proven to recognize patterns associated with multiple types of fractures [25].Combined with its image-generation capacity, AI models will be able to create radiographic representations of fractures that are of no true scientific value but can be used to alter the results of a study.</p>
<p>Additionally, with the ever-increasing human inability to distinguish AI-and human-generated work, new rules must be written to ensure the scientific integrity of every published paper.Suggestions have included an increase in transparency in the design of AI models [26], as well as complete transparency in the use of AI by authors.This includes where and how LLMs were used in scientific projects [8,27].</p>
<p>Understanding the algorithms of these programs might aid in conceiving new and better programs to counteract fraud in its many forms.In an article in the journal Nature, the company Turnitin was reported to have incorporate AI-detection software [28].</p>
<p>Finally, and perhaps most importantly, the integrity of research is the most important aspect of the evolving discussion around the use of AI.Many previously conducted cross-examinations of academic publications revealed that research data obtained from prestigious academic institutions and published in equally prestigious academic journals were falsified.Whether these findings were intentionally corrupted or were errors of data collection is of little significance compared to the effects they might have on clinical and academic work.Thus, one can say that AI is just a tool, and its potential to cause good or harm is derived from individual motivations, experience level, and integrity [2].Calls to completely ban AI from academic endeavors are, in the eyes of the authors, exaggerated, and future fraud can be minimized by optimizing self-regulatory mechanisms [29] and AI-detection models [30,31].As well as this, the authors of this paper agree that detection of academic fraud is a responsibility of editors and journals, as a letter to Nature previously suggested [32].However, the central role of researchers cannot be overemphasized.</p>
<p>Limitations</p>
<p>Limitations of this study include the inability to trace AI use in the original articles included in this study.However, we assumed that if AI were used, it would have been reported in the methodology or declarations sections.A second limitation of this study is that English is not the native language of the assessors.However, all the involved researchers have deep levels of proficiency, having published prior research in English.A third limitation is the small sample size of examined individuals and AI-recognition software, which does not allow us to draw definite conclusions on the matter at hand.However, as LLMs in the field of AI become more sophisticated, the recommendations that were made by previous authors and mentioned in this paper will still hold.The final limitation of this study is that a subset of articles dealing with meniscal injuries was chosen from the immense field of orthopedics.This is particularly important when considering the "hot topic" subset.</p>
<p>Conclusions</p>
<p>The statistical and qualitative analysis of the presented material showed that researchers were unable to differentiate human-written from AI-generated texts.Furthermore, the secondary finding of this study was that previously suggested criteria, such as originality and comprehension, did not aid in the differentiation of human-written and LLM-generated texts.Both findings show that humans and AI-detection software currently fail to properly identify the use of LLMs in the academic literature.Furthermore, one can only speculate about the amount of undisclosed AI use in the academic literature.However, with the ever-increasing sophistication of LLMs, the integrity of future projects will be entirely dependent on scientists' attitudes, as AI can serve as a facilitator and accelerator in publishing but can also be used with malicious intent.With regard to replicating this study, the authors strongly recommend that a larger sample size of articles with a larger number of researchers should be considered.</p>
<p>Table 1 .
1
The number of human-written and artificial intelligence (AI)-generated texts that were correctly or incorrectly identified by academics with different levels of academic expertise.
Identified role of evaluatorEvaluations of original texts (n=5), nEvaluations of texts rewritten by AI (n=20), nHuman written (correctly identified)AI-generatedHuman writtenAI-generated (correctly identified)Junior orthopedic surgeon23515Senior orthopedic surgeon14416Junior qualitative researcher23812Senior qualitative researcher411010</p>
<p>Table 2 .
2
This table details how authors judged manuscripts with artificial intelligence (AI)-generated abstracts with respect to whether an advanced large language model or a newer large language model was used.
Identified role of evaluatorEvaluations of advanced AI-generated abstracts (n=10), nEvaluations of newer unadvanced AI-generatedabstracts (n=10), nAdvanced AI (correctly identified)Newer unadvanced AIAdvanced AINewer unadvanced AI (correctlyidentified)Junior orthopedic surgeon3755Senior orthopedic surgeon4664Junior qualitative researcher3791Senior qualitative researcher1973</p>
<p>Table 3 .
3
This table represents how artificial intelligence (AI)-detector software judged the articles.
AbstractsPredicted to be human written, nPredicted to be AI generated, nWritten by humans41Rewritten by advanced AI13Rewritten by newer unadvanced AI32Completely generated by advanced AI14Completely generated by newer unadvanced AI23
JMIR Form Res 2024 | vol. 8 | e52164 | p. 2 https://formative.jmir.org/2024/1/e52164 (page number not for citation purposes)
JMIR Form Res 2024 | vol. 8 | e52164 | p. 3 https://formative.jmir.org/2024/1/e52164 (page number not for citation purposes)
JMIR Form Res 2024 | vol. 8 | e52164 | p. 4 https://formative.jmir.org/2024/1/e52164 (page number not for citation purposes)
(page number not for citation purposes)
JMIR Form Res 2024 | vol. 8 | e52164 | p. 6 https://formative.jmir.org/2024/1/e52164 (page number not for citation purposes)
JMIR Form Res 2024 | vol. 8 | e52164 | p. 7 https://formative.jmir.org/2024/1/e52164 (page number not for citation purposes)
Data AvailabilityData will be made available by the corresponding author upon request.Authors' ContributionsHTH was the main author of the manuscript and the principal investigator.RP, FM, and LK contributed to the design of the study.MO reviewed the scientific soundness of the included literature.BL and NR curated and analyzed the data.Conflicts of InterestNone declared.Research, is properly cited.The complete bibliographic information, a link to the original publication on https://formative.jmir.org,as well as this copyright and license information must be included.Multimedia Appendix 1
Artificial intelligence and orthopaedics: an introduction for clinicians. T Myers, P Ramkumar, B Ricciardi, K Urish, J Kipper, C Ketonis, 10.2106/JBJS.19.01128J Bone Joint Surg Am. 1029May 06. 2020FREE Full text. Medline: 32379124</p>
<p>From human writing to artificial intelligence generated text: examining the prospects and potential threats of ChatGPT in academic writing. I Dergaa, K Chamari, P Zmijewski, Ben Saad, H , 10.5114/biolsport.2023.125623Biol Sport. 402Apr 2023FREE Full text. Medline: 37077800</p>
<p>A surgeon's guide to understanding artificial intelligence and machine learning studies in orthopaedic surgery. R M Shah, C Wong, N C Arpey, A A Patel, S N Divi, 10.1007/s12178-022-09738-7Curr Rev Musculoskelet Med. 152Apr 2022FREE Full text. Medline: 35141847</p>
<p>Artificial intelligence in orthopaedics: A scoping review. S J Federer, G G Jones, 10.1371/journal.pone.0260471Medline: 34813611]PLoS One. 1611e02604712021FREE Full text</p>
<p>Artificial intelligence in sports medicine: Could GPT-4 make human doctors obsolete?. K Cheng, Q Guo, Y He, Y Lu, R Xie, C Li, 10.1007/s10439-023-03213-1Ann Biomed Eng. 518Aug 2023</p>
<p>Advancements in artificial intelligence for foot and ankle surgery: a systematic review. P Gupta, K A Kingston, O' Malley, M Williams, R J Ramkumar, P N , 10.1177/24730114221151079Foot Ankle Orthop. 8124730114221151079Jan 2023FREE Full text. Medline: 36817020</p>
<p>Artificial intelligence fracture recognition on computed tomography: review of literature and recommendations. Lhm Dankelman, S Schilstra, Ffa Ijpma, J N Doornberg, J W Colaris, Mhj Verhofstad, 10.1007/s00068-022-02128-1Eur J Trauma Emerg Surg. 492Apr 2023FREE Full text</p>
<p>Ethics: disclose use of AI in scientific manuscripts. A Gaggioli, 10.1038/d41586-023-00381-xNature. 6147948413Feb 2023</p>
<p>Tools such as ChatGPT threaten transparent science; here are our ground rules for their use. 10.1038/d41586-023-00191-1Nature. 6137945612Jan 2023</p>
<p>Comparing scientific abstracts generated by ChatGPT to real abstracts with detectors and blinded human reviewers. C Gao, F Howard, N Markov, E C Dyer, S Ramesh, Y Luo, 10.1038/s41746-023-00819-6NPJ Digit Med. Apr. 261752023FREE Full text. Medline: 37100871</p>
<p>Meniscal repair: technique. P Beaufils, N Pujol, 10.1016/j.otsr.2017.04.016Orthop Traumatol Surg Res. Feb. 1041S2018FREE Full text. Medline: 29175557</p>
<p>Meniscal repair techniques. T Spalding, I Damasena, R Lawton, 10.1016/j.csm.2019.08.012Medline: 31767109]Clin Sports Med. 391Jan 2020</p>
<p>Rehabilitation following meniscal repair: a systematic review. Spang Iii, R C Nasr, M C Mohamadi, A Deangelis, J P Nazarian, A Ramappa, A J , 10.1136/bmjsem-2016-000212BMJ Open Sport Exerc Med. 41e0002122018FREE Full text. Medline: 29682310</p>
<p>Meniscal injuries: mechanism and classification. M Wells, J Scanaliato, J Dunn, E Garcia, 10.1097/JSA.0000000000000311Sports Med Arthrosc Rev. Sep. 0132021Medline: 34398118</p>
<p>Return to play following meniscal repair. T J Wiley, N J Lemme, S Marcaccio, S Bokshan, P D Fadale, C Edgar, 10.1016/j.csm.2019.08.002Clin Sports Med. 391Jan 2020Medline: 31767105</p>
<p>Are applied growth factors able to mimic the positive effects of mesenchymal stem cells on the regeneration of meniscus in the avascular zone?. J Zellner, C D Taeger, M Schaffer, J C Roldan, M Loibl, M B Mueller, 10.1155/2014/537686Biomed Res Int. 5376862014. 2014FREE Full text. Medline: 25250325</p>
<p>Younger age increases the risk of sustaining multiple concomitant injuries with an ACL rupture. M Nicholls, T Ingvarsson, K Briem, 10.1007/s00167-021-06538-3Knee Surg Sports Traumatol Arthrosc. 298Aug 2021Medline: 33772603</p>
<p>Biological augmentation of meniscal repair: a systematic review. R E Keller, O' Donnell, E A Medina, Gis Linderman, S E Cheng, Ttw Sabbag, O D , 10.1007/s00167-021-06849-5Knee Surg Sports Traumatol Arthrosc. 306Jun 2022Medline: 35258647</p>
<p>AI vs human --differentiation analysis of scientific content generation. Y Ma, J Liu, F Yi, February 12. 2023FREE Full text</p>
<p>Use of artificial intelligence in scientific paper writing. E J Ciaccio, 10.1016/J.IMU.2023.101253Inform Med Unlocked. 411012532023</p>
<p>Could AI help you to write your next paper? Nature. M Hutson, 10.1038/d41586-022-03479-wNov 2022611</p>
<p>ChatGPT utility in healthcare education, research, and practice: systematic review on the promising perspectives and valid concerns. Healthcare (Basel). M Sallam, 10.3390/healthcare11060887Mar 19. 202311887FREE Full text. Medline: 36981544</p>
<p>Exploring the boundaries of reality: investigating the phenomenon of artificial intelligence hallucination in scientific writing through ChatGPT references. Cureus. S Athaluri, S Manthena, V Kesapragada, V Yarlagadda, T Dave, R Duddumpudi, 10.7759/cureus.37432Apr 202315e37432FREE Full text. Medline: 37182055</p>
<p>AI-enabled image fraud in scientific publications. Patterns (N Y). J Gu, X Wang, C Li, J Zhao, W Fu, G Liang, 10.1016/j.patter.2022.100511Jul 08. 20223100511FREE Full text. Medline: 35845832</p>
<p>Artificial intelligence in fracture detection: a systematic review and meta-analysis. Ryl Kuo, C Harrison, T Curran, B Jones, A Freethy, D Cussons, 10.1148/radiol.211785Radiology. 3041Jul 2022FREE Full text. Medline: 35348381</p>
<p>Transparency guidance for ChatGPT usage in scientific writing. B Aczel, E Wagenmakers, 10.31234/osf.io/b58exPsyArXiv. Preprint posted online. February 06, 2023</p>
<p>The ethics of disclosing the use of artificial intelligence tools in writing scholarly manuscripts. Res Ethics. M Hosseini, D B Resnik, K Holmes, 10.1177/17470161231180449Jun 15. 202319</p>
<p>AI writing tools promise faster manuscripts for researchers. A Tay, 2024-02-08Nature Index. </p>
<p>Reflection on whether Chat GPT should be banned by academia from the perspective of education and teaching. H Yu, 10.3389/fpsyg.2023.1181712Front Psychol. 1411817122023FREE Full text. Medline: 37325766</p>
<p>Differentiate ChatGPT-generated and human-written medical texts. W Liao, Z Liu, H Dai, April 23. 2023FREE Full text</p>
<p>Fighting fire with fire: can ChatGPT detect AI-generated text? arXiv. Preprint posted online. A Bhattacharjee, H Liu, August 17. 2023FREE Full text</p>
<p>ChatGPT: detection in academic journals is editors' and publishers' responsibilities. Teixeira Da Silva, J A , 10.1007/s10439-023-03247-5Ann Biomed Eng. 5110Oct 2023Medline: 37244883</p>            </div>
        </div>

    </div>
</body>
</html>
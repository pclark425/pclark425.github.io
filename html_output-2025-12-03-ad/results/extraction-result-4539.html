<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4539 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4539</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4539</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-1601699d52b474efe80ea9a41e66b3fc9d6ac3ad</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1601699d52b474efe80ea9a41e66b3fc9d6ac3ad" target="_blank">Automated Hypothesis Validation with Agentic Sequential Falsifications</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Compared to human scientists, Popper achieved comparable performance in validating complex biological hypotheses while reducing time by 10 folds, providing a scalable, rigorous solution for hypothesis validation.</p>
                <p><strong>Paper Abstract:</strong> Hypotheses are central to information acquisition, decision-making, and discovery. However, many real-world hypotheses are abstract, high-level statements that are difficult to validate directly. This challenge is further intensified by the rise of hypothesis generation from Large Language Models (LLMs), which are prone to hallucination and produce hypotheses in volumes that make manual validation impractical. Here we propose Popper, an agentic framework for rigorous automated validation of free-form hypotheses. Guided by Karl Popper's principle of falsification, Popper validates a hypothesis using LLM agents that design and execute falsification experiments targeting its measurable implications. A novel sequential testing framework ensures strict Type-I error control while actively gathering evidence from diverse observations, whether drawn from existing data or newly conducted procedures. We demonstrate Popper on six domains including biology, economics, and sociology. Popper delivers robust error control, high power, and scalability. Furthermore, compared to human scientists, Popper achieved comparable performance in validating complex biological hypotheses while reducing time by 10 folds, providing a scalable, rigorous solution for hypothesis validation.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4539.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4539.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>POPPER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>POPPER: Agentic Sequential Falsifications Framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agentic LLM-driven framework that validates free-form natural language hypotheses by iteratively designing and executing falsification experiments and aggregating evidence with sequential e-value based testing to guarantee Type-I error control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>POPPER sequential falsification with e-value aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>POPPER decomposes a high-level hypothesis into measurable falsifiable sub-hypotheses via an Experiment Design Agent, executes each test via an Experiment Execution Agent (which produces valid p-values), maps p-values to e-values using a p-to-e calibrator, and multiplies e-values into a cumulative e-process. If the aggregated e-value exceeds 1/α the null is rejected. The framework enforces assumptions (implication, sequential validity, optional stopping) to ensure valid Type-I error control.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Primary: Type-I error control at pre-specified α (probability of false validation). Secondary: statistical power (ability to detect true alternatives), scalability (number of tests / runtime), relevance of designed falsification tests, robustness to optional stopping, and quality of execution (valid p-values).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-Sonnet-3.5 (default in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General (demonstrated in biology, sociology, economics, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Hypothesis validation for causal/associational claims (mechanistic/causal candidate hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Empirically controlled Type-I error across benchmarks (POPPER Type-I error: DiscoveryBench 0.103±0.020, TargetVal-IL2 0.082±0.046, TargetVal-IFNG 0.085±0.028 at nominal α=0.1). Highest power among methods that control Type-I error (DiscoveryBench power 0.638±0.066, TargetVal-IL2 0.580±0.125, TargetVal-IFNG 0.591±0.069). Matched human experts on Type-I error and power in an expert study while being ~9.7× faster and producing 3.6× more lines of code and 2.5× more statistical tests.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: automated statistical evaluation (Type-I error, power, e-value thresholds) on held-out positives/negatives plus a human expert user study comparing decisions, time, and qualitative overlap of tests.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Theoretical guarantee (Theorem proving aggregated e-process is a valid e-value under stated assumptions) plus empirical validation on two benchmarks (DiscoveryBench and TargetVal) with synthetic negatives (permuted data) to estimate Type-I error and positives from ground-truth sources; and an expert human study with 9 participants.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires assumptions (Implication: sub-hypotheses implied by main null; Sequential validity: p-values valid conditional on prior info; Optional stopping as stopping time). Sensitive to test relevance (necessity of relevance checker), LLM reasoning/coding capability, and correct p-value interpretation. Type-I error control does not by itself guarantee truth of discoveries when many hypotheses are tested (family-wise or FDR concerns).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>DiscoveryBench (multi-domain dataset) and TargetVal (target validation biology benchmark aggregating GTEx, GWAS Catalog, BioGrid etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Hypothesis Validation with Agentic Sequential Falsifications', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4539.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4539.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sequential e-value testing (safe testing)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequential e-value aggregation and safe testing (e-process)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework using e-values (non-negative random variables with expectation ≤1 under the null) that can be multiplied across sequential/adaptive tests to form an e-process with anytime-valid inference and optional stopping guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>E-values: Calibration, combination and applications.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Sequential e-value based hypothesis testing / safe testing</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Each test yields an e-value e_i satisfying E[e_i | history] ≤ 1 under the relevant null. Aggregated evidence E_t = ∏_{s=1}^t e_s forms a non-negative supermartingale (e-process). By Markov and optional stopping, P(E_τ ≥ 1/α) ≤ α for any stopping time τ, so rejecting when E ≥ 1/α controls Type-I error at α even with adaptive testing.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Controls Type-I error under optional stopping/adaptive test selection; supports accumulation of evidence (power) as more valid tests are performed; robustness to dependence among tests so long as conditional e-value validity holds.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General statistical methodology applicable across domains</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Statistical validation framework for hypothesis tests (applies to falsification experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>The paper proves (Theorem 4) that under listed assumptions the product e-process is a valid e-value and yields Type-I error ≤ α. Empirically used in POPPER to aggregate multiple LLM-generated tests while maintaining error control.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated/statistical (theoretical proof plus simulation/empirical benchmark evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Mathematical proof showing E[E_τ] ≤ 1 under H0 via supermartingale/Doob optional stopping theorem; empirical demonstration on benchmarks showing controlled empirical Type-I error.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires each per-iteration e-value to be sequentially valid conditional on past data and decisions; breaking implication assumptions or invalid p-values can invalidate guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied in this paper to DiscoveryBench and TargetVal datasets</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Hypothesis Validation with Agentic Sequential Falsifications', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4539.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4539.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>p-to-e calibrator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>p-to-e calibrator (power-law family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generic transformation mapping p-values to e-values using a parameter κ in (0,1) via e = κ * p^{κ-1}, providing a distribution-free way to obtain e-values from valid p-values.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>E-values: Calibration, combination and applications.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>p-to-e calibration (κ-power transform)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Transforms a conditionally valid p-value p into an e-value e = κ × p^{κ-1} with κ∈(0,1). This mapping guarantees E[e] ≤ 1 under the null when p is valid, enabling combination into an e-process without model-specific likelihoods.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Requires per-test p-values to be valid conditional on past information; trades some optimality for model-free applicability; supports sequential aggregation and optional stopping.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Statistical calibration method for hypothesis testing</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used throughout POPPER to convert p-values from heterogeneous tests (t-tests, permutation tests, chi-squared, etc.) into e-values that aggregate safely; empirical results show this supports Type-I error control in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated statistical transformation validated via theoretical expectation bounds and empirical performance in benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Mathematical derivation (from Vovk & Wang) showing expectation bound under the null; empirical demonstration in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Choice of κ affects sensitivity/power; assumes provided p-values are valid (a primary practical challenge).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Used on DiscoveryBench and TargetVal experiments in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Hypothesis Validation with Agentic Sequential Falsifications', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4539.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4539.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Relevance Checker (ReleCheck)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Relevance Checker for sub-hypothesis implication strength</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based scoring function R(h) ∈ [0,1] that estimates how strongly a proposed null sub-hypothesis h is implied by the main hypothesis H0; used to prune irrelevant tests to preserve implication assumption and Type-I control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Relevance scoring / pruning of falsification proposals</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Design agent proposals are passed to a relevance-checker that scores implication strength; experiments with R(h) below threshold r0 are discarded and a new proposal is requested. This guards against testing sub-hypotheses that are not logically implied by the main null (which could inflate Type-I error).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Relevance (implication strength), novelty (avoid redundancy), implementability (feasibility given metadata).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-Sonnet-3.5 (used as judging LLM in many experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General; applied to biological hypothesis validation in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Method for ensuring logical implication between main hypothesis and sub-tests</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Removing relevance checking (POPPER-NoReleCheck) substantially increased Type-I error in experiments (e.g., TargetVal-IL2 Type-I error rose to 0.340), demonstrating the relevance check's practical necessity. ReleCheck agent ratings correlated with human annotators (Spearman's ρ = 0.55, p = 5×10^-6); agent labeled 85% proposals as 'strongly implied' vs 77% by humans.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: automated LLM scoring validated against calibrated human annotators (high inter-rater agreement Kendall's W post-calibration = 0.91).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Correlation with human annotations and ablation experiments showing effect on Type-I error when removed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Relevance checker slightly overestimates implication strength; needs further calibration and domain-specific expertise to avoid pruning good tests or admitting irrelevant ones.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied to DiscoveryBench and TargetVal falsification proposals</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Hypothesis Validation with Agentic Sequential Falsifications', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4539.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4539.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Type-I error control</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Classical Type-I error rate control (supremum over null family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Primary statistical criterion used: the probability of incorrectly rejecting the null hypothesis under any distribution in the null family is controlled at or below pre-specified α.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On the use and interpretation of certain test criteria for purposes of statistical inference part i.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Type-I error control (supremum over P0)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>POPPER frames validation as controlling sup_{P∈P0} P(ŷ=1) ≤ α, where ŷ indicates 'validated' (reject H0). Aggregation via e-values and stopping rules is designed so that rejection decisions meet this bound even under adaptivity and optional stopping.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Nominal significance level α (e.g., 0.1); empirical Type-I error estimated via negative examples (permutations) and reported with standard deviations across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Statistical criterion for validating hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>POPPER maintained empirical Type-I error near/below nominal α=0.1 across benchmarks (Table 3). Several baseline LLM-driven pipelines failed to control Type-I error (e.g., CodeGen and Fisher Combined Test variations were liberal).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated statistical evaluation using synthetic negatives (column-wise permutations) and measured rejection rates.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical measurement on negative examples and theoretical guarantees provided by e-process framework.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Controls per-hypothesis false positive rate but does not directly control family-wise error or FDR across many hypotheses without additional procedures; dependent on correctness of assumptions (valid p-values and implication).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Measured on DiscoveryBench and TargetVal; negative examples created by random column-wise permutations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Hypothesis Validation with Agentic Sequential Falsifications', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4539.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4539.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Power (sensitivity)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Statistical power (probability of correctly rejecting null under alternative)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Secondary evaluation metric used to quantify the framework's ability to detect true hypotheses (true positives) while maintaining Type-I error control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Empirical power measurement</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Power is estimated as the empirical fraction of positive (ground-truth) hypotheses for which the system rejects the null (ŷ=1), measured on benchmark positives and averaged over runs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Higher power given controlled Type-I error indicates better sensitivity/efficiency of falsification testing strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluation metric for hypothesis testing systems</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>POPPER achieved substantially higher power than other methods that also controlled Type-I error (e.g., DiscoveryBench power 0.638±0.066 for POPPER vs 0.383±0.017 for ReAct). Power increased with larger budgets (more tests).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated measurement on ground-truth positives paired with empirical Type-I error control.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical experiments on benchmark positives with repeated runs to estimate mean and std.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Power is meaningful only when Type-I error is controlled; dependent on quality of designed falsification tests and execution (valid p-values).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Hypothesis Validation with Agentic Sequential Falsifications', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4539.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4539.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human expert comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expert user study comparing POPPER to human computational biologists</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human study recruiting 9 PhD-level computational biologists/bioinformaticians to validate biological hypotheses (TargetVal-IL2), comparing Type-I error, power, speed, and methodological overlap with POPPER.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human expert benchmarking (speed, Type-I error, power, qualitative overlap)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Participants performed hypothesis validation tasks on the same datasets at α=0.1 and documented notebooks; metrics collected included empirical Type-I error and power, task completion time, lines of code produced, number and types of statistical tests, and qualitative categories of proposed falsification experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Empirical Type-I error and power parity; task completion time; breadth/depth of tests; overlap in types of falsification experiments and statistical methods.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biology (genotype-phenotype; IL-2)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Validation of biological causal/regulatory hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>No statistically significant differences in Type-I error and power between POPPER and human experts given the small sample size; POPPER was 9.7× faster, produced 3.6× more lines of code, and ran 2.5× more statistical tests; substantial qualitative overlap in designed falsification experiments and statistical methods (correlations, network interactions, eQTL, permutations, t-tests, chi-squared).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: human-based evaluations (expert tasks) combined with automated metrics (Type-I error, power) computed from results.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Direct head-to-head tasks on TargetVal-IL2 with controlled α and measurement of performance and resource metrics; statistical comparisons reported (no significant differences noted, sample size small).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Small number of human participants limits statistical power of comparisons; humans were free to use internet/LLMs for coding help (but not to query hypothesis), which may affect comparability; POPPER's speed and throughput advantage may reflect automation rather than conceptual superiority.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Hypothesis Validation with Agentic Sequential Falsifications', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4539.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4539.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multiple-testing extensions (FDR/FWER / eBH)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Family-wise and False Discovery Rate control with e-values (including eBH)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Extensions discussed for controlling multiple discovery errors when validating many hypotheses: Bonferroni for FWER, and e-value-based eBH procedure for FDR control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>False discovery rate control with e-values</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>FWER via Bonferroni; FDR via eBH (e-value Benjamini–Hochberg)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>To control FWER across M hypotheses, run POPPER for each at level α/M (Bonferroni). For FDR, use per-hypothesis e-values produced by POPPER and apply eBH or related e-value FDR control procedures to select validated hypotheses with FDR guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Family-wise error avoidance vs expected proportion of false discoveries (trade-off between conservativeness and power).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General multiple-hypothesis testing</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Error-rate control frameworks for multiple validated hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Described as feasible extensions; not empirically evaluated in this work. The paper points out that Type-I control per hypothesis does not imply low overall false discovery burden when validating many hypotheses and suggests eBH as an appropriate method using produced e-values.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Methodological discussion (automated statistical procedures) rather than empirical comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Theoretical literature references (Wang & Ramdas 2022) and conceptual guidance; left for future work to implement/evaluate empirically.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not implemented/evaluated in the present work; practical application requires careful handling of dependencies and selection effects across many automated hypothesis validations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Hypothesis Validation with Agentic Sequential Falsifications', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>E-values: Calibration, combination and applications. <em>(Rating: 2)</em></li>
                <li>Safe testing <em>(Rating: 2)</em></li>
                <li>False discovery rate control with e-values <em>(Rating: 2)</em></li>
                <li>Discoverybench: Towards data-driven discovery with large language models <em>(Rating: 2)</em></li>
                <li>Statistical methods for research workers <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4539",
    "paper_id": "paper-1601699d52b474efe80ea9a41e66b3fc9d6ac3ad",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "POPPER",
            "name_full": "POPPER: Agentic Sequential Falsifications Framework",
            "brief_description": "An agentic LLM-driven framework that validates free-form natural language hypotheses by iteratively designing and executing falsification experiments and aggregating evidence with sequential e-value based testing to guarantee Type-I error control.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "POPPER sequential falsification with e-value aggregation",
            "evaluation_method_description": "POPPER decomposes a high-level hypothesis into measurable falsifiable sub-hypotheses via an Experiment Design Agent, executes each test via an Experiment Execution Agent (which produces valid p-values), maps p-values to e-values using a p-to-e calibrator, and multiplies e-values into a cumulative e-process. If the aggregated e-value exceeds 1/α the null is rejected. The framework enforces assumptions (implication, sequential validity, optional stopping) to ensure valid Type-I error control.",
            "evaluation_criteria": "Primary: Type-I error control at pre-specified α (probability of false validation). Secondary: statistical power (ability to detect true alternatives), scalability (number of tests / runtime), relevance of designed falsification tests, robustness to optional stopping, and quality of execution (valid p-values).",
            "model_name": "Claude-Sonnet-3.5 (default in experiments)",
            "model_size": null,
            "scientific_domain": "General (demonstrated in biology, sociology, economics, etc.)",
            "theory_type": "Hypothesis validation for causal/associational claims (mechanistic/causal candidate hypotheses)",
            "human_comparison": true,
            "evaluation_results": "Empirically controlled Type-I error across benchmarks (POPPER Type-I error: DiscoveryBench 0.103±0.020, TargetVal-IL2 0.082±0.046, TargetVal-IFNG 0.085±0.028 at nominal α=0.1). Highest power among methods that control Type-I error (DiscoveryBench power 0.638±0.066, TargetVal-IL2 0.580±0.125, TargetVal-IFNG 0.591±0.069). Matched human experts on Type-I error and power in an expert study while being ~9.7× faster and producing 3.6× more lines of code and 2.5× more statistical tests.",
            "automated_vs_human_evaluation": "Hybrid: automated statistical evaluation (Type-I error, power, e-value thresholds) on held-out positives/negatives plus a human expert user study comparing decisions, time, and qualitative overlap of tests.",
            "validation_method": "Theoretical guarantee (Theorem proving aggregated e-process is a valid e-value under stated assumptions) plus empirical validation on two benchmarks (DiscoveryBench and TargetVal) with synthetic negatives (permuted data) to estimate Type-I error and positives from ground-truth sources; and an expert human study with 9 participants.",
            "limitations_challenges": "Requires assumptions (Implication: sub-hypotheses implied by main null; Sequential validity: p-values valid conditional on prior info; Optional stopping as stopping time). Sensitive to test relevance (necessity of relevance checker), LLM reasoning/coding capability, and correct p-value interpretation. Type-I error control does not by itself guarantee truth of discoveries when many hypotheses are tested (family-wise or FDR concerns).",
            "benchmark_dataset": "DiscoveryBench (multi-domain dataset) and TargetVal (target validation biology benchmark aggregating GTEx, GWAS Catalog, BioGrid etc.)",
            "uuid": "e4539.0",
            "source_info": {
                "paper_title": "Automated Hypothesis Validation with Agentic Sequential Falsifications",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Sequential e-value testing (safe testing)",
            "name_full": "Sequential e-value aggregation and safe testing (e-process)",
            "brief_description": "A framework using e-values (non-negative random variables with expectation ≤1 under the null) that can be multiplied across sequential/adaptive tests to form an e-process with anytime-valid inference and optional stopping guarantees.",
            "citation_title": "E-values: Calibration, combination and applications.",
            "mention_or_use": "use",
            "evaluation_method_name": "Sequential e-value based hypothesis testing / safe testing",
            "evaluation_method_description": "Each test yields an e-value e_i satisfying E[e_i | history] ≤ 1 under the relevant null. Aggregated evidence E_t = ∏_{s=1}^t e_s forms a non-negative supermartingale (e-process). By Markov and optional stopping, P(E_τ ≥ 1/α) ≤ α for any stopping time τ, so rejecting when E ≥ 1/α controls Type-I error at α even with adaptive testing.",
            "evaluation_criteria": "Controls Type-I error under optional stopping/adaptive test selection; supports accumulation of evidence (power) as more valid tests are performed; robustness to dependence among tests so long as conditional e-value validity holds.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "General statistical methodology applicable across domains",
            "theory_type": "Statistical validation framework for hypothesis tests (applies to falsification experiments)",
            "human_comparison": null,
            "evaluation_results": "The paper proves (Theorem 4) that under listed assumptions the product e-process is a valid e-value and yields Type-I error ≤ α. Empirically used in POPPER to aggregate multiple LLM-generated tests while maintaining error control.",
            "automated_vs_human_evaluation": "Automated/statistical (theoretical proof plus simulation/empirical benchmark evaluation).",
            "validation_method": "Mathematical proof showing E[E_τ] ≤ 1 under H0 via supermartingale/Doob optional stopping theorem; empirical demonstration on benchmarks showing controlled empirical Type-I error.",
            "limitations_challenges": "Requires each per-iteration e-value to be sequentially valid conditional on past data and decisions; breaking implication assumptions or invalid p-values can invalidate guarantees.",
            "benchmark_dataset": "Applied in this paper to DiscoveryBench and TargetVal datasets",
            "uuid": "e4539.1",
            "source_info": {
                "paper_title": "Automated Hypothesis Validation with Agentic Sequential Falsifications",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "p-to-e calibrator",
            "name_full": "p-to-e calibrator (power-law family)",
            "brief_description": "A generic transformation mapping p-values to e-values using a parameter κ in (0,1) via e = κ * p^{κ-1}, providing a distribution-free way to obtain e-values from valid p-values.",
            "citation_title": "E-values: Calibration, combination and applications.",
            "mention_or_use": "use",
            "evaluation_method_name": "p-to-e calibration (κ-power transform)",
            "evaluation_method_description": "Transforms a conditionally valid p-value p into an e-value e = κ × p^{κ-1} with κ∈(0,1). This mapping guarantees E[e] ≤ 1 under the null when p is valid, enabling combination into an e-process without model-specific likelihoods.",
            "evaluation_criteria": "Requires per-test p-values to be valid conditional on past information; trades some optimality for model-free applicability; supports sequential aggregation and optional stopping.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "General",
            "theory_type": "Statistical calibration method for hypothesis testing",
            "human_comparison": null,
            "evaluation_results": "Used throughout POPPER to convert p-values from heterogeneous tests (t-tests, permutation tests, chi-squared, etc.) into e-values that aggregate safely; empirical results show this supports Type-I error control in experiments.",
            "automated_vs_human_evaluation": "Automated statistical transformation validated via theoretical expectation bounds and empirical performance in benchmarks.",
            "validation_method": "Mathematical derivation (from Vovk & Wang) showing expectation bound under the null; empirical demonstration in this paper.",
            "limitations_challenges": "Choice of κ affects sensitivity/power; assumes provided p-values are valid (a primary practical challenge).",
            "benchmark_dataset": "Used on DiscoveryBench and TargetVal experiments in this paper",
            "uuid": "e4539.2",
            "source_info": {
                "paper_title": "Automated Hypothesis Validation with Agentic Sequential Falsifications",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Relevance Checker (ReleCheck)",
            "name_full": "LLM-based Relevance Checker for sub-hypothesis implication strength",
            "brief_description": "An LLM-based scoring function R(h) ∈ [0,1] that estimates how strongly a proposed null sub-hypothesis h is implied by the main hypothesis H0; used to prune irrelevant tests to preserve implication assumption and Type-I control.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Relevance scoring / pruning of falsification proposals",
            "evaluation_method_description": "Design agent proposals are passed to a relevance-checker that scores implication strength; experiments with R(h) below threshold r0 are discarded and a new proposal is requested. This guards against testing sub-hypotheses that are not logically implied by the main null (which could inflate Type-I error).",
            "evaluation_criteria": "Relevance (implication strength), novelty (avoid redundancy), implementability (feasibility given metadata).",
            "model_name": "Claude-Sonnet-3.5 (used as judging LLM in many experiments)",
            "model_size": null,
            "scientific_domain": "General; applied to biological hypothesis validation in experiments",
            "theory_type": "Method for ensuring logical implication between main hypothesis and sub-tests",
            "human_comparison": null,
            "evaluation_results": "Removing relevance checking (POPPER-NoReleCheck) substantially increased Type-I error in experiments (e.g., TargetVal-IL2 Type-I error rose to 0.340), demonstrating the relevance check's practical necessity. ReleCheck agent ratings correlated with human annotators (Spearman's ρ = 0.55, p = 5×10^-6); agent labeled 85% proposals as 'strongly implied' vs 77% by humans.",
            "automated_vs_human_evaluation": "Hybrid: automated LLM scoring validated against calibrated human annotators (high inter-rater agreement Kendall's W post-calibration = 0.91).",
            "validation_method": "Correlation with human annotations and ablation experiments showing effect on Type-I error when removed.",
            "limitations_challenges": "Relevance checker slightly overestimates implication strength; needs further calibration and domain-specific expertise to avoid pruning good tests or admitting irrelevant ones.",
            "benchmark_dataset": "Applied to DiscoveryBench and TargetVal falsification proposals",
            "uuid": "e4539.3",
            "source_info": {
                "paper_title": "Automated Hypothesis Validation with Agentic Sequential Falsifications",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Type-I error control",
            "name_full": "Classical Type-I error rate control (supremum over null family)",
            "brief_description": "Primary statistical criterion used: the probability of incorrectly rejecting the null hypothesis under any distribution in the null family is controlled at or below pre-specified α.",
            "citation_title": "On the use and interpretation of certain test criteria for purposes of statistical inference part i.",
            "mention_or_use": "use",
            "evaluation_method_name": "Type-I error control (supremum over P0)",
            "evaluation_method_description": "POPPER frames validation as controlling sup_{P∈P0} P(ŷ=1) ≤ α, where ŷ indicates 'validated' (reject H0). Aggregation via e-values and stopping rules is designed so that rejection decisions meet this bound even under adaptivity and optional stopping.",
            "evaluation_criteria": "Nominal significance level α (e.g., 0.1); empirical Type-I error estimated via negative examples (permutations) and reported with standard deviations across runs.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "General",
            "theory_type": "Statistical criterion for validating hypotheses",
            "human_comparison": null,
            "evaluation_results": "POPPER maintained empirical Type-I error near/below nominal α=0.1 across benchmarks (Table 3). Several baseline LLM-driven pipelines failed to control Type-I error (e.g., CodeGen and Fisher Combined Test variations were liberal).",
            "automated_vs_human_evaluation": "Automated statistical evaluation using synthetic negatives (column-wise permutations) and measured rejection rates.",
            "validation_method": "Empirical measurement on negative examples and theoretical guarantees provided by e-process framework.",
            "limitations_challenges": "Controls per-hypothesis false positive rate but does not directly control family-wise error or FDR across many hypotheses without additional procedures; dependent on correctness of assumptions (valid p-values and implication).",
            "benchmark_dataset": "Measured on DiscoveryBench and TargetVal; negative examples created by random column-wise permutations.",
            "uuid": "e4539.4",
            "source_info": {
                "paper_title": "Automated Hypothesis Validation with Agentic Sequential Falsifications",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Power (sensitivity)",
            "name_full": "Statistical power (probability of correctly rejecting null under alternative)",
            "brief_description": "Secondary evaluation metric used to quantify the framework's ability to detect true hypotheses (true positives) while maintaining Type-I error control.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Empirical power measurement",
            "evaluation_method_description": "Power is estimated as the empirical fraction of positive (ground-truth) hypotheses for which the system rejects the null (ŷ=1), measured on benchmark positives and averaged over runs.",
            "evaluation_criteria": "Higher power given controlled Type-I error indicates better sensitivity/efficiency of falsification testing strategy.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "General",
            "theory_type": "Evaluation metric for hypothesis testing systems",
            "human_comparison": null,
            "evaluation_results": "POPPER achieved substantially higher power than other methods that also controlled Type-I error (e.g., DiscoveryBench power 0.638±0.066 for POPPER vs 0.383±0.017 for ReAct). Power increased with larger budgets (more tests).",
            "automated_vs_human_evaluation": "Automated measurement on ground-truth positives paired with empirical Type-I error control.",
            "validation_method": "Empirical experiments on benchmark positives with repeated runs to estimate mean and std.",
            "limitations_challenges": "Power is meaningful only when Type-I error is controlled; dependent on quality of designed falsification tests and execution (valid p-values).",
            "uuid": "e4539.5",
            "source_info": {
                "paper_title": "Automated Hypothesis Validation with Agentic Sequential Falsifications",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Human expert comparison",
            "name_full": "Expert user study comparing POPPER to human computational biologists",
            "brief_description": "A human study recruiting 9 PhD-level computational biologists/bioinformaticians to validate biological hypotheses (TargetVal-IL2), comparing Type-I error, power, speed, and methodological overlap with POPPER.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Human expert benchmarking (speed, Type-I error, power, qualitative overlap)",
            "evaluation_method_description": "Participants performed hypothesis validation tasks on the same datasets at α=0.1 and documented notebooks; metrics collected included empirical Type-I error and power, task completion time, lines of code produced, number and types of statistical tests, and qualitative categories of proposed falsification experiments.",
            "evaluation_criteria": "Empirical Type-I error and power parity; task completion time; breadth/depth of tests; overlap in types of falsification experiments and statistical methods.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Biology (genotype-phenotype; IL-2)",
            "theory_type": "Validation of biological causal/regulatory hypotheses",
            "human_comparison": true,
            "evaluation_results": "No statistically significant differences in Type-I error and power between POPPER and human experts given the small sample size; POPPER was 9.7× faster, produced 3.6× more lines of code, and ran 2.5× more statistical tests; substantial qualitative overlap in designed falsification experiments and statistical methods (correlations, network interactions, eQTL, permutations, t-tests, chi-squared).",
            "automated_vs_human_evaluation": "Hybrid: human-based evaluations (expert tasks) combined with automated metrics (Type-I error, power) computed from results.",
            "validation_method": "Direct head-to-head tasks on TargetVal-IL2 with controlled α and measurement of performance and resource metrics; statistical comparisons reported (no significant differences noted, sample size small).",
            "limitations_challenges": "Small number of human participants limits statistical power of comparisons; humans were free to use internet/LLMs for coding help (but not to query hypothesis), which may affect comparability; POPPER's speed and throughput advantage may reflect automation rather than conceptual superiority.",
            "uuid": "e4539.6",
            "source_info": {
                "paper_title": "Automated Hypothesis Validation with Agentic Sequential Falsifications",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Multiple-testing extensions (FDR/FWER / eBH)",
            "name_full": "Family-wise and False Discovery Rate control with e-values (including eBH)",
            "brief_description": "Extensions discussed for controlling multiple discovery errors when validating many hypotheses: Bonferroni for FWER, and e-value-based eBH procedure for FDR control.",
            "citation_title": "False discovery rate control with e-values",
            "mention_or_use": "mention",
            "evaluation_method_name": "FWER via Bonferroni; FDR via eBH (e-value Benjamini–Hochberg)",
            "evaluation_method_description": "To control FWER across M hypotheses, run POPPER for each at level α/M (Bonferroni). For FDR, use per-hypothesis e-values produced by POPPER and apply eBH or related e-value FDR control procedures to select validated hypotheses with FDR guarantees.",
            "evaluation_criteria": "Family-wise error avoidance vs expected proportion of false discoveries (trade-off between conservativeness and power).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "General multiple-hypothesis testing",
            "theory_type": "Error-rate control frameworks for multiple validated hypotheses",
            "human_comparison": null,
            "evaluation_results": "Described as feasible extensions; not empirically evaluated in this work. The paper points out that Type-I control per hypothesis does not imply low overall false discovery burden when validating many hypotheses and suggests eBH as an appropriate method using produced e-values.",
            "automated_vs_human_evaluation": "Methodological discussion (automated statistical procedures) rather than empirical comparison.",
            "validation_method": "Theoretical literature references (Wang & Ramdas 2022) and conceptual guidance; left for future work to implement/evaluate empirically.",
            "limitations_challenges": "Not implemented/evaluated in the present work; practical application requires careful handling of dependencies and selection effects across many automated hypothesis validations.",
            "uuid": "e4539.7",
            "source_info": {
                "paper_title": "Automated Hypothesis Validation with Agentic Sequential Falsifications",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "E-values: Calibration, combination and applications.",
            "rating": 2
        },
        {
            "paper_title": "Safe testing",
            "rating": 2
        },
        {
            "paper_title": "False discovery rate control with e-values",
            "rating": 2
        },
        {
            "paper_title": "Discoverybench: Towards data-driven discovery with large language models",
            "rating": 2
        },
        {
            "paper_title": "Statistical methods for research workers",
            "rating": 1
        }
    ],
    "cost": 0.01752075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Automated Hypothesis Validation with Agentic Sequential Falsifications</h1>
<p>Kexin Huang<em> ${ }^{1}$ Ying Jin ${ }^{</em> 2}$ Ryan Li* ${ }^{1}$ Michael Y. Li ${ }^{1}$ Emmanuel Candès ${ }^{34}$ Jure Leskovec ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Hypotheses are central to information acquisition, decision-making, and discovery. However, many real-world hypotheses are abstract, highlevel statements that are difficult to validate directly. This challenge is further intensified by the rise of hypothesis generation from Large Language Models (LLMs), which are prone to hallucination and produce hypotheses in volumes that make manual validation impractical. Here we propose POPPER, an agentic framework for rigorous automated validation of free-form hypotheses. Guided by Karl Popper's principle of falsification, POPPER validates a hypothesis using LLM agents that design and execute falsification experiments targeting its measurable implications. A novel sequential testing framework ensures strict Type-I error control while actively gathering evidence from diverse observations, whether drawn from existing data or newly conducted procedures. We demonstrate POPPER on six domains including biology, economics, and sociology. POPPER delivers robust error control, high power, and scalability. Furthermore, compared to human scientists, POPPER achieved comparable performance in validating complex biological hypotheses while reducing time by 10 folds, providing a scalable, rigorous solution for hypothesis validation. POPPER is freely available at https://github. com/snap-stanford/POPPER.</p>
<h2>1. Introduction</h2>
<p>A hypothesis is a theory or an explanation based on limited evidence. It forms the backbone of decision-making, information acquisition, and discovery across domains (Thompson \&amp; Skau, 2023). For example, a robot evaluates different</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>hypotheses to decide what action to take next. A scientist decides which experiments to run to evaluate a hypothesis/theory. The marketing strategy decisions are guided by the hypothesized effect on increasing customer retention. Similarly, policymakers may rely on hypotheses about the outcomes of proposed interventions.</p>
<p>Given their profound implications, it is important to validate hypotheses with supporting evidence. This need has grown increasingly urgent with the recent surge in hypotheses generated by Large Language Models (LLMs) (Wang et al., 2024b; Zhou et al., 2024). While these systems exhibit remarkable creativity and diversity, the plausibility of their generated hypotheses can vary significantly due to potential hallucinations (Huang et al., 2023). Moreover, the sheer volume of LLM-generated hypotheses makes it impractical to invest in each one immediately. Therefore, obtaining a reliable, scalable understanding of the quality of these hypotheses is essential to fully unlock their potential.</p>
<p>Having said this, many real-world hypotheses are abstract natural language statements that are difficult to directly evaluate (Thompson \&amp; Skau, 2023; Godfrey-Smith, 2009). For example, while we might hypothesize that "a gene causes a disease," it is infeasible to test this statement directly as it stands. Instead, it must be translated into specific, measurable implications that can be experimented rigorously (Jun et al., 2022). Yet, even for a single hypothesis, the space of potential supportive implications is vast, highlighting the need for frameworks that can automate this evaluation process. Notably, such frameworks must also be statistically rigorous, avoiding false verifications of hypotheses that are not true (Neyman \&amp; Pearson, 1928; 1933; Fisher, 1936). Without such control, research efforts risk being misdirected, resources wasted, and harmful conclusions drawn, ultimately undermining progress and trust. Overall, this raises a critical question: How can we rigorously validate free-form hypotheses at scale?</p>
<p>Present work. We introduce POPPER, a novel framework for rigorous and automated validation of free-form natural language hypotheses using LLM agents. Inspired by Karl Popper's principle of falsification (Popper, 2005), POPPER systematically challenges hypotheses by sequentially testing their measurable implications through diverse experiments, ranging from data analysis and simulations to real-world</p>
<p>experiments and interventions.
To automate this process, POPPER employs two specialized LLM agents with complementary roles. The Experiment Design Agent leverages reasoning capabilities and domain knowledge to identify a measurable implication (sub-hypothesis) of the main hypothesis and design a falsification experiment. Notably, this sub-hypothesis needs to be falsifiable with clear null and alternative definitions. Once designed, the Experiment Execution Agent implements the experiments, which may involve data collection, simulations, statistical analyses, or real-world procedures. This agent ultimately produces a p-value that summarizes the outcome of the falsification experiment.</p>
<p>To maintain statistical rigor, POPPER introduces a novel sequential testing framework that aggregates evidence from multiple, potentially dependent LLM-generated tests while strictly controlling the Type-I error rate (i.e., the probability of incorrectly rejecting a true null hypothesis). Individual p-values are converted into e-values (Vovk \&amp; Wang, 2021), enabling the aggergation of cumulative evidence. By adaptively combining these e-values, POPPER determines whether to reject the hypothesis, conduct further experiments, or terminate the validation process. The framework's ability to make dynamic, statistically sound decisions is ensured by the any-time validity property of the combined e-values (Grünwald et al., 2020). By iteratively testing adaptively solicited implications of a hypothesis, POPPER systematically explores its flexibility while adhering to rigorous statistical principles. This provides a scalable and automated approach to hypothesis validation.</p>
<p>We instantiated POPPER across six diverse domains, including biology, sociology, and economics. In our implementation, POPPER designs falsification experiments by leveraging large-scale, hypothesis-free datasets and executes them with a Python code environment. The process involves systematic data identification, preprocessing, analysis, and statistical evaluation, ultimately generating sequentially valid p-values. Our results demonstrate that POPPER effectively controls the Type-I error rate while achieving significant power improvements over existing methods. Additionally, an expert user study involving nine PhD-level biostatisticians and computational biologists found that POPPER matched human performance in hypothesis validation tasks while reducing validation time by an order of magnitude.</p>
<h2>2. POPPER: a general framework for automated hypothesis validation</h2>
<h3>2.1. Background and Problem Formulation</h3>
<p>Following Majumder et al. (2024); Thompson \&amp; Skau (2023), we broadly define hypothesis $H$ as a statement that defines relationships $(r)$ between a set of variables $(V)$ under contexts $(c)$. For example, in the hypothesis $H$ "Gene VAV1 regulates IL2 production in immune tissue", $\mathcal{V}={$ "VAV1", "IL2 production" $}, r=$ "regulate", and $c=$ "in the immune tissue". To formalize the discussion, the hypothesis $H$ is associated with a null hypothesis $H_{0} . H_{0}$ describes a family $\mathcal{P}<em 0="0">{0}$ of distributions that generate the data under the null, i.e., in uninteresting situations (such as "Gene VAV1 does not regulate IL2 production"). In this way, $H</em>$ and suggest evidence for the alternative.}$ being incorrect is of interest (the alternative hypothesis). Hypothesis validation aims to test the null hypothesis $H_{0</p>
<p>The hypothesis validation task is defined as $f: H \rightarrow{0,1}$, where 0 stands for unvalidated and 1 stands for validated (claiming the alternative). Given a hypothesis $H$, a system or a program $f$ designs and performs experiments and generates an answer in ${0,1}$. We denote $\hat{y}$ as the predicted validation status. An experiment is typically associated with the collection (or retrieval) and processing of datasets denoted as $\mathcal{D}$. An LLM agent $A$ is broadly defined as a program that takes in instructions in natural language and performs actions $\mathcal{T}$ with reasoning capabilities to solve the task given the instruction and outputs a natural language answer (Yao et al., 2023).</p>
<p>For rigorous hypothesis validation, we adopt the classical Type-I error control as our primary criterion. The Type-I error is the probability of the system incorrectly claiming an "interesting" finding (e.g., enriched gene expression) when the null hypothesis is true. Formally, the Type-I error rate is $\sup <em 0="0">{\mathbb{P} \in \mathcal{P}</em>$ is the data distribution. While power-the ability to detect true effects-is important, its improvement is meaningful only when Type-I error control is ensured. Without this foundation, increased power risks invalid conclusions.}} \mathbb{P}(\hat{y}=1)$, where the probability is over the data and the validation system. To ensure rigor, our goal is to control the Type-I error at a pre-defined level $\alpha \in(0,1)$. Another important concept is the power of the validation system, which we define as $\mathbb{P}(\hat{y}=1)$ where $\mathbb{P</p>
<h3>2.2. Overview of POPPER</h3>
<p>POPPER is an agentic framework to systematically validate a hypothesis by actively designing and executing a sequence of falsification experiments. This perspective is inspired by Karl Popper's philosophy of falsification (Popper, 2005): rather than trying to directly prove a hypothesis of interest, one can attempt to refute its logical implications through experiments.</p>
<p>Suppose we want to investigate whether gene $X$ is related to disease $Y$. Directly establishing such a relationship may be difficult; however, we can test one of its implications: if $X$ truly has no relationship to $Y$, we might expect no significant difference in $X$ 's expression levels when comparing cell types implicated in $Y$ versus unrelated cell types.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of POPPER. Given a hypothesis and a pre-defined significance level $\alpha \in(0,1)$, POPPER constructs sequential experiments to falsify the hypothesis. Each iteration proceeds as follows. First, an experiment design agent proposes a falsification experiment, which is refined through a self-critique process considering factors such as causality, data availability, and redundancy. The experiment is then evaluated by an LLM-as-a-judge relevance checker to ensure its alignment with the main hypothesis. If deemed relevant, the test is implemented by a ReAct-based experiment execution agent which obtains a p-value. P-values from multiple falsification experiments are aggregated into sequential e-values using a sequential testing framework. If the aggregated e-value exceeds $1 / \alpha$, we declare sufficient evidence to reject the null hypothesis. Otherwise, the process continues with the next falsification test.</p>
<p>Hence, a potential falsification experiment is to measure expression for $X$, collect the relevant samples, and apply a statistical test (e.g. a $t$-test) for the null hypothesis that there is no difference in mean expression. In this sense, each experimental design leverages a logical implication of the main hypothesis to gather evidence. One can design multiple experiments like this to refute the primary hypothesis.</p>
<p>POPPER implements an iterative, LLM-driven framework for systematic falsification. At each round $i$, an experiment design agent proposes a falsification test for a subhypothesis $h_{i}^{0}$ (e.g., "no difference in expression"), based on the main hypothesis and available resources. An experiment execution agent then carries out the test - either by analyzing existing data, conducting lab measurements, or running simulations - and reports a p-value $p_{i}$. A sequential error control step converts $p_{i}$ into an e-value $e_{i}$ (detailed in Section 2.3), ensuring statistically valid accumulation of evidence. This process repeats over multiple iterations, collecting e-values until either (i) the aggregated evidence surpasses a predefined threshold, leading to a rejection of the null hypothesis $H_{0}$, or (ii) a maximum number of iterations is reached. Each experiment may involve real-world data collection or simulations. The only restriction is that it produces a valid p-value suitable for e-value computation
under the specified null sub-hypothesis. Next, we formalize the theoretical underpinnings of this sequential approach and provide descriptions of the POPPER framework.</p>
<h3>2.3. Validity of Type-I Error Control in POPPER</h3>
<p>This part lays out the general conditions needed for valid Type-I error control in POPPER.
Assumption 1 (Implication). If $H_{0}$ is true, then the null sub-hypothesis $h_{i}^{0}$ is true for all $i \geq 1$.
Assumption 1 requires that the null sub-hypothesis $h_{i}^{0}$ describes a range of data generating processes that are contained in those described by $H_{0}$. As we are to detail in Section 3, we leverage the reasoning capabilities of LLMs, as well as additional checks to overcome the intrinsic randomness in LLM agents to approximately fulfill this condition.</p>
<p>Recall that an e-value $e_{i} \in \mathbb{R}$ is computed based on the collected data in each iteration. Following Vovk \&amp; Wang (2021), an e-value is a non-negative random variable whose expectation is below 1 under the null hypothesis and such that if it were to take a large value, it would indicate strong evidence for refuting the null. E-values are our key instruments for Type-I error control. Compared with the classical concept of p-values, their advantages include (i) flexible</p>
<p>combination of evidence ${ }^{1}$ and (ii) adaptive stopping of the validation process (Grünwald et al., 2020). Let $\mathcal{D}$ be the data, POPPER could potentially interact with (including yet-to-collect ones). To achieve these benefits, in POPPER we require the e-values to be sequentially valid.
Assumption 2 (Sequential information). The training process of the agents is independent of $\mathcal{D}$. Let $\mathcal{D}<em s="s">{i-1}:=$ $\left{D</em>\right}<em i="i">{s \leq i-1}$ be the datasets used by the agents before iteration $i$. The e-values obey $\mathbb{E}\left[e</em>} \mid \mathcal{D<em i="i">{i-1}\right] \leq 1$ under $h</em>$.}^{0</p>
<p>Assumption 2 requires that the e-value at each iteration is valid conditional on prior information. As we shall see in Section 3, POPPER achieves this by carefully controlling the information used at each iteration. In specific, suppose at iteration $i$, the agents determine a sub-hypothesis $h_{i}^{0}$ and a test function $f_{i}(\cdot)$, and then compute $e_{i}=f_{i}\left(D_{i}\right)$ based on a collected dataset $D_{i}$ (e.g. through transforming a p-value). Then, Assumption 2 holds if (1) the selection of $h_{i}^{0}$, and $f_{i}(\cdot)$ only relies on $\mathcal{D}<em i="i">{i-1}$ and metadata without involving the samples in the unused, yet-to-be-chosen datasets, and (2) $\mathbb{E}[f(D)] \leq 1$ for any fixed value of $h$ (resp. $f$ ) that $f</em>$ is actively collected, then (1) is natural as the data must be collected after the design stage.}$ (resp. $h_{i}^{0}$ ) may take and any dataset $D$ whose distribution obeys $h$. If $D_{i}$ is a dataset from a static database, then condition (1) means the decision of using $D_{i}$ does not involve the data in it; if $D_{i</p>
<p>The last assumption concerns the stopping rule of the validation process. It ensures that the aggregated evidence at the terminal iteration supports rigorous validation outputs.
Assumption 3 (Optional stopping). The random variable $\tau \in \mathbb{N}^{+}$denoting the termination iteration is a stopping time with respect to the filtration $\mathcal{F}<em i="i">{i}=\sigma\left(\mathcal{D}</em>$.}\right)$. That is, for every $i$, the event ${\tau=i}$ is measurable with respect to $\mathcal{F}_{i</p>
<p>Assumption 3 holds if the decision to stop or continue at iteration $i$ only depends on $\mathcal{D}<em i="i">{i}$. In POPPER, we determine termination through the aggregated evidence $E</em>$.
These assumptions ensure the aggregated evidence $\left{E_{i}\right}$ is a super-martingale (also called e-process (Shafer, 2019; Grünwald et al., 2020)), and thus the $E_{i}$ at the terminal step can be used to produce the validation output with error control. Theorem 4 is a standard result following (Grünwald et al., 2020), proved in Appendix A. 2 for completeness.
Theorem 4. Define the aggregated evidence at the termination iteration as $E:=\prod_{s=1}^{s} e_{s}$. Under Assumptions 1, 2 and 3, $E$ is a valid e-value, i.e., $\mathbb{E}[E] \leq 1$ under $H_{0}$. In addition, define the validation status as $\hat{y}=\mathbb{1}{E \geq 1 / \alpha}$. Then, $\mathbb{P}(\hat{y}=1) \leq \alpha$ under $H_{0}$, where the probability $\mathbb{P}$ is over the randomness in the agents and the collected data.}:=\prod_{s=1}^{i} e_{s</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>2.4. Agentic hypothesis validation framework</h3>
<p>We now introduce each component of POPPER in a general form. Although the particular implementation we showcase later uses a static database, POPPER can be deployed in any environment capable of producing valid p-values whether that involves laboratory experiments, real-time data collection, or computational simulations. The essence is to iteratively design and execute falsification experiments on sub-hypotheses derived from a main hypothesis $H$. Below, we describe how our agents accomplish this while maintaining the assumptions needed for Type-I error control.</p>
<p>Experiment design agent. Given the main hypothesis $H$ and history of previously tested sub-hypotheses (and their outcomes), the design agent proposes a new falsification experiment intended to refute $H_{0}$. Concretely, it specifies:</p>
<ul>
<li>A sub-hypothesis capturing a concrete implication of the main hypothesis.</li>
<li>The null $h_{i}^{0}$ and alternative $h_{i}^{1}$ to be tested.</li>
<li>Details of how to conduct the experiment in a given domain. This may involve recommending the collection of new laboratory samples, setting up a targeted simulation, or identifying a suitable dataset (if available).
The design agent is assumed to have domain expertise or access to domain knowledge, allowing it to propose experiments that are both relevant for falsifying $H_{0}$ and feasible to implement. For instance, it might propose measuring gene-expression levels, or running a randomized simulation study, or analyzing an existing database - whatever is best to challenge the null sub-hypothesis. Critically, the design agent must ensure that the proposed experiment can, in principle, yield a valid p-value under $h_{i}^{0}$. We will later show how this agent's operations are automated in practice in Section 3.</li>
</ul>
<p>Experiment execution agent. Once an experiment is designed, it is handed off to the execution agent, which is responsible for carrying it out. In a laboratory setting, this agent might interface with robotic lab equipment or prompt human technicians to conduct the specified protocol. In a simulation, it would set up and run the relevant computational model. In a data analytics context, it would query and analyze the dataset. Regardless of the experimental modality, the only restriction is that it outputs a valid pvalue under $h_{i}^{0}$ (Assumption 2). If an experiment fails because the protocol cannot be completed or the data are insufficient - it is simply recorded as a failed attempt, and the procedure moves on. In Section 3, we show how this agent is instantiated using a code-generation framework that automatically executes data queries and statistical analyses.</p>
<p>Sequential aggregation of statistics for error control. After obtaining the new p-value $p_{i}$, we aggregate existing falsification tests to collectively measure evidence for the main hypothesis while maintaining Type-I error control. As de-</p>
<p>Table 1: Experiment design example. Designs for the hypothesis "Gene ZAP70 regulates the production of Interleukin-2".</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Round</th>
<th style="text-align: left;">Falsification experiment description generated from POPPER <br> experiment design agent</th>
<th style="text-align: center;">P-value</th>
<th style="text-align: center;">Cum. e-value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: left;">"Test if ZAP70 has significant physical protein-protein inter- <br> actions with IL-2 pathway components using affinity capture <br> Mass Spectrometry data"</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: left;">"Test if ZAP70 expression levels correlate with IL-2 pathway <br> genes across tissues using GTEx tissue expression data"</td>
<td style="text-align: center;">8.8e-3</td>
<td style="text-align: center;">2.678</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: left;">"Test if genetic variants affecting ZAP70 expression (eQTLs) <br> are also associated with changes in IL-2 pathway activity in <br> immune cells using UKBB eQTL data"</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: left;">"Test if rare missense variants in ZAP70 are significantly <br> associated with immune phenotypes related to IL-2 function <br> using GeneBASS missense variant data"</td>
<td style="text-align: center;">4.7e-04</td>
<td style="text-align: center;">30.78P</td>
</tr>
</tbody>
</table>
<p>Table 2: Experiment execution example. Execution steps for the experiment "Test if variants in the MAK16 locus region show overrepresentation of immune-trait GWAS associations." We provide a summarized pseudo-code here for illustration purposes.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Step</th>
<th style="text-align: left;">Execution steps description from POPPER experiment execution agent</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: left;">Define a helper function to check if a trait is immune-related</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: left;">Find the MAK16 gene in df_gene_info</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: left;">Determine gene region bounds on chromosome (100 kb)</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: left;">Subset df_variant_table for variants in this region</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: left;">Merge with GWAS catalog</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: left;">Filter merged results for (a) p-value 5e-8 (b) immune-related traits using <br> helper function in 1</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: left;">Perform 500 permutations by randomly selecting a chromosome and a <br> matching-length region, gathering variants, merging with the GWAS cata- <br> log, filtering for immune-related traits with p-value 5e-8, and recording <br> the immune-hit count for each permutation.</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: left;">Compute the empirical p-value</td>
</tr>
</tbody>
</table>
<p>scribed in the proposed sequential testing framework in Section 2.3, the main technical tools we use are e-values (Vovk \&amp; Wang, 2021), which are amenable to combination of evidence and adaptive decisions to continue or not (safe testing) (Grünwald et al., 2020). Many e-value constructions (e.g. likelihood ratios) require modeling assumptions, which are unsuitable given the flexibility given to our agent. Thus, we use the general "p-to-e calibrator" (Vovk \&amp; Wang, 2021) to construct</p>
<p>$$
e_{i}=\kappa \times p_{i}^{\kappa-1}, \quad \kappa \in(0,1)
$$</p>
<p>It is straightforward to check that $\mathbb{E}\left[e_{i} \mid \mathcal{D}<em i="i">{i-1}\right] \leq 1$ if each $p</em>}$ is a conditionally valid p -value, i.e., $\mathbb{P}\left(p_{i} \leq t \mid \mathcal{D<em i="i">{i-1}\right) \leq t$ for any $t \in[0,1]$. We then compute the aggregated evidence $E</em>$ is rejected and $H$ is verified (obeying Assumption 3). If not, we proceed to the next iteration until a budget is reached. Theorem 4 ensures the Type-I error control of this procedure.}=\prod_{s=1}^{t} e_{s}$. If $E_{i} \geq 1 / \alpha$, then $H_{0</p>
<h2>3. Instantiation of POPPER</h2>
<p>Thus far, we have described POPPER as a general, agentic framework capable of executing any type of experiment laboratory procedures, simulations, or data analyses - to test sub-hypotheses under a unifying Popperian falsification paradigm. In this section, we focus on our current instantiation, where experiments are drawn from a static corpus of massive hypothesis-free datasets $(\mathcal{D})$ rather than real-world
or real-time data acquisition. We emphasize that this is only one possible deployment of POPPER, chosen here for ease of implementation and reproducibility.</p>
<p>Domains and hypotheses. Our demonstration uses two collections. The first, Target Validation (TargetVal), addresses genotype-phenotype hypotheses in biology; it aggregates 22 tables (totaling $\sim 85$ million records) from sources such as GTEx (Consortium, 2020), GWAS Catalog (MacArthur et al., 2017), and BioGrid (Oughtred et al., 2019). Hypotheses in TargetVal follow the template "Gene A regulates Phenotype B," and we assess them using two subtasks: Interleukin-2 (TargetVal-IL2) and Interferon-gamma (TargetVal-IFNG). Ground-truth hypotheses (treated as "positive" references) were approximated based on genome-wide CRISPR screen data (Schmidt et al., 2022). The second, DiscoveryBench (Majumder et al., 2024), spans six domains (sociology, biology, humanities, economics, engineering, and meta-science), yielding 86 non-null hypotheses (after deduplication) that are grounded in peer-reviewed research. Each hypothesis is paired with a set of relevant dataset. In all cases, POPPER is provided only with the high-level schema (row and column names, any available short text descriptions) of each dataset and the main hypothesis $H$. It must then propose and implement sub-hypothesis falsification experiments by querying and analyzing the raw data.</p>
<p>Instantiation of the experiment design agent. At iteration $i$, the Design Agent $A_{\text {design }}$ receives the main hypothesis $H$, previously proposed falsification subhypotheses $\left{h_{1}, \ldots, h_{i-1}\right}$, their corresponding p-values $\left{p_{1}, \ldots, p_{i-1}\right}$, and the metadata from the database $\mathcal{D}$, and then intelligently designs a new falsification experiment with sub-hypothesis $h_{i}$. To ensure robustness, $A_{\text {design }}$ operates under metadata-only access, meaning it sees only the schema of each table but has no access to raw data or summary statistics, thereby satisfying Assumption 2. In the experiment proposal step, the agent generates a concise rationale, along with a null hypothesis $h_{i}^{0}$ and an alternative hypothesis $h_{i}^{1}$. To enhance quality, we incorporate Self-Refinement (Madaan et al., 2024), employing a chain-of-thought approach that prompts the LLM to iteratively improve its proposal based on three key criteria: novelty (avoiding redundant sub-hypotheses), implementability (ensuring feasibility given metadata), and logical relevance (confirming that $H$ implies $h_{i}$ ). A real-world example is illustrated in Table 1. This demonstrates the agent's ability to systematically design rigorous and biologically meaningful experiments, highlighting its effectiveness in guiding the falsification process. A detailed analysis of the proposed experiments is available at Section 4.2.</p>
<p>Relevance checker. Even with self-refinement, the Design Agent may produce experiments that are tangential to the main hypothesis $H$. To enforce Assumption 1, we</p>
<p>Table 3: Type-I error/power across baselines, variations, ablations, and POPPER. A method is considered to achieve Type I-error control if the pre-defined threshold falls within 1 standard deviation of the method’s result. For methods that fail to meet this criterion, the power metric is grayed out, as it becomes invalid. Mean and standard deviation for all metrics are calculated from 5 independent runs.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Type I Error $(\alpha=0.1)$</th>
<th></th>
<th></th>
<th>Power</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>DiscoveryBench</td>
<td>TargetVal-IL2</td>
<td>TargetVal-IFNG</td>
<td>DiscoveryBench</td>
<td>TargetVal-IL2</td>
<td>TargetVal-IFNG</td>
</tr>
<tr>
<td>CodeGen</td>
<td>$0.145 \pm 0.031$</td>
<td>$0.020 \pm 0.014$</td>
<td>$0.004 \pm 0.009$</td>
<td>$0.378 \pm 0.066$</td>
<td>$0.140 \pm 0.022$</td>
<td>$0.040 \pm 0.042$</td>
</tr>
<tr>
<td>CodeGen (o1)</td>
<td>$0.248 \pm 0.015$</td>
<td>$0.013 \pm 0.012$</td>
<td>$0.000 \pm 0.000$</td>
<td>$0.419 \pm 0.028$</td>
<td>$0.250 \pm 0.100$</td>
<td>$0.183 \pm 0.076$</td>
</tr>
<tr>
<td>ReAct</td>
<td>$0.078 \pm 0.001$</td>
<td>$0.000 \pm 0.000$</td>
<td>$0.000 \pm 0.000$</td>
<td>$0.383 \pm 0.017$</td>
<td>$0.010 \pm 0.022$</td>
<td>$0.020 \pm 0.045$</td>
</tr>
<tr>
<td>Self-Refine</td>
<td>$0.117 \pm 0.028$</td>
<td>$0.100 \pm 0.069$</td>
<td>$0.067 \pm 0.064$</td>
<td>$0.476 \pm 0.066$</td>
<td>$0.183 \pm 0.029$</td>
<td>$0.067 \pm 0.064$</td>
</tr>
<tr>
<td>Fisher Combined Test</td>
<td>$0.311 \pm 0.049$</td>
<td>$0.264 \pm 0.083$</td>
<td>$0.173 \pm 0.023$</td>
<td>$0.741 \pm 0.058$</td>
<td>$0.800 \pm 0.071$</td>
<td>$0.650 \pm 0.050$</td>
</tr>
<tr>
<td>LLM-Likelihood ratio</td>
<td>$0.152 \pm 0.031$</td>
<td>$0.016 \pm 0.014$</td>
<td>$0.180 \pm 0.028$</td>
<td>$0.428 \pm 0.034$</td>
<td>$0.185 \pm 0.074$</td>
<td>$0.357 \pm 0.132$</td>
</tr>
<tr>
<td>POPPER-NoReleCheck</td>
<td>$0.134 \pm 0.021$</td>
<td>$0.340 \pm 0.139$</td>
<td>$0.300 \pm 0.113$</td>
<td>$0.610 \pm 0.042$</td>
<td>$0.897 \pm 0.004$</td>
<td>$0.717 \pm 0.126$</td>
</tr>
<tr>
<td>POPPER-CodeGen</td>
<td>$0.140 \pm 0.022$</td>
<td>$0.105 \pm 0.017$</td>
<td>$0.090 \pm 0.045$</td>
<td>$0.544 \pm 0.032$</td>
<td>$0.526 \pm 0.133$</td>
<td>$0.450 \pm 0.079$</td>
</tr>
<tr>
<td>POPPER (Ours)</td>
<td>$0.103 \pm 0.020$</td>
<td>$0.082 \pm 0.046$</td>
<td>$0.085 \pm 0.028$</td>
<td>$\mathbf{0 . 6 3 8} \mathbf{0} \mathbf{0} \mathbf{0 . 0 6 6}$</td>
<td>$\mathbf{0 . 5 8 0} \mathbf{0} \mathbf{0} \mathbf{0 . 1 2 5}$</td>
<td>$\mathbf{0 . 5 9 1} \mathbf{0} \mathbf{0 . 0 6 9}$</td>
</tr>
</tbody>
</table>
<p>introduce a relevance checker, an LLM-based function $R(h) \in[0,1]$ that estimates how strongly the proposed null sub-hypothesis $h$ is implied by $H_{0}$. If $R(h)&lt;r_{0}$ (a predefined threshold), we discard that experiment and prompt $A_{\text {design }}$ to propose a new one. This pruning mitigates the risk that an irrelevant null might be "falsified," incorrectly supporting the hypothesis (thus inflating the Type-I error).</p>
<p>Instantiation of the experiment execution agent. Once a proposed experiment passes the relevance check, the Execution Agent $A_{\text {exec }}$ carries it out by querying and analyzing the raw data in $\mathcal{D}$ to output a p-value. To give the agent flexibility, we provide a coding environment where it can write and run Python scripts using essential libraries including pandas, statsmodels, and scipy. Concretely, we employ ReAct <em>(Yao et al., 2023)</em> where the agent incrementally executes the experiment via a cycle of actions (executing code), observations (inspecting code output), and reasoning based on the observed output. In practice, $A_{\text {exec }}$ typically inspects and retrieves the dataset, performs preprocessing, fixes errors, runs appropriate statistical tests, fits models, and finally summarizes or visualizes the findings. Without explicit prompting, it selects suitable tests (e.g., $t$-test, chi-squared, Mann-Whitney $U$-test) based on the data distribution. Table 2 shows an example, and Section 4.2 analyzes the execution steps in detail.</p>
<h2>4 Experiments</h2>
<p>We evaluate POPPER in terms of Type-I error control, power improvements, expert user studies, ablations, human annotations, and failure analysis.</p>
<p>Evaluation setup. We assess Type-I error by creating negative examples through random column-wise permutations in each dataset, ensuring the null hypothesis holds. For DiscoveryBench, we generate as many negative examples as positive ones. For the target validation benchmark (with only 20 positives), we create 50 negatives. We measure Type-I error by the proportion of "reject" decisions $(\hat{y}=1)$ on negative examples and Power by the proportion of "reject" decisions on positive examples. We set a nominal Type-I error level $\alpha=0.1$. Unless noted otherwise, we use Claude-Sonnet-3.5 as our LLM, with a maximum of 3 tests on DiscoveryBench and 5 on target validation (due to more complex hypotheses in the latter scenario).</p>
<p>Baselines &amp; variations. We group comparing methods into two categories. (1) Baselines. Since this is a novel application with no direct references, we compare against three general-purpose task resolvers: CodeGen <em>(Ridnik et al., 2024)</em>, which generates code; ReAct <em>(Yao et al., 2023)</em>, which iteratively combines reasoning and coding; and Selfrefine <em>(Madaan et al., 2024)</em>, which refines CodeGen outputs via a critic. None include specialized mechanisms for statistical rigor. We also evaluated an enhanced CodeGen-o1 with improved reasoning. (2) Variations of POPPER. These include Fisher, which uses p-values and Fisher’s combined test <em>(Fisher, 1970)</em> instead of e-values; LLM-Likelihood Ratio, which relies on an LLM to estimate the (optimal) likelihood ratio <em>(Zheng et al., 2023)</em> rather than a p-to-e calibrator; POPPER-NoReleCheck, omitting the relevance checker; and POPPER-CodeGen, which substitutes ReAct with direct code generation for statistical tests.</p>
<h3>4.1 Results</h3>
<p>POPPER achieves Type-I error control. Table 3 reports the Type-I error rates and several key observations are in order. First, most baselines fail to consistently control the Type-I error, while POPPER remains below the nominal level across all datasets. This underscores the necessity of principled statistical design in LLM-driven hypothesis validation; without such rigor, the flexibility of LLM agents can inflate Type-I errors. Second, the comparison against Fisher’s combined test highlights the benefits of e-values in aggregating evidence. Third, the LLM-Likelihood Ratio method lacks calibration, overly conservative for TargetVal-</p>
<p>IL2 and too liberal for DiscoveryBench and TargetVal-IFNG, illustrating the need for strict statistical control rather than relying solely on LLM-based estimations. Finally, removing the relevance checker (PoPPER-NoReleCheck) significantly raises the Type-I error due to irrelevant and misleading tests. Together, these results establish POPPER as a robust framework for agentic hypothesis validation.</p>
<p>PoPPER has significant power improvement. Table 3 shows the power across three benchmarks. First, we exclude any method with an uncontrolled Type-I error (gray-shaded in the table), as their power estimates are invalid. Among methods that do control the Type-I error, PoPPER consistently achieves the highest power: on DiscoveryBench, it delivers $66.5 \%$ greater power than ReAct, and on TargetValIL2, it outperforms Self-Refine by a factor of 3.17. This highlights the strength of PoPPER's iterative testing mechanism, which continually accumulates evidence to improve validation. Second, PoPPER with the ReAct coding agent outperforms PoPPER-CodeGen in power - even with a lower Type-I error. The likely cause is that its reasoning module enables more effective falsification tests. Overall, these results confirm the ability of PoPPER to balance high power with error control, making it a reliable and efficient approach to hypothesis validation.</p>
<p>PoPPER compares with human experts. We recruited nine computational biologists and bioinformaticians (either PhD holders or candidates) to perform hypothesis validation on TargetVal-IL2 (details in Appendix F). Figure 2 shows that the Type-I error and power of PoPPER closely match those of the human participants, with no statistically significant differences given the small sample size. Notably, PoPPER completed tasks 9.7 times faster, generated 3.6 times more lines of code, and performed 2.5 times more statistical tests, underscoring its efficiency gains. Qualitative analysis (the right half of Figure 2, where the numbers represent the amount of distinct statistical tests in each category) revealed substantial overlap between human experts and PoPPER in both biological falsification experiments (e.g., correlation in gene expression levels, network interactions, eQTL tests) and statistical methods (e.g., permutation, $t$-test, chi-squared), reinforcing the soundness of PoPPER in automating validation tasks.</p>
<p>Performance varies across a wide range of LLMs. Since PoPPER must propose meaningful falsification tests and compute valid p-values (per Assumptions 1 and 2), it requires strong reasoning and coding capabilities. We evaluated several LLMs on DiscoveryBench and TargetVal-IL2, including closed-source models (Claude Haiku 3.5, Sonnet 3.5, GPT-4o, o1) and the open-source Llama 3.3 70B. Table 4 shows that higher-capability models are critical: Claude Haiku 3.5 has a high Type-I error, whereas Llama, GPT-4o, Sonnet, and o1 maintained reasonable error control.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Expert human study. PoPPER achieved similar power and Type-I error rates to human experts while significantly reducing task completion time. It also generated more lines of code and conducted more statistical tests. Qualitatively, PoPPER and human experts exhibited substantial overlap in both the designed falsification experiments and the statistical methods employed.</p>
<p>Table 4: Evaluation of various LLM backbones with PoPPER.</p>
<p>| Method | Type I Error $(\alpha=0.1)$ |  | Power |  |
|  | DiscoveryBench | TargetVal-IL2 | DiscoveryBench | TargetVal-IL2 |
| Claude-Haiku-3.5 | $0.230 \pm 0.079$ | $0.780 \pm 0.120$ | $0.844 \pm 0.017$ | $0.835 \pm 0.113$ |
| Llama 3.3 70B | $0.147 \pm 0.036$ | $0.116 \pm 0.020$ | $0.690 \pm 0.027$ | $0.515 \pm 0.078$ |
| GPT-4o | $0.143 \pm 0.039$ | $0.096 \pm 0.045$ | $0.730 \pm 0.054$ | $0.385 \pm 0.102$ |
| Claude-Sonnet-3.5 | $0.103 \pm 0.020$ | $0.082 \pm 0.046$ | $0.638 \pm 0.066$ | $\mathbf{0 . 5 8 0 <em>} \pm 0.125$ |
| o1 | $\mathbf{0 . 0 9 1 </em>} \pm 0.015$ | $\mathbf{0 . 0 3 1 <em>} \pm 0.015$ | $\mathbf{0 . 6 5 4 </em>} \pm 0.019$ | $0.336 \pm 0.121$ |</p>
<p>Among them, o1 performed best on DiscoveryBench, and GPT-4o excelled in power for DiscoveryBench, whereas Sonnet led on TargetVal-IL2. These results emphasize the importance of robust reasoning and coding skills for effective hypothesis validation and highlight nuanced performance trade-offs.</p>
<h3>4.2. Analysis and Discussion</h3>
<p>Qualitative characterization. We characterize the trajectories of PoPPER in Figure 3 (procedure described in Appendix E). In TargetVal, we observe that PoPPER designed experiments that span a broad set of biological tests, including protein-protein interaction networks, expression correlation analyses, eQTL regulatory tests, loss-of-function studies, and genetic perturbation tests. During each iteration, the execution agent typically performs up to 14 distinct steps: dataset inspection, preprocessing, model fitting, error handling, statistical testing, visualization, and summarization. Notably, PoPPER carefully selects statistical methods based on modeling assumptions (e.g., chi-squared, hypergeometric, Fisher's, and permutation tests) and often includes well-chosen negative controls. Interestingly, non-parametric tests are most frequent, making them robust to various data distributions. Visualizing the e-value trajectories reveals that evidence against the null accumulates quickly under the alternative while remaining below the nominal threshold un-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Characterization of POPPER. (1) POPPER designs biologically relevant falsification experiments. (2) It performs multiple logical steps to execute the experiment. (3) It employs a wide range of statistical tests. (4) Progression of cumulative e-values across multiple iterations of falsification tests. More details are available in Appendix E.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Sensitivity analysis. (1) Empirical Type-I error at various nominal levels α. (2) Power and Type-I error at various budgets as a function of the number of maximum tests.</p>
<p>Der the null, underscoring the rigor and power of sequential testing.</p>
<p><strong>Sensitivity analysis.</strong> Figure 4 presents the robustness of POPPER under different settings. First, we varied the significance level α and found that POPPER consistently maintained Type-I error control. Second, we examined the effect of increasing the budget (maximum number of tests). While Type-I error remained well-controlled, the power rose with additional tests, indicating that POPPER can accumulate more diverse evidence when given more computational resources. These results demonstrate the scalability of e-values to both small and large numbers of sequential tests, allowing POPPER to achieve higher discovery rates as resources increase.</p>
<p><strong>Human annotations of falsification test quality.</strong> To assess the implication strength of LLM-generated falsification tests, three authors independently rated 90 randomly selected proposals using the same rubric provided to the ReleCheck agent (Appendix 4). After calibration, the annotators achieved a high inter-rater agreement (Kendall's <em>W</em> = 0.91). The agent's ratings correlated strongly with</p>
<p>human judgments (Spearman's ρ = 0.55, <em>p</em> = 5 × 10<sup>−6</sup>), though it slightly overestimated the relevance of the implications: it labeled 85% of proposals as "strongly implied," compared to a 77% pass rate among human evaluators. These findings indicate that while the ReleCheck agent aligns reasonably well with human perspectives, further calibration and domain-specific expertise are needed to enhance the reliability of falsification test selection.</p>
<p><strong>Error analysis.</strong> We analyzed potential failure modes in POPPER's hypothesis validation workflow. Using an LLM to categorize errors followed by human inspections, we identified the top reasons for failure: misinterpreted p-values (35.9%), ineffective falsification experiment design (28.1%), falsification test breaks implication (17.2%), and incorrect test implementation (8.6%). Hallucination was minimal (0.8%). More details are provided in Appendix D. Overall, while agentic automation holds promise, our findings highlight areas needing further improvement, guiding future work on more robust hypothesis validation pipelines.</p>
<h1>5. Related Work</h1>
<p>We discuss here related works that are closest to POPPER and provide extended discussion on other related works in Appendix B. LLMs have been widely explored for hypothesis generation, with works focusing on domain-specific ideas (Wang et al., 2024a; Baek et al., 2024; Yang et al., 2024b) and comparisons between AI-generated and expert proposals (Si et al., 2024). Beyond idea generation, some studies refine hypotheses (Honovich et al., 2023; Wang et al., 2024c) or ground them in datasets (Majumder et al., 2024), yet few systematically test free-form hypotheses under rigorous statistical controls. While certain works evaluate LLM-driven experimental protocols (Tian et al., 2024; Gu</p>
<p>et al., 2024) or integrate hypothesis and code generation (Li et al., 2024b; Lu et al., 2024; Ifargan et al., 2024; Majumder et al., 2024), they often lack strong error control. Unlike these, POPPER conducts robust statistical validation of both LLM- and human-generated hypotheses through a sequential falsification framework, ensuring reliability. Although Li et al. (2024a) also uses hypothesis testing as a way to challenge language models, POPPER uniquely targets freeform natural language hypotheses and offers rigorous error control.</p>
<h2>6. Conclusion</h2>
<p>We proposed POPPER, an LLM-based framework for validating free-form hypotheses. By integrating a sequential testing paradigm with automated experiment design and execution, POPPER delivers scalable, statistically rigorous hypothesis validation. This work represents an early exploration, and several aspects can be further improved. Refining test relevance and ensuring robust LLM implementations remain challenges. Future work can also extend POPPER to control other error metrics (e.g., false discovery rate), further broadening its utility in scientific discovery and beyond.</p>
<h2>Impact Statement</h2>
<p>This work introduces POPPER, a statistically rigorous agentic framework for hypothesis validation using Large Language Model (LLM) agents. By combining advanced natural language processing capabilities with robust statistical methodologies, POPPER addresses the critical challenge of evaluating and validating hypotheses generated by LLMs, ensuring that only evidence-backed hypotheses guide future research. The broader implications of this work span multiple domains, including biology, economics, and social sciences, where hypothesis generation and validation play a pivotal role in advancing knowledge.</p>
<p>From an ethical perspective, POPPER's emphasis on rigorous statistical validation and Type-I error control mitigates the risks associated with hallucinated or unsupported hypotheses. This ensures that research resources are directed toward meaningful and plausible hypotheses, reducing the potential for wasted efforts and false conclusions that could mislead scientific progress or policy decisions. Additionally, by automating and accelerating the hypothesis validation process, POPPER democratizes access to high-quality scientific methodologies, enabling smaller research teams and resource-limited institutions to conduct advanced analyses.</p>
<h2>Acknowledgement</h2>
<p>We thank Tatsunori Hashimoto and members of the Jure Leskovec lab for discussions and for providing feedback on our manuscript. We thank the expert user study participants:</p>
<p>Michael Bereket, Minta Lu, Peter Pao-Huang, Weixu Wang, Boyang Fu, Hanchen Wang, Hao Xue, Serena Zhang, Yanay Rosen, and Zoe Piran. We also gratefully acknowledge the support of NSF under Nos. OAC-1835598 (CINES), CCF-1918940 (Expeditions), DMS-2327709 (IHBEM), IIS2403318 (III); Stanford Data Applications Initiative, Wu Tsai Neurosciences Institute, Stanford Institute for HumanCentered AI, Chan Zuckerberg Initiative, Amazon, Genentech, GSK, Hitachi, SAP, and UCB. The content is solely the responsibility of the authors and does not necessarily represent the official views of the funding entities.</p>
<h2>References</h2>
<p>Agassi, J. Popper and his popular critics: Thomas kuhn, paul feyerabend and imre lakatos. In SpringerBriefs in Philosophy. Springer, 2014. doi: 10.1007/978-3-319-06587-8.</p>
<p>Ajith, A., Xia, M., Chevalier, A., Goyal, T., Chen, D., and Gao, T. Litsearch: A retrieval benchmark for scientific literature search, 2024. URL https://arxiv.org/ abs/2407.18940.</p>
<p>Alet, F., Lopez-Contreras, J., Koppel, J., Nye, M., SolarLezama, A., Lozano-Perez, T., Kaelbling, L., and Tenenbaum, J. A large-scale benchmark for few-shot program induction and synthesis. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 175-186. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/ v139/alet21a.html.</p>
<p>Baek, J., Jauhar, S. K., Cucerzan, S., and Hwang, S. J. Researchagent: Iterative research idea generation over scientific literature with large language models, 2024. URL https://arxiv.org/abs/2404.07738.</p>
<p>Benjamini, Y. Selective inference: The silent killer of replicability. 2020.</p>
<p>Benjamini, Y. and Hochberg, Y. Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal statistical society: series B (Methodological), 57(1):289-300, 1995.</p>
<p>Brown, M. B. 400: A method for combining nonindependent, one-sided tests of significance. Biometrics, pp. 987-992, 1975.</p>
<p>Collaboration, O. S. Estimating the reproducibility of psychological science. Science, 349(6251):aac4716, 2015.</p>
<p>Consortium, G. The gtex consortium atlas of genetic regulatory effects across human tissues. Science, 369(6509): $1318-1330,2020$.</p>
<p>D’Arcy, M., Hope, T., Birnbaum, L., and Downey, D. Marg: Multi-agent review generation for scientific papers, 2024. URL https://arxiv.org/abs/2401.04259.</p>
<p>Fisher, R. A. Design of experiments. British Medical Journal, 1(3923):554, 1936.</p>
<p>Fisher, R. A. Statistical methods for research workers. In Breakthroughs in statistics: Methodology and distribution, pp. 66-70. Springer, 1970.</p>
<p>Gendron, G., Bao, Q., Witbrock, M., and Dobbie, G. Large language models are not strong abstract reasoners, 2024. URL https://arxiv.org/abs/2305.19555.</p>
<p>Godfrey-Smith, P. Theory and reality: An introduction to the philosophy of science. University of Chicago Press, 2009.</p>
<p>Goodman, N. Fact, Fiction, and Forecast. Harvard University Press, Cambridge, MA, 1983.</p>
<p>Grünwald, P., de Heide, R., and Koolen, W. M. Safe testing. In 2020 Information Theory and Applications Workshop (ITA), pp. 1-54. IEEE, 2020.</p>
<p>Gu, K., Shang, R., Jiang, R., Kuang, K., Lin, R.-J., Lyu, D., Mao, Y., Pan, Y., Wu, T., Yu, J., Zhang, Y., Zhang, T. M., Zhu, L., Merrill, M. A., Heer, J., and Althoff, T. Blade: Benchmarking language model agents for datadriven science, 2024. URL https://arxiv.org/ abs/2408.09667.</p>
<p>Han, S. J., Ransom, K., Perfors, A., and Kemp, C. Inductive reasoning in humans and large language models, 2023. URL https://arxiv.org/abs/2306.06548.</p>
<p>Honovich, O., Shaham, U., Bowman, S. R., and Levy, O. Instruction induction: From few examples to natural language task descriptions. In Rogers, A., BoydGraber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 19351952, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long. 108. URL https://aclanthology.org/2023. acl-long. 108.</p>
<p>Huang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H., Chen, Q., Peng, W., Feng, X., Qin, B., et al. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 2023.</p>
<p>Ifargan, T., Hafner, L., Kern, M., Alcalay, O., and Kishony, R. Autonomous llm-driven research from data to humanverifiable research papers, 2024. URL https:// arxiv.org/abs/2404.17605.</p>
<p>Ioannidis, J. P. Why most published research findings are false. PLoS medicine, 2(8):e124, 2005.</p>
<p>Jun, E., Birchfield, M., De Moura, N., Heer, J., and Just, R. Hypothesis formalization: Empirical findings, software limitations, and design implications. ACM Transactions on Computer-Human Interaction (TOCHI), 29(1):1-28, 2022.</p>
<p>Kuhn, T. S. The Structure of Scientific Revolutions. University of Chicago Press, Chicago, 1st edition, 1962.</p>
<p>Lakatos, I. The Methodology of Scientific Research Programmes. Cambridge University Press, Cambridge, 1978.</p>
<p>Lehr, S. A., Caliskan, A., Liyanage, S., and Banaji, M. R. Chatgpt as research scientist: Probing gpt's capabilities as a research librarian, research ethicist, data generator and data predictor, 2024. URL https://arxiv.org/ abs/2406.14765.</p>
<p>Li, M. Y., Vajipey, V., Goodman, N. D., and Fox, E. B. Critical: Critic automation with language models. arXiv preprint arXiv:2411.06590, 2024a.</p>
<p>Li, R., Patel, T., Wang, Q., and Du, X. Mlr-copilot: Autonomous machine learning research based on large language models agents, 2024b. URL https://arxiv. org/abs/2408.14033.</p>
<p>Liang, W., Zhang, Y., Cao, H., Wang, B., Ding, D., Yang, X., Vodrahalli, K., He, S., Smith, D., Yin, Y., McFarland, D., and Zou, J. Can large language models provide useful feedback on research papers? a large-scale empirical analysis, 2023. URL https://arxiv.org/abs/ 2310.01783.</p>
<p>Lu, C., Lu, C., Lange, R. T., Foerster, J., Clune, J., and Ha, D. The ai scientist: Towards fully automated open-ended scientific discovery, 2024. URL https: //arxiv.org/abs/2408.06292.</p>
<p>MacArthur, J., Bowler, E., Cerezo, M., Gil, L., Hall, P., Hastings, E., Junkins, H., McMahon, A., Milano, A., Morales, J., et al. The new nhgri-ebi catalog of published genome-wide association studies (gwas catalog). Nucleic acids research, 45(D1):D896-D901, 2017.</p>
<p>Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et al. Self-refine: Iterative refinement with self-feedback. NeurIPS, 36, 2024.</p>
<p>Majumder, B. P., Surana, H., Agarwal, D., Mishra, B. D., Meena, A., Prakhar, A., Vora, T., Khot, T., Sabharwal, A., and Clark, P. Discoverybench: Towards data-driven discovery with large language models. arXiv preprint arXiv:2407.01725, 2024.</p>
<p>Manning, B. S., Zhu, K., and Horton, J. J. Automated social science: Language models as scientist and subjects, 2024. URL https://arxiv.org/abs/2404.11794.</p>
<p>Maxwell, N. Popper, kuhn, lakatos and aim-oriented empiricism. arXiv preprint, 2012.</p>
<p>Mirchandani, S., Xia, F., Florence, P., Ichter, B., Driess, D., Arenas, M. G., Rao, K., Sadigh, D., and Zeng, A. Large language models as general pattern machines, 2023. URL https://arxiv.org/abs/2307.04721.</p>
<p>Moskvichev, A., Odouard, V. V., and Mitchell, M. The conceptarc benchmark: Evaluating understanding and generalization in the arc domain, 2023. URL https: //arxiv.org/abs/2305.07141.</p>
<p>Neyman, J. and Pearson, E. S. On the use and interpretation of certain test criteria for purposes of statistical inference part i. Biometrika, 20(1-2):175-240, 1928.</p>
<p>Neyman, J. and Pearson, E. S. The testing of statistical hypotheses in relation to probabilities a priori. In Mathematical proceedings of the Cambridge philosophical society, volume 29, pp. 492-510. Cambridge University Press, 1933.</p>
<p>Oughtred, R., Stark, C., Breitkreutz, B.-J., Rust, J., Boucher, L., Chang, C., Kolas, N., ODonnell, L., Leung, G., McAdam, R., et al. The biogrid interaction database: 2019 update. Nucleic acids research, 47(D1):D529D541, 2019.</p>
<p>Philosophy Institute. Imre lakatos approach: Bridging popper and kuhn in philosophy of science, 2023. URL https://philosophy.institute/ philosophy-of-science-and-cosmology/ imre-lakatos-philosophy-science-bridge/. Accessed: 2025-01-29.</p>
<p>Popper, K. The Logic of Scientific Discovery. Hutchinson, London, 1959.</p>
<p>Popper, K. The logic of scientific discovery. Routledge, 2005.</p>
<p>Press, C. U. Normal science and dogmatism, paradigms and progress: Kuhn versus popper and lakatos. 2009.</p>
<p>Press, O., Hochlehnert, A., Prabhu, A., Udandarao, V., Press, O., and Bethge, M. Citeme: Can language models accurately cite scientific claims?, 2024. URL https://arxiv.org/abs/2407.12861.</p>
<p>Qiu, L., Jiang, L., Lu, X., Sclar, M., Pyatkin, V., Bhagavatula, C., Wang, B., Kim, Y., Choi, Y., Dziri, N., and Ren, X. Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis
refinement, 2024. URL https://arxiv.org/abs/ 2310.08559.</p>
<p>Ridnik, T., Kredo, D., and Friedman, I. Code generation with alphacodium: From prompt engineering to flow engineering. arXiv preprint arXiv:2401.08500, 2024.</p>
<p>Rubin, M. The replication crisis is less of a "crisis" in lakatos' philosophy of science. European Journal for Philosophy of Science, 15(5), 2025. doi: 10.1007/ s13194-024-00629-x.</p>
<p>Schmidt, R., Steinhart, Z., Layeghi, M., Freimer, J. W., Bueno, R., Nguyen, V. Q., Blaeschke, F., Ye, C. J., and Marson, A. Crispr activation and interference screens decode stimulation responses in primary human t cells. Science, 375(6580):eabj4008, 2022.</p>
<p>Shafer, G. The language of betting as a strategy for statistical and scientific communication. arXiv preprint arXiv:1903.06991, 2019.</p>
<p>Si, C., Yang, D., and Hashimoto, T. Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers, 2024. URL https://arxiv. org/abs/2409.04109.</p>
<p>Tang, X., Zheng, Z., Li, J., Meng, F., Zhu, S.-C., Liang, Y., and Zhang, M. Large language models are in-context semantic reasoners rather than symbolic reasoners, 2023. URL https://arxiv.org/abs/2305.14825.</p>
<p>Thompson, W. H. and Skau, S. On the scope of scientific hypotheses. Royal Society Open Science, 10(8):230607, 2023.</p>
<p>Tian, M., Gao, L., Zhang, S. D., Chen, X., Fan, C., Guo, X., Haas, R., Ji, P., Krongchon, K., Li, Y., Liu, S., Luo, D., Ma, Y., Tong, H., Trinh, K., Tian, C., Wang, Z., Wu, B., Xiong, Y., Yin, S., Zhu, M., Lieret, K., Lu, Y., Liu, G., Du, Y., Tao, T., Press, O., Callan, J., Huerta, E., and Peng, H. Scicode: A research coding benchmark curated by scientists, 2024. URL https://arxiv.org/abs/ 2407.13168.
van Fraassen, B. C. The Scientific Image. Clarendon Press, Oxford, 1980.</p>
<p>Vovk, V. and Wang, R. E-values: Calibration, combination and applications. The Annals of Statistics, 49(3):17361754, 2021.</p>
<p>Wang, Q., Downey, D., Ji, H., and Hope, T. Scimon: Scientific inspiration machines optimized for novelty, 2024a. URL https://arxiv.org/abs/2305.14259.</p>
<p>Wang, R. and Ramdas, A. False discovery rate control with e-values. Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(3):822-852, 2022.</p>
<p>Wang, R., Zelikman, E., Poesia, G., Pu, Y., Haber, N., and Goodman, N. D. Hypothesis search: Inductive reasoning with language models. ICLR, 2024b.</p>
<p>Wang, R., Zelikman, E., Poesia, G., Pu, Y., Haber, N., and Goodman, N. D. Hypothesis search: Inductive reasoning with language models, 2024c. URL https://arxiv. org/abs/2309.05660.</p>
<p>Webb, T., Holyoak, K. J., and Lu, H. Emergent analogical reasoning in large language models, 2023. URL https: //arxiv.org/abs/2212.09196.</p>
<p>Xu, F., Lin, Q., Han, J., Zhao, T., Liu, J., and Cambria, E. Are large language models really good logical reasoners? a comprehensive evaluation and beyond, 2024a. URL https://arxiv.org/abs/2306.09841.</p>
<p>Xu, Y., Li, W., Vaezipoor, P., Sanner, S., and Khalil, E. B. Llms and the abstraction and reasoning corpus: Successes, failures, and the importance of object-based representations, 2024b. URL https://arxiv.org/ abs/2305.18354.</p>
<p>Yang, Z., Dong, L., Du, X., Cheng, H., Cambria, E., Liu, X., Gao, J., and Wei, F. Language models as inductive reasoners, 2024a. URL https://arxiv.org/abs/ 2212.10923.</p>
<p>Yang, Z., Du, X., Li, J., Zheng, J., Poria, S., and Cambria, E. Large language models for automated open-domain scientific hypotheses discovery, 2024b. URL https: //arxiv.org/abs/2309.02726.</p>
<p>Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. React: Synergizing reasoning and acting in language models. ICLR, 2023.</p>
<p>Zhang, X., Xie, Y., Huang, J., Ma, J., Pan, Z., Liu, Q., Xiong, Z., Ergen, T., Shim, D., Lee, H., and Mei, Q. Massw: A new dataset and benchmark tasks for ai-assisted scientific workflows, 2024. URL https://arxiv.org/abs/ 2406.06357.</p>
<p>Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36: $46595-46623,2023$.</p>
<p>Zhou, Y., Liu, H., Srivastava, T., Mei, H., and Tan, C. Hypothesis generation with large language models. arXiv preprint arXiv:2404.04326, 2024.</p>
<h1>A. Algorithm and theory</h1>
<h2>A.1. Detailed algorithm for POPPER</h2>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mh">1</span><span class="w"> </span><span class="n">Sequential</span><span class="w"> </span><span class="n">Falsification</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">HypothesisAgent</span>
<span class="w">    </span><span class="nl">Input:</span><span class="w"> </span><span class="n">main</span><span class="w"> </span><span class="n">hypothesis</span><span class="w"> </span><span class="n">\(\mathcal{H}\),</span><span class="w"> </span><span class="n">dataset</span><span class="w"> </span><span class="n">\(\mathcal{D}\)</span>
<span class="w">    </span><span class="n">Initialize</span><span class="w"> </span><span class="n">Experiment</span><span class="w"> </span><span class="n">Design</span><span class="w"> </span><span class="n">Agent</span><span class="w"> </span><span class="n">\(A_{\text</span><span class="w"> </span><span class="p">{</span><span class="n">design</span><span class="w"> </span><span class="p">}}</span><span class="n">\),</span><span class="w"> </span><span class="n">Relevance</span><span class="w"> </span><span class="n">Checker</span><span class="w"> </span><span class="n">\(A_{\text</span><span class="w"> </span><span class="p">{</span><span class="n">rel</span><span class="w"> </span><span class="p">}}</span><span class="n">\),</span><span class="w"> </span><span class="n">Experiment</span><span class="w"> </span><span class="n">Execution</span><span class="w"> </span><span class="n">Agent</span><span class="w"> </span><span class="n">\(A_{\text</span><span class="w"> </span><span class="p">{</span><span class="n">exec</span><span class="w"> </span><span class="p">}}</span><span class="n">\),</span><span class="w"> </span><span class="n">Summarizer</span><span class="w"> </span><span class="n">\(S\),</span>
<span class="w">    </span><span class="n">Coding</span><span class="w"> </span><span class="n">Agent</span><span class="w"> </span><span class="n">Implementation</span><span class="w"> </span><span class="n">\(I\),</span><span class="w"> </span><span class="n">Implication</span><span class="w"> </span><span class="n">Strength</span><span class="w"> </span><span class="n">Threshold</span><span class="w"> </span><span class="n">\(\tau\),</span><span class="w"> </span><span class="n">Alpha</span><span class="w"> </span><span class="n">Threshold</span><span class="w"> </span><span class="n">\(\alpha\),</span><span class="w"> </span><span class="n">Max</span><span class="w"> </span><span class="n">Number</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">Tests</span><span class="w"> </span><span class="n">\(N_{\max</span><span class="w"> </span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">\text</span><span class="w"> </span><span class="p">{</span><span class="n">tests</span><span class="w"> </span><span class="p">}}</span><span class="n">\)</span>
<span class="w">    </span><span class="n">Max</span><span class="w"> </span><span class="n">Retries</span><span class="w"> </span><span class="n">\(N_{\max</span><span class="w"> </span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">\text</span><span class="w"> </span><span class="p">{</span><span class="n">retries</span><span class="w"> </span><span class="p">}}</span><span class="n">\),</span><span class="w"> </span><span class="n">Aggregation</span><span class="w"> </span><span class="n">Method</span><span class="w"> </span><span class="n">\(\mathcal{A}\)</span>
<span class="w">    </span><span class="n">\(\mathcal{F}_{\text</span><span class="w"> </span><span class="p">{</span><span class="n">success</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span><span class="n">\leftarrow</span><span class="w"> </span><span class="n">\emptyset,</span><span class="w"> </span><span class="n">\mathcal{F}_{\text</span><span class="w"> </span><span class="p">{</span><span class="n">failed</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span><span class="n">\leftarrow</span><span class="w"> </span><span class="n">\emptyset,</span><span class="w"> </span><span class="n">\mathcal{O}</span><span class="w"> </span><span class="n">\leftarrow</span><span class="w"> </span><span class="n">\emptyset\)</span>
<span class="w">    </span><span class="n">done</span><span class="w"> </span><span class="n">\(\leftarrow\)</span><span class="w"> </span><span class="n">false</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">\(i=1\)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">\(N_{\max</span><span class="w"> </span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">\text</span><span class="w"> </span><span class="p">{</span><span class="n">tests</span><span class="w"> </span><span class="p">}}</span><span class="n">\)</span><span class="w"> </span><span class="k">do</span>
<span class="w">        </span><span class="n">\(\mathcal{T}</span><span class="w"> </span><span class="n">\leftarrow</span><span class="w"> </span><span class="n">A_</span><span class="p">{</span><span class="n">\text</span><span class="w"> </span><span class="p">{</span><span class="n">design</span><span class="w"> </span><span class="p">}}</span><span class="n">\left(\mathcal{H},</span><span class="w"> </span><span class="n">\mathcal{D},</span><span class="w"> </span><span class="n">\mathcal{F}_{\text</span><span class="w"> </span><span class="p">{</span><span class="n">success</span><span class="w"> </span><span class="p">}},</span><span class="w"> </span><span class="n">\mathcal{F}_{\text</span><span class="w"> </span><span class="p">{</span><span class="n">failed</span><span class="w"> </span><span class="p">}}</span><span class="n">\right)\)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">\(A_{\text</span><span class="w"> </span><span class="p">{</span><span class="n">rel</span><span class="w"> </span><span class="p">}}(</span><span class="n">\mathcal{T})&lt;\tau\)</span><span class="w"> </span><span class="n">then</span>
<span class="w">            </span><span class="n">\(\mathcal{F}_{\text</span><span class="w"> </span><span class="p">{</span><span class="n">failed</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span><span class="n">\leftarrow</span><span class="w"> </span><span class="n">\mathcal{F}_{\text</span><span class="w"> </span><span class="p">{</span><span class="n">failed</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span><span class="n">\cup\{\mathcal{T}\}\)</span>
<span class="w">        </span><span class="k">else</span>
<span class="w">            </span><span class="n">success</span><span class="w"> </span><span class="n">\(\leftarrow\)</span><span class="w"> </span><span class="n">false</span><span class="p">,</span><span class="w"> </span><span class="n">obs</span><span class="w"> </span><span class="n">\(_{i}</span><span class="w"> </span><span class="n">\leftarrow\)</span><span class="w"> </span><span class="n">None</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="n">\(j=1\)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">\(N_{\max</span><span class="w"> </span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">\text</span><span class="w"> </span><span class="p">{</span><span class="n">retries</span><span class="w"> </span><span class="p">}}</span><span class="n">\)</span><span class="w"> </span><span class="k">do</span>
<span class="w">                </span><span class="n">\(\left(\right.\)</span><span class="w"> </span><span class="n">success</span><span class="p">,</span><span class="w"> </span><span class="n">obs</span><span class="w"> </span><span class="n">\(\left._{i}\right)</span><span class="w"> </span><span class="n">\leftarrow</span><span class="w"> </span><span class="n">A_</span><span class="p">{</span><span class="n">\text</span><span class="w"> </span><span class="p">{</span><span class="n">exec</span><span class="w"> </span><span class="p">}}(</span><span class="n">\mathcal{T},</span><span class="w"> </span><span class="n">\mathcal{D},</span><span class="w"> </span><span class="n">I</span><span class="p">)</span><span class="n">\)</span>
<span class="w">                </span><span class="k">if</span><span class="w"> </span><span class="n">success</span><span class="w"> </span><span class="n">then</span>
<span class="w">                    </span><span class="n">exit</span><span class="w"> </span><span class="n">inner</span><span class="w"> </span><span class="n">loop</span>
<span class="w">                </span><span class="k">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">            </span><span class="k">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">success</span><span class="w"> </span><span class="n">then</span>
<span class="w">            </span><span class="n">\(\mathcal{F}_{\text</span><span class="w"> </span><span class="p">{</span><span class="n">failed</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span><span class="n">\leftarrow</span><span class="w"> </span><span class="n">\mathcal{F}_{\text</span><span class="w"> </span><span class="p">{</span><span class="n">failed</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span><span class="n">\cup\{\mathcal{T}\}\)</span>
<span class="w">            </span><span class="k">else</span>
<span class="w">            </span><span class="n">\(\mathcal{F}_{\text</span><span class="w"> </span><span class="p">{</span><span class="n">success</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span><span class="n">\leftarrow</span><span class="w"> </span><span class="n">\mathcal{F}_{\text</span><span class="w"> </span><span class="p">{</span><span class="n">success</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span><span class="n">\cup\{\mathcal{T}\}\)</span>
<span class="w">            </span><span class="n">\(\mathcal{O}</span><span class="w"> </span><span class="n">\leftarrow</span><span class="w"> </span><span class="n">\mathcal{O}</span><span class="w"> </span><span class="n">\cup\left\{o</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="n">s_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="n">\right\}\)</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">\(\mathcal{A}(\mathcal{O})&gt;\frac{1}{\alpha}\)</span><span class="w"> </span><span class="n">then</span>
<span class="w">                </span><span class="n">done</span><span class="w"> </span><span class="n">\(\leftarrow\)</span><span class="w"> </span><span class="n">true</span>
<span class="w">                </span><span class="k">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">            </span><span class="k">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">        </span><span class="k">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">done</span><span class="w"> </span><span class="n">then</span>
<span class="w">            </span><span class="n">exit</span><span class="w"> </span><span class="n">outer</span><span class="w"> </span><span class="n">loop</span>
<span class="w">            </span><span class="k">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">\(S\left(H,</span><span class="w"> </span><span class="n">\mathbf{e}_{1,</span><span class="w"> </span><span class="n">\ldots,</span><span class="w"> </span><span class="n">i</span><span class="p">},</span><span class="w"> </span><span class="n">\alpha,</span><span class="w"> </span><span class="n">\mathcal{F}_{\text</span><span class="w"> </span><span class="p">{</span><span class="n">success</span><span class="w"> </span><span class="p">}},</span><span class="w"> </span><span class="n">\mathcal{F}_{\text</span><span class="w"> </span><span class="p">{</span><span class="n">failed</span><span class="w"> </span><span class="p">}}</span><span class="n">\right)\)</span>
</code></pre></div>

<h2>A.2. Proof of Theorem 4</h2>
<p>Proof of Theorem 4. Throughout, we condition on the training process of the LLM agents. Under Assumptions 1 and 2, each e-value also obeys $\mathbb{E}\left[e_{i} \mid \mathcal{D}<em 0="0">{i-1}\right] \leq 1$ under $H</em>}$ since $H_{0}$ implies $h_{i}^{0}$ for each $i \geq 1$. Define $E_{i}=\prod_{s=1}^{i} e_{s}$ as the aggregated evidence at each iteration $i \geq 1$, and $E_{0}=1$. Also, recall that $\mathcal{F<em i="i">{i}=\sigma\left(\mathcal{D}</em>\right)$ is the filtration in Assumption 3. Then, we have</p>
<p>$$
\mathbb{E}\left[E_{i} \mid \mathcal{F}<em i-1="i-1">{i-1}\right]=E</em>} \cdot \mathbb{E}\left[e_{i} \mid \mathcal{F<em i-1="i-1">{i-1}\right] \leq E</em>
$$</p>
<p>where we use the takeout property and the fact that $E_{i-1}$ is measurable with respect to $\mathcal{F}<em i="i">{i-1}$. In addition, it is clear that $E</em>}$ is measurable with respect to $\mathcal{F<em i="i">{i}$. Therefore, $\left{E</em>\right}<em i="i">{i \geq 1}$ is a non-negative super-martingale adapted to the filteration $\left{\mathcal{F}</em>\right}<em i="i">{i \geq 1}$. Applying Doob's optional stopping theorem, we know that for any stopping time $\tau$ adapted to the filteration $\left{\mathcal{F}</em>\right}<em _tau="\tau">{i \geq 1}$, $E:=E</em>$, thus completing the proof of Theorem 4.}$ obeys $\mathbb{E}[E] \leq E_{0}=1$ under $H_{0}$. Finally, by Markov's inequality, we know that $\mathbb{P}(\hat{y}=1)=\mathbb{P}(E \geq 1 / \alpha) \leq$ $\alpha \cdot \mathbb{E}[E] \leq \alpha$ under $H_{0</p>
<h1>B. Full related works</h1>
<p>Philosophy of science The philosophical foundations of hypothesis validation are rooted in debates about the nature of scientific inquiry. Central to our framework is Karl Popper's falsificationism (Popper, 1959), which argues that scientific hypotheses cannot be definitively proven but can only be refuted through empirical tests. While Popper emphasized iterative falsification, critiques such as those synthesized in Agassi (Agassi, 2014) highlight tensions between his ideas and those of contemporaries like Thomas Kuhn. Kuhn's paradigm shifts (Kuhn, 1962) challenged falsificationism by emphasizing the sociotechnical embeddedness of scientific progress, a perspective further refined by Lakatos' methodology of scientific research programmes(Lakatos, 1978). Lakatos' framework, which evaluates hypotheses within evolving theoretical systems, aligns with our treatment of auxiliary assumptions (e.g., dataset relevance) as prerequisites for testing, as discussed in (Philosophy Institute, 2023). Modern critiques, such as Rubin (Rubin, 2025), argue that Lakatos' approach mitigates challenges like the replication crisis by emphasizing progressive problem shifts over strict falsification. Similarly, van Fraassen's constructive empiricism (van Fraassen, 1980), which prioritizes empirical adequacy over ontological truth, mirrors our focus on observable implications rather than abstract claims. Goodman's "grue" paradox (Goodman, 1983), which interrogates inductive reasoning, underscores the epistemic risks inherent in generalizing from data-risks our framework pragmatically addresses through statistical safeguards like e-values. Maxwell (Maxwell, 2012) positions aim-oriented empiricism as a synthesis of Popperian, Kuhnian, and Lakatosian ideas, advocating for explicit epistemic aims in scientific practice. This resonates with our adaptive sequential testing paradigm, which balances empirical rigor with iterative refinement. While our framework abstracts sociotechnical dimensions noted in Kuhn and Lakatos, the need for transparency in automated systems echoes their emphasis on communal validation (Press, 2009). By integrating these perspectives, POPPER bridges classical philosophy of science and modern data-driven inquiry, offering a scalable yet philosophically grounded approach to hypothesis validation.</p>
<p>LLM for hypothesis generation. Many methods have used LLM to generate novel research ideas. For example, Wang et al. (2024a),Baek et al. (2024), and Yang et al. (2024b) propose methods for generating creative, domain-specific research ideas. Si et al. (2024) conducted large-scale human studies comparing AI-generated research ideas with those from experts. Moving beyond ideas, many also explore hypothesis generation with LLMs with a focus in the commonsense domains (Gendron et al., 2024; Yang et al., 2024a; Moskvichev et al., 2023; Mirchandani et al., 2023; Tang et al., 2023; Xu et al., 2024a; Han et al., 2023; Xu et al., 2024b; Alet et al., 2021; Webb et al., 2023). Notably, Honovich et al. (2023) explores LLMs' capabilities in inducing rules from example demonstrations. Qiu et al. (2024) and Wang et al. (2024c) further extends this idea to generating and iteratively refining candidate hypotheses from a set of examples or observations. (Majumder et al., 2024) grounds hypothesis generation with a given dataset and a question. However, these works focus on hypothesis generation rather than rigorous validation. POPPER is complementary to this line of research as it takes in a hypothesis (generated from either LLM or human) and develops a systematic, data-driven process for evaluating whether a hypothesis withstands statistical scrutiny.</p>
<p>LLM for hypothesis testing and experiments. To the best of our knowledge, there is no work that investigates rigorous validation of a free-form hypothesis grounded with data using AI agent. Some studies have tested LLMs' abilities to implement experiments as a form of validation. For example, Tian et al. (2024) and Gu et al. (2024) evaluate LLMs' coding capabilities in executing experimental protocols. While these works focus narrowly on code generation, POPPER presents a framework for validating natural language-based free-form hypothesis. Additionally, prior research into automated scientific discovery has explored combining hypothesis and code generation for end-to-end workflows (Li et al., 2024b; Lu et al., 2024; Ifargan et al., 2024; Majumder et al., 2024). While these studies focus on automation, they often lack rigorous statistical grounding. In contrast, POPPER focuses on the hypothesis testing component and incorporates robust Type-I error control, ensuring the reliability and scientific rigor of its results. (Li et al., 2024a) (CriticAL) used LLMs to identify and evaluate discrepancies between model predictions and data through hypothesis testing. While CriticAL focuses on validating statistical predefined models, POPPER tackles the challenge of validating free-form natural language hypotheses with a sequential falsification framework.</p>
<p>LLM for automating research. LLMs have also been used for several other research-related tasks, including automated review generation (D'Arcy et al., 2024; Liang et al., 2023), related work curation (Ajith et al., 2024; Press et al., 2024), experiment outcome predictions (Manning et al., 2024; Zhang et al., 2024; Lehr et al., 2024), and future work recommendations (Zhang et al., 2024). While these are interesting applications, our work focuses on hypothesis testing.</p>
<h1>C. Limitations</h1>
<p>Type-I error v.s. false discoveries. We view hypothesis validation with POPPER as an initial step towards rigorous automatic scientific discovery. One limitation of our current framework arises from the limitation of Type-I error as an error criterion for scientific discovery. Let us denote rejecting the null as a "discovery", and it is a true discovery if the alternative holds. The shortcoming of Type-I error control is that it does not necessarily imply the discoveries are true (which is more pronounced when POPPER is used to validate many hypotheses). Awareness of this issue emerged much later than the appearance of concept of Type-I error, but has been quite important nowadays in the fields of hypothesis testing, selective inference, and replicability (Ioannidis, 2005; Collaboration, 2015; Benjamini, 2020). To see this point, consider an extreme case where all hypotheses being passed on to POPPER are null ones. Then, the Type-I error control only implies that we reject each hypothesis with no greater than a chance of $\alpha$, but every discovery, once made, must be false. Therefore, we stress that one should be cautious in interprating the validated hypotheses by POPPER as true discoveries to act upon.</p>
<p>In the following, we discuss possible extended uses of POPPER for more advanced error critria on false discoveries. Consider using POPPER to validate $M \geq 1$ (abstract) hypotheses. The family-wise-error-rate (FWER) is the probability of making any false discovery; FWER control at level $\alpha \in(0,1)$ can be achieved by Bonferroni's correction, i.e., running POPPER for each hypothesis at level $\alpha / M$ and gather all rejected null hypothesis. A more liberal criterion is the false discovery rate (FDR) (Benjamini \&amp; Hochberg, 1995), which is the average fraction of false discoveries among all discoveries. The FDR is suitable for measuring the wastage of follow-up resources on validated (rejected) hypotheses. Since our framework produces a valid e-value $E_{\tau}$ for each hypothesis, these e-values can be readily used to derive a set of validated hypotheses with FDR control by employing the eBH procedure (Wang \&amp; Ramdas, 2022). However, these use cases are beyond the scope of this work, and we leave the evaluation and further developments of such capabilities of POPPER for future investigation.</p>
<h2>D. Error analysis</h2>
<p>In this section, we provide insights into the common failure modes of POPPER. We first manually inspected 20 randomly sampled failed experiment logs produced by POPPER, and created a list of 10 possible failure categories based on the model's behaviors. Table 5 provides detailed definitions of the 10 failure categories. Then, we collected a total of 128 failed experiment logs from benchmark runs across TargetVal-IFNG, TargetVal-IL2, and DiscoveryBench. We then query a reasoning LLM (OpenAI O1) with the failed trajectory logs, the agent's incorrect conclusion, and the ground truth conclusion to automatically categorize each failed experiment into one or more failure modes described in Table 5. We manually checked 30 labeled experiment logs for quality assurance. $93.3 \%$ of O1's labels aligned with human judgment. According to Figure 5, 35.9\% of the failures accompany the agent misinterpreting the context for p-values. $28.1 \%$ and $17.2 \%$ of the errors occur when the agent fails to find effective falsification tests or uses tests that breaks implication. 8.6\% and $7.0 \%$ of the errors are caused by incorrect test implementation and failure to locate relevant data. It is worth noting that we only observed 1 instance of hallucination across 128 failure cases, and no signs of p-hacking were observed.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Failure mode distribution for POPPER, labaled automatically by O1 and manually checked by humans.</p>
<p>Table 5: Definitions of failure mode categories</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Failure Type</th>
<th style="text-align: left;">Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Falsification Test Breaks Implication</td>
<td style="text-align: left;">The agent selects falsification tests that are not logically implied by the main <br> hypothesis. This occurs when the falsification sub-hypothesis could be true even if <br> the main hypothesis is false, leading to irrelevant p-values and misleading results.</td>
</tr>
<tr>
<td style="text-align: left;">Ineffective Test Selection</td>
<td style="text-align: left;">The agent fails to identify or design falsification tests that are capable of effectively <br> addressing the main hypothesis, resulting in weak or inconclusive evidence.</td>
</tr>
<tr>
<td style="text-align: left;">Malformed Falsification Test</td>
<td style="text-align: left;">The design of the falsification test is flawed. For example, the test assesses an <br> alternative sub-hypothesis that contradict the main hypothesis, or lack a clear <br> framework for accepting or rejecting the null sub-hypothesis.</td>
</tr>
<tr>
<td style="text-align: left;">Incorrect Test Implementation</td>
<td style="text-align: left;">The agent incorrectly implements the falsification test. While the test appears to <br> execute successfully, it contains undetected bugs or methodological errors that result <br> in invalid or misleading p-values and conclusions.</td>
</tr>
<tr>
<td style="text-align: left;">P-Hacking</td>
<td style="text-align: left;">The agent manipulate data analysis, experimental procedures, or selectively report <br> results to artificially achieve statistically significant p-values, leading to misleading <br> conclusion.</td>
</tr>
<tr>
<td style="text-align: left;">Misinterpreted P-Value</td>
<td style="text-align: left;">The agent misinterprets or overlooks important context when analyzing p-values. <br> This includes failing to recognize invalid p-values, ignoring assumptions of the <br> statistical test, or drawing incorrect conclusions from the results.</td>
</tr>
<tr>
<td style="text-align: left;">Hallucination</td>
<td style="text-align: left;">The agent generates data entries, data interpretations, assumptions, observations, <br> p-values, or conclusions that are fabricated or not grounded in the provided data or <br> context.</td>
</tr>
<tr>
<td style="text-align: left;">Failed to Recover from Test Errors</td>
<td style="text-align: left;">The agent encounters errors during test execution and fails to recover or adapt. <br> This may result in the agent repeating the same errors or becoming stuck in an <br> unproductive loop of failed tests.</td>
</tr>
<tr>
<td style="text-align: left;">Failed to Locate Relevant Data</td>
<td style="text-align: left;">The agent is unable to identify, retrieve, or preprocess the necessary data required <br> for conducting critical falsification tests, preventing effective hypothesis evaluation.</td>
</tr>
<tr>
<td style="text-align: left;">Other</td>
<td style="text-align: left;">There was some other problem that prevented the agent from arriving at the correct <br> conclusion.</td>
</tr>
</tbody>
</table>
<h1>E. Tests and trajectory analysis</h1>
<p>In this section, we detail how we categorized the statistical and domain-specific tests performed by POPPER during falsification experiments, as well as how we summarized the agent's trajectories for executing each falsification test, as visualized in Figure 3.</p>
<p>We parsed and sampled 1500 falsification test designs and their execution logs, and then asked GPT-4o to identify and group the statistical tests performed in the falsification experiments.</p>
<p>We limit our analysis of domain-specific tests to biological hypotheses only, as we have an abundance of biological hypotheses from TargetVal benchmark. The other five domains provided by DiscoveryBench contains limited number of unique hypotheses per domain, and the analysis does not converge. We sampled 462 falsification tests proposed by the experiment design agent and used GPT-4o to extract and group them into standardized biological tests.</p>
<p>For agent trajectories, we first manually inspected the behaviors of the experiment execution agent over 20 experiments and summarized a list of 11 possible high-level actions taken by the agent. Detailed definitions of these actions are listed in Table 6. We then randomly sampled 80 trajectories of the experiment execution agent, and prompted GPT-4o to convert each trajectory into a list of high-level actions as detailed in Table 6. We observe that the agent's workflow closely mirrors that of a human data analyst. It begins by inspecting the dataset and assembling relevant information, then proceeds with a cycle of</p>
<p>test implementation, execution, and iterative error resolution. Upon observing the test results, the agent may optionally check validity criteria (e.g., model assumptions and sample sizes) and refine its approach if necessary. Finally, the agent compiles all findings into a summary to draw a final conclusion.</p>
<p>Table 6: Names and definitions of actions taken by the experiment execution agent.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Action Name</th>
<th style="text-align: left;">Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Inspect Dataset</td>
<td style="text-align: left;">Actions where the agent checks or explores the structure/content of the dataset (e.g., looking at <br> dimensions, columns, and sample rows).</td>
</tr>
<tr>
<td style="text-align: left;">Visualize Data</td>
<td style="text-align: left;">Actions where the agent creates visualizations to explore the distribution and relationships within <br> the data.</td>
</tr>
<tr>
<td style="text-align: left;">Retrieve Data</td>
<td style="text-align: left;">Actions where the agent extracts specific portions of the dataset relevant to the current hypothesis <br> or analysis.</td>
</tr>
<tr>
<td style="text-align: left;">Prepare Data</td>
<td style="text-align: left;">Actions where the agent cleans, transforms, and structures data (e.g., grouping, calculating <br> summary statistics, handling missing values) before applying tests or models.</td>
</tr>
<tr>
<td style="text-align: left;">Fit Model</td>
<td style="text-align: left;">Actions where the agent employs a statistical or machine-learning model to test or explore <br> relationships in the data.</td>
</tr>
<tr>
<td style="text-align: left;">Implement Test</td>
<td style="text-align: left;">Actions where the agent applies a formal statistical test (e.g., correlation test, t-test, ANOVA) or <br> other relevant procedure to evaluate a hypothesis.</td>
</tr>
<tr>
<td style="text-align: left;">Fix Errors</td>
<td style="text-align: left;">Actions where the agent identifies and corrects issues or bugs in the testing procedure (e.g., <br> coding errors, incorrect data handling, syntax problems).</td>
</tr>
<tr>
<td style="text-align: left;">Inspect Test</td>
<td style="text-align: left;">Actions where the agent verifies the results of a test-checking the shape of data arrays, the number <br> of observations, and ensuring that the calculations (e.g., p-values, effect sizes) are valid.</td>
</tr>
<tr>
<td style="text-align: left;">Analyze Results</td>
<td style="text-align: left;">TActions where the agent interprets the output of a test or model (e.g., evaluating coefficients, <br> p-values, confidence intervals) to determine whether the data supports or refutes the hypothesis.</td>
</tr>
<tr>
<td style="text-align: left;">Summarize Conclusion</td>
<td style="text-align: left;">Actions where the agent provides a final statement or verdict about the hypothesis.</td>
</tr>
<tr>
<td style="text-align: left;">Other</td>
<td style="text-align: left;">Any agent actions that are not covered by the ones above.</td>
</tr>
</tbody>
</table>
<h1>F. Human study details</h1>
<p>We recruited 11 computational biologists and bioinformaticians (PhD holders or candidates) for our human study, and 9 adhered. Each participant was asked to complete a short questionnaire on their educational background and relevant experience (Listing 1). We present the background distributions of recruited participants in Figure 6. Of the 9 participants, 6 hold (or are pursuing) a PhD, 1 holds a Master's degree, and 2 are postdoctoral researchers. In terms of experience with data analysis and coding for genetic and genomic data, 2 participants identified as beginners, 1 as intermediate, and 6 as experts. Regarding familiarity with statistical hypothesis testing, 2 participants identified as beginners, 2 as intermediate, and 5 as experts. Finally, 6 participants reported that they have never performed wet-lab experiments, while 3 indicated having done so.</p>
<p>We sampled a total of 18 tasks from the TargetVal-IL2 benchmark to evaluate the Type-I error (9 tasks) and statistical power ( 9 tasks) of our method. Each participant was randomly assigned two tasks to complete. To prevent inference of one hypothesis from the other, a participant might receive two positive, two null, or one positive and one null hypothesis. Participants were free to use the internet or large language models for general coding questions (e.g., library usage, syntax) and statistical tests, but not to query the specific biological hypothesis directly. All conclusions were to be derived solely from the data provided in the TargetVal-IL2 benchmark, with each hypothesis tested at significance level $\alpha=0.1$. All work was documented in Jupyter Notebooks.</p>
<div class="codehilite"><pre><span></span><code><span class="gu">##</span> A quick questionnaire about you
What is your highest level of education? (e.g. PhD in progress, PhD, Master’s degree,
    Bachelor’s degree, etc.)
**Your answer:**
What is your major of study? (e.g. biostatistics, computer science, etc.)
**Your answer:**
What is your research interest?
**Your answer:**
What is your experience with data analysis/writing code on genetic &amp; genomic data? (choose
    from beginner, intermediate, expert)
**Your answer:**
What is your experience with statistical hypothesis testing? (choose from beginner,
    intermediate, expert)
**Your answer:**
Have you ever performed wet-lab experiments in a biology lab? (yes, no)
**Your answer:**
</code></pre></div>

<p>Listing 1: Background questionnaire used for human study recruitment</p>
<h1>G. Human Annotation Details</h1>
<p>We randomly sampled 90 falsification test proposals from the three benchmarks. Each of the three annotators first individually annotated a common set of 20 proposals using the same 0.1-1.0 rubric as the Relevance Checker 4. The annotators then discuss and calibrate their decisions and independently annotate 10 more proposals after the calibration. The annotators achieved a Kendall's $W$ of 0.62 before the calibration, and 0.91 post calibration. Finally, each annotator individually annotate a separate set of 20 falsification proposals. The human annotators and the relevance checker agent achieved a Kendall's Tau of $0.43(p=1 e-06)$ and Spearman's correlation of $0.55(p=5 e-6)$. The relevance checker agent ranked $84 \%$ of the proposed falsification tests as "Strongly Relevant" (score $&gt;=0.8$ ), whereas human annotators ranked $77 \%$ of the test proposals as "Strongly Relevant".</p>
<h2>H. Qualitative Analysis</h2>
<p>This section provides qualitative analysis on one successful falsification trajectory and one failure case trajectory on the TargetVal-IL2 benchmark.</p>
<p>Figure 10 presents an example trajectory of POPPER running on a TargetVal-IL2 hypothesis. We can see the agent attempted multiple rounds of diverse falsification experiments, including expression correlation analysis, LCP2 regulatory network analysis, LCP2 variant-immune phenotype association test, and LCP2 eQTL-IL2 regulatory region test. POPPER performs sequential error control to rigorously aggregate the evidence from all four experiments, and then rejects the main null hypothesis as the summarized sequential statistics (i.e., cumulated e-values) passes our alpha-threshold of 0.1 .</p>
<p>We observe that the experiment design agent autonomously refines its proposal to enhance the implication strength and feasibility of the proposed falsification experiment. The experiment execution agent iteratively inspects and interacts with multiple data sources to evaluate the feasibility of the experiment, before implementing and conducting the statistical tests. The experiment execution agent also shows attempts to account for model assumptions and inspect the validity of test statistics before arriving at a final conclusion (e.g., Round 3). We note that with rigorous Type I error control, POPPER also provides more tolerance and leniency for test execution failures. Notice that in Round 1, the experiment execution agent incorrectly concluded that LCP2 and IL2 are not present in the datasets. However, benefiting from the sequential</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Backgrounds of human study participants.
falsification frameowkr, POPPER is eventually able to reach the correct conclusion after multiple experiment trials.
Figure 11 shows an example false positive trajectory on TargetVal-IL2. The critical error lies in Round 3, where the experiment design agent proposes to test whether "genetic variants near RAB39A are significant QTLs for IL-2 related immune phenotypes," but the experiment execution agent only looked at eQTLs for RAB39A expression in neutrophils, a cell type that may or may not produce IL-2. The agent then converts the eQTL score to a highly significant p-value for "RAB39A expression in neutrophils", but it does not imply that RAB39A regulates IL-2. Hence, while the proposed falsification experiment is valid, the implementation of the test violates the implication assumption. We categorize this failure case as "Incorrect Test Implementation", and "Misinterpreted P-Value". Overall, we found that understanding and reasoning about the context and validity of effect sizes and p-values remains to be a main challenge for POPPER.</p>
<h1>I. Prompting Details</h1>
<p>Listings 2, 3, 4, 5, and 6 detail the prompts used for different modules of POPPER.</p>
<p>Once you open the notebook, run the following cell to start the time clock</p>
<div class="codehilite"><pre><span></span><code><span class="n">In</span> <span class="p">[</span><span class="mi">3</span><span class="p">]:</span> <span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</code></pre></div>

<h1>Instructions</h1>
<p>Given a biology hypothesis "Gene MAK16 regulates the production of Interleukin-2 (IL-2).", your task is to validate it using the given raw databases by performing relevant data analysis, formulating statistical tests, and implementing them. The validation should be purely datadriven, not literature-driven. For statistical test, use significance level of alpha $=0.1$.</p>
<p>Output (1) If the hypothesis is valid or not given the data (2) relevant statistics (e.g. p-value, etc)</p>
<h2>IMPORTANT</h2>
<ul>
<li>You must only use the database folder in the current task folder to perform the analysis. DO NOT use the data from the other task or any external data.</li>
<li>You must NOT use LLMs or internet about the direct answer to the biological hypothesis.</li>
<li>You can use internet/LLMs if you are not sure about the code syntax or library usage or statistical tests or have biological questions in general.</li>
<li>You can use any python library to perform the analysis.</li>
<li>The tasks are randomly sampled and may be one true \&amp; one false / all true / all false</li>
</ul>
<p>Here are the list of available data sources with columns and example rows:</p>
<div class="codehilite"><pre><span></span><code><span class="nx">df_gtex_tissue_gene_tpm</span><span class="p">:</span>
<span class="p">{</span><span class="err">&#39;</span><span class="nx">Description</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">ENSG00000186092</span><span class="err">&#39;</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">Tissue</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">Adipose</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">Subcutaneous</span><span class="err">&#39;</span><span class="p">,</span>
<span class="err">&#39;</span><span class="nx">Expression</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="m m-Double">0.0453961</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">Gene</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">OR4F5</span><span class="err">&#39;</span><span class="p">}</span>
<span class="nx">df_gene_info</span><span class="p">:</span>
<span class="p">{</span><span class="err">&#39;</span><span class="nx">gene_id</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">ENSG00000228037</span><span class="err">&#39;</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">transcript_id</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">ENST00000424215</span><span class="err">&#39;</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">chr</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="sc">&#39;1&#39;</span><span class="p">,</span>
<span class="err">&#39;</span><span class="nx">gene_start</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="mi">2581560</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">gene_end</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="mi">2584533</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">strand</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">transcript_start</span><span class="err">&#39;</span><span class="p">:</span>
<span class="mi">2581560</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">transcript_end</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="mi">2584533</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">tss</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="mi">2581560</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">transcript_is_canonical</span><span class="err">&#39;</span><span class="p">:</span>
<span class="m m-Double">1.0</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">gene_name</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="nx">nan</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">percentage_gene_gc_content</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="m m-Double">51.11</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">gene_type</span><span class="err">&#39;</span><span class="p">:</span>
<span class="err">&#39;</span><span class="nx">lncRNA</span><span class="err">&#39;</span><span class="p">}</span>
<span class="nx">df_genetic_interaction</span><span class="p">:</span>
<span class="p">{</span><span class="err">&#39;</span><span class="nx">interaction_id</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="mi">206363</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">gene_a_id</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">YCR011C</span><span class="err">&#39;</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">gene_b_id</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">YCL025C</span><span class="err">&#39;</span><span class="p">,</span>
<span class="err">&#39;</span><span class="nx">experimental_system_type</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">genetic</span><span class="err">&#39;</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">pubmed_id</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">PUBMED</span><span class="p">:</span><span class="mi">16269340</span><span class="err">&#39;</span><span class="p">,</span>
<span class="err">&#39;</span><span class="nx">organism_id_a</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="mi">559292</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">organism_id_b</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="mi">559292</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">throughput_type</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">High</span>
<span class="nx">Throughput</span><span class="err">&#39;</span><span class="p">,</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">experimental_score</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="err">&#39;</span><span class="o">~</span><span class="m m-Double">5.6431</span><span class="err">&#39;</span><span class="p">}</span>
<span class="err">#</span><span class="w"> </span><span class="nx">some</span><span class="w"> </span><span class="nx">dataframes</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">omitted</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">presentation</span><span class="w"> </span><span class="nx">purposes</span>
</code></pre></div>

<p>In [3]: ## loading the datasets
import pandas as pd
import glob
database $={ }$</p>
<p>Figure 7: Example human study interface (1/2).</p>
<div class="codehilite"><pre><span></span><code>for path in glob.glob(&#39;./database/*.pkl&#39;):
    database[&#39;df_` + path.split(&#39;/&#39;)[-1].split(&#39;.pkl&#39;)[0]] = pd.read_pickle(path)
</code></pre></div>

<h1>Record all your analysis here</h1>
<p>In [ ]:</p>
<p>Once you finished the analysis, and reached the conclusion, please write the conclusion in the following cell and end the time clock by running the cell below.</p>
<p>Is the hypothesis valid? (Yes/No)
Your answer
What is the statistics that support your conclusion?
Your answer</p>
<p>In [ ]: end_time = time.time()
print("Execution time: ", end_time - start_time)</p>
<p>Figure 8: Example human study interface (2/2).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Traditional methods like Fisher's combined test (Fisher, 1970) or Brown's method (Brown, 1975) rely on strong assumptions such as independent p -values or accurate modeling. They also cannot ensure Type-I error control with optional stopping (Assumption 3).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
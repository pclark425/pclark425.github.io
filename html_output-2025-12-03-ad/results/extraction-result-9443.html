<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9443 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9443</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9443</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-fb49e88c6bd676516898e911e42b4f8479e6f1bf</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/fb49e88c6bd676516898e911e42b4f8479e6f1bf" target="_blank">Ask Me Anything: A simple strategy for prompting language models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work develops an understanding of the effective prompt formats and proposes to use weak supervision, a procedure for combining the noisy predictions, to produce the final predictions for the inputs of a large language model.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt that demonstrates how to perform the task and no additional training. Prompting is a brittle process wherein small modifications to the prompt can cause large variations in the model predictions, and therefore significant effort is dedicated towards designing a painstakingly"perfect prompt"for a task. To mitigate the high degree of effort involved in prompt-design, we instead ask whether producing multiple effective, yet imperfect, prompts and aggregating them can lead to a high quality prompting strategy. Our observations motivate our proposed prompting method, ASK ME ANYTHING (AMA). We first develop an understanding of the effective prompt formats, finding that question-answering (QA) prompts, which encourage open-ended generation ("Who went to the park?") tend to outperform those that restrict the model outputs ("John went to the park. Output True or False."). Our approach recursively uses the LLM itself to transform task inputs to the effective QA format. We apply the collected prompts to obtain several noisy votes for the input's true label. We find that the prompts can have very different accuracies and complex dependencies and thus propose to use weak supervision, a procedure for combining the noisy predictions, to produce the final predictions for the inputs. We evaluate AMA across open-source model families (e.g., EleutherAI, BLOOM, OPT, and T0) and model sizes (125M-175B parameters), demonstrating an average performance lift of 10.2% over the few-shot baseline. This simple strategy enables the open-source GPT-J-6B model to match and exceed the performance of few-shot GPT3-175B on 15 of 20 popular benchmarks. Averaged across these tasks, the GPT-J-6B model outperforms few-shot GPT3-175B. We release our code here: https://github.com/HazyResearch/ama_prompting</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9443.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9443.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Open-ended QA vs Restrictive Prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open-ended Question-Answering Prompt Formats versus Restrictive (True/False) Prompt Formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Finding that prompt formats which encourage open-ended generation (cloze or free-form QA: 'Who went to the park?') outperform restrictive formats that force particular tokens (e.g., 'Output True or False') across tasks, models, and sizes; motivates reformulating tasks into QA format.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various (evaluated primarily with GPT-J-6B; comparisons across EleutherAI, BLOOM, OPT, T0, GPT-3 reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple (SuperGLUE tasks including CB, RTE, WSC; classification tasks DBPedia, AGNews; many others)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Natural language understanding, NLI, coreference resolution (WSC), and multi-class classification tasks that were originally presented with varied prompt templates.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Open-ended QA formats (cloze and free-form Wh/Yes-no question prompts that ask the model to generate an answer) vs restrictive formats (prompts that constrain model to output specific tokens e.g., 'True or False', or fixed label tokens). Experiments include few-shot in-context examples (k-shot), and the AMA two-step prompt-chains: question(): statement -> question; answer(): question -> answer.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Restricted prompts (e.g., 'Output True or False?') and cloze prompts (fill-in-the-blank) compared to open-ended QA (standard Wh and Yes/No phrasing). Also calibration variants (Calibrate before use).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Across multiple tasks and models: converting tasks originally in restrictive format (CB, RTE, WSC) to open-ended QA increased GPT-J-6B average from 41.7% to 71.5% (paper reports this as a +72% relative improvement for the small model in that ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Restricted (original) average ~41.7% (GPT-J-6B) -> Open-ended QA average ~71.5% (GPT-J-6B). Specific task examples given in the paper (see other entries).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+72% (relative) average improvement on the three SuperGLUE tasks CB, RTE, WSC for GPT-J-6B when reformatting to open-ended QA.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Open-ended QA aligns better with the next-token prediction objective used during LM pretraining; QA-style prompts appear far more frequently in pretraining corpora (open-ended QA structures are ~1000x more frequent than restrictive 'True/False' templates in a random sample of the Pile). Restrictive prompts also expose class-token frequency biases (e.g., 'yes' vs 'no' imbalance) learned during pretraining, hurting performance.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Ablation primarily on GPT-J-6B with k-shot in-context examples (k values used elsewhere: 0,2,4,8 etc for plots). The reformattings use prompt-chains (question() and answer()) created with 3–6 prompt-chains per task in AMA evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ask Me Anything: A simple strategy for prompting language models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9443.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9443.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-J-6B Prompt-Format Ablation (CB/WSC/RTE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation comparing Restricted 8-shot, Calibration, Cloze-QA, and Standard-QA prompt styles on GPT-J-6B for CB, WSC, RTE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantified ablation showing how different prompt styles affect accuracy for three SuperGLUE tasks on GPT-J-6B.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J-6B (EleutherAI)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CB (CommitmentBank), WSC (Winograd Schema Challenge), RTE (Recognizing Textual Entailment)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary/structured NLU/NLI tasks: determine validity/resolve pronoun or entailment decisions given context.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Restricted 8-shot (original restrictive GPT-3-style prompts asking for True/False), Calibration (Zhao et al. style calibration where applicable), Cloze-QA (fill-in-the-blank style), Standard-QA (explicit Wh/Yes-No free-form question).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Restricted 8-shot and Calibration vs Cloze-QA and Standard-QA open-ended formats.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported accuracies for GPT-J-6B: CB: Restricted 8-shot 22.0% -> Calibration 58.9% -> Cloze-QA 48.2% -> Standard-QA 83.3%. WSC: Restricted 50.0% -> Cloze-QA 56.0% -> Standard-QA 69.2%. RTE: Restricted 53.0% -> Calibration 59.2% -> Cloze-QA 62.5% -> Standard-QA 62.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Standard-QA outperforms Restricted on CB by 61.3 percentage points (22.0 -> 83.3), on WSC by 19.2 points (50.0 -> 69.2), on RTE modestly by ~9 points depending on calibration/format. Calibration sometimes helps but cannot be applied to every task.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>CB: +61.3 pp (Restricted -> Standard-QA); WSC: +19.2 pp; RTE: up to ≈+9 pp (varies by calibration/cloze).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>More precise, targeted questions (e.g., extracting the exact phrase to ask about) focus the model on the core information and exploit the LM's next-token prediction strengths. Calibration helps where applicable. Restrictive prompts are misaligned with the model's pretraining distribution and can suffer from token-frequency biases.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Ablation performed on GPT-J-6B. 'Restricted' used 8-shot variants from Brown et al. Calibration applied per Zhao et al. Cloze and Standard-QA variants constructed from the original tasks by reformatting inputs to QA-style prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ask Me Anything: A simple strategy for prompting language models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9443.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9443.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Open-ended outputs -> Class mapping (multi-class)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Two-step: open-ended QA answer generation followed by prompt-based mapping of free-form answer to finite label classes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>For multi-class classification tasks where open-ended answers might not directly match required label tokens, inserting an additional LLM step that maps the free-form answer to a discrete category significantly improves accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J-6B (evaluated), methodology applied across models</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>DBPedia (14-way classification), AGNews (4-way classification)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Document/topic classification where labels are a closed set of categories.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Open-ended QA prompt asking 'What is the document about?' (LM typically outputs a short summary like 'journal'), followed by another prompt that maps the open-ended answer to the finite label taxonomy (e.g., 'A "journal" maps to category: written work').</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Direct few-shot restrictive classification prompts vs the open-ended QA then mapping step.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports mapping step yields lifts of 33.3% (DBPedia) and 11.1% (AGNews) over the few-shot baseline in the described experiments. In benchmark tables: GPT-J-6B few-shot DBPedia 50.7% -> QA 81.4% -> QA+WS 83.9%; AGNews few-shot 74.5% -> QA 83.7% -> QA+WS 86.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>DBPedia: +30.7 pp (50.7 -> 81.4) with QA reformating; further small gains with WS. AGNews: +9.2 pp (74.5 -> 83.7) with QA reformating; further gains with WS.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>DBPedia: approximately +30–33 pp; AGNews: approximately +9–11 pp, depending on exact baseline definition.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Models generate semantically appropriate free-form answers which then must be normalized to label space; providing an explicit mapping step leverages the model's generative strengths while ensuring outputs are valid labels.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Mapping performed by an additional prompt converting open-ended output to one of the fixed categories. Experiments used 3–6 prompt-chains and then aggregated votes (for QB tasks QA+WS used majority vote when WS is complex for open-ended outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ask Me Anything: A simple strategy for prompting language models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9443.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9443.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pretraining Frequency (Pile) Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Analysis of frequency of QA-style vs restrictive prompt structures in pretraining corpus (The Pile)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical analysis over a 2% random sample of The Pile shows open-ended QA structures occur ~1000x more frequently than restrictive 'Output True or False' style structures; class/token frequency imbalances (e.g., 'yes' vs 'no') also observed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (corpus analysis relevant to EleutherAI models trained on The Pile)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Pretraining corpus analysis</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Quantifies how frequently different prompt-like textual structures appear in LM pretraining data.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Not a task prompt per se; statistical analysis of text patterns in pretraining data classified as open-ended QA-like vs restrictive instruction-like.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Open-ended QA-like structures vs restrictive 'True/False' instruction structures.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Finding reported as a frequency ratio: open-ended QA structures ~1000x more frequent than restrictive-prompt structures in the 2% sample of The Pile.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Approximately 1000x frequency difference (open-ended vs restrictive) in pretraining sample.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Higher pretraining exposure to QA-style text makes the LM better at handling open-ended QA prompts; restrictive templates are rare during pretraining and therefore misaligned with learned next-token distributions, leading to worse performance and class/token biases.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Analysis done over ~2% random sample of The Pile (the corpus used to train EleutherAI models). Additional detailed frequency tables reported in Appendix F of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ask Me Anything: A simple strategy for prompting language models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9443.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9443.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Aggregation Format: Majority Vote (MV) vs Weak Supervision (WS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of aggregation method on collections of prompt outputs: Majority Vote vs Weak Supervision</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>AMA shows that aggregating multiple open-ended prompt-chain outputs benefits from using weak supervision to model varying accuracies and dependencies among prompts; WS can substantially outperform majority vote (MV) and avoid MV's failure modes when prompt accuracies are varied and outputs are correlated.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J-6B (examples), BLOOM-7.1B and other open-source LLMs evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple (CB, RTE, WSC, WiC, many SuperGLUE and classification tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where for each input multiple prompt-chains produce votes that must be aggregated into a final label.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Multiple prompt()-chains in open-ended QA formats produce m predictions per example; aggregator compares MV (equal weight, independence assumption) vs WS (learns accuracies and dependencies from unlabeled data using structure learning and parameter estimation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Majority Vote vs Weak Supervision (with and without modeling dependencies). Also compared to weighted MV with labeled data and Pick-Best using labeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>AMA reports WS can provide up to +8.7 absolute points over MV and on 9 tasks recovering dependencies via WS yielded up to +9.6 points (average +2.2 points when modeling dependencies). Example: GPT-J-6B RTE: QA average 61.7% -> MV aggregation 66.0% -> WS aggregation 75.1% (paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Typical observed: MV improves marginally or can hurt relative to best prompt; WS yields larger, consistent gains (examples across tasks show WS > MV by several percentage points). On 16/20 tasks AMA (WS) does not do worse than MV; when worse, by at most 1.0 point.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Up to +8.7 pp improvement over MV in reported experiments; dependency modeling recovered up to +9.6 pp on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Collections of prompt outputs exhibit varied overall and class-conditional accuracies and often have correlated errors (Jaccard index much higher than i.i.d. error assumption). MV assumes equal, independent sources and thus can compress useful information poorly. WS models per-prompt accuracies and dependencies (structure learned from unlabeled votes) to weight and de-correlate signals, reducing information loss.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>WS procedure used structure learning (Varma et al. 2019) on unlabeled datasets (test set + 1000 training samples unlabeled) to estimate graph G and accuracy params θ (Ratner et al. 2018); AMA typically used 3–6 prompt-chains per task. Reported runtimes: WS learning & aggregation ~13s without dependencies, ~84s when modeling dependencies for RTE on given hardware (A100 GPUs used for inference).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ask Me Anything: A simple strategy for prompting language models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9443.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9443.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency vs AMA (small models)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of self-consistency (sampling chain-of-thought outputs and majority voting over final answers) to AMA prompting with open-ended QA + WS</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Self-Consistency (Wang et al.) provides little or no improvement for small LMs (<10B); AMA's reformattings + WS substantially outperform Self-Consistency on small model (GPT-J-6B) baselines in evaluated tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J-6B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>RTE, BoolQ, ANLI-R1 (examples reported)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>NLI / yes-no style benchmarks evaluated using chain-of-thought / self-consistency baselines and AMA.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Self-Consistency: chain-of-thought prompting with temperature sampling to get multiple reasoning paths, then majority vote over final answers. AMA: open-ended QA prompt-chains + aggregation with WS (or MV).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Self-Consistency (chain-of-thought, sampling temperatures {0.0,0.3,0.5,0.6,0.7}) vs AMA (5 aggregated outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported in paper Table 7: RTE Self-Consistency 47.3% vs AMA 75.1%; BoolQ Self-Consistency 63.1% vs AMA 67.2%; ANLI-R1 Self-Consistency 33.4% vs AMA 37.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>AMA substantially outperforms Self-Consistency for GPT-J-6B on RTE (+27.8 pp) and modestly on other tasks reported.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>RTE: +27.8 pp; BoolQ: +4.1 pp; ANLI-R1: +4.4 pp in the experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Self-Consistency depends on chain-of-thought style generation and sampling variability; small models (<10B) show limited benefit from chain-of-thought sampling. AMA's approach leverages prompt reformatting to formats aligned with pretraining and uses WS to combine multiple views, providing larger gains for small LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Comparison used 5 aggregated outputs for both methods and the same model (GPT-J-6B). Self-Consistency sampling used multiple temperatures as listed; AMA used the prompt-chains described in the paper with WS aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ask Me Anything: A simple strategy for prompting language models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9443.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9443.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PromptSource / T0 aggregation result</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aggregation of multiple public PromptSource prompt formats for T0 (3B) evaluated with MV and WS</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using 10 different PromptSource formats for each task and aggregating improves T0 performance; WS improves more than MV in these settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CB, WiC, WSC, RTE (examples reported)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot tasks for T0 where PromptSource provides multiple prompt formulations.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Aggregating zero-shot PromptSource prompts (10 unique formats per task) using Majority Vote or Weak Supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Single-format performance vs MV over 10 PromptSource formats vs WS over 10 formats. Also compared to AMA prompt-collection (functional prompt-chains) with MV/WS.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>From Table 2: For PromptSource 10 formats, MV yields average lift across tasks ~+3.7 pp, WS yields ~+6.1 pp. Example numbers: CB PromptSource 10 formats MV 60.7% (same as WS 60.7%); WSC PromptSource MV 68.3% -> WS 69.2%; RTE PromptSource MV 60.6% -> WS 69.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>WS over PromptSource formats outperforms MV by a few points (average +2.4 pp in reported grouping; per-task improvements up to ~9.1 pp in some comparisons in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Average lift using PromptSource prompts: MV +3.7 pp; WS +6.1 pp across the evaluated tasks for T0(3B).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>T0 was trained on many prompt-like examples; PromptSource prompts align with its training distribution. Aggregating multiple prompt formats captures complementary signal; WS better leverages varying accuracies and correlations than MV.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>10 PromptSource formats per task used for T0 (3B); results reported in Table 2 of the paper; the AMA prompt-chain MV/WS comparisons also shown (AMA MV/WS with AMA chains performed differently).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ask Me Anything: A simple strategy for prompting language models', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Calibrate before use: Improving few-shot performance of language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Learning dependency structures for weak supervision models <em>(Rating: 2)</em></li>
                <li>Snorkel: Rapid training data creation with weak supervision <em>(Rating: 2)</em></li>
                <li>Promptsource: An integrated development environment and repository for natural language prompts <em>(Rating: 1)</em></li>
                <li>The Pile: An 800GB dataset of diverse text for language modeling <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9443",
    "paper_id": "paper-fb49e88c6bd676516898e911e42b4f8479e6f1bf",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "Open-ended QA vs Restrictive Prompts",
            "name_full": "Open-ended Question-Answering Prompt Formats versus Restrictive (True/False) Prompt Formats",
            "brief_description": "Finding that prompt formats which encourage open-ended generation (cloze or free-form QA: 'Who went to the park?') outperform restrictive formats that force particular tokens (e.g., 'Output True or False') across tasks, models, and sizes; motivates reformulating tasks into QA format.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various (evaluated primarily with GPT-J-6B; comparisons across EleutherAI, BLOOM, OPT, T0, GPT-3 reported)",
            "model_size": null,
            "task_name": "Multiple (SuperGLUE tasks including CB, RTE, WSC; classification tasks DBPedia, AGNews; many others)",
            "task_description": "Natural language understanding, NLI, coreference resolution (WSC), and multi-class classification tasks that were originally presented with varied prompt templates.",
            "presentation_format": "Open-ended QA formats (cloze and free-form Wh/Yes-no question prompts that ask the model to generate an answer) vs restrictive formats (prompts that constrain model to output specific tokens e.g., 'True or False', or fixed label tokens). Experiments include few-shot in-context examples (k-shot), and the AMA two-step prompt-chains: question(): statement -&gt; question; answer(): question -&gt; answer.",
            "comparison_format": "Restricted prompts (e.g., 'Output True or False?') and cloze prompts (fill-in-the-blank) compared to open-ended QA (standard Wh and Yes/No phrasing). Also calibration variants (Calibrate before use).",
            "performance": "Across multiple tasks and models: converting tasks originally in restrictive format (CB, RTE, WSC) to open-ended QA increased GPT-J-6B average from 41.7% to 71.5% (paper reports this as a +72% relative improvement for the small model in that ablation).",
            "performance_comparison": "Restricted (original) average ~41.7% (GPT-J-6B) -&gt; Open-ended QA average ~71.5% (GPT-J-6B). Specific task examples given in the paper (see other entries).",
            "format_effect_size": "+72% (relative) average improvement on the three SuperGLUE tasks CB, RTE, WSC for GPT-J-6B when reformatting to open-ended QA.",
            "explanation_or_hypothesis": "Open-ended QA aligns better with the next-token prediction objective used during LM pretraining; QA-style prompts appear far more frequently in pretraining corpora (open-ended QA structures are ~1000x more frequent than restrictive 'True/False' templates in a random sample of the Pile). Restrictive prompts also expose class-token frequency biases (e.g., 'yes' vs 'no' imbalance) learned during pretraining, hurting performance.",
            "null_or_negative_result": false,
            "experimental_details": "Ablation primarily on GPT-J-6B with k-shot in-context examples (k values used elsewhere: 0,2,4,8 etc for plots). The reformattings use prompt-chains (question() and answer()) created with 3–6 prompt-chains per task in AMA evaluations.",
            "uuid": "e9443.0",
            "source_info": {
                "paper_title": "Ask Me Anything: A simple strategy for prompting language models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "GPT-J-6B Prompt-Format Ablation (CB/WSC/RTE)",
            "name_full": "Ablation comparing Restricted 8-shot, Calibration, Cloze-QA, and Standard-QA prompt styles on GPT-J-6B for CB, WSC, RTE",
            "brief_description": "Quantified ablation showing how different prompt styles affect accuracy for three SuperGLUE tasks on GPT-J-6B.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-J-6B (EleutherAI)",
            "model_size": "6B",
            "task_name": "CB (CommitmentBank), WSC (Winograd Schema Challenge), RTE (Recognizing Textual Entailment)",
            "task_description": "Binary/structured NLU/NLI tasks: determine validity/resolve pronoun or entailment decisions given context.",
            "presentation_format": "Restricted 8-shot (original restrictive GPT-3-style prompts asking for True/False), Calibration (Zhao et al. style calibration where applicable), Cloze-QA (fill-in-the-blank style), Standard-QA (explicit Wh/Yes-No free-form question).",
            "comparison_format": "Restricted 8-shot and Calibration vs Cloze-QA and Standard-QA open-ended formats.",
            "performance": "Reported accuracies for GPT-J-6B: CB: Restricted 8-shot 22.0% -&gt; Calibration 58.9% -&gt; Cloze-QA 48.2% -&gt; Standard-QA 83.3%. WSC: Restricted 50.0% -&gt; Cloze-QA 56.0% -&gt; Standard-QA 69.2%. RTE: Restricted 53.0% -&gt; Calibration 59.2% -&gt; Cloze-QA 62.5% -&gt; Standard-QA 62.0%.",
            "performance_comparison": "Standard-QA outperforms Restricted on CB by 61.3 percentage points (22.0 -&gt; 83.3), on WSC by 19.2 points (50.0 -&gt; 69.2), on RTE modestly by ~9 points depending on calibration/format. Calibration sometimes helps but cannot be applied to every task.",
            "format_effect_size": "CB: +61.3 pp (Restricted -&gt; Standard-QA); WSC: +19.2 pp; RTE: up to ≈+9 pp (varies by calibration/cloze).",
            "explanation_or_hypothesis": "More precise, targeted questions (e.g., extracting the exact phrase to ask about) focus the model on the core information and exploit the LM's next-token prediction strengths. Calibration helps where applicable. Restrictive prompts are misaligned with the model's pretraining distribution and can suffer from token-frequency biases.",
            "null_or_negative_result": false,
            "experimental_details": "Ablation performed on GPT-J-6B. 'Restricted' used 8-shot variants from Brown et al. Calibration applied per Zhao et al. Cloze and Standard-QA variants constructed from the original tasks by reformatting inputs to QA-style prompts.",
            "uuid": "e9443.1",
            "source_info": {
                "paper_title": "Ask Me Anything: A simple strategy for prompting language models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Open-ended outputs -&gt; Class mapping (multi-class)",
            "name_full": "Two-step: open-ended QA answer generation followed by prompt-based mapping of free-form answer to finite label classes",
            "brief_description": "For multi-class classification tasks where open-ended answers might not directly match required label tokens, inserting an additional LLM step that maps the free-form answer to a discrete category significantly improves accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-J-6B (evaluated), methodology applied across models",
            "model_size": "6B",
            "task_name": "DBPedia (14-way classification), AGNews (4-way classification)",
            "task_description": "Document/topic classification where labels are a closed set of categories.",
            "presentation_format": "Open-ended QA prompt asking 'What is the document about?' (LM typically outputs a short summary like 'journal'), followed by another prompt that maps the open-ended answer to the finite label taxonomy (e.g., 'A \"journal\" maps to category: written work').",
            "comparison_format": "Direct few-shot restrictive classification prompts vs the open-ended QA then mapping step.",
            "performance": "Paper reports mapping step yields lifts of 33.3% (DBPedia) and 11.1% (AGNews) over the few-shot baseline in the described experiments. In benchmark tables: GPT-J-6B few-shot DBPedia 50.7% -&gt; QA 81.4% -&gt; QA+WS 83.9%; AGNews few-shot 74.5% -&gt; QA 83.7% -&gt; QA+WS 86.4%.",
            "performance_comparison": "DBPedia: +30.7 pp (50.7 -&gt; 81.4) with QA reformating; further small gains with WS. AGNews: +9.2 pp (74.5 -&gt; 83.7) with QA reformating; further gains with WS.",
            "format_effect_size": "DBPedia: approximately +30–33 pp; AGNews: approximately +9–11 pp, depending on exact baseline definition.",
            "explanation_or_hypothesis": "Models generate semantically appropriate free-form answers which then must be normalized to label space; providing an explicit mapping step leverages the model's generative strengths while ensuring outputs are valid labels.",
            "null_or_negative_result": false,
            "experimental_details": "Mapping performed by an additional prompt converting open-ended output to one of the fixed categories. Experiments used 3–6 prompt-chains and then aggregated votes (for QB tasks QA+WS used majority vote when WS is complex for open-ended outputs).",
            "uuid": "e9443.2",
            "source_info": {
                "paper_title": "Ask Me Anything: A simple strategy for prompting language models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Pretraining Frequency (Pile) Analysis",
            "name_full": "Analysis of frequency of QA-style vs restrictive prompt structures in pretraining corpus (The Pile)",
            "brief_description": "Empirical analysis over a 2% random sample of The Pile shows open-ended QA structures occur ~1000x more frequently than restrictive 'Output True or False' style structures; class/token frequency imbalances (e.g., 'yes' vs 'no') also observed.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "N/A (corpus analysis relevant to EleutherAI models trained on The Pile)",
            "model_size": null,
            "task_name": "Pretraining corpus analysis",
            "task_description": "Quantifies how frequently different prompt-like textual structures appear in LM pretraining data.",
            "presentation_format": "Not a task prompt per se; statistical analysis of text patterns in pretraining data classified as open-ended QA-like vs restrictive instruction-like.",
            "comparison_format": "Open-ended QA-like structures vs restrictive 'True/False' instruction structures.",
            "performance": "Finding reported as a frequency ratio: open-ended QA structures ~1000x more frequent than restrictive-prompt structures in the 2% sample of The Pile.",
            "performance_comparison": null,
            "format_effect_size": "Approximately 1000x frequency difference (open-ended vs restrictive) in pretraining sample.",
            "explanation_or_hypothesis": "Higher pretraining exposure to QA-style text makes the LM better at handling open-ended QA prompts; restrictive templates are rare during pretraining and therefore misaligned with learned next-token distributions, leading to worse performance and class/token biases.",
            "null_or_negative_result": null,
            "experimental_details": "Analysis done over ~2% random sample of The Pile (the corpus used to train EleutherAI models). Additional detailed frequency tables reported in Appendix F of the paper.",
            "uuid": "e9443.3",
            "source_info": {
                "paper_title": "Ask Me Anything: A simple strategy for prompting language models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Aggregation Format: Majority Vote (MV) vs Weak Supervision (WS)",
            "name_full": "Effect of aggregation method on collections of prompt outputs: Majority Vote vs Weak Supervision",
            "brief_description": "AMA shows that aggregating multiple open-ended prompt-chain outputs benefits from using weak supervision to model varying accuracies and dependencies among prompts; WS can substantially outperform majority vote (MV) and avoid MV's failure modes when prompt accuracies are varied and outputs are correlated.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-J-6B (examples), BLOOM-7.1B and other open-source LLMs evaluated",
            "model_size": null,
            "task_name": "Multiple (CB, RTE, WSC, WiC, many SuperGLUE and classification tasks)",
            "task_description": "Tasks where for each input multiple prompt-chains produce votes that must be aggregated into a final label.",
            "presentation_format": "Multiple prompt()-chains in open-ended QA formats produce m predictions per example; aggregator compares MV (equal weight, independence assumption) vs WS (learns accuracies and dependencies from unlabeled data using structure learning and parameter estimation).",
            "comparison_format": "Majority Vote vs Weak Supervision (with and without modeling dependencies). Also compared to weighted MV with labeled data and Pick-Best using labeled data.",
            "performance": "AMA reports WS can provide up to +8.7 absolute points over MV and on 9 tasks recovering dependencies via WS yielded up to +9.6 points (average +2.2 points when modeling dependencies). Example: GPT-J-6B RTE: QA average 61.7% -&gt; MV aggregation 66.0% -&gt; WS aggregation 75.1% (paper tables).",
            "performance_comparison": "Typical observed: MV improves marginally or can hurt relative to best prompt; WS yields larger, consistent gains (examples across tasks show WS &gt; MV by several percentage points). On 16/20 tasks AMA (WS) does not do worse than MV; when worse, by at most 1.0 point.",
            "format_effect_size": "Up to +8.7 pp improvement over MV in reported experiments; dependency modeling recovered up to +9.6 pp on some tasks.",
            "explanation_or_hypothesis": "Collections of prompt outputs exhibit varied overall and class-conditional accuracies and often have correlated errors (Jaccard index much higher than i.i.d. error assumption). MV assumes equal, independent sources and thus can compress useful information poorly. WS models per-prompt accuracies and dependencies (structure learned from unlabeled votes) to weight and de-correlate signals, reducing information loss.",
            "null_or_negative_result": false,
            "experimental_details": "WS procedure used structure learning (Varma et al. 2019) on unlabeled datasets (test set + 1000 training samples unlabeled) to estimate graph G and accuracy params θ (Ratner et al. 2018); AMA typically used 3–6 prompt-chains per task. Reported runtimes: WS learning & aggregation ~13s without dependencies, ~84s when modeling dependencies for RTE on given hardware (A100 GPUs used for inference).",
            "uuid": "e9443.4",
            "source_info": {
                "paper_title": "Ask Me Anything: A simple strategy for prompting language models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Self-Consistency vs AMA (small models)",
            "name_full": "Comparison of self-consistency (sampling chain-of-thought outputs and majority voting over final answers) to AMA prompting with open-ended QA + WS",
            "brief_description": "Self-Consistency (Wang et al.) provides little or no improvement for small LMs (&lt;10B); AMA's reformattings + WS substantially outperform Self-Consistency on small model (GPT-J-6B) baselines in evaluated tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-J-6B",
            "model_size": "6B",
            "task_name": "RTE, BoolQ, ANLI-R1 (examples reported)",
            "task_description": "NLI / yes-no style benchmarks evaluated using chain-of-thought / self-consistency baselines and AMA.",
            "presentation_format": "Self-Consistency: chain-of-thought prompting with temperature sampling to get multiple reasoning paths, then majority vote over final answers. AMA: open-ended QA prompt-chains + aggregation with WS (or MV).",
            "comparison_format": "Self-Consistency (chain-of-thought, sampling temperatures {0.0,0.3,0.5,0.6,0.7}) vs AMA (5 aggregated outputs).",
            "performance": "Reported in paper Table 7: RTE Self-Consistency 47.3% vs AMA 75.1%; BoolQ Self-Consistency 63.1% vs AMA 67.2%; ANLI-R1 Self-Consistency 33.4% vs AMA 37.8%.",
            "performance_comparison": "AMA substantially outperforms Self-Consistency for GPT-J-6B on RTE (+27.8 pp) and modestly on other tasks reported.",
            "format_effect_size": "RTE: +27.8 pp; BoolQ: +4.1 pp; ANLI-R1: +4.4 pp in the experiments reported.",
            "explanation_or_hypothesis": "Self-Consistency depends on chain-of-thought style generation and sampling variability; small models (&lt;10B) show limited benefit from chain-of-thought sampling. AMA's approach leverages prompt reformatting to formats aligned with pretraining and uses WS to combine multiple views, providing larger gains for small LMs.",
            "null_or_negative_result": false,
            "experimental_details": "Comparison used 5 aggregated outputs for both methods and the same model (GPT-J-6B). Self-Consistency sampling used multiple temperatures as listed; AMA used the prompt-chains described in the paper with WS aggregation.",
            "uuid": "e9443.5",
            "source_info": {
                "paper_title": "Ask Me Anything: A simple strategy for prompting language models",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "PromptSource / T0 aggregation result",
            "name_full": "Aggregation of multiple public PromptSource prompt formats for T0 (3B) evaluated with MV and WS",
            "brief_description": "Using 10 different PromptSource formats for each task and aggregating improves T0 performance; WS improves more than MV in these settings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T0",
            "model_size": "3B",
            "task_name": "CB, WiC, WSC, RTE (examples reported)",
            "task_description": "Zero-shot tasks for T0 where PromptSource provides multiple prompt formulations.",
            "presentation_format": "Aggregating zero-shot PromptSource prompts (10 unique formats per task) using Majority Vote or Weak Supervision.",
            "comparison_format": "Single-format performance vs MV over 10 PromptSource formats vs WS over 10 formats. Also compared to AMA prompt-collection (functional prompt-chains) with MV/WS.",
            "performance": "From Table 2: For PromptSource 10 formats, MV yields average lift across tasks ~+3.7 pp, WS yields ~+6.1 pp. Example numbers: CB PromptSource 10 formats MV 60.7% (same as WS 60.7%); WSC PromptSource MV 68.3% -&gt; WS 69.2%; RTE PromptSource MV 60.6% -&gt; WS 69.7%.",
            "performance_comparison": "WS over PromptSource formats outperforms MV by a few points (average +2.4 pp in reported grouping; per-task improvements up to ~9.1 pp in some comparisons in the paper).",
            "format_effect_size": "Average lift using PromptSource prompts: MV +3.7 pp; WS +6.1 pp across the evaluated tasks for T0(3B).",
            "explanation_or_hypothesis": "T0 was trained on many prompt-like examples; PromptSource prompts align with its training distribution. Aggregating multiple prompt formats captures complementary signal; WS better leverages varying accuracies and correlations than MV.",
            "null_or_negative_result": false,
            "experimental_details": "10 PromptSource formats per task used for T0 (3B); results reported in Table 2 of the paper; the AMA prompt-chain MV/WS comparisons also shown (AMA MV/WS with AMA chains performed differently).",
            "uuid": "e9443.6",
            "source_info": {
                "paper_title": "Ask Me Anything: A simple strategy for prompting language models",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Calibrate before use: Improving few-shot performance of language models",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Learning dependency structures for weak supervision models",
            "rating": 2
        },
        {
            "paper_title": "Snorkel: Rapid training data creation with weak supervision",
            "rating": 2
        },
        {
            "paper_title": "Promptsource: An integrated development environment and repository for natural language prompts",
            "rating": 1
        },
        {
            "paper_title": "The Pile: An 800GB dataset of diverse text for language modeling",
            "rating": 1
        }
    ],
    "cost": 0.020024999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ASK ME ANYTHING: A SIMPLE STRATEGY FOR PROMPTING LANGUAGE MODELS</h1>
<p>Simran Arora ${ }^{1, <em>}$, Avanika Narayan ${ }^{1, </em>}$ Mayee F. Chen ${ }^{1}$, Laurel Orr ${ }^{1}$, Neel Guha ${ }^{1}$, Kush Bhatia ${ }^{1}$, Ines Chami ${ }^{2}$, Frederic Sala $^{3}$, and Christopher Ré ${ }^{1}$<br>${ }^{1}$ Stanford University<br>${ }^{2}$ Numbers Station<br>${ }^{3}$ University of Wisconsin-Madison</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt that demonstrates how to perform the task and no additional training. Prompting is a brittle process wherein small modifications to the prompt can cause large variations in the model predictions, and therefore significant effort is dedicated towards designing a painstakingly perfect prompt for a task. To mitigate the high degree of effort involved in prompting, we instead ask whether collecting multiple effective, yet imperfect, prompts and aggregating them can lead to a high quality prompting strategy. Our observations motivate our proposed prompting method, ASK Me Anything Prompting (AMA). We first develop an understanding of the effective prompt formats, finding question-answering (QA) prompts, which encourage open-ended generation ("Who went to the park?") tend to outperform those that restrict the model outputs ("John went to the park. Output True or False"). Our approach recursively uses the LLM to transform task inputs to the effective QA format. We apply these prompts to collect several noisy votes for the input's true label. We find that these prompts can have very different accuracies and complex dependencies and thus propose to use weak supervision, a procedure for combining the noisy predictions, to produce the final predictions. We evaluate AMA across open-source model families (EleutherAI, BLOOM, OPT, and T0) and sizes ( $125 \mathrm{M}-175 \mathrm{~B}$ parameters), demonstrating an average performance lift of $10.2 \%$ over the few-shot baseline. This simple strategy enables the open-source GPT-J-6B model to match and exceed the performance of few-shot GPT3-175B on 15 of 20 popular benchmarks. Averaged across these tasks, the GPT-J-6B model outperforms few-shot GPT3-175B. We release our code for reproducing the results here: https://github.com/HazyResearch/ama_prompting.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) are bringing us closer to the goal of task-agnostic machine learning [Brown et al., 2020, Bommasani et al., 2021]. Rather than training models for new tasks, LLMs are being applied to new tasks out-of-the box. In this paradigm, termed in-context learning, LLMs are instead controlled via natural language task specifications, or prompts. A prompt is defined by a template, which contains placeholders for the description and demonstrations of the inputs and outputs for the task.</p>
<p>Recent work has evaluated LLM prompting performance on a broad set of tasks and finds the process to be brittle small changes to the prompt result in large performance variations [Zhao et al., 2021, Holtzman et al., 2021]. The performance further varies depending on the chosen LLM family [Ouyang et al., 2022, Sanh et al., 2022, inter alia.] and model size [Wei et al., 2022a, Lampinen et al., 2022]. To improve reliability, significant effort is dedicated towards designing a painstakingly perfect prompt. For instance, Mishra et al. [2021] and Wu et al. [2022] recommend that users manually explore large search-spaces of strategies to tune their prompts on a task-by-task basis.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: AMA first recursively uses the LLM to reformat tasks and prompts to effective formats, and second aggregates the predictions across prompts using weak-supervision. The reformatting is performed using prompt-chains, which consist of functional (fixed, reusable) prompts that operate over the varied task inputs. Here, given the input example, the prompt-chain includes a question()-prompt through which the LLM converts the input claim to a question, and an answer() prompt, through which the LLM answers the question it generated. Different prompt-chains (i.e., differing in the in-context question and answer demonstrations) lead to different predictions for the input's true label.</p>
<p>This work instead considers aggregating the predictions of multiple effective, yet imperfect, prompts to improve prompting performance over a broad set of models and tasks. Given a task input, each prompt produces a vote for the input's true label, and these votes are aggregated to produce a final prediction. In pursuit of high quality prompting via aggregation, we face the following challenges:</p>
<ol>
<li>Effective prompts: High quality prompts are a precursor to improvements from aggregation. We take the original prompts which yield near-random performance in Brown et al. [2020] for two SuperGLUE tasks (CB, RTE). Generating multiple prompts in the same format and taking majority vote prediction across prompts has a minor effect ( $+4 \%$ for CB ) and can even hurt performance versus the average prompt performance ( $-2 \%$ for RTE). Many proposals for improved prompts focus on a single task type and evaluate on a single model-family and/or size [Wei et al., 2022a, Jung et al., 2022]. We need a structure for prompting that works across tasks and models.</li>
<li>Scalable collection: After identifying effective prompt formats, we need to obtain multiple prompts in these formats - these prompts will be used to collect votes for an input's true label. The original format of a task varies widely and prior works manually rewrite input examples to new formats in a task-specific manner [Mishra et al., 2021, Wu et al., 2022], which is challenging to scale. We need a scalable strategy for reformatting task inputs.</li>
<li>Prompt aggregation: Using the prompts above (for CB and RTE), we see $9.5 \%$ average variation in accuracy and that the Jaccard index over errors is $69 \%$ higher than if prompt errors were i.i.d. Majority vote (MV) is the primary unsupervised aggregation strategy in prior prompting work [Jiang et al., 2020, Schick and Schütze, 2021], but it does not account for either property, making it unreliable. We need a strategy that accounts for the varying accuracies and dependencies.
In this work, we propose Ask Me Anything Prompting (AMA), a simple approach that surprisingly enables open-source LLMs with 30x fewer parameters to exceed the few-shot performance of GPT3-175B. In AMA:</li>
<li>We identify properties of prompts that improve effectiveness across tasks, model types, and model sizes. We study standard prompt-formats categorized by prior work [Brown et al., 2020] and find prompts that encourage open-ended answers ("Where did John go?") to be more effective than prompts that restrict the model output to particular tokens (e.g. "John went to the park. Output True or False?"). For instance, converting three SuperGLUE tasks (CB, RTE, WSC) from the original restrictive formats in [Brown et al., 2020] to open-ended formats provides a $72 \%$ performance improvement (Section 3.2). Given a task input, we find that a simple structure of (1) forming questions based on the input and (2) prompting the LLM to answer the questions applies quite generally and improves performance across diverse benchmark tasks.</li>
<li>We propose a strategy for scalably reformatting task inputs to the effective formats found in (1). We propose to transform task inputs to the effective open-ended question-answering format by recursively using the LLM itself in a fixed two step pipeline. We first use question()-prompts, which contain task-agnostic examples of how to transform statements to various (e.g., yes-no, cloze) questions and second use answer()-prompts that demonstrate ways of answering questions (e.g., concise or lengthy answers). Applying prompt-chains- answer(question(x))—</li>
</ol>
<p>gives a final prediction for the input $x$. ${ }^{2}$ Chains are (1) reused across inputs and (2) different pairs of functional prompts can be combined to create variety. We apply the varying functional prompt-chains to an input to collect multiple votes for the input's true label.
3. We propose the use of weak supervision (WS) to reliably aggregate predictions. We find that the errors produced by the predictions of different chains can be highly varying and correlated. While majority vote (MV) may do well on certain sets of prompts, it performs poorly in the above cases. AMA accounts for these cases by identifying dependencies among prompts and using WS, a procedure for modeling and combining noisy predictions without any labeled data [Ratner et al., 2017, Varma et al., 2019]. We apply WS to prompting broadly for the first time in this work, showing it improves the reliability of prompting with off-the-shelf LLMs and no further training. We find that AMA can achieve up to 8.7 points of lift over MV and that on 9 tasks, it recovers dependencies among prompts to boost performance by up to 9.6 points.</p>
<p>We apply our proposed prompt-aggregation strategy, AMA, to 20 popular language benchmarks and 14 open-source LLMs from 4 model families (EleutherAI [Black et al., 2021, Wang and Komatsuzaki, 2021, EleutherAI], BLOOM blo [2022], OPT [Zhang et al., 2022], and T0 [Sanh et al., 2022]) spanning 3 orders-of-magnitude (125M-175B parameters). Our proof-of-concept provides an improvement over the few-shot $(k=3)$ baseline by an average of $10.2 \% \pm 6.1 \%$ absolute ( $21.4 \% \pm 11.2 \%$ relative) lift across models. We find the largest gains are on tasks where the knowledge required to complete the task is found in the provided context and comparatively less on closed-book tasks (e.g., factual recall). Most excitingly, Ask Me Anything Prompting enables an open-source LLM, which is furthermore 30x parameters smaller, to match or exceed the challenging GPT3-175B few-shot baseline results in Brown et al. [2020] on 15 of 20 benchmarks. We hope AMA and future work help address painpoints of using LLMs [Arora and Ré, 2022, Narayan et al., 2022] by improving the ability to proceed with less-than-perfect prompts and enabling the use of small, private, and open-source LLMs.</p>
<h1>2 Related Work</h1>
<p>Several existing works study how to improve the zero-to-few-shot task-transfer abilities of LLMs.
Training based strategies Prior works have improved prompting performance by training larger models over more or curated data, and for longer [Kaplan et al., 2020, Chowdhery et al., 2022] - or by explicitly fine-tuning LMs over prompts [Wang et al., 2022a, Wei et al., 2022b, Sanh et al., 2022, Ouyang et al., 2022]. We complementarily aim to improve the prompting performance of off-the-shelf language models with no additional fine-tuning.</p>
<p>Prompt-engineering Prompt-engineering is the process of designing natural language specifications of a task, which are used to condition the LLM at inference time. Prior work finds that the prompt format changes the model behavior and proposes particular formats. Some formats are designed-for or evaluated-on a narrow task type, model type, or model size [Wei et al., 2022a, Jung et al., 2022]. Others require users to manually rewrite task-inputs to the prescribed formats on a example-by-example basis in a task-specific manner [Mishra et al., 2021, Patel et al., 2022, Wu et al., 2022]. Our recursive use of the LLM is similar to Jung et al. [2022], which focuses on commonsense reasoning. We draw inspiration from and share similar ideas with these lines of work.
Complementary work investigates how to simplify complex tasks (e.g., multi-hop reasoning), to achieve better performance in the prompting paradigm. Creswell et al. [2022], Wu et al. [2022] explicitly decompose the complex tasks into steps, which are each handled in a separate inference-pass. However, these methods draw a distinction between explicitly compositional tasks which can be naturally decomposed into multiple steps and "single-step" language tasks. These prior works do not support the single-step tasks, which are the focus of our work.</p>
<p>Prompt sensitivity Prior works note the sensitivity of prompting under slight modifications and propose strategies to improve the performance of single prompts [Zhao et al., 2021, Liu et al., 2021]. Complementing this, we manage the noise by aggregating over multiple prompt outputs. Prompt aggregation has been applied in several prior works. Many works train models to perform the aggregation and/or to achieve strong results with small LMs [Jiang et al., 2020, Schick and Schütze, 2021, Cobbe et al., 2021, Zelikman et al., 2022, inter alia.]. Self-Consistency Wang et al. [2022b], which requires no training, does not report improvements for small LMs ( $&lt;10$ B parameters). We also compare AMA to Self-Consistency in Appendix B. The unsupervised aggregation strategy used in prior works is Majority Vote - we are the first to use Weak Supervision for unsupervised prompt aggregation.</p>
<p>Weak supervision (WS) WS is a powerful framework that learns the accuracies and correlations of multiple noisy sources and aggregates them to produce weak labels for training data [Ratner et al., 2017, 2016, 2018, Varma et al.,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>2019, Fu et al., 2020]. WS has been applied to prompting in the context of distilling a LLM by aggregating the outputs of hand-curated prompts into a labeled dataset and training a smaller model on it [Smith et al., 2022]. In contrast, we aim to use aggregation to improve out-of-the-box LLM performance reliably.</p>
<h1>3 Ask Me Anything Prompting</h1>
<p>We propose Ask Me Anything Prompting (AMA), a prompting approach that uses multiple imperfect promptsrather than one painstakingly crafted perfect prompt-and reliably aggregates their outputs. We describe and motivate AMA's prompt format (Section 3.2), how AMA scalably produces collections of prompts (Section 3.3), and AMA's aggregation method (Section 3.4).</p>
<h3>3.1 Preliminaries</h3>
<p>We consider supervised tasks, $(\mathcal{X}, \mathcal{Y})$, where $x \in \mathcal{X}$ is the example and $y \in \mathcal{Y}$ is the output. We have an unlabeled dataset $\mathcal{D}=\left{x_{i}\right}<em i="i">{i=1}^{n}$ for which we wish to predict each $y</em>=p(x)$. Specifically, the LLM runs inference on $p$ with $x$ substituted for the placeholder in the template.
We denote a collection of $m$ prompts as $\mathbf{P}=\left[p_{1}, p_{2}, \ldots, p_{m}\right]$. Given input $\mathcal{D}$, we (1) apply a collection $\mathbf{P}$ to each $x \in \mathcal{D}$ and (2) aggregate their predictions, denoted as $\mathbf{P}(x)=\left[p_{1}(x), \ldots, p_{m}(x)\right]$, using an aggregator function $\phi: \mathcal{Y}^{m} \rightarrow \mathcal{Y}$ to produce outputs $\hat{y}$ on each $x$. We can thus express the procedure via two key components we aim to understand, the prompts $\mathbf{P}$ and aggregator $\phi$.
Running examples For the motivating observations in the rest of this section, we use three SuperGLUE [Wang et al., 2019] tasks-CommitmentBank (CB), Recognizing Textual Entailement (RTE), and Winograd Schema Challenge (WSC)—and the DBPedia and AGNews classification tasks [Zhang et al., 2015]. We evaluate over the GPT-J-6B model Wang and Komatsuzaki [2021]. CB and RTE require determining the vailidity of a statement is given some context (as in Figure 1), WSC requires outputting the subject corresponding to a given pronoun, and DBPedia and AGNews contain 14 and 4 classes respectively. We use as a running example: determine if the statement "John went to the park" is valid, given the context "John invited Mark to watch Jurassic Park with his family at the theater".
Simple baseline To provide some intuition on the challenges around effectively designing the two levers, $\mathbf{P}$ and aggregator $\phi$, we start with a naïve baseline with off-the-shelf prompts and the unsupervised majority vote prompt aggregation strategy used in prior work [Jiang et al., 2020, Schick and Schütze, 2021]. We take the prompts proposed in [Brown et al., 2020] for GPT-3 and produce $\mathbf{P}$ with five prompts for each task by using different sets of in-context examples. Comparing majority vote (MV), the unsupervised aggregation strategy used in prior work, to the average performance of the prompts, MV gives $39.3 \%$ ( $+2.2 \%$ ) for CB and $54.5 \%$ ( $-2 \%$ ) for RTE. The delta from aggregating is minor and in the worst case, harmful. Ideally, we would expect that aggregation should lead to improvement by reducing noise, but we find that performance here is only comparable to the single prompt baseline.}$. We apply LLMs to this task by using a prompt-a natural language prefix that demonstrates how to complete a task. A prompt consists of a prompt template, with placeholders for (1) zero or more in-context task demonstrations and (2) for the inference example $x$ as shown in Figure 3. Given a prompt $p$, we use $p: \mathcal{X} \rightarrow \mathcal{Y}$ to refer the output of the prompted LLM which produces a prediction $\hat{y</p>
<h3>3.2 Effective Prompt Formats</h3>
<p>First, we explore what makes an effective prompt format towards improving the quality of $\mathbf{P}(x)$.
Standard prompt formats We ground our analysis in three standard categories of prompts used in prior work including Brown et al. [2020], Sanh et al. [2022, inter alia.]: (1) questions that restrict the model output particular tokens ("John invited Mark to come watch Jurassic Park. Output True or False?"); (2) cloze-questions which ask the model to fill in the remaining text ("John invited Mark to come watch Jurassic _" and using the LLM to fill-the-blank, " $\underline{\text { Park }}$ "); and (3) traditional (yes-no, Wh) free-form questions ("Where did John invite Mark?"). We compare these three prompting formats and make the following observations:</p>
<ol>
<li>Open-ended prompts appear to outperform restrictive-prompts. We first group the results in Brown et al. [2020] based on the format used for the task, along the above categorizations (see Figure 2). When scaling from GPT3-6.7B to GPT3-175B, we find that the relative gain is far lower on open-ended (cloze and traditional QA) formats vs. restricted formats.
Next, CB, RTE, and WSC are originally formatted with restrictive-prompts in Brown et al. [2020], and we form copies of the tasks in the open-ended question (cloze and free-form QA) formats. This improves the performance of the small model on average from $41.7 \%$ to $71.5 \%(+72 \%)$. Intuitively, the task of answering open-ended questions</li>
</ol>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompt Format</th>
<th style="text-align: center;">CB</th>
<th style="text-align: center;">WSC</th>
<th style="text-align: center;">RTE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Restricted 8-shot</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">53.0</td>
</tr>
<tr>
<td style="text-align: left;">+ Calibration</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">59.2</td>
</tr>
<tr>
<td style="text-align: left;">Cloze-QA Format</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">$\mathbf{6 2 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">Standard-QA Format</td>
<td style="text-align: center;">$\mathbf{8 3 . 3}$</td>
<td style="text-align: center;">$\mathbf{6 9 . 2}$</td>
<td style="text-align: center;">62.0</td>
</tr>
</tbody>
</table>
<p>Figure 2: Relative lift with model scale using results and prompt-styles reported in Brown et al. [2020] (Left). Ablating the promptstyle using the GPT-J-6B model. We include calibration results Zhao et al. [2021] and the "-" indicates the method cannot be applied to the task (Right).
is aligned with the next-token prediction language modeling objective. We observe that more precise questions give larger lifts. For WSC the restrictive prompt form is: "The pronoun 'his' refers to "Mark" in the context. True or False?", given the context "Mark went to the park with his dog.". Reformatting to "What does 'his' refer to?" and evaluating whether the answer is "Mark" provides 38\% lift ( $69.2 \%$ accuracy). Yet, further extracting the portion of the context that mentions the pronoun ("his dog"), reformatting ("Whose dog?") and prompting with precise questions gives $49.4 \%$ lift ( $74.7 \%$ ).
2. The use of open-ended questions over restrictive-prompts can increase the difficulty of mapping open-ended answers to valid output classes. For tasks with output spaces that are likely observed during pretraining (yes-no questions, sentiment classification), we see that the LLM naturally generates valid $\hat{y} \in \mathfrak{\mathcal { V }}$. For tasks with specialized output classes (i.e. multi-class classification), we need to map the answer to the open-ended question (e.g., "What is the document about?") to a valid output class. For example, given 'Personality and Mental Health ... is a quarterly peer-reviewed academic journal published by ...", we observe that the LLM typically outputs semantically correct summaries of the document topic, e.g. "journal". We find that inserting a step for the LLM to map the open-ended output "journal" to a valid category via the prompt "A 'journal' maps to category: written work" enables a 33.3\% and $11.1 \%$ lift over the few-shot baseline on DBPedia (14-way classification) and AGNews (4-way) respectively.</p>
<p>Why is the QA prompt format effective? We analyze the LM pretraining corpus to better understand why the proposed QA prompt template may be effective. The EleutherAI models are trained on The Pile corpus Black et al. [2021], Wang and Komatsuzaki [2021], Gao et al. [2021]. Over a $2 \%$ random sample of the $\sim 200$ B token Pile data, we find that open-ended QA structures (i.e., which ask the model "Is ...?", "Who ...?") appear on the order of $1000 \times$ more frequently than the restrictive-prompt structures (i.e., which instruct the model to output "True or False", "Yes or No"). The prompt structures and frequencies are in Table 8.</p>
<p>When applying the few-shot restrictive prompts, we observe large imbalances in the F1-scores for different classes (Table 10). Therefore, we next ask if answering the restrictive prompts is challenging due to biases acquired during pretraining. We find in Pile that there are large imbalances between the frequencies of "yes" vs. "no", and "True" vs. "False" for instance, which may instil the biases and contribute to the low quality from restrictive-prompts. Detailed results of the Pile analysis are in Appendix F.</p>
<p>AMA's prompt format Motivated by our observations about the effectiveness of QA prompt structures, we proceed in AMA with a two-step prompting pipeline: (1) generating questions based on the input and (2) prompting the LLM to answer the generated questions. These prompts are effective, and to further improve performance we next turn to generating and aggregating over multiple prompt-outputs for each input. For intuition, different questions (with our running example: "Who went to the park?", "Did John go the park?", "Where did John go?") emphasize different aspects of the input and can provide complementary information towards reasoning about the answer. Manually generating multiple prompts per input is challenging, and so we study how to do this at scale in the following section.</p>
<h1>3.3 Creating Prompt Collections at Scale</h1>
<p>Our goal is to produce a collection of prompts, $\mathbf{P}$, that can be applied to tasks at scale. To produce prompts in the effective open-ended question-answering format, our insight is to recursively apply the LLM itself using a chain of functional prompts, referred to as a prompt( )-chain. We describe these prompts as functional because they apply a task-agnostic operation to all inputs in the tasks, without any example-level customization. We describe the two functional prompts used in AMA below. We use Figure 1 as a running example to explain each type.
(a) question(): $x \rightarrow q$ generates a question $q$ (such as "Did John go to the park?") from an input $x$ ("John went to the park."). question() prompts simply contain demonstrations of how a statement can be transformed to an open-ended question.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Example prompt with the in-context demonstrations and placeholder (Left) with two different prompt variations (Right) created by changing demonstrations and question style.
(b) answer(): $q \rightarrow a$ applies the question generated by (a) to the context of $x$ to produce intermediate answers $a$ (such as "No" or "theater"). The answer() prompts contain demonstrations of how to answer a question (optionally) given some input context.
To create $\mathbf{P}$ for aggregation, AMA constructs different prompt()-chains where each unique prompt()-chain is a different view of the task and can emphasize different aspects of $x$. Inspired by <em>Sanh et al. [2022]</em> and <em>Liu et al. [2021]</em>, we also vary chains through two key levers-the in-context demonstrations and the style of prompt questions-as shown in Figure 3. To vary the style of open-ended prompt questions, we construct question() and answer() prompts that produce and answer either Yes/No, Wh, multiple-choice, or cloze- questions.</p>
<h1>3.4 Prompt Aggregation</h1>
<p>To aggregate the prompt predictions $\mathbf{P}(x)$ into outputs $\hat{y}$ reliably, we apply tools from weak supervision, a powerful approach for learning high-quality models from weaker sources of signal without labeled data <em>[Ratner et al., 2017]</em>. We first describe properties of $\mathbf{P}(x)$ that illustrate when the simple baseline of majority vote may perform poorly. We then describe our aggregator $\phi_{\mathrm{ws}}$, which explicitly identifies and then accounts for these properties.</p>
<p>Baseline observations We understand how to aggregate $\mathbf{P}(x)$ by presenting a set of observations on CB, RTE, and WSC. For each, we compare two baselines for constructing $\mathbf{P}$ : (1) $\mathbf{P}<em _mathrm_E="\mathrm{E">{\mathrm{T}}$ : varying the prompt template with no overlap in the in-context examples, and (2) $\mathbf{P}</em>$ :}}$ : varying the in-context examples for a fixed prompt template, all with $|\mathbf{P}|=5$. We observe the following properties on $\mathbf{P</p>
<ol>
<li>Varied overall accuracies: While prompts in $\mathbf{P}<em _mathrm_T="\mathrm{T">{\mathrm{E}}$ may seem more similar than those in $\mathbf{P}</em>}}$, the gap between the best and worst $p_{i} \in \mathbf{P}$ is large in both cases - $12.1 \%$ for $\mathbf{P<em _mathrm_T="\mathrm{T">{\mathrm{E}}$ and $9.6 \%$ for $\mathbf{P}</em>$.}</li>
<li>Varied class-conditional accuracies <em>[Zhao et al., 2021]</em>: Beyond overall prompt accuracy, the average variance of class-conditional prompt accuracies is $9.7 \%$ across the tasks and baselines.</li>
<li>Highly-correlated outputs: Prompt predictions have dependencies among each other. The Jaccard index over error sets averaged across tasks is 42.2 for $\mathbf{P}<em _mathrm_T="\mathrm{T">{\mathrm{E}}$ and 39.9 for $\mathbf{P}</em>$. For reference, two prompts that produce i.i.d. errors and have $60 \%$ accuracy each would have a score of 0.25 .}</li>
</ol>
<p>The three observations present challenges in aggregating predictions via simple approaches like MV. MV tends to do better than using one prompt, but it weights all prompts equally and treats them independently. Such an aggregation method may be sufficient over certain collections of prompts but is not reliable across general $\mathbf{P}$ that may exhibit the three properties we have observed.</p>
<p>AMA Aggregation Given the varied accuracies and dependencies among prompt()-chains, in AMA we draw on recent work in weak supervision <em>[Ratner et al., 2017]</em>, which is able to account for the accuracy and dependency properties without relying on labeled data. We learn a probabilistic graphical model on $\operatorname{Pr}<em _mathrm_ws="\mathrm{ws">{G, \theta}(y, \mathbf{P}(x))$ and define the aggregator as $\phi</em>(x)=\arg \max }<em G_="G," _theta="\theta">{y \in \mathcal{Y}} \operatorname{Pr}</em>$, so our procedure is as follows:}(y \mid \mathbf{P}(x)) . G=(V, E)$ is a dependency graph where $V={y, \mathbf{P}(x)}$ and $E$ is an edgeset where $\left(p_{i}(x), p_{j}(x)\right) \in E$ iff $p_{i}(x)$ and $p_{j}(x)$ are conditionally independent given $y . \theta$ are the accuracy parameters for $\mathbf{P}(x)$. Since we lack labeled data $y$, we cannot estimate $G$ or $\theta$ directly from $\mathcal{D</p>
<ol>
<li>We use a structure learning approach from <em>Varma et al. [2019]</em> to recover the dependency structure $\hat{G}$ using $\mathbf{P}(x)$ applied to $\mathcal{D}$.</li>
<li>We use $\hat{G}, \mathcal{D}$, and $\mathbf{P}(x)$ to learn the accuracies $\theta$ of the prompts $\mathbf{P}$ from <em>Ratner et al. [2018]</em>.</li>
<li>We compute $\operatorname{Pr}_{\hat{G}, \hat{\theta}}(y \mid \mathbf{P}(x))$ and aggregate our predictions.</li>
</ol>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The top plots are for EleutherAI models of sizes $\in{125 \mathrm{M}, 1.3 \mathrm{~B}, 6 \mathrm{~B}, 20 \mathrm{~B}}$ and the bottom plots are for BLOOM models of sizes $\in{560 \mathrm{M}, 1.7 \mathrm{~B}, 7.1 \mathrm{~B}, 175 \mathrm{~B}}$. The left plots show the conditional entropy metric $H(y \mid \hat{y})$ as a function of model size. Lines represent different prompts $p$ with $k={0,2,4,8}$ in-context examples and AMA prompt-chains without aggregation. The right plots show the conditional entropy as we aggregate predictions over an increasing number of AMA prompt-chains, with both the majority vote (MV) and weak supervision (WS) aggregation strategies for the GPT-J-6B and BLOOM 7.1B models. All plots are over RTE and each k-shot point is the average of 4 seeds.</p>
<p>The key insight is that the inverse covariance matrix on $V, \Sigma^{-1}$, is graph-structured, meaning that $\Sigma_{i j}^{-1}=0$ iff $p_{i}(x)$ and $p_{j}(x)$ are conditionally independent. This property yields systems of equations on $V$ from which we can recover dependencies and accuracies, without any training. WS hence improves the reliability of aggregation.</p>
<h1>4 Information Flow in AMA</h1>
<p>Before evaluating end-to-end quality, we look at a simple information theoretic metric to understand the contributions of the individual components - $\mathbf{P}$ and $\phi$ - in the prompting procedure.</p>
<p>Information flow metric Specifically, we examine the conditional entropy, $H(y \mid \hat{y})$, which measures the amount of uncertainty remaining in the true label $y$ given a prediction $\hat{y}$. Intuitively, $H(y \mid \hat{y})$ will be low when $\hat{y}$ encodes information relevant to $y$. In our setting, $\hat{y}=\phi(\mathbf{P}(x))$ is dependent on the two components of the prompting procedure, the prompts $\mathbf{P}$ and aggregator $\phi$. The following simple decomposition of $H(y \mid \hat{y})$ enables studying the contribution of each component:</p>
<p>Through the first term $H(y \mid \mathbf{P}(x)), H(y \mid \hat{y})$ depends on the quality and quantity of the individual prompts in $\mathbf{P}(x)$ (since $H(y \mid \mathbf{P}(x)) \leq H(y \mid p(x))$ ). A set of prompts that contains relevant information for $y$ contributes to a low $H(y \mid \hat{y})$. The second term $H(y \mid \hat{y})-H(y \mid \mathbf{P}(x))$ shows that $H(y \mid \hat{y})$ depends on how the aggregation step compresses the information in $\mathbf{P}(x)$ to predict $\hat{y}$. An aggregator $\phi$ that more accurately matches the true $\operatorname{Pr}(y \mid \mathbf{P}(x))$ reduces the information loss in the compression step.
Evaluation We use (1) to evaluate our proposed solution AMA both empirically and theoretically. First considering $H(y \mid \mathbf{P}(x))$, in Figure 4 (Left) we observe AMA outperforms $k$-shot baselines with expected scaling in terms of both individual prompt( $)$-chain quality (as shown by AMA No Agg) and their quantity.</p>
<p>Next we consider the gap term $H(y|\hat{y})-H(y|\mathbf{P}(x))$. It enables us to understand why MV is insufficient: it compresses information from $\mathbf{P}(x)$ according to a specific construction of $\operatorname{Pr}(y,\mathbf{P}(x))$, for which $p_{i}(x) \perp p_{j}(x) \mid y$ for all $i, j \in[m]$, and $\operatorname{Pr}\left(p_{i}(x)=c \mid y=c\right)$ for $c \in \mathcal{Y}$ is a single better-than-random constant across $i$ and $c$. When the true distribution is vastly different-as is common-this misspecification results in a large gap between the optimal $H(y \mid \mathbf{P}(x))$ and $H\left(y \mid \hat{y}<em _mathrm_AMA="\mathrm{AMA">{\mathrm{MV}}\right)$ in Figure 4 (Right). Weak supervision can improve $\phi$ over the standard MV baseline to reduce the information loss $H\left(y \mid \hat{y}</em>(x))$.
In addition to empirical measurements, we can provide a theoretical characterization for the information flow. In Appendix D, we express $H\left(y \mid \hat{y}_{\mathrm{AMA}}\right)$ in terms of the individual prompt accuracies under the standard weak supervision model (i.e., Ising model on $y$ and $\mathbf{P}(x)$ Ratner et al., 2018)).
There has been recent interest in how LLMs improve primarily along the three axes of parameter scale, training data, and compute Kaplan et al., 2020, Hoffmann et al., 2022, Wei et al., 2022c]. In Figure 4, as we increase the number of prompts to be aggregated, the conditional entropy reduces. Prompt aggregation may be another useful axis for understanding LLM scaling performance.}}\right)-H(y \mid \mathbf{P</p>
<h1>5 Results</h1>
<p>We evaluate Ask Me Anything Prompting on 20 popular language benchmarks used in Brown et al. [2020], Sanh et al. [2022]. We report results across 14 unique LLMs including 4 model families (EleutherAI Black et al., 2021, Wang and Komatsuzaki, 2021], OPT Zhang et al., 2022], BLOOM, and T0 Sanh et al., 2022]) spanning 3 orders-of-magnitude in size (125M-175B). We aim to validate whether AMA provides consistent lift across diverse tasks (Section 5.1), works across model families (Section 5.2), and reliably aggregates the predictions across prompts (Section 5.3).</p>
<p>Experimental details We use a diverse set of tasks: SuperGLUE Wang et al., 2019], NLI Mostafazadeh et al., 2017, Nie et al., 2020], classification Zhang et al., 2015, Socher et al., 2013, He and McAuley, 2016], and QA tasks Kasai et al., 2022, Kwiatkowski et al., 2019, Berant et al., 2013, Dua et al., 2019]. For all tasks, we compare to published results of the OpenAI few-shot-prompted GPT3-175B parameter model using the numbers reported in Brown et al. [2020] and, for classification tasks, Zhao et al. [2021]. Brown et al. [2020] uses $k \in[32 . .70]$ depending on the task and Zhao et al. [2021] uses $k \in[1 . .8]$, providing a challenging baseline for comparison.
For AMA we use $3-6$ prompt( $($-chains to generate predictions per input. We model the correlations between prompt-predictions per task, without using any labeled training data, to obtain the final prediction per example via weak supervision (WS). We report both the average performance over the prompt( $($-chains (QA) and with AMA's WS aggregation (QA + WS). We report QA + WS across 5 random seeds for the model. Model details and prompt( $($ chains are in the Appendix.</p>
<h3>5.1 Main Results</h3>
<p>We report benchmark results in Table 1 comparing the open-source GPT-J-6B and few-shot ( $k \in[32 . .70]$ ) GPT3175B. We find that the open-source 6B parameter model exceeds the average few-shot performance of the GPT3-175B model on 15 of 20 benchmarks. Over the 20 tasks, AMA gives an average improvement of $41 \%$ over the 6B parameter model's few-shot $(k=3)$ performance to achieve this.
We find that AMA provides the most lift on tasks where all requisite knowledge is included in the task input (e.g., reading comprehension) and that largely rely on model's natural language understanding (NLU) abilities. The lift is lower on tasks that rely on the LLMs memorized knowledge (e.g. commonsense, closed-book). AMA can help close the gap on knowledge-intensive tasks. The closed-book WebQ task includes simple questions, where the answers are likely seen during pretraining. We find that using an open-ended prompt that asks the LM to generate relevant context, and then prompting the model to answer the original question using the generated context is effective. However, there are limitations as seen on NQ.
We similarly see limitations when tasks cannot rely on the latent knowledge. We observe a small performance gap between model sizes on RealTimeQA, which includes questions that have temporally changing answers that are less likely to be memorized. Similarly, for tasks requiring domain knowledge, e.g. the "Amazon Instant Video" class in the Amazon task, all model-sizes achieve near-0 performance. In such cases, information retrieval may help close the gap. The flexible LLM interface permits asking and answering questions over diverse knowledge sources such as databases or a search engine Nakano et al., 2021]. We provide an extended error analysis Table 1 results in Appendix G.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Neo Few-Shot</th>
<th style="text-align: center;">Neo (QA)</th>
<th style="text-align: center;">Neo (QA + WS)</th>
<th style="text-align: center;">GPT-3 Few-Shot</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"># Params</td>
<td style="text-align: center;">6B</td>
<td style="text-align: center;">6B</td>
<td style="text-align: center;">6B</td>
<td style="text-align: center;">175B</td>
</tr>
<tr>
<td style="text-align: left;">Natural Language Understanding</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">:--</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
</tr>
<tr>
<td style="text-align: left;">BoolQ</td>
<td style="text-align: center;">$66.5_{(3)}$</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">$67.2_{\pm 0.0}$</td>
<td style="text-align: center;">$\mathbf{7 7 . 5}_{(\mathbf{3 2})}$</td>
</tr>
<tr>
<td style="text-align: left;">CB</td>
<td style="text-align: center;">$25.0_{(3)}$</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">$\mathbf{8 3 . 9}_{\pm 0.0}$</td>
<td style="text-align: center;">$82.1_{(32)}$</td>
</tr>
<tr>
<td style="text-align: left;">COPA</td>
<td style="text-align: center;">$77.0_{(3)}$</td>
<td style="text-align: center;">58.2</td>
<td style="text-align: center;">$84.0_{\pm 0.0}$</td>
<td style="text-align: center;">$\mathbf{9 2 . 0}_{(\mathbf{3 2})}$</td>
</tr>
<tr>
<td style="text-align: left;">MultiRC</td>
<td style="text-align: center;">$60.8_{(3)}$</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">$63.8_{\pm 0.0}$</td>
<td style="text-align: center;">$\mathbf{7 4 . 8}_{(\mathbf{3 2})}$</td>
</tr>
<tr>
<td style="text-align: left;">ReCoRD</td>
<td style="text-align: center;">$75.6_{(3)}$</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">$74.4_{\pm 0.0}$</td>
<td style="text-align: center;">$\mathbf{8 9 . 0}_{(\mathbf{3 2})}$</td>
</tr>
<tr>
<td style="text-align: left;">RTE</td>
<td style="text-align: center;">$58.8_{(3)}$</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">$\mathbf{7 5 . 1}_{\pm 0.0}$</td>
<td style="text-align: center;">$72.9_{(32)}$</td>
</tr>
<tr>
<td style="text-align: left;">WSC</td>
<td style="text-align: center;">$36.5_{(3)}$</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">$\mathbf{7 7 . 9}_{\pm 0.0}$</td>
<td style="text-align: center;">$75.0_{(32)}$</td>
</tr>
<tr>
<td style="text-align: left;">WiC</td>
<td style="text-align: center;">$53.3_{(3)}$</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">$\mathbf{6 1 . 0}_{\pm 0.2}$</td>
<td style="text-align: center;">$55.3_{(32)}$</td>
</tr>
<tr>
<td style="text-align: left;">Natural Language Inference</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ANLI R1</td>
<td style="text-align: center;">$32.3_{(3)}$</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">$\mathbf{3 7 . 8}_{\pm 0.2}$</td>
<td style="text-align: center;">$36.8_{(50)}$</td>
</tr>
<tr>
<td style="text-align: left;">ANLI R2</td>
<td style="text-align: center;">$33.5_{(3)}$</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">$\mathbf{3 7 . 9}_{\pm 0.2}$</td>
<td style="text-align: center;">$34.0_{(50)}$</td>
</tr>
<tr>
<td style="text-align: left;">ANLI R3</td>
<td style="text-align: center;">$33.8_{(3)}$</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">$\mathbf{4 0 . 9}_{\pm 0.5}$</td>
<td style="text-align: center;">$40.2_{(50)}$</td>
</tr>
<tr>
<td style="text-align: left;">StoryCloze</td>
<td style="text-align: center;">$51.0_{(3)}$</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">$\mathbf{8 7 . 8}_{\pm 0.0}$</td>
<td style="text-align: center;">$87.7_{(70)}$</td>
</tr>
<tr>
<td style="text-align: left;">Classification</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">AGNews</td>
<td style="text-align: center;">$74.5_{(3)}$</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">$\mathbf{8 6 . 4}_{\pm 0.0}$</td>
<td style="text-align: center;">$79.1_{(8)}$</td>
</tr>
<tr>
<td style="text-align: left;">Amazon</td>
<td style="text-align: center;">$62.5_{(3)}$</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">$\mathbf{6 8 . 2}_{\pm 0.0}$</td>
<td style="text-align: center;">$41.9_{(8)}$</td>
</tr>
<tr>
<td style="text-align: left;">DBPedia</td>
<td style="text-align: center;">$50.7_{(3)}$</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">$\mathbf{8 3 . 9}_{\pm 0.0}$</td>
<td style="text-align: center;">$83.2_{(8)}$</td>
</tr>
<tr>
<td style="text-align: left;">SST</td>
<td style="text-align: center;">$64.9_{(3)}$</td>
<td style="text-align: center;">94.5</td>
<td style="text-align: center;">$\mathbf{9 5 . 7}_{\pm 0.0}$</td>
<td style="text-align: center;">$95.6_{(8)}$</td>
</tr>
<tr>
<td style="text-align: left;">Question-Answering</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">DROP</td>
<td style="text-align: center;">$32.3_{(3)}$</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">$\mathbf{5 1 . 6}_{\pm 0.0}$</td>
<td style="text-align: center;">$36.5_{(20)}$</td>
</tr>
<tr>
<td style="text-align: left;">NQ</td>
<td style="text-align: center;">$13.7_{(3)}$</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">$19.6_{\pm 0.0}$</td>
<td style="text-align: center;">$\mathbf{2 9 . 9}_{(64)}$</td>
</tr>
<tr>
<td style="text-align: left;">RealTimeQA</td>
<td style="text-align: center;">$34.7_{(3)}$</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">$\mathbf{3 6 . 0}_{\pm 0.0}$</td>
<td style="text-align: center;">$35.4_{(1)}$</td>
</tr>
<tr>
<td style="text-align: left;">WebQs</td>
<td style="text-align: center;">$29.1_{(3)}$</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;">$\mathbf{4 4 . 1}_{\pm 0.0}$</td>
<td style="text-align: center;">$41.5_{(64)}$</td>
</tr>
</tbody>
</table>
<p>Table 1: AMA results for the GPT-J-6B parameter model [Black et al., 2021] compared to the few-shot GPT3-175B. The GPT-175B numbers are as reported in Brown et al. [2020], Zhao et al. [2021], where the numbers of in-context examples is in parentheses. Note that prompts can <em>abstain</em> from predicting, which can lead to lower average numbers for QA, including on COPA and StoryCloze. For the question-answering tasks and ReCoRD, we report the majority vote aggregation, as using WS is complex with the open-ended output space. The same results for the BLOOM 7.1B parameter model are in Appendix 3.</p>
<h3>5.2 Evaluation across Models</h3>
<p>Benchmark results We evaluate the lift from AMA over out-of-the-box few-shot performance across different sizes of four open-source LMs (EleutherAI, OPT, BLOOM, and T0) across 7 tasks (4 NLU, 2 NLI, 1 classification). In this analysis, we want to understand the effectiveness of AMA’s $\operatorname{prompt()}$-chains reformattings across models and report the average prompt performance over the 3-6 $\operatorname{prompt()}$-chains used per task. EleutherAI, OPT, and BLOOM are GPT models, while T0 is obtained by explicitly fine-tuning a T5 LM [Raffel et al., 2019] on prompt-input-output tuples.</p>
<p>Excitingly, the AMA $\operatorname{prompt()}$-chains apply quite generally. We see a $10.2\% \pm 6.1\%$ absolute ( $21.4 \% \pm 11.2\%$ relative) lift on average across models and tasks (see Figure 5a (a)). We observe the absolute lift increases with model size and levels out, however we note that there are few-models per size grouping. The average absolute (relative) lift by model family (across tasks and sizes) is $11.0\%$ ( $24.4 \%$ ) for EleutherAI, $11.0\%$ ( $23.4 \%$ ) for BLOOM, and $11.9\%$ ( $22.7 \%$ ) for OPT, and $2.9 \%$ ( $8.3 \%$ ) for T0. We hypothesize the lower lift on T0 arises because the model was fine-tuned on zero-shot prompts, which may compromise its in-context learning abilities.</p>
<p>Diagnostics for understanding AMA lift To further understand why models see different degrees lift, we create a set of diagnostic tasks that correspond to the steps in $\operatorname{prompt()}$-chains. The diagnostics measure four basic operations —question generation, answer generation, answer selection, and extraction. For each operation, we create 1-3 tasks with 50 manually-labeled samples per task. See Appendix E for task details.</p>
<p>We measure the average performance across each operation across different sizes of models in the four families (EleutherAI, OPT, BLOOM, and T0). We group models and sizes into four buckets of T0 (3B parameters) and GPT models ( $&lt;1$ B, 1B, and $6-7$ B parameters). Figure 5b shows results where the buckets are ordered by their average AMA lift across the 7 tasks from Section 5.2, meaning T0 (3B) sees the least lift while $6-7$ B GPT models realize the most lift. We find that overall, models with higher performance across the four operations see more lift with AMA. T0 performs poorly on the generative tasks, indicating the importance of text and question generation for AMA.</p>
<h3>5.3 Evaluation against other aggregation methods</h3>
<p>We compare our WS aggregation approach with the standard unsupervised approach, majority vote (MV), on $\operatorname{prompt()}$-chains. We find that AMA can achieve up to 8.7 points of lift over MV, and does not do worse than MV on</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Evaluation across model sizes for diagnostics and benchmarks. We report the absolute lift from AMA over few-shot ( $k=3$ ) performance, averaged over 7 tasks with $95 \%$ confidence intervals (Left). Diagnostic plots are ordered by the amount of lift models of the size-category see on 7 the benchmarks (Right).</p>
<p>16 out of 20 tasks. On the remaining 4 tasks, we perform worse than MV by at most 1.0 points. We also examine the effect of modeling dependencies in WS. We find that on 9 tasks, our approach recovers dependencies in the data (rather than assuming conditionally independent $\mathbf{P}(x)$ ), which improves performance by up to 9.6 points and an average of 2.2 points. We provide more details and evaluation against labeled data baselines in Table 5 (Appendix B.3).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">CB</th>
<th style="text-align: center;">WIC</th>
<th style="text-align: center;">WSC</th>
<th style="text-align: center;">RTE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">T0 (3B) Mean</td>
<td style="text-align: center;">45.4</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">64.6</td>
</tr>
<tr>
<td style="text-align: left;">T0 (3B) 10 Formats MV</td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">60.6</td>
</tr>
<tr>
<td style="text-align: left;">T0 (3B) AMA MV</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">49.5</td>
</tr>
<tr>
<td style="text-align: left;">T0 (3B) 10 Formats WS</td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">69.7</td>
</tr>
<tr>
<td style="text-align: left;">T0 (3B) AMA WS</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">66.4</td>
<td style="text-align: center;">59.2</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance of T0 as reported in Sanh et al. [2022] compared to majority vote (MV) and weak supervision (WS) over 10 different prompt formats in Prompt-Source. When using the Prompt-Source prompts, the average lift across tasks is 3.6 points for MV and 6.1 points for WS.
Next, we evaluate T0 on zero-shot prompts from the public PromptSource [Bach et al., 2022], which are better aligned with how this model has been trained. Specifically, we take 10 unique PromptSource prompts for CB, WIC, WSC and RTE respectively, and find that aggregating with MV yields an average lift of 3.7 accuracy points and aggregating with WS gives an average lift of 6.1 accuracy points (see Table 2).</p>
<h1>6 Conclusion</h1>
<p>In this work, we introduce Ask Me Anything Prompting which (1) scalably obtains multiple prompts given a task input and (2) combines the intermediate answers to these prompts using weak supervision to give the final prediction. The steps in AMA stem from our observations on the effectiveness of open-ended questions over restrictive prompts, and the ability to model the varying accuracies and dependencies across a collection of prompts using weaksupervision. Overall, AMA provides lift across four language model families and across model sizes ranging from 125M-175B parameters. Most excitingly, we find that AMA enables a 30x smaller LM to exceed the average performance of few-shot GPT3-175B averaged across 20 popular language benchmarks. Several LM applications involve private data or require operating over large amounts of data - for these applications, using APIs to access closedsource models or hosting large models locally is challenging. We hope the strategies in AMA and subsequent work help enable such applications.</p>
<h2>7 Reproducibility Statement</h2>
<p>We release prompts and code for reproducing all benchmark results for few-shot and AMA prompting, and our diagnostic evaluation splits here: https://github.com/HazyResearch/ama_prompting.</p>
<h2>8 Ethics Statement</h2>
<p>We intend for AMA to aid practitioners in their exploration and use of LLMs-especially smaller, open-source LLMs. However, we recognize that AMA could be used to perform harmful or unethical tasks. AMA is a proof-of-concept;</p>
<p>it has error-modes and we recognize the inherent risks to using LLMs. Detailed discussions of these risks are in Bommasani et al. [2021], Weidinger et al. [2021].</p>
<h1>Acknowledgements</h1>
<p>The computation required in this work was provided by Together Computer (https://together.xyz/). We are grateful to the Numbers Station (https://numbersstation.ai/), Snorkel (https://snorkel.ai/), Stanford Center for Research on Foundation Models (https://crfm.stanford.edu/), and Stanford HAI (https: //hai.stanford.edu/) organizations for the resources that supported this work. We thank Karan Goel, Maya Varma, Joel Johnson, Sabri Eyuboglu, Kawin Ethayarajh, Niladri Chatterji, Neha Gupta, Alex Ratner, and Rishi Bommasani for their helpful feedback and discussions. We gratefully acknowledge the support of DARPA under Nos. FA86501827865 (SDH) and FA86501827882 (ASED); NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No. N000141712266 (Unifying Weak Supervision); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Swiss Re, Brown Institute for Media Innovation, Department of Defense (DoD) through the National Defense Science and Engineering Graduate Fellowship (NDSEG) Program, Fannie and John Hertz Foundation, National Science Foundation Graduate Research Fellowship Program, Texas Instruments, and members of the Stanford DAWN project: Teradata, Facebook, Google, Ant Financial, NEC, VMWare, and Infosys. SA is supported by a Stanford Graduate Fellowship. LO is supported by an Intelligence Community Postdoctoral Fellowship. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government.</p>
<h2>References</h2>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.
Tony Z. Zhao, 1 Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In arXiv:2102.09690v2, 2021. URL https://arxiv.org/pdf/2102.09690. pdf.
Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi Choi, and Luke Zettlemoyer. Surface form competition: Why the highest probability answer isn't always right. arXiv:2104.08315, 2021.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.
Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. In The Tenth International Conference on Learning Representations, 2022.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022a.
Andrew K. Lampinen, Ishita Dasgupta, Stephanie C. Y. Chan, Kory Mathewson, Michael Henry Tessler, Antonia Crwswell, James L. McClelland, Jane X. Wang, and Felix Hill. Can language models learn from explanations in context?, 2022.
Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. Reframing instructional prompts to gptk's language. arXiv preprint arXiv:2109.07830, 2021.
Tongshuang Wu, Michael Terry, and Carrie Jun Cai. Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts. In CHI Conference on Human Factors in Computing Systems, pages 1-22, 2022.</p>
<p>Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. Maieutic prompting: Logically consistent reasoning with recursive explanations, 2022. URL https://arxiv. org/abs/2205.11822.</p>
<p>Zhengbao Jiang, Frank F. Xu, Jun Araki Araki, and Graham Neubig. How can we know what language models know? Transactions of the Association for Computational Linguistics (TACL), 2020.
Timo Schick and Hinrich Schütze. It's not just size that matters: Small language models are also few-shot learners. arXiv:2009.07118v2, 2021.
Alexander Ratner, Stephen H. Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher Ré . Snorkel: Rapid training data creation with weak supervision. Proceedings of the VLDB Endowment, 11(3):269-282, nov 2017. doi:10.14778/3157794.3157797. URL https://doi.org/10.14778\%2F3157794.3157797.
Paroma Varma, Frederic Sala, Ann He, Alexander Ratner, and Christopher Re. Learning dependency structures for weak supervision models. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 6418-6427. PMLR, 09-15 Jun 2019. URL https://proceedings.mlr.press/v97/varma19a.html.
Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https://doi.org/10.5281/zenodo. 5297715. If you use this software, please cite it using these metadata.
Ben Wang and Aran Komatsuzaki. Gpt-j-6b: A 6 billion parameter autoregressive language model, 2021.
EleutherAI. URL https://www.eleuther.ai/.
Bigscience large open-science open-access multilingual language model, 2022. URL https://huggingface.co/ bigscience/bloom.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022. URL https://arxiv.org/abs/2205.01068.</p>
<p>Simran Arora and Christopher Ré. Can foundation models help us achieve perfect secrecy?, 2022. URL https: //arxiv.org/abs/2205.13722.
Avanika Narayan, Ines Chami, Laurel Orr, and Christopher Ré. Can foundation models wrangle your data? arXiv preprint arXiv:2205.09911, 2022.
Jared Kaplan, Sam McClandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv:2001.08361, 2020.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, et al. Benchmarking generalization via in-context instructions on 1,600+ language tasks. arXiv, 2022a.
Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Le Quoc V. Finetuned language models are zero-shot learners. arXiv:2109.01652, 2022b.
Pruthvi Patel, Swaroop Mishra, Mihir Parmar, and Chitta Baral. Is a question decomposition unit all we need?, 2022. URL https://arxiv.org/abs/2205.12538.
Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712, 2022.
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good incontext examples for gpt-3?, 2021.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv:2110.14168v2, 2021. URL https://arxiv.org/pdf/2110.14168.pdf.
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Self-taught reasoner bootstrapping reasoning with reasoning. arXiv:2203.14465v2, 2022. URL https://arxiv.org/pdf/2203.14465.pdf.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le Le, Ed H. Cho, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. 2022b.
Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel Selsam, and Christopher Ré. Data programming: Creating large training sets, quickly. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. URL https://proceedings. neurips.cc/paper/2016/file/6709e8d64a5f47269ed5cea9f625f7ab-Paper.pdf.</p>
<p>Alexander Ratner, Braden Hancock, Jared Dunnmon, Frederic Sala, Shreyash Pandey, and Christopher Ré. Training complex models with multi-task weak supervision, 2018. URL https://arxiv.org/abs/1810.02840.
Daniel Fu, Mayee Chen, Frederic Sala, Sarah Hooper, Kayvon Fatahalian, and Christopher Re. Fast and three-rious: Speeding up weak supervision with triplet methods. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 3280-3291. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/fu20a.html.
Ryan Smith, Jason A. Fries, Braden Hancock, and Stephen H. Bach. Language models in the loop: Incorporating prompting into weak supervision. arXiv:2205.02318v1, 2022.
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. arXiv preprint 1905.00537, 2019.
Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In NIPS, 2015.
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling, 2021. URL https://arxiv.org/abs/2101.00027.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022. URL https: //arxiv.org/abs/2203.15556.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models, 2022c. URL https://arxiv.org/abs/2206. 07682.</p>
<p>Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and James Allen. Lsdsem 2017 shared task: The story cloze test. In Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourselevel Semantics, pages 46-51, 2017.
Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial nli: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2020.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631-1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/ D13-1170.</p>
<p>Ruining He and Julian McAuley. Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. In proceedings of the 25th international conference on world wide web, pages 507-517, 2016.</p>
<p>Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir Radev, Noah A Smith, Yejin Choi, and Kentaro Inui. Realtime qa: What's the answer right now? arXiv preprint arXiv:2207.13332, 2022.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, MingWei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 15331544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://www. aclweb.org/anthology/D13-1160.
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proc. of NAACL, 2019.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted question-answering with human feedback. CoRR, abs/2112.09332, 2021. URL https://arxiv.org/abs/2112.09332.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. CoRR, abs/1910.10683, 2019. URL http://arxiv.org/abs/1910.10683.</p>
<p>Stephen H Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, et al. Promptsource: An integrated development environment and repository for natural language prompts. arXiv preprint arXiv:2202.01279, 2022.
Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359, 2021.
HuggingFace, Nov 2021. URL https://huggingface.co/models.
OpenAI, Nov 2021. URL https://openai.com/api/.
Emmanuel J. Candès, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis? J. ACM, 58(3), jun 2011. ISSN 0004-5411. doi:10.1145/1970392.1970395. URL https://doi.org/10.1145/1970392.1970395.
Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https: //aclanthology.org/W04-1013.
Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for squad. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784-789, 2018.
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924-2936, 2019.
Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank: Investigating projection in naturally occurring discourse. In proceedings of Sinn und Bedeutung, volume 23, pages 107-124, 2019.
Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In AAAI spring symposium: logical formalizations of commonsense reasoning, pages $90-95,2011$.
Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 252-262, 2018.
Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. ReCoRD: Bridging the gap between human and machine commonsense reading comprehension. arXiv preprint 1810.12885, 2018.
Mohammad Taher Pilehvar and Jose Camacho-Collados. Wic: the word-in-context dataset for evaluating contextsensitive meaning representations. arXiv preprint arXiv:1808.09121, 2018.
Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth international conference on the principles of knowledge representation and reasoning, 2012.</p>
<h1>A Experiment Details</h1>
<p>We use A100 NVidia GPUs to run all experiments.</p>
<h2>A. 1 Models</h2>
<p>We evaluate over 4 model families: T0, BLOOM, EleutherAI, OPT, and GPT3. In our evaluations, we use the following model family variants: EleutherAI (GPT-Neo-125M, GPT-Neo-1.3B, GPT-J-6B, GPT-NeoX-20B), BLOOM (BLOOM-560M, BLOOM-1.7B, BLOOM-7.1B, BLOOM-176B), OPT(OPT-125M, OPT-1.3B, OPT-6.7B, OPT-13B, OPT-175B), T0 (T0-3B), and GPT-3 (davinci). We download T0, BLOOM, OPT, and EleutherAI models from the</p>
<p>HuggingFace Model Hub [HuggingFace, 2021]. All inference calls to the OpenAI Davinci model were made using the OpenAI API davinci endpoint [OpenAI, 2021], the original GPT-3 175B parameter model used in [Brown et al., 2020]. We access these models by passing our input prompts to the endpoint for a per-sample fee.</p>
<h1>A. 2 Metrics</h1>
<p>For RealTimeQA, the reported GPT-3 performance in Kasai et al. [2022] is reported over the text-davinci-002 API endpoint. Given that all our GPT-3 evaluations are over davinci, we re-evaluate the GPT-3 performance on RealTimeQA using the davinci endpoint and the few-shot prompt from RealTimeQA ${ }^{4}$.</p>
<p>We follow the metrics used in Brown et al. [2020]. All tasks are scored using matching accuracy except for DROP/RealTimeQA that use text f1, WebQ/NQ that use span overlap accuracy, and MultiRC that uses f1a accuracy.</p>
<h2>A. 3 Weak Supervision</h2>
<p>For each task, we use an unlabeled dataset constructed from the test set as well as 1000 samples from the training set (ignoring the labels). We run the structure learning part of the weak supervision algorithm (for $\hat{G}$ ) with the default parameters from Varma et al. [2019]. If the recovered sparse matrix has all entries greater than 1, we pass in an empty edgeset to the next step of learning $\hat{\theta}$ (e.g., data is too noisy to learn structure from); otherwise, we pass in the edge with the highest value in the sparse matrix.</p>
<h2>B Additional Results</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">BLOOM Few-Shot</th>
<th style="text-align: center;">BLOOM (QA)</th>
<th style="text-align: center;">BLOOM (QA + WS)</th>
<th style="text-align: center;">GPT-3 Few-Shot</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"># Params</td>
<td style="text-align: center;">7.1B</td>
<td style="text-align: center;">7.1B</td>
<td style="text-align: center;">7.1B</td>
<td style="text-align: center;">175B</td>
</tr>
<tr>
<td style="text-align: center;">Natural Language Understanding</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BoolQ</td>
<td style="text-align: center;">$47.0_{(3)}$</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">$67.9_{ \pm 0.0}$</td>
<td style="text-align: center;">77.5 $5_{(32)}$</td>
</tr>
<tr>
<td style="text-align: center;">CB</td>
<td style="text-align: center;">$41.1_{(3)}$</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">$77.6_{ \pm 0.9}$</td>
<td style="text-align: center;">82.1 $1_{(32)}$</td>
</tr>
<tr>
<td style="text-align: center;">COPA</td>
<td style="text-align: center;">$65.0_{(3)}$</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">$74.0_{ \pm 0.5}$</td>
<td style="text-align: center;">92.0 $0_{(32)}$</td>
</tr>
<tr>
<td style="text-align: center;">MultiRC</td>
<td style="text-align: center;">$21.3_{(3)}$</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">$59.7_{ \pm 0.0}$</td>
<td style="text-align: center;">74.8 $8_{(32)}$</td>
</tr>
<tr>
<td style="text-align: center;">ReCoRD</td>
<td style="text-align: center;">$71.6_{(3)}$</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">$69.8_{ \pm 0.0}$</td>
<td style="text-align: center;">89.0 $0_{(32)}$</td>
</tr>
<tr>
<td style="text-align: center;">RTE</td>
<td style="text-align: center;">$53.1_{(3)}$</td>
<td style="text-align: center;">60.6</td>
<td style="text-align: center;">$67.5_{ \pm 0.0}$</td>
<td style="text-align: center;">72.9 $9_{(32)}$</td>
</tr>
<tr>
<td style="text-align: center;">WiC</td>
<td style="text-align: center;">$51.3_{(3)}$</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">61.4 $4_{ \pm 0.0}$</td>
<td style="text-align: center;">$55.3_{(32)}$</td>
</tr>
<tr>
<td style="text-align: center;">WSC</td>
<td style="text-align: center;">$63.5_{(3)}$</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">$64.4_{ \pm 0.0}$</td>
<td style="text-align: center;">75.0 $0_{(32)}$</td>
</tr>
<tr>
<td style="text-align: center;">Natural Language Inference</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ANLI R1</td>
<td style="text-align: center;">$34.9_{(3)}$</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">$31.5_{ \pm 0.0}$</td>
<td style="text-align: center;">36.8 $8_{(50)}$</td>
</tr>
<tr>
<td style="text-align: center;">ANLI R2</td>
<td style="text-align: center;">$33.6_{(3)}$</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">35.1 $1_{ \pm 0.0}$</td>
<td style="text-align: center;">$34.0_{(50)}$</td>
</tr>
<tr>
<td style="text-align: center;">ANLI R3</td>
<td style="text-align: center;">$32.3_{(3)}$</td>
<td style="text-align: center;">34.9</td>
<td style="text-align: center;">$37.1_{ \pm 0.7}$</td>
<td style="text-align: center;">40.2 $2_{(50)}$</td>
</tr>
<tr>
<td style="text-align: center;">StoryCloze</td>
<td style="text-align: center;">$46.7_{(3)}$</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">$79.0_{ \pm 0.0}$</td>
<td style="text-align: center;">87.7 $7_{(70)}$</td>
</tr>
<tr>
<td style="text-align: center;">Classification</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">AGNews</td>
<td style="text-align: center;">$68.3_{(3)}$</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">81.7 $7_{ \pm 0.3}$</td>
<td style="text-align: center;">$79.1_{(8)}$</td>
</tr>
<tr>
<td style="text-align: center;">Amazon</td>
<td style="text-align: center;">$51.9_{(3)}$</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">65.2 $2_{ \pm 0.0}$</td>
<td style="text-align: center;">$41.9_{(8)}$</td>
</tr>
<tr>
<td style="text-align: center;">DBPedia</td>
<td style="text-align: center;">$72.3_{(3)}$</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">$70.5_{ \pm 0.0}$</td>
<td style="text-align: center;">83.2 $2_{(8)}$</td>
</tr>
<tr>
<td style="text-align: center;">SST2</td>
<td style="text-align: center;">$56.4_{(3)}$</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">$91.0_{ \pm 0.0}$</td>
<td style="text-align: center;">95.6 $6_{(8)}$</td>
</tr>
<tr>
<td style="text-align: center;">Question Answering</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DROP</td>
<td style="text-align: center;">$30.5_{(3)}$</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">67.9 $9_{ \pm 0.0}$</td>
<td style="text-align: center;">$36.5_{(20)}$</td>
</tr>
<tr>
<td style="text-align: center;">NQ</td>
<td style="text-align: center;">$12.1_{(3)}$</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">$15.1_{ \pm 0.0}$</td>
<td style="text-align: center;">29.9 $9_{(64)}$</td>
</tr>
<tr>
<td style="text-align: center;">RealTimeQA</td>
<td style="text-align: center;">$21.8_{(3)}$</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">$29.0_{ \pm 0.0}$</td>
<td style="text-align: center;">35.4 $4_{(1)}$</td>
</tr>
<tr>
<td style="text-align: center;">WebQs</td>
<td style="text-align: center;">$26.9_{(3)}$</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: center;">$34.8_{ \pm 0.0}$</td>
<td style="text-align: center;">41.5 $5_{(64)}$</td>
</tr>
</tbody>
</table>
<p>Table 3: AMA results for the BLOOM-7.1B parameter model compared to the few-shot GPT3-175B. The GPT-175B numbers are as reported in Brown et al. [2020], where the numbers of shots is in parentheses, and the classification task baselines are from from Zhao et al. [2021].</p>
<h2>B. 1 BLOOM Model Results</h2>
<p>In Table 3, we provide results using the BLOOM-7.1B parameter model over all 20 benchmarks. We observe consistent lift over few-shot performance using AMA, though the performance remains below that of the comparably sized GPT-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Model</th>
<th>GPT-J Few-Shot</th>
<th>GPT-J Few-Shot</th>
<th>GPT-J Few-Shot</th>
<th>GPT-J AMA</th>
</tr>
</thead>
<tbody>
<tr>
<td>Aggregation</td>
<td>Average</td>
<td>Majority Vote</td>
<td>Weak Supervision</td>
<td>Weak Supervision</td>
</tr>
<tr>
<td>Natural Language Understanding</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>CB</td>
<td>23.8</td>
<td>17.9</td>
<td>50.0</td>
<td>83.9</td>
</tr>
<tr>
<td>RTE</td>
<td>53.5</td>
<td>53.1</td>
<td>54.2</td>
<td>75.1</td>
</tr>
<tr>
<td>WSC</td>
<td>46.2</td>
<td>38.5</td>
<td>38.5</td>
<td>77.9</td>
</tr>
<tr>
<td>COPA</td>
<td>80.0</td>
<td>81.0</td>
<td>81.0</td>
<td>84.0</td>
</tr>
<tr>
<td>Natural Language Inference</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>ANLI R1</td>
<td>33.4</td>
<td>33.5</td>
<td>33.5</td>
<td>37.8</td>
</tr>
<tr>
<td>ANLI R2</td>
<td>33.2</td>
<td>32.9</td>
<td>32.2</td>
<td>37.9</td>
</tr>
<tr>
<td>ANLI R3</td>
<td>35.4</td>
<td>36.5</td>
<td>34.6</td>
<td>40.2</td>
</tr>
<tr>
<td>Classification</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>AGNews</td>
<td>70.3</td>
<td>70.7</td>
<td>75.0</td>
<td>86.4</td>
</tr>
<tr>
<td>Amazon</td>
<td>61.9</td>
<td>62.4</td>
<td>62.5</td>
<td>68.2</td>
</tr>
</tbody>
</table>
<p>Table 4: Results from applying prompt aggregation via Majority Vote and Weak Supervision to 3 random few-shot ( $k=3$ ) prompts. Here we apply no prompt reformatting to the proposed AMA QA template.</p>
<p>J-6B parameter model reported in Table 5.1. We note that the few-shot results for BLOOM 7.1B are also often lower than the few-shot results for GPT-J-6B.</p>
<h1>B. 2 AMA Ablations</h1>
<p>Here we extend the observations in Section 3 on additional tasks.
We study the degree to which both prompt re-formatting and aggregation are required to achieve high quality. Specifically, we produce 3 few-shot prompts (each with a different set of $k=3$ in-context prompt examples), prompt using each, and aggregate the results using majority vote and weak supervision. We reiterate that the proposed AMA QA reformatting is not applied. We find that aggregation alone leaves large performance gaps. Aggregation alone is useful compared to the average performance, however aggregation and re-formatting are both critical and complementary in yielding an effective prompting solution.</p>
<h2>B. 3 Weak Supervision Ablations</h2>
<p>Comparison to other aggregation baselines Table 5 compares AMA's aggregation method against several other baselines for aggregating prompt()-chains, including majority vote. We compare against weighted majority vote (WMV), where we use labeled data to weight according to each prompt's accuracy by constructing $\phi_{\text {WMV }}(\mathbf{P}(x))=$ $\sum_{i=1}^{m} \exp \left(-\eta \varepsilon_{i}\right) \mathbb{1}\left{p_{i}(x)=y\right} . \varepsilon_{i}$ is the error of prompt $p_{i}$ on a training set of 1000 examples, and $\eta$ is a temperature hyperparameter, for which we perform a sweep over $[0.25,0.5,1,2,4,8,16,32]$ using a $20 \%$ validation split. We also compare against a simple strategy of using the prompt that performs the best on the labeled set of data (Pick Best). Finally, AMA (no deps) is our method when we pass in an empty edgeset to the algorithm in <em>Ratner et al. (2018)</em>.</p>
<p>Varying amount of additional data We study the effect of varying the amount of additional unlabeled training data that is used in learning the probabilistic graphical model on $y, \mathbf{P}(x)$. On three tasks (RTE, WSC, and AGNews) averaged over 5 runs, we run AMA with $100 \%, 50 \%, 20 \%, 10 \%$, and $0 \%$ of the additional dataset while still evaluating on the fixed test dataset. Figure 6 shows AMA's accuracy versus the amount of additional unlabeled data used. We find that even without any of the additional data, average accuracy does not decrease on WSC or AGNews, and only decreases by 0.4 points on RTE, still outperforming GPT3-175B few-shot. This suggests that the additional data is not necessary for AMA's performance.</p>
<p>Latency of Weak Supervsion Over RTE, WSC, and AGNews, we find that WS (both learning the graphical model and aggregating outputs) takes an average of 13.0 seconds when dependencies are not modeled. When dependencies are modeled in RTE (as dependencies are ignored in WSC and AGNews because they both exhibit dense recovered structured matrices), the algorithm takes an average of 84.3 seconds to run. As a point of comparison, we include Table 6 which shows the time in seconds for running inference with the GPT-J-6B model on the same tasks. The latency introduced by running weak supervision is comparatively low.</p>
<table>
<thead>
<tr>
<th></th>
<th># Prompts</th>
<th>Avg</th>
<th>MV</th>
<th>WMV</th>
<th>Pick Best</th>
<th>AMA (no dep)</th>
<th>AMA (WS)</th>
</tr>
</thead>
<tbody>
<tr>
<td>No labels:</td>
<td></td>
<td></td>
<td>$\checkmark$</td>
<td></td>
<td></td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>Natural Language Understanding</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>WSC</td>
<td>3</td>
<td>74.7</td>
<td>77.8</td>
<td>77.8</td>
<td>75.0</td>
<td>$77.8_{\pm 0.0}$</td>
<td>$\mathbf{7 7 . 8}_{\pm 0.0}$</td>
</tr>
<tr>
<td>WiC</td>
<td>5</td>
<td>59.0</td>
<td>61.3</td>
<td>60.9</td>
<td>60.0</td>
<td>$60.8_{\pm 0.0}$</td>
<td>$\mathbf{6 1 . 3}_{\pm 0.2}$</td>
</tr>
<tr>
<td>RTE</td>
<td>5</td>
<td>61.4</td>
<td>66.0</td>
<td>71.4</td>
<td>62.0</td>
<td>$65.1_{\pm 0.5}$</td>
<td>$\mathbf{7 5 . 1}_{\pm 0.0}$</td>
</tr>
<tr>
<td>CB</td>
<td>3</td>
<td>83.3</td>
<td>82.1</td>
<td>82.1</td>
<td>83.9</td>
<td>$82.1_{\pm 0.0}$</td>
<td>$\mathbf{8 3 . 9}_{\pm 0.0}$</td>
</tr>
<tr>
<td>MultiRC</td>
<td>3</td>
<td>58.8</td>
<td>63.8</td>
<td>63.4</td>
<td>63.4</td>
<td>$63.7_{\pm 0.0}$</td>
<td>$\mathbf{6 3 . 8}_{\pm 0.0}$</td>
</tr>
<tr>
<td>BoolQ</td>
<td>5</td>
<td>64.9</td>
<td>65.9</td>
<td>67.2</td>
<td>$\mathbf{6 8 . 3}$</td>
<td>$65.9_{\pm 0.0}$</td>
<td>$67.2_{\pm 0.0}$</td>
</tr>
<tr>
<td>COPA</td>
<td>4</td>
<td>58.3</td>
<td>$\mathbf{8 5 . 0}$</td>
<td>82.0</td>
<td>82.0</td>
<td>$84.0_{\pm 0.0}$</td>
<td>$84.0_{\pm 0.0}$</td>
</tr>
<tr>
<td>Natural Language Inference</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>ANLI R1</td>
<td>5</td>
<td>34.6</td>
<td>37.6</td>
<td>36.1</td>
<td>36.8</td>
<td>$37.4_{\pm 1.0}$</td>
<td>$\mathbf{3 7 . 8}_{\pm 0.2}$</td>
</tr>
<tr>
<td>ANLI R2</td>
<td>5</td>
<td>35.4</td>
<td>36.3</td>
<td>36.0</td>
<td>36.0</td>
<td>$\mathbf{3 8 . 7}_{\pm 0.4}$</td>
<td>$37.9_{\pm 0.2}$</td>
</tr>
<tr>
<td>ANLI R3</td>
<td>5</td>
<td>37.0</td>
<td>39.0</td>
<td>38.4</td>
<td>38.4</td>
<td>$39.6_{\pm 0.9}$</td>
<td>$\mathbf{4 0 . 9}_{\pm 0.5}$</td>
</tr>
<tr>
<td>StoryCloze</td>
<td>6</td>
<td>76.3</td>
<td>$\mathbf{8 7 . 9}$</td>
<td>81.8</td>
<td>81.8</td>
<td>$82.2_{\pm 0.0}$</td>
<td>$87.8_{\pm 0.0}$</td>
</tr>
<tr>
<td>Classification</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>DBPedia</td>
<td>3</td>
<td>81.4</td>
<td>$\mathbf{8 4 . 1}$</td>
<td>83.9</td>
<td>82.2</td>
<td>$83.9_{\pm 0.0}$</td>
<td>$83.9_{\pm 0.0}$</td>
</tr>
<tr>
<td>SST2</td>
<td>3</td>
<td>94.5</td>
<td>95.7</td>
<td>95.7</td>
<td>95.2</td>
<td>$95.7_{\pm 0.0}$</td>
<td>$\mathbf{9 5 . 7}_{\pm 0.0}$</td>
</tr>
<tr>
<td>Amazon</td>
<td>3</td>
<td>67.0</td>
<td>68.6</td>
<td>68.6</td>
<td>67.3</td>
<td>$68.6_{\pm 0.0}$</td>
<td>$\mathbf{6 8 . 6}_{\pm 0.0}$</td>
</tr>
<tr>
<td>AGNews</td>
<td>3</td>
<td>83.7</td>
<td>$\mathbf{8 6 . 5}$</td>
<td>84.2</td>
<td>83.8</td>
<td>$86.4_{\pm 0.0}$</td>
<td>$86.4_{\pm 0.0}$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 5: AMA Aggregation method ablation for the GPT-J-6B parameter model, as well as the number of prompt()-chains used for each task. For ReCoRD, and QA tasks (DROP, WebQs, RealTimeQA, NQ), we use 3 prompts each and use majority vote as our aggregation strategy reported in the (QA + WS) columns of Table 1 and Table 3.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Performance on RTE, WSC, and AGNews averaged over 5 runs when using varying amounts of additional unlabeled training data for estimating $\operatorname{Pr}(y, \mathbf{P}(x))$ in WS.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">Number of Examples</th>
<th style="text-align: center;">Total Inference Cost (seconds)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RTE</td>
<td style="text-align: center;">277</td>
<td style="text-align: center;">8310</td>
</tr>
<tr>
<td style="text-align: left;">WSC</td>
<td style="text-align: center;">104</td>
<td style="text-align: center;">3141</td>
</tr>
<tr>
<td style="text-align: left;">AGNews</td>
<td style="text-align: center;">7600</td>
<td style="text-align: center;">53200</td>
</tr>
</tbody>
</table>
<p>Table 6: Total inference cost in applying the AMA prompt chains to achieve the results in Table 5.1, using the GPT-J-6B model.</p>
<h1>B. 4 Additional AMA Baselines</h1>
<p>Here we compare AMA to Self-Consistency <em>Wang et al. (2022b)</em>, which is particularly relevant in that it also aggregates over multiple prompt outputs without requiring any additional supervised training. Self-Consistency builds on Chain-of-Thought prompting <em>Wei et al. (2022a)</em>, which proposes to guide the LM to generate reasoning paths in addition to the final prediction. We use the exact prompts and overlapping benchmark tasks provided in the Appendix of <em>Wang et al. (2022b)</em>, using GPT-J-6B and report the results in Table 7. For Self-Consistency, we use temperature based sampling as discussed in <em>Wang et al. (2022b)</em>, using temperatures $\in{0.0,0.3,0.5,0.6,0.7}$.
Overall, we observe AMA outperforms Self-Consistency at this model scale. This agrees with the results in <em>Wang et al. (2022b)</em> and <em>Wei et al. (2022a)</em>, which report limited performance improvements for small LMs (&lt;10B).</p>
<table>
<thead>
<tr>
<th>Baseline</th>
<th>Self-Consistency with Chain-of-Thought</th>
<th>Ask Me Anything</th>
</tr>
</thead>
<tbody>
<tr>
<td>Aggregation over # Outputs</td>
<td>5</td>
<td>5</td>
</tr>
<tr>
<td>RTE</td>
<td>47.3</td>
<td>75.1</td>
</tr>
<tr>
<td>BoolQ</td>
<td>63.1</td>
<td>67.2</td>
</tr>
<tr>
<td>ANLI-R1</td>
<td>33.4</td>
<td>37.8</td>
</tr>
</tbody>
</table>
<p>Table 7: Comparison between Self-Consistency <em>Wang et al. (2022b)</em> and AMA using GPT-J-6B and the same number of prompts.
Procedure 1: AMA Aggregation Method
1: Input: Dataset $\mathcal{D}=\left{x_{i}\right}<em i="i">{i=1}^{n}$, collection of $\operatorname{prompt()}$-chains $\mathbf{P}$. Output: Predictions $\left{\hat{y}</em>\right}<em _mathbf_P="\mathbf{P">{i=1}^{n}$.
2: Prompt the LLM with $\mathbf{P}$ to produce $m$ predictions $\mathbf{P}(x)$ per input $x \in \mathcal{D}$, constructing dataset $\mathcal{D}</em>$.
3: Learn $\hat{G}=(V, \hat{E})$ via structure learning on $\mathcal{D}}} \in \mathbb{R}^{n \times m<em _hat_G="\hat{G">{\mathbf{P}}$ (Algorithm 1 in <em>Varma et al. (2019)</em>).
4: Learn $\operatorname{Pr}</em>}, \hat{\theta}}(y, \mathbf{P}(x))$ using $\mathcal{D<em _mathrm_WS="\mathrm{WS">{\mathbf{P}}$ and $\hat{G}$ (Algorithm 1 in <em>Ratner et al. (2018)</em>).
5: Construct aggregator $\phi</em>(x))=\arg \max }}(\mathbf{P<em _hat_G="\hat{G">{y \in \mathcal{Y}} \operatorname{Pr}</em>(x))$.
6: Returns: $\hat{y}}, \hat{\theta}}(y \mid \mathbf{P<em _mathrm_WS="\mathrm{WS">{\mathrm{AMA}}=\phi</em>$.}}(x)$ for all $x \in \mathcal{D</p>
<h1>C Weak Supervision Algorithm</h1>
<p>We briefly explain the weak supervision algorithm used for constructing $\phi_{\mathrm{WS}}$. Weak supervision models learn the latent variable graphical model on the distribution $\operatorname{Pr}(y, \mathbf{P}(x))$ using the dataset $\mathcal{D}$, and aggregate votes using the learned distribution by setting $\phi(x)=\arg \max <em i="i">{y} \operatorname{Pr}(y \mid \mathbf{P}(x))$. Our key insight in our aggregation approach is to parametrize $\operatorname{Pr}(y, \mathbf{P}(x))$ so that we can capture variations in accuracy as well as dependencies if they exist. The overall procedure of our aggregation is in Algorithm 1. Formally, we model $\operatorname{Pr}(y, \mathbf{P}(x))$ as a probabilistic graphical model with dependency graph $G=(V, E)$, where $V={y, \mathbf{P}(x)}$. If $p</em>(x), y\right)$ for each $i \in[m]$.
The algorithm uses $\mathbf{P}(x)$ and $\mathcal{D}$ to first learn the dependency structure $\hat{G}$ among prompts using the approach from Varma et al. (2019). The key insight from that work is that the inverse covariance matrix $\Sigma^{-1}$ over $y$ and $\mathbf{P}(x)$ is graph-structured, meaning that $\Sigma_{i j}^{-1}=0$ iff $p_{i}(x)$ and $p_{j}(x)$ are conditionally independent given $y$. The graph structure means that the inverse covariance over just $\mathbf{P}(x)$ decomposes into sparse and low-rank matrices, which can hence be estimated together using RobustPCA }(x)$ and $p_{j}(x)$ are not conditionally independent given $y$ and the other $\operatorname{prompt()}$-chains, then $\left(p_{i}(x), p_{j}(x)\right) \in E$. $E$ also contains edges $\left(p_{i<em>(Candès et al., 2011)</em>, and the sparse matrix can be used to recover the graph. Next, the algorithm uses the recovered $\hat{G}$ along with $\mathbf{P}(x)$ and $\mathcal{D}$ to learn the accuracies of the prompts with the approach from Ratner et al. (2018). The key insight from that work is to use the sparsity of $\Sigma^{-1}$ to construct a system of equations set equal to 0 that recover the latent accuracy parameters. Once the parameters of the distribution are learned, we can compute $\operatorname{Pr}_{\hat{G}, \hat{\theta}}(y \mid \mathbf{P}(x))$ and aggregate our predictions.</p>
<h2>D Information-Flow Theoretical Result</h2>
<p>In equation 1, we decompose $H(y \mid \hat{y})$ into $H(y \mid \mathbf{P}(x))$ and $H(y \mid \hat{y})-H(y \mid \mathbf{P}(x))$. For AMA, suppose that the weak supervision algorithm exactly recovers $\operatorname{Pr}(y, \mathbf{P}(x))$. That is, $\hat{y}_{\mathrm{AMA}}$ is drawn from $\operatorname{Pr}(\cdot \mid \mathbf{P}(x))$. Then, the second term $H(y \mid \hat{y})-H(y \mid \mathbf{P}(x))$ can be thought of as an irreducible error corresponding to how much information about $y$ is lost in converting $\mathbf{P}(x)$ into an i.i.d. $y^{\prime}$ randomly drawn from $\operatorname{Pr}(\cdot \mid \mathbf{P}(x))$. Since $y^{\prime}$ is more likely to change values when this distribution has high entropy, the second term is correlated with our first term $H(y \mid \mathbf{P}(x))$, the amount of randomness in $\operatorname{Pr}(y \mid \mathbf{P}(x))$. We thus focus on obtaining an expression for $H(y \mid \mathbf{P}(x))$ in terms of individual prompt accuracies.
We assume that $\mathcal{Y}={-1,1}$. We model $\operatorname{Pr}(y, \mathbf{P}(x))$ as a probabilistic graphical model with dependency graph $G=(V, E)$, where $V={y, \mathbf{P}(x)}$. The density of $\operatorname{Pr}(y, \mathbf{P}(x))$ follows the following Ising model commonly used in weak supervision <em>(Ratner et al., 2017; Fu et al., 2020)</em>:</p>
<p>$$
\operatorname{Pr}<em y="y">{G, \theta}(y, \mathbf{P}(x))=\frac{1}{Z} \exp \left(\theta</em>(x)\right)
$$} y+\sum_{i=1}^{m} \theta_{i} p_{i}(x) y+\sum_{(i, j) \in E} \theta_{i j} p_{i}(x) p_{j</p>
<p>where $Z$ is the partition function for normalization and $\left{\theta_{y}, \theta_{i} \forall i \in[m], \theta_{i j} \forall(i, j) \in E\right}$. Each $\theta_{i}$ can be viewed as the strength of the correlation between $y$ and $p_{i}(x)$, while each $\theta_{i j}$ can be viewed as the strength of the dependence between $p_{i}(x)$ and $p_{j}(x)$. We assume that $\theta_{y}=0$, which corresponds to $\operatorname{Pr}(y=1)=\frac{1}{2}$.</p>
<p>We present our expression for $H(y \mid \mathbf{P}(x))$. Define $\Theta=\left[\theta_{1}, \ldots, \theta_{m}\right]$ to be the vector of canonical parameters corresponding to the strength of correlation between $y$ and each $p_{i}(x)$. Define $\mu=\mathbb{E}\left[p_{i}(x)\right]$, which can be written as $2 \operatorname{Pr}\left(p_{i}(x)=y\right)-1$, a notion of accuracy scaled to $[-1,1]$.
Note that the above form of the distribution is in terms of canonical parameters $\theta$. This distribution can also be parametrized in terms of the mean parameters corresponding to $\theta$, which are $\mathbb{E}[y], \mathbb{E}\left[p_{i}(x) y\right]$ for $i \in[m]$, and $\mathbb{E}\left[p_{i}(x) p_{j}(x)\right]$ for $\left(p_{i}(x), p_{j}(x)\right) \in E$.
Theorem 1. Assume $\operatorname{Pr}(y, \mathbf{P}(x))$ follows equation 2 above. Then, the conditional entropy $H(y \mid \mathbf{P}(x))$ can be expressed as</p>
<p>$$
H(y \mid \mathbf{P}(x))=H(y)-\left(\Theta^{\top} \mu-\mathbb{E}_{\mathbf{P}(x)}\left[\log \cosh \Theta^{\top} \mathbf{P}(x)\right]\right)
$$</p>
<p>The quantity being subtracted from $H(y)$ corresponds to the reduction in entropy of $y$ given that we observe $\mathbf{P}(x)$. Within this expression, there are two terms. First, $\Theta^{\top} \mu$ is correlated with how much signal each $p_{i}(x)$ contains about $y$. Note that this quantity is symmetric-if $p_{i}(x)$ is negatively correlated with $y$, it still provides information since both $\theta_{i}$ and $\mathbb{E}\left[p_{i}(x) y\right]$ will be negative. The second term, $\mathbb{E}<em i="i" j="j">{\mathbf{P}(x)}\left[\log \cosh \Theta^{\top} \mathbf{P}(x)\right]$, is for normalization (otherwise, the first term can grow arbitrarily large with $\Theta$ ). Note that this quantity is independent of $\theta</em>$, the interactions between prompts.</p>
<p>Proof. We can write $H(y \mid \mathbf{P}(x))$ as $H(y, \mathbf{P}(x))-H(\mathbf{P}(x))$, and $H(y, \mathbf{P}(x))$ as $H(\mathbf{P}(x) \mid y)+H(y)$. Therefore, $H(y \mid \mathbf{P}(x))=H(y)-(H(\mathbf{P}(x))-H(\mathbf{P}(x) \mid y))$. We focus on simplifying $H(\mathbf{P}(x))-H(\mathbf{P}(x) \mid y)$ :</p>
<p>$$
\begin{aligned}
H(\mathbf{P}(x))-H(\mathbf{P}(x) \mid y) &amp; =-\sum_{\mathbf{P}(x) \in{-1,1}^{m}} \operatorname{Pr}(\mathbf{P}(x)) \log \operatorname{Pr}(\mathbf{P}(x))+\sum_{\mathbf{P}(x) \in{-1,1}^{m}, y} \operatorname{Pr}(y, \mathbf{P}(x)) \log \operatorname{Pr}(\mathbf{P}(x) \mid y) \
&amp; =-\sum_{\mathbf{P}(x) \in{-1,1}^{m}, y} \operatorname{Pr}(\mathbf{P}(x), y)(\log \operatorname{Pr}(\mathbf{P}(x))-\log \operatorname{Pr}(\mathbf{P}(x) \mid y)) \
&amp; =-\sum_{\mathbf{P}(x) \in{-1,1}^{\infty}} \operatorname{Pr}(\mathbf{P}(x), y=-1)(\log \operatorname{Pr}(\mathbf{P}(x))-\log \operatorname{Pr}(\mathbf{P}(x) \mid y=-1)) \
&amp; +\operatorname{Pr}(\mathbf{P}(x), y=1)(\log \operatorname{Pr}(\mathbf{P}(x))-\log \operatorname{Pr}(\mathbf{P}(x) \mid y=1)))
\end{aligned}
$$</p>
<p>We now write $\operatorname{Pr}(\mathbf{P}(x)), \operatorname{Pr}(\mathbf{P}(x) \mid y=-1)$ and $\operatorname{Pr}(\mathbf{P}(x) \mid y=1)$ according to our Ising model in equation 2. Let $A_{\mathbf{P}(x)}=\sum_{i=1}^{m} \theta_{i} p_{i}(x)$, and let $B_{\mathbf{P}(x)}=\sum_{(i, j) \in E} \theta_{i j} p_{i}(x) p_{j}(x)$, so that $\operatorname{Pr}(y, \mathbf{P}(x))=\frac{1}{Z} \exp \left(A_{\mathbf{P}(x)} y+B_{\mathbf{P}(x)}\right)$ :</p>
<p>$$
\begin{aligned}
\operatorname{Pr}(\mathbf{P}(x)) &amp; =\operatorname{Pr}(\mathbf{P}(x), y=-1)+\operatorname{Pr}(\mathbf{P}(x), y=1) \
&amp; =\frac{1}{Z} \exp \left(A_{\mathbf{P}(x)}+B_{\mathbf{P}(x)}\right)+\frac{1}{Z} \exp \left(-A_{\mathbf{P}(x)}+B_{\mathbf{P}(x)}\right) \
&amp; =\frac{1}{Z} \exp \left(B_{\mathbf{P}(x)}\right)\left(\exp \left(A_{\mathbf{P}(x)}\right)+\exp \left(-A_{\mathbf{P}(x)}\right)\right) \
\operatorname{Pr}(\mathbf{P}(x) \mid y=-1) &amp; =2 \operatorname{Pr}(\mathbf{P}(x), y=-1)=\frac{2}{Z} \exp \left(-A_{\mathbf{P}(x)}+B_{\mathbf{P}(x)}\right) \
\operatorname{Pr}(\mathbf{P}(x) \mid y=1) &amp; =2 \operatorname{Pr}(\mathbf{P}(x), y=1)=\frac{2}{Z} \exp \left(A_{\mathbf{P}(x)}+B_{\mathbf{P}(x)}\right))
\end{aligned}
$$</p>
<p>Therefore, we have that</p>
<p>$$
\begin{aligned}
&amp; \log \operatorname{Pr}(\mathbf{P}(x))-\log \operatorname{Pr}(\mathbf{P}(x) \mid y=-1)=-\log Z+B_{\mathbf{P}(x)}+\log \left(\exp \left(A_{\mathbf{P}(x)}\right)+\exp \left(-A_{\mathbf{P}(x)}\right)\right) \
&amp; \quad-\log 2+\log Z+A_{\mathbf{P}(x)}-B_{\mathbf{P}(x)}=-\log 2+A_{\mathbf{P}(x)}+\log \left(\exp \left(A_{\mathbf{P}(x)}\right)+\exp \left(-A_{\mathbf{P}(x)}\right)\right) \
&amp; \log \operatorname{Pr}(\mathbf{P}(x))-\log \operatorname{Pr}(\mathbf{P}(x) \mid y=1)=-\log Z+B_{\mathbf{P}(x)}+\log \left(\exp \left(A_{\mathbf{P}(x)}\right)+\exp \left(-A_{\mathbf{P}(x)}\right)\right) \
&amp; \quad-\log 2+\log Z-A_{\mathbf{P}(x)}-B_{\mathbf{P}(x)}=-\log 2-A_{\mathbf{P}(x)}+\log \left(\exp \left(A_{\mathbf{P}(x)}\right)+\exp \left(-A_{\mathbf{P}(x)}\right)\right)
\end{aligned}
$$</p>
<p>Plugging this back into equation 4, we have</p>
<p>$$
\begin{aligned}
&amp; \sum_{\mathbf{P}(x) \in{-1,1}^{m}, y} \operatorname{Pr}(\mathbf{P}(x), y) A_{\mathbf{P}(x)} y-\operatorname{Pr}(\mathbf{P}(x)) \left(\log \left(\exp \left(A_{\mathbf{P}(x)}\right)+\exp \left(-A_{\mathbf{P}(x)}\right)\right)-\log 2\right) \
&amp; =\sum_{\mathbf{P}(x) \in{-1,1}^{m}, y} \operatorname{Pr}(\mathbf{P}(x), y) A_{\mathbf{P}(x)} y-\operatorname{Pr}(\mathbf{P}(x)) \log \cosh A_{\mathbf{P}(x)} \
&amp; =\mathbb{E}\left[A_{\mathbf{P}(x)} y\right]-\mathbb{E}\left[\log \cosh A_{\mathbf{P}(x)}\right]
\end{aligned}
$$</p>
<p>Substituting in our definitions of $\Theta$ and $\mu$ give us our desired expression for $H(y \mid \mathbf{P}(x))$.</p>
<h1>E AMA Diagnostics</h1>
<p>We present a suite of 8 diagnostic tasks, which can be categorized into four task types: question generation, answer generation, answer selection and extraction. We provided details about the tasks and scoring below.
Question Generation: We measure the ability of the model to transform a statement to a question. We construct 3 question generation tasks which evaluate the models ability to transform a statement to a yes/no question (see Question Generation (Yes/No)), transform a statement to a wh- question (see Question Generation (wh-)) and finally, transform a statement about a placeholder entity to a question about the placeholder (see Question Generation (@placeholder)). All question generation tasks are scored using the ROUGE score [Lin, 2004].</p>
<h2>Question Generation (Yes/No)</h2>
<p>Input
Rewrite the statement as a yes/no question.
Statement: The father and son went camping to California.
Question:
Output
Did the father and son go camping?</p>
<h2>Question Generation (wh-)</h2>
<p>Input
Convert statement to a question.
Statement: Aristide kills Prime Minister Robert Malval
Question:
Output
Who killed Prime Minister Robert Malval?</p>
<h2>Question Generation (@placeholder)</h2>
<p>Input
Rewrite the statement as a question about the [at]placeholder.
Statement: Most of the light comes from the [at]placeholder
Question:
Output
Where does most of the light come from?</p>
<p>Answer Selection: We construct 2 answer selection tasks which measure the model's ability to generate an answer that is faithful to a set of provided answer choices. Concretely, we measure the models ability to select object categories from a fixed set of options specified in the context (see Answer Selection (category)). Further, we measure the model's ability to complete a sentence when provided with a context and set of sentence completion candidates (see Answer Selection (completion)). In both tasks, an answer is marked as correct if the generated response is one of the candidates provided in the context.</p>
<h1>Answer Selection (category)</h1>
<p>Input</p>
<div class="codehilite"><pre><span></span><code>Select the correct category.
&quot;Categories&quot;:
- company
- educational institution
- artist
- athlete
- office holder
- mean of transportation
- building
- natural place
- village
- animal
- plant
- album
- film
- written work
</code></pre></div>

<p>Example: A "journal" fits "Category":</p>
<p>Output
written work</p>
<h2>Answer Selection (sentence)</h2>
<p>Input</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Select</span><span class="w"> </span><span class="nx">one</span><span class="w"> </span><span class="kd">choice</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">passage</span><span class="p">.</span>
<span class="nx">Select</span><span class="w"> </span><span class="nx">One</span><span class="w"> </span><span class="nx">Choice</span><span class="p">:</span>
<span class="mi">1</span><span class="p">.</span><span class="w"> </span><span class="nx">consumer</span><span class="w"> </span><span class="nx">electronics</span>
<span class="mi">2</span><span class="p">.</span><span class="w"> </span><span class="nx">Play</span><span class="w"> </span><span class="nx">Stations</span>
<span class="mi">3</span><span class="p">.</span><span class="w"> </span><span class="nx">cameras</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">Passage</span><span class="o">:</span><span class="w"> </span><span class="n">Microsoft</span><span class="w"> </span><span class="n">Corporation</span><span class="w"> </span><span class="n">produces</span><span class="w"> </span><span class="n">computer</span><span class="w"> </span><span class="n">software</span><span class="o">,</span><span class="w"> </span><span class="n">consumer</span><span class="w"> </span><span class="n">electronics</span><span class="o">,</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">personal</span>
<span class="w">    </span><span class="n">computers</span><span class="o">.</span><span class="w"> </span><span class="n">It</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">headquartered</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">Microsoft</span><span class="w"> </span><span class="n">Redmond</span><span class="w"> </span><span class="n">campus</span><span class="w"> </span><span class="n">located</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">Redmond</span><span class="o">,</span><span class="w"> </span><span class="n">Washington</span>
<span class="w">    </span><span class="o">,</span><span class="w"> </span><span class="n">United</span><span class="w"> </span><span class="n">States</span><span class="o">.</span>
</code></pre></div>

<p>The passage "Passage" states: Microsoft Corporation sells: "Choice":.</p>
<h2>Output</h2>
<p>consumer electronics</p>
<p>Answer Generation: We construct 1 answer generation task which measures the model's ability to generate candidate sentence completions given a context and portion of a statement (see Answer Generation). Here, a generated answer is marked as correct if the model generates 2 candidate answers.</p>
<h2>Answer Generation</h2>
<p>Input</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ https://github.com/realtimeqa/realtimeqa_public&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
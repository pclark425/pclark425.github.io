<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2553 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2553</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2553</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-65.html">extraction-schema-65</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-006c4c7470566327e5b02b94936d0be0033fc9f5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/006c4c7470566327e5b02b94936d0be0033fc9f5" target="_blank">MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> A novel LLM-based Multi-Agent framework for GitHub Issue reSolution, MAGIS, consisting of four agents customized for software evolution: Manager, Repository Custodian, Developer, and Quality Assurance Engineer agents, which achieves an eight-fold increase in resolved ratio over the direct application of GPT-4, the advanced LLM.</p>
                <p><strong>Paper Abstract:</strong> In software development, resolving the emergent issues within GitHub repositories is a complex challenge that involves not only the incorporation of new code but also the maintenance of existing code. Large Language Models (LLMs) have shown promise in code generation but face difficulties in resolving Github issues, particularly at the repository level. To overcome this challenge, we empirically study the reason why LLMs fail to resolve GitHub issues and analyze the major factors. Motivated by the empirical findings, we propose a novel LLM-based Multi-Agent framework for GitHub Issue reSolution, MAGIS, consisting of four agents customized for software evolution: Manager, Repository Custodian, Developer, and Quality Assurance Engineer agents. This framework leverages the collaboration of various agents in the planning and coding process to unlock the potential of LLMs to resolve GitHub issues. In experiments, we employ the SWE-bench benchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and Claude-2. MAGIS can resolve 13.94% GitHub issues, significantly outperforming the baselines. Specifically, MAGIS achieves an eight-fold increase in resolved ratio over the direct application of GPT-4, the advanced LLM.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2553.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2553.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue ReSolution</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical multi-agent framework that orchestrates LLM-based agents (Manager, Repository Custodian, Developer, QA Engineer) to plan, locate, implement, and review repository-level code changes to resolve GitHub issues by decomposing tasks, reusing repository summaries, and iterating with review feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>MAGIS is a purpose-built LLM-based multi-agent system for repository-level GitHub issue resolution. It decomposes the overall task into a planning phase (file/location retrieval and task decomposition) and a coding phase (iterative code edits and review). The architecture centers on a Manager agent that coordinates team formation and meetings, a Repository Custodian that retrieves and summarizes candidate files with a memory mechanism, Developer agents that perform file-level edits by identifying line-hunks and generating replacement snippets, and paired QA Engineer agents that produce review comments and binary review decisions to accept or request revisions. The system leverages prompts to a base LLM (GPT-4 in experiments), git diffs for incremental summaries and change generation, and an iterative loop between Developers and QA until quality or iteration limits are reached.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>4 (Manager, Repository Custodian, Developer(s), QA Engineer(s))</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Manager: decomposes issues into file-level tasks, assembles Developer agents, runs kick-off meetings and produces a main plan; Repository Custodian: ranks files via BM25, summarizes files (and diffs) into a repository evolution memory for reuse and filters irrelevant files; Developer agents: for each assigned file-level task, locate line intervals to change, generate new code snippets for those hunks, and produce candidate patch diffs; QA Engineer agents: review generated diffs, produce review_comment and a binary review_decision (accept/reject), and request further iterations if needed.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Implementation and evaluation within software-evolution tasks: planning (requirement decomposition, file localization), implementation (code generation and patch creation), and evaluation/QA (automated code review via QA agents and test-based resolution checks). (Not designed for literature review or hypothesis generation outside software evolution.)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Hierarchical and centralized orchestration led by a Manager agent: Repository Custodian provides inputs for planning; Manager decomposes tasks and 'recruits' Developer agents with role descriptions; Manager runs a structured kick-off meeting (circular speech) to align tasks and concurrency; Developers operate on assigned subtasks and coordinate with paired QA Engineers in iterative review loops; final patches are merged into repository-level change. Memory (shared repository-evolution memory) is used for coordination efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Primarily natural-language prompt exchanges to/from the chosen LLM (GPT-4 in experiments) augmented by structured artifacts: BM25-ranked file lists, summary entries in a repository memory M, git diffs (Δd) used as compact representations of file changes, task descriptions and role descriptions as structured text, meeting recordings (text) used to update agent roles and generate the main plan. Agents exchange: (1) natural language prompts/responses, (2) summarized file tokens, (3) git diffs and file splits (old_part/new_part), and (4) review comments and binary decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Iterative Developer–QA loops: after generating a candidate patch (Δd_i), the QA Engineer produces a review_comment and review_decision (true/false). If negative, the review_comment is appended to the task and the Developer retries up to a configured max iteration (n_max). Repository Custodian filters files and the Manager updates tasks after meetings. Memory reuse (summaries + commit-message-like summaries of diffs) provides feedback into subsequent retrieval and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>On-demand and phase-driven: (a) planning-phase exchanges: Repository Custodian -> Manager (file lists and summaries), Manager -> Developers (role/task descriptions), then a kick-off meeting among Manager and Developers; (b) coding-phase exchanges: per coding iteration (Developer generates hunks -> QA reviews -> Developer revises) until QA accepts or max iterations reached. The paper frames communication as occurring at phase transitions and after each code-generation step (i.e., after each iteration for a task).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Software engineering / software evolution (GitHub issue resolution, repository-level code repair and feature change).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Applied ratio (patch can be applied via git apply) and Resolved ratio (patch applied and test-suite indicates issue resolved). Reported results on SWE-bench (25% subset): MAGIS applied = 97.39%, resolved = 13.94% (absolute). On SWE-bench lite MAGIS resolved = 25.33%. Ablation variants: MAGIS (w/o QA) resolved = 10.63%; MAGIS (w/o hints) resolved = 10.28%; MAGIS (w/o hints, w/o QA) resolved = 8.71%. Average runtime per instance: under 5 minutes on average; resolved instances typically ~3 minutes. Additional stats: MAGIS improves line-locating coverage distributions relative to baselines (figures in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against single-LLM baselines (GPT-3.5, GPT-4, Claude-2, SWE-Llama variants) on SWE-bench subset: GPT-4 resolved = 1.74%, GPT-3.5 = 0.84%, Claude-2 = 4.88%; MAGIS resolved = 13.94% — an ~8x improvement over direct GPT-4. On SWE-bench lite MAGIS (full) = 25.33% vs AutoCodeRover 16.11% (or 22.33% under union), SWE-Agent 18.00%.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Substantial improvements in repository-level issue resolution: increased resolved ratio (13.94% vs GPT-4 1.74%), vastly higher applied ratio (97.39%). Coordination yields better file/line locating (custodian+developer pipeline and memory reuse) and mitigates negative effects of complexity (logistic regression shows much weaker negative correlations between #files/#functions and resolution for MAGIS versus GPT-4). Pairing Developers with QA Engineers raises resolved ratio (absolute increases shown in ablation: full MAGIS 13.94% vs w/o QA 10.63% => QA contributes +3.31 percentage points in this dataset). Planning (Manager) improves task alignment (most generated task descriptions scored >=3 by GPT-4 evaluator) and reduces irrelevant file processing (better recall-vs-file-number curve than BM25).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Reported limitations include LLM context-length constraints and computational cost of querying many files (motivating the memory/summarization mechanism), potential retrieval of irrelevant files (necessitating filtering), variable performance across repositories (resolved ratios vary from ~0% to ~40%), iteration limits for Developer–QA loops may truncate convergence, and extra overhead from meetings/QA. The paper notes trade-offs: adding more files improves recall but can degrade LLM generation quality due to longer context.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Yes. Main ablations: (1) MAGIS (w/o QA): applied = 92.71%, resolved = 10.63% (vs full 13.94%) showing QA contribution; (2) MAGIS (w/o hints): applied = 94.25%, resolved = 10.28%; (3) MAGIS (w/o hints, w/o QA): applied = 91.99%, resolved = 8.71%. These quantify the effect of QA and of using pre-PR 'hints' (human textual hints from PR comments). Also SWE-bench lite ablation: MAGIS full 25.33% vs w/o QA 23.33% etc.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Paper recommends (qualitatively): hierarchical Manager-led decomposition into file-level tasks, using a Repository-Evolution Memory to reuse file summaries and git-diff-based commit summaries to reduce context length and cost, limiting candidate file set (top-k) after BM25 ranking then filtering, designing Developer roles per task, and pairing Developers with QA agents with iterative review loops. No single numeric hyperparameter is claimed optimal in general; k (top files) and n_max (max iterations) are configurable but exact optimal values are not universally fixed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2553.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2553.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MetaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent LLM-based system that simulates programming-team SOPs to coordinate code generation tasks, demonstrated to perform strongly on code generation benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Metagpt: Meta programming for a multi-agent collaborative framework</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MetaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as an example of an LLM-based multi-agent system that simulates a programming team's Standard Operating Procedures to coordinate agents for software development tasks and achieves leading scores on benchmarks like HumanEval and MBPP. The paper cites MetaGPT as prior work demonstrating multi-agent collaboration for code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>not specified in this paper (see original MetaGPT paper)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>not specified in this paper; described generally as team-simulating roles per SOPs in original work</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>implementation (code generation) and team-like coordination for software development (as described in citation)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>described at high level as simulating human SOPs (hierarchical/team-based), exact mechanism not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>not specified in this paper (original MetaGPT paper should be consulted)</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>software development / code generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>paper reports MetaGPT achieves leading scores on HumanEval and MBPP (no numeric details provided here)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>mentioned as prior multi-agent baseline; detailed comparisons not contained in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>claimed benefits: improved code generation performance on benchmarks via team-style collaboration (no quantitative details in MAGIS paper)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>not discussed in detail in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>not provided in this paper (see original MetaGPT work)</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2553.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2553.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatDev</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatDev</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent virtual development company that decomposes requirements into atomic tasks and uses mutual communication and self-reflection among agents to reduce hallucinations in generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Communicative agents for software development</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatDev</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as a prior LLM multi-agent system modeled as a virtual development company; it decomposes requirements into atomic tasks, employs mutual communication among agents and self-reflection techniques to mitigate hallucinations, and coordinates agents to produce code artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>not specified here beyond 'roles that decompose and implement tasks' as described in the cited work</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>implementation (task decomposition and code generation) per cited description</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>multi-agent communication and self-reflection; details not provided in MAGIS paper</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>mutual agent communication and self-reflection noted, specifics absent in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>software development / code generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>not provided in MAGIS paper; referenced as prior art</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>referenced as related work; no direct experimental comparison in MAGIS except as general context</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>reduces hallucinations via mutual communication and self-reflection (as summarized in MAGIS related work)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>not specified in MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>not provided in MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2553.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2553.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autogen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autogen</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent conversation framework aimed at enabling next-generation LLM applications via structured agent interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autogen: Enabling next-gen LLM applications via multi-agent conversation framework</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Autogen</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an enabling multi-agent conversation framework that structures agent interactions to build complex LLM applications; referenced in related work as an example of multi-agent systems enabling richer agent coordination.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>not specified here</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>general multi-agent conversation support (framework-level), applicability to implementation-level tasks per cited work</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>multi-agent conversation orchestration (details not provided in MAGIS paper)</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>not specified here</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>not specified here</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>general LLM applications / software development facilitation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>not provided here</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>not compared in MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>enables structured multi-agent conversation for complex tasks (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>not discussed in MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>not present in MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2553.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2553.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenDevin</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenDevin: An Open Platform for AI Software Developers as Generalist Agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open platform for AI 'developer' agents (Devin), mentioned as a contemporaneous agent-based system evaluated on SWE-bench; reported to use developer tools (shell, editor, browser) and optionally internet access.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OpenDevin: An Open Platform for AI Software Developers as Generalist Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OpenDevin (Devin)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as a contemporaneous agent that functions as an autonomous developer with access to developer tools and possibly internet; MAGIS compares resolved-instance counts and runtime characteristics against Devin on a shared subset (MAGIS resolved 21/140 vs Devin 18/140 in shared instances). MAGIS emphasizes it runs without external browsing whereas Devin may use additional tools.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>not specified in MAGIS (OpenDevin describes generalist agents; see original)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>not specified in MAGIS; described as generalist developer agents in original OpenDevin report</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>implementation and environment-interactive repair (per MAGIS discussion of Devin's capabilities)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>not specified in MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>not specified in MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Devin is described as capable of integrating environmental feedback (per original report), but MAGIS does not detail mechanisms</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>not specified in MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>software development / GitHub issue resolution</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>MAGIS reports comparison on overlapping 140-instance subset: MAGIS resolved 21 (15%), Devin resolved 18 (12.86%); MAGIS average runtime per resolved issue ~3 minutes vs Devin often >10 minutes for 72% of Devin's resolved instances (as reported in MAGIS discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>direct comparison on shared instances reported in paper; MAGIS slightly higher resolved count and faster average runtime</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>MAGIS claims faster wall-clock time and comparable or better resolved counts on the shared subset, partly due to focused shell-only tools and efficient memory/retrieval approach</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>MAGIS notes that comparisons are not fully equitable because Devin may use broader tool access (browser, internet) and different dataset splits</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>not applicable in MAGIS for Devin; MAGIS compared its own ablations instead</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2553.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2553.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SWE-Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Swe-agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent-based system for software engineering language models referenced as a contemporary baseline; evaluated on SWE-bench lite.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Swe-agent: Agent computer interfaces enable software engineering language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SWE-Agent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as a comparative multi-agent approach in the SWE-bench lite evaluations; MAGIS reports higher resolved ratio on the same benchmark subset (MAGIS 25.33% vs SWE-Agent 18.00%). The MAGIS paper does not detail SWE-Agent's internal coordination.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>not specified in MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>not specified in MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>software development implementation (per context)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>not specified in MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>not specified in MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>not specified in MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>not specified in MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>software development / GitHub issue resolution</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported (from MAGIS comparison on SWE-bench lite): SWE-Agent resolved = 18.00% (MAGIS resolved = 25.33% on same benchmark subset).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Direct comparison on SWE-bench lite; MAGIS outperforms SWE-Agent per reported numbers</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>not described in MAGIS; only that MAGIS outperforms SWE-Agent on reported metric</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>not discussed in MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>not provided in MAGIS for SWE-Agent</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2553.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2553.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoCodeRover</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autocoderover: Autonomous program improvement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autonomous program-improvement system referenced as a contemporary baseline in SWE-bench lite comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autocoderover: Autonomous program improvement</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoCodeRover / AutoCodeRover (Autocoderover)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as a recent multi-agent or autonomous program-improvement approach evaluated on SWE-bench lite. MAGIS reports higher resolved ratio than AutoCodeRover on the lite benchmark (MAGIS 25.33% vs AutoCodeRover 16.11% average over runs or 22.33% under union across runs). MAGIS does not provide AutoCodeRover's internal coordination details.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>not specified in MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>not specified in MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>software program repair / improvement</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>not specified in MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>not specified in MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>not specified in MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>not specified in MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>software development / program repair</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in MAGIS comparison on SWE-bench lite: AutoCodeRover resolved = 16.11% (avg) or 22.33% under run-union; MAGIS resolved = 25.33%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Direct benchmark comparison on SWE-bench lite; MAGIS reported superior resolved ratio</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>not described in MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>not discussed in MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>not provided in MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Metagpt: Meta programming for a multi-agent collaborative framework <em>(Rating: 2)</em></li>
                <li>Communicative agents for software development <em>(Rating: 2)</em></li>
                <li>Autogen: Enabling next-gen LLM applications via multi-agent conversation framework <em>(Rating: 2)</em></li>
                <li>OpenDevin: An Open Platform for AI Software Developers as Generalist Agents <em>(Rating: 2)</em></li>
                <li>Swe-agent: Agent computer interfaces enable software engineering language models <em>(Rating: 2)</em></li>
                <li>Autocoderover: Autonomous program improvement <em>(Rating: 2)</em></li>
                <li>Autodev: Automated ai-driven development <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2553",
    "paper_id": "paper-006c4c7470566327e5b02b94936d0be0033fc9f5",
    "extraction_schema_id": "extraction-schema-65",
    "extracted_data": [
        {
            "name_short": "MAGIS",
            "name_full": "MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue ReSolution",
            "brief_description": "A hierarchical multi-agent framework that orchestrates LLM-based agents (Manager, Repository Custodian, Developer, QA Engineer) to plan, locate, implement, and review repository-level code changes to resolve GitHub issues by decomposing tasks, reusing repository summaries, and iterating with review feedback.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "MAGIS",
            "system_description": "MAGIS is a purpose-built LLM-based multi-agent system for repository-level GitHub issue resolution. It decomposes the overall task into a planning phase (file/location retrieval and task decomposition) and a coding phase (iterative code edits and review). The architecture centers on a Manager agent that coordinates team formation and meetings, a Repository Custodian that retrieves and summarizes candidate files with a memory mechanism, Developer agents that perform file-level edits by identifying line-hunks and generating replacement snippets, and paired QA Engineer agents that produce review comments and binary review decisions to accept or request revisions. The system leverages prompts to a base LLM (GPT-4 in experiments), git diffs for incremental summaries and change generation, and an iterative loop between Developers and QA until quality or iteration limits are reached.",
            "number_of_agents": "4 (Manager, Repository Custodian, Developer(s), QA Engineer(s))",
            "agent_specializations": "Manager: decomposes issues into file-level tasks, assembles Developer agents, runs kick-off meetings and produces a main plan; Repository Custodian: ranks files via BM25, summarizes files (and diffs) into a repository evolution memory for reuse and filters irrelevant files; Developer agents: for each assigned file-level task, locate line intervals to change, generate new code snippets for those hunks, and produce candidate patch diffs; QA Engineer agents: review generated diffs, produce review_comment and a binary review_decision (accept/reject), and request further iterations if needed.",
            "research_phases_covered": "Implementation and evaluation within software-evolution tasks: planning (requirement decomposition, file localization), implementation (code generation and patch creation), and evaluation/QA (automated code review via QA agents and test-based resolution checks). (Not designed for literature review or hypothesis generation outside software evolution.)",
            "coordination_mechanism": "Hierarchical and centralized orchestration led by a Manager agent: Repository Custodian provides inputs for planning; Manager decomposes tasks and 'recruits' Developer agents with role descriptions; Manager runs a structured kick-off meeting (circular speech) to align tasks and concurrency; Developers operate on assigned subtasks and coordinate with paired QA Engineers in iterative review loops; final patches are merged into repository-level change. Memory (shared repository-evolution memory) is used for coordination efficiency.",
            "communication_protocol": "Primarily natural-language prompt exchanges to/from the chosen LLM (GPT-4 in experiments) augmented by structured artifacts: BM25-ranked file lists, summary entries in a repository memory M, git diffs (Δd) used as compact representations of file changes, task descriptions and role descriptions as structured text, meeting recordings (text) used to update agent roles and generate the main plan. Agents exchange: (1) natural language prompts/responses, (2) summarized file tokens, (3) git diffs and file splits (old_part/new_part), and (4) review comments and binary decisions.",
            "feedback_mechanism": "Iterative Developer–QA loops: after generating a candidate patch (Δd_i), the QA Engineer produces a review_comment and review_decision (true/false). If negative, the review_comment is appended to the task and the Developer retries up to a configured max iteration (n_max). Repository Custodian filters files and the Manager updates tasks after meetings. Memory reuse (summaries + commit-message-like summaries of diffs) provides feedback into subsequent retrieval and planning.",
            "communication_frequency": "On-demand and phase-driven: (a) planning-phase exchanges: Repository Custodian -&gt; Manager (file lists and summaries), Manager -&gt; Developers (role/task descriptions), then a kick-off meeting among Manager and Developers; (b) coding-phase exchanges: per coding iteration (Developer generates hunks -&gt; QA reviews -&gt; Developer revises) until QA accepts or max iterations reached. The paper frames communication as occurring at phase transitions and after each code-generation step (i.e., after each iteration for a task).",
            "task_domain": "Software engineering / software evolution (GitHub issue resolution, repository-level code repair and feature change).",
            "performance_metrics": "Applied ratio (patch can be applied via git apply) and Resolved ratio (patch applied and test-suite indicates issue resolved). Reported results on SWE-bench (25% subset): MAGIS applied = 97.39%, resolved = 13.94% (absolute). On SWE-bench lite MAGIS resolved = 25.33%. Ablation variants: MAGIS (w/o QA) resolved = 10.63%; MAGIS (w/o hints) resolved = 10.28%; MAGIS (w/o hints, w/o QA) resolved = 8.71%. Average runtime per instance: under 5 minutes on average; resolved instances typically ~3 minutes. Additional stats: MAGIS improves line-locating coverage distributions relative to baselines (figures in paper).",
            "baseline_comparison": "Compared against single-LLM baselines (GPT-3.5, GPT-4, Claude-2, SWE-Llama variants) on SWE-bench subset: GPT-4 resolved = 1.74%, GPT-3.5 = 0.84%, Claude-2 = 4.88%; MAGIS resolved = 13.94% — an ~8x improvement over direct GPT-4. On SWE-bench lite MAGIS (full) = 25.33% vs AutoCodeRover 16.11% (or 22.33% under union), SWE-Agent 18.00%.",
            "coordination_benefits": "Substantial improvements in repository-level issue resolution: increased resolved ratio (13.94% vs GPT-4 1.74%), vastly higher applied ratio (97.39%). Coordination yields better file/line locating (custodian+developer pipeline and memory reuse) and mitigates negative effects of complexity (logistic regression shows much weaker negative correlations between #files/#functions and resolution for MAGIS versus GPT-4). Pairing Developers with QA Engineers raises resolved ratio (absolute increases shown in ablation: full MAGIS 13.94% vs w/o QA 10.63% =&gt; QA contributes +3.31 percentage points in this dataset). Planning (Manager) improves task alignment (most generated task descriptions scored &gt;=3 by GPT-4 evaluator) and reduces irrelevant file processing (better recall-vs-file-number curve than BM25).",
            "coordination_challenges": "Reported limitations include LLM context-length constraints and computational cost of querying many files (motivating the memory/summarization mechanism), potential retrieval of irrelevant files (necessitating filtering), variable performance across repositories (resolved ratios vary from ~0% to ~40%), iteration limits for Developer–QA loops may truncate convergence, and extra overhead from meetings/QA. The paper notes trade-offs: adding more files improves recall but can degrade LLM generation quality due to longer context.",
            "ablation_studies": "Yes. Main ablations: (1) MAGIS (w/o QA): applied = 92.71%, resolved = 10.63% (vs full 13.94%) showing QA contribution; (2) MAGIS (w/o hints): applied = 94.25%, resolved = 10.28%; (3) MAGIS (w/o hints, w/o QA): applied = 91.99%, resolved = 8.71%. These quantify the effect of QA and of using pre-PR 'hints' (human textual hints from PR comments). Also SWE-bench lite ablation: MAGIS full 25.33% vs w/o QA 23.33% etc.",
            "optimal_configurations": "Paper recommends (qualitatively): hierarchical Manager-led decomposition into file-level tasks, using a Repository-Evolution Memory to reuse file summaries and git-diff-based commit summaries to reduce context length and cost, limiting candidate file set (top-k) after BM25 ranking then filtering, designing Developer roles per task, and pairing Developers with QA agents with iterative review loops. No single numeric hyperparameter is claimed optimal in general; k (top files) and n_max (max iterations) are configurable but exact optimal values are not universally fixed in the paper.",
            "uuid": "e2553.0",
            "source_info": {
                "paper_title": "MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "MetaGPT",
            "name_full": "MetaGPT",
            "brief_description": "A multi-agent LLM-based system that simulates programming-team SOPs to coordinate code generation tasks, demonstrated to perform strongly on code generation benchmarks.",
            "citation_title": "Metagpt: Meta programming for a multi-agent collaborative framework",
            "mention_or_use": "mention",
            "system_name": "MetaGPT",
            "system_description": "Mentioned as an example of an LLM-based multi-agent system that simulates a programming team's Standard Operating Procedures to coordinate agents for software development tasks and achieves leading scores on benchmarks like HumanEval and MBPP. The paper cites MetaGPT as prior work demonstrating multi-agent collaboration for code generation.",
            "number_of_agents": "not specified in this paper (see original MetaGPT paper)",
            "agent_specializations": "not specified in this paper; described generally as team-simulating roles per SOPs in original work",
            "research_phases_covered": "implementation (code generation) and team-like coordination for software development (as described in citation)",
            "coordination_mechanism": "described at high level as simulating human SOPs (hierarchical/team-based), exact mechanism not specified in this paper",
            "communication_protocol": "not specified in this paper (original MetaGPT paper should be consulted)",
            "feedback_mechanism": "not specified in this paper",
            "communication_frequency": "not specified in this paper",
            "task_domain": "software development / code generation",
            "performance_metrics": "paper reports MetaGPT achieves leading scores on HumanEval and MBPP (no numeric details provided here)",
            "baseline_comparison": "mentioned as prior multi-agent baseline; detailed comparisons not contained in this paper",
            "coordination_benefits": "claimed benefits: improved code generation performance on benchmarks via team-style collaboration (no quantitative details in MAGIS paper)",
            "coordination_challenges": "not discussed in detail in this paper",
            "ablation_studies": "not provided in this paper (see original MetaGPT work)",
            "optimal_configurations": "not specified in this paper",
            "uuid": "e2553.1",
            "source_info": {
                "paper_title": "MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "ChatDev",
            "name_full": "ChatDev",
            "brief_description": "A multi-agent virtual development company that decomposes requirements into atomic tasks and uses mutual communication and self-reflection among agents to reduce hallucinations in generated code.",
            "citation_title": "Communicative agents for software development",
            "mention_or_use": "mention",
            "system_name": "ChatDev",
            "system_description": "Cited as a prior LLM multi-agent system modeled as a virtual development company; it decomposes requirements into atomic tasks, employs mutual communication among agents and self-reflection techniques to mitigate hallucinations, and coordinates agents to produce code artifacts.",
            "number_of_agents": "not specified in this paper",
            "agent_specializations": "not specified here beyond 'roles that decompose and implement tasks' as described in the cited work",
            "research_phases_covered": "implementation (task decomposition and code generation) per cited description",
            "coordination_mechanism": "multi-agent communication and self-reflection; details not provided in MAGIS paper",
            "communication_protocol": "not specified in this paper",
            "feedback_mechanism": "mutual agent communication and self-reflection noted, specifics absent in this paper",
            "communication_frequency": "not specified in this paper",
            "task_domain": "software development / code generation",
            "performance_metrics": "not provided in MAGIS paper; referenced as prior art",
            "baseline_comparison": "referenced as related work; no direct experimental comparison in MAGIS except as general context",
            "coordination_benefits": "reduces hallucinations via mutual communication and self-reflection (as summarized in MAGIS related work)",
            "coordination_challenges": "not specified in MAGIS",
            "ablation_studies": "not provided in MAGIS",
            "uuid": "e2553.2",
            "source_info": {
                "paper_title": "MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Autogen",
            "name_full": "Autogen",
            "brief_description": "A multi-agent conversation framework aimed at enabling next-generation LLM applications via structured agent interactions.",
            "citation_title": "Autogen: Enabling next-gen LLM applications via multi-agent conversation framework",
            "mention_or_use": "mention",
            "system_name": "Autogen",
            "system_description": "Cited as an enabling multi-agent conversation framework that structures agent interactions to build complex LLM applications; referenced in related work as an example of multi-agent systems enabling richer agent coordination.",
            "number_of_agents": "not specified in this paper",
            "agent_specializations": "not specified here",
            "research_phases_covered": "general multi-agent conversation support (framework-level), applicability to implementation-level tasks per cited work",
            "coordination_mechanism": "multi-agent conversation orchestration (details not provided in MAGIS paper)",
            "communication_protocol": "not specified in this paper",
            "feedback_mechanism": "not specified here",
            "communication_frequency": "not specified here",
            "task_domain": "general LLM applications / software development facilitation",
            "performance_metrics": "not provided here",
            "baseline_comparison": "not compared in MAGIS",
            "coordination_benefits": "enables structured multi-agent conversation for complex tasks (as cited)",
            "coordination_challenges": "not discussed in MAGIS",
            "ablation_studies": "not present in MAGIS",
            "uuid": "e2553.3",
            "source_info": {
                "paper_title": "MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "OpenDevin",
            "name_full": "OpenDevin: An Open Platform for AI Software Developers as Generalist Agents",
            "brief_description": "An open platform for AI 'developer' agents (Devin), mentioned as a contemporaneous agent-based system evaluated on SWE-bench; reported to use developer tools (shell, editor, browser) and optionally internet access.",
            "citation_title": "OpenDevin: An Open Platform for AI Software Developers as Generalist Agents",
            "mention_or_use": "mention",
            "system_name": "OpenDevin (Devin)",
            "system_description": "Referenced as a contemporaneous agent that functions as an autonomous developer with access to developer tools and possibly internet; MAGIS compares resolved-instance counts and runtime characteristics against Devin on a shared subset (MAGIS resolved 21/140 vs Devin 18/140 in shared instances). MAGIS emphasizes it runs without external browsing whereas Devin may use additional tools.",
            "number_of_agents": "not specified in MAGIS (OpenDevin describes generalist agents; see original)",
            "agent_specializations": "not specified in MAGIS; described as generalist developer agents in original OpenDevin report",
            "research_phases_covered": "implementation and environment-interactive repair (per MAGIS discussion of Devin's capabilities)",
            "coordination_mechanism": "not specified in MAGIS",
            "communication_protocol": "not specified in MAGIS",
            "feedback_mechanism": "Devin is described as capable of integrating environmental feedback (per original report), but MAGIS does not detail mechanisms",
            "communication_frequency": "not specified in MAGIS",
            "task_domain": "software development / GitHub issue resolution",
            "performance_metrics": "MAGIS reports comparison on overlapping 140-instance subset: MAGIS resolved 21 (15%), Devin resolved 18 (12.86%); MAGIS average runtime per resolved issue ~3 minutes vs Devin often &gt;10 minutes for 72% of Devin's resolved instances (as reported in MAGIS discussion).",
            "baseline_comparison": "direct comparison on shared instances reported in paper; MAGIS slightly higher resolved count and faster average runtime",
            "coordination_benefits": "MAGIS claims faster wall-clock time and comparable or better resolved counts on the shared subset, partly due to focused shell-only tools and efficient memory/retrieval approach",
            "coordination_challenges": "MAGIS notes that comparisons are not fully equitable because Devin may use broader tool access (browser, internet) and different dataset splits",
            "ablation_studies": "not applicable in MAGIS for Devin; MAGIS compared its own ablations instead",
            "uuid": "e2553.4",
            "source_info": {
                "paper_title": "MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "SWE-Agent",
            "name_full": "Swe-agent",
            "brief_description": "An agent-based system for software engineering language models referenced as a contemporary baseline; evaluated on SWE-bench lite.",
            "citation_title": "Swe-agent: Agent computer interfaces enable software engineering language models",
            "mention_or_use": "mention",
            "system_name": "SWE-Agent",
            "system_description": "Mentioned as a comparative multi-agent approach in the SWE-bench lite evaluations; MAGIS reports higher resolved ratio on the same benchmark subset (MAGIS 25.33% vs SWE-Agent 18.00%). The MAGIS paper does not detail SWE-Agent's internal coordination.",
            "number_of_agents": "not specified in MAGIS",
            "agent_specializations": "not specified in MAGIS",
            "research_phases_covered": "software development implementation (per context)",
            "coordination_mechanism": "not specified in MAGIS",
            "communication_protocol": "not specified in MAGIS",
            "feedback_mechanism": "not specified in MAGIS",
            "communication_frequency": "not specified in MAGIS",
            "task_domain": "software development / GitHub issue resolution",
            "performance_metrics": "Reported (from MAGIS comparison on SWE-bench lite): SWE-Agent resolved = 18.00% (MAGIS resolved = 25.33% on same benchmark subset).",
            "baseline_comparison": "Direct comparison on SWE-bench lite; MAGIS outperforms SWE-Agent per reported numbers",
            "coordination_benefits": "not described in MAGIS; only that MAGIS outperforms SWE-Agent on reported metric",
            "coordination_challenges": "not discussed in MAGIS",
            "ablation_studies": "not provided in MAGIS for SWE-Agent",
            "uuid": "e2553.5",
            "source_info": {
                "paper_title": "MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "AutoCodeRover",
            "name_full": "Autocoderover: Autonomous program improvement",
            "brief_description": "An autonomous program-improvement system referenced as a contemporary baseline in SWE-bench lite comparisons.",
            "citation_title": "Autocoderover: Autonomous program improvement",
            "mention_or_use": "mention",
            "system_name": "AutoCodeRover / AutoCodeRover (Autocoderover)",
            "system_description": "Cited as a recent multi-agent or autonomous program-improvement approach evaluated on SWE-bench lite. MAGIS reports higher resolved ratio than AutoCodeRover on the lite benchmark (MAGIS 25.33% vs AutoCodeRover 16.11% average over runs or 22.33% under union across runs). MAGIS does not provide AutoCodeRover's internal coordination details.",
            "number_of_agents": "not specified in MAGIS",
            "agent_specializations": "not specified in MAGIS",
            "research_phases_covered": "software program repair / improvement",
            "coordination_mechanism": "not specified in MAGIS",
            "communication_protocol": "not specified in MAGIS",
            "feedback_mechanism": "not specified in MAGIS",
            "communication_frequency": "not specified in MAGIS",
            "task_domain": "software development / program repair",
            "performance_metrics": "Reported in MAGIS comparison on SWE-bench lite: AutoCodeRover resolved = 16.11% (avg) or 22.33% under run-union; MAGIS resolved = 25.33%.",
            "baseline_comparison": "Direct benchmark comparison on SWE-bench lite; MAGIS reported superior resolved ratio",
            "coordination_benefits": "not described in MAGIS",
            "coordination_challenges": "not discussed in MAGIS",
            "ablation_studies": "not provided in MAGIS",
            "uuid": "e2553.6",
            "source_info": {
                "paper_title": "MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Metagpt: Meta programming for a multi-agent collaborative framework",
            "rating": 2
        },
        {
            "paper_title": "Communicative agents for software development",
            "rating": 2
        },
        {
            "paper_title": "Autogen: Enabling next-gen LLM applications via multi-agent conversation framework",
            "rating": 2
        },
        {
            "paper_title": "OpenDevin: An Open Platform for AI Software Developers as Generalist Agents",
            "rating": 2
        },
        {
            "paper_title": "Swe-agent: Agent computer interfaces enable software engineering language models",
            "rating": 2
        },
        {
            "paper_title": "Autocoderover: Autonomous program improvement",
            "rating": 2
        },
        {
            "paper_title": "Autodev: Automated ai-driven development",
            "rating": 1
        }
    ],
    "cost": 0.019757999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MAGİS: LLM-Based Multi-Agent Framework for GitHub Issue ReSolution</h1>
<p>Wei Tao<br>Fudan University<br>wtao18@fudan.edu.cn<br>Yucheng Zhou<br>University of Macau<br>yucheng.zhou@connect.um.edu.mo<br>Yanlin Wang<br>Sun Yat-sen University<br>wangylin36@mail.sysu.edu.cn<br>Wenqiang Zhang<br>Fudan University<br>wqzhang@fudan.edu.cn<br>Hongyu Zhang<br>Chongqing University<br>hyzhang@cqu.edu.cn</p>
<p>Yu Cheng<br>The Chinese University of Hong Kong<br>chengyu@cse.cuhk.edu.hk</p>
<h4>Abstract</h4>
<p>In software development, resolving the emergent issues within GitHub repositories is a complex challenge that involves not only the incorporation of new code but also the maintenance of existing code. Large Language Models (LLMs) have shown promise in code generation but face difficulties in resolving Github issues, particularly at the repository level. To overcome this challenge, we empirically study the reason why LLMs fail to resolve GitHub issues and analyze the major factors. Motivated by the empirical findings, we propose a novel LLM-based MultiAgent framework for GitHub Issue reSolution, MAGIS, consisting of four agents customized for software evolution: Manager, Repository Custodian, Developer, and Quality Assurance Engineer agents. This framework leverages the collaboration of various agents in the planning and coding process to unlock the potential of LLMs to resolve GitHub issues. In experiments, we employ the SWE-bench benchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and Claude2. MAGIS can resolve $\mathbf{1 3 . 9 4 \%}$ GitHub issues, significantly outperforming the baselines. Specifically, MAGIS achieves an eight-fold increase in resolved ratio over the direct application of GPT-4, the advanced LLM.</p>
<h2>1 Introduction</h2>
<p>In real-world software development, the code repository for a project is rarely set in stone. Highquality and popular software always evolves to address emergent bugs or new requirements. On platforms such as GitHub [21], issues typically signify the requirement for software evolution. However, addressing these issues poses significant challenges, as it requires implementing the code change across the entire repository and maintaining the existing functionality while integrating new capabilities. For example, django, a framework for over 1.6M projects has 34 K issues [19]. Consequently, resolving GitHub issues remains a significant challenge across academia and industry [27, 5].
Large language models (LLMs) have demonstrated remarkable capabilities across a variety of tasks [8], including code generation and code understanding [65, 48]. Specifically, LLMs excel in generating function-level code, as evidenced by their performance on numerous benchmark datasets such as MBPP [2] and HumanEval [12]. Despite their success, LLMs remain challenged in tasks that require advanced code generation capabilities, such as class-level code generation [14]. Moreover,</p>
<p>LLMs exhibit limitations in processing excessively long context inputs and are subject to constraints regarding their input context length [33]. This limitation is particularly evident in repository-level coding tasks, such as solving GitHub issues, where the context comprises the entire repository, thus imposing constraints on directly using the full repository as input to LLMs.</p>
<p>To harness the full potential of LLMs, many LLM-based multi-agent systems are designed [23, 44, 53]. These methods have significantly improved LLMs’ efficacy in code generation, enabling these systems to construct code repositories based on LLM. While these methods address the process of transitioning code repositories from inception to establishment, they rarely consider the handling of software evolution, e.g., resolving GitHub issues. For GitHub repositories, especially the popular ones, a large number of commits are pushed every day. These commits derive from a spectrum of evolutionary requirements that span bug fixes, feature additions, performance enhancements, etc [50]. For open-source software, new requirements frequently emerge as issues in the project’s repository.</p>
<p>Recently, Jimenez et al. [27] developed a benchmark, namely SWE-bench, to investigate the capability of popular LLMs in addressing GitHub issues. Their study reveals that LLMs fail to resolve over $95\%$ of instances, even when file paths that require modifications are provided. This significantly low rate underscores the importance of understanding the reasons behind their suboptimal performance.</p>
<p>In this study, we analyze the factors impacting the effectiveness of LLMs in resolving GitHub issues. Furthermore, our empirical analysis has concluded a correlation between locating files/lines to be modified and the performance of resolving GitHub issues. Based on these insights, we propose a novel LLM-based multi-agent framework, termed MAGIS, comprising four types of agents: Manager, Repository Custodian, Developer, and Quality Assurance (QA) Engineer. Our approach facilitates the resolution of GitHub issues through collaboration among agents, each fulfilling a unique role: the Manager coordinates the entire process, the Repository Custodian enhances locating files, the Developer performs code changes after locating lines, and the QA Engineer reviews the code change.</p>
<p>In our experiment, we evaluate our framework on SWE-bench and compare its performance against existing popular LLMs, such as ChatGPT-3.5 [37], GPT-4 [38], and Claude-2 [1]. The results demonstrate that our framework, utilizing GPT-4 as its base model, significantly outperforms baselines and achieves an eight-fold performance gain compared to the direct application of GPT-4. Further analysis reveals that additional factors, i.e., the planning of code change, locating lines within the code file, and code review process, can significantly influence the resolution rate.</p>
<p>Our main contributions are summarized as follows:</p>
<ul>
<li>We conduct an empirical analysis of LLMs in resolving GitHub issues and explore the correlation between locating code file/line, complexity of the code change, and the success rate in resolution.</li>
<li>We propose a novel LLM-based multi-agennt framework, MAGIS, to alleviate the limitations of existing LLMs on GitHub issue resolution. Both our designed four-type agents and their collaboration for planning and coding unlock LLMs’ potential on the repository-level coding task.</li>
<li>We compare our framework and other strong LLM competitors (i.e., GPT-3.5, GPT-4, and Claude-2) on the SWE-bench dataset. The results show MAGIS significantly outperforms these competitors. Further analysis confirms the effectiveness and necessity of our framework design.</li>
</ul>
<h2>2 Empirical Study</h2>
<p>SWE-bench [27] reveals the challenges LLMs face in addressing GitHub issue resolution. For example, in their evaluation, GPT-4 can only resolve less than 2% issues of the test set. Conversely, in tasks like function-level code generation, LLMs exhibit superior performance (e.g., GPT-4 gets the score of 67.0 on HumanEval [36]). Given the complexity of GitHub issue resolution akin to repository-level coding, we aim to investigate Why the Performance of Directly Using LLMs to Resolve GitHub Issue is Limited? (RQ 1). We answer this RQ from the following three aspects:</p>
<p>Locating the Files to be Modified. GitHub issue resolution is a repository-level coding task, distinguishing it from file-level coding tasks primarily in the challenge of locating the files requiring modification. Jimenez et al. [27] employ the BM25 method [46] to retrieve relevant code files that are subsequently utilized as input to the LLM. After employing retrieval methods, it is necessary to select the top-$K$ files or truncate the content based on the maximum context length of the LLM.</p>
<p>Incorporating more files can enhance recall scores. However, it also imposes significant demands on the capabilities of LLMs. As demonstrated by the study [27], Claude-2 exhibits a decrease in the resolved ratio (from 1.96 to 1.22) as recall scores increase (from 29.58 to 51.06). This decline may be attributed to the inclusion of irrelevant files or the limited capacity of LLMs to process longer contexts effectively. Consequently, optimizing the performance of LLMs can be better achieved by striving for higher recall scores with a minimized set of files, thus suggesting a strategic balance between recall optimization and the number of chosen files.</p>
<p>Locating the Lines to be Modified. Beyond the impact of file locating, we delve into the generation of failed instances when the correct modified files were provided. A typical code change consists of multiple hunks, each specifying the line numbers targeted for modification and detailing the changes made at these locations. To quantitatively analyze the accuracy of line localization, we use the line numbers' range of the modified content in the reference code change as the basis assuming that the correct modification location of the code change is uniquely determined in most cases. By calculating the coverage ratio of the line number ranges of the generated and reference, we can estimate the accuracy of line localization in the generation process, i.e.,</p>
<p>$$
\text { Coverage Ratio }=\frac{\sum_{i=0}^{n} \sum_{j=0}^{m}\left|\left[s_{i}, e_{i}\right] \cap\left[s_{j}^{\prime}, e_{j}^{\prime}\right]\right|}{\sum_{i=0}^{n}\left(e_{i}-s_{i}+1\right)}
$$</p>
<p>where the numerator is the length of the intersection of modified lines between the reference divided into $n$ hunks and the generation divided into $m$ hunks, and the denominator is the number of modified lines in the reference. More details about Equation 1 can be found in Appendix A.1.</p>
<p>For 574 instances in the SWE-bench that experiments GPT-4 [27], the distribution of the coverage ratio between the results generated by three LLMs and the reference is shown in Fig. 1. From this, we observe that the performance of LLMs in generating the code change is probably related to their ability to locate code lines accurately (Detailed explanation can be found in Appendix A.2).</p>
<p>Furthermore, we assess the relationship between the coverage ratio and the issue resolution by calculating their correlation coefficient. Given that the distribution of these variables exhibits skewness, and the resolution result is binary (resolved or not), logistic regression is employed for the analysis across three LLMs. However, due to the limited number of successfully generated instances on GPT-4 and GPT-3.5, a statistically significant relationship is only detected in the result generated by Claude-2. The result, i.e., P-value $&lt;0.05$, shows statistical significance. Specifically, with a coefficient, 0.5997 , on Claude-2, there is a substantial and positive relation between improvements in the coverage ratio and the probability of successfully resolving issues, which demonstrates that locating lines is a key factor for GitHub issue resolution.</p>
<p>Complexity of the Code Changes. The complexity of the code change is reflected in various indices: the number of modified files, functions, hunks, and lines added or deleted. Firstly, we quantitatively assess the complexity by calculating the value of various indices corresponding to the reference code change. Secondly, the coefficient is calculated between the numbers in each index and the issue resolution. Tab. 1 shows the correlation scores under the logistic regression.</p>
<p>As shown in Tab. 1, all three LLMs demonstrate a statistically significant correlation with the issue resolution across several indices. The correlation scores for the number of files and functions modified are notably negative for all models, indicating that an increase in these indices is associated with a decreasing likelihood of issue resolution. This suggests that the more complex the code change, as indicated by a higher number of files and functions modified, may hinder the issue resolution. More analysis can be found in Appendix A.3. The analysis reveals a relationship between the complexity, as measured by several indices, and whether to successfully resolve the issues in software evolution. The negative correlations suggest that increased complexity, particularly in terms of the number of files and functions changed, tends to hinder issue resolution.</p>
<p>Table 1: Correlation between the complexity indices and the issue resolution.</p>
<table>
<thead>
<tr>
<th>LLM</th>
<th># Files</th>
<th># Functions</th>
<th># Hunks</th>
<th># Added LoC</th>
<th># Deleted LoC</th>
<th># Changed LoC</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-3.5</td>
<td>$-17.57^{*}$</td>
<td>$-17.57^{*}$</td>
<td>$-0.06^{*}$</td>
<td>$-0.02$</td>
<td>$-0.03$</td>
<td>$-0.53^{*}$</td>
</tr>
<tr>
<td>GPT-4</td>
<td>$-25.15^{*}$</td>
<td>$-25.15^{*}$</td>
<td>$-0.06$</td>
<td>$-0.10$</td>
<td>$-0.04$</td>
<td>$-0.21$</td>
</tr>
<tr>
<td>Claude-2</td>
<td>$-1.47^{*}$</td>
<td>$-1.47^{*}$</td>
<td>$-0.11^{*}$</td>
<td>$-0.09^{*}$</td>
<td>$-0.07^{*}$</td>
<td>$-0.44^{*}$</td>
</tr>
</tbody>
</table>
<ul>
<li>The correlation between the index and the issue resolution is significant (P-value $&lt;0.05$).
<img alt="img-0.jpeg" src="img-0.jpeg" /></li>
</ul>
<p>Figure 2: Overview of our framework, MAGIS. The detailed version can be found in Fig. 14.</p>
<h1>3 Methodology</h1>
<p>Based on the empirical study identifying key factors affecting LLMs' issue resolution, we design the framework illustrated in Fig. 2. This framework aims to mitigate negative impacts by transforming the complex task of GitHub issue resolution into a collaborative effort. It incorporates four key roles for LLM-based agents working collaboratively in the workflow: (1) Manager: this role tasks with team assembly, meeting organization, and plan formulation. (2) Repository Custodian: it is responsible for locating the relevant files in the repository acording to the GitHub issue and recording the change of the repository. (3) Developer: this role participates in planning discussions and completes tasks from the Manager. (4) Quality Assurance (QA) Engineer: it reviews the code change from Developers to ensure the quality of the whole repository.
The collaborative process involves planning and coding. In the planning, an issue is assigned to the Manager and the Repository Custodian. The custodian identifies candidate files relevant to the issue for modification. With the issue description and a list of candidate files, the Manager defines tasks and assembles a team, where each member is a Developer specifically designed for the defined task. The Manager holds a kick-off meeting with Developers and devises a plan. During coding, Developers undertake their assigned tasks from the Manager, and the QA Engineer reviews each code change. If a change fails to meet quality standards, the QA Engineer provides feedback, prompting further revisions until the QA Engineer approves or a set iteration limit is reached.</p>
<h3>3.1 Agent Role Design</h3>
<p>Our workflow draws inspiration from the GitHub Flow[22], an effective human workflow paradigm adopted by many software teams. Both the human workflow and our LLM-based agent framework prioritize collaboration among individuals with diverse skills. While the underlying principles are similar, there are notable differences. Accordingly, we have tailored the roles as follows:</p>
<ul>
<li>
<p>Manager. The Manager's role is pivotal in planning. In conventional setups, managers decompose the issue into tasks according to the pre-formed team and allocate these tasks for members with different skills. In contrast, our Manager agent can first decompose the issue into tasks and then design Developer agents to form a team. This setup improves team flexibility and adaptability, enabling the formation of teams that can meet various issues efficiently.</p>
</li>
<li>
<p>Repository Custodian. Considering extensive files in a repository, the custodian agent's task is to locate files relevant to the issue. Unlike humans, who can browse through the entire repository, the LLM-based agent faces challenges in browsing. Although LLMs have extended context limits, their application is constrained in two aspects. First, it is a high computational cost to query each file in an entire repository for each update, particularly when some repositories update frequently. Second, the performance of LLMs degrades when the context input is long [31, 33, 68].</p>
</li>
<li>2 Developer. Compared to human developers, the Developer agent can work continuously and efficiently. Therefore, scheduling the agent to work in parallel is easier than scheduling humans who require considering factors beyond the task. Additionally, although numerous developer agents are capable of generating code [23, 44], their ability to modify existing code is not equally proficient. To address this issue, our framework decomposes the code modification process into sub-operations including code generation. This approach enables Developers to leverage the benefits of automatic code generation thereby producing applicable code changes.</li>
<li>3 QA Engineer. In software evolution, QA Engineers play a crucial role in maintaining software quality through code review [34, 30]. Despite their importance, code review practices are often undervalued or even overlooked [4]. Such neglect can hinder software development, illustrated by instances where developers may experience delays of up to 96 hours awaiting code review feedback [6]. To address this problem, our framework pairs each Developer agent with a QA Engineer agent, designed to offer task-specific, timely feedback. This personalized QA approach aims to boost the review process thereby better ensuring the software quality.</li>
</ul>
<h1>3.2 Collaborative Process</h1>
<h3>3.2.1 Planning</h3>
<p>Three types of role agents engage in the planning: Repository Custodian, Manager, and Developer. This process comprises three phases: locating code files, team building, and kick-off meeting.</p>
<p>Locating Code Files. Firstly, the Repository Custodian employs the BM25 algorithm [46] to rank the files in the repository based on the GitHub issue description. Subsequently, the top $k$ files are selected as potential candidates for further coding. However, as described in $\S 2$, this simple retrieval method can introduce irrelevant files, increasing the cost and reducing the effectiveness of subsequent coding process. Therefore, we filter these files based on relevance to minimize their number. While it is feasible to directly assess the relevance between each file and the issue by LLMs, queries to the LLM may contain the same code snippets as previous ones, leading to unnecessary computational costs. Considering that applying the code change often modifies a specific part of the file rather than the entire file, we propose a memory mechanism to reuse the previously queried information.
Algorithm 1 outlines the process of locating files with our designed memory $\mathcal{M}$. If a file $f_{i}$ is compared for the first time with an issue $q_{x}$, the LLM $\mathcal{L}$ with prompt $\mathcal{P}<em i="i">{2}$ compresses it into the summary $s</em>$ can}$, where $i$ denotes the file's version. This summary is shorter than the code content in the file and it is stored in memory for future reuse. If the file $f_{i}$ has been previously compared, the latest previous version $(h)$ of the file $f_{h}$ can be found by the script find. Since $f_{i}$ can be represented as the combination of $f_{h}$ and the difference between them ( $\Delta d$ that be obtained via the "git diff" command), LLMs can understand $f_{i}$ by using $f_{h}$ and $\Delta d$. If the difference is small and the file $f_{i}$ is long, it is valuable to reuse the previous summary $s_{h}$ stored in memory rather than the content of $f_{i}$. Specifically, if the length of $s_{h}$ is less than that of $f_{i}, \mathcal{L}$ with prompt $\mathcal{P}_{1</p>
<div class="codehilite"><pre><span></span><code>Algorithm <span class="mi">1</span> Locating<span class="o">.</span>
    Input<span class="p">:</span> repository<span class="p">:</span> <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>R<span class="p">}</span>_<span class="p">{</span>i<span class="p">}</span><span class="err">\</span><span class="p">)</span> including files
        <span class="err">\</span><span class="p">(</span><span class="err">\</span>left<span class="err">\</span><span class="p">{</span>f_<span class="p">{</span>i<span class="p">}</span><span class="err">\</span>right<span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span> GitHub issue<span class="p">:</span> <span class="err">\</span><span class="p">(</span>q_<span class="p">{</span>x<span class="p">}</span><span class="err">\</span><span class="p">),</span> LLM<span class="p">:</span> <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>L<span class="p">}</span><span class="err">\</span><span class="p">)</span>
    Config<span class="p">:</span> filter top width<span class="p">:</span> <span class="err">\</span><span class="p">(</span>k<span class="err">\</span><span class="p">),</span> prompts<span class="p">:</span> <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>P<span class="p">}</span><span class="err">\</span><span class="p">),</span>
        find the latest previous version of the file
        <span class="ow">and</span> its summary<span class="p">:</span> find
    Output<span class="p">:</span> candidate files<span class="p">:</span> <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>C<span class="p">}</span>_<span class="p">{</span>i<span class="p">}</span><span class="err">^</span><span class="p">{</span>k<span class="p">}</span> <span class="err">\</span>leftarrow <span class="err">\</span>emptyset<span class="err">\</span><span class="p">),</span> repos-
        itory evolution memory<span class="p">:</span> <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>M<span class="p">}</span> <span class="err">\</span>leftarrow <span class="err">\</span>emptyset<span class="err">\</span><span class="p">)</span>
    <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>R<span class="p">}</span>_<span class="p">{</span>i<span class="p">}</span> <span class="err">\</span>leftarrow <span class="err">\</span>operatorname<span class="p">{</span>BM25<span class="p">}</span><span class="err">\</span>left<span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>R<span class="p">}</span>_<span class="p">{</span>i<span class="p">},</span> q_<span class="p">{</span>x<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span>
    <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>C<span class="p">}</span>_<span class="p">{</span>i<span class="p">}</span><span class="err">^</span><span class="p">{</span>h<span class="p">}</span> <span class="err">\</span>leftarrow <span class="err">\</span>mathcal<span class="p">{</span>R<span class="p">}</span>_<span class="p">{</span>i<span class="p">}</span><span class="err">\</span>lfloor<span class="err">|</span>k<span class="err">|\</span><span class="p">)</span>
    for <span class="err">\</span><span class="p">(</span>f_<span class="p">{</span>i<span class="p">}</span> <span class="err">\</span><span class="k">in</span> <span class="err">\</span>mathcal<span class="p">{</span>C<span class="p">}</span>_<span class="p">{</span>i<span class="p">}</span><span class="err">^</span><span class="p">{</span>h<span class="p">}</span><span class="err">\</span><span class="p">)</span> do
        <span class="err">\</span><span class="p">(</span>f_<span class="p">{</span>h<span class="p">},</span> s_<span class="p">{</span>h<span class="p">}</span> <span class="err">\</span>leftarrow <span class="err">\</span>operatorname<span class="p">{</span>find<span class="p">}</span><span class="err">\</span>left<span class="p">(</span>f_<span class="p">{</span>i<span class="p">},</span> <span class="err">\</span>mathcal<span class="p">{</span>M<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span>
        <span class="k">if</span> <span class="err">\</span><span class="p">(</span><span class="err">\</span>exists f_<span class="p">{</span>h<span class="p">}</span><span class="err">\</span><span class="p">)</span> <span class="ow">and</span> <span class="err">\</span><span class="p">(</span><span class="err">\</span>operatorname<span class="p">{</span>len<span class="p">}</span><span class="err">\</span>left<span class="p">(</span>s_<span class="p">{</span>h<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="o">&lt;</span><span class="err">\</span>operatorname<span class="p">{</span>len<span class="p">}</span><span class="err">\</span>left<span class="p">(</span>f_<span class="p">{</span>i<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span> <span class="k">then</span>
            <span class="k">if</span> <span class="err">\</span><span class="p">(</span>h<span class="err">\</span><span class="p">)</span> is <span class="err">\</span><span class="p">(</span>i<span class="err">\</span><span class="p">)</span> <span class="k">then</span>
                <span class="err">\</span><span class="p">(</span>s_<span class="p">{</span>i<span class="p">}</span> <span class="err">\</span>leftarrow s_<span class="p">{</span>h<span class="p">}</span><span class="err">\</span><span class="p">)</span>
            <span class="k">else</span>
                <span class="err">\</span><span class="p">(</span><span class="err">\</span>Delta d <span class="err">\</span>leftarrow <span class="err">\</span>operatorname<span class="p">{</span>diff<span class="p">}</span><span class="err">\</span>left<span class="p">(</span>f_<span class="p">{</span>h<span class="p">},</span> f_<span class="p">{</span>i<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span>
                <span class="err">\</span><span class="p">(</span>m <span class="err">\</span>leftarrow <span class="err">\</span>mathcal<span class="p">{</span>L<span class="p">}</span><span class="err">\</span>left<span class="p">(</span><span class="err">\</span>Delta d<span class="p">,</span> <span class="err">\</span>mathcal<span class="p">{</span>P<span class="p">}</span>_<span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span>
                <span class="err">\</span><span class="p">(</span>s_<span class="p">{</span>i<span class="p">}</span> <span class="err">\</span>leftarrow s_<span class="p">{</span>h<span class="p">}</span> <span class="err">\</span>cup m<span class="err">\</span><span class="p">)</span>
            end <span class="k">if</span>
        <span class="k">else</span>
            <span class="err">\</span><span class="p">(</span>s_<span class="p">{</span>i<span class="p">}</span> <span class="err">\</span>leftarrow <span class="err">\</span>mathcal<span class="p">{</span>L<span class="p">}</span><span class="err">\</span>left<span class="p">(</span>f_<span class="p">{</span>i<span class="p">},</span> <span class="err">\</span>mathcal<span class="p">{</span>P<span class="p">}</span>_<span class="p">{</span><span class="mi">2</span><span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span>
        end <span class="k">if</span>
        <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>M<span class="p">}</span> <span class="err">\</span>leftarrow <span class="err">\</span>mathcal<span class="p">{</span>M<span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="o">.</span>update <span class="err">\</span><span class="p">(</span><span class="err">\</span>left<span class="p">(</span><span class="err">\</span>left<span class="err">\</span><span class="p">{</span>f_<span class="p">{</span>i<span class="p">}:</span> s_<span class="p">{</span>i<span class="p">}</span><span class="err">\</span>right<span class="err">\</span><span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span>
        <span class="k">if</span> <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>L<span class="p">}</span><span class="err">\</span>left<span class="p">(</span><span class="err">\</span>left<span class="p">(</span>s_<span class="p">{</span>i<span class="p">},</span> q_<span class="p">{</span>x<span class="p">}</span><span class="err">\</span>right<span class="p">),</span> <span class="err">\</span>mathcal<span class="p">{</span>P<span class="p">}</span>_<span class="p">{</span><span class="mi">3</span><span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span> is <span class="no">false</span> <span class="k">then</span>
            <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathcal<span class="p">{</span>C<span class="p">}</span>_<span class="p">{</span>i<span class="p">}</span><span class="err">^</span><span class="p">{</span>h<span class="p">}</span> <span class="err">\</span>leftarrow <span class="err">\</span>mathcal<span class="p">{</span>C<span class="p">}</span>_<span class="p">{</span>i<span class="p">}</span><span class="err">^</span><span class="p">{</span>h<span class="p">}</span><span class="o">-</span>f_<span class="p">{</span>i<span class="p">}</span><span class="err">\</span><span class="p">)</span>
        end <span class="k">if</span>
    end for
</code></pre></div>

<p>summarize the code changes $\Delta d$ as a "commit message" $m$. The combination of $s_{h}$ and $m$ forms the description of the newer version $f_{i}$, enabling the LLM $\mathcal{L}$ with prompt $\mathcal{P}_{3}$ to determine whether it is relevant to the issue in fewer context length. Based on their relevance, the custodian agent filters irrelevant files, allowing the Manager agent to define tasks with remaining relevant files.</p>
<p>Team Building. In this process, the Manager agent has the flexibility to "recruit" team members as the issue needs. Firstly, upon receiving the located files, the Manager begins with analyzing the GitHub issue for the repository and breaks them into detailed file-level tasks. Specifically, for each code file $f_{i}$ in the candidate set $\mathcal{C}<em 4="4">{i}^{k}$, the Manager leverages the LLM $\mathcal{L}$ with the prompt $\mathcal{P}</em>}$ and the issue description $q_{x}$ to define the corresponding file-level task $t_{i}$. One issue can be converted to multiple tasks. These tasks, along with the associated code file, are stored in a task set $\mathcal{T<em i="i">{i}^{k}$. Once a task is clarified, the Manager defines the personality role $r</em>}$ of the Developer by invoking LLM $\mathcal{L}$ with the prompt $\mathcal{P<em i="i">{5}$ and the task $t</em>$, thus forming the development team. The details of the team building are shown in Algorithm 2. This approach simplifies the task for LLMs because each team member only needs to handle a sub-task rather than resolving the entire complex issue.}$. By iterating through these candidate code files, the Manager agent ultimately designs a collection of Developer agent role descriptions $\mathcal{D}_{i}^{k</p>
<p>Kick-off Meeting. After building the team, the Manager organizes a kick-off meeting. This meeting serves two purposes: (1) To confirm whether the tasks</p>
<div class="codehilite"><pre><span></span><code>Algorithm 2 Making the plan.
    Input: candidate files: \(\mathcal{C}_{i}^{k}\), issue: \(q_{x}\), LLM: \(\mathcal{L}\)
    Config: prompts: \(\mathcal{P}\)
    Output: tasks: \(\mathcal{T}_{i}^{k} \leftarrow \emptyset\), Developer agents&#39;
        role description: \(\mathcal{D}_{i}^{k} \leftarrow \emptyset\), plan: \(c_{\text {main }}\)
        for \(f_{i} \in \mathcal{C}_{i}^{k}\) do
            \(t_{i} \leftarrow \mathcal{L}\left(\left(f_{i}, q_{x}\right), \mathcal{P}_{4}\right)\)
            \(\mathcal{T}_{i}^{k} \leftarrow \mathcal{T}_{i}^{k} \cup\left(f_{i}, t_{i}\right)\)
            \(r_{i} \leftarrow \mathcal{L}\left(\left(t, q_{x}\right), \mathcal{P}_{5}\right)\)
            \(\mathcal{D}_{i}^{k} \leftarrow \mathcal{D}_{i}^{k} \cup r_{i}\)
        end for
        recording \(=\) kick_off_meeting \(\left(\mathcal{D}_{i}^{k}\right)\)
        \(\mathcal{D}_{i}^{k} \leftarrow \mathcal{L}\left(\left(\mathcal{D}_{i}^{k}\right.\), recording \(), \mathcal{P}_{6}\right)\)
        \(c_{\text {main }} \leftarrow \mathcal{L}\left(\right.\) recording, \(\left.\mathcal{P}_{7}\right)\)
</code></pre></div>

<p>assigned by the Manager are reasonable and ensure that all Developers in the team can collaboratively resolve the issue $q_{x}$, (2) To determine which Developers' tasks can be executed concurrently and which tasks have dependencies need to be sorted. The meeting takes the form of a circular speech: the Manager is responsible for opening the speech, guiding the discussion and summarizing the results, and the Developers provide their opinions based on previous discussions in turn. One example of the meeting can be found in Appendix B. After the meeting, Developers adjust their role descriptions $\mathcal{D}<em 7="7">{i}^{k}$ based on the discussion recording, and the Manager, leveraging the LLM $\mathcal{L}$ and the prompt $\mathcal{P}</em>$. This plan is presented as code, and embedded into the program for execution. The meeting makes collaboration among Developers more efficient and avoids potential conflicts.}$, generates a main work plan $c_{\text {main }</p>
<h1>3.2.2 Coding</h1>
<p>Based on the empirical study on line locating and the complexity (§2), we transform the code change generation into the multi-step coding process that is designed to leverage the strengths of LLMs in code generation while mitigating their weaknesses in code change generation. Two types of agents participate in the coding process: Developers and QA Engineers. As outlined in Algorithm 3, for each task $t_{i}$ and its associated code file $f_{i}$ in $\mathcal{T}<em i="i">{i}^{k}$, the Developer agent generates the role description of the QA Engineer $a</em>}$ by the LLM $\mathcal{L}$ with the prompt $\mathcal{P<em i="i">{8}$. Subsequently, Developers collaborate with their QA Engineers to execute the coding tasks. During each execution of the Developer, the range of lines of code that need to be modified is firstly determined as a set of intervals $\left{\left[s</em>$ is the ending line number. The determination is}^{\prime}, e_{i}^{\prime}\right]\right}$ where $s_{i}^{\prime}$ represents the starting line number in the $i$-th hunk, and $e_{i}^{\prime</p>
<div class="codehilite"><pre><span></span><code>Algorithm 3 Coding task execution.
    Input: file-task pairs set: \(\mathcal{T}_{i}^{k}\), LLM: \(\mathcal{L}\)
    Config: prompts: \(\mathcal{P}\), the max of iteration: \(n_{\text {max }}\)
    Output: code changes: \(\mathcal{D}\)
    for \(f_{i}, t_{i} \in \mathcal{T}_{i}^{k}\) do
        \(a_{i} \leftarrow \mathcal{L}\left(\left(f_{i}, t_{i}\right), \mathcal{P}_{8}\right)\)
        for \(j \in\left\{0, n_{\max }\right)\) do
            if \(j&gt;0\) then
                \(t_{i}=\left(t_{i}\right.\), review_comment \()\)
            end if
            \(\left\{\left[s_{i}^{\prime}, e_{i}^{\prime}\right]\right\} \leftarrow \mathcal{L}\left(\left(f_{i}, t_{i}\right), \mathcal{P}_{9}\right)\)
            \(f_{i}\), old_part \(\leftarrow \operatorname{split}\left(f_{i},\left\{\left[s_{i}^{\prime}, e_{i}^{\prime}\right]\right\}\right)\)
            new_part \(\leftarrow \mathcal{L}\left(\left(f_{i}, t_{i}\right.\), old_part \(\left.), \mathcal{P}_{10}\right)\)
            \(f_{i}^{\prime} \leftarrow \operatorname{replace}\left(f_{i},\left\{\left[s_{i}^{\prime}, e_{i}^{\prime}\right]\right\}\right.\), new_part \()\)
            \(\Delta d_{i} \leftarrow \operatorname{diff}\left(f_{i}, f_{i}^{\prime}\right)\)
            review_comment \(=\mathcal{L}\left(\left(t_{i}, \Delta d_{i}\right), \mathcal{P}_{11}\right)\)
            review_decision \(=\mathcal{L}(\) (review_comment) \(, \mathcal{P}_{11}\) )
            if review_decision is true then
                break
            end if
        end for
        \(\Delta d \leftarrow \operatorname{diff}\left(f_{i}^{\prime}, f_{i}\right)\)
        \(\mathcal{D} \leftarrow \mathcal{D} \cup \Delta d\)
    end for
</code></pre></div>

<p>generated by analyzing the task content $t_{i}$ and file content $f_{i}$ using $\mathcal{L}$ with the prompt $\mathcal{P}<em i="i">{9}$. These intervals split the original code file $f</em>}$ into parts to be modified (old_part) and parts to be retained. Developers then generate new code snippets, new_part, by $\mathcal{L}$ with the prompt $\mathcal{P<em i="i">{10}$. The code snippets replace old_part, resulting in a new version of the code file $f</em>$ as the issue solution.}^{\prime}$. Utilizing Git tools, the code change $\Delta d_{i}$ for this file $f_{i}$ is generated. With the code change $\Delta d_{i}$, QA Engineer produce review_comment and review_decision, by the LLM $\mathcal{L}$ with the prompt $\mathcal{P}_{11}$. If the decision, review_decision, is negative (i.e., false), the feedback, review_comment, prompts Developers to revise the code in the next attempt. This iterative process continues until the code change meets the quality standards (i.e., review_decision is true) or reaches a predefined maximum number of iterations. After the iteration, the final version of the code change, $\Delta d$, is fixed, which is the ultimate modification result on each file. All generated final-version code changes during this process are merged into the repository-level code change $\mathcal{D</p>
<h1>4 Experiments and Analysis</h1>
<h3>4.1 Setup</h3>
<p>In the experiments, we employ the SWE-bench dataset as the evaluation benchmark because it is the latest dataset specifically designed for evaluating the performance of the GitHub issue resolution. SWE-bench comprises 2,294 issues extracted from 12 popular Python repositories, representing real software evolution requirements. Given the observation that experimental outcomes on the $25 \%$ subset of SWE-bench align with those obtained from the entire dataset [27], we opt for the same $25 \%$ subset previously utilized in experiments for GPT-4 according to their materials [13]. Moreover, the experimental scores for the five LLMs, have been made available by them [28].
Our framework is flexible to integrate various LLMs. To compare with the scores reported by SWE-bench, GPT-4 is selected as the base LLM. Another reason for the selection is that GPT-4 shows remarkable performance on code generation and understanding as demonstrated on benchmarks such as MBPP [2] and HumanEval [12]. Claude-2 is not chosen due to the unavailability of API access.
Following SWE-bench [27], the applied and resolved ratio is used to evaluate the performance under the setting with the files requiring modification provided. The applied ratio indicates the proportion of instances where the code change is successfully generated and can be applied to the code repository by Git. The resolved ratio refers to the proportion of instances where the code change is successfully applied and passes a series of tests. Additional elaboration is provided in Appendix C.</p>
<h3>4.2 How Effective is Our Framework? (RQ 2)</h3>
<p>The comparative performance analysis between our framework and other LLMs on the same dataset is presented in Tab. 2. The results indicate that our framework significantly outperforms other LLMs. Notably, with a resolved ratio of $13.94 \%$, our framework's effectiveness is eight-fold that of the base LLM, GPT-4. This substantial increase underscores our framework's capability to harness the potential of LLMs more effectively. Furthermore, when contrasted with the previous state-of-the-art LLM, Claude-2, our framework's resolved ratio exceeds that benchmark by more than two-fold. This superior performance unequivocally establishes the advance of our method.</p>
<p>The ablation study is designed to simulate two scenarios: (1) Without QA (w/o QA): Considering the QA Engineer agent as optional within our framework, we directly evaluate the code changes generated by the Developer agent, bypassing the QA process. This scenario aims to investigate the effectiveness and necessity of QA Engineer review. (2) Without hints (w/o hints): Hints refer to the textual content found in the comments section of pull requests, which are typically created before the first commit of the pull request. This setting means our framework oper-</p>
<p>Table 2: The comparison of overall performance between MAGIS and baselines on SWE-bench.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">\% Applied</th>
<th style="text-align: center;">\% Resolved</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: center;">11.67</td>
<td style="text-align: center;">0.84</td>
</tr>
<tr>
<td style="text-align: left;">Claude-2</td>
<td style="text-align: center;">49.36</td>
<td style="text-align: center;">4.88</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">13.24</td>
<td style="text-align: center;">1.74</td>
</tr>
<tr>
<td style="text-align: left;">SWE-Llama 7b</td>
<td style="text-align: center;">51.56</td>
<td style="text-align: center;">2.12</td>
</tr>
<tr>
<td style="text-align: left;">SWE-Llama 13b</td>
<td style="text-align: center;">49.13</td>
<td style="text-align: center;">4.36</td>
</tr>
<tr>
<td style="text-align: left;">MAGIS</td>
<td style="text-align: center;">$\mathbf{9 7 . 3 9}$</td>
<td style="text-align: center;">$\mathbf{1 3 . 9 4}$</td>
</tr>
<tr>
<td style="text-align: left;">MAGIS (w/o QA)</td>
<td style="text-align: center;">92.71</td>
<td style="text-align: center;">10.63</td>
</tr>
<tr>
<td style="text-align: left;">MAGIS (w/o hints)</td>
<td style="text-align: center;">94.25</td>
<td style="text-align: center;">10.28</td>
</tr>
<tr>
<td style="text-align: left;">MAGIS (w/o hints, w/o QA)</td>
<td style="text-align: center;">91.99</td>
<td style="text-align: center;">8.71</td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Comparison of recall scores between Ours and BM25.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Distribution of the correlation score between the generated task description and the reference code change.
ates without any clarifications except for the issue, despite such information being available on GitHub before the issue resolution process begins. This analysis aims to explore if the participation of humans could potentially improve the success rate of issue resolution.</p>
<p>Our framework shows a significant improvement in issue resolution, even without QA or hints. It achieves a resolved ratio of 8.71 , which is five times higher than that of the base LLM. This increase underscores the contribution of other agents in MAGIS to its overall performance. Furthermore, integrating cooperation with QA or hints separately can further elevate the resolved ratio by 1.92 or 1.57 , respectively. These findings underscore the value of QA Engineers and the participation of humans, as demonstrated by the resolved rates achieved through their integration.</p>
<p>For instance, to resolve the issue [17] from the repository Django [15], the developer modifies four hunks in two files [16], as shown in Fig. 15. Despite the availability of two provided files, our method opts for modifications in only one file, as illustrated in Figure 16. Remarkably, this simpler code change enables the repository to pass all requisite test cases.</p>
<p>Additional comparison can be found in Appendix D and E, and detailed case study is shown in Appendix H. Furthermore, the statistics on the generated code changes can be found in Appendix F.</p>
<h1>4.3 How Effective is Our Planning Process? (RQ 3)</h1>
<p>To investigate the effectiveness of the planning process, we analyze the Repository Custodian and Manager agent. The performance of the Repository Custodian agent is observed in the recall score versus the file number curve, as shown in Fig. 3. This curve demonstrates that our method consistently outperforms the BM25 baseline across varying numbers of selected files, indicating that our approach can identify the maximum number of relevant code files with the minimum selection.</p>
<p>For the Manager agent, we examined the alignment of its generated task descriptions with the reference code change by LLM. Following the study [64], we select GPT-4 as an evaluator to score the correlation between the reference code change and the generated task description. The correlation scores are determined based on a set of criteria defined in Tab. 6. A higher correlation score indicates a better alignment and thus, a more accurate and effective planning direction. The distribution of these correlation scores is presented in Fig. 4. Notably, most of the scores are 3 or above, implying that the majority of task descriptions are in the right direction concerning planning. Furthermore, the higher scores correlate with a higher probability of issue resolution, indicated by a larger proportion of "resolved" outcomes in scores 4 and 5 . This signifies that when the generated task description closely aligns with the reference, there is a higher possibility of resolving the issue. The analysis above demonstrates the effectiveness of both the Repository Custodian and the Manager agent in the planning process of our framework.</p>
<h3>4.4 How Effective is Our Coding Process? (RQ 4)</h3>
<p>To evaluate the effectiveness of the coding process in our framework, we analyze the performance of Developers in locating code lines and resolving issues of different complexity.</p>
<p>Fig. 5 illustrates the distribution of the line locating coverage ratio of MAGIS and the baselines. This visualization reveals that our Developer agent frequently attains a line locating coverage ratio nearing 1. Compared with baselines, the Developer agent demonstrates a pronounced preference for higher</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Comparison of line locating coverage between MAGIS (Ours) and baselines.</p>
<p>Figure 6: Resolved ratio in different line locating coverage intervals.
distribution values close to 1 , and conversely, a reduced preference for lower distribution values near 0 . Such a distribution validates the superior performance of MAGIS in locating code lines.</p>
<p>Further analysis is provided in Fig. 6 illustrating the relationship between the line locating coverage ratio and the issue resolved ratio within those coverages. As shown in Fig. 6, the right four bars are higher than the five left, which indicates that the resolved ratio can increase with the line locating coverage. This observation also suggests that locating lines accurately is important for issue resolution. The cumulative frequency curve, shown in orange, provides an additional analysis, indicating the cumulative proportion of issues resolved ratio up to each point along the line locating coverage. A steady increase in cumulative frequency accompanies the increase in line locating coverage, reinforcing the idea that resolving issues is more successful in areas of high coverage. The slope of the curve's left half is lower than that of the right half, indicating that the benefits of increasing the coverage ratio are less pronounced at lower coverage ratios than at higher ones. Therefore, the Developer agent should prioritize improving its capability of locating code lines.</p>
<p>Moreover, as shown in Tab. 3, we present a logistic regression analysis that quantifies the correlation between several complexity indices and issue resolution. The results show that GPT-4 has significant negative correlations across the number of files and functions, suggesting that as these indices increase, the likelihood of issue resolution decreases. Conversely, the negative correlations are less pronounced with our model, MAGIS, particularly in the number of files and functions, suggesting mitigation of challenges corresponding to these complexity indices.</p>
<p>Table 3: Correlation between the complexity indices and the issue resolution.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;"># Files</th>
<th style="text-align: center;"># Functions</th>
<th style="text-align: center;"># Hunks</th>
<th style="text-align: center;"># Added LoC</th>
<th style="text-align: center;"># Deleted LoC</th>
<th style="text-align: center;"># Changed LoC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">$-25.15^{*}$</td>
<td style="text-align: center;">$-25.15^{*}$</td>
<td style="text-align: center;">-0.06</td>
<td style="text-align: center;">-0.10</td>
<td style="text-align: center;">-0.04</td>
<td style="text-align: center;">-0.21</td>
</tr>
<tr>
<td style="text-align: left;">MAGIS</td>
<td style="text-align: center;">$-1.55^{*}$</td>
<td style="text-align: center;">$-1.55^{*}$</td>
<td style="text-align: center;">$-0.12^{*}$</td>
<td style="text-align: center;">$-0.04^{*}$</td>
<td style="text-align: center;">$-0.06^{*}$</td>
<td style="text-align: center;">$-0.57^{*}$</td>
</tr>
</tbody>
</table>
<ul>
<li>The correlation between the index and the issue resolution is significant ( $\mathrm{P} \cdot$ value $&lt;0.05$ ).</li>
</ul>
<p>To evaluate the performance of the QA Engineer, the ablation experiment is conducted and the results are shown in Tab. 2. As the table shows, in settings with and without hints, the presence of the QA Engineer can increase the resolved ratio by $1.57 \%$ and $3.31 \%$, respectively. This overall enhancement substantiates the QA Engineer's contribution to improving outcomes. Furthermore, a case detailed in Appendix I underscores the QA Engineer's effectiveness.</p>
<h1>5 Related Work</h1>
<p>Researchers have developed LLM-based multi-agent systems, enabling more complex task completion. For instance, MetaGPT [23, 24] simulates a programming team's Standardized Operating Procedures (SOPs) and achieves leading scores on benchmarks like HumanEval [12] and MBPP [2]. Similarly, ChatDev [44] functions as a virtual development company, decomposing requirements into atomic tasks and utilizing mutual communication and self-reflection to mitigate LLM hallucinations. While these systems excel in transforming requirements into code, they often overlook the challenges of code change generation during software evolution [25]. GitHub issues include different types of</p>
<p>requirements and most of them belong to bug fixing. Previous researchers have proposed methods to localize the bugs [66, 43] and some researchers explored various methods to automatic program repair $[58,7,56,3,60,54]$. The full version of related work can be found in Appendix J.</p>
<h1>6 Conclusion</h1>
<p>This paper illuminates the potential of LLMs in software development, particularly in resolving GitHub issues. Our empirical study identifies the challenges of direct LLM application. To address the challenges, we propose a novel LLM-based multi-agent framework, MAGIS, enhancing issue resolution through well-designed agents' collaboration. The superiority of MAGIS on the SWEbench against popular LLMs highlights its effectiveness, pointing towards a promising direction for integrating LLMs into software evolution workflows.</p>
<h2>References</h2>
<p>[1] Anthropic. Claude 2. https://www.anthropic.com/news/claude-2, 2023.
[2] Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. arXiv Preprint, abs/2108.07732, 2021. URL https: //arxiv.org/abs/2108.07732.
[3] Thomas H. Austin, Thomas Schmitz, and Cormac Flanagan. Multiple facets for dynamic information flow with exceptions. ACM Trans. Program. Lang. Syst., 39(3):10:1-10:56, 2017. doi: $10.1145 / 3024086$. URL https://doi.org/10.1145/3024086.
[4] Tobias Baum, Olga Liskin, Kai Niklas, and Kurt Schneider. Factors influencing code review processes in industry. In Thomas Zimmermann, Jane Cleland-Huang, and Zhendong Su, editors, Proceedings of the 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering, FSE 2016, Seattle, WA, USA, November 13-18, 2016, pages 85-96. ACM, 2016. doi: 10.1145/2950290.2950323. URL https://doi.org/10.1145/2950290.2950323.
[5] Tegawendé F. Bissyandé, David Lo, Lingxiao Jiang, Laurent Réveillère, Jacques Klein, and Yves Le Traon. Got issues? who cares about it? A large scale investigation of issue trackers from github. In IEEE 24th International Symposium on Software Reliability Engineering, ISSRE 2013, Pasadena, CA, USA, November 4-7, 2013, pages 188-197. IEEE Computer Society, 2013. doi: 10.1109/ISSRE.2013.6698918. URL https://doi.org/10.1109/ISSRE.2013.6698918.
[6] Amiangshu Bosu and Jeffrey C. Carver. Impact of developer reputation on code review outcomes in OSS projects: an empirical investigation. In Maurizio Morisio, Tore Dybå, and Marco Torchiano, editors, 2014 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement, ESEM '14, Torino, Italy, September 18-19, 2014, pages 33:133:10. ACM, 2014. doi: 10.1145/2652524.2652544. URL https://doi.org/10.1145/ 2652524.2652544.
[7] Islem Bouzenia, Premkumar T. Devanbu, and Michael Pradel. Repairagent: An autonomous, llm-based agent for program repair. arXiv Preprint, abs/2403.17134, 2024. doi: 10.48550/ ARXIV.2403.17134. URL https://doi.org/10.48550/arXiv.2403.17134.
[8] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi, Marco Túlio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with GPT-4. arXiv Preprint, abs/2303.12712, 2023. doi: 10.48550/ARXIV.2303.12712. URL https://doi.org/10.48550/arXiv.2303.12712.
[9] Jiayi Geng Carlos E. Jimenez, John Yang. Swe-bench lite. https://www.swebench.com/ lite.html, 2024.
[10] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators through multi-agent debate. arXiv Preprint, abs/2308.07201, 2023. doi: 10.48550/ARXIV.2308.07201. URL https://doi.org/10.48550/arXiv.2308.07201.</p>
<p>[11] Lichang Chen, Jiuhai Chen, Heng Huang, and Minhao Cheng. PTP: boosting stability and performance of prompt tuning with perturbation-based regularizer. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 1351213525. Association for Computational Linguistics, 2023. URL https://aclanthology.org/ 2023.emnlp-main. 833 .
[12] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. arXiv Preprint, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374.
[13] Google Drive. Swe-bench_api_generation. https://drive.google.com/drive/folders/ 1EnrKzGAnsb_NmZKyECGmA2DrAc8ZuJ80, 2024.
[14] Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng, and Yiling Lou. Classeval: A manually-crafted benchmark for evaluating llms on class-level code generation, 2023.
[15] Django Software Foundation. Django. https://github.com/django/django, 2024.
[16] Django Software Foundation. Fixed #30255 - fixed admindocs errors when rendering docstrings without leading newlines. https://github.com/django/django/pull/12155/files, 2024.
[17] Django Software Foundation. #30255 (docutils reports an error rendering view docstring when the first line is not empty). https://code.djangoproject.com/ticket/30255, 2024.
[18] Django Software Foundation. #30664 (sqlite3 migrations can fail when used quoted db_table.). https://code.djangoproject.com/ticket/30664, 2024.
[19] Django Software Foundation. Custom query - django. https://code.djangoproject.com/ query, May 11, 2024.
[20] Xinyang Geng and Hao Liu. Openllama: An open reproduction of llama, May 2023. URL https://github.com/openlm-research/open_llama.
[21] Inc. GitHub. Github. https://github.com, 2024.
[22] Inc. GitHub. Github flow. https://docs.github.com/en/get-started/using-github/ github-flow, 2024.
[23] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. Metagpt: Meta programming for a multi-agent collaborative framework, 2023. URL https://doi.org/10.48550/arXiv.2308.00352.
[24] Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Li Zhang, Lingyao Zhang, Min Yang, Mingchen Zhuge, Taicheng Guo, Tuo Zhou, Wei Tao, Wenyi Wang, Xiangru Tang, Xiangtao Lu, Xiawu Zheng, Xinbing Liang, Yaying Fei, Yuheng Cheng, Zongze Xu, and Chenglin Wu. Data interpreter: An LLM agent for data science. arXiv Preprint, abs/2402.18679, 2024. doi: 10.48550/ARXIV.2402.18679. URL https://doi.org/10.48550/arXiv.2402.18679.</p>
<p>[25] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John C. Grundy, and Haoyu Wang. Large language models for software engineering: A systematic literature review. arXiv Preprint, abs/2308.10620, 2023. doi: 10.48550/ARXIV.2308.10620. URL https://doi.org/10.48550/arXiv.2308.10620.
[26] Xing Hu, Xin Xia, David Lo, Zhiyuan Wan, Qiuyuan Chen, and Thomas Zimmermann. Practitioners' expectations on automated code comment generation. In 44th IEEE/ACM 44th International Conference on Software Engineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022, pages 1693-1705. ACM, 2022. doi: 10.1145/3510003.3510152. URL https://doi.org/10.1145/3510003.3510152.
[27] Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=VTF8yNQM66.
[28] Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Official comments to reviewer bfzn on swe-bench: Can language models resolve real-world github issues? https://openreview.net/forum?id=VTF8yNQM66\&amp;noteId= lfJF38VxJr, 2024.
[29] Thomas Johnsson. Attribute grammars as a functional programming paradigm. In Gilles Kahn, editor, Functional Programming Languages and Computer Architecture, Portland, Oregon, USA, September 14-16, 1987, Proceedings, volume 274 of Lecture Notes in Computer Science, pages 154-173. Springer, 1987. doi: 10.1007/3-540-18317-5 $_10$. URL https://doi.org/ $10.1007 / 3-540-18317-5 _10$.
[30] Oleksii Kononenko, Olga Baysal, Latifa Guerrouj, Yaxin Cao, and Michael W. Godfrey. Investigating code review quality: Do people and participation matter? In Rainer Koschke, Jens Krinke, and Martin P. Robillard, editors, 2015 IEEE International Conference on Software Maintenance and Evolution, ICSME 2015, Bremen, Germany, September 29 - October 1, 2015, pages 111-120. IEEE Computer Society, 2015. doi: 10.1109/ICSM.2015.7332457. URL https://doi.org/10.1109/ICSM.2015.7332457.
[31] Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen. Long-context llms struggle with long in-context learning. arXiv Preprint, abs/2404.02060, 2024. doi: 10.48550/ ARXIV.2404.02060. URL https://doi.org/10.48550/arXiv.2404.02060.
[32] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 43e9d647ccd3e4b7b5baab53f0368686-Abstract-Conference.html.
[33] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. arXiv Preprint, abs/2307.03172, 2023. doi: 10.48550/ARXIV.2307.03172. URL https://doi.org/10.48550/arXiv.2307.03172.
[34] Shane McIntosh, Yasutaka Kamei, Bram Adams, and Ahmed E. Hassan. The impact of code review coverage and code review participation on software quality: a case study of the qt, vtk, and ITK projects. In Premkumar T. Devanbu, Sung Kim, and Martin Pinzger, editors, 11th Working Conference on Mining Software Repositories, MSR 2014, Proceedings, May 31 - June 1, 2014, Hyderabad, India, pages 192-201. ACM, 2014. doi: 10.1145/2597073.2597076. URL https://doi.org/10.1145/2597073.2597076.
[35] Fangwen Mu, Xiao Chen, Lin Shi, Song Wang, and Qing Wang. Developer-intent driven code comment generation. In 45th IEEE/ACM International Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023, pages 768-780. IEEE, 2023. doi: 10.1109/ ICSE48619.2023.00073. URL https://doi.org/10.1109/ICSE48619.2023.00073.</p>
<p>[36] OpenAI. GPT-4 technical report. Arxiv Preprint, abs/2303.08774, 2023. doi: 10.48550/ ARXIV.2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774.
[37] OpenAI. Gpt-3.5 turbo fine-tuning and api updates. https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates, 2023.
[38] OpenAI. Gpt-4. https://openai.com/research/gpt-4, 2023.
[39] OpenDevin Team. OpenDevin: An Open Platform for AI Software Developers as Generalist Agents. https://github.com/OpenDevin/OpenDevin, 2024. Accessed: ENTER THE DATE YOU ACCESSED THE PROJECT.
[40] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. scikitlearn. https://github.com/scikit-learn/scikitlearn, 2024.
[41] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. [mrg] add seeds when n_jobs=1 and use seed as random_state. https://github.com/scikit-learn/scikit-learn/pull/9288, 2024.
[42] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Kmeans gives slightly different result for n_jobs=1 vs. n_jobs 1. https://github.com/scikit-learn/scikit-learn/issues/9784, 2024.
[43] Binhang Qi, Hailong Sun, Wei Yuan, Hongyu Zhang, and Xiangxin Meng. Dreamloc: A deep relevance matching-based framework for bug localization. IEEE Trans. Reliab., 71(1):235-249, 2022. doi: 10.1109/TR.2021.3104728. URL https://doi.org/10.1109/TR.2021.3104728.
[44] Chen Qian, Xin Cong, Wei Liu, Cheng Yang, Weize Chen, Yusheng Su, Yufan Dang, Jiahao Li, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development. arXiv Preprint, 2023.
[45] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training, 2018.
[46] Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. Okapi at TREC-3. In Donna K. Harman, editor, Proceedings of The Third Text REtrieval Conference, TREC 1994, Gaithersburg, Maryland, USA, November 2-4, 1994, volume 500-225 of NIST Special Publication, pages 109-126. National Institute of Standards and Technology (NIST), 1994. URL http://trec.nist.gov/pubs/trec3/papers/city.ps.gz.
[47] Jessica Shieh. Best practices for prompt engineering with openai api. OpenAI, February https://help. openai. com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api, 2023.
[48] Qiushi Sun, Zhirui Chen, Fangzhi Xu, Kanzhi Cheng, Chang Ma, Zhangyue Yin, Jianing Wang, Chengcheng Han, Renyu Zhu, Shuai Yuan, Qipeng Guo, Xipeng Qiu, Pengcheng Yin, Xiaoli Li, Fei Yuan, Lingpeng Kong, Xiang Li, and Zhiyong Wu. A survey of neural code intelligence: Paradigms, advances and beyond, 2024.
[49] Yashar Talebirad and Amirhossein Nadiri. Multi-agent collaboration: Harnessing the power of intelligent LLM agents. arXiv Preprint, abs/2306.03314, 2023. doi: 10.48550/ ARXIV.2306.03314. URL https://doi.org/10.48550/arXiv.2306.03314.
[50] Wei Tao, Yucheng Zhou, Yanlin Wang, Hongyu Zhang, Haofen Wang, and Wenqiang Zhang. Kadel: Knowledge-aware denoising learning for commit message generation. ACM Trans. Softw. Eng. Methodol., jan 2024. ISSN 1049-331X. doi: 10.1145/3643675. URL https: //doi.org/10.1145/3643675.
[51] The Cognition Team. Swe-bench technical report, 2024. URL https://www.cognition-labs.com/post/swe-bench-technical-report.</p>
<p>[52] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv Preprint, abs/2302.13971, 2023. doi: 10.48550/ARXIV.2302.13971. URL https://doi.org/10.48550/arXiv.2302.13971.
[53] Michele Tufano, Anisha Agarwal, Jinu Jang, Roshanak Zilouchian Moghaddam, and Neel Sundaresan. Autodev: Automated ai-driven development, 2024. URL https://doi.org/ 10.48550/arXiv. 2403.08299.
[54] Weishi Wang, Yue Wang, Shafiq Joty, and Steven C. H. Hoi. Rap-gen: Retrieval-augmented patch generation with codet5 for automatic program repair. In Satish Chandra, Kelly Blincoe, and Paolo Tonella, editors, Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2023, San Francisco, CA, USA, December 3-9, 2023, pages 146-158. ACM, 2023. doi: 10.1145/ 3611643.3616256. URL https://doi.org/10.1145/3611643.3616256.
[55] Yue Wang, Weishi Wang, Shafiq R. Joty, and Steven C. H. Hoi. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. In MarieFrancine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 8696-8708. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.emnlp-main.685. URL https://doi.org/10.18653/v1/2021.emnlp-main.685.
[56] Chu-Pan Wong, Priscila Santiesteban, Christian Kästner, and Claire Le Goues. Varfix: balancing edit expressiveness and search effectiveness in automated program repair. In Diomidis Spinellis, Georgios Gousios, Marsha Chechik, and Massimiliano Di Penta, editors, ESEC/FSE '21: 29th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Athens, Greece, August 23-28, 2021, pages 354-366. ACM, 2021. doi: 10.1145/3468264.3468600. URL https://doi.org/10.1145/3468264.3468600.
[57] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen LLM applications via multi-agent conversation framework. arXiv Preprint, abs/2308.08155, 2023. doi: 10.48550/ ARXIV.2308.08155. URL https://doi.org/10.48550/arXiv.2308.08155.
[58] Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. Automated program repair in the era of large pre-trained language models. In 45th IEEE/ACM International Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023, pages 14821494. IEEE, 2023. doi: 10.1109/ICSE48619.2023.00129. URL https://doi.org/10.1109/ ICSE48619.2023.00129.
[59] John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent computer interfaces enable software engineering language models, 2024.
[60] He Ye and Martin Monperrus. ITER: iterative neural repair for multi-location patches. In Proceedings of the 46th IEEE/ACM International Conference on Software Engineering, ICSE 2024, Lisbon, Portugal, April 14-20, 2024, pages 10:1-10:13. ACM, 2024. doi: 10.1145/ 3597503.3623337. URL https://doi.org/10.1145/3597503.3623337.
[61] Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. Autocoderover: Autonomous program improvement. arXiv Preprint, abs/2404.05427, 2024. doi: 10.48550/ ARXIV.2404.05427. URL https://doi.org/10.48550/arXiv.2404.05427.
[62] Ziyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao, Zi Gong, Hang Yu, Jianguo Li, and Rui Wang. Unifying the perspectives of nlp and software engineering: A survey on language models for code, 2024. URL https://arxiv.org/abs/2311.07989.
[63] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen,</p>
<p>Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language models. arXiv Preprint, abs/2303.18223, 2023. doi: 10.48550/ARXIV.2303.18223. URL https://doi.org/10.48550/arXiv.2303.18223.
[64] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html.
[65] Zibin Zheng, Kaiwen Ning, Jiachi Chen, Yanlin Wang, Wenqing Chen, Lianghong Guo, and Weicheng Wang. Towards an understanding of large language models in software engineering tasks. arXiv Preprint, abs/2308.11396, 2023. doi: 10.48550/ARXIV.2308.11396. URL https://doi.org/10.48550/arXiv.2308.11396.
[66] Jian Zhou, Hongyu Zhang, and David Lo. Where should the bugs be fixed? more accurate information retrieval-based bug localization based on bug reports. In Martin Glinz, Gail C. Murphy, and Mauro Pezzè, editors, 34th International Conference on Software Engineering, ICSE 2012, June 2-9, 2012, Zurich, Switzerland, pages 14-24. IEEE Computer Society, 2012. doi: 10.1109/ICSE.2012.6227210. URL https://doi.org/10.1109/ICSE.2012.6227210.
[67] Xiang Zhou, Xin Peng, Tao Xie, Jun Sun, Chao Ji, Wenhai Li, and Dan Ding. Fault analysis and debugging of microservice systems: Industrial survey, benchmark system, and empirical study. IEEE Trans. Software Eng., 47(2):243-260, 2021. doi: 10.1109/TSE.2018.2887384. URL https://doi.org/10.1109/TSE.2018.2887384.
[68] Yucheng Zhou, Xiubo Geng, Tao Shen, Chongyang Tao, Guodong Long, Jian-Guang Lou, and Jianbing Shen. Thread of thought unraveling chaotic contexts. arXiv Preprint, abs/2311.08734, 2023. doi: 10.48550/ARXIV.2311.08734. URL https://doi.org/ 10.48550/arXiv. 2311.08734.
[69] Tong Zhu, Xiaoye Qu, Daize Dong, Jiacheng Ruan, Jingqi Tong, Conghui He, and Yu Cheng. Llama-moe: Building mixture-of-experts from llama with continual pre-training. arXiv preprint arXiv:2406.16554, 2024. URL https://arxiv.org/abs/2406.16554.</p>
<h1>A Detailed Explanation in Empirical Study</h1>
<h2>A. 1 Coverage Ratio</h2>
<p>The formula for calculating the coverage ratio is Equation 1. As it shows, for each instance of GitHub issue resolution, the range of code change (in terms of the number of lines) in the reference $r$ is represented as a set of intervals $\boldsymbol{L}<em 0="0">{r}=\left{\left[s</em>}, e_{0}\right], \ldots,\left[s_{n}, e_{n}\right]\right}$, while the line ranges of the generated code change $g$ is $\boldsymbol{L<em 0="0">{g}=\left{\left[s</em>\right]\right}$, where $s$ and $e$ respectively represent the starting and ending line number of each modification hunk in the file, with $n$ hunks in the reference code change and $m$ hunks in the generated one.}^{\prime}, e_{0}^{\prime}\right], \ldots,\left[s_{m}^{\prime}, e_{m}^{\prime</p>
<h2>A. 2 Observation on Fig. 1</h2>
<p>As shown in Fig. 1, we observe that: (1) The distribution near the coverage ratio 0 (left side of the figure) is the highest for all three LLMs, indicating that in most cases, the content generated by these models has a very low coverage ratio with the reference in terms of locating code lines. This means that these LLMs are most likely not able to accurately locate code lines that need to be modified in the process of generating the code change. (2) In the distribution near the line locating coverage of 1 (right side of the figure), the three models show a consistent ranking (i.e., Claude-2 $&gt;$ GPT-4 $&gt;$ GPT-3.5) and this ranking is also consistent with the proportion of instances solved by the three models. This phenomenon suggests that the performance of LLMs in generating the code change is probably related to their ability to locate code lines accurately.</p>
<h2>A. 3 Analysis on Complexity of the Code Change</h2>
<p>As shown in Fig. 1, compared with GPT-3.5 and GPT-4, Claude-2 exhibits a different pattern, with much lower negative correlations for the number of files and functions, which indicates that it is a more efficient approach to generate the code change for GitHub issue resolution. However, it also shows significant negative correlations across other indices such as the number of hunks, added lines of code (LoC), deleted LoC, and changed LoC.</p>
<h2>B Kick-off Meeting Example</h2>
<p>Figure 7 illustrates a kick-off team meeting. In this meeting, three participants are present: the Manager agent, Oliver CodeLead, and two Developer agents, Django Database Specialist and Alex Rossini. They discuss a specific issue ${ }^{1}$, assigned tasks, and determine the workflow sequence.</p>
<h2>C Metrics</h2>
<p>The applied ratio indicates the proportion of instances where the code change is successfully generated and can be applied to the existing code repository using Git tools, i.e.,</p>
<p>$$
\text { Applied Ratio }=\frac{|\mathcal{D}|}{|\mathcal{I}|}
$$</p>
<p>where $\mathcal{D}$ represents the set of instances in the generated code change set that could be applied to the original code repository using the "git apply" operation, and $\mathcal{I}$ is the set of all instances in the test set. The resolved ratio refers to the proportion of instances in which the code change is successfully applied and passed a series of tests, i.e.,</p>
<p>$$
\text { Resolved Ratio }=\frac{\left|\sum_{i=0}^{l}\left(\left{T_{o l d}\left(d_{i}\right)\right} \cap\left{T_{\text {new }}\left(d_{i}\right)\right}\right)\right|}{|\mathcal{I}|}
$$</p>
<p>where $T_{o l d}$ denotes all the test cases that the old version of the code repository could pass, $T_{\text {new }}$ represents all the test cases designed for new requirements, and $d_{i}$ denotes the code change generated to resolve the issue in the $i$-th instance. Furthermore, $T(d)=$ True means that the code change $d$ can pass all the test cases in $T$.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7: Kick-off meeting to resolve the issue [18].</p>
<p>The recall score versus file number curve is used to measure the effectiveness of locating code files to be modified. The recall score refers to the proportion of files that are successfully located out of all the files that require modification. The formula for calculating the file locating recall score for the $i$-th instance is as follows:</p>
<p>$$
\text { Recall }=\frac{\left|\mathcal{G}<em i="i">{i} \cap \mathcal{R}</em> \times 100 \%
$$}\right|}{\left|\mathcal{R}_{i}\right|</p>
<p>where $\mathcal{G}<em j="0">{i}=\sum</em>}^{n} g_{i, j}$ represents the set of file paths located by our framework, with each file path in the set denoted as $g_{i, j}$ and the total number of files as $n ; \mathcal{R<em k="0">{i}=\sum</em>$ and the total file number as $m$. In this curve, "file number" refers to the average number of files that need to be processed across all instances to achieve the given recall score. Specifically, it illustrates how many files averagely need to be located by our framework before reaching the recall score denoted by the curve at any point. This metric represents both the effectiveness and efficiency of file locating.}^{m} r_{i, k}$ denotes the paths of the files that need to be modified, with each reference file path denoted as $r_{i, k</p>
<h1>D Comparison Result on SWE-bench Lite</h1>
<p>Recently, some contemporaneous works [39], e.g., AutoCodeRover [61] and SWE-Agent [59], have been proposed for this task. These methods are evaluated using SWE-bench lite, a canonical subset of SWE-bench, which is recommended for evaluation [9]. Considering budget constraints, we conducted experiments on SWE-bench lite to compare with them on the same issues' resolution.</p>
<p>The experimental results are shown in Tab. 4. MAGIS achieves the highest resolved ratio, $25.33 \%$, than other baselines. The performance of MAGIS slightly decreased when evaluated without QA, reaching $23.33 \%$, and dropped under the other two ablation settings. This comparative study underscores the robustness of MAGIS, particularly when provided with comprehensive inputs, and highlights the impact of QA and hints on its performance. The results indicate that while new methods like AutoCodeRover and SWE-Agent show promise, MAGIS remains an effective method for GitHub issue resolution.</p>
<p>Table 4: The comparison of overall performance between MAGIS and baselines on SWE-bench lite.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">AutoCodeRover</th>
<th style="text-align: center;">SWE-Agent</th>
<th style="text-align: center;">MAGIS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">w/o QA</td>
<td style="text-align: center;">w/o hints</td>
<td style="text-align: center;">w/o (hints, QA)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Resolved</td>
<td style="text-align: center;">$16.11 \%(22.33 \% *)$</td>
<td style="text-align: center;">$18.00 \%$</td>
<td style="text-align: center;">$\mathbf{2 5 . 3 3 \%}$</td>
<td style="text-align: center;">$23.33 \%$</td>
<td style="text-align: center;">$16.67 \%$</td>
<td style="text-align: center;">$16.00 \%$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>${ }^{*}$ Note that 16.11 is the average scores among 3 runs while 22.33 is under the union of from the 3 runs.</p>
<h2>E Comparison with Devin</h2>
<p>Devin is a novel agent for software development [51], and its performance has also been assessed using the SWE-bench. However, the evaluation dataset employed by Devin differs from the subset used for experiments with GPT-4 reported by the paper of SWE-bench [27]. An analysis of the repository name and pull request ID of each instance reveals that only 140 instances coverage between the two datasets.</p>
<p>Within the shared pool of 140 instances, our framework successfully resolves $21(15 \%)$ issues, surpassing Devin's resolution of $18(12.86 \%)$ issues ${ }^{2}$. This comparison, however, may not be entirely equitable. Devin's possible underlying LLM is unknown, and it possesses the capability to integrate feedback from the environment. Moreover, Devin's reported scores are under the setting given the entire repository, and it operates with "common developer tools including the shell, code editor, and browser", and "agents with internet access could potentially find external information through other methods" as detailed at the report ${ }^{3}$. In contrast, our approach solely relies on the shell, without the need of any additional external tools.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>For running time, 72\% of instances resolved by Devin require greater than 10 minutes to complete. In contrast, our framework finalizes each resolved issue within approximately 3 minutes. On average, our framework completes the processing of each instance in under 5 minutes, demonstrating its capability to assist in resolving GitHub issues with minimal time expenditure.</p>
<h1>F Statistics on the Generated Code Changes</h1>
<p>This section provides statistics on code changes corresponding to resolved issues and those applicable but unresolved using our framework.</p>
<p>The statistics on the code change for instances with resolved issues are presented in Tab. 5. Overall, the statistical information of the generated code changes for these instances, such as the average number of code files, functions, hunks, and deleted lines, all differ slightly (not exceeding 0.3 ) from the reference solutions written by humans. This indicates that for these instances, the complexity of the code change generated by our framework is similar to that of humans. Furthermore, the maximum values observed in the table reveal that our framework can implement code modifications involving two files, four hunks, and 1,655 lines modification, with single modifications reaching up to 190 lines. Results demonstrate the effectiveness of our method in resolving complex issues that need to modify the code file on multiple locations and with long context.</p>
<p>Specifically, the distribution of the number of modified lines for the resolving instances is shown in Fig. 8. We observe that the distribution of the number of modified lines in our framework for the solved instances exceeds that of the reference solution, especially in terms of the number of added lines being significantly higher than the reference. Upon manual inspection, we found that the generation results provided by our framework often contained more comment information, which led to an increase in the total number of modified lines. For example, Fig. 10 displays the generation result of our framework. Lines $365,368,371,374,383$ in the new version file correspond to the comment for the added code. These natural language descriptions are valuable in actual software evolution [26, 35]. In contrast, Fig. 12 shows a human-written solution lacking such explanatory comments, which might disadvantage software maintainers in reading and understanding.</p>
<p>The statistics on the code change for instances without resolved issues are shown in Tab. 5. From the table, our framework can generate applicable code changes including up to 13 files and 28 hunks, and the location of the modifications can be as far as line 7,150 , with a single modification reaching up to 9,367 lines. These results suggest that our method has a strong adaptability in generating applicable code changes. However, considering that these code changes have not passed all the potential test cases they could pass, which indicates that there is still room for improvement.</p>
<p>To further analyze the reasons behind the failure of test cases in these instances, we have quantified the distribution of the lengths of code changes in the unresolved instances, as shown in Fig. 9. From the figure, we observe that for unresolved instances, the framework tends to delete a larger number of lines while adding fewer lines, in contrast to the distribution of human-written changes. This
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 8: Distribution of the LoC in the resolved instances.
Figure 9: Distribution of the LoC in the applied but not resolved instances.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 10: Case from scikit-learn (ours, after review) for the issue [42].</p>
<p>Figure 11: Case from scikit-learn (ours, before review) for the issue [42].
discrepancy may point to different repair strategies or attitudes towards problem-solving, where the framework presented herein might prefer to reduce errors by removing potentially problematic code, whereas human developers may lean towards adding new code to address issues.</p>
<p>Moreover, a comparison between the resolved instances and not resolved ones shown in Tab. 5 reveals that the latter contains a higher overall number of files, hunks, and changed lines of code. These instances, involving more modification locations, correspond to more complex scenarios. This phenomenon suggests that the performance of our framework in resolving such complex issues requires further enhancement.</p>
<p>Furthermore, the variability in difficulty across different software repositories may influence the effectiveness of code changes. To this end, we compile statistics on the resolved ratios in various software repositories, as shown in Fig. 13. From the figure, we observe that there is a significant variation in the resolved ratios across different repositories in our framework. Some repositories have a resolved ratio as high as $40 \%$, while others are close to $0 \%$. This suggests that the differences among</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 12: Case from scikit-learn (gold) [41].
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 13: The number of applied and resolved instances in different repositories.
various software such as code structure and coding style can impact the generation and application of the code change.</p>
<p>Table 5: The statistical analysis of our framework on resolved and applied but not resolved instances.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Resolved Instances</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Applied but Not Resolved Instances</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MAGIS</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MAGIS</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Min</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">Min</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">Min</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;">Min</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;">Avg.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"># Code Files</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1.02</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1.04</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">1.50</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">1.61</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"># Functions</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1.02</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1.04</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">1.50</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">1.61</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"># Hunks</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1.45</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">1.66</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2.52</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">3.72</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"># Added Lines</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">146</td>
<td style="text-align: center;">9.75</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">4.34</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">920</td>
<td style="text-align: center;">40.38</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3,050</td>
<td style="text-align: center;">28.27</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"># Deleted Lines</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">5.27</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">115</td>
<td style="text-align: center;">5.16</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">9,160</td>
<td style="text-align: center;">327.27</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2,975</td>
<td style="text-align: center;">14.51</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Change Start Index</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1,655</td>
<td style="text-align: center;">270.12</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1,657</td>
<td style="text-align: center;">256.09</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">4,568</td>
<td style="text-align: center;">424.84</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">6,651</td>
<td style="text-align: center;">485.01</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Change End Index</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">1,665</td>
<td style="text-align: center;">301.68</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1,666</td>
<td style="text-align: center;">315.05</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">7,150</td>
<td style="text-align: center;">513.13</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">6,658</td>
<td style="text-align: center;">728.96</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"># Changed Lines</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">190</td>
<td style="text-align: center;">15.02</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">115</td>
<td style="text-align: center;">9.50</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">9,367</td>
<td style="text-align: center;">367.65</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">6,025</td>
<td style="text-align: center;">42.79</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<h1>G Evaluation on Task Description</h1>
<p>Since there is no ground truth for the task descriptions generated by the Manager, we utilize GPT-4 to simulate human evaluation and score each task description based on its corresponding reference code change. Table 6 illustrates the standards used by GPT-4 to assess the correlation between the task description and the code change. The score given by GPT-4 is considered the performance metric for the task description.</p>
<p>Table 6: The meaning of scores in GPT-4 evaluation on the correlation between the generated task description and the reference code change.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Score</th>
<th style="text-align: left;">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: left;">The code changes are unrelated to the task description.</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: left;">The code changes address a minor part of the task but are largely irrelevant.</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: left;">The code changes partially meet the task requirements but lack completeness or accuracy.</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: left;">The code changes are relevant and mostly complete, with minor discrepancies from the task description.</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: left;">The code changes perfectly align with the task description, fully addressing all specified requirements with high accuracy and completeness.</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://github.com/CognitionAI/devin-swebench-results/tree/main/output_diffs/ pass
${ }^{3}$ https://www.cognition-labs.com/introducing-devin&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
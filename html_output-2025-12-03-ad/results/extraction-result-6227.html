<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6227 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6227</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6227</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-122.html">extraction-schema-122</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <p><strong>Paper ID:</strong> paper-a4a41319d5805a29316f24ed9519f09db77d4c29</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a4a41319d5805a29316f24ed9519f09db77d4c29" target="_blank">Benchmarking Large Language Models for News Summarization</a></p>
                <p><strong>Paper Venue:</strong> Transactions of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> It is found instruction tuning, not model size, is the key to the LLM’s zero-shot summarization capability and summaries are judged to be on par with human written summaries.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown promise for automatic summarization but the reasons behind their successes are poorly understood. By conducting a human evaluation on ten LLMs across different pretraining methods, prompts, and model scales, we make two important observations. First, we find instruction tuning, not model size, is the key to the LLM’s zero-shot summarization capability. Second, existing studies have been limited by low-quality references, leading to underestimates of human performance and lower few-shot and finetuning performance. To better evaluate LLMs, we perform human evaluation over high-quality summaries we collect from freelance writers. Despite major stylistic differences such as the amount of paraphrasing, we find that LLM summaries are judged to be on par with human written summaries.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6227.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6227.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reference-based automated metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reference-based automated evaluation metrics (e.g., ROUGE-L, METEOR, BertScore, BLEURT, BARTScore)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Common automatic metrics that score a generated summary by comparing it to a reference summary; in this paper they are used to quantify correlation with human judgments across faithfulness, coherence, and relevance for news summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking Large Language Models for News Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Single-document news summarization (CNN/DailyMail and XSUM)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>ROUGE-L, METEOR, BertScore, BLEURT, BARTScore (these are the reference-based metrics used as 'automated judges')</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>30 US-based Mechanical Turk annotators (>=98% approval, >=10k HITs) rated 100 examples per dataset with 3 annotators per summary; criteria: binary faithfulness and 1–5 Likert coherence and relevance; reported pairwise agreement: faithfulness 75%, coherence 81%, relevance 86%.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Kendall's tau rank correlation between each automated metric and human ratings on faithfulness, coherence, and relevance (system-level).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>On CNN/DM, reference-based metrics achieve moderate correlations for some axes (e.g., ROUGE-L correlates with relevance at tau=0.72); on XSUM correlations are much weaker or even negative for some axes (e.g., ROUGE-L faithfulness tau = -0.27). Overall, reference-based metrics correlate better with human judgments on aspects and datasets where the references themselves score well (e.g., CNN/DM relevance) and poorly when references are low quality (e.g., XSUM).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>These metrics depend critically on reference quality: poor references degrade metric usefulness and can invert signal (negative correlation). They also preferentially reward surface overlap and can favor finetuned systems that match reference style over LLMs that paraphrase, thus misranking systems relative to human preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>On XSUM (where references are often just the first sentence and low quality), ROUGE-L and other reference-based metrics show very weak or negative correlations with human judgments; reference-based in-context conditioning using bad references made Instruct Davinci summaries worse (faithfulness dropped from 0.97 to 0.77 after 5-shot conditioning with XSUM references).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Use higher-quality reference summaries (the paper collected freelance-writer references and showed metric correlations improve when using them), complement reference-based metrics with reference-free evaluations and human judgments, and avoid training/evaluating models on low-quality references; control confounds like length when comparing outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Large Language Models for News Summarization', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6227.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6227.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reference-free / model-based metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reference-free and model-based automated evaluation metrics (e.g., SummaC, QAFactEval, BLANC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Metrics that assess generated summaries without direct reliance on a single gold reference, often via NLI, QA-based factuality checks, or cloze-style masking using language models; used to evaluate faithfulness and correlate with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking Large Language Models for News Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Single-document news summarization (CNN/DailyMail and XSUM)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>SummaC (NLI-based), QAFactEval (QA-based factual consistency), BLANC (blank-filling based), i.e., models and procedures acting as automated judges for faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Same Mechanical Turk setup as above: 3 annotators per summary, binary faithfulness plus Likert coherence and relevance; system-level correlations computed to compare metric outputs to these human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Kendall's tau between reference-free metrics and human judgments, focusing particularly on faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Reference-free metrics are less affected by low-quality references and tend to correlate best with faithfulness (e.g., QAFactEval tau=0.64 on CNN/DM faithfulness and 0.55 on XSUM faithfulness), but they correlate poorly with relevance and coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>These metrics are mostly specialized to detect factual consistency and do not capture overall relevance or coherence well; they can have spurious correlations and may miss stylistic or informativeness differences important to human raters.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Even BLANC, designed to measure overall summary quality, correlated best with faithfulness and much worse for relevance and coherence, indicating diagnostic gaps when used as a general judge.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Use reference-free metrics specifically for faithfulness evaluation while retaining human evaluation for coherence/relevance; combine multiple metrics that target complementary aspects; improve metric robustness to spurious correlations (noted as future work).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Large Language Models for News Summarization', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6227.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6227.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human paired evaluation (LLM vs freelance writers)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Blinded human pairwise preference evaluation comparing Instruct Davinci summaries to freelance writer summaries</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human evaluation in which annotators performed blinded pairwise comparisons (prefer A, prefer B, or tie) and judged informativeness (count of facts) to compare LLM-generated summaries to high-quality human-written summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking Large Language Models for News Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Single-document news summarization (comparison of Instruct Davinci outputs vs Upwork freelance-writer references)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>N/A (this entry describes human judgments acting as the evaluator of LLM outputs, not an LLM-as-judge).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Five annotators recruited via Upwork (after qualification), each annotated the same set of 100 summary pairs; annotators could pick one summary as better, or say they are equal; also an informativeness question comparing number of facts; average lengths controlled (~50 words). Inter-annotator agreement: informativeness Krippendorff's alpha = 0.32; overall preference alpha = 0.07 (low).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Pairwise preference percentages and informativeness preference; additional coverage and density extractiveness statistics (coverage: Instruct Davinci 0.92 vs writers 0.81; density: 12.1 vs 2.07).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>On aggregate, annotators equally preferred Instruct Davinci and freelance writers (no significant aggregate preference). Informative preference was 51.1% in favor of abstractive human summaries (not significant). There is substantial per-annotator variability: individual raters showed stable but divergent preferences (some preferring extractive LLM outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>LLM outputs (Instruct Davinci) are much more extractive (less paraphrasing) than human writers; stylistic mismatch can cause human raters to prefer different styles, leading to noisy / subjective judgments; low inter-annotator agreement limits statistical power to detect small quality differences.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Annotator behavior mismatch: annotator 1 (a freelance writer) preferred the more extractive Instruct Davinci summaries 57% of the time despite writing abstractive summaries themselves—illustrating subjectivity and a gap between writing and rating behavior. Also, LLMs sometimes generate factual errors (example: zero-shot GPT-3 claimed a handbag was 'most expensive in the world' contrary to article).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Collect higher-quality human references for metric computation; control confounds such as length; use more samples and less noisy measurement protocols; consider grounding evaluation in downstream tasks/applications to reduce subjective freedom; use diverse annotator pools and paired comparisons to reduce noise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Large Language Models for News Summarization', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Revisiting the gold standard: Grounding summarization evaluation with robust human evaluation <em>(Rating: 2)</em></li>
                <li>Summeval: Reevaluating summarization evaluation <em>(Rating: 2)</em></li>
                <li>On faithfulness and factuality in abstractive summarization <em>(Rating: 2)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 1)</em></li>
                <li>QAFactEval: Improved QA-based factual consistency evaluation for summarization <em>(Rating: 2)</em></li>
                <li>Summac: Re-visiting nli-based models for inconsistency detection in summarization <em>(Rating: 2)</em></li>
                <li>BARTScore: Evaluating generated text as text generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6227",
    "paper_id": "paper-a4a41319d5805a29316f24ed9519f09db77d4c29",
    "extraction_schema_id": "extraction-schema-122",
    "extracted_data": [
        {
            "name_short": "Reference-based automated metrics",
            "name_full": "Reference-based automated evaluation metrics (e.g., ROUGE-L, METEOR, BertScore, BLEURT, BARTScore)",
            "brief_description": "Common automatic metrics that score a generated summary by comparing it to a reference summary; in this paper they are used to quantify correlation with human judgments across faithfulness, coherence, and relevance for news summarization.",
            "citation_title": "Benchmarking Large Language Models for News Summarization",
            "mention_or_use": "use",
            "task_domain": "Single-document news summarization (CNN/DailyMail and XSUM)",
            "llm_judge_model": "ROUGE-L, METEOR, BertScore, BLEURT, BARTScore (these are the reference-based metrics used as 'automated judges')",
            "human_evaluation_setup": "30 US-based Mechanical Turk annotators (&gt;=98% approval, &gt;=10k HITs) rated 100 examples per dataset with 3 annotators per summary; criteria: binary faithfulness and 1–5 Likert coherence and relevance; reported pairwise agreement: faithfulness 75%, coherence 81%, relevance 86%.",
            "metrics_compared": "Kendall's tau rank correlation between each automated metric and human ratings on faithfulness, coherence, and relevance (system-level).",
            "reported_differences": "On CNN/DM, reference-based metrics achieve moderate correlations for some axes (e.g., ROUGE-L correlates with relevance at tau=0.72); on XSUM correlations are much weaker or even negative for some axes (e.g., ROUGE-L faithfulness tau = -0.27). Overall, reference-based metrics correlate better with human judgments on aspects and datasets where the references themselves score well (e.g., CNN/DM relevance) and poorly when references are low quality (e.g., XSUM).",
            "llm_specific_limitations": "These metrics depend critically on reference quality: poor references degrade metric usefulness and can invert signal (negative correlation). They also preferentially reward surface overlap and can favor finetuned systems that match reference style over LLMs that paraphrase, thus misranking systems relative to human preferences.",
            "notable_failure_cases": "On XSUM (where references are often just the first sentence and low quality), ROUGE-L and other reference-based metrics show very weak or negative correlations with human judgments; reference-based in-context conditioning using bad references made Instruct Davinci summaries worse (faithfulness dropped from 0.97 to 0.77 after 5-shot conditioning with XSUM references).",
            "mitigation_strategies": "Use higher-quality reference summaries (the paper collected freelance-writer references and showed metric correlations improve when using them), complement reference-based metrics with reference-free evaluations and human judgments, and avoid training/evaluating models on low-quality references; control confounds like length when comparing outputs.",
            "uuid": "e6227.0",
            "source_info": {
                "paper_title": "Benchmarking Large Language Models for News Summarization",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Reference-free / model-based metrics",
            "name_full": "Reference-free and model-based automated evaluation metrics (e.g., SummaC, QAFactEval, BLANC)",
            "brief_description": "Metrics that assess generated summaries without direct reliance on a single gold reference, often via NLI, QA-based factuality checks, or cloze-style masking using language models; used to evaluate faithfulness and correlate with human judgments.",
            "citation_title": "Benchmarking Large Language Models for News Summarization",
            "mention_or_use": "use",
            "task_domain": "Single-document news summarization (CNN/DailyMail and XSUM)",
            "llm_judge_model": "SummaC (NLI-based), QAFactEval (QA-based factual consistency), BLANC (blank-filling based), i.e., models and procedures acting as automated judges for faithfulness.",
            "human_evaluation_setup": "Same Mechanical Turk setup as above: 3 annotators per summary, binary faithfulness plus Likert coherence and relevance; system-level correlations computed to compare metric outputs to these human labels.",
            "metrics_compared": "Kendall's tau between reference-free metrics and human judgments, focusing particularly on faithfulness.",
            "reported_differences": "Reference-free metrics are less affected by low-quality references and tend to correlate best with faithfulness (e.g., QAFactEval tau=0.64 on CNN/DM faithfulness and 0.55 on XSUM faithfulness), but they correlate poorly with relevance and coherence.",
            "llm_specific_limitations": "These metrics are mostly specialized to detect factual consistency and do not capture overall relevance or coherence well; they can have spurious correlations and may miss stylistic or informativeness differences important to human raters.",
            "notable_failure_cases": "Even BLANC, designed to measure overall summary quality, correlated best with faithfulness and much worse for relevance and coherence, indicating diagnostic gaps when used as a general judge.",
            "mitigation_strategies": "Use reference-free metrics specifically for faithfulness evaluation while retaining human evaluation for coherence/relevance; combine multiple metrics that target complementary aspects; improve metric robustness to spurious correlations (noted as future work).",
            "uuid": "e6227.1",
            "source_info": {
                "paper_title": "Benchmarking Large Language Models for News Summarization",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Human paired evaluation (LLM vs freelance writers)",
            "name_full": "Blinded human pairwise preference evaluation comparing Instruct Davinci summaries to freelance writer summaries",
            "brief_description": "A human evaluation in which annotators performed blinded pairwise comparisons (prefer A, prefer B, or tie) and judged informativeness (count of facts) to compare LLM-generated summaries to high-quality human-written summaries.",
            "citation_title": "Benchmarking Large Language Models for News Summarization",
            "mention_or_use": "use",
            "task_domain": "Single-document news summarization (comparison of Instruct Davinci outputs vs Upwork freelance-writer references)",
            "llm_judge_model": "N/A (this entry describes human judgments acting as the evaluator of LLM outputs, not an LLM-as-judge).",
            "human_evaluation_setup": "Five annotators recruited via Upwork (after qualification), each annotated the same set of 100 summary pairs; annotators could pick one summary as better, or say they are equal; also an informativeness question comparing number of facts; average lengths controlled (~50 words). Inter-annotator agreement: informativeness Krippendorff's alpha = 0.32; overall preference alpha = 0.07 (low).",
            "metrics_compared": "Pairwise preference percentages and informativeness preference; additional coverage and density extractiveness statistics (coverage: Instruct Davinci 0.92 vs writers 0.81; density: 12.1 vs 2.07).",
            "reported_differences": "On aggregate, annotators equally preferred Instruct Davinci and freelance writers (no significant aggregate preference). Informative preference was 51.1% in favor of abstractive human summaries (not significant). There is substantial per-annotator variability: individual raters showed stable but divergent preferences (some preferring extractive LLM outputs).",
            "llm_specific_limitations": "LLM outputs (Instruct Davinci) are much more extractive (less paraphrasing) than human writers; stylistic mismatch can cause human raters to prefer different styles, leading to noisy / subjective judgments; low inter-annotator agreement limits statistical power to detect small quality differences.",
            "notable_failure_cases": "Annotator behavior mismatch: annotator 1 (a freelance writer) preferred the more extractive Instruct Davinci summaries 57% of the time despite writing abstractive summaries themselves—illustrating subjectivity and a gap between writing and rating behavior. Also, LLMs sometimes generate factual errors (example: zero-shot GPT-3 claimed a handbag was 'most expensive in the world' contrary to article).",
            "mitigation_strategies": "Collect higher-quality human references for metric computation; control confounds such as length; use more samples and less noisy measurement protocols; consider grounding evaluation in downstream tasks/applications to reduce subjective freedom; use diverse annotator pools and paired comparisons to reduce noise.",
            "uuid": "e6227.2",
            "source_info": {
                "paper_title": "Benchmarking Large Language Models for News Summarization",
                "publication_date_yy_mm": "2023-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Revisiting the gold standard: Grounding summarization evaluation with robust human evaluation",
            "rating": 2
        },
        {
            "paper_title": "Summeval: Reevaluating summarization evaluation",
            "rating": 2
        },
        {
            "paper_title": "On faithfulness and factuality in abstractive summarization",
            "rating": 2
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 1
        },
        {
            "paper_title": "QAFactEval: Improved QA-based factual consistency evaluation for summarization",
            "rating": 2
        },
        {
            "paper_title": "Summac: Re-visiting nli-based models for inconsistency detection in summarization",
            "rating": 2
        },
        {
            "paper_title": "BARTScore: Evaluating generated text as text generation",
            "rating": 1
        }
    ],
    "cost": 0.01347925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Benchmarking Large Language Models for News Summarization</h1>
<p>Tianyi Zhang ${ }^{1 <em>}$, Faisal Ladhak ${ }^{2 </em>}$, Esin Durmus ${ }^{1}$, Percy Liang ${ }^{1}$, Kathleen McKeown ${ }^{2}$, Tatsunori B. Hashimoto ${ }^{1}$<br>${ }^{1}$ Stanford University, USA ${ }^{2}$ Columbia University, USA</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have shown promise for automatic summarization but the reasons behind their successes are poorly understood. By conducting a human evaluation on ten LLMs across different pretraining methods, prompts, and model scales, we make two important observations. First, we find instruction tuning, not model size, is the key to the LLM's zero-shot summarization capability. Second, existing studies have been limited by low-quality references, leading to underestimates of human performance and lower few-shot and finetuning performance. To better evaluate LLMs, we perform human evaluation over high-quality summaries we collect from freelance writers. Despite major stylistic differences such as the amount of paraphrasing, we find that LLM summaries are judged to be on par with human written summaries.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have shown promising results in zero-/few-shot tasks across a wide range of domains (Chowdhery et al., 2022; Bai et al., 2022; Brown et al., 2020; Zhang et al., 2022) and have raised significant interest for their potential for automatic summarization (Goyal et al., 2022; Liu et al., 2022a). However, the design decisions contributing to its success in summarization remain poorly understood, and while prior work has shown that LLMs outperform the prior state of the art, it remains unclear whether their outputs are comparable to human writers. Examining these questions is crucial for advancing future research in automatic summarization.</p>
<p>To answer the first question, we perform a systematic evaluation of ten diverse LLMs with human evaluation on news summarization; our</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>evaluation identifies instruction tuning to be the key to zero-shot summarization capability. In contrast, self-supervised learning alone cannot induce strong summarization performance in the zero-shot setting (Figure 1). In fact, even a 350M parameter instruction-tuned GPT-3 can perform on par with the 175B parameter GPT-3.</p>
<p>To benchmark LLMs, we evaluated the standard CNN/DM (Hermann et al., 2015) and XSUM datasets (Narayan et al., 2018) but found that existing reference summaries caused several issues. The reference summaries in these benchmarks were originally created in a different use context and, when evaluated as part of a generic news summarization benchmark, human annotators judge them to be worse than the outputs of most automatic systems (Figure 1). When computing automatic metrics using these references, their poor quality reduces the correlation between metric results and human judgment. Not only does this make evaluation difficult, but it also degrades the performance of systems that take supervision either through finetuning or few-shot prompting and makes comparison difficult.</p>
<p>To address the quality issues of reference summaries and better understand how LLMs compare to human summary writers, we recruit freelance writers from Upwork ${ }^{1}$ to re-annotate 100 articles from the test set of CNN/DM and XSUM. Comparing the best performing LLM, Instruct Davinci, to the freelance writers, we find that the Instruct Davinci summaries are much more extractive. By manually annotating the summarization operations (Jing and McKeown, 2000) used in these summaries, we find that Instruct Davinci paraphrases much less frequently although it is able to combine copied segments coherently.</p>
<p>Given their stylistic differences, we recruit annotators to compare the Instruct Davinci summaries to those written by freelance writers. On aggregate, we find that Instruct Davinci is rated</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Selected annotator ratings of summary coherence on a 1 to 5 Likert scale.
as comparable to the freelance writers. Examination of the annotations from each individual rater shows that every rater has their own consistent preference for either Instruct Davinci or the freelance writers.</p>
<p>Together, our work makes the following key contributions. First, we identify instruction tuning, instead of model scale, as the key to LLMs' summarization capability. Second, we show that reference summaries used in XSUM, which are simply the first sentence of the news article, are judged by humans to be worse than the best LLMgenerated summaries. Third, to address these issues with references, we collect better quality summaries from freelance writers and we show that the best LLM is rated as comparable to Upwork freelance writers. In combination, these results call into question recent claims made about LLM summarization. In particular, summarization progress cannot be measured using reference-based metrics applied on XSUM. Furthermore, the question of whether fine-tuned, few-shot, or zero-shot models perform better remains an open question due to the poor quality of training data. To encourage future work on improved evaluations, we release the high-quality summaries written by freelance writers and the evaluation data on 18 model settings and two datasets as resources. ${ }^{2}$</p>
<h2>2 Background and Related Work</h2>
<h3>2.1 News Summarization</h3>
<p>News summarization is the task of producing a concise paragraph that captures the main points of a news article and has been a core problem within</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>the field of automatic summarization (Radev et al., 2002; Nenkova and McKeown, 2011). Early work focused mostly on extractive approaches, using unsupervised data-driven methods that relied on different variants of word frequency to determine salience (e.g., Salton et al., 1997; Hovy and Lin, 1999; Lin and Hovy, 2002; Mani and Bloedorn, 1999; Conroy et al., 2006; Nenkova et al., 2006). Other approaches to extractive summarization relied on aspects of discourse semantics (e.g., lexical chains and rhetorical structure theory) (Barzilay and Elhadad, 1997; Marcu, 1997; Silber and McCoy, 2002; Steinberger et al., 2007), or graphbased methods (e.g., Radev et al., 2000; Mihalcea and Tarau, 2005; Erkan and Radev, 2004). These extractive approaches were developed both for single-document and multi-document news summarization, with far more work focusing on multi-document than the single-document task. Humans, however, rely on more abstractive operations (such as paraphrasing, generalizations, etc.) in order to write fluent summaries (Jing and McKeown, 1999). This has led to a push toward building abstractive summarization systems, with initial research focusing on designing post-processing algorithms for extractive summarizers that focused on specific operations such as sentence fusion (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Krahmer et al., 2008; Filippova and Strube, 2008; Thadani and McKeown, 2013), generation (Barzilay et al., 1999) and sentence compression (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Cohn and Lapata, 2008). More scalable, data-driven approaches for building abstractive summarization systems were made possible with more effective neural systems for conditional generation (Sutskever et al., 2014; Bahdanau et al., 2015) as well as large-scale datasets (Rush et al., 2015; Hermann et al., 2015), leading to steady progress over the years (See et al., 2017; Chen and Bansal, 2018; Dong et al., 2019; Liu and Lapata, 2019; Lewis et al., 2019; Zhang et al., 2020).</p>
<p>This work benchmarks LLMs on news summarization using two popular benchmarks, CNN/DM (Hermann et al., 2015) and XSUM (Narayan et al., 2018). These datasets contain hundreds of thousands of article-summary pairs but were created using "incidental supervision", i.e., the reference summaries were not written specifically for the task but adapted from content on the websites. CNN/DM includes articles from the CNN</p>
<p>and DailyMail websites as the source articles and adapts the bullet point highlights that accompany the articles as reference summaries. XSUM includes articles from BBC News and adapts the bolded introductory sentence(s) as reference summaries. As a result, the reference summaries in these datasets are known to have quality issues (Maynez et al., 2020; Kang and Hashimoto, 2020), motivating us to address these defects to improve LLM evaluation.</p>
<p>To contextualize the performance of LLMs, we mainly compare to previous state-of-the-art approaches that leveraged supervised finetuning (Liu and Lapata, 2019; Lewis et al., 2019; Zhang et al., 2020; Liu et al., 2022b). Summarization evaluation is another active area of research. Many automatic metrics have been proposed (Lin, 2004; Zhang et al., 2020; Sellam et al., 2020; Durmus et al., 2020; Maynez et al., 2020; Deutsch and Roth, 2021) but they do not always correlate with human evaluation of summarization systems (Fabbri et al., 2020; Durmus et al., 2022). In this work, we evaluate the effectiveness of automatic metrics for evaluating LLMs and show that the usefulness of reference-based evaluation is closely linked to the quality of the references.</p>
<h3>2.2 Large Language Models</h3>
<p>LLMs (Bommasani et al., 2021; Chowdhery et al., 2022; Brown et al., 2020) have two distinctive features over previous pretrained models. First, LLMs have a much larger scale in terms of model parameters and training data. Second, unlike previous pretrained models that require finetuning, LLMs can be prompted zero-shot or few-shot to solve a task. In the zero-shot setting, prompting presents the LLMs with inputs (e.g., news articles) and a natural language instruction (e.g., "summarize this news article in three sentences") and solicits outputs by having LLMs generate answers directly. When few-shot training examples are available, LLMs have the ability to learn "in context". Incontext learning prepends training input-output pairs along with the same style of instruction to the testing input.</p>
<p>Recently, instruction-tuning has emerged as an effective way to improve LLM prompting performance (Sanh et al., 2021; Wang et al., 2022; Ouyang et al., 2022). In this approach, a diverse set of natural language processing tasks are reformulated into the prompting format and the LLM's
parameters are updated for these tasks either through supervised finetuning or reinforcement learning.</p>
<p>Recent work (Goyal and Durrett, 2020) shows that the instruct-tuned GPT-3 Davinci model is better than finetuned LMs, but does not show the design decision that contributes to the improved performance. In our work, we carry out a more comprehensive benchmark on ten different LLMs, to understand the effect of model scale, in-context learning, and instruction tuning. Given that automatic metrics may not be reliable, we focus on human evaluation as our benchmarking method.</p>
<h2>3 Human Evaluation on News Summarization Benchmarks</h2>
<p>In this section, we use human evaluation to systematically benchmark a diverse set of ten LLMs on news summarization. We observe that instruction tuning is the key to strong summarization capability and reference summaries in current benchmarks may underestimate few-shot or finetuning performance.</p>
<h3>3.1 Experimental Setup</h3>
<p>Data We conduct our human evaluation on CNN/DM and XSUM by sampling a hundred examples from each validation set, respectively. For the few-shot in-context learning settings, we sample five examples from the training set to be the demonstration examples. Due to the limited context window, we sample five articles that are between 50 and 150 tokens in length according to the GPT-2 tokenizer. For XSUM, we find that a uniform sampling occasionally results in articles that are unreadable due to data preprocessing so we manually pick from the training set.</p>
<p>Model Details We consider ten LLMs across different pretraining strategies and model scales. ${ }^{3}$ Table 1 lists the details of the LLMs we consider. Due to limited computational resources and model access, we benchmark all models in the five-shot setting but only benchmark three OpenAI GPT-3 models and three OpenAI instruction-tuned GPT-3 models in the zero-shot setting.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Model Creator</th>
<th style="text-align: center;"># Parameters</th>
<th style="text-align: center;">Instruction Tuning</th>
<th style="text-align: center;">Reference</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT-3 davinci v1</td>
<td style="text-align: center;">OpenAI</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">Brown et al. (2020)</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 curie v1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">6.7B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 ada v1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">350M</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT davinci v2</td>
<td style="text-align: center;">OpenAI</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Ouyang et al. (2022)</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT curie v1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">6.7B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT ada v1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">350M</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">OPT 175B</td>
<td style="text-align: center;">Meta</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">Zhang et al. (2022)</td>
</tr>
<tr>
<td style="text-align: center;">GLM</td>
<td style="text-align: center;">Tsinghua <br> University</td>
<td style="text-align: center;">130B</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">Du et al. (2021)</td>
</tr>
<tr>
<td style="text-align: center;">Cohere xlarge v20220609</td>
<td style="text-align: center;">Cohere</td>
<td style="text-align: center;">52.4B</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">Cohere (2022)</td>
</tr>
<tr>
<td style="text-align: center;">Anthropic-LM v4-s3</td>
<td style="text-align: center;">Anthropic</td>
<td style="text-align: center;">52B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Bai et al. (2022)</td>
</tr>
</tbody>
</table>
<p>Table 1: List of large language models we benchmarked with human evaluation.</p>
<p>For CNN/DM, we solicit LLM summaries with the following prompt template "Article: [article]. Summarize the article in three sentences. Summary:"
For XSUM, we modify the prompt template to summarize in one sentence to match the style of the reference summaries. For all LLMs we consider, we sample with temperature 0.3 following prior work (Wu et al., 2021).</p>
<p>To contextualize our LLM benchmarking results, we also evaluate two state-of-the-art finetuned LMs: Pegasus (Zhang et al., 2020) and BRIO (Liu et al., 2022b). We decode the finetuned LMs using a beam size of 5 following prior work (Lewis et al., 2019). In addition, we also evaluate the existing reference summaries in the CNN/DM and XSUM validation sets.</p>
<p>Human Evaluation Protocol We recruit annotators from Amazon Mechanical Turk, compensating them at California minimum wage of $\$ 15.00 / \mathrm{hr}$ using conservative time estimates as recommended by Whiting et al. (2019). We recruited a total of 30 annotators from the US who have a lifetime HIT approval rate of $98 \%$ or above with at least 10,000 approved HITs (Figure 8). ${ }^{4}$ Summaries are presented in random order and are evaluated independently by three annotators. We report average scores for each summary based on ratings from all three annotators.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Our annotators evaluate each summary based on three criteria: faithfulness, coherence, and relevance. We define these terms and collect data according to the guidelines in Fabbri et al. (2020). Coherence and relevance ratings are collected on a 1 to 5 Likert scale while faithfulness ratings are collected as a binary value, since it is inherently binary in nature. Unlike Fabbri et al. (2020), we do not evaluate fluency because we find LLM outputs to be mostly fluent. The average pairwise agreement for the annotators in our annotator pool was $75 \%$ for faithfulness, $81 \%$ for coherence, and $86 \%$ for relevance. ${ }^{5}$ The full annotation guidelines are included in our code release.</p>
<h3>3.2 Evaluation Results</h3>
<p>Table 2 presents the evaluation results. ${ }^{6}$ We now discuss two main observations.</p>
<p>Instruction Tuned Models Have Strong Summarization Ability. Across the two datasets and three aspects, we find that the zero-shot instruction-tuned GPT-3 models, especially Instruct Curie and Davinci, perform the best overall. Compared to the fine-tuned LMs (e.g., Pegasus), Instruct Davinci achieves higher coherence and relevance scores ( 4.15 vs. 3.93 and 4.60 vs. 4.40 )</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Setting</th>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">CNN/Daily Mail</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">XSUM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Faithfulness</td>
<td style="text-align: center;">Coherence</td>
<td style="text-align: center;">Relevance</td>
<td style="text-align: center;">Faithfulness</td>
<td style="text-align: center;">Coherence</td>
<td style="text-align: center;">Relevance</td>
</tr>
<tr>
<td style="text-align: center;">Zero-shot language models</td>
<td style="text-align: center;">GPT-3 (350M)</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">1.92</td>
<td style="text-align: center;">1.84</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">2.03</td>
<td style="text-align: center;">1.90</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3 (6.7B)</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">1.77</td>
<td style="text-align: center;">1.93</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">3.16</td>
<td style="text-align: center;">3.39</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3 (175B)</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">2.65</td>
<td style="text-align: center;">3.50</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">2.78</td>
<td style="text-align: center;">3.52</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ada Instruct v1 (350M*)</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">4.02</td>
<td style="text-align: center;">4.26</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">3.90</td>
<td style="text-align: center;">3.87</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Curie Instruct v1 (6.7B*)</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">4.24</td>
<td style="text-align: center;">4.59</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">4.27</td>
<td style="text-align: center;">4.34</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Davinci Instruct v2 (175B*)</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">4.15</td>
<td style="text-align: center;">4.60</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">4.41</td>
<td style="text-align: center;">4.28</td>
</tr>
<tr>
<td style="text-align: center;">Five-shot language models</td>
<td style="text-align: center;">Anthropic-LM (52B)</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">3.88</td>
<td style="text-align: center;">4.33</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">4.77</td>
<td style="text-align: center;">4.14</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cohere XL (52.4B)</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">3.42</td>
<td style="text-align: center;">4.48</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">4.79</td>
<td style="text-align: center;">4.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GLM (130B)</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">3.69</td>
<td style="text-align: center;">4.24</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">4.72</td>
<td style="text-align: center;">4.12</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">OPT (175B)</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">3.64</td>
<td style="text-align: center;">4.33</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">4.80</td>
<td style="text-align: center;">4.01</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3 (350M)</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">3.73</td>
<td style="text-align: center;">3.85</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3 (6.7B)</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">3.87</td>
<td style="text-align: center;">4.17</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">4.19</td>
<td style="text-align: center;">3.36</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3 (175B)</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">3.95</td>
<td style="text-align: center;">4.34</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">4.69</td>
<td style="text-align: center;">4.03</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ada Instruct v1 (350M*)</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">3.84</td>
<td style="text-align: center;">4.07</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">3.54</td>
<td style="text-align: center;">3.07</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Curie Instruct v1 (6.7B*)</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">4.30</td>
<td style="text-align: center;">4.43</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">4.28</td>
<td style="text-align: center;">3.80</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Davinci Instruct v2 (175B*)</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">4.13</td>
<td style="text-align: center;">4.49</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">4.83</td>
<td style="text-align: center;">4.33</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tuned language models</td>
<td style="text-align: center;">Brio</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">3.94</td>
<td style="text-align: center;">4.40</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">4.68</td>
<td style="text-align: center;">3.89</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pegasus</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">3.93</td>
<td style="text-align: center;">4.38</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">4.73</td>
<td style="text-align: center;">3.85</td>
</tr>
<tr>
<td style="text-align: center;">Existing references</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">3.20</td>
<td style="text-align: center;">3.94</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">4.13</td>
<td style="text-align: center;">3.00</td>
</tr>
</tbody>
</table>
<p>Table 2: Human evaluation results for zero-shot and five-shot LLMs, finetuned LMs, and reference summaries. We bold the entries that are not statistically significantly different from the best numbers in each column at $p=0.05$, using a bootstrap-based paired mean difference test.
on CNN and higher faithfulness and relevance scores ( 0.97 vs. 0.57 and 4.28 vs. 3.85 ) on XSUM, which is consistent with recent work (Goyal et al., 2022). In contrast to instruction tuning, we find scale to be less important. Even the largest 175B model often ignores the instruction and generates irrelevant content while the much smaller Instruct Ada outperforms the 175B GPT-3 model on coherence and relevance.</p>
<p>In the five-shot setting, non-instruction-tuned LLMs can improve their summarization performance through in-context learning. For faithfulness scores on CNN/DM and coherence scores on XSUM, several non-instruction-tuned LLMs can perform as well as the instruction-tuned LLMs. However, for other aspects, we still find the instruction-tuned LLMs to be better.</p>
<p>Reference Summaries in Current Benchmarks Should Not Be Used for Training and Evaluating Generic News Summarization Systems. We arrive at this conclusion based on two observations. First, most automatic summarization systems score better than the reference summaries across all three aspects. Second, applying incontext learning with the current reference summaries makes instruction-tuned models generate worse summaries. For example, on the XSUM dataset, after conditioning on five reference sum-
maries, the faithfulness score of Instruct Davinci drops from 0.97 to 0.77 .</p>
<p>The reference summaries make it difficult to compare LLMs to both finetuned models and humans. When comparing to finetuned models, the relatively poor performance of finetuned models can be attributed to the low quality of references in the training data. This suggests we could be underestimating the potential performance of finetuning approaches. When comparing to humans, the existing low-quality references are not representative of actual human performance since they were created through heuristics. As a result, the differences between instruction-tuned LLMs and human performance are likely overstated in Table 3.</p>
<p>Qualitative Examples. Figure 2 showcases example summaries on an article from the CNN/DM validation set, comparing the summaries of zeroshot GPT-3 Davinci, instruction-tuned GPT-3 Davinci, and the CNN/DM reference summary.</p>
<p>We start by noting that the zero-shot GPT-3 model cannot follow the instructions to summarize well. After the summary paragraph, the model generates an additional question that is completely irrelevant. In addition to the failure to follow instructions, the generated summary contains a factual error, stating that the handbag</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">CNN/DailyMail</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">XSUM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Metric</td>
<td style="text-align: center;">Faithfulness</td>
<td style="text-align: center;">Coherence</td>
<td style="text-align: center;">Relevance</td>
<td style="text-align: center;">Faithfulness</td>
<td style="text-align: center;">Coherence</td>
<td style="text-align: center;">Relevance</td>
</tr>
<tr>
<td style="text-align: left;">Rouge-L</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">-0.27</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.30</td>
</tr>
<tr>
<td style="text-align: left;">METEOR</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">-0.22</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.38</td>
</tr>
<tr>
<td style="text-align: left;">BertScore</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">-0.23</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.30</td>
</tr>
<tr>
<td style="text-align: left;">BARTScore</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">-0.22</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.35</td>
</tr>
<tr>
<td style="text-align: left;">BLEURT</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">-0.08</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.41</td>
</tr>
<tr>
<td style="text-align: left;">SummaC</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">-0.41</td>
<td style="text-align: center;">-0.29</td>
</tr>
<tr>
<td style="text-align: left;">QAFactEval</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.37</td>
</tr>
<tr>
<td style="text-align: left;">BLANC</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.32</td>
</tr>
</tbody>
</table>
<p>Table 3: System-level kendall's tau correlation with human scores across different axes.</p>
<h1>Article: https://www.dailymail.co.uk/femail/article-3052124</h1>
<h2>Reference summary:</h2>
<p>Rare tote is so exclusive that it can't even be brought in store. Experts say it's an extremely good investment as only increases in value. Kim Kardashian and Victoria Beckham are both fans of Hermes.</p>
<h2>0-shot GPT-3 Davinci summary:</h2>
<p>The most expensive handbag in the world is a pink Hermes Birkin bag, which has been put on sale for $£ 140,000$. The bag is so exclusive that it can't even be brought in store and experts say it's an extremely good investment. With the same amount of money that the handbag costs, you could splash out on an Aston Martin DB9 Volante.</p>
<p>What is the main idea of the article?</p>
<h2>0-shot Instruct Davinci summary:</h2>
<p>The article discusses a pink Hermes Birkin bag that is on sale for $£ 140,000$. The bag is made-tomeasure and embellished with rubies. The article also discusses how the bag is a collector's item and an investment.</p>
<h2>Freelance writer summary:</h2>
<p>Hermes is a brand rich in history and value, so much so that experts are saying their new $£ 140,000$ handbag is an extremely good investment. The bag, studded in rubies, is only the latest in Hermes Birkin's long line of highly expensive handbags for highly stylish women, which include celebrities and royalty.</p>
<p>Figure 2: Examples summaries generated by GPT-3 models (Section 3) or written by freelance writers (Section 4) of an article from the CNN/DM dataset. We find that the instruction-tuned GPT-3 model can generate a much better summary compared to the non-instruction-tuned variant. The reference summary from CNN/DM is not coherent whereas the freelance writer summary is both coherent and relevant.
mentioned is the most expensive in the world, which contradicts the original article. In contrast, the instruction-tuned GPT-3 model generates a summary that is both faithful and coherent.</p>
<p>We also observe from Figure 2 that the reference summary is not coherent. The brand "Hermes" is not introduced until the end and its connection to
the rest of the story is unclear. This is unsurprising as reference summaries in the CNN/DM dataset were originally bullet points accompanying the articles as opposed to a coherent paragraph. While such reference summaries might be suited in the original context, we argue that they are not useful for evaluating generic news summarization.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: System-level Rouge-L vs. annotator rated relevance scores.</p>
<h3>3.3 Understanding Automatic Metrics</h3>
<p>We compute system-level correlations against human ratings for eight popular automated evaluation metrics. For reference-based metrics we consider: Rouge-L (Lin, 2004), METEOR (Banerjee and Lavie, 2005), BertScore (Zhang et al., 2020), BLEURT (Sellam et al., 2020), and BARTScore (Yuan et al., 2021). For reference-free metrics we consider: SummaC (Laban et al., 2021), QAFactEval (Fabbri et al., 2022), and BLANC (Vasilyev et al., 2020).</p>
<p>Table 3 shows Kendall's tau rank correlations between automated metrics and human judgments. We observe significantly different trends on CNN/ DM and XSUM so we discuss them separately in the following paragraphs.</p>
<p>For CNN/DM, we observe that the referencebased automatic metrics have a moderate correlation with some aspects of human judgments, e.g., Rouge-L has a 0.72 Kendall's tau correlation coefficient with relevance in Table 3. Such a level of correlation is comparable to that reported in the study of Fabbri et al. (2020), which measures the correlation of automatic metrics on evaluating finetuned LMs and even earlier neural summarization systems. Therefore, we conclude that on CNN/DM automatic, reference-based metrics can still provide useful signals for relevance.</p>
<p>Studying the results more closely, we find that Rouge-L and human evaluation are more correlated when comparing within each model group. We plot Rouge-L over the relevance rating in Figure 3 as an example. First, we observe that Rouge-L still prefers finetuned LMs (green points on top of the plots) to LLMs, consistent with prior work (Goyal et al., 2022). Despite this mistake, when only comparing LLMs with each other, we find that a larger than 0.05 Rouge-L difference usually translates to improved human evaluation.</p>
<p>On XSUM, reference-based metrics have a very low correlation with faithfulness and relevance since the reference summaries themselves are terrible in these aspects (Table 3; also see Maynez et al., 2020). With such low-quality references, we do not expect reference-based metrics to extract useful information.</p>
<p>In general, across both datasets, we find that reference-based metrics correlate better with human judgments on the aspects for which reference summaries also have better scores (e.g., CNN/DM relevance, XSUM coherence). This points to the important role of quality reference summaries for reference-based metrics, as previously observed in machine translation (Freitag et al., 2020). Reference-free metrics are less handicapped by the low-quality references but they are mostly geared towards measuring faithfulness. Even BLANC, which is designed to measure overall summary quality, correlates best with faithfulness and much worse for relevance and coherence.</p>
<h2>4 Comparing to Freelance Writers</h2>
<p>In Section 3, we see that the low-quality reference summaries make studying and benchmarking LLMs difficult. In this section, we address this by recruiting Upwork freelance writers to collect higher-quality summaries. With this data, we aim to answer two important questions. First, we would like to know whether the best LLM has reached human-level performance and how the summaries written by the best LLM differ from the ones written by humans. Second, we aim to examine the correlation between reference-based metrics and human judgments when the metrics are calculated using our higher-quality reference summaries.</p>
<h3>4.1 Experimental Setup</h3>
<p>In this section, we describe the recruitment process and instructions for the summary writing task.</p>
<p>Data. For data used in our study, we select 50 articles from each of the CNN/DM and XSUM evaluation sets described in Section 3.1 and assign each article to three writers. For XSUM, we use the full articles rather than the preprocessed version where the first bolded sentence is removed.</p>
<p>Writer Recruitment. We recruit six writers who have had previous experience in writing blog posts, landing page introductions, or product descriptions from the freelance work platform Upwork. After conducting a qualification round by asking writers to summarize five articles, we selected the best writers according to the faithfulness, coherence, and relevance of their summaries. Through an initial pilot study, we estimate that the time required to summarize a CNN/DM or XSUM article is around 12 to 15 minutes. Therefore, we pay our writers $\$ 4$ for every article they summarize following the recommended practice (Whiting et al., 2019). We based the assignments on writers' availability, with the most prolific writer summarizing 100 articles and the least prolific writer summarizing 35 articles. We include our annotation guideline for freelance writers in Figure 7.</p>
<p>Summary Writing Instructions. For the annotation instruction, we instruct our writers to summarize each article in around 50 words. ${ }^{7}$ To give better task grounding, we ask the writers to summarize as if they are writing a newsletter to update their readers on the news. We release the full annotation guideline along with our code release.</p>
<p>LLM Summaries Generation. Recently, Liu et al. (2022a) showed that length is a confounding factor in the human evaluation of summarization. To control this potential length confound, we modify the zero-shot prompt in Section 3.1 to elicit summaries that are around 50 words, which is the same word limit provided to the freelance writers. We found that the Instruct Davinci model consistently produces summaries that exceed a given word limit. Therefore, we intentionally prompt the Instruct Davinci model with a 25 -word limit to produce summaries with an average length of 50 words. With this new prompt, we generate the summaries using the same hyperparameters described in Section 3.1.</p>
<p>Quality Control. To verify the quality of the summaries written by freelance writers, we evaluate a random subset of 100 summaries using the</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 4: Amazon Mechanical Turker evaluation results of the freelance writer summaries. Results of zero-shot Instruct Davinci and reference summaries are taken from Table 2 after averaging the corresponding ratings.
same annotation scheme in Section 3.1 using Mechanical Turkers. Table 4 reports the evaluation results, where we see that the freelance writer summaries have much higher quality than the original reference summaries in CNN/DM and XSUM. In addition, we see that the difference between the freelance writer and Instruct Davinci in this evaluation is small. Next, we carry out more targeted evaluations to compare the summaries written by freelance writers and Instruct Davinci.</p>
<h3>4.2 Paired Comparison between LLM and Freelance Writers</h3>
<p>Comparing Stylistic Differences. Despite the similar performance in our quality control study, we find that LLM summaries and freelance writer summaries have distinctive styles. Figure 2 shows an example summary written by the freelance writer. Compared to the LLM-generated summary, we find the freelance writer summary often contains more paraphrasing and copies less from the article.</p>
<p>To illustrate this stylistic difference, we measure two extractiveness measures, coverage and density, following Grusky et al. (2018). Coverage is defined as the percentage of words in the summary that are also present in the article; density is defined as the average length of the continuous text spans in the summary that are copied from the article. Our analysis shows that the coverage and density for Instruct Davinci generated summaries are 0.92 and 12.1 whereas those for the writers' summaries are 0.81 and 2.07. These measures show that the summaries generated by Instruct Davinci are highly extractive whereas the summaries written by the freelance writers are much more abstractive.</p>
<p>To have a fine-grained understanding of these stylistic differences, we manually analyze the distribution of "cut-and-paste operations" in these</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Distributions of cut and paste operations in the summaries written by freelance writers and by Instruct Davinci. By comparison, human-written summaries contain more lexical paraphrasing and sentence reduction whereas the Instruct Davinci model has more direct copying from the article.
two sets of summaries. Jing and McKeown (2000) identify a set of "cut and paste" operations for reusing text from the article, including sentence reduction, sentence combination, syntactic transformation, lexical paraphrasing, and generalization or specification. On top of these operations, we additionally include a sentence copy operation to account for summary sentences that are directly copied from the article. Using this guideline, we manually annotate ten randomly sampled summary pairs written by Instruct Davinci and the freelance writers.</p>
<p>Figure 4 reports the distribution of the cut-andpaste operations, showing the fraction of sentences that contain each operation. First, we observe that the freelance writer summaries use lexical paraphrasing and generalization/specification much more frequently than the Instruct Davinci generated summaries. Because both operations often involve using novel words that are not present in the article, this matches with the fact that the freelance writer summaries have lower coverage ( 0.81 vs. 0.92 ) than the Instruct Davinci summaries. Second, we find that sentence combination is a common strategy used by both the freelance writers and Instruct Davinci. Third, we find that the freelance writers never copy an entire sentence directly from the article but Instruct Davinci does this more frequently.</p>
<p>In conclusion, we find that Instruct Davinci summarizes in a very different style than human writers. We emphasize here that the freelance writers write in an abstractive style despite the fact that we have not explicitly instructed them to do so. We also observe similarly abstractive styles across the six freelance writers.</p>
<p>Comparing Human Preference. We now return to our original goal of understanding whether LLM-generated summaries have quality on par with the human-written ones. In the following paragraphs, we discuss our annotation design and recruitment process.</p>
<p>We conduct a blinded pairwise comparison evaluation between the best LLM Instruct Davinci and the freelance writers, similar to the evaluation in Goyal and Durrett (2020). Besides selecting the better summary within each pair, the annotators can decide the summary pair to be equally good. We release the full annotation instructions along with the code release for this project.</p>
<p>In order to compare the best LLM with the freelance writers, we annotate two aspects. First, we solicit annotators' overall preference, which balances the multiple quality aspects such as faithfulness, coherence, and relevance. Second, we solicit a more targeted measure of informativeness by asking the annotators to compare the number of facts in each summary. For the informativeness measure, we are motivated by the hypothesis that a more abstractive writing style can pack more information into the summary given the same word count. While it is also interesting to compare summary coherence and relevance, we omit them because annotators were unable to differentiate these aspects from the overall preference in a pilot study.</p>
<p>For our recruitment process, we recruit five additional annotators through Upwork and retain one writer who participated in the previous round of summary writing. ${ }^{8}$ We carry out a qualification round and reject annotators whose ratings differ significantly from the authors' on a set of control questions for informativeness. We give each annotator the same set of 100 summary pairs, where the average length of the freelance writer summaries and the Instruct Davinci summaries are 53.2 and 52.0 , respectively.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Human evaluation results comparing summaries written by freelance writers and summaries generated by Instruct GPT-3 Davinci. On aggregate, annotators equally prefer freelance writers and Instruct Davinci. However, there is high variability in individual annotators' preferences. Notably, annotator 1 writes abstractive summaries but prefers the more extractive Instruct Davinci summaries.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: System-level Rouge-L vs. annotating rating of faithfulness. The left plot is computed with XSUM references, where the correlation is weak, and the right plot is computed with the freelance writer summaries, where the correlation is much improved.</p>
<p>Figure 5 shows the results of the paired comparison. While we hypothesized that the more abstractive writing style could lead to more informative summaries, we did not find a significant effect in our annotator pool, who rate the more abstractive summaries to be more informative only 51.1% of the time. On the informative question, our annotators reached a moderate agreement (Krippendorff's alpha is 0.32), validating our annotation instruction and recruitment process. Moving onto the more subjective overall preference, we find that our annotators equally prefer the freelance writer summaries and the Instruct Davinci summaries. However, a closer analysis shows that there is significant variability in individual annotators' preferences and the inter-annotator agreement is low (Krippendorff's alpha is 0.07). This suggests that the quality of generated summaries is getting close to that of the freelance writer summaries and the comparison is dependent on each annotator's stylistic preference.</p>
<p>One example of such stylistic preference is seen in the results from annotator 1, who also participated in the first round of summary writing. Like other writers, annotator 1 summarizes in an abstractive style (2.5 density and 0.86 coverage). However, annotator 1 prefers Instruct Davinci 57% of the time even though it generated much more extractive summaries. These results suggest an intriguing gap between annotator preferences when writing and evaluating summaries.</p>
<h3>4.3 Reevaluating Reference-based Metrics</h3>
<p>In Section 3.3, we saw that the performance of automated metrics may depend on the quality of reference summaries. With the freelance writer summaries, we now conduct an initial study on the effect of using better quality summaries. We focus on using Rouge-L for faithfulness evaluation on the XSUM dataset because the current reference summaries are known to be highly unfaithful (Maynez et al., 2020).</p>
<p>In Figure 6, we plot the system-level Rouge-L against the human ratings. The left plot shows the results of computing Rouge-L with existing reference summaries from XSUM, which has a negative correlation with human ratings. This result matches our expectations because the existing reference summaries are highly unfaithful.</p>
<h1>Task Description</h1>
<p>Imagine you are a writer/editor of a blog where you summarize important news events for your readers daily. Your goal is to provide your readers with an accurate and informative summary of a given news article in a concise manner. You want your readers to understand the article's main points without having to read it.</p>
<p>In this task, we ask you to write a summary for a given news article with the above goal in mind. The interface will ask you to first select the most important points from the news article and write a short, coherent summary incorporating these points. Overall, it should take roughly 15 minutes to complete the task.</p>
<p>We will compare your summary to summaries written by AI systems and hopefully show how current AI systems fail on the summarization task. If you encounter any problems or have other feedback, please contact [redacted].</p>
<p>By participating in this study, you confirm that you are (1) 18 years or older, (2) currently reside in the US, and (3) have read and understand the information above and agree to participate. By clicking submit you consent to participate in this study. If you do not wish to participate, do not click submit.</p>
<h2>Reading the article</h2>
<p>Please read the article carefully and try to understand the main idea of the article. Once you are done reading, we ask you to highlight the most important parts of the article. These highlights should represent the information that you think is the most important information that you would include in your summary. You can do this by using your cursor (mouse) to select the relevant portions of the article text. A selected phrase/sentence will be highlighted in red. If you highlight something by mistake, you can always correct it by simply clicking on the erroneous highlight again.</p>
<p>If you find that an article is incomplete or has obvious formatting mistakes, you can contact us and ask for a different article. If you have trouble understanding an article, you can contact us to assign you a different article.</p>
<h2>Writing a summary</h2>
<p>There are several aspects we look for in a good summary:</p>
<ul>
<li>Consistency: A good summary should be consistent with the article. Please do not add any new information to the summary, even if you know more about the article's content. For example, if the article does mention the first name of "President Biden," please do not use the full name "Joe Biden" in your summary.</li>
<li>Relevance: A good summary should only include important information from the article. In general, you should include the key points you highlighted in your summary, but feel free to omit details you think are unnecessary to convey the main idea of the article.</li>
<li>Conciseness: A good summary should be short and to the point. We generally expect a summary to be 45 to 55 words long, but it is okay if the summary is shorter when the article is short.</li>
<li>Coherence: A good summary is more than just a list of sentences. It should be a coherent paragraph that makes sense.</li>
</ul>
<p>Before you submit, please remember to proofread and improve any typos or grammar errors. We recommend installing grammarly or other automatic spell checker to your browser. If there are too many typos in your submitted summaries, we will ask you to correct them before we can pay you.</p>
<p>Thank you so much for participating in our study!!
Figure 7: Annotation guideline for freelance writers.
the right, we see the results of computing Rouge-L with the freelance writer summaries, which leads to a much more positive correlation. Hence, we see that the usefulness of reference-based evaluation is closely linked to the quality of the references and we can improve metric correlation by using better reference summaries.</p>
<h2>5 Discussion</h2>
<p>Implication for Model Development. In this study, we systematically evaluate a diverse set of LLMs and find that instruction tuning contributes the most to LLMs' summarization capability. We believe that there is much research beyond our benchmarking effort that needs to be done to better understand the effect of instruction tuning. Here we hypothesize three aspects that could account for the success of instruction tuning.</p>
<p>First, the quality of the summarization data used in instruction tuning can serve an important role. Our findings in Section 3 show that currently, we are finetuning language models on lowquality training data, which can account for their
ineffectiveness. At this point, we cannot rule out the possibility that when finetuned on higher quality data, finetuned LMs may perform much better.</p>
<p>Second, the learning algorithm used for instruction tuning can be important (Ouyang et al., 2022). While the exact training details are unknown, the success of Instruct Davinci might be credited to "learning from human feedback" (LHF; Stiennon et al., 2020; Ziegler et al., 2019). Contrary to supervised finetuning that trains systems on written summaries, learning from human feedback trains systems from binary labels of human preferences. As we observe in Section 4.2, there is a discrepancy in how annotators write and rate summaries. While it is possible that LHF has merits over the supervised learning/finetuning approach in exploiting this discrepancy, more analysis is needed to validate this hypothesis.</p>
<p>Third, multi-task learning can be important. Instruct Davinci is trained on a diverse distribution of inputs and many previous studies have confirmed the effectiveness of multi-task learning. We look forward to understanding how summarization benefits from learning on other tasks.</p>
<h1>Evaluating Machine Generated Summaries</h1>
<h2>Instruction:</h2>
<p>In this task, we need your help in evaluating machine generated summaries. In the below table, we first show you a news article (top) and a summary generated by an AI system (below)
In this task, you will help us to evaluate the consistency, relevance, and coherence of the summary.</p>
<ul>
<li>
<p>First, we ask you to give a binary decision about whether the summary is consistent with the new article. We define a summary to be consistent if the all the information expressed by the summary can be inferred from the new article. We now give you two examples where the first example is inconsistent and the second example is consistent. For illustration purpose, we omit parts of the article.</p>
</li>
<li>
<p>Here is an example where the summary is inconsistent because the summary mistakenly summarizes that the person died on March 5, 1998 when they were actually born on March 5, 1998:</p>
</li>
</ul>
<p>Article: The world's oldest person has died a few weeks after celebrating her 117th birthday. Born on March 5, 1998, the great-grandmother had lived through two world wars, the invention of the television and the first successful powered aeroplane flight by the wright brothers (...)</p>
<p>Summary: The world 's oldest person has died on March 5, 1998.
2. Here is an example where the output is consistent because all information can be inferred from the news article:</p>
<p>Article: Andy Murray (...) is into the semi-finals of the Miami Open, but not before getting a scare from 21 year-old Austrian Dominic Thiem, who pushed him to 4-4 in the second set before going down $3-9$ 6-4, 6-1 in an hour and three quarters. (...)
Summary: Andy Murray defeated Dominic Thiem 3-6 6-4, 6-1 in an hour and three quarters.
The summary should only contain information from the original article. If the information presented in the summary is true based on general knowledge (e.g. the earth is round) but is not supported by the original article, you should mark it as unfaithful.
It is okay for the output to have minor grammatical errors. If you can understand what the output expresses despite the minor grammatical errors and if the information is supported by the article, select consistent.
If the output is nonsensical, select inconsistent.</p>
<ul>
<li>
<p>We ask you to judge whether the summary is relevant to the news article on a 1 to 5 scale. The summary should include only important information from the source document. If the summary contains redundancies and excess information, you should consider it as less relevant.</p>
</li>
<li>
<p>Here is an example where the summary is not very relevant because the information summarized is not the main point of the article.</p>
</li>
</ul>
<p>Article: The world's oldest person has died a few weeks after celebrating her 117th birthday. Born on March 5, 1998, the great-grandmother had lived through two world wars, the invention of the television and the first successful powered aeroplane flight by the wright brothers (...)
Summary: The first successful powered aeroplane flight was by the wright brothers.</p>
<ul>
<li>
<p>Finally, we ask you to judge whether the summary is coherent on a 1 to 5 scale. A good summary should organize the relevant information into a well-structured summary. The summary should not just be a heap of related information, but should build from sentences into a coherent body of information about a topic.</p>
</li>
<li>
<p>Here is an example where the summary is not very coherent because the two sentences don't flow well together.</p>
</li>
</ul>
<p>Article: The world's oldest person has died a few weeks after celebrating her 117th birthday. Born on March 5, 1998, the great-grandmother had lived through two world wars, the invention of the television and the first successful powered aeroplane flight by the wright brothers (...)
Summary: The world's oldest person has died at the age of 117. The first successful powered aeroplane flight was by the wright brothers.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">News Article</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Article text.</td>
</tr>
<tr>
<td style="text-align: left;">Machine Generated Summary: Summary text.</td>
</tr>
<tr>
<td style="text-align: left;">Is the summary consistent with the article?</td>
</tr>
<tr>
<td style="text-align: left;">$\square$ Consistent $\square$ Inconsistent</td>
</tr>
<tr>
<td style="text-align: left;">Is the summary relevant to the article? Choose 5 if the summary is very relevant and 1 if the summary is not relevant at all.</td>
</tr>
<tr>
<td style="text-align: left;">$\square 1 \square 2 \square 3 \square 4 \square 5$</td>
</tr>
<tr>
<td style="text-align: left;">Is the summary coherent? Choose 5 if the summary is very coherent and 1 if the summary is not coherent at all.</td>
</tr>
<tr>
<td style="text-align: left;">$\square 1 \square 2 \square 3 \square 4 \square 5$</td>
</tr>
</tbody>
</table>
<p>Figure 8: MTurk annotation guideline for summary quality evaluation.</p>
<p>Implication for Summarization Evaluation. Our work also reveals the difficulties in evaluating high-performance LLMs. As LLMs become increasingly close to human-level performance, human evaluation requires a larger number of samples and less noisy measurements to evaluate the quality of LLMs. Recently, Liu et al. (2022a) also pointed out the difficulties in conducting human evaluation for summarization and advocated using fine-grained semantic units to match with reference summaries. However, as our evaluation points out, not only are the existing reference
summaries unreliable but the summaries written by well-paid freelance writers also may not outperform LLM summaries significantly. Therefore, defining reference summaries as the ground truth may be overly restrictive as LLMs are approaching or even exceeding average human-level performance.</p>
<p>We acknowledge that summarization evaluation is dependent on the application scenarios and the existing reference summaries could be suitable in another context. For example, the bullet points style summary in CNN/DM may suffice</p>
<p>for being displayed on news websites. The quality issues (such as coherence) we pointed out in this paper may not constitute a concern in specific application scenarios. However, we emphasize that the research on single document news summarization is often abstracted away from the downstream applications and used for judging the generic summarization capability. Our findings in this paper are tied to this research context. This is the reason why the major results of our study rely on new summaries written by freelance writers.</p>
<p>Not only is human evaluation limited by the reference quality, but it also is affected by the subjectivity in evaluation. Individual variation shows that there are many acceptable ways to summarize and individuals may even show different preferences at different points in time (writing vs rating). These factors in combination lead to the fact that we may have reached the limit of single-document news summarization. Existing benchmarks can still play a role in evaluating new models but only if evaluation is done correctly. As LLMs improve, we believe that summarization can be better grounded in downstream applications where user values are better defined so that annotators have a lower degree of freedom in balancing which quality aspects matter most to them.</p>
<p>Limitations. Due to time constraints, this study has only evaluated systems on English news summarization where the summaries are designed to have around 50 words. We also acknowledge that as automatic systems improve, it becomes increasingly difficult for annotators to unambiguously rank summaries by quality due to differences in their individual preferences.</p>
<h2>6 Conclusion</h2>
<p>In this work, we conducted a comprehensive human evaluation of ten LLMs, across the two most popular news summarization benchmarks. Through our experiments, we find that the state-of-the-art LLM performs on par with summaries written by freelance writers, with instruction tuning being the key factor for success. Beyond these findings, our work highlights the crucial role of good reference summaries in both summarization model development and evaluation. Unless the reference quality issue is addressed, comparing zero-shot, few-shot, and finetuning performance
will remain an open question, and the current benchmarks will provide limited value when used with reference-based evaluation. Even when we address the quality issue and conduct a human evaluation with high-quality references, we observe a significant amount of individual variation from our annotator pool. Due to these factors, evaluations for single document news summarization may be reaching their limits.</p>
<h2>Acknowledgments</h2>
<p>This work is supported by an Open Philanthropy grant and partially supported by a gift from Northrup Grumman. We thank the reviewers and editors for their comments, as well as the Stanford NLP group and the Stanford Center for Research on Foundation Models community for their feedback.</p>
<h2>References</h2>
<p>Dzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015.</p>
<p>Yushi Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, T. J. Henighan, Nicholas Joseph, Saurav Kadavath, John Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Christopher Olah, Benjamin Mann, and Jared Kaplan. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862.</p>
<p>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In IEEvaluation@ACL.</p>
<p>Regina Barzilay and Michael Elhadad. 1997. Using lexical chains for text summarization. In Proceedings of ISTS, ACL 1997.</p>
<p>Regina Barzilay and Kathleen R. McKeown. 2005. Sentence fusion for multidocument news summarization. Computational Linguistics, 31(3):297-328. https://doi.org/10.1162 /089120105774321091</p>
<p>Regina Barzilay, Kathleen R. McKeown, and Michael Elhadad. 1999. Information fusion in the context of multi-document summarization. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 550-557, College Park, Maryland, USA. Association for Computational Linguistics. https://doi.org/10 .3115/1034678.1034760</p>
<p>Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen A. Creel, Jared Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, O. Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Benjamin Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, J. F. Nyarko, Giray Ogut, Laurel J. Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Robert Reich, Hongyu Ren, Frieda Rong, Yusuf H. Roohani, Camilo Ruiz, Jack Ryan, Christopher Re, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishna Parasuram Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramer, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang</p>
<p>Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei A. Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In NeurIPS.</p>
<p>Yen-Chun Chen and Mohit Bansal. 2018. Fast abstractive summarization with reinforceselected sentence rewriting. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 675-686, Melbourne, Australia. Association for Computational Linguistics. https://doi.org/10.18653/v1/P18 -1063</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek B. Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark</p>
<p>Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Cohere. 2022. Introduction to large language models. https://docs.cohere.ai/docs /introduction-to-large-language -models</p>
<p>Trevor Cohn and Mirella Lapata. 2008. Sentence compression beyond word deletion. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 137-144, Manchester, UK. Coling 2008 Organizing Committee. https://doi.org /10.3115/1599081.1599099
J. Conroy, J. Schlessinger, D. O’Leary, and J. Goldstein. 2006. Back to basics: Classy 2006. In Proceedings of the Document Understanding Conference. https://doi.org/10.12968 /sece.2006.11.755</p>
<p>Daniel Deutsch and Dan Roth. 2021. Understanding the extent to which content quality metrics measure the information quality of summaries. In Proceedings of the 25th Conference on Computational Natural Language Learning, pages 300-309, Online. Association for Computational Linguistics. https://doi.org /10.18653/v1/2021.conll-1.24</p>
<p>Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-training for natural language understanding and generation. https://doi .org/10.48550/arXiv.1905.03197</p>
<p>Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2021. Glm: General language model pretraining with autoregressive blank infilling. In $A C L$.</p>
<p>Esin Durmus, He He, and Mona Diab. 2020. FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5055-5070, Online. Association for Computational Linguistics. https://doi.org/10.18653/v1/2020 .acl-main. 454</p>
<p>Esin Durmus, Faisal Ladhak, and Tatsunori Hashimoto. 2022. Spurious correlations in reference-free evaluation of text generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). https://doi .org/10.18653/v1/2022.acl-long. 102</p>
<p>Güneş Erkan and Dragomir R. Radev. 2004. Lexrank: Graph-based centrality as salience in text summarization. Journal of Artificial Intelligence Research. https://doi.org/10 .1613/jair. 1523</p>
<p>Alexander Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2022. QAFactEval: Improved QA-based factual consistency evaluation for summarization. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2587-2601, Seattle, United States. Association for Computational Linguistics. https://doi.org/10.18653/v1/2022 .naacl-main. 187</p>
<p>Alexander R. Fabbri, Wojciech Kryscinski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2020. Summeval: Reevaluating summarization evaluation. arXiv preprint arXiv:2007.12626. https://doi .org/10.1162/tacl_a_00373</p>
<p>Katja Filippova and Michael Strube. 2008. Sentence fusion via dependency graph compression. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 177-185. https://doi .org/10.3115/1613715.1613741</p>
<p>Markus Freitag, David Grangier, and Isaac Caswell. 2020. BLEU might be guilty but references are not innocent. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). https://doi.org/10.18653/v1/2020 .emnlp-main. 5</p>
<p>Tanya Goyal and Greg Durrett. 2020. Evaluating factuality in generation with dependencylevel entailment. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3592-3603, Online. Association for Computational Linguistics. https://doi.org /10.18653/v1/2020.findings-emnlp. 322</p>
<p>Tanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022. News summarization and evaluation in the era of gpt-3. ArXiv, abs/2209.12356.
Max Grusky, Mor Naaman, and Yoav Artzi. 2018. Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. In North American Chapter of the Association for Computational Linguistics. https://doi .org/10.18653/v1/N18-1065
Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In NeurIPS.</p>
<p>Eduard Hovy and Chin-Yew Lin. 1999. Automated text summarization in summarist. In Advances in Automatic Text Summarization, pages 82-94.
Hongyan Jing. 2000. Sentence reduction for automatic text summarization. In Applied Natural Language Processing Conference. https:// doi.org/10.3115/974147.974190
Hongyan Jing and Kathleen McKeown. 2000. Cut and paste based text summarization. In Applied Natural Language Processing Conference.
Hongyan Jing and Kathleen R. McKeown. 1999. The decomposition of human-written summary sentences. In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 129-136. https://doi.org /10.1145/312624.312666
Daniel Kang and Tatsunori B. Hashimoto. 2020. Improved natural language generation via loss truncation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 718-731, Online. Association for Computational Linguistics. https://doi.org/10.18653/v1/2020 .acl-main. 66
Kevin Knight and Daniel Marcu. 2002. Summarization beyond sentence extraction: A probabilistic approach to sentence compression. Artificial Intelligence, 139(1):91-107. https://doi.org/10.1016/S0004-3702 (02) $00222-9$</p>
<p>Emiel Krahmer, Erwin Marsi, and Paul van Pelt. 2008. Query-based sentence fusion is better
defined and leads to more preferred results than generic sentence fusion. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 193-196. https://doi.org/10.3115/1557690 . 1557745</p>
<p>Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. 2021. Summac: Re-visiting nli-based models for inconsistency detection in summarization. Transactions of the Association for Computational Linguistics, 10:163-177. https://doi.org/10.1162 /tacl_a_00453</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Annual Meeting of the Association for Computational Linguistics. https://doi.org/10.18653 /v1/2020.acl-main. 703</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Re, Diana AcostaNavas, Drew A. Hudson, E. Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan S. Kim, Neel Guha, Niladri S. Chatterji, O. Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas F. Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110.
C. Lin and E. Hovy. 2002. From single to multidocument summarization: A prototype system and its evaluation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 457-464. https:// doi.org/10.3115/1073083.1073160</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Annual Meeting of the Association for Computational Linguistics.</p>
<p>Yang Liu and Mirella Lapata. 2019. Text summarization with pretrained encoders. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 3730-3740, Hong Kong, China. Association for Computational Linguistics. https://doi.org/10.18653/v1/D19 -1387</p>
<p>Yixin Liu, Alexander R. Fabbri, Pengfei Liu, Yilun Zhao, Linyong Nan, Ruilin Han, Simeng Han, Shafiq R. Joty, Chien-Sheng Wu, Caiming Xiong, and Dragomir R. Radev. 2022a. Revisiting the gold standard: Grounding summarization evaluation with robust human evaluation. ArXiv, abs/2212.07981. https://doi.org /10.18653/v1/2023.acl-long. 228</p>
<p>Yixin Liu, Pengfei Liu, Dragomir R. Radev, and Graham Neubig. 2022b. Brio: Bringing order to abstractive summarization. In Annual Meeting of the Association for Computational Linguistics. https://doi.org/10.18653 /v1/2022.acl-long. 207</p>
<p>Inderjeet Mani and Eric Bloedorn. 1999. Summarizing similarities and differences among related documents. Information Retrieval, 1(1-2):35-67. https://doi.org/10.1023 /A:1009930203452</p>
<p>Daniel Marcu. 1997. From discourse structures to text summaries. In Intelligent Scalable Text Summarization.</p>
<p>Erwin Marsi and Emiel Krahmer. 2005. Explorations in sentence fusion. In Proceedings of the European Workshop on Natural Language Generation 2005, pages 109-117.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906-1919, Online. Association for Computational Linguistics. https://doi.org /10.18653/v1/2020.acl-main. 173</p>
<p>Ryan McDonald. 2006. Discriminative sentence compression with soft syntactic evidence. In 11th Conference of the European Chapter of the Association for Computational Linguistics, pages 297-304.</p>
<p>Rada Mihalcea and Paul Tarau. 2005. Multidocument summarization with iterative graphbased algorithms. In Proceedings of the First International Conference on Intelligent Analysis Methods and Tools (IA 2005). McLean, VA.</p>
<p>Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don't give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. https://doi.org/10.18653/v1/D18 -1206</p>
<p>Ani Nenkova and Kathleen McKeown. 2011. Automatic summarization. Foundations and Trends in Information Retrieval, 52(2-3):103-233. https://doi.org/10.1561/1500000015</p>
<p>Ani Nenkova, Lucy Vanderwende, and Kathleen McKeown. 2006. A compositional context sensitive multi-document summarizer: Exploring the factors that influence summarization. In Proceedings of the Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 573-580. https://doi.org/10.1145/1148170 . 1148269</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.</p>
<p>Dragomir R. Radev, Eduard H. Hovy, and Kathleen McKeown. 2002. Introduction to the special issue on summarization. Computational Linguistics, 28:399-408. https://doi.org /10.1162/089120102762671927</p>
<p>Dragomir R. Radev, Hongyan Jing, and Malgorzata Budzikowska. 2000. Centroidbased summarization of multiple documents:</p>
<p>Sentence extraction, utility-based evaluation, and user studies. In NAACL-ANLP 2000 Workshop: Automatic Summarization.</p>
<p>Alexander M. Rush, Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence summarization. Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. https://doi.org/10.18653/v1/D15 -1044</p>
<p>Gerard Salton, Amit Singhal, Mandar Mitra, and Chris Buckley. 1997. Automatic text structuring and summarization. Information Processing \&amp; Management, 33(2):193-207. Methods and Tools for the Automatic Construction of Hypertext. https://doi.org /10.1016/S0306-4573(96)00062-3</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M. Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike TianJian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Stella Rose Biderman, Leo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush. 2021. Multitask prompted training enables zeroshot task generalization. arXiv preprint arXiv:2110.08207.</p>
<p>Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073-1083, Vancouver, Canada. Association for Computational Linguistics. https://doi.org/10 .18653/v1/P17-1099</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur P. Parikh. 2020. Bleurt: Learning robust metrics for text generation. In Annual Meeting of the Association for Computational Linguistics. https://doi.org/10.18653/v1/2020 .acl-main. 704
H. Grogory Silber and Kathleen F. McCoy. 2002. Efficiently computed lexical chains as an intermediate representation for automatic text summarization. Computational Linguistics, 28(4):487-496. https://doi.org/10.1162 /089120102762671954</p>
<p>Josef Steinberger, Massimo Poesio, Mijail A. Kabadjov, and Karel Jeek. 2007. Two uses of anaphora resolution in summarization. Information Processing and Management, 43(6):1663-1680. https://doi.org/10 .1016/j.ipm.2007.01.010</p>
<p>Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan J. Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2020. Learning to summarize from human feedback. arXiv preprint arXiv:2009.01325.</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. Advances in Neural Information Processing Systems, 27.</p>
<p>Kapil Thadani and Kathleen McKeown. 2013. Supervised sentence fusion with single-stage inference. In Proceedings of IJCNLP, Nagoya, Japan.</p>
<p>Oleg Vasilyev, Vedant Dharnidharka, and John Bohannon. 2020. Fill in the BLANC: Humanfree quality estimation of document summaries. In Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems, pages 11-20, Online. Association for Computational Linguistics. https://doi.org/10 .18653/v1/2020.eval4nlp-1.2</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal, M. Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Shailaja Keyur Sampat, Savan Doshi, Siddharth Deepak Mishra, Sujan Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Hannaneh Hajishirzi,</p>
<p>Noah A. Smith, and Daniel Khashabi. 2022. Benchmarking generalization via in-context instructions on 1, 600+ language tasks. arXiv preprint arXiv:2204.07705.</p>
<p>Mark E. Whiting, Grant Hugh, and Michael S. Bernstein. 2019. Fair work: Crowd work minimum wage with one line of code. In AAAI Conference on Human Computation \&amp; Crowdsourcing. https://doi.org/10 .1609/hcomp.v7i1.5283</p>
<p>Jeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano. 2021. Recursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862.</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. ArXiv, abs/2106.11520.</p>
<p>Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. 2020. Pegasus: Pre-training
with extracted gap-sentences for abstractive summarization. In ICML.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pre-trained transformer language models. ArXiv, abs/2205.01068.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations.</p>
<p>Daniel M. Ziegler, Nisan Stiennon, Jeff Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ Other annotators left during the course of study due to a change in freelance work schedule.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{5}$ To compute agreement for coherence and relevance, we first binarize the Likert scores, with a score of 3 or above being mapped to 1 .
${ }^{6}$ We note that the 350M GPT-3 consistently generates empty outputs on the XSUM dataset so we omit it from the human evaluation.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
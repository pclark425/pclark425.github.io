<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2223 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2223</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2223</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-60.html">extraction-schema-60</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <p><strong>Paper ID:</strong> paper-277787795</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.11393v2.pdf" target="_blank">DataDecide: How to Predict Best Pretraining Data with Small Experiments</a></p>
                <p><strong>Paper Abstract:</strong> Because large language models are expensive to pretrain on different datasets, using smaller-scale experiments to decide on data is crucial for reducing costs. Which benchmarks and methods of making decisions from observed performance at small scale most accurately predict the datasets that yield the best large models? To empower open exploration of this question, we release models, data, and evaluations in DataDecide -- the most extensive open suite of models over differences in data and scale. We conduct controlled pretraining experiments across 25 corpora with differing sources, deduplication, and filtering up to 100B tokens, model sizes up to 1B parameters, and 3 random seeds. We find that the ranking of models at a single, small size (e.g., 150M parameters) is a strong baseline for predicting best models at our larger target scale (1B) (~80% of com parisons correct). No scaling law methods among 8 baselines exceed the compute-decision frontier of single-scale predictions, but DataDecide can measure improvement in future scaling laws. We also identify that using continuous likelihood metrics as proxies in small experiments makes benchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval>80% predictable at the target 1B scale with just 0.01% of the compute.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2223.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2223.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ranking Single Scale</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ranking Single-Scale Experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simple decision rule that trains candidate pretraining data choices at one small, fixed model scale and selects the dataset with the highest observed downstream metric (or proxy) at that scale as the winner for the large target scale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Ranking Single-Scale Experiments</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Run all candidate pretraining recipes at a single small model size / compute budget and rank them by observed downstream performance (either discrete accuracy or continuous likelihood-based proxies). The top-ranked recipe is chosen for the target large-scale model. Implemented across 25 pretraining recipes and multiple small scales in DATADECIDE.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Machine learning / Natural language processing (pretraining dataset selection)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Small-scale downstream ACCURACY or continuous likelihood proxies (e.g., CORRECT_PROB, TOTAL_PROB)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Observed downstream performance on small models (e.g., ACCURACY on OLMES tasks computed in cloze formulation) or continuous likelihood-based proxies: CORRECT_PROB = mean model probability assigned to the correct continuation; TOTAL_PROB = mean sum of probabilities assigned to all candidate continuations; MARGIN, NORM_CORRECT_PROB variants described in Table 3. Likelihoods are character- or token-normalized.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>data-driven ML prediction / empirical surrogate (continuous likelihood as surrogate for discrete accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Large-scale downstream ACCURACY (OLMES macro-average at 1B target models)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Mean downstream performance (task accuracy with curated normalizations per task) measured on fully trained 1B-parameter models, averaged over 3 random seeds; used as the gold ranking for which pretraining recipe is truly better at target scale.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Decision accuracy: single-scale ranking at small size (e.g., 150M) predicts pairwise winners at 1B with ≈80% of comparisons correct (reported ~80% decision accuracy aggregated across 25 recipes); per-task compute-to-accuracy curves show variability. No single scalar R² reported. See also run-to-run std up to ~2 percentage points for some recipes at 1B.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>At 150M single-scale ranking: ≈80% decision accuracy in aggregate when predicting winners at 1B (paper statement: 'Pretrain 25 datasets @ 150M to predict pairs of 25 datasets @ 1B ~80% correct'). Performance varies by task (e.g., MMLU and ARC predictable with much less compute).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Gold target is the 1B mean ACCURACY over 3 seeds; absolute % values per task provided in released DATADECIDE but aggregate ground-truth performance values are task-dependent (not summarized as a single number in the text).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>In-distribution / near-training-data comparisons: ranking compares alternative pretraining recipes (variations on existing corpora, deduplication, filtering), so decisions are largely within distribution of known data choices rather than radical extrapolations.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Use of continuous likelihood proxy metrics (CORRECT_PROB, TOTAL_PROB) at small scale, averaging over seeds/checkpoints, and selecting appropriate small-scale hyperparameters via OLMo model ladder to reduce confounding. Also increasing compute (%C) improves decision accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Using likelihood proxies and optimizing compute: per-paper, continuous likelihood metrics make some tasks (MMLU, ARC, HellaSwag, MBPP, HumanEval) >80% predictable at 1B with just 0.01% of the target compute; overall, increasing small-scale compute yields roughly log-linear improvements in decision accuracy. No single numerical global improvement (e.g., delta %) is given for the proxy swap across all tasks, but code tasks went from near-trivial to ~80% decision accuracy when switching to CORRECT_PROB.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Proxy (small-scale) evaluation uses between orders of magnitude less compute than full 1B training; paper measures compute as %C (FLOPs relative to target): examples include predictions at 0.01% of target compute achieving >80% predictability for some tasks. Decision accuracy improves with more compute; single-scale ranking often attains the same compute-to-decision frontier as tested scaling-law methods, implying small experiments are cost-effective.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging/maturing empirical practice: scaling laws and small-scale proxy-based decision-making are active, well-studied areas but predicting downstream performance remains challenging; the paper positions dataset-selection-by-small-experiments as a practical engineering approach.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td>The paper reports run-to-run standard deviation (e.g., 1B 5×C scale std can be as high as ~2 percentage points of accuracy for some recipes) and reports decision accuracy averaged over 3 seeds and standard deviations across runs; however explicit calibration of predictive uncertainties to proxy-to-ground-truth gap is not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td>Multiple proxies show different behaviors: CORRECT_PROB and TOTAL_PROB often give better decision signal at small scales, while NORM_CORRECT_PROB and MARGIN can overtake at larger compute; failure modes are distinct across tasks (some proxies flat vs compute while others rise). No numerical pairwise correlation matrix is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Yes — computational small-scale experiments (proxy observation) → decision (select recipe) → validation by training final 1B models and measuring OLMES ACCURACY. Errors propagate from small-scale noise and crossovers in scaling trends, bounding decision accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Ranking single-scale cannot detect crossovers in scaling behavior between recipes (i.e., when one recipe overtakes another at scales between small experiments and target), run-to-run variance introduces noise (std up to ~2 accuracy percentage points at 1B), task sensitivity varies widely (some tasks predictable with tiny compute, others remain unpredictable), analysis limited to one token-parameter ratio (100, 5×Chinchilla) and multiple-choice cloze tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Run-to-run variance (random init and data order), spread of recipe performance (how separable recipes are on a benchmark), task sensitivity (some tasks have low signal at small scales), and crossovers in scaling trends across scale.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2223.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2223.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scaling Laws (multi-scale)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Extrapolating Scaling Laws (multi-scale fit variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fit parametric scaling-law relationships from compute (or N,D) to loss and then to downstream performance, using multiple small-scale observations, and extrapolate to predict target-scale performance and thereby pick the best pretraining dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Scaling-law Extrapolation (multiple variants: 2-, 3-, 5-parameter, single-step, helper points, checkpoint filtering)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Two-step (compute → loss → accuracy) and single-step fits mapping model compute (or N and D) to downstream accuracy. The default is a 3-parameter loss fit (L(C) = A C^α + E) chained to a sigmoid mapping from loss to accuracy; variants test removing irreducible loss, combining into single-step functions, adding helper points, and filtering early checkpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Machine learning / NLP performance prediction</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Predicted downstream ACCURACY extrapolated from fitted scaling law</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Predicted final-task accuracy at target scale computed by fitting parametric functions to observed small-scale checkpoints (final checkpoints and/or intermediate checkpoints) — e.g., fit loss as function of compute then map loss to accuracy via sigmoid.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>data-driven ML prediction / parametric extrapolation (empirical scaling laws)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Measured downstream ACCURACY on fully trained 1B models (OLMES tasks, averaged over 3 seeds)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Same gold target as above: mean ACCURACY at 1B across 3 seeds measured on OLMES benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Average prediction error for 1B targets (ACCURACY) reported in Table 4: best multi-scale variants (3-parameter variants) have Relative Error ≈5.6–6.5% and Absolute Error ≈2.6–3.2% (percent); single-step and larger-parameter variants perform much worse (e.g., 5-parameter single-step: Relative Error 42.8%, Absolute Error 17.4%; 5-parameter: 230.8% relative).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>Decision accuracy from multi-scale scaling-law approaches reaches at best the same compute-to-decision frontier as single-scale ranking (no scaling-law baseline among the 8 tested exceeded single-scale ranking). Specific prediction errors (relative/absolute) by variant reported (see quantitative_gap_measure).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Gold 1B ACCURACY per OLMES tasks (as above); scaling-law predictions compared against these in reported prediction-error metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>Applied to in-distribution dataset-variation settings (different recipes drawn from known corpora and preprocessing interventions); crossovers (where one recipe overtakes another at larger scale) are an extrapolative phenomenon relative to small-scale observations.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>Paper observes frequent crossovers in scaling trends which bound single-scale approximations and complicate extrapolation; scaling-law prediction error and decision accuracy are sensitive to presence of crossovers and to noise in small-scale checkpoints. No numeric sensitivity-by-distance metric is provided, but crossovers reduce decision accuracy relative to the constant single-scale ranking assumption.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Variants tested include adding helper anchor points (e.g., Acc=1 at L=0), filtering out early noisy checkpoints (>50% checkpoint filtering), and restricting fit ranges; these aim to reduce step-to-step noise and improve extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Best 3-parameter variants (including helpers and checkpoint filtering) reduced relative error to ~5.6% and absolute error to ~2.6% for 1B target ACCURACY predictions, improving over worse variants (single-step/5-parameter) which showed much larger errors. Despite these reductions, none of the tested scaling-law variants surpassed the compute-decision frontier set by single-scale ranking in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Multi-scale fitting requires multiple fully trained checkpoints across sizes; paper constrains budgets by using subsets of sizes/computes and reports compute budgets (%C) for prediction; multi-scale methods did not provide better decision accuracy per compute than single-scale experiments in these trials.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established empirical methodology (scaling-law literature), but downstream performance prediction remains challenging; the paper positions scaling-law extrapolation as an active but not-yet-solved area for dataset decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Yes — fit scaling laws to multiple small-scale checkpoints (compute/loss observations) → map to downstream accuracy → pick winner → validate by training 1B models. Errors propagate from noisy checkpoints and mis-specified functional forms.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Scaling-law extrapolation can be misled by checkpoint noise, early-phase training points, and crossovers; higher-parameter single-step models can catastrophically fail (very large prediction errors). Requires more small-scale fully trained checkpoints (higher compute) to get reliable fits.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Sensitivity to intermediate checkpoint noise, need for appropriate functional form (loss→accuracy), crossovers between recipes at different scales, dependence on training schedules and token-parameter ratios.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2223.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2223.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Likelihood Proxy Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Continuous Likelihood-Based Proxy Metrics (CORRECT_PROB, TOTAL_PROB, MARGIN, NORM_CORRECT_PROB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Continuous likelihood-derived metrics computed from model output probabilities used as surrogates for discrete accuracy on small-scale experiments to improve predictable ranking of pretraining datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Likelihood-based Proxy Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Compute per-item probabilities for candidate continuations and aggregate across dataset items to produce continuous scores: CORRECT_PROB (mean probability of the correct answer), TOTAL_PROB (mean sum of probabilities of all answer options), MARGIN (probability gap between correct and top incorrect), NORM_CORRECT_PROB (probability of correct conditioned on set), with character- or token-normalization. Used as small-scale proxies to predict large-scale ACCURACY.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Machine learning / NLP evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>CORRECT_PROB, TOTAL_PROB, MARGIN, NORM_CORRECT_PROB (character-normalized likelihoods)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>CORRECT_PROB = mean P(correct | context) over items; TOTAL_PROB = mean sum of P(all candidates | context); MARGIN = mean (P(correct) - max P(incorrect)); NORM_CORRECT_PROB = mean P(correct | correct ∪ incorrect set). Likelihoods normalized per character or token to account for length.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>empirical surrogate (data-driven ML-derived continuous proxy)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Discrete ACCURACY (OLMES tasks cloze formulation) measured on fully trained 1B models</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>OLMES ACCURACY per task computed with curated normalization; used as the decision gold standard at target scale.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Using CORRECT_PROB or TOTAL_PROB as proxies at very small compute (0.01% of target FLOPs) makes several benchmarks (MMLU, ARC, HellaSwag, MBPP, HumanEval) >80% predictable at the 1B target scale. No single scalar correlation coefficient is reported across all tasks; improvements are presented as decision-accuracy percentages and task-specific curves.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>For many tasks at small scales, CORRECT_PROB and TOTAL_PROB give decision accuracy at least as good as other metrics; specifically, code tasks (MBPP, HumanEval) go from near-trivial decision accuracy with ACCURACY at small scale to ~80% when using CORRECT_PROB.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Target ACCURACY on 1B evaluated per OLMES and code benchmarks; used for computing decision accuracy but exact per-task numeric ACCURACY values are in released DATADECIDE artifacts (not fully enumerated in text).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>Metrics applied to in-distribution test tasks; using likelihoods can reveal domain exposure even when discrete accuracy is near-zero for small models (helpful for out-of-accuracy but in-domain signal).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>Paper reports that CORRECT_PROB widens spreads and/or reduces noise for many tasks, improving separability among recipes; for tasks that are too hard for accuracy at small scale, likelihood proxies can lift decision accuracy substantially (e.g., code tasks). Math benchmarks did not benefit similarly, suggesting the proxy-to-accuracy gap depends on task type/novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Replace discrete accuracy with continuous likelihood-based proxies at the small experimental scale; normalize likelihoods per character/token; average over checkpoints and seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Enables >80% predictability on several benchmarks at tiny compute budgets (0.01% of target), and raises decision accuracy for code tasks from trivial to ~80%. No single aggregate improvement metric across all tasks is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Likelihood proxies allow useful predictions with orders-of-magnitude less compute relative to training target models; specific claim: >80% predictability on several tasks with 0.01% of target compute.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Practical empirical evaluation technique; continuous metrics have been noted in prior work as more smoothly scaling than discrete accuracy, and are adopted here as effective proxies at small scale.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td>Different proxies have different trends: CORRECT_PROB and TOTAL_PROB often give strong early signals (flat or higher at small scales) while NORM_CORRECT_PROB and MARGIN (which penalize incorrect-answer probability) may overtake near-target compute; failure modes differ by task, but no formal correlation coefficients are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Used as the small-scale stage in the computational → decision → large-scale validation cascade (small-model likelihoods → choose recipe → train 1B models → compare ACCURACY).</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not universally effective (e.g., some math benchmarks don't benefit), can sometimes plateau or decrease in decision accuracy near the final compute compared to accuracy-based metrics, and may reflect exposure to distractor/incorrect answers (TOTAL_PROB interpretation).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Task-specific signal/noise characteristics (some tasks provide stronger small-scale likelihood signal), length-normalization choices (per-char vs per-token) affect metric behavior, and run-to-run seed variance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2223.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2223.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DATADECIDE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DATADECIDE Suite</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A controlled suite of pretraining experiments (25 data recipes × 14 model scales × 3 seeds; >1,050 models) and accompanying evaluations designed to empirically measure how well small-scale proxy experiments predict large-scale dataset choices.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DATADECIDE experimental suite</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A public benchmark and artifact release that trains models across 25 pretraining data recipes at 14 scales up to 1B parameters (target) and 100B tokens, with multiple seeds and evaluations on 10 OLMES tasks; provides checkpoints, evaluations, and tooling for measuring decision accuracy of proxy methods and scaling-law extrapolations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Machine learning / empirical methods for dataset selection in NLP pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Small-scale downstream ACCURACY and continuous likelihood proxies (CORRECT_PROB, TOTAL_PROB, etc.) used within the suite for decision experiments</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Same proxies as above; DATADECIDE standardizes measuring these proxies over many datasets, scales, and seeds to quantify decision accuracy and prediction error between small experiments and large-model ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>empirical surrogate / data-driven ML prediction</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Fully trained target-scale (1B) downstream ACCURACY (OLMES macro-average and per-task accuracies), averaged over 3 seeds</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Gold standard rankings are computed from fully trained 1B models for each recipe, averaging over 3 reruns to account for run-to-run variance; used to evaluate decisions made from small experiments and scaling-law extrapolations.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Aggregate decision accuracy and prediction error metrics reported across the suite: single-scale (150M→1B) yields ≈80% pairwise decision accuracy; best scaling-law variants show relative prediction errors ≈5.6–6.5% and absolute errors ≈2.6–3.2% on ACCURACY predictions (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>Across tasks and recipes, small-scale proxies (especially continuous likelihoods) can provide high decision accuracy: e.g., MMLU/ARC predictable with small compute; code tasks (MBPP, HumanEval) become ~80% predictable using CORRECT_PROB at small scales.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Fully trained 1B model accuracies (per OLMES task) constitute the ground truth; exact numeric per-task ground-truth ACCURACY values are provided in released dataset/model artifacts rather than fully enumerated in-line in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>Suite focuses on variations of existing corpora and preprocessing (deduplication, filtering, mixes), so recipe comparisons are mainly in-distribution; the suite can detect crossovers where extrapolation would be required.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>Decision accuracy depends heavily on task difficulty and separability/spread of recipe performance: some tasks have low run-to-run noise and good spread (predictable), others have high noise/low spread (hard to predict). Crossovers in scaling trends are observed and limit some prediction approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Providing many controlled checkpoints, multiple seeds at target scale, and multiple proxy metrics; tools for helper points and checkpoint filtering to improve scaling-law fits; encouraging use of continuous proxies to reduce noise.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Enables precise measurement of prediction errors across methods and improvement quantification (e.g., showing that 3-parameter with helpers and checkpoint filtering lowers prediction error to ~5.6% relative / 2.6% absolute). Demonstrated that certain proxies achieve >80% decision accuracy at tiny compute budgets for specific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>DATADECIDE explicitly reports compute budgets as %C (FLOPs relative to target) and maps compute to decision accuracy, showing tasks where predictions are cheap (orders of magnitude less compute) vs those requiring more compute. Example: some tasks >80% predictable at 0.01% of target FLOPs using likelihood proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Designed as an empirical infrastructure to raise maturity of dataset-selection research; it formalizes evaluation standards for proxy-vs-ground-truth comparisons but the broader problem remains an active research area.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td>DATADECIDE includes 3-seed reruns at the 1B target to quantify run-to-run variance (std reported up to ~2 percentage points); however explicit probabilistic calibration of proxy uncertainties vs actual errors is not presented.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td>The suite's analyses show different proxies yield different separability and noise characteristics per task (e.g., CORRECT_PROB vs NORM_CORRECT_PROB), indicating partially independent failure modes; no explicit correlation matrices are given in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Yes — DATADECIDE is explicitly built to support the cascade: small experiments and proxies → ranking or scaling-law extrapolation decisions → validation by fully trained 1B models with multiple seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Suite limited to one token-to-parameter ratio (100, 5×Chinchilla), to multiple-choice cloze-formulation tasks (OLMES), and to scales up to 1B parameters—findings may not generalize to other regimes; crossovers and seed variance remain key difficulties.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Choice of tasks (OLMES multiple-choice), token-parameter ratio, hyperparameter scheduling choices, and run-to-run randomness affect proxy-to-ground-truth gap; some tasks inherently have low signal at the explored scales.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Establishing task scaling laws via compute-efficient model ladders. <em>(Rating: 2)</em></li>
                <li>Language models scale reliably with over-training and on downstream tasks. <em>(Rating: 2)</em></li>
                <li>In search of the next generation of training sets for language models <em>(Rating: 1)</em></li>
                <li>Scaling laws for neural language models. <em>(Rating: 2)</em></li>
                <li>Loss-to-loss prediction: Scaling laws for all datasets. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2223",
    "paper_id": "paper-277787795",
    "extraction_schema_id": "extraction-schema-60",
    "extracted_data": [
        {
            "name_short": "Ranking Single Scale",
            "name_full": "Ranking Single-Scale Experiments",
            "brief_description": "Simple decision rule that trains candidate pretraining data choices at one small, fixed model scale and selects the dataset with the highest observed downstream metric (or proxy) at that scale as the winner for the large target scale.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Ranking Single-Scale Experiments",
            "system_description": "Run all candidate pretraining recipes at a single small model size / compute budget and rank them by observed downstream performance (either discrete accuracy or continuous likelihood-based proxies). The top-ranked recipe is chosen for the target large-scale model. Implemented across 25 pretraining recipes and multiple small scales in DATADECIDE.",
            "domain": "Machine learning / Natural language processing (pretraining dataset selection)",
            "proxy_metric_name": "Small-scale downstream ACCURACY or continuous likelihood proxies (e.g., CORRECT_PROB, TOTAL_PROB)",
            "proxy_metric_description": "Observed downstream performance on small models (e.g., ACCURACY on OLMES tasks computed in cloze formulation) or continuous likelihood-based proxies: CORRECT_PROB = mean model probability assigned to the correct continuation; TOTAL_PROB = mean sum of probabilities assigned to all candidate continuations; MARGIN, NORM_CORRECT_PROB variants described in Table 3. Likelihoods are character- or token-normalized.",
            "proxy_metric_type": "data-driven ML prediction / empirical surrogate (continuous likelihood as surrogate for discrete accuracy)",
            "ground_truth_metric": "Large-scale downstream ACCURACY (OLMES macro-average at 1B target models)",
            "ground_truth_description": "Mean downstream performance (task accuracy with curated normalizations per task) measured on fully trained 1B-parameter models, averaged over 3 random seeds; used as the gold ranking for which pretraining recipe is truly better at target scale.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Decision accuracy: single-scale ranking at small size (e.g., 150M) predicts pairwise winners at 1B with ≈80% of comparisons correct (reported ~80% decision accuracy aggregated across 25 recipes); per-task compute-to-accuracy curves show variability. No single scalar R² reported. See also run-to-run std up to ~2 percentage points for some recipes at 1B.",
            "proxy_performance": "At 150M single-scale ranking: ≈80% decision accuracy in aggregate when predicting winners at 1B (paper statement: 'Pretrain 25 datasets @ 150M to predict pairs of 25 datasets @ 1B ~80% correct'). Performance varies by task (e.g., MMLU and ARC predictable with much less compute).",
            "ground_truth_performance": "Gold target is the 1B mean ACCURACY over 3 seeds; absolute % values per task provided in released DATADECIDE but aggregate ground-truth performance values are task-dependent (not summarized as a single number in the text).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_characterization": "In-distribution / near-training-data comparisons: ranking compares alternative pretraining recipes (variations on existing corpora, deduplication, filtering), so decisions are largely within distribution of known data choices rather than radical extrapolations.",
            "gap_varies_with_novelty": null,
            "gap_variation_details": "",
            "gap_reduction_method": "Use of continuous likelihood proxy metrics (CORRECT_PROB, TOTAL_PROB) at small scale, averaging over seeds/checkpoints, and selecting appropriate small-scale hyperparameters via OLMo model ladder to reduce confounding. Also increasing compute (%C) improves decision accuracy.",
            "gap_reduction_effectiveness": "Using likelihood proxies and optimizing compute: per-paper, continuous likelihood metrics make some tasks (MMLU, ARC, HellaSwag, MBPP, HumanEval) &gt;80% predictable at 1B with just 0.01% of the target compute; overall, increasing small-scale compute yields roughly log-linear improvements in decision accuracy. No single numerical global improvement (e.g., delta %) is given for the proxy swap across all tasks, but code tasks went from near-trivial to ~80% decision accuracy when switching to CORRECT_PROB.",
            "validation_cost_comparison": "Proxy (small-scale) evaluation uses between orders of magnitude less compute than full 1B training; paper measures compute as %C (FLOPs relative to target): examples include predictions at 0.01% of target compute achieving &gt;80% predictability for some tasks. Decision accuracy improves with more compute; single-scale ranking often attains the same compute-to-decision frontier as tested scaling-law methods, implying small experiments are cost-effective.",
            "temporal_validation": null,
            "domain_maturity": "Emerging/maturing empirical practice: scaling laws and small-scale proxy-based decision-making are active, well-studied areas but predicting downstream performance remains challenging; the paper positions dataset-selection-by-small-experiments as a practical engineering approach.",
            "uncertainty_quantification": true,
            "uncertainty_calibration": "The paper reports run-to-run standard deviation (e.g., 1B 5×C scale std can be as high as ~2 percentage points of accuracy for some recipes) and reports decision accuracy averaged over 3 seeds and standard deviations across runs; however explicit calibration of predictive uncertainties to proxy-to-ground-truth gap is not provided.",
            "multiple_proxies": true,
            "proxy_correlation": "Multiple proxies show different behaviors: CORRECT_PROB and TOTAL_PROB often give better decision signal at small scales, while NORM_CORRECT_PROB and MARGIN can overtake at larger compute; failure modes are distinct across tasks (some proxies flat vs compute while others rise). No numerical pairwise correlation matrix is reported.",
            "validation_cascade": "Yes — computational small-scale experiments (proxy observation) → decision (select recipe) → validation by training final 1B models and measuring OLMES ACCURACY. Errors propagate from small-scale noise and crossovers in scaling trends, bounding decision accuracy.",
            "publication_bias_discussion": null,
            "limitations_challenges": "Ranking single-scale cannot detect crossovers in scaling behavior between recipes (i.e., when one recipe overtakes another at scales between small experiments and target), run-to-run variance introduces noise (std up to ~2 accuracy percentage points at 1B), task sensitivity varies widely (some tasks predictable with tiny compute, others remain unpredictable), analysis limited to one token-parameter ratio (100, 5×Chinchilla) and multiple-choice cloze tasks.",
            "domain_specific_factors": "Run-to-run variance (random init and data order), spread of recipe performance (how separable recipes are on a benchmark), task sensitivity (some tasks have low signal at small scales), and crossovers in scaling trends across scale.",
            "uuid": "e2223.0"
        },
        {
            "name_short": "Scaling Laws (multi-scale)",
            "name_full": "Extrapolating Scaling Laws (multi-scale fit variants)",
            "brief_description": "Fit parametric scaling-law relationships from compute (or N,D) to loss and then to downstream performance, using multiple small-scale observations, and extrapolate to predict target-scale performance and thereby pick the best pretraining dataset.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Scaling-law Extrapolation (multiple variants: 2-, 3-, 5-parameter, single-step, helper points, checkpoint filtering)",
            "system_description": "Two-step (compute → loss → accuracy) and single-step fits mapping model compute (or N and D) to downstream accuracy. The default is a 3-parameter loss fit (L(C) = A C^α + E) chained to a sigmoid mapping from loss to accuracy; variants test removing irreducible loss, combining into single-step functions, adding helper points, and filtering early checkpoints.",
            "domain": "Machine learning / NLP performance prediction",
            "proxy_metric_name": "Predicted downstream ACCURACY extrapolated from fitted scaling law",
            "proxy_metric_description": "Predicted final-task accuracy at target scale computed by fitting parametric functions to observed small-scale checkpoints (final checkpoints and/or intermediate checkpoints) — e.g., fit loss as function of compute then map loss to accuracy via sigmoid.",
            "proxy_metric_type": "data-driven ML prediction / parametric extrapolation (empirical scaling laws)",
            "ground_truth_metric": "Measured downstream ACCURACY on fully trained 1B models (OLMES tasks, averaged over 3 seeds)",
            "ground_truth_description": "Same gold target as above: mean ACCURACY at 1B across 3 seeds measured on OLMES benchmarks.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Average prediction error for 1B targets (ACCURACY) reported in Table 4: best multi-scale variants (3-parameter variants) have Relative Error ≈5.6–6.5% and Absolute Error ≈2.6–3.2% (percent); single-step and larger-parameter variants perform much worse (e.g., 5-parameter single-step: Relative Error 42.8%, Absolute Error 17.4%; 5-parameter: 230.8% relative).",
            "proxy_performance": "Decision accuracy from multi-scale scaling-law approaches reaches at best the same compute-to-decision frontier as single-scale ranking (no scaling-law baseline among the 8 tested exceeded single-scale ranking). Specific prediction errors (relative/absolute) by variant reported (see quantitative_gap_measure).",
            "ground_truth_performance": "Gold 1B ACCURACY per OLMES tasks (as above); scaling-law predictions compared against these in reported prediction-error metrics.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_characterization": "Applied to in-distribution dataset-variation settings (different recipes drawn from known corpora and preprocessing interventions); crossovers (where one recipe overtakes another at larger scale) are an extrapolative phenomenon relative to small-scale observations.",
            "gap_varies_with_novelty": true,
            "gap_variation_details": "Paper observes frequent crossovers in scaling trends which bound single-scale approximations and complicate extrapolation; scaling-law prediction error and decision accuracy are sensitive to presence of crossovers and to noise in small-scale checkpoints. No numeric sensitivity-by-distance metric is provided, but crossovers reduce decision accuracy relative to the constant single-scale ranking assumption.",
            "gap_reduction_method": "Variants tested include adding helper anchor points (e.g., Acc=1 at L=0), filtering out early noisy checkpoints (&gt;50% checkpoint filtering), and restricting fit ranges; these aim to reduce step-to-step noise and improve extrapolation.",
            "gap_reduction_effectiveness": "Best 3-parameter variants (including helpers and checkpoint filtering) reduced relative error to ~5.6% and absolute error to ~2.6% for 1B target ACCURACY predictions, improving over worse variants (single-step/5-parameter) which showed much larger errors. Despite these reductions, none of the tested scaling-law variants surpassed the compute-decision frontier set by single-scale ranking in the experiments.",
            "validation_cost_comparison": "Multi-scale fitting requires multiple fully trained checkpoints across sizes; paper constrains budgets by using subsets of sizes/computes and reports compute budgets (%C) for prediction; multi-scale methods did not provide better decision accuracy per compute than single-scale experiments in these trials.",
            "temporal_validation": null,
            "domain_maturity": "Established empirical methodology (scaling-law literature), but downstream performance prediction remains challenging; the paper positions scaling-law extrapolation as an active but not-yet-solved area for dataset decisions.",
            "uncertainty_quantification": false,
            "uncertainty_calibration": "",
            "multiple_proxies": false,
            "proxy_correlation": "",
            "validation_cascade": "Yes — fit scaling laws to multiple small-scale checkpoints (compute/loss observations) → map to downstream accuracy → pick winner → validate by training 1B models. Errors propagate from noisy checkpoints and mis-specified functional forms.",
            "publication_bias_discussion": null,
            "limitations_challenges": "Scaling-law extrapolation can be misled by checkpoint noise, early-phase training points, and crossovers; higher-parameter single-step models can catastrophically fail (very large prediction errors). Requires more small-scale fully trained checkpoints (higher compute) to get reliable fits.",
            "domain_specific_factors": "Sensitivity to intermediate checkpoint noise, need for appropriate functional form (loss→accuracy), crossovers between recipes at different scales, dependence on training schedules and token-parameter ratios.",
            "uuid": "e2223.1"
        },
        {
            "name_short": "Likelihood Proxy Metrics",
            "name_full": "Continuous Likelihood-Based Proxy Metrics (CORRECT_PROB, TOTAL_PROB, MARGIN, NORM_CORRECT_PROB)",
            "brief_description": "Continuous likelihood-derived metrics computed from model output probabilities used as surrogates for discrete accuracy on small-scale experiments to improve predictable ranking of pretraining datasets.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Likelihood-based Proxy Metrics",
            "system_description": "Compute per-item probabilities for candidate continuations and aggregate across dataset items to produce continuous scores: CORRECT_PROB (mean probability of the correct answer), TOTAL_PROB (mean sum of probabilities of all answer options), MARGIN (probability gap between correct and top incorrect), NORM_CORRECT_PROB (probability of correct conditioned on set), with character- or token-normalization. Used as small-scale proxies to predict large-scale ACCURACY.",
            "domain": "Machine learning / NLP evaluation metrics",
            "proxy_metric_name": "CORRECT_PROB, TOTAL_PROB, MARGIN, NORM_CORRECT_PROB (character-normalized likelihoods)",
            "proxy_metric_description": "CORRECT_PROB = mean P(correct | context) over items; TOTAL_PROB = mean sum of P(all candidates | context); MARGIN = mean (P(correct) - max P(incorrect)); NORM_CORRECT_PROB = mean P(correct | correct ∪ incorrect set). Likelihoods normalized per character or token to account for length.",
            "proxy_metric_type": "empirical surrogate (data-driven ML-derived continuous proxy)",
            "ground_truth_metric": "Discrete ACCURACY (OLMES tasks cloze formulation) measured on fully trained 1B models",
            "ground_truth_description": "OLMES ACCURACY per task computed with curated normalization; used as the decision gold standard at target scale.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Using CORRECT_PROB or TOTAL_PROB as proxies at very small compute (0.01% of target FLOPs) makes several benchmarks (MMLU, ARC, HellaSwag, MBPP, HumanEval) &gt;80% predictable at the 1B target scale. No single scalar correlation coefficient is reported across all tasks; improvements are presented as decision-accuracy percentages and task-specific curves.",
            "proxy_performance": "For many tasks at small scales, CORRECT_PROB and TOTAL_PROB give decision accuracy at least as good as other metrics; specifically, code tasks (MBPP, HumanEval) go from near-trivial decision accuracy with ACCURACY at small scale to ~80% when using CORRECT_PROB.",
            "ground_truth_performance": "Target ACCURACY on 1B evaluated per OLMES and code benchmarks; used for computing decision accuracy but exact per-task numeric ACCURACY values are in released DATADECIDE artifacts (not fully enumerated in text).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_characterization": "Metrics applied to in-distribution test tasks; using likelihoods can reveal domain exposure even when discrete accuracy is near-zero for small models (helpful for out-of-accuracy but in-domain signal).",
            "gap_varies_with_novelty": true,
            "gap_variation_details": "Paper reports that CORRECT_PROB widens spreads and/or reduces noise for many tasks, improving separability among recipes; for tasks that are too hard for accuracy at small scale, likelihood proxies can lift decision accuracy substantially (e.g., code tasks). Math benchmarks did not benefit similarly, suggesting the proxy-to-accuracy gap depends on task type/novelty.",
            "gap_reduction_method": "Replace discrete accuracy with continuous likelihood-based proxies at the small experimental scale; normalize likelihoods per character/token; average over checkpoints and seeds.",
            "gap_reduction_effectiveness": "Enables &gt;80% predictability on several benchmarks at tiny compute budgets (0.01% of target), and raises decision accuracy for code tasks from trivial to ~80%. No single aggregate improvement metric across all tasks is provided.",
            "validation_cost_comparison": "Likelihood proxies allow useful predictions with orders-of-magnitude less compute relative to training target models; specific claim: &gt;80% predictability on several tasks with 0.01% of target compute.",
            "temporal_validation": null,
            "domain_maturity": "Practical empirical evaluation technique; continuous metrics have been noted in prior work as more smoothly scaling than discrete accuracy, and are adopted here as effective proxies at small scale.",
            "uncertainty_quantification": null,
            "uncertainty_calibration": "",
            "multiple_proxies": true,
            "proxy_correlation": "Different proxies have different trends: CORRECT_PROB and TOTAL_PROB often give strong early signals (flat or higher at small scales) while NORM_CORRECT_PROB and MARGIN (which penalize incorrect-answer probability) may overtake near-target compute; failure modes differ by task, but no formal correlation coefficients are reported.",
            "validation_cascade": "Used as the small-scale stage in the computational → decision → large-scale validation cascade (small-model likelihoods → choose recipe → train 1B models → compare ACCURACY).",
            "publication_bias_discussion": null,
            "limitations_challenges": "Not universally effective (e.g., some math benchmarks don't benefit), can sometimes plateau or decrease in decision accuracy near the final compute compared to accuracy-based metrics, and may reflect exposure to distractor/incorrect answers (TOTAL_PROB interpretation).",
            "domain_specific_factors": "Task-specific signal/noise characteristics (some tasks provide stronger small-scale likelihood signal), length-normalization choices (per-char vs per-token) affect metric behavior, and run-to-run seed variance.",
            "uuid": "e2223.2"
        },
        {
            "name_short": "DATADECIDE",
            "name_full": "DATADECIDE Suite",
            "brief_description": "A controlled suite of pretraining experiments (25 data recipes × 14 model scales × 3 seeds; &gt;1,050 models) and accompanying evaluations designed to empirically measure how well small-scale proxy experiments predict large-scale dataset choices.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "DATADECIDE experimental suite",
            "system_description": "A public benchmark and artifact release that trains models across 25 pretraining data recipes at 14 scales up to 1B parameters (target) and 100B tokens, with multiple seeds and evaluations on 10 OLMES tasks; provides checkpoints, evaluations, and tooling for measuring decision accuracy of proxy methods and scaling-law extrapolations.",
            "domain": "Machine learning / empirical methods for dataset selection in NLP pretraining",
            "proxy_metric_name": "Small-scale downstream ACCURACY and continuous likelihood proxies (CORRECT_PROB, TOTAL_PROB, etc.) used within the suite for decision experiments",
            "proxy_metric_description": "Same proxies as above; DATADECIDE standardizes measuring these proxies over many datasets, scales, and seeds to quantify decision accuracy and prediction error between small experiments and large-model ground truth.",
            "proxy_metric_type": "empirical surrogate / data-driven ML prediction",
            "ground_truth_metric": "Fully trained target-scale (1B) downstream ACCURACY (OLMES macro-average and per-task accuracies), averaged over 3 seeds",
            "ground_truth_description": "Gold standard rankings are computed from fully trained 1B models for each recipe, averaging over 3 reruns to account for run-to-run variance; used to evaluate decisions made from small experiments and scaling-law extrapolations.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Aggregate decision accuracy and prediction error metrics reported across the suite: single-scale (150M→1B) yields ≈80% pairwise decision accuracy; best scaling-law variants show relative prediction errors ≈5.6–6.5% and absolute errors ≈2.6–3.2% on ACCURACY predictions (Table 4).",
            "proxy_performance": "Across tasks and recipes, small-scale proxies (especially continuous likelihoods) can provide high decision accuracy: e.g., MMLU/ARC predictable with small compute; code tasks (MBPP, HumanEval) become ~80% predictable using CORRECT_PROB at small scales.",
            "ground_truth_performance": "Fully trained 1B model accuracies (per OLMES task) constitute the ground truth; exact numeric per-task ground-truth ACCURACY values are provided in released dataset/model artifacts rather than fully enumerated in-line in the text.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_characterization": "Suite focuses on variations of existing corpora and preprocessing (deduplication, filtering, mixes), so recipe comparisons are mainly in-distribution; the suite can detect crossovers where extrapolation would be required.",
            "gap_varies_with_novelty": true,
            "gap_variation_details": "Decision accuracy depends heavily on task difficulty and separability/spread of recipe performance: some tasks have low run-to-run noise and good spread (predictable), others have high noise/low spread (hard to predict). Crossovers in scaling trends are observed and limit some prediction approaches.",
            "gap_reduction_method": "Providing many controlled checkpoints, multiple seeds at target scale, and multiple proxy metrics; tools for helper points and checkpoint filtering to improve scaling-law fits; encouraging use of continuous proxies to reduce noise.",
            "gap_reduction_effectiveness": "Enables precise measurement of prediction errors across methods and improvement quantification (e.g., showing that 3-parameter with helpers and checkpoint filtering lowers prediction error to ~5.6% relative / 2.6% absolute). Demonstrated that certain proxies achieve &gt;80% decision accuracy at tiny compute budgets for specific tasks.",
            "validation_cost_comparison": "DATADECIDE explicitly reports compute budgets as %C (FLOPs relative to target) and maps compute to decision accuracy, showing tasks where predictions are cheap (orders of magnitude less compute) vs those requiring more compute. Example: some tasks &gt;80% predictable at 0.01% of target FLOPs using likelihood proxies.",
            "temporal_validation": null,
            "domain_maturity": "Designed as an empirical infrastructure to raise maturity of dataset-selection research; it formalizes evaluation standards for proxy-vs-ground-truth comparisons but the broader problem remains an active research area.",
            "uncertainty_quantification": true,
            "uncertainty_calibration": "DATADECIDE includes 3-seed reruns at the 1B target to quantify run-to-run variance (std reported up to ~2 percentage points); however explicit probabilistic calibration of proxy uncertainties vs actual errors is not presented.",
            "multiple_proxies": true,
            "proxy_correlation": "The suite's analyses show different proxies yield different separability and noise characteristics per task (e.g., CORRECT_PROB vs NORM_CORRECT_PROB), indicating partially independent failure modes; no explicit correlation matrices are given in the paper.",
            "validation_cascade": "Yes — DATADECIDE is explicitly built to support the cascade: small experiments and proxies → ranking or scaling-law extrapolation decisions → validation by fully trained 1B models with multiple seeds.",
            "publication_bias_discussion": null,
            "limitations_challenges": "Suite limited to one token-to-parameter ratio (100, 5×Chinchilla), to multiple-choice cloze-formulation tasks (OLMES), and to scales up to 1B parameters—findings may not generalize to other regimes; crossovers and seed variance remain key difficulties.",
            "domain_specific_factors": "Choice of tasks (OLMES multiple-choice), token-parameter ratio, hyperparameter scheduling choices, and run-to-run randomness affect proxy-to-ground-truth gap; some tasks inherently have low signal at the explored scales.",
            "uuid": "e2223.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Establishing task scaling laws via compute-efficient model ladders.",
            "rating": 2
        },
        {
            "paper_title": "Language models scale reliably with over-training and on downstream tasks.",
            "rating": 2
        },
        {
            "paper_title": "In search of the next generation of training sets for language models",
            "rating": 1
        },
        {
            "paper_title": "Scaling laws for neural language models.",
            "rating": 2
        },
        {
            "paper_title": "Loss-to-loss prediction: Scaling laws for all datasets.",
            "rating": 1
        }
    ],
    "cost": 0.019031,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>How to Predict Best Pretraining Data with Small Experiments
13 Jul 2025</p>
<p>Ian Magnusson 
Allen Institute for AI</p>
<p>Paul G. Allen School of Computer Science &amp; Engineering
University of Wash-ington</p>
<p>Nguyen Tai 
University of Pennsylvania</p>
<p>Ben Bogin 
Allen Institute for AI</p>
<p>David Heineman 
Allen Institute for AI</p>
<p>Jena Hwang 
Allen Institute for AI</p>
<p>Luca Soldaini 
Allen Institute for AI</p>
<p>Akshita Bhagia 
Allen Institute for AI</p>
<p>Jiacheng Liu 
Allen Institute for AI</p>
<p>Paul G. Allen School of Computer Science &amp; Engineering
University of Wash-ington</p>
<p>Dirk Groeneveld 
Allen Institute for AI</p>
<p>Oyvind Tafjord 
Allen Institute for AI</p>
<p>Noah A Smith 
Allen Institute for AI</p>
<p>Paul G. Allen School of Computer Science &amp; Engineering
University of Wash-ington</p>
<p>Pang Wei Koh 
Allen Institute for AI</p>
<p>Paul G. Allen School of Computer Science &amp; Engineering
University of Wash-ington</p>
<p>Jesse Dodge 
Allen Institute for AI</p>
<p>Ian Mag- Nusson 
How to Predict Best Pretraining Data with Small Experiments
13 Jul 2025419E21A3A48F1A533EACBB826C8F12D6arXiv:2504.11393v2[cs.LG]
Because large language models are expensive to pretrain on different datasets, using smaller-scale experiments to decide on data is crucial for reducing costs.Which benchmarks and methods of making decisions from observed performance at small scale most accurately predict the datasets that yield the best large models?To empower open exploration of this question, we release models, data, and evaluations in DATADECIDE-the most extensive open suite of models over differences in data and scale.We conduct controlled pretraining experiments across 25 corpora with differing sources, deduplication, and filtering up to 100B tokens, model sizes up to 1B parameters, and 3 random seeds.We find that the ranking of models at a single, small size (e.g., 150M parameters) is a strong baseline for predicting best models at our larger target scale (1B) (∼ 80% of comparisons correct).No scaling law methods among 8 baselines exceed the compute-decision frontier of single-scale predictions, but DATADECIDE can measure improvement in future scaling laws.We also identify that using continuous likelihood metrics as proxies in small experiments makes benchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval &gt; 80% predictable at the target 1B scale with just 0.01% of the compute.</p>
<p>Introduction</p>
<p>The cost of training large language models (LMs) necessitates methods of trying out options at small scale, but it also makes it expensive to validate the accuracy of development decisions made with such methods.We focus on the question of choosing between pretraining datasets to use-one of the most impactful development decisions.Common practice (e.g., Li et al., 2024) uses a single, small scale of experiments to cheaply test pretraining data intended for larger-scale models, where scale is determined by number of model parameters and training tokens.The other predominant approach is to fit scaling laws (Kaplan et al., 2020;Hoffmann et al., 2022;Choshen et al., 2024) to the trend in performance observed over multiple small scales, with recent work extending this to the prediction of downstream performance instead of language modeling loss (Gadre et al., 2024;Dubey et al., 2024;Bhagia et al., 2024).</p>
<p>So far decision-making approaches have only been validated without observing the counterfactual outcome, either by producing a single large model on the chosen decision with impressive performance or by low error in predicting the magnitude of observed performance of a small number of large models.Knowing what amount of error in predicting performance over scale is a low enough to actually make a correct decision among datasets, requires a suite of comparable models trained on many datasets.Although a wide variety of open-source pretraining corpora are available, the scaling behavior of data is difficult to assess from off-theshelf models that vary simultaneously in data, optimizer, and modeling decisions.</p>
<p>To make it possible to empirically study what methods make the best decisions over data, we build DATADECIDE1 -a suite of models we pretrain on 25 corpora up to 100B tokens, over 14 different model sizes ranging from 4M parameters up to 1B parameters (more than 30K model checkpoints in total).We evaluate all models across a suite of 10 downstream tasks and calculate how accurately small models predict which pretraining corpora lead to better performance</p>
<p>Targets</p>
<p>Pretrain 25 datasets @ 150M to predict pairs of 25 datasets @ 1B ~80% correct Figure 1.Which pretraining data to use? Ideally, compare performance of large models with fixed configurations averaged over random seeds (left).In practice, cheaper, smaller-scale experiments are used (center).Here DATADECIDE measures accuracy of pairwise decisions between 25 pretraining corpora to find efficient prediction methods (right).</p>
<p>at our largest scale.Our conclusions provide practical recommendations for the best benchmarks, prediction methods, and metrics to use to make decisions.</p>
<p>We call the 25 corpora we train on data recipes as they range across popular corpora including Dolma (Soldaini et al., 2024), DCLM (Li et al., 2024), RefinedWeb (Penedo et al., 2023), C4 (Raffel et al., 2019), and FineWeb (Penedo et al., 2024) as well as combinations of interventions on these datasets such as source mixing, deduplication, and filtering.Previous work has considered only 2 (Biderman et al., 2023) or 6 recipes (Magnusson et al., 2024;Brandfonbrener et al., 2024).We also offer a novel affordance by including 3 random seed reruns for even our largest runs, to help quantify whether variation occurs due to random initialization and data order or differences in the distribution of data.</p>
<p>Concretely, DATADECIDE allows analyses such as Figure 1 (right), which shows the relationship between compute used to predict a ranking of datasets and how accurately that ranking reflects mean performance over 3 seed runs (quantified here by OLMES; Gu et al., 2024) for models fully trained on those datasets at the target (1B) scale.We measure the accuracy of decisions as the percent of compared pairs of datasets where the prediction identifies the correct winner.Each point represents the average decision accuracy of a given method over 3 prediction attempts using small models with different random seeds, and shading shows standard deviation.</p>
<p>Measuring the tradeoff of compute cost to better decisions lets us make the following recommendations about small experiments for making data decisions:</p>
<p>• §3.1 -The amount of compute you need to allocate for a given decision accuracy depends heavily on task.MMLU and ARC are much cheaper to predict than Hel-laSwag and some tasks such as SocialIQA are difficult to predict at all scales.</p>
<p>• §3.2 -8 baseline scaling law methods do not exceed the compute to decision accuracy frontier set by ranking single scale experiments.</p>
<p>• §3.3 -At small scales, continuous metrics using answer likelihood are better or equivalent predictors of decisions than using the same discrete accuracy target metric.</p>
<p>• §3.4 -Better decisions can be explained in part by low run-to-run variance and a wide spread of benchmark performance values for different data, traits which can be improved by proxy metrics.</p>
<p>Future research can extend DATADECIDE with little extra compute by running new evaluations on our checkpoints, pretraining additional small models to compare against the large target models we provide, or trying new prediction Dolma1.6++Original Dolma 1.6 plus additional sources from Dolma 1.7: RedPajama's arxiv subset, openwebmath, algebraic stack, flan, starcoder, falcon.</p>
<p>C4 Original</p>
<p>The C4 dataset (Raffel et al., 2019) as prepared in Dolma 1.7, heuristically filtered from the April 2019 Common Crawl.</p>
<p>FineWeb-Pro Original</p>
<p>The FineWeb Pro corpus (Zhou et al., 2024), featuring model-driven data cleaning on FineWeb.</p>
<p>FineWeb-Edu Original</p>
<p>The deduplicated FineWeb-Edu subset of SmolLM-Corpus (Ben Allal et al., 2024), focused on educational web pages.</p>
<p>Falcon Original</p>
<p>The Falcon RefinedWeb corpus (Penedo et al., 2023) in Dolma 1.7, derived from Common Crawl through June 2023 and more aggressively filtered/deduplicated than C4.</p>
<p>Falcon+CC Original, QC 10%, QC 20%, QC Orig 10%, QC Tulu 10%</p>
<p>Falcon and Dolma 1.7's Common Crawl.We quality filter to top 10% or 20% documents with reproduced or original (Li et al., 2024) filter or retrain filter on pre-release version of Tulu-v3 (Lambert et al., 2024).methods with lightweight manipulations such as smoothing and curve fitting on top of our released evaluation results.</p>
<p>DCLM-Baseline</p>
<p>Methods</p>
<p>Our aim is to empirically test the predictability of downstream performance at a larger, target scale using small experiments.We describe DATADECIDE §2.1, the prediction methods we examine §2.2, the metrics we use to assess predictions §2.3, how we measure downstream performance §2.4, and proxy metrics for our performance evaluations §2.5.We will release all models, checkpoints, pretraining corpora, and evaluations.</p>
<p>The DATADECIDE Suite</p>
<p>We pretrain a suite of 1,050 models using 25 data recipes × 14 model scales × 3 random seeds for initialization and data order.Table 1 describes the 25 data recipes included in DATADECIDE that aim to provide coverage of common data preparation choices such as deduplication, ablating domains, mixes of existing datasets, as well as quality filters with different implementations, training data, and thresholds for quality classifiers.</p>
<p>We select a token to parameter ratio of 100, which at 5× "Chinchilla" (5 × C) optimal ratio (Hoffmann et al., 2022) captures the typical overtraining favored for inference savings.</p>
<p>All 1B (target size) models have 3 full reruns with different seeds, while other model sizes have second and third seed runs that are terminated early after 25% of the target compute budget.We train the 1B reruns all the way to completion to allow our target "gold" predictions to account for run-to-run variance in evaluations due to weight initialization and data order.For instance, we find that the standard deviation between runs at the 1B 5×C scale can be as high as 2% points of accuracy for some recipes on most tasks.Meanwhile, at the non-target scales we wish to make predictions with a small fraction of the target compute, so we avoid reruns that would use an impractically large prediction budget.</p>
<p>Whether for extrapolating scaling laws or ranking single scale experiments, it is important to select reasonable hyperparameters for each scale to avoid confounding in performance differences that are simply due to suboptimal hyperparameters.We use OLMo's model ladder (Groeneveld et al., 2024;OLMo et al., 2025;Bhagia et al., 2024) to programmatically create LM pretraining configurations for a specified parameter size and token-parameter ratio to enable running a grid of model scaling experiments.The model ladder uses heuristics from the literature (Porian et al., 2024) to set global batch size and learning rate based on scaling factors.The hyperparameters that determine parameter count (layers, hidden dimension, number of heads, MLP dimension) were handpicked by OLMo developers for each scale to achieve the desired number of parameters.Appendix Table 2 details the configurations of all our models.</p>
<p>Prediction Methods</p>
<p>Broadly, there are two approaches in the literature to predicting large-scale performance based on small-scale experiments.We use straightforward implementations of each to assess where they succeed and fail at making decisions about which data recipes to use.</p>
<p>Ranking Single Scale Experiments (Single Scale) This simple approach is employed by work such as Li et al. (2024) and consists of running a set of ablations or experiments over data recipe options while holding constant all other modeling variables including scale.The winning data recipe by downstream accuracy (or proxies) at the small experimental scale is assumed to extrapolate to the target scale.</p>
<p>Extrapolating Scaling Laws (Multi Scale) Another approach to making decisions with predictions across scales used in works such as Dubey et al. (2024) is to fit scaling laws to multiple small experiments across a range of scales for each of the data recipes.The winning recipe is decided as the one whose scaling law shows the highest extrapolated performance at the target scale.Although scaling laws were first observed for language modeling loss (Kaplan et al., 2020;Hoffmann et al., 2022), they have been extended to predict downstream performance through a two-step approach that also fits a function from loss to downstream performance (Gadre et al., 2024;Bhagia et al., 2024).We follow a method from Bhagia et al. (2024).Their proposed approach incorporates separate parameters for number of model parameters and number of tokens trained to account for over or undertrained models.But as our suite only includes one token-parameter ratio, we use the simplified 3 parameter baseline, L(C), as a first step which we chain with second step, Acc(L), defined as follows where A, α, E, a, b, k, L 0 are optimized parameters:
L(C) = A C α + E (1) Acc(L) = a 1 + e −k(L−L0) + b (2)
Following Bhagia et al. (2024) we fit Equation 1 only on observations of final, fully trained checkpoints as accounting for the learning rate schedule's impact on intermediate checkpoints would require further parameters in the equation increasing the required number of observations and cost.To account for step-to-step noise in evaluation we average the last 10% of checkpoints as the final observed loss.Equation 2, however, is fit on all observations including intermediate checkpoints.We explore variations for a total of 8 multi scale approaches defined in Appendix C; none of these make for substantially better decisions than the method defined in this section.</p>
<p>Prediction Metrics</p>
<p>Our predictive task is to forecast which of a pair of data recipes will perform better at some target scale based on small-scale experiments.We use the following metrics to measure the quality of these predictions.</p>
<p>Prediction Error Scaling laws literature (Bhagia et al., 2024;Gadre et al., 2024) typically evaluates success from predicted and actual downstream performance, using relative error ( |predicted−actual| actual × 100%) or absolute error (|predicted − actual| × 100%).We call these absolute or relative "prediction error" to distinguish from the following metric.</p>
<p>Decision Accuracy Unlike previous work, we also measure the impact of predictions on decisions about which data recipe is better than another.The metric we use to capture this is decision accuracy, an accuracy over all pairs of data recipes A and B where either A or B is defined as the correct winner based on which achieves higher performance at the target scale.This is nearly equivalent to Kendall's τ , but ranges from 0 to 1.We define the target-scale winner based on mean downstream performance over 3 random seeds.Thus decision accuracy can be formalized as follows.Let P be the set of all data recipe pairs (A, B) with observed mean performance y A , y B and predicted performance ŷA , ŷB , respectively, then decision accuracy is:
1 |P| (A,B)∈P I sign(ŷ A − ŷB ) = sign(y A − y B ) (3)
Percent of Target Compute Budget (%C) We measure compute in terms of theoretical FLOPs following the simplifying assumption made in most scaling literature that the costs associated with training a model are captured well enough by FLOPs = 6N D, based solely on the number of parameters (N ) and tokens trained (D) (Kaplan et al., 2020).We consider the efficiency of a prediction based on the ratio of the experimental budget and the target budget in FLOPs, %C = c C × 100%.</p>
<p>Performance Evaluation with OLMES</p>
<p>We use the OLMES suite of 10 multiple choice question answering benchmarks (Gu et al., 2024): MMLU (Hendrycks et al., 2021), HellaSwag (Zellers et al., 2019), ARC Challenge (Clark et al., 2018), ARC Easy (Clark et al., 2018), PIQA (Bisk et al., 2020), CommonsenseQA (Talmor et al., 2019),SocialIQA (Sap et al., 2019), OpenBookQA (Mihaylov et al., 2018), BoolQ (Clark et al., 2019), and Wino-Grande (Sakaguchi et al., 2020).These tasks are well suited for the model scales we examine with all but BoolQ receiving non-trivial performance.Unless otherwise noted, we consider the macro average of these ten tasks.The underlying metric for each task is accuracy, for which OLMES specifies a different length normalization scheme per task.</p>
<p>Our target "gold" rankings which we aim to predict are always based on the "cloze" formulation (CF) accuracy with curated normalization per task, which we refer to as ACCU-RACY.We diverge from OLMES only in that we make use of all available items in the specified split of each benchmark rather than subsampling them, to reduce variance over the task distribution.</p>
<p>Note that while we focus just on OLMES multiple choice evaluations in this work, our method of validating decisions made through predictions can be applied to other benchmarks.We chose these tasks based on their appropriateness to our range of model scales, and one would have to select different tasks when targeting a larger scale.Moreover, DATADECIDE could be used to identify new evaluations that are sensitive within our range of scales.</p>
<p>Proxy Metrics for Performance Evaluation</p>
<p>Previous work has noted how discrete metrics such as accuracy can cause jumps in performance across scale that otherwise see more predictable improvements with scale for continuous metrics (Schaeffer et al., 2023).We experiment with using continuous metrics at small scale as proxies of the accuracies selected by OLMES for each task (ACCURACY) at the target scale to improve decision accuracy.We use the following metrics: CORRECT PROB is the average probabilities of the correct continuations.MARGIN is the average difference between the probability of the correct continuation and the most likely incorrect continuation.NORM CORRECT PROB is the average probability of the correct continuation conditioned on the response being in the set of correct or incorrect continuations.TOTAL PROB is the average of the sum of probabilities of all correct and incorrect continuations.ACCURACY is the fraction of instances where the correct continuation has the highest probability.</p>
<p>Each of these can be computed with likelihoods normalized by number of tokens or characters; unless otherwise specified we use character length normalization.Appendix Table 3 shows formal definitions.</p>
<p>Results</p>
<p>3.1.What is the best way to spend compute for data decisions?</p>
<p>More compute makes better decisions.Decisions from intermediate checkpoints are as good as compute equivalent final checkpoints.The amount of compute needed to make good predictions varies between tasks.ARC and MMLU are predictable with much less compute than HellaSwag.The rest of OLMES tasks give markedly less reliable predictions across the scales we examine.</p>
<p>First looking at the aggregation of all 10 OLMES tasks (Figure 1 right), we see that there is a positive and roughly log-linear relationship between experimental compute and decision accuracy.Specifically, this figure illustrates the relationship between the compute used for predicting best data recipes and the decision accuracy those predictions achieve against targets ranked by OLMES performance at the 1B scale.Each point represents the average decision accuracy over three runs with different random seeds, with shading indicating standard deviation.Points with the same color show all intermediate checkpoints from a given parameter size.The color shows each model size for predicting using ranking single scale experiments.The stars show predictions from extrapolating scaling laws using our default 3-parameter approach, the details of which are discussed further in §3.2.</p>
<p>The ease of prediction is greatly influenced by which evaluation benchmark we use.In Figure 2, we show the relationship of compute and decision accuracy for each of the tasks in OLMES individually.The predictive sensitivity of tasks at a given compute varies significantly, with ARC Easy being consistently predictable with 5 orders of magnitude less compute and BoolQ only reaching beyond trivial decision accuracy for intermediate checkpoints of the target runs.HellaSwag, SocialIQA, WinoGrande show distinct periods of insensitivity followed by roughly log-linear increase after hitting some compute threshold.</p>
<p>How does extrapolating scaling laws compare to ranking single scale experiments?</p>
<p>A selection of 8 baseline scaling law methods are no more efficient than ranking single scale experiments.Future scaling law methods can be assessed on DATADECIDE.
MMLU 10M 14M 150M 1B 20M 300M 4M 530M 60M 6M 750M 8M 90M 16M Multi-Scale Fit Figure 2
. Accuracy in pairwise decisions on best data when evaluating on the 10 OLMES tasks with ACCURACY (shown aggregated in Figure 1).Specific tasks have very distinct ranges of sensitivity, with some like ARC Easy being predictable at small scales and others like HellaSwag requiring substantially more compute to predict.cost of the model sizes used to make the prediction.We try the following combinations of models sizes: We use {{s 1 , . . ., s k } | 3 ≤ k ≤ 14}, where s is the ordered set of sizes, to explore the improvements of progressively adding larger model sizes beyond the minimum 3 required for fitting.We also use {{s k , . . ., s 14 } | 2 ≤ k ≤ 11} to try removing potentially noisy information from small models.Unlike single scale results, we make only one prediction attempt with the default fully trained random seed, as final checkpoints are required for fitting the first step of these scaling law variants but are not available for all seeds.</p>
<p>Our scaling law approaches vary in the number of parameters fit, using hard coded points to define the minimum and maximum performance, using only the second half of intermediate checkpoints for fitting the second step, or fitting a function directly from compute to accuracy in a single step.Each of the scaling law variants are defined formally in Appendix C. The 2 and 3 parameter variants all achieve among the top decision accuracy.</p>
<p>A priori we know that ranking single scale experiments cannot correctly predict when the scaling trend of one data recipe overtakes another at scales between our small experiments and target scale.Such crossovers bound the decision accuracy of this constant approximation of performance.Nevertheless ranking single scale experiments sets a high baseline decision accuracy, implying relatively little crossover occurs.It is difficult to distinguish evaluation variance from true crossovers, but the scaling trends we empirically observe cross over frequently.Improved future scaling laws may be able to advance the Pareto frontier on DATADECIDE as they are not bound by crossovers.</p>
<p>What proxy metrics give better signal for</p>
<p>predictions at small scale?</p>
<p>At small scales, continuous metrics using the character normalized likelihood of correct or all answer options serve as better or equivalent predictors of decisions than using the same ACCURACY as used at the target scale.</p>
<p>Figure 4 shows the decision accuracy over different proxy metrics.Here we chose a single length normalization, * PER CHAR.Metrics follow similar trends regardless of length normalization and this one is empirically optimal for most of the tasks that we observe.</p>
<p>Using CORRECT PROB or TOTAL PROB leads to decision accuracy at least as good as any other metric for most small scales.These continuous metrics are simple likelihoods over answer strings.In particular, TOTAL PROB may be interpretable as signal of a model having exposure to the domain of a given task in the form of higher likelihoods on incorrect but presumably relevant additional answers.</p>
<p>We notice two very distinct types of trends over the different tasks.Either the different proxy metrics are nearly indistinguishable and increase in decision accuracy with compute or CORRECT PROB and TOTAL PROB are flat with respect to scale and the other metrics only rise up to that level of decision accuracy towards the full target compute budget.</p>
<p>In the last order of magnitude below the target compute AC-CURACY and the other metrics tend to overtake CORRECT PROB and TOTAL PROB, while these two metrics sometimes even decrease in decision accuracy.Notably these other metrics that trend with ACCURACY include continuous metrics that penalize probability assigned to incorrect answers, NORM CORRECT PROB and MARGIN.</p>
<p>3.4.How can we make evaluation benchmarks more predictable?</p>
<p>The decision accuracy on a task is driven in part by low run-to-run variance and a wide spread of performance values for different data recipes.Using CORRECT PROB sees wider spreads or reduced noise for many tasks.Using this metric enables predicting rankings for code tasks that are too hard for accuracy metrics at small scales.</p>
<p>What underlies differences in decision accuracy when benchmarks and metrics change?The evaluation must separate pairs of data recipes by an amount greater than combined noise from run-to-run variance of each of the pair's runs.</p>
<p>In Figure 5, we plot tasks with a given metric using fully trained 150M models over these two characteristics: 1) noise-the standard deviation over 3 random seed runs averaged over all recipes, and 2) spread-the standard deviation among the mean performance of the different data recipes.Each point also shows the decision accuracy.We see that some highly predictable tasks (e.g., MMLU) are characterized by having low run-to-run noise, while others (e.g., ARC Easy) widely spread the different data recipes.We also see that improvements from using CORRECT PROB often align with improvements in one of these two characteristics.</p>
<p>As a practical application of these insights, we demonstrate that a change of proxy metric makes predictable two code tasks (Austin et al., 2021;Chen et al., 2021) that are otherwise too challenging for our small models.Figure 6 shows how decision accuracy goes from trivial to 80% when using CORRECT PROB.The switch of metric allows small models to get above the noise floor for these tasks, while still predicting large-scale accuracy metrics.Notably, two math benchmarks (Lewkowycz et al., 2022;Cobbe et al., 2021) do not see such a benefit.They do however give decision accuracy above 80% if we switch the target metric to CORRECT PROB, raising a question for future work to   .Code tasks such as humaneval and MBPP go from trivial decision accuracy to largely predictable when using using continuous CORRECT PROB instead of discrete ACCURACY.Meanwhile common math tasks remain near trivial decision accuracy regardless of metric.</p>
<p>explore whether changing the target metric can be justified.</p>
<p>Related Work</p>
<p>Prediction Much work studies scaling behavior in language models.Initially this focused on predicting LM loss from scale as determined by parameter count and tokens trained (Kaplan et al., 2020;Hoffmann et al., 2022).Special consideration is also given to the case of data constrained scaling (Muennighoff et al., 2023;Goyal et al., 2024).Unlike predicting loss, predicting downstream performance from scale is generally harder (Schaeffer et al., 2024).However, recent work has demonstrated it can be done based on a two step prediction that chains together predictions from scale to loss and loss to downstream performance (Gadre et al., 2024;Bhagia et al., 2024;Dubey et al., 2024), sometimes using training loss (Du et al., 2024) or transferring losses from different data recipes (Brandfonbrener et al., 2024;Ruan et al., 2024).The one line of work targeting pretraining data considers the special case of deciding mixing proportions of several data sources optimized through scaling laws (Kang et al., 2024;Ye et al., 2024).Most relevant to our work, Choshen et al. (2024) consider practical methods for better scaling prediction error such as how much compute to use or whether to include intermediate checkpoints.Orthogonally to these findings, we propose a way to assess the accuracy of decisions made with such predictions.</p>
<p>Suites over Data Differences DATADECIDE follows in the footsteps of the Pythia Suite (Biderman et al., 2023) which was the first to offer a controlled comparison of 2 data recipes, using compute scales up to 2 × 10 22 FLOPs.Subsequent suites have offered 6 data recipes at 9 × 10 20 scale (Magnusson et al., 2024) and 6 data recipes over a range of scales up to 10 21 (Brandfonbrener et al., 2024).</p>
<p>Our DATADECIDE offers a range of 14 scales up to 7 × 10 20 FLOPs, while including an order of magnitude more finegrained data differences.Meanwhile, DCLM also makes extensive use of ranking single scale experiments to drive improvement in data recipes (Li et al., 2024).They release their best data and a model trained on it, but do not release models from their decision making experiments and do not search over multiple recipes at their largest scale.Where their goal is creating a proposed best recipe, our DATADE-CIDE enables the assessment of whether a method for decision making really does find the best among proposed recipes.</p>
<p>Limitations</p>
<p>The scope of our work is limited to just one ratio of tokens to parameters, 100 or 5× "Chinchilla" optimal ratio (Hoffmann et al., 2022).We believe this captures the typical case, as most models now favor overtraining for inference savings.Due to compute limitations and the need for a standardized set of model configurations over a long period of time in which compute became available for pretraining, we opt for 14 specific configurations from 4M-1B parameter scale.While observations across more configurations would always be better, this must be traded off with exploring the other dimensions of data recipes and random seed reruns.Likewise, while our 25 data recipes is an order of magnitude more than previous suites, there is always the possibility that findings across these will not be representative of future data recipes.In our evaluations we focus on multiple choice tasks with a "cloze" formulation as we find these to be a good fit for our range of scales</p>
<p>A. Hyperparameters</p>
<p>Table 2 provides OLMo model ladder configurations for all models in DATADECIDE.</p>
<p>B. Proxy Metric Definitions</p>
<p>Table 3 provides formal definitions for our proxy metrics ( §2.5).</p>
<p>C. Scaling Law Variants</p>
<p>Baseline 3-parameter fit.Our default setup (described in §2.2) follows the two-step fit from (Bhagia et al., 2024) and uses Equation 1 to map compute C to task loss L, and Equation 2 to map task loss to metric score.This variant fits three parameters (A, α, E) in the first step.</p>
<p>2-parameter fit.This is a restricted version of the baseline where the irreducible loss term E is removed from Equation 1, leaving only two parameters: Single-step prediction.In this variant, the two-stage fitting procedure is replaced with a single step that directly maps compute C to accuracy:
L(C) = A C α(Acc(C) = a 1 + exp −k A C α + E − L 0 + b (6)
This combines the loss and accuracy mapping into one function.</p>
<p>5-parameter, single step.We also test a single-step variant that directly maps from (N, D) to accuracy using a logistic function over the predicted loss.This merges Equations 5 and 2 into:
Acc(N, D) = a 1 + exp − A N α + B D β + E + b (7)
This formulation retains the same five parameters from the two-step (N, D) loss function.(Groeneveld et al., 2024;OLMo et al., 2025;Bhagia et al., 2024) to programmatically create configurations for 14 model sizes with hyperparameters determined by heuristics in Porian et al. (2024).All models have sequence length of 2024 and MLP ratio of 8.Each configuration is pretrained over 25 data recipes (Table 1).Each recipe and configuration is also trained for 3 random seeds where model sizes &lt; 1B are stopped early at 25% of the compute used to train the 1B model for all but the default seed.Model size is number of non-embedding parameters.Batch size is the number of sequences per batch.</p>
<p>Metric Name Equation</p>
<p>CORRECT PROB   (i) is the set of possible continuations for item i and N is the number of items in a benchmark.Each each of the first 5 metrics have * per token and * per char variants in which likelihoods are normalized as defined in the bottom two rows.
1 N N i=1 P (c (i) correct | context i ) MARGIN 1 N N i=1 P (c (i) correct | context i ) − max c ′ ̸ =c (i) correct ∈C (i) P (c ′ | context i ) NORM CORRECT PROB 1
Relative Error Absolute Error Scaling Law Variant 3-parameter with helpers and &gt;50% checkpoints 5.6 2.6 3-parameter with helper points 6.0 2.8 3-parameter step 2 fit with &gt;50% checkpoints 5.9 2.9 3-parameter 6.5 3.1 2-parameter 6.5 3.2 5-parameter, single step 42.8 17.4 3-parameter, single step 42.9 42.3 5-parameter 230.8 65.4</p>
<p>Table 4. Average prediction error for 1B targets for the different scaling law setups across tasks and recipes on ACCURACY fit to all models but 1B.We see that other than the single step and 5-parameter variants errors are comparable, and these variants also roughly follow the compute-decision frontier in Figure 3.</p>
<p>Use of helper points.Following Bhagia et al. (2024), we optionally include an extra point (L = 0.0, Acc = 1.0) in the second-stage fit.This "helper" point anchors the upper asymptote of the accuracy prediction.</p>
<p>Filtering early checkpoints.We experiment with excluding the first 50% of intermediate checkpoints when fitting the second-stage sigmoid.This reduces noise from high-loss early training points and often improves the fit for extrapolation.</p>
<p>Helpers and &gt; 50% checkpoints.Lastly we experiment with combining the previous two techniques on the baseline 3-parameter fit.</p>
<p>Prediction Error.We report prediction errors in Table 4 for each setup.As the best scaling laws variants are all roughly comparable to the simple 3-parameter set up, we use this one as our baseline.</p>
<p>Figure 3
3
Figure 3 contrasts different approaches to fitting scaling laws over multiple scales of small experiments.Each of the 8 approaches is shown in a different color.Multi-scale predictions have a compute budget equal to the training</p>
<p>Figure 3 .
3
Figure 3. Decision accuracy over 8 baseline scaling law variants.At best, these approaches reach only the same compute to decision accuracy frontier as ranking single scale experiments.DATADECIDE can be used to iterate on future scaling law prediction methods.</p>
<p>Figure 4 .
4
Figure4.Per-task decision accuracy using character normalized proxy metrics for ACCURACY targets.5 tasks benefit at smaller scales from using raw likelihood of answers (CORRECT PROB and TOTAL PROB), as opposed to discrete ACCURACY or continuous metrics that penalize probability on incorrect answers (NORM CORRECT PROB, MARGIN).</p>
<p>Figure 5 .
5
Figure5.Why do some tasks or metrics get better or worse decision accuracy?At 150M with CORRECT PROB tasks like HellaSwag succeed with low run-to-run variance and tasks like SocialIQA widely spread the performance assigned to different pretraining data.</p>
<p>4) 5-parameter (N, D) fit.Instead of modeling loss as a function of compute C, this variant uses both number of tokens N and number of parameters D directly in the loss function: parameters: A, α, B, β, and E.</p>
<p>max c∈C (i) P (c | context i ) = c (i) correct * per token log(P (c|context)) /tokens(c) * per char log(P (c|context)) /chars(c)</p>
<p>Table 1 .
1
DATADECIDE enables the study of data differences over scales through controlled pretraining experiments on 25 data recipes.These take different source datasets and apply interventions from ablating domains, deduplication, mixing, to quality filtering with different classifiers and thresholds.We release all pretraining corpora, as well as models trained on each recipe and each of the 14 model configurations in Table2with 3 random seeds.
Original, QCA SOTA Common Crawl corpus using best ablated deduplication, cleaning heuristics, and quality7% FW2, QC 7% FW3, QC FWfilter. We quality filter to top 7% of DCLM classified documents and further take 2+ or 3+ scores3%, QC FW 10%, QC 10%, QCwith FineWeb-edu classifier; or filter to top 3% or 10% with FineWeb-edu classifier; or take top20%10% or 20% with reproduced DCLM classifier.λ% DCLM-Baseline + 1 − λ%Fractional combinations of Dolma1.7 and DCLM-Baseline mixing different proportions of theDolma1.7two datasets for λ ∈ {25%, 50%, 75%}.</p>
<p>. Using DATADECIDE, new evaluations can be assessed easily by others without any additional pretraining.Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. HellaSwag: Can a machine really finish your sentence?In Korhonen, A., Traum, D., and Màrquez, L. (eds.),ACL, pp.4791-4800, Florence, Italy, July 2019.doi: 10.18653/v1/P19-1472.
Zhou, F., Wang, Z., Liu, Q., Li, J., and Liu, P. Program-ming every example: Lifting pre-training data quality likeexperts at scale. arXiv preprint arXiv:2409.17115, 2024.</p>
<p>Table 2 .
2
DATADECIDE uses OLMo's model ladder</p>
<p>Table 3 .
3
Proxy metrics used as alternative inputs to our prediction methods, C</p>
<p>DataDecide collection on HuggingFace
AcknowledgmentsWe would like to thank Dave Wadden, Kyle Lo, Valentin Hofmann, and Hannaneh Hajishirzi for fruitful conversations.This material is based upon work supported by the U.S. National Science Foundation under Grant No. 2313998.Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the U.S. National Science Foundation.IM is supported by the NSF CSGrad4US Fellowship.PWK is supported by the Singapore National Research Foundation and the National AI Group in the Singapore Ministry of Digital Development and Information under the AI Visiting Professorship Programme (award number AIVP-2024-001) and by the AI2050 program at Schmidt Sciences.
Program synthesis with large language models. J Austin, A Odena, M Nye, M Bosma, H Michalewski, D Dohan, E Jiang, C Cai, M Terry, Q Le, arXiv:2108.077322021arXiv preprint</p>
<p>Smollm-corpus. L Ben Allal, A Lozhkov, G Penedo, T Wolf, L Werra, July 2024</p>
<p>A Bhagia, J Liu, A Wettig, D Heineman, O Tafjord, A H Jha, L Soldaini, N A Smith, D Groeneveld, P W Koh, J Dodge, H Hajishirzi, Establishing task scaling laws via compute-efficient model ladders. 2024</p>
<p>S Biderman, H Schoelkopf, Q Anthony, H Bradley, K O'brien, E Hallahan, M A Khan, S Purohit, U S Prashanth, E Raff, A Skowron, L Sutawika, A suite for analyzing large language models across training and scaling. 2023</p>
<p>PIQA: Reasoning about physical commonsense in natural language. Y Bisk, R Zellers, R Le Bras, J Gao, Y Choi, 10.1609/aaai.v34i05.6239Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceApr. 202034</p>
<p>Loss-to-loss prediction: Scaling laws for all datasets. D Brandfonbrener, N Anand, N Vyas, E Malach, S Kakade, 2024</p>
<p>. M Chen, J Tworek, H Jun, Q Yuan, H P De Oliveira Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, A Ray, R Puri, G Krueger, M Petrov, H Khlaaf, G Sastry, P Mishkin, B Chan, S Gray, N Ryder, M Pavlov, A Power, L Kaiser, M Bavarian, C Winter, P Tillet, F P Such, D Cummings, M Plappert, F Chantzis, E Barnes, A Herbert-Voss, W H Guss, A Nichol, A Paino, N Tezak, J Tang, I Babuschkin, S Balaji, S Jain, W Saunders, C Hesse, A N Carr, J Achiam, J Misra, V Morikawa, E Radford, A Knight, M Brundage, M Murati, M Mayer, K Welinder, P Mc-Grew, B Amodei, D Mccandlish, S Sutskever, I Zaremba, W , 2021Evaluating large language models trained on code</p>
<p>A hitchhiker's guide to scaling law estimation. L Choshen, Y Zhang, Andreas , J , 2024</p>
<p>Exploring the surprising difficulty of natural yes/no questions. C Clark, K Lee, M.-W Chang, T Kwiatkowski, M Collins, K Toutanova, Boolq, 10.18653/v1/N19-1300NAACL. J Burstein, C Doran, T Solorio, Minneapolis, MinnesotaJune 2019</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. P Clark, I Cowhey, O Etzioni, T Khot, A Sabharwal, C Schoenick, O Tafjord, ArXiv. 2018</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, C Hesse, J Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Common Crawl. Common crawl. </p>
<p>Understanding emergent abilities of language models from the loss perspective. Z Du, A Zeng, Y Dong, J Tang, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>. A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Yang, A Fan, A Goyal, A S Hartshorn, A Yang, A Mitra, A Sravankumar, A Korenev, A Hinsvark, A Rao, A Zhang, A Rodriguez, A Gregerson, A Spataru, B Rozière, B Biron, B Tang, B Chern, C Caucheteux, C Nayak, C Bi, C Marra, C Mcconnell, C Keller, C Touret, C Wu, C Wong, C C Ferrer, C Nikolaidis, D Allonsius, D Song, D Pintz, D Livshits, D Esiobu, D Choudhary, D Mahajan, D Garcia-Olano, D Perino, D Hupkes, E Lakomkin, E A Albadawy, E Lobanova, E Dinan, E M Smith, F Radenovic, F Zhang, G Synnaeve, G Lee, G L Anderson, G Nail, G Mialon, G Pang, G Cucurell, H Nguyen, H Korevaar, H Xu, H Touvron, I Zarov, I A Ibarra, I M Kloumann, I Misra, I Evtimov, J Copet, J Lee, J L Geffert, J Vranes, J Park, J Mahadeokar, J Shah, J Van Der Linde, J Billock, J Hong, J Lee, J Fu, J Chi, J Huang, J Liu, J Wang, J Yu, J Bitton, J Spisak, J Park, J Rocca, J Johnstun, J Saxe, J.-Q Jia, K V Alwala, K Upasani, K Plawiak, K Li, K - Neth Heafield, K Stone, K El-Arini, K Iyer, K Malik, K Chiu, K Bhalla, L Rantala-Yeary, L Van Der Maaten, L Chen, L Tan, L Jenkins, L Martin, L Madaan, L Malo, L Blecher, L Landzaat, L De Oliveira, M Muzzi, M B Pasupuleti, M Singh, M Paluri, M Kardas, M Oldham, M Rita, M Pavlova, M H M Kambadur, M Lewis, M Si, M K Singh, M Hassan, N Goyal, N Torabi, N Bashlykov, N Bogoychev, N S Chatterji, O Duchenne, O Ccelebi, P Alrassy, P Zhang, P Li, P Vasić, P Weng, P Bhargava, P Dubal, P Krishnan, P S Koura, P Xu, Q He, Q Dong, R Srinivasan, R Ganapathy, R Calderer, R S Cabral, R Stojnic, R Raileanu, R Girdhar, R Patel, R Sauvestre, R Polidoro, R Sumbaly, R Taylor, R Silva, R Hou, R Wang, S Hosseini, S Chennabasappa, S Singh, S Bell, S S Kim, S Edunov, S Nie, S Narang, S C Raparthy, S Shen, S Wan, S Bhosale, S Zhang, S Vandenhende, S Batra, S Whitman, S Sootla, S Collot, S Gururangan, S Borodinsky, T Herman, T Fowler, T Sheasha, T Georgiou, T Scialom, T Speckbacher, T Mihaylov, T Xiao, U Karn, V Goswami, V Gupta, V Ramanathan, V Kerkez, V Gonguet, V Do, V Vogeti, V Petrovic, W Chu, W Xiong, W Fu, W Ney Meers, X Martinet, X Wang, X E Tan, X Xie, X Jia, X Wang, Y Goldschlag, Y Gaur, Y Babaei, Y Wen, Y Song, Y Zhang, Y Li, Y Mao, Z D Coudert, Z Yan, Z Chen, Z Papakipos, A K Singh, A Grattafiori, A Jain, A Kelsey, A Shajnfeld, A Gangidi, A Victoria, A Goldstand, A Menon, A Sharma, A Boesenberg, A Vaughan, A Baevski, A Feinstein, A Kallet, A Sangani, A Yunus, A Lupu, A Alvarado, A Caples, A Gu, A Ho, A Poulton, A Ryan, A Ramchandani, A Franco, A Saraf, A Chowdhury, A Gabriel, A Bharambe, A Eisenman, A Yazdan, B James, B Maurer, B Leonhardi, P.-Y B Huang, B Loyd, B D Paola, B Paranjape, B Liu, B Wu, B Ni, B Hancock, B Wasti, B Spence, B Stojkovic, B Gamido, B Montalvo, C Parker, C Burton, C Mejia, C Wang, C Kim, C Zhou, C Hu, C.-H Chu, C Cai, C Tindal, C Feichtenhofer, D Civin, D Beaty, D Kreymer, S.-W Li, D Wyatt, D Adkins, D Xu, D Testuggine, D David, D Parikh, D Liskovich, D Foss, D Wang, D Le, D Holland, E Dowling, E Jamil, E Montgomery, E Presani, E Hahn, E Wood, E Brinkman, E Arcaute, E Dunbar, E Smothers, F Sun, F Kreuk, F Tian, F Ozgenel, F Caggioni, F Guzm'an, F J Kanayet, F Seide, G M Florez, G Schwarz, G Badeer, G Swee, G Halpern, G Thattai, G Herman, G G Sizov, G Zhang, G Lakshminarayanan, H Shojanazeri, H Zou, H Wang, H Zha, H Habeeb, H Rudolph, H Suk, H Aspegren, H Goldman, I Molybog, I Tufanov, I.-E Veliche, I Gat, J Weissman, J Geboski, J Kohli, J Asher, J.-B Gaya, J Marcus, J Tang, J Chan, J Zhen, J Reizenstein, J Teboul, J Zhong, J Jin, J Yang, J Cummings, J Carvill, J Shepard, J Mcphie, J Torres, J Ginsburg, J Wang, K Wu, U Kamhou, K Saxena, K Prasad, K Khandelwal, K Zand, K Matosich, K Veeraraghavan, K Michelena, K Li, K Huang, K Chawla, K Lakhotia, K Huang, L Chen, L Garg, A Lavender, L Silva, L Bell, L Zhang, L Guo, L Yu, L Moshkovich, L Wehrstedt, M Khabsa, M Avalani, M Bhatt, M Tsimpoukelli, M Mankus, M Hasson, M Lennie, M Reso, M Groshev, M Naumov, M Lathi, M Keneally, M L Seltzer, M Valko, M Restrepo, M Patel, M Vyatskov, M Samvelyan, M Clark, M Macey, M Wang, M J Hermoso, M Metanat, M Rastegari, M ; P Bansal, N Dong, N Zhang, N Cheng, O Chernoguz, O Hart, O Salpekar, O Kalinli, P Kent, P Parekh, P Saab, P Balaji, P Rittner, P Bontrager, P Roux, P Dollár, P Zvyagina, P Ratanchandani, P Yuvraj, Q Liang, R Alao, R Rodriguez, R Ayub, R Murthy, R Nayani, R Mitra, R Li, R Hogan, R Battey, R Wang, R Maheswari, R Howes, R Rinott, S J Bondu, S Datta, S Chugh, S Hunt, S Dhillon, S Sidorov, S Pan, S Verma, S Yamamoto, S Ramaswamy, S Lindsay, S Feng, S Lin, S C Zha, S Shankar, S Zhang, S Wang, S Agarwal, S Sajuyigbe, S Chintala, S Max, S Chen, S Kehoe, S Satterfield, S Govindaprasad, S Gupta, S.-B Cho, S Virk, S Subramanian, S Choudhury, S Goldman, T Remez, T Glaser, T Best, T Kohler, T Robinson, T Li, T Zhang, T Matthews, T Chou, T Shaked, V Vontimitta, V Ajayi, V Montanez, V Mohan, V S Kumar, V Mangla, V Ionescu, V A Poenaru, V T Mihailescu, V Ivanov, W Li, W Wang, W Jiang, W Bouaziz, W Constable, X Tang, X Wang, X Wu, X Wang, X Xia, X Wu, X Gao, Y Chen, Y Hu, Y Jia, Y Qi, Y Li, Y Zhang, Y Zhang, Y Adi, Y Nam, Y Wang, Y Hao, Y Qian, Y He, Z Rait, Z De-Vito, Z Rosnbrick, Z Wen, Z Yang, Zhao, ArXiv, abs/2407.217832024Santhanam, N., Parks, N., White, N., Bawa, N., Singhal, N., Egebo, N., Usunier, N., Laptev, N</p>
<p>Language models scale reliably with over-training and on downstream tasks. S Y Gadre, G Smyrnis, V Shankar, S Gururangan, M Wortsman, R Shao, J Mercat, A Fang, J Li, S Keh, R Xin, M Nezhurina, I Vasiljevic, J Jitsev, L Soldaini, A G Dimakis, G Ilharco, P W Koh, S Song, T Kollar, Y Carmon, A Dave, R Heckel, N Muennighoff, L Schmidt, 2024</p>
<p>Scaling laws for data filtering -data curation cannot be compute agnostic. S Goyal, P Maini, Z C Lipton, A Raghunathan, J Z Kolter, 10.48550/arXiv.2404.071772024</p>
<p>D Groeneveld, I Beltagy, P Walsh, A Bhagia, R Kinney, O Tafjord, A H Jha, H Ivison, I Magnusson, Y Wang, S Arora, D Atkinson, R Authur, K R Chandu, A Cohan, J Dumas, Y Elazar, Y Gu, J Hessel, T Khot, W Merrill, J Morrison, N Muennighoff, A Naik, C Nam, M E Peters, V Pyatkin, A Ravichander, D Schwenk, S Shah, W Smith, E Strubell, N Subramani, M Wortsman, P Dasigi, N Lambert, K Richardson, L Zettlemoyer, J Dodge, K Lo, L Soldaini, N A Smith, H Hajishirzi, Olmo, Accelerating the science of language models. 2024</p>
<p>Y Gu, O Tafjord, B Kuehl, D Haddad, J Dodge, H Hajishirzi, Olmes, A standard for language model evaluations. 2024</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>Training compute-optimal large language models. J Hoffmann, S Borgeaud, A Mensch, E Buchatskaya, T Cai, E Rutherford, D De Las Casas, L A Hendricks, J Welbl, A Clark, T Hennigan, E Noland, K Millican, G Van Den Driessche, B Damoc, A Guy, S Osindero, K Simonyan, E Elsen, J W Rae, O Vinyals, L Sifre, 2022</p>
<p>Autoscale: Automatic prediction of compute-optimal data composition for training llms. F Kang, Y Sun, B Wen, S Chen, D Song, R Mahmood, R Jia, ArXiv, abs/2407.201772024</p>
<p>Scaling laws for neural language models. J Kaplan, S Mccandlish, T Henighan, T B Brown, B Chess, R Child, S Gray, A Radford, J Wu, D Amodei, 2020</p>
<p>. N Lambert, J Morrison, V Pyatkin, S Huang, H Ivison, F Brahman, L J V Miranda, A Liu, N Dziri, S Lyu, Y Gu, S Malik, V Graf, J D Hwang, J Yang, R L Bras, O Tafjord, C Wilhelm, L Soldaini, N A Smith, Y Wang, P Dasigi, H Hajishirzi, 20243Pushing frontiers in open language model post-training</p>
<p>Solving quantitative reasoning problems with language models. A Lewkowycz, A Andreassen, D Dohan, E Dyer, H Michalewski, V Ramasesh, A Slone, C Anil, I Schlag, T Gutman-Solo, Y Wu, B Neyshabur, G Gur-Ari, V Misra, 2022</p>
<p>. J Li, A Fang, G Smyrnis, M Ivgi, M Jordan, S Gadre, H Bansal, E Guha, S Keh, K Arora, S Garg, R Xin, N Muennighoff, R Heckel, J Mercat, M Chen, S Gururangan, M Wortsman, A Albalak, Y Bitton, M Nezhurina, A Abbas, C.-Y Hsieh, D Ghosh, J Gardner, M Kilian, H Zhang, R Shao, S Pratt, S Sanyal, G Ilharco, G Daras, K Marathe, A Gokaslan, J Zhang, K Chandu, T Nguyen, I Vasiljevic, S Kakade, S Song, S Sanghavi, F Faghri, S Oh, L Zettlemoyer, K Lo, A El-Nouby, H Pouransari, A Toshev, S Wang, D Groeneveld, L Soldaini, P W Koh, J Jitsev, T Kollar, A G Dimakis, Y Carmon, A Dave, L Schmidt, V Shankar, Datacomplm, 2024In search of the next generation of training sets for language models</p>
<p>I Magnusson, A Bhagia, V Hofmann, L Soldaini, A H Jha, O Tafjord, D Schwenk, E P Walsh, Y Elazar, K Lo, D Groeneveld, I Beltagy, H Hajishirzi, N A Smith, K Richardson, J Dodge, Paloma, A benchmark for evaluating language model fit. 2024</p>
<p>Can a suit of armor conduct electricity? a new dataset for open book question answering. T Mihaylov, P Clark, T Khot, A Sabharwal, 10.18653/v1/D18-1260EMNLP. E Riloff, D Chiang, J Hockenmaier, J Tsujii, Brussels, BelgiumOctober-November 2018</p>
<p>Scaling data-constrained language models. N Muennighoff, A Rush, B Barak, T Le Scao, N Tazi, A Piktus, S Pyysalo, T Wolf, C A Raffel, T Olmo, P Walsh, L Soldaini, D Groeneveld, K Lo, S Arora, A Bhagia, Y Gu, S Huang, M Jordan, N Lambert, D Schwenk, O Tafjord, T Anderson, D Atkinson, F Brahman, C Clark, P Dasigi, N Dziri, M Guerquin, H Ivison, P W Koh, J Liu, S Malik, W Merrill, L J V Miranda, J Morrison, T Murray, C Nam, V Pyatkin, A Rangapur, M Schmitz, S Skjonsberg, D Wadden, C Wilhelm, M Wilson, L Zettlemoyer, A Farhadi, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc2023. 202536Smith, N. A., and Hajishirzi, H. 2 olmo 2 furious</p>
<p>The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only. G Penedo, Q Malartic, D Hesslow, R.-A Cojocaru, A Cappelli, H Alobeidli, B Pannier, E Almazrouei, J Launay, ArXiv, abs/2306.011162023259063761</p>
<p>The fineweb datasets: Decanting the web for the finest text data at scale. G Penedo, H Kydlíček, L B Lozhkov, A Mitchell, M Raffel, C Werra, L V Wolf, T , 2024</p>
<p>Resolving discrepancies in compute-optimal scaling of language models. T Porian, M Wortsman, J Jitsev, L Schmidt, Carmon , Y , ArXiv, abs/2406.191462024</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, 2019arXiv e-prints</p>
<p>Observational scaling laws and the predictability of langauge model performance. Y Ruan, C J Maddison, T Hashimoto, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>An adversarial winograd schema challenge at scale. K Sakaguchi, R Le Bras, C Bhagavatula, Y Choi, Winogrande, 10.1609/aaai.v34i05.6399Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceApr. 202034</p>
<p>Commonsense reasoning about social interactions. M Sap, H Rashkin, D Chen, R Le Bras, Y Choi, Iqa, 10.18653/v1/D19-1454EMNLP. K Inui, J Jiang, V Ng, Wan , X , Hong Kong, ChinaNovember 2019</p>
<p>Are emergent abilities of large language models a mirage?. R Schaeffer, B Miranda, S Koyejo, 2023</p>
<p>Why has predicting downstream capabilities of frontier AI models with scale remained elusive?. R Schaeffer, H Schoelkopf, B Miranda, G Mukobi, V Madan, A Ibrahim, H Bradley, S Biderman, S Koyejo, Trustworthy Multi-modal Foundation Models and AI Agents (TiFA). 2024</p>
<p>L Soldaini, R Kinney, A Bhagia, D Schwenk, D Atkinson, R Authur, B Bogin, K Chandu, J Dumas, Y Elazar, V Hofmann, A H Jha, S Kumar, L Lucy, X Lyu, N Lambert, I Magnusson, J Morrison, N Muennighoff, A Naik, C Nam, M E Peters, A Ravichander, K Richardson, Z Shen, E Strubell, N Subramani, O Tafjord, P Walsh, L Zettlemoyer, N A Smith, H Hajishirzi, I Beltagy, D Groeneveld, J Dodge, K Lo, Dolma, an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research. 2024arXiv preprint</p>
<p>Com-monsenseQA: A question answering challenge targeting commonsense knowledge. A Talmor, J Herzig, N Lourie, J Berant, 10.18653/v1/N19-1421NAACL. J Burstein, C Doran, T Solorio, Minneapolis, MinnesotaJune 2019</p>
<p>Data mixing laws: Optimizing data mixtures by predicting language modeling performance. J Ye, P Liu, T Sun, Y Zhou, J Zhan, X Qiu, ArXiv, abs/2403.169522024</p>            </div>
        </div>

    </div>
</body>
</html>
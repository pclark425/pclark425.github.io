<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1149 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1149</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1149</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-272524562</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.04744v3.pdf" target="_blank">Reward Guidance for Reinforcement Learning Tasks Based on Large Language Models: The LMGT Framework</a></p>
                <p><strong>Paper Abstract:</strong> The inherent uncertainty in the environmental transition model of Reinforcement Learning (RL) necessitates a delicate balance between exploration and exploitation. This balance is crucial for optimizing computational resources to accurately estimate expected rewards for the agent. In scenarios with sparse rewards, such as robotic control systems, achieving this balance is particularly challenging. However, given that many environments possess extensive prior knowledge, learning from the ground up in such contexts may be redundant. To address this issue, we propose Language Model Guided reward Tuning (LMGT), a novel, sample-efficient framework. LMGT leverages the comprehensive prior knowledge embedded in Large Language Models (LLMs) and their proficiency in processing non-standard data forms, such as wiki tutorials. By utilizing LLM-guided reward shifts, LMGT adeptly balances exploration and exploitation, thereby guiding the agent's exploratory behavior and enhancing sample efficiency. We have rigorously evaluated LMGT across various RL tasks and evaluated it in the embodied robotic environment Housekeep. Our results demonstrate that LMGT consistently outperforms baseline methods. Furthermore, the findings suggest that our framework can substantially reduce the computational resources required during the RL training phase.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1149.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1149.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LMGT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Model Guided reward Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that uses large language models (LLMs) to generate reward shifts (positive/zero/negative) for state-action pairs during training, thereby adaptively steering exploration and exploitation via reward shaping recorded in the agent's replay buffer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LMGT (framework)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Not a single network architecture but a training-time wrapper: an RL agent (any off-policy or on-policy learner) interacts with the environment; for each observed state and chosen action an LLM (e.g., Vicuna-30B quantized) scores the behavior and returns a reward shift (+1/0/-1 or finer), which is added to the environment reward; experience with shifted rewards is stored and used to update the agent (TD learning, policy gradients, etc.). Visual Instruction Tuning (LLaVA) is used upstream to handle image inputs when needed.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>LLM-guided adaptive reward shaping / adaptive exploration via reward shifting (a form of active, prior-knowledge-driven reward shaping)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each interaction the LLM evaluates the state-action pair using prior knowledge supplied in prompts (or embedded in model weights) and returns a reward shift: positive for actions deemed valuable, negative for 'valueless' actions, zero for unknown; the agent intensifies exploration around LLM-approved actions and avoids LLM-disapproved directions. Reward shifts effectively re-initialize/modify Q-values and therefore dynamically alter action-selection probabilities over training.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Various (Atari Pitfall, Montezuma's Revenge; watch repair pocket-watch task; Blackjack (box/human formats); CartPole, Pendulum; Housekeep embodied task; RecSim Choc-vs-Kale recommendation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown/stochastic transition dynamics, often sparse rewards (Atari), partially observable/high-dimensional visual observations (Housekeep, Blackjack human format), delayed credit assignment (watch repair), latent/partially observable user state (RecSim), discrete and continuous action spaces depending on base RL algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Varies by experiment: Atari long-horizon sparse-reward games (large state spaces, long episode horizons), watch repair task with delayed final reward, Housekeep subset of 4 scenes (each: 1 room × 5 misplaced objects), RecSim episodes of sequential recommendation (slates), standard control tasks (CartPole, Pendulum) of small state/action sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Reported consistent improvements across tasks; (examples) Watch repair: LMGT+TD required 417 episodes and 114 s training time to reach >90% profitable decisions; Atari (after 3.5×10^10 frames) Pitfall: LMGT+R2D2 score 6503.5 vs NGU+R2D2 5973.4 and R2D2 baseline 3613.5; Montezuma's Revenge: LMGT+R2D2 12365.5 vs NGU+R2D2 9049.4 and R2D2 2687.2. In RecSim SlateQ experiments LMGT produced average-reward gains (performance gains reported +102.542, +211.643, +23.115 across evaluated episode counts). In Housekeep LMGT outperformed APT and matched/surpassed ELLM with learned descriptors (no exact numeric success rates provided in text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baselines: RUDDER required 2029 episodes and 171 s in the watch repair task; NGU+R2D2 and plain R2D2 reported substantially lower scores in Atari (e.g., R2D2 3613.5 Pitfall, 2687.2 Montezuma's Revenge at 3.5×10^10 frames). For RecSim SlateQ baseline A.R. values lower than LMGT (exact baseline A.R. table entries present but partially garbled in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>LMGT reduces required environment interactions in several tasks: example watch repair ~417 episodes vs 2029 for RUDDER (≈79% reduction in episodes); Atari early-training gains substantial (at 0.5×10^10 frames Pitfall LMGT 1535.5 vs NGU 786.9), indicating faster early discovery of valuable regions. Exact sample counts vary per environment/algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Balanced by LLM-generated reward shifts: positive shifts increase exploitation around prior-knowledge-endorsed actions while the agent still follows conventional exploration policies (e.g., ϵ-greedy, policy entropy) and intensifies local exploration near LLM-preferred actions; negative shifts discourage exploration of LLM-flagged low-value actions. Authors note equivalence of reward shifts to Q-function initialization (affecting optimism/pessimism).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>RUDDER (reward redistribution), NGU (intrinsic curiosity/exploration), standard baselines (R2D2, TD, MC, DQN, PPO, A2C, SAC, TD3, PPO for various tasks), APT (Active Pre-Training) in Housekeep, ELLM (visual-instruction baseline), SlateQ in RecSim.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>LLM-guided reward shifting (LMGT) substantially improves sample efficiency and final performance in sparse/delayed-reward and complex tasks by injecting prior knowledge: (1) accelerates credit assignment in delayed-reward tasks (watch repair), (2) improves exploration efficiency in hard-exploration Atari games, achieving higher early-stage and late-stage scores than NGU and baselines, (3) scales to embodied Housekeep tasks and industrial recommendation (SlateQ) with measurable gains; framework is flexible across RL algorithms and leverages prompting strategies (CoT, Zero-shot) and multimodal pipelines (LLaVA → Vicuna).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performance depends on LLM capability and prompt design; multimodal workloads (image understanding + scoring) can degrade LLM effectiveness (Blackjack 'human' format: LMGT performance near or below baseline). Computational overhead from LLM inference during training is nontrivial and not fully quantified. If LLM prior knowledge is incorrect or absent for a domain, guidance can be misleading. Quantization and model size trade-offs affect inferential capability (smaller/poorly quantized LLMs underperform).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reward Guidance for Reinforcement Learning Tasks Based on Large Language Models: The LMGT Framework', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1149.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1149.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LMGT+TD (watch repair)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LMGT applied with Temporal-Difference learning on pocket-watch repair delayed-reward task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LMGT integrated with TD learning where an LLM redistributes reward via scoring of actions to address delayed credit assignment in a pocket-watch repair profitability task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LMGT + TD</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Off-policy TD learner (temporal-difference core) whose recorded rewards are shifted by an LLM evaluator per state-action; training proceeds with standard TD updates using experiences containing LLM-added reward shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>LLM-guided reward redistribution (adaptive reward shaping) to improve credit assignment</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>LLM evaluates actions and assigns reward shifts that reassign delayed terminal rewards to preceding decisions, enabling more direct credit signals for TD updates; this adaptively changes which earlier actions receive learning signal.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Pocket-watch repair (delayed-reward profitability decision task)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Delayed/delayed-credit-assignment, stochastic repair costs, unknown profitability until costs realized (delayed reward), discrete decision sequence environment.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Finite MDP with delayed final reward; complexity arises from multi-step decision sequences and uncertain costs; episode lengths and state/action counts not precisely enumerated.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Reached >90% profitable decision rate in 417 training episodes and 114 seconds (including LLM inference time).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>RUDDER required 2029 episodes and 171 seconds to reach the same threshold (baseline TD/MC reported but table entries in the paper are corrupted and unclear).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Approximate 79.4% reduction in number of episodes needed compared to RUDDER (2029→417 episodes); computational time reduced by ≈33.3% (171s→114s).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>LLM shifts reward to highlight key decision points (positive shifts for valuable earlier actions), thereby biasing TD updates toward promising actions and reducing wasted exploration in low-value directions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>RUDDER (return decomposition), TD and Monte Carlo baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>LLM-based reward shifts substantially speed up learning in delayed-reward tasks by producing more immediate learning signals for prior important decisions, outperforming RUDDER in episodes-to-threshold and wall-clock training time.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Results rely on LLM providing reliable redistribution signals; computational overhead of LLM inference included but not fully optimized; generalization to larger, continuous delayed-reward tasks not fully tested.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reward Guidance for Reinforcement Learning Tasks Based on Large Language Models: The LMGT Framework', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1149.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1149.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LMGT+R2D2 (Atari)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LMGT applied to R2D2 (Recurrent Replay Distributed DQN) on hard-exploration Atari games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Integration of LMGT reward-shifting with R2D2 to improve exploration in sparse-reward Atari games like Pitfall and Montezuma's Revenge, using LLM evaluations to bias exploration toward promising trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LMGT + R2D2</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>R2D2 base agent (recurrent DQN with distributed replay) which records environment rewards shifted by an LLM scorer; LLM guidance biases replay and action selection during learning.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Prior-knowledge-driven reward shaping via LLM to direct exploration (complementary to intrinsic-motivation methods)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>LLM assigns positive shifts to state-action pairs consistent with prior knowledge of valuable trajectories; these shifts alter replayed transitions' TD targets and thus prioritization of learning, leading to more sampling of high-value regions and local exploration near recommended actions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Atari 2600: Pitfall and Montezuma's Revenge</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Extremely sparse rewards, large discrete state space (pixels), long-horizon episodes, stochasticity in transitions from player/environment interactions, partially observable from single frames but handled via R2D2 recurrence.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>High; long-horizon exploration required, standard Atari frame-based observations (high-dimensional pixel inputs), training measured in frames up to 3.5×10^10 frames.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>After 3.5×10^10 frames: Pitfall average reward 6503.5 vs NGU+R2D2 5973.4 and R2D2 baseline 3613.5; Montezuma's Revenge average reward 12365.5 vs NGU+R2D2 9049.4 and R2D2 baseline 2687.2. Early training advantage: at 0.5×10^10 frames Pitfall LMGT 1535.5 vs NGU 786.9; Montezuma LMGT 5635.4 vs NGU 4818.5.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baseline R2D2 much lower (e.g., Pitfall 3613.5, Montezuma 2687.2 at 3.5×10^10 frames); NGU improves over baseline but LMGT surpasses NGU in both early and late stages.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>LMGT shows markedly better early-sample efficiency (roughly 2× score in Pitfall at 0.5×10^10 frames compared to NGU), enabling faster discovery of rewarding regions.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>LLM reward shifts push R2D2 to exploit trajectories endorsed by prior knowledge while maintaining recurrence-driven temporal credit assignment and standard exploration mechanisms; exploration becomes more focused (local exploration near LLM-approved actions).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>NGU+R2D2 (intrinsic motivation), plain R2D2 baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>LLM-guided reward shaping substantially improves both early-stage and asymptotic performance in notoriously hard-exploration Atari games, outperforming intrinsic-motivation NGU and standard R2D2.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Dependence on LLM quality and prompt design; computational cost of LLM inference at Atari training scale not fully characterized.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reward Guidance for Reinforcement Learning Tasks Based on Large Language Models: The LMGT Framework', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1149.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1149.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LMGT (Housekeep)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LMGT applied to Housekeep embodied robotic environment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LMGT deployed in an embodied simulation where a robot must place misplaced household objects into correct containers, using Visual Instruction Tuning to feed image-derived embeddings to the LLM scorer that guides reward shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LMGT (Housekeep)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>RL agent for embodied manipulation/navigation whose reward signals are augmented by an LLM that receives visual embedding inputs (via LLaVA / Visual Instruction Tuning) and scores actions to produce reward shifts; agent learns from adjusted rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Multimodal LLM-guided reward shaping with Visual Instruction Tuning (adaptive guidance under high-dimensional perceptual inputs)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>LLM uses visual/contextual cues and prior knowledge (object-container associations) to positively shift rewards for actions that move toward correct placements and negatively shift irrelevant actions; agent uses these shifts to prioritize exploration of promising object-placement sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Housekeep (embodied household tidying simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Embodied agent with high-dimensional visual observations, partially observable scenes, long-horizon object-placement tasks, requirement for commonsense mapping (object→container) inferred from crowdsourced data, sparse success-based reward signals.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Moderate-to-high: experiments used a simplified subset (4 scenes; each scene: 1 room with 5 misplaced objects and multiple candidate containers), requires reasoning over perceptual input and commonsense associations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>LMGT consistently outperformed Active Pre-Training (APT) baseline and matched or exceeded ELLM when ELLM used learned descriptors; exact numeric success rates are shown in Figure 5 (not tabulated in main text), reported as higher correct-arrangement success rates on the 4-object tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>APT (reward-free active pretraining) and ELLM (captioner/descriptor baselines) achieved lower success rates; ELLM with ground-truth simulator states sometimes performed better but LMGT matched/surpassed ELLM when ELLM used learned descriptors.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reported faster skill acquisition than APT and some ELLM variants; precise episode/sample counts not explicitly tabulated in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>LLM guidance biases exploration toward semantically plausible object-container pairings, reducing aimless exploration in sparse-reward embodied tasks while keeping policy exploration mechanisms intact.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Active Pre-Training (APT), ELLM (captioner-based / learned descriptors), baseline RL variants.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>LMGT's LLM-guided reward shaping is effective in embodied, visually-rich, partially observable environments when using Visual Instruction Tuning; it outperforms APT and can match or exceed pipeline captioner-based baselines without access to ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>LMGT's performance dips when LLMs are overloaded with simultaneous image-processing and scoring tasks; reliance on multimodal pipeline quality and LLM capability is a bottleneck; numeric computational cost of multimodal inference left unquantified.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reward Guidance for Reinforcement Learning Tasks Based on Large Language Models: The LMGT Framework', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1149.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1149.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LMGT+SlateQ (RecSim)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LMGT integrated with SlateQ recommendation algorithm in RecSim Choc-vs-Kale</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of LMGT reward-shifting to a slate-based recommendation RL algorithm (SlateQ) in a simulated RecSim environment, where LLM guidance helps balance short-term engagement vs long-term satisfaction via reward shifts informed by domain priors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LMGT + SlateQ</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>SlateQ recommendation agent (slate decomposition RL) that incorporates LLM-generated reward shifts per slate/item to guide exploration of recommendable items (chocolate vs kale tradeoff) and long-term user satisfaction.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>LLM-guided adaptive reward shaping to bias slate selection towards long-term-satisfaction-favorable items; uses prior knowledge to inform reward shifts for candidate documents.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>LLM evaluates slates (or items) with respect to long-term user satisfaction priors and applies reward shifts that encourage presenting a mix (kale vs choc) aligned with long-term objectives; these shifts change Q-value estimates and thus action selection in SlateQ.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>RecSim 'Choc vs. Kale' recommendation simulation</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable user state (latent satisfaction and net kale exposure), stochastic user choices and engagement (lognormal), continuous document features (kaleness in [0,1]), sequential interactions with memory/decay (β) and noise (η).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Moderate; slate-based action space (select N items from a large catalog), episodes composed of multiple timesteps where user latent state updates; simulated user dynamics with noise and memory parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>LMGT produced consistent average-reward improvements over baseline SlateQ; reported performance gains (Perf. Gain) of +102.542, +211.643, and +23.115 across tested episode counts (n=10, n=50, n=5000 respectively) in Table 7.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baseline SlateQ achieved lower average rewards (exact baseline A.R. table entries present but partially garbled in paper); LMGT shows faster skill acquisition and higher average rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>LMGT accelerates agent skill acquisition and reduces training cost via more efficient use of episodes/samples; specific sample counts correspond to reported gains across episode counts in Table 7.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>LLM reward shifts encourage exploitation of slates that prior knowledge suggests improve long-term satisfaction while allowing exploration in unshifted regions; shifts alter SlateQ's action-value estimates and thereby skew slate selection distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Baseline SlateQ without LMGT.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>LMGT can transfer to industrial-scale recommendation settings, improving average reward and accelerating learning in a simulated long-term-satisfaction tradeoff task; demonstrates adaptability beyond toy/synthetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Paper notes omission: computational cost analysis for integrating LLMs into training not provided; effect of poor prior knowledge or mis-specified prompts on recommendation outcomes not deeply studied.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reward Guidance for Reinforcement Learning Tasks Based on Large Language Models: The LMGT Framework', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1149.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1149.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NGU+R2D2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Never Give Up (NGU) intrinsic-motivation exploration combined with R2D2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intrinsic-motivation exploration method (NGU) that augments extrinsic rewards with novelty-driven intrinsic rewards, used here as a baseline combined with R2D2 for Atari hard-exploration games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>NGU + R2D2</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>R2D2 base agent augmented with NGU intrinsic rewards, which supply novelty-driven signals to encourage exploration of unfamiliar states while preserving extrinsic reward learning.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Curiosity-driven intrinsic motivation (novelty bonuses) — an adaptive exploration mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Intrinsic novelty reward increases for less-visited or novel states, decaying as states become familiar; the combined intrinsic-extrinsic signal biases action selection toward exploration until novelty diminishes.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Atari Pitfall and Montezuma's Revenge (hard-exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Sparse rewards, large pixel state spaces, long-horizon episodes, stochastic transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>High (same as LMGT Atari experiments), training measured in frames up to 3.5×10^10.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Reported NGU+R2D2 scores: Pitfall after 3.5×10^10 frames 5973.4; Montezuma's Revenge 9049.4 (both lower than LMGT+R2D2 but substantially higher than plain R2D2). Early training: Pitfall 786.9 at 0.5×10^10 frames; Montezuma ~4818.5 at early stages.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Plain R2D2 baseline much lower (e.g., Pitfall 3613.5, Montezuma 2687.2 at 3.5×10^10 frames).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Improves exploration sample-efficiency relative to plain R2D2 but less efficient than LMGT in these experiments (LMGT shows superior early-stage scores).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Balances via intrinsic rewards for novelty that encourage exploration until novelty decays; no LLM prior-knowledge injection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared directly to LMGT+R2D2 and plain R2D2 in Atari experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>NGU provides strong intrinsic-motivation-driven exploration but is outperformed by LMGT in both early and late stages in these particular Atari benchmarks according to the reported scores.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires many environment interactions to build effective exploration strategies; still relies only on environment-driven novelty signals and cannot leverage external prior knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reward Guidance for Reinforcement Learning Tasks Based on Large Language Models: The LMGT Framework', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1149.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1149.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RUDDER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RUDDER: return decomposition for delayed rewards</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reward redistribution method that decomposes cumulative delayed rewards and attributes them to preceding key decision points to simplify Q-value estimation under delayed reward.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RUDDER: return decomposition for delayed rewards</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RUDDER</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Algorithm that learns a redistribution function to assign parts of delayed cumulative reward to earlier time steps, enabling faster learning by converting delayed-return problems into denser surrogate rewards for standard RL learners (used here as a baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Reward redistribution (temporal credit assignment improvement) — an adaptive method to reassign delayed returns to earlier transitions</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Learns and applies a redistribution mapping that identifies and credits important prior actions, adaptively modifying the reward signal used for learning to reduce variance and accelerate credit assignment.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Watch repair delayed-reward task (pocket-watch repair)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Delayed final reward revealing profitability only after cost realization; credit assignment challenging; stochastic costs.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Finite MDP with delayed terminal reward; complexity due to multi-step decisions and uncertainty in costs.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Required 2029 episodes and 171 seconds to reach >90% profitable decision rate in the watch repair task (as reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Compared to LMGT+TD which reached same threshold in 417 episodes and 114 seconds.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Less sample-efficient than LMGT in the reported watch repair experiments: ~2029 vs 417 episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Improves exploitation by providing denser, redistributed rewards that accelerate learning of profitable policies; does not directly inject external prior knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against LMGT+TD and baseline TD/MC in the delayed-reward experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Effective at accelerating learning for delayed-return tasks compared to standard TD/MC but outperformed by LMGT when external prior knowledge (LLM) is available and used to guide redistribution/shifting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Does not incorporate external prior knowledge and can be less effective in domains where such priors would materially accelerate credit assignment; computational overhead and generalization to very large state spaces not discussed in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reward Guidance for Reinforcement Learning Tasks Based on Large Language Models: The LMGT Framework', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>RUDDER: return decomposition for delayed rewards <em>(Rating: 2)</em></li>
                <li>Never Give Up (NGU) <em>(Rating: 1)</em></li>
                <li>Behavior from the void: unsupervised active pre-training <em>(Rating: 1)</em></li>
                <li>Housekeep: Tidying virtual households using commonsense reasoning <em>(Rating: 2)</em></li>
                <li>SlateQ: a tractable decomposition for reinforcement learning with recommendation sets <em>(Rating: 2)</em></li>
                <li>Visual instruction tuning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1149",
    "paper_id": "paper-272524562",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "LMGT",
            "name_full": "Language Model Guided reward Tuning",
            "brief_description": "A framework that uses large language models (LLMs) to generate reward shifts (positive/zero/negative) for state-action pairs during training, thereby adaptively steering exploration and exploitation via reward shaping recorded in the agent's replay buffer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LMGT (framework)",
            "agent_description": "Not a single network architecture but a training-time wrapper: an RL agent (any off-policy or on-policy learner) interacts with the environment; for each observed state and chosen action an LLM (e.g., Vicuna-30B quantized) scores the behavior and returns a reward shift (+1/0/-1 or finer), which is added to the environment reward; experience with shifted rewards is stored and used to update the agent (TD learning, policy gradients, etc.). Visual Instruction Tuning (LLaVA) is used upstream to handle image inputs when needed.",
            "adaptive_design_method": "LLM-guided adaptive reward shaping / adaptive exploration via reward shifting (a form of active, prior-knowledge-driven reward shaping)",
            "adaptation_strategy_description": "At each interaction the LLM evaluates the state-action pair using prior knowledge supplied in prompts (or embedded in model weights) and returns a reward shift: positive for actions deemed valuable, negative for 'valueless' actions, zero for unknown; the agent intensifies exploration around LLM-approved actions and avoids LLM-disapproved directions. Reward shifts effectively re-initialize/modify Q-values and therefore dynamically alter action-selection probabilities over training.",
            "environment_name": "Various (Atari Pitfall, Montezuma's Revenge; watch repair pocket-watch task; Blackjack (box/human formats); CartPole, Pendulum; Housekeep embodied task; RecSim Choc-vs-Kale recommendation)",
            "environment_characteristics": "Unknown/stochastic transition dynamics, often sparse rewards (Atari), partially observable/high-dimensional visual observations (Housekeep, Blackjack human format), delayed credit assignment (watch repair), latent/partially observable user state (RecSim), discrete and continuous action spaces depending on base RL algorithm.",
            "environment_complexity": "Varies by experiment: Atari long-horizon sparse-reward games (large state spaces, long episode horizons), watch repair task with delayed final reward, Housekeep subset of 4 scenes (each: 1 room × 5 misplaced objects), RecSim episodes of sequential recommendation (slates), standard control tasks (CartPole, Pendulum) of small state/action sizes.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Reported consistent improvements across tasks; (examples) Watch repair: LMGT+TD required 417 episodes and 114 s training time to reach &gt;90% profitable decisions; Atari (after 3.5×10^10 frames) Pitfall: LMGT+R2D2 score 6503.5 vs NGU+R2D2 5973.4 and R2D2 baseline 3613.5; Montezuma's Revenge: LMGT+R2D2 12365.5 vs NGU+R2D2 9049.4 and R2D2 2687.2. In RecSim SlateQ experiments LMGT produced average-reward gains (performance gains reported +102.542, +211.643, +23.115 across evaluated episode counts). In Housekeep LMGT outperformed APT and matched/surpassed ELLM with learned descriptors (no exact numeric success rates provided in text).",
            "performance_without_adaptation": "Baselines: RUDDER required 2029 episodes and 171 s in the watch repair task; NGU+R2D2 and plain R2D2 reported substantially lower scores in Atari (e.g., R2D2 3613.5 Pitfall, 2687.2 Montezuma's Revenge at 3.5×10^10 frames). For RecSim SlateQ baseline A.R. values lower than LMGT (exact baseline A.R. table entries present but partially garbled in paper).",
            "sample_efficiency": "LMGT reduces required environment interactions in several tasks: example watch repair ~417 episodes vs 2029 for RUDDER (≈79% reduction in episodes); Atari early-training gains substantial (at 0.5×10^10 frames Pitfall LMGT 1535.5 vs NGU 786.9), indicating faster early discovery of valuable regions. Exact sample counts vary per environment/algorithm.",
            "exploration_exploitation_tradeoff": "Balanced by LLM-generated reward shifts: positive shifts increase exploitation around prior-knowledge-endorsed actions while the agent still follows conventional exploration policies (e.g., ϵ-greedy, policy entropy) and intensifies local exploration near LLM-preferred actions; negative shifts discourage exploration of LLM-flagged low-value actions. Authors note equivalence of reward shifts to Q-function initialization (affecting optimism/pessimism).",
            "comparison_methods": "RUDDER (reward redistribution), NGU (intrinsic curiosity/exploration), standard baselines (R2D2, TD, MC, DQN, PPO, A2C, SAC, TD3, PPO for various tasks), APT (Active Pre-Training) in Housekeep, ELLM (visual-instruction baseline), SlateQ in RecSim.",
            "key_results": "LLM-guided reward shifting (LMGT) substantially improves sample efficiency and final performance in sparse/delayed-reward and complex tasks by injecting prior knowledge: (1) accelerates credit assignment in delayed-reward tasks (watch repair), (2) improves exploration efficiency in hard-exploration Atari games, achieving higher early-stage and late-stage scores than NGU and baselines, (3) scales to embodied Housekeep tasks and industrial recommendation (SlateQ) with measurable gains; framework is flexible across RL algorithms and leverages prompting strategies (CoT, Zero-shot) and multimodal pipelines (LLaVA → Vicuna).",
            "limitations_or_failures": "Performance depends on LLM capability and prompt design; multimodal workloads (image understanding + scoring) can degrade LLM effectiveness (Blackjack 'human' format: LMGT performance near or below baseline). Computational overhead from LLM inference during training is nontrivial and not fully quantified. If LLM prior knowledge is incorrect or absent for a domain, guidance can be misleading. Quantization and model size trade-offs affect inferential capability (smaller/poorly quantized LLMs underperform).",
            "uuid": "e1149.0",
            "source_info": {
                "paper_title": "Reward Guidance for Reinforcement Learning Tasks Based on Large Language Models: The LMGT Framework",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LMGT+TD (watch repair)",
            "name_full": "LMGT applied with Temporal-Difference learning on pocket-watch repair delayed-reward task",
            "brief_description": "LMGT integrated with TD learning where an LLM redistributes reward via scoring of actions to address delayed credit assignment in a pocket-watch repair profitability task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LMGT + TD",
            "agent_description": "Off-policy TD learner (temporal-difference core) whose recorded rewards are shifted by an LLM evaluator per state-action; training proceeds with standard TD updates using experiences containing LLM-added reward shifts.",
            "adaptive_design_method": "LLM-guided reward redistribution (adaptive reward shaping) to improve credit assignment",
            "adaptation_strategy_description": "LLM evaluates actions and assigns reward shifts that reassign delayed terminal rewards to preceding decisions, enabling more direct credit signals for TD updates; this adaptively changes which earlier actions receive learning signal.",
            "environment_name": "Pocket-watch repair (delayed-reward profitability decision task)",
            "environment_characteristics": "Delayed/delayed-credit-assignment, stochastic repair costs, unknown profitability until costs realized (delayed reward), discrete decision sequence environment.",
            "environment_complexity": "Finite MDP with delayed final reward; complexity arises from multi-step decision sequences and uncertain costs; episode lengths and state/action counts not precisely enumerated.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Reached &gt;90% profitable decision rate in 417 training episodes and 114 seconds (including LLM inference time).",
            "performance_without_adaptation": "RUDDER required 2029 episodes and 171 seconds to reach the same threshold (baseline TD/MC reported but table entries in the paper are corrupted and unclear).",
            "sample_efficiency": "Approximate 79.4% reduction in number of episodes needed compared to RUDDER (2029→417 episodes); computational time reduced by ≈33.3% (171s→114s).",
            "exploration_exploitation_tradeoff": "LLM shifts reward to highlight key decision points (positive shifts for valuable earlier actions), thereby biasing TD updates toward promising actions and reducing wasted exploration in low-value directions.",
            "comparison_methods": "RUDDER (return decomposition), TD and Monte Carlo baselines.",
            "key_results": "LLM-based reward shifts substantially speed up learning in delayed-reward tasks by producing more immediate learning signals for prior important decisions, outperforming RUDDER in episodes-to-threshold and wall-clock training time.",
            "limitations_or_failures": "Results rely on LLM providing reliable redistribution signals; computational overhead of LLM inference included but not fully optimized; generalization to larger, continuous delayed-reward tasks not fully tested.",
            "uuid": "e1149.1",
            "source_info": {
                "paper_title": "Reward Guidance for Reinforcement Learning Tasks Based on Large Language Models: The LMGT Framework",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LMGT+R2D2 (Atari)",
            "name_full": "LMGT applied to R2D2 (Recurrent Replay Distributed DQN) on hard-exploration Atari games",
            "brief_description": "Integration of LMGT reward-shifting with R2D2 to improve exploration in sparse-reward Atari games like Pitfall and Montezuma's Revenge, using LLM evaluations to bias exploration toward promising trajectories.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LMGT + R2D2",
            "agent_description": "R2D2 base agent (recurrent DQN with distributed replay) which records environment rewards shifted by an LLM scorer; LLM guidance biases replay and action selection during learning.",
            "adaptive_design_method": "Prior-knowledge-driven reward shaping via LLM to direct exploration (complementary to intrinsic-motivation methods)",
            "adaptation_strategy_description": "LLM assigns positive shifts to state-action pairs consistent with prior knowledge of valuable trajectories; these shifts alter replayed transitions' TD targets and thus prioritization of learning, leading to more sampling of high-value regions and local exploration near recommended actions.",
            "environment_name": "Atari 2600: Pitfall and Montezuma's Revenge",
            "environment_characteristics": "Extremely sparse rewards, large discrete state space (pixels), long-horizon episodes, stochasticity in transitions from player/environment interactions, partially observable from single frames but handled via R2D2 recurrence.",
            "environment_complexity": "High; long-horizon exploration required, standard Atari frame-based observations (high-dimensional pixel inputs), training measured in frames up to 3.5×10^10 frames.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "After 3.5×10^10 frames: Pitfall average reward 6503.5 vs NGU+R2D2 5973.4 and R2D2 baseline 3613.5; Montezuma's Revenge average reward 12365.5 vs NGU+R2D2 9049.4 and R2D2 baseline 2687.2. Early training advantage: at 0.5×10^10 frames Pitfall LMGT 1535.5 vs NGU 786.9; Montezuma LMGT 5635.4 vs NGU 4818.5.",
            "performance_without_adaptation": "Baseline R2D2 much lower (e.g., Pitfall 3613.5, Montezuma 2687.2 at 3.5×10^10 frames); NGU improves over baseline but LMGT surpasses NGU in both early and late stages.",
            "sample_efficiency": "LMGT shows markedly better early-sample efficiency (roughly 2× score in Pitfall at 0.5×10^10 frames compared to NGU), enabling faster discovery of rewarding regions.",
            "exploration_exploitation_tradeoff": "LLM reward shifts push R2D2 to exploit trajectories endorsed by prior knowledge while maintaining recurrence-driven temporal credit assignment and standard exploration mechanisms; exploration becomes more focused (local exploration near LLM-approved actions).",
            "comparison_methods": "NGU+R2D2 (intrinsic motivation), plain R2D2 baseline.",
            "key_results": "LLM-guided reward shaping substantially improves both early-stage and asymptotic performance in notoriously hard-exploration Atari games, outperforming intrinsic-motivation NGU and standard R2D2.",
            "limitations_or_failures": "Dependence on LLM quality and prompt design; computational cost of LLM inference at Atari training scale not fully characterized.",
            "uuid": "e1149.2",
            "source_info": {
                "paper_title": "Reward Guidance for Reinforcement Learning Tasks Based on Large Language Models: The LMGT Framework",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LMGT (Housekeep)",
            "name_full": "LMGT applied to Housekeep embodied robotic environment",
            "brief_description": "LMGT deployed in an embodied simulation where a robot must place misplaced household objects into correct containers, using Visual Instruction Tuning to feed image-derived embeddings to the LLM scorer that guides reward shifts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LMGT (Housekeep)",
            "agent_description": "RL agent for embodied manipulation/navigation whose reward signals are augmented by an LLM that receives visual embedding inputs (via LLaVA / Visual Instruction Tuning) and scores actions to produce reward shifts; agent learns from adjusted rewards.",
            "adaptive_design_method": "Multimodal LLM-guided reward shaping with Visual Instruction Tuning (adaptive guidance under high-dimensional perceptual inputs)",
            "adaptation_strategy_description": "LLM uses visual/contextual cues and prior knowledge (object-container associations) to positively shift rewards for actions that move toward correct placements and negatively shift irrelevant actions; agent uses these shifts to prioritize exploration of promising object-placement sequences.",
            "environment_name": "Housekeep (embodied household tidying simulation)",
            "environment_characteristics": "Embodied agent with high-dimensional visual observations, partially observable scenes, long-horizon object-placement tasks, requirement for commonsense mapping (object→container) inferred from crowdsourced data, sparse success-based reward signals.",
            "environment_complexity": "Moderate-to-high: experiments used a simplified subset (4 scenes; each scene: 1 room with 5 misplaced objects and multiple candidate containers), requires reasoning over perceptual input and commonsense associations.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "LMGT consistently outperformed Active Pre-Training (APT) baseline and matched or exceeded ELLM when ELLM used learned descriptors; exact numeric success rates are shown in Figure 5 (not tabulated in main text), reported as higher correct-arrangement success rates on the 4-object tasks.",
            "performance_without_adaptation": "APT (reward-free active pretraining) and ELLM (captioner/descriptor baselines) achieved lower success rates; ELLM with ground-truth simulator states sometimes performed better but LMGT matched/surpassed ELLM when ELLM used learned descriptors.",
            "sample_efficiency": "Reported faster skill acquisition than APT and some ELLM variants; precise episode/sample counts not explicitly tabulated in main text.",
            "exploration_exploitation_tradeoff": "LLM guidance biases exploration toward semantically plausible object-container pairings, reducing aimless exploration in sparse-reward embodied tasks while keeping policy exploration mechanisms intact.",
            "comparison_methods": "Active Pre-Training (APT), ELLM (captioner-based / learned descriptors), baseline RL variants.",
            "key_results": "LMGT's LLM-guided reward shaping is effective in embodied, visually-rich, partially observable environments when using Visual Instruction Tuning; it outperforms APT and can match or exceed pipeline captioner-based baselines without access to ground truth.",
            "limitations_or_failures": "LMGT's performance dips when LLMs are overloaded with simultaneous image-processing and scoring tasks; reliance on multimodal pipeline quality and LLM capability is a bottleneck; numeric computational cost of multimodal inference left unquantified.",
            "uuid": "e1149.3",
            "source_info": {
                "paper_title": "Reward Guidance for Reinforcement Learning Tasks Based on Large Language Models: The LMGT Framework",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LMGT+SlateQ (RecSim)",
            "name_full": "LMGT integrated with SlateQ recommendation algorithm in RecSim Choc-vs-Kale",
            "brief_description": "Application of LMGT reward-shifting to a slate-based recommendation RL algorithm (SlateQ) in a simulated RecSim environment, where LLM guidance helps balance short-term engagement vs long-term satisfaction via reward shifts informed by domain priors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LMGT + SlateQ",
            "agent_description": "SlateQ recommendation agent (slate decomposition RL) that incorporates LLM-generated reward shifts per slate/item to guide exploration of recommendable items (chocolate vs kale tradeoff) and long-term user satisfaction.",
            "adaptive_design_method": "LLM-guided adaptive reward shaping to bias slate selection towards long-term-satisfaction-favorable items; uses prior knowledge to inform reward shifts for candidate documents.",
            "adaptation_strategy_description": "LLM evaluates slates (or items) with respect to long-term user satisfaction priors and applies reward shifts that encourage presenting a mix (kale vs choc) aligned with long-term objectives; these shifts change Q-value estimates and thus action selection in SlateQ.",
            "environment_name": "RecSim 'Choc vs. Kale' recommendation simulation",
            "environment_characteristics": "Partially observable user state (latent satisfaction and net kale exposure), stochastic user choices and engagement (lognormal), continuous document features (kaleness in [0,1]), sequential interactions with memory/decay (β) and noise (η).",
            "environment_complexity": "Moderate; slate-based action space (select N items from a large catalog), episodes composed of multiple timesteps where user latent state updates; simulated user dynamics with noise and memory parameters.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "LMGT produced consistent average-reward improvements over baseline SlateQ; reported performance gains (Perf. Gain) of +102.542, +211.643, and +23.115 across tested episode counts (n=10, n=50, n=5000 respectively) in Table 7.",
            "performance_without_adaptation": "Baseline SlateQ achieved lower average rewards (exact baseline A.R. table entries present but partially garbled in paper); LMGT shows faster skill acquisition and higher average rewards.",
            "sample_efficiency": "LMGT accelerates agent skill acquisition and reduces training cost via more efficient use of episodes/samples; specific sample counts correspond to reported gains across episode counts in Table 7.",
            "exploration_exploitation_tradeoff": "LLM reward shifts encourage exploitation of slates that prior knowledge suggests improve long-term satisfaction while allowing exploration in unshifted regions; shifts alter SlateQ's action-value estimates and thereby skew slate selection distribution.",
            "comparison_methods": "Baseline SlateQ without LMGT.",
            "key_results": "LMGT can transfer to industrial-scale recommendation settings, improving average reward and accelerating learning in a simulated long-term-satisfaction tradeoff task; demonstrates adaptability beyond toy/synthetic tasks.",
            "limitations_or_failures": "Paper notes omission: computational cost analysis for integrating LLMs into training not provided; effect of poor prior knowledge or mis-specified prompts on recommendation outcomes not deeply studied.",
            "uuid": "e1149.4",
            "source_info": {
                "paper_title": "Reward Guidance for Reinforcement Learning Tasks Based on Large Language Models: The LMGT Framework",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "NGU+R2D2",
            "name_full": "Never Give Up (NGU) intrinsic-motivation exploration combined with R2D2",
            "brief_description": "An intrinsic-motivation exploration method (NGU) that augments extrinsic rewards with novelty-driven intrinsic rewards, used here as a baseline combined with R2D2 for Atari hard-exploration games.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "NGU + R2D2",
            "agent_description": "R2D2 base agent augmented with NGU intrinsic rewards, which supply novelty-driven signals to encourage exploration of unfamiliar states while preserving extrinsic reward learning.",
            "adaptive_design_method": "Curiosity-driven intrinsic motivation (novelty bonuses) — an adaptive exploration mechanism",
            "adaptation_strategy_description": "Intrinsic novelty reward increases for less-visited or novel states, decaying as states become familiar; the combined intrinsic-extrinsic signal biases action selection toward exploration until novelty diminishes.",
            "environment_name": "Atari Pitfall and Montezuma's Revenge (hard-exploration)",
            "environment_characteristics": "Sparse rewards, large pixel state spaces, long-horizon episodes, stochastic transitions.",
            "environment_complexity": "High (same as LMGT Atari experiments), training measured in frames up to 3.5×10^10.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Reported NGU+R2D2 scores: Pitfall after 3.5×10^10 frames 5973.4; Montezuma's Revenge 9049.4 (both lower than LMGT+R2D2 but substantially higher than plain R2D2). Early training: Pitfall 786.9 at 0.5×10^10 frames; Montezuma ~4818.5 at early stages.",
            "performance_without_adaptation": "Plain R2D2 baseline much lower (e.g., Pitfall 3613.5, Montezuma 2687.2 at 3.5×10^10 frames).",
            "sample_efficiency": "Improves exploration sample-efficiency relative to plain R2D2 but less efficient than LMGT in these experiments (LMGT shows superior early-stage scores).",
            "exploration_exploitation_tradeoff": "Balances via intrinsic rewards for novelty that encourage exploration until novelty decays; no LLM prior-knowledge injection.",
            "comparison_methods": "Compared directly to LMGT+R2D2 and plain R2D2 in Atari experiments.",
            "key_results": "NGU provides strong intrinsic-motivation-driven exploration but is outperformed by LMGT in both early and late stages in these particular Atari benchmarks according to the reported scores.",
            "limitations_or_failures": "Requires many environment interactions to build effective exploration strategies; still relies only on environment-driven novelty signals and cannot leverage external prior knowledge.",
            "uuid": "e1149.5",
            "source_info": {
                "paper_title": "Reward Guidance for Reinforcement Learning Tasks Based on Large Language Models: The LMGT Framework",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "RUDDER",
            "name_full": "RUDDER: return decomposition for delayed rewards",
            "brief_description": "A reward redistribution method that decomposes cumulative delayed rewards and attributes them to preceding key decision points to simplify Q-value estimation under delayed reward.",
            "citation_title": "RUDDER: return decomposition for delayed rewards",
            "mention_or_use": "use",
            "agent_name": "RUDDER",
            "agent_description": "Algorithm that learns a redistribution function to assign parts of delayed cumulative reward to earlier time steps, enabling faster learning by converting delayed-return problems into denser surrogate rewards for standard RL learners (used here as a baseline).",
            "adaptive_design_method": "Reward redistribution (temporal credit assignment improvement) — an adaptive method to reassign delayed returns to earlier transitions",
            "adaptation_strategy_description": "Learns and applies a redistribution mapping that identifies and credits important prior actions, adaptively modifying the reward signal used for learning to reduce variance and accelerate credit assignment.",
            "environment_name": "Watch repair delayed-reward task (pocket-watch repair)",
            "environment_characteristics": "Delayed final reward revealing profitability only after cost realization; credit assignment challenging; stochastic costs.",
            "environment_complexity": "Finite MDP with delayed terminal reward; complexity due to multi-step decisions and uncertainty in costs.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Required 2029 episodes and 171 seconds to reach &gt;90% profitable decision rate in the watch repair task (as reported in paper).",
            "performance_without_adaptation": "Compared to LMGT+TD which reached same threshold in 417 episodes and 114 seconds.",
            "sample_efficiency": "Less sample-efficient than LMGT in the reported watch repair experiments: ~2029 vs 417 episodes.",
            "exploration_exploitation_tradeoff": "Improves exploitation by providing denser, redistributed rewards that accelerate learning of profitable policies; does not directly inject external prior knowledge.",
            "comparison_methods": "Compared against LMGT+TD and baseline TD/MC in the delayed-reward experiments.",
            "key_results": "Effective at accelerating learning for delayed-return tasks compared to standard TD/MC but outperformed by LMGT when external prior knowledge (LLM) is available and used to guide redistribution/shifting.",
            "limitations_or_failures": "Does not incorporate external prior knowledge and can be less effective in domains where such priors would materially accelerate credit assignment; computational overhead and generalization to very large state spaces not discussed in detail in this paper.",
            "uuid": "e1149.6",
            "source_info": {
                "paper_title": "Reward Guidance for Reinforcement Learning Tasks Based on Large Language Models: The LMGT Framework",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "RUDDER: return decomposition for delayed rewards",
            "rating": 2,
            "sanitized_title": "rudder_return_decomposition_for_delayed_rewards"
        },
        {
            "paper_title": "Never Give Up (NGU)",
            "rating": 1,
            "sanitized_title": "never_give_up_ngu"
        },
        {
            "paper_title": "Behavior from the void: unsupervised active pre-training",
            "rating": 1,
            "sanitized_title": "behavior_from_the_void_unsupervised_active_pretraining"
        },
        {
            "paper_title": "Housekeep: Tidying virtual households using commonsense reasoning",
            "rating": 2,
            "sanitized_title": "housekeep_tidying_virtual_households_using_commonsense_reasoning"
        },
        {
            "paper_title": "SlateQ: a tractable decomposition for reinforcement learning with recommendation sets",
            "rating": 2,
            "sanitized_title": "slateq_a_tractable_decomposition_for_reinforcement_learning_with_recommendation_sets"
        },
        {
            "paper_title": "Visual instruction tuning",
            "rating": 2,
            "sanitized_title": "visual_instruction_tuning"
        }
    ],
    "cost": 0.019212,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Reward Guidance for Reinforcement Learning Tasks Based on Large Language Models: The LMGT Framework
20 May 2025</p>
<p>Yongxin Deng 
School of Electronic and Electrical Engineering
Shanghai University of Engineering Science
ShanghaiChina</p>
<p>Xihe Qiu qiuxihe@sues.edu.cn 
School of Electronic and Electrical Engineering
Shanghai University of Engineering Science
ShanghaiChina</p>
<p>Jue Chen 
School of Electronic and Electrical Engineering
Shanghai University of Engineering Science
ShanghaiChina</p>
<p>Xiaoyu Tan 
INFLY TECH (Shanghai) Co., Ltd
ShanghaiChina</p>
<p>Reward Guidance for Reinforcement Learning Tasks Based on Large Language Models: The LMGT Framework
20 May 202527648DAE9DC0C617AB0F6A54B5D4008CarXiv:2409.04744v3[cs.LG]Preprint submitted to ElsevierReinforcement LearningDeep Learning MethodsLearning from Demonstration
The inherent uncertainty in the environmental transition model of Reinforcement Learning (RL) necessitates a delicate balance between exploration and exploitation.This balance is crucial for optimizing computational resources to accurately estimate expected rewards for the agent.In scenarios with sparse rewards, such as robotic control systems, achieving this balance is particularly challenging.However, given that many environments possess extensive prior knowledge, learning from the ground up in such contexts may be redundant.To address this issue, we propose Language Model Guided reward Tuning (LMGT), a novel, sample-efficient framework.LMGT leverages the comprehensive prior knowledge embedded in Large Language Models (LLMs) and their proficiency in processing non-standard data forms, such as wiki tutorials.By utilizing LLM-guided reward shifts, LMGT adeptly balances exploration and exploitation, thereby guiding the agent's exploratory behavior and enhancing sample efficiency.We have rigorously evaluated LMGT across various RL tasks and evaluated it in the embodied robotic environment Housekeep.Our results demonstrate that LMGT consistently outperforms baseline methods.Furthermore, the findings suggest that our framework can substantially reduce the computational resources required during the RL training phase.</p>
<p>Introduction</p>
<p>Reinforcement Learning (RL) faces a fundamental challenge in striking an optimal balance between exploration and exploitation [1,2].This equilibrium is crucial for ensuring the robustness of RL algorithms in real-world applications [3].Agents in these environments encounter the exploration-exploitation dilemma due to the inherent unknown and stochastic nature of their surroundings, making it impossible to deduce the exact environmental model.</p>
<p>Through interactions with the environment, agents obtain estimates of expected rewards, denoted as Ê(R), for various actions.However, estimating rewards using sample statistics inherently introduces error, preventing certainty that the action with the highest Ê(R) is truly optimal.This uncertainty necessitates exploration to reduce the discrepancy between sample means and true means.Conversely, in resource-constrained scenarios, it is often advantageous to select actions with the highest estimated rewards-a practice known as exploitation.The accurate estimation of optimal actions is fundamental to RL [4,5], whereas suboptimal actions require less computational evaluation.This inherent conflict necessitates selecting actions that balance proximity to the current "best action" estimate while maintaining sufficient diversity [6].</p>
<p>Numerous approaches have attempted to address this challenge.Traditional strategies include ϵ-greedy [6], Softmax [6], upper confidence bound [7], and Thompson sampling [8].The selection of an appropriate exploration-exploitation strategy typically depends on the specific application context and problem requirements, as different RL problems demand different approaches.However, with the expanding scope of RL applications, manually selecting distinct strategies for  each environment becomes impractical.Particular difficulties arise from multimodal and long-tailed data distributions.While some adaptive algorithms adjust the exploration-exploitation balance based on agent experiences [9], these algorithms still have constraints that can significantly impact model performance and robustness when applied beyond their intended scope.Moreover, previous strategies either rely solely on adjusting ratios based on data distribution without utilizing prior knowledge, or require substantial domain expertise and task understanding, potentially reducing learning performance if improperly designed.</p>
<p>To overcome these limitations, we propose a novel framework called Language Model Guided reward Tuning (i.e., LMGT), which leverages prior knowledge from various sources to guide agents' learning with limited resources.Based on the work of Bruce et al. [10], who demonstrated that valuable information can be derived from effective offline demonstration data, our approach enables agents to align themselves correctly with desirable behaviors.By capturing patterns of sound policies and using these for intrinsic motivation, RL agents can effectively map themselves to skillful demonstration-defined subspaces where even undirected exploration can significantly enhance environmental understanding if deviations align with ratio-nal pathways.</p>
<p>Our LMGT framework harnesses the text comprehension and generation capabilities of Large Language Models (LLMs) to incorporate prior knowledge, thereby enhancing the agent's environmental understanding and achieving an effective balance between exploration and exploitation.Leveraging the powerful language processing capabilities of LLMs, our framework removes the need for highly structured prior knowledge, thereby allowing broader utilization of human knowledge sources.This distinct advantage sets our approach apart from other methods.Moreover, the text generated by LLMs often reflects structural patterns of the real world, embedding common-sense knowledge about various aspects of human reasoning and intuition [11,12,13].</p>
<p>Compared to some recent methods [14,15,16,17,18,19] that directly use LLMs as agents within the RL process, the advantage of our approach lies in the fact that LLMs are only required during the training phase to assist the agent in learning.Once training is complete, our agent can be deployed independently without LLMs.In contrast to agents utilizing LLM kernels, conventional RL agents founded upon multilayer perceptrons or convolutional neural networks exhibit a comparative advantage regarding computational resource utilization [20,21].These advantages become particularly relevant in large-scale application scenarios and latency-sensitive environments.</p>
<p>Our interaction with LLMs involves LLMs processing environmental information and scoring agent behavior to guide exploration and exploitation through reward-shifting mechanisms.For certain non-text environments, we employ Visual Instruction Tuning [22] to align visual information with the projection matrices of LLMs.The rationale for this approach will be detailed in Section 4.4.Additionally, our method aligns with a key principle: reward shifting is equivalent to modifying the initialization of the Q-function, effectively balancing the exploration and exploitation aspects of RL [23].</p>
<p>We conducted extensive experiments across diverse settings and environments to evaluate our proposed method.The results demonstrate that our approach effectively leverages prior knowledge, significantly reducing the computational resources required for model training compared to baseline methods.Additionally, we assessed the performance of various LLMs within our framework, providing insights into their inferential capabilities in this context.To validate the versatility of our approach, we implemented our framework in Housekeep [24], a simulated robotic environ-Figure 2: The structure of our LMGT framework.The LLM can observe the environment's state and the actions selected by the agent.It will evaluate the agent's behavior using prior knowledge, adjusting the final reward accordingly (via reward shifting).Thus, the agent's stored experience inherently includes a component of prior knowledge.ment, and evaluated its performance on complex robotic tasks.Furthermore, we applied our framework to Google's industrial-grade recommendation algorithm, SlateQ [25].</p>
<p>Our contributions can be summarized as follows:</p>
<p>• We propose a novel framework for balancing exploration and exploitation in RL by leveraging LLMs.This framework addresses the explorationexploitation dilemma and effectively guides the behavior of agents.</p>
<p>• We validate our proposed method across various RL environments and algorithms.Our approach significantly reduces the training cost of RL models while maintaining generality and ease of use.</p>
<p>• We demonstrate the effectiveness of our method in an industrial application context, providing a practical and straightforward solution to reduce the training cost of RL models for industry practitioners.</p>
<p>Related Work</p>
<p>Our research has significant connections to various preceding studies.Within the realm of reinforcement learning, there has been a comprehensive exploration of strategies to balance exploration and exploitation, a concept that resonates with our methodology.Our approach innovates within this field by harmonizing reinforcement learning principles with the capabilities of LLMs.This harmonization broadens our comprehension and application of balancing strategies, empowering us to tackle decision-making challenges in intricate settings more effectively.Moreover, our study draws inspiration from contemporary research on the evolution of LLMs, which has laid the theoretical groundwork and catalyzed noteworthy advancements in our inquiry.</p>
<p>Exploration-Exploitation Trade-off in RL</p>
<p>Numerous strategies addressing the RL explorationexploitation dilemma have been proposed, encompassing various tasks.Established approaches such as the ϵ-greedy [6], Softmax [6], Upper Confidence Bound (UCB) [7], and Thompson sampling [8] methods are widely recognized.Furthermore, entropy-based RL frameworks have seen progressive advancements [26,27], facilitating diverse exploration pathways in Markov Decision Processes (MDPs), rendering them highly effective in continuous action spaces.The emergence of curiosity-driven RL strategies that leverage intrinsic curiosity as a reward signal [28] has spurred extensive research [29].Notably, these approaches enable agents to interact with their environment and develop new skills.Despite their varying success in numerous contexts, these strategies typically exhibit static features, limiting their adaptability to dynamic settings.An innovative decoupling method was introduced, combining goal-specific and goal-agnostic exploration strategies for improved adaptability [30].Additionally, deep covering options were investigated to enhance explo-ration efficiency without reliance on explicit state spaces or unwieldy action sets, courtesy of deep learning models and spectral analysis [31].Daochen et al. [32] proposed a simplifying exploration method suitable for generative settings, fostering model-independent exploration through a ranking mechanism in environments with sparse rewards.While adept at accommodating changes autonomously, these approaches have restricted applicability due to their inattention to prior knowledge infusion, which could advance the equilibrium between exploration and exploitation.Our framework demonstrates versatility across diverse environments and RL algorithms, incorporating previously untapped prior knowledge to heighten agents' learning efficacy and minimize resource expenditure during training.</p>
<p>Hao et al. [23] theoretically prove that negative reward shifts aid exploration while positive reward shifts limit exploration.Ingmar et al. [33] introduce a novel reward shaping method that relaxes the strict optimality guarantee of potential-based reward shaping (PB-RS) while maintaining long-term behavioral consistency and enhancing sample efficiency compared to PB-RS.Inspired by these studies, we employed the reward shifts generated by LLMs to balance exploration and exploitation, enabling fine control of the impact on the existing RL process while making the overall learning process more efficient.</p>
<p>Relevant Research in Large Language Models</p>
<p>LLMs, particularly those based on the Transformer architecture [34], such as OpenAI's GPT [35,36], Meta's Llama [37], Google's Bard [38], LMSYS Org's Vicuna [39], and Anthropic's Claude [40], have widespread applications across various domains.Recent work [41] in the field of LLMs has provided theoretical foundations for our research.[11] demonstrates that the knowledge embedded within LLMs offers valuable inductive biases applicable to both traditional Natural Language Processing (NLP) tasks and non-traditional tasks involving training symbolic reasoning engines.The text generated by LLMs typically reflects structural patterns in the "real world", indicating that the weights within LLMs encode implicit common-sense knowledge related to various aspects of human reasoning and intuition.Through an analysis of learning representations in the Llama-2 series models across three spatial datasets and three temporal datasets, Wes et al. [12] presents evidence that LLMs form a coherent model of the data-generating process, which suggests that LLMs acquire structured knowledge regarding space and time fundamental dimensions.We posit that the weights embedded within LLMs inherently carry prior knowledge that can be utilized to guide agents' behavior.</p>
<p>Methodology</p>
<p>In this section, we present the overall structure and provide an in-depth exploration of the aspects relevant to prompts within our framework.</p>
<p>Framework Structure</p>
<p>RL methods are categorized into "on-policy" and "off-policy" based on how data is generated and processed.On-policy and off-policy methods are often viewed as distinct due to significant differences in their policy frameworks and algorithmic implementations in practice.These differences influence algorithm selection and optimization techniques.For instance, offpolicy methods must address the importance of sampling issues associated with using data from non-target policies-a challenge not faced by on-policy methods.Broadly, however, on-policy methods can be seen as a subset of off-policy methods, where the behavior policy (which generates the data) aligns with the target policy (the policy under optimization).Thus, off-policy definitions are inherently broader, encompassing all scenarios, even those where the learning and behavior policies coincide.All descriptions related to RL mentioned below refer to off-policy methods.A common RL training process is as follows:</p>
<ol>
<li>
<p>Initialization of the evaluation policy and the behavioral policy.The evaluation policy may be initialized as a stochastic policy, such as a random policy, while the behavioral policy may take the form of an ϵ-greedy policy, incorporating a probability of random exploration.2. The agent engages with the environment based on the behavioral policy, yielding training data in the form of state-action-reward-next state tuples.These data are then archived within an experience replay buffer.3. Training data is sampled from the experience replay buffer, and the agent's parameters are updated based on the evaluation policy and the sampled data, employing techniques such as Temporal Difference (TD) learning or Monte Carlo methods.4. Periodic evaluation of the evaluation policy's performance within the environment, with training termination contingent on the attainment of a predefined performance threshold.</p>
</li>
<li>
<p>The process iterates by returning to step 2, with periodic adjustments to the behavioral policy, such as the gradual reduction of ϵ in the case of an ϵgreedy policy.</p>
</li>
</ol>
<p>To ensure the wide-ranging applicability of our improvements, we aim to preserve the fundamental principles of the original RL training process with minimal intervention.LMGT introduces a specific modification to the second step, which involves adjusting the acquired experiences of the agent.We consider the LLM as the "evaluator".When the agent observes the environmental state, it selects an action based on the prevailing behavioral policy and communicates this action to the environment.We replicate and transmit both the observable state of the environment and the chosen action to the LLM.The LLM assesses the agent's actions and assigns a score, taking into account the prior knowledge that is embedded in its weights or introduced through the prompt (such as game rules).This score serves as a reward shift, which is incorporated into the reward generated by the environment itself.In contrast to the conventional RL process, LMGT involves the agent recording adjusted rewards instead of relying on the inherent rewards provided by the environment.The agent then learns from these adapted rewards to gain guidance from the LLMs.</p>
<p>In situations with sparse rewards, the agent faces challenges in accumulating information through trial and error.Hence, we employ the LLM to guide the agent, to avoid the pursuit of directions that have been previously determined as "valueless" based on prior knowledge, as indicated by a negative reward shift.The LLM assigns a positive reward shift for actions identified as "valuable" according to prior knowledge, encouraging the agent to focus on exploitation.While maintaining the traditional explorationexploitation strategy from classical RL, the agent intensifies its exploration of actions neighboring those deemed "valuable" in the prior knowledge, increasing the likelihood of discovering the "optimal" action.Additionally, for actions not referenced in prior knowledge, the LLM assigns a "0" reward shift, allowing the agent to explore based on the original exploration policy.</p>
<p>The framework of LMGT is presented in Figure 2.For different tasks, the LLM provides various forms of reward shifts, guided by the principle that intricate tasks require more nuanced reward shifts, while simpler tasks require simpler reward shifts, using "+1," "0," and "-1" to represent "approval", "neutral", and "disapproval", respectively.Select an action a using the behavior policy: a = SelectAction(s, P); Break from the current loop and move to the next episode;</p>
<p>Prompt Design</p>
<p>In this section, we will discuss the engineering methodology applied to optimize the performance of LLMs.The primary emphasis is placed on the performance attributes of LLMs, specifically concerning the magnitude of embedded prior knowledge in their weight configurations, as well as their capacity to comprehend and harness pre-existing knowledge about textual genres.The efficacy of the reward-shifting mechanism, generated by LLMs, fundamentally dictates the success of our approach and the extent of enhancement in comparison to the baseline.</p>
<p>Table 4 catalogs a detailed inventory of immediate enhancements utilized in our experimental design.It is important to note that the primary distinction between Zero-shot [42] and Baseline involves Zero-shot's integration of specific, task-related information into the prompt, which guides the LLM regarding the appropriate information to produce.The Name method could be perceived as perplexing.It involves attributing a name to an LLM in the prompt with the aim of improving performance.Nonetheless, our experimental results indicate that this technique does not yield any improvements.For additional details on the experiments, please see Section 4.3.It is commonplace to deploy multiple prompt enhancements concurrently.</p>
<p>Furthermore, our prompt design is further categorized into two distinct classes: "prior-knowledgeinclusive prompt statements" and "prior-knowledgeexclusive prompt statements".The former class provides an all-encompassing evaluation of the LLMs' ability to harness their embedded prior knowledge, including their proficiency in leveraging prior knowledge presented in non-standard linguistic forms, such as natural language text.The latter class, on the other hand, exclusively investigates the LLMs' aptitude for exploiting implicit prior knowledge embedded within their model weights.</p>
<p>Section 4 presents an elucidation of the effects of various prompt methods, along with a rationale for our methodological choices.</p>
<p>Experiment</p>
<p>The experiment is structured into three distinct parts.In the initial phase, we scrutinize the benefits of our proposed framework over conventional approaches for addressing sparse reward challenges.Specifically, we compare LMGT with Return Decomposition for Delayed Rewards (RUDDER) [43], which is a novel RL approach for delayed rewards in finite MDPs.RUD-DER's objective is to neutralize expected future rewards, thereby simplifying Q-value estimations to the average of immediate rewards.Despite RUDDER's expedited processing in scenarios with delayed rewards compared to traditional RL methods, it fails to incorporate prior knowledge-an area where LMGT particularly excels.Consequently, we anticipate LMGT to facilitate the expedited development of effective behavioral strategies by agents.The second segment of the experiment evaluates our framework's versatility by applying it across diverse RL algorithms and environments to ascertain its efficacy.Herein, we examined the enhancement in performance attributable to our framework across various RL algorithms, compared to some classic baselines.This phase further includes an assessment of the impact of different prompting techniques on our framework's performance and an exploratory evaluation of the reasoning capabilities of LLMs within our framework.To ensure that the evaluation conclusions of our framework extend beyond synthetic settings, the final section investigates its application to and performance in complex robotic environments.We conducted experiments using the Housekeep environment [24].Housekeep is a simulated environment for embodied agents, where robots must perform housekeeping tasks by correctly placing outof-place objects into appropriate containers.To ensure that the evaluation conclusions of our framework extend beyond synthetic settings, the final section investigates its practical applications and improvements.Specifically, we explore its integration with Google's SlateQ [25], a sophisticated recommendation algorithm that employs slate decomposition.This approach effectively manages the complexity of recommending multiple items simultaneously, addressing the challenge of large action spaces found in previous RL recommendation algorithms.This implementation was tested within a simulated environment on RecSim [44], a versatile platform for developing simulation environments for recommender systems (RSs), facilitating sequential user interactions.</p>
<p>Experimental Settings</p>
<p>For both LLM inference and agent training, we utilize a single NVIDIA A800-80G GPU.We adhere to the recommended settings by Llama for precise inference, which encompass a temperature of 0.7, top p of 0.1, a repetition penalty of 1.18, and top k of 40.</p>
<p>Comparison Experiments with Traditional</p>
<p>Exploration-Exploitation Trade-off Methods Reinforcement learning (RL) presents two fundamental challenges: the credit assignment problem for delayed rewards and the efficiency of exploration in complex environments.The former concerns the attribution of sparse, delayed rewards to preceding actions in a decision sequence, while the latter addresses the optimal balance between exploration and exploitation in unknown environments.Addressing these challenges has led researchers to develop various approaches, notably reward redistribution methods such as RUDDER and intrinsic motivation techniques such as NGU (Never Give Up).In this section, we empirically evaluate LMGT's efficacy in addressing these core challenges through comparative experiments with these representative algorithms.</p>
<p>Comparison with RUDDER's Reward Redistri-</p>
<p>bution Method RUDDER (Return Decomposition for Delayed Rewards) represents a specialized algorithm designed to address delayed reward problems.It employs a reward redistribution function that decomposes cumulative delayed rewards and attributes them to key preceding decision points.This approach is particularly effective for scenarios with challenging credit assignment, exemplified by the pocket watch repair task utilized in our experiments.In this task, agents must determine whether to repair watches of specific brands by conducting costbenefit analyses between known selling prices and uncertain repair costs.The task embodies a classic delayed reward problem, as the profitability of repair decisions becomes apparent only after all associated costs are finalized.RUDDER was selected as a baseline for comparison because it represents a state-of-the-art method for addressing delayed reward problems and closely aligns with our research objectives.To ensure methodological consistency, LMGT was implemented with temporal difference (TD) learning, matching RUDDER's foundational approach.Experiments were conducted using multiple runs with random seeds 42, 43, 44, 45, 46, and results were averaged.Performance was evaluated using two metrics: the number of training episodes required to achieve a profitable decision rate exceeding 90% (reported as "Episode") and the total computational time needed for training (reported as "Time"), inclusive of LLM inference time.</p>
<p>As illustrated in Table 1, experimental results demonstrate that LMGT substantially outperforms RUDDER on both metrics: LMGT requires only 417 training episodes and 114 seconds to develop a qualified strategy, whereas RUDDER necessitates 2029 episodes and 171 seconds.Notably, LMGT's proportional advantage in training episodes (approximately 79.4% reduction) significantly exceeds its advantage in computational time (approximately 33.3% reduction).This disparity suggests that the prior knowledge embedded in the language model effectively accelerates the value learning process, resulting in more efficient credit assignment.</p>
<p>Comparison with NGU's Exploration-Driven</p>
<p>Method With respect to the second critical challenge in reinforcement learning-exploration efficiency-NGU (Never Give Up) represents an advanced exploration methodology based on intrinsic motivation principles.NGU integrates novelty-driven intrinsic rewards with environmental extrinsic rewards, thereby encouraging continuous exploration of unknown state spaces while maintaining focus on high-value regions.While this approach demonstrates excellent performance in environments with sparse rewards, it nevertheless requires extensive environmental interactions to develop effective exploration strategies.</p>
<p>To assess LMGT's exploration efficiency advantages, we selected two Atari games characterized by highly sparse rewards: Pitfall and Montezuma's Revenge.These environments have been established as particularly challenging benchmarks for exploration algorithms due to their complex state spaces and extremely sparse reward mechanisms.For these experiments, R2D2 (Recurrent Replay Distributed DQN) served as the base algorithm, combined separately with NGU and LMGT.Performance was compared across various training frame quantities.</p>
<p>Experimental results (as shown in Figure 3 and Table 2) reveal that LMGT+R2D2 progressively achieves superior performance in both environments as training progresses.After 3.5×10 10 frames of training, LMGT+R2D2 attains an average reward of 6503.5 in the Pitfall environment, surpassing NGU+R2D2 (5973.4) by approximately 8.9% and exceeding the baseline R2D2 implementation (3613.5)by approximately 80.0%.In the more challenging Montezuma's Revenge environment, the performance differential increases markedly: LMGT+R2D2 achieves a reward of 12365.5, exceeding NGU+R2D2 (9049.4) by approximately 36.6% and outperforming baseline R2D2 (2687.2) by a factor of 4.6.</p>
<p>Perhaps more significant are the performance disparities observed during early training phases.With merely 0.5×10 10 training frames, LMGT+R2D2 achieves a reward of 1535.5 in Pitfall, nearly double that of NGU+R2D2 (786.9).Similarly, in Montezuma's Revenge, LMGT+R2D2 (5635.4)substantially outperforms NGU+R2D2 (4818.5).These early-stage advantages indicate that LMGT, leveraging prior knowledge embedded in language models, can more rapidly identify high-value regions, thereby significantly enhancing exploration efficiency.</p>
<p>Comprehensive Analysis and Discussion</p>
<p>Our comparative experiments with RUDDER and NGU demonstrate LMGT's substantial advantages in addressing both core reinforcement learning challenges.These advantages derive from LMGT's distinctive operational mechanism: language models not only provide structural environmental knowledge but also guide exploration through intelligent reward shifting, simultaneously optimizing both credit assignment processes and exploration strategies.</p>
<p>For delayed reward problems, LMGT utilizes language model-based action value assessment to provide more direct credit assignment mechanisms, enabling agents to rapidly identify critical decision points and substantially reducing required training episodes.For exploration challenges, LMGT employs language model evaluations of state-action pairs to assign positive reward shifts to potentially valuable trajectories, thus avoiding inefficient exploration and achieving more optimal exploration-exploitation balance in complex environments.</p>
<p>It is particularly noteworthy that LMGT's advantages become increasingly pronounced as task complexity increases.This scalability arises because language models based on Transformer architectures maintain relatively constant inference speeds regardless of task complexity, whereas traditional methods typically require exponentially increasing exploration time as environments become more complex.This characteristic renders LMGT especially suitable for high-dimensional environments and long-horizon decision problems.</p>
<p>In conclusion, by integrating the cognitive capabilities of large language models with reinforcement learning frameworks, LMGT provides an innovative approach that simultaneously addresses credit assignment and exploration efficiency challenges.This integration opens new avenues for reinforcement learning applications in resource-constrained scenarios, potentially transforming how complex sequential decision problems are approached.</p>
<p>Evaluation of LMGT among Various Reinforcement Learning Algorithms and Environments</p>
<p>Main experiment</p>
<p>This section undertakes a comprehensive evaluation of the efficacy of our LMGT framework across various RL environments, employing diverse RL algorithms.Table 3 presents a comprehensive overview of our experimental results.The values in the table indicate the performance gain in terms of reward achieved by LMGT compared to the baseline method, referred to as the 'boosted reward'.Notably, red figures denote scenarios where our proposed method underperforms compared to the baseline.Please be advised that all rewards presented in the results correspond to the environments' intrinsic rewards.The environments are identified with their observable states fur-  nished to the LLMs in two distinct formats: a standardized numerical representation, denoted as "box" (e.g., a tuple encapsulating information on object positions), and a more intuitively comprehensible visual format referred to as "human" (such as a screenshot of the current frame).Our metric for assessing our approach against baseline methods is the "average reward of the model after a fixed number of training time steps".Specifically, agents are trained separately using our method and baseline techniques within the same environment, and the trained weights are preserved after a predefined number of time steps.Subsequently, we evaluate the performance of models trained using different methods, employing an equivalent number of training time steps in the same environment, while comparing their average rewards.For a more intuitive representation of the results, Figure 4 illustrates the learning trajectories of agents under various experimental conditions, providing a visual comparison of performance over time steps.</p>
<p>Throughout these experiments, we maintained a consistent choice of LLM and prompt techniques.Specifically, two prompt methods were employed: CoT and Zero-shot prompt, to formulate our prompts.The 4bit quantized version of the Vicuna-30B model [45], with GPTQ quantization [46], was utilized as our guiding LLM within our framework.This model is utilized to assess the quality of agent behavior in distinct en-vironmental states.We contend that this configuration optimizes the performance of our framework, and we will delve into the influence of different prompt techniques and LLMs on the framework's performance in other parts of this section.</p>
<p>RL environments are seldom conveyed through purely textual descriptions; thus, LLMs necessitate multimodal capabilities to process such information.Common LLMs such as Llama, Llama2, and Vicuna do not inherently support multimodal functionality.To address this limitation, we adopted a pipeline model approach, where multiple single-modal models work synergistically, with each model responsible for processing specific data types and passing results to the next model to accomplish tasks.In our experiments, we integrated LLaVA [22] as the image processing model preceding the LLM.Therefore, in the aforementioned experiments, LLaVA was integrated with the Vicuna-30B model and operated collaboratively, equipping our "scorer" with image processing capabilities.Notably, our integration method does not simply convert the environment into text through an image captioning model before transmission to LLMs.Instead, it directly inputs embeddings representing the environmental state into the LLMs.This approach allows us to preserve more meaningful information to support LLMs in their decision-making processes.Table 3 illustrates that our framework consistently outperforms baseline methods across a majority of environments and various RL algorithms.It effectively achieves a trade-off between exploration and exploitation in RL methods, enabling agents to acquire skills more rapidly, thus leading to cost savings during training.Moreover, we observed that our framework's performance is relatively inferior in tasks necessitating the utilization of pipeline models to process visual information compared to tasks that exclusively involve text information processing.In essence, if Vicuna-30B is required to handle additional image information from LLaVA, its performance tends to deteriorate.An intriguing observation proposed in [47] suggests that attempting to enforce strict adherence of the LLM to response templates results in reduced performance across all scenarios.We hypothesize that both these scenarios signify a degradation in LLM performance in multitask settings [48].Within our framework, "understanding extracted image information" and "assigning scores to agent behavior based on a combination of different information" represent distinct tasks, while the phenomenon mentioned in [47] pertains to "providing responses based on prompts" and "formatting responses as required" as two separate tasks.</p>
<p>We also investigated the influence of different prompt methods on the performance of our framework.Sim-ilar to the previous experiments, while keeping other variables constant, we continued to employ the 4-bit quantized version of the Vicuna-30B model as our LLM and the A2C algorithm as our RL technique.We conducted tests on two representative environments, and the experimental results are presented in Table 4.It is assumed that prompts in the table all inherently contain prior knowledge.For both simple (Cart Pole) and complex (Blackjack) environments, the most effective prompt method was found to be CoT.CoT particularly excelled in enhancing performance for complex tasks.We discovered that the model often overlooked the provided information and resulted in a uniform outcome unless explicitly instructed to employ hierarchical thinking in challenging tasks.Furthermore, we observed that merely assigning a simple name to the model scarcely enhanced its performance.</p>
<p>An intriguing observation emerged when comparing prompt methods on our task: the "Zero-shot prompt" method outperformed the "Few-shot prompt" method.Few-shot prompts often led the Vicuna-30B model to generate results with a sense of "illusion".Vicuna-30B frequently produced arbitrary extensions based on the provided examples.Furthermore, we observed that incorporating prior knowledge into the prompts can lead to an improvement in the performance of our framework, despite the fact that the weights within the Vicuna-30B model already encompass the requisite knowledge for addressing the challenges presented by the environment.</p>
<p>We also conducted experiments to assess the performance of different LLMs serving as the "evaluators" within our framework, thereby partially evaluating their inferential capabilities, we opted for the Blackjack environment for testing.The experimental results are presented in Table 6."Vicuna-30B-4bit-GPTQ" indicates the use of the Vicuna model, with a size of 30 billion parameters, employing GPTQ quantization with 4bit precision."Llama2-13B-8bit" signifies the use of the Llama2 model with a size of 13 billion parameters, without any quantization, running in 8-bit floating-point precision.We kept the prompt statements constant by using CoT and Zero-shot prompt, with the inclusion of prior knowledge, and fixed the RL algorithm (A2C).</p>
<p>From Table 6, we observe that the precision of quantization has a limited impact on inferential capabilities in the same model.A well-considered quantization method can effectively mitigate the performance loss resulting from quantization.Model size, on the other hand, has a more significant influence on a model's inferential capabilities, a minimally sized language model fails to yield any significant improvement.Additionally, models of identical scale exhibit variations in their inferential capabilities, confined solely within the scope of our framework.</p>
<p>Ablation study</p>
<p>In Section 4.3.1,we noted that requiring a LLM to perform multiple tasks simultaneously within a single query might compromise its capability [47].Based on this principle, we designed an ablation experiment to test the performance of LMGT in both the 'box' and 'human' formats within a more visually complex Blackjack environment.For the latter, recognizing card infor-mation and converting it into numerical data constitutes a highly specialized task.When the LLM must first process complex visual data, its reasoning ability diminishes.The experimental results, as shown in Table 5, reveal that LMGT's performance in the 'human' format fluctuates around the baseline, indicating performance deterioration in this context.This finding demonstrates that our LMGT effectively leverages the LLM's capabilities to guide the agent's learning: when the LLM's capability is insufficient to provide guidance, the agent's performance reverts to the baseline.</p>
<p>Verification Experiments in the Housekeep Environment</p>
<p>To assess the efficacy of LMGT in complex realworld robotic tasks, we conducted experiments using the Housekeep environment [24].Housekeep is an embodied agent simulation where the robot must organize a household by correctly placing misplaced objects into their appropriate containers.The agent must deduce the correct object-container pairings without explicit instructions, based on mappings derived from crowdsourced data on typical object-container associations.We compared LMGT with the baseline method Active Pre-Training (APT) [50].APT learns behaviors and representations through active exploration of novel states in reward-free environments, utilizing non-parametric entropy maximization in an abstract representation space.This approach circumvents the challenges of density modeling, enabling better scalability in environments with high-dimensional observations, such as those with image inputs.To demonstrate the significance of Visual Instruction Tuning (illustrated in Figure 1), we introduced ELLM [51] as an additional baseline.We evaluated ELLM under two conditions: a) Using ground truth simulator states as LLM input, representing the theoretical optimal performance achievable through textual en-    For consistency, we adopted the experimental setup from ELLM, focusing on a simplified Housekeep subset comprising four distinct scenes.Each scene contained one room with five misplaced objects and various potential containers.</p>
<p>Figure 5 presents the comparative results of LMGT and the baseline methods in the Housekeep environment.LMGT consistently outperformed APT, highlighting the capacity of LLMs to effectively guide agent learning in complex scenarios.Notably, LMGT's performance surpassed that of ELLM when the latter employed learned descriptors.Despite similar limitations in accessing ground truth environmental states, LMGT, leveraging Visual Instruction Tuning, retained more information conducive to LLM decision-making.Moreover, despite ELLM utilizing ground truth values in certain settings, our LMGT was able to match, and in some scenarios even surpass, ELLM's performance without access to ground truth information.</p>
<p>Experiments in Industrial Recommendation Scenarios</p>
<p>In this section, we further apply our framework to Google's RL recommendation algorithm, SlateQ [25], to elucidate its potential in industrial applications.</p>
<p>Simulation environment</p>
<p>RecSim [44] is a simulation platform for constructing and evaluating recommendation systems that naturally support sequential interactions with users.Developed by Google, it simulates users and environments to assess the effectiveness and performance of recommendation algorithms.We employ RecSim to create an environment that reflects user behavior and item structure to evaluate our LMGT framework.</p>
<p>We construct a "Choc vs. Kale" recommendation scenario, where the goal is to maximize user satisfaction and engagement over the long term by recommending a certain proportion of "chocolate" and "kale" elements.In this scenario, the "chocolate" element represents content that is interesting but not conducive to long-term satisfaction, while the "kale" element represents relatively less exciting but beneficial content for long-term satisfaction.The recommendation algorithm needs to balance these two elements to achieve maximized longterm user satisfaction.In our scenario, the entire simulation environment consists primarily of document models and user models.The document model serves as the main interface for interaction between users and the recommendation system (agent) and is responsible for selecting a subset of documents from a database containing a large number of documents to deliver to the recommendation sys-tem.The user model simulates user behavior and reacts to the slates provided by the recommendation system.</p>
<p>The database in the document model essentially serves as a container for observable and unobservable features of underlying documents.In this scenario, document attributes are modeled as continuous features with values in the range of [0, 1], referred to as the Kaleness scale.A document assigned a score of 0 represents pure "chocolate", which is intriguing but regrettable, whereas a document with a score of 1 represents pure"kale", which is less exciting but nutritious.Additionally, each document has a unique integer ID, and the document model selects N candidate documents in sequential order based on their IDs.</p>
<p>The user model includes both observable and unobservable user features.Based on these features, the model responds to the received slate according to certain rules.Each user is characterized by the features of net kale exposure (nke t ) and satisfaction (sat t ), which are associated through the sigmoid function σ to ensure that sat t is constrained within a bounded range.Specifically, the satisfaction level is modeled as a sigmoid function of the net kale exposure, which determines the user's satisfaction with the recommended slate:
sat t = σ(τ • nke t )(1)
Where τ is a user-specific sensitivity parameter.Upon receiving a Slate from the recommendation system, users select items to consume based on the Kaleness scale of the documents.Specifically, for item i, the probability of it being chosen is determined by p ∼ e 1−kaleness(i) .After making their selections, the net kale exposure evolves as follows:
nke t+1 = β • nke t + 2(k i − 1/2) + N(0, η)(2)
Where β represents a user-specific memory discount, while k i corresponds to the kaleness of the selected item, and η denotes some noise standard deviation.Lastly, our focus will be on the user's engagement s i , i.e. a lognormal distribution with parameters linearly interpolating between the pure kale response (µ k , σ k ) and the pure choc response (µ c , σ c ):
s i ∼ logN(k i µ k + (1 − k i )µ c , k i σ k + (1 − k i )σ c ) (3)
The satisfaction variable sat t represents the sole dynamic component of the user's state, and thus, we generate the user's observable state based on it.In the simulation, user satisfaction is modeled and computed as a latent state.However, to simulate real-world scenarios, we map the latent state to an observable state by introducing noise to account for user uncertainty.</p>
<p>Experimental results</p>
<p>The experimental configurations for LMGT and the baseline SlateQ approach are identical.We independently trained agents using both our method and the baseline SlateQ, evaluating their performance over an equivalent number of episodes.In the "Choc vs. Kale" scenario, each episode consists of a set number of time steps.As illustrated in Table 7, our results conclusively show that our approach significantly accelerates skill acquisition in agents, enabling them to adeptly navigate the complex challenges of the environment.This rapid development of expertise leverages prior knowledge and skillfully balances the tension between exploration and exploitation.As a consequence, there is an efficient use of sample resources, leading to a marked decrease in the training costs associated with RL models.Nonetheless, our study has its limitations.A notable omission is the analysis of computational resources required for integrating LLMs into the training process.Future research will focus on optimizing the use of computational resources in RL training by applying prior knowledge while addressing the heightened resource demand that comes with incorporating LLMs.Additionally, we have not yet formulated a theoretical framework to explain how LLMs dynamically influence reward structures.Addressing this represents a promising avenue for future research.</p>
<p>Conclusion</p>
<p>We introduce LMGT, a novel framework that harnesses the extensive knowledge and sophisticated capabilities of LLMs to enhance RL.LMGT effectively balances exploration and exploitation in RL by leveraging LLMs' domain expertise and information processing abilities, seamlessly integrating with existing RL workflows.Our comprehensive experiments across diverse settings and algorithms demonstrate LMGT's efficacy in optimizing the exploration-exploitation trade-off while simultaneously reducing training costs.To validate its practical applicability, we successfully implemented LMGT in the complex Housekeep robotic environment, highlighting its potential for real-world embodied AI applications.While our study yields promising results, it also opens avenues for future research.Specifically, we need to investigate the computational impact of integrating LLMs into the RL training process.Future work will focus on striking a balance between the enhanced capabilities offered by LLMs and the optimization of computational resource allocation in RL training environments.</p>
<p>Limitations and Future Work</p>
<p>Despite the promising results demonstrated by LMGT across various environments, our research is not without limitations.This section discusses these constraints and outlines potential directions for future research.</p>
<p>Current Limitations</p>
<p>Our study has several notable limitations.First, we have not comprehensively analyzed the computational overhead introduced by integrating LLMs into the RL training process.While LMGT demonstrates improved sample efficiency, the computational resources required for LLM inference during training represent a significant consideration for practical deployments.This trade-off between enhanced learning performance and computational cost warrants further investigation.</p>
<p>Second, the efficacy of LMGT is inherently dependent on the quality and relevance of prior knowledge embedded within the LLM.In domains where LLMs possess insufficient or inaccurate knowledge, the guidance provided may be suboptimal or potentially misleading.Our experiments in the Blackjack environment with the 'human' format (Table 5) illustrate this limitation, where performance deteriorated when the LLM was tasked with both processing complex visual information and providing guidance simultaneously.</p>
<p>Third, our current implementation does not fully address the theoretical foundations explaining how LLMs dynamically influence reward structures over time.While we observe empirical improvements, a more rigorous theoretical framework would enhance our understanding of LMGT's underlying mechanisms.</p>
<p>Finally, while we have evaluated LMGT across various environments, our testing in highly complex, dynamic, and partially observable environments remains limited.The scalability of our approach to such scenarios requires further validation.</p>
<p>Future Directions</p>
<p>Extending LMGT to More Complex Situations</p>
<p>We envision several promising approaches to extend LMGT to more complex situations.For tasks with high complexity, implementing a hierarchical structure where top-level LLMs provide strategic guidance while lower-level LLMs offer tactical reward shifts could effectively manage complexity through task abstraction at different levels.In scenarios with computational constraints, knowledge distillation from larger LLMs into smaller, specialized models could enable efficient deployment while maintaining guidance quality.</p>
<p>Complex real-world environments often involve diverse sensory inputs.Enhancing LMGT with multimodal fusion mechanisms that coherently integrate visual, audio, and proprioceptive information before generating reward guidance would address this challenge.This integration could be complemented by an adaptive mechanism that dynamically adjusts the magnitude and frequency of reward shifts based on the agent's learning progress and task complexity, optimizing the balance between LLM guidance and autonomous exploration.</p>
<p>For rapidly changing complex environments, extending LMGT with meta-learning capabilities would enable the framework to quickly adapt its reward guidance strategy to new situations based on limited experience.Additionally, in environments with partial observability, incorporating uncertainty estimation in LMGT's reward shifts would provide more conservative guidance when environmental understanding is limited.</p>
<p>For extremely complex scenarios, a hybrid approach where human experts periodically review and refine the LLM's reward guidance could create a continuous improvement cycle, combining the scalability of automated systems with human judgment.</p>
<p>Theoretical Advancements</p>
<p>Future research should focus on developing a theoretical framework that explains how LLM-guided reward shifts influence the convergence properties and exploration patterns of RL algorithms.This would include analyzing the relationship between reward shifts and value function initialization, as well as studying how different types of prior knowledge affect explorationexploitation dynamics.</p>
<p>Computational Efficiency</p>
<p>Investigating methods to reduce the computational overhead of LMGT through techniques such as model compression, selective inference, and asynchronous guidance represents another important research direction.Developing strategies that balance the benefits of LLM guidance with computational efficiency would enhance the practical applicability of our framework.</p>
<p>Multi-Agent and Collaborative Settings</p>
<p>Extending LMGT to multi-agent reinforcement learning scenarios, where LLMs could provide guidance on collaboration strategies or mediate between competing objectives, offers exciting possibilities.Research in this direction could lead to more effective solutions for complex multi-agent problems in areas such as autonomous vehicle coordination, distributed robotics, and strategic games.</p>
<p>Personalized and Contextual Guidance</p>
<p>Developing mechanisms for LMGT to provide personalized guidance based on the specific characteristics and learning history of individual agents could further enhance sample efficiency.This would involve adapting the reward shifts based on the agent's strengths, weaknesses, and learning progress, creating a more tailored learning experience.</p>
<p>By addressing these limitations and pursuing these future directions, we believe LMGT can evolve into a more robust, efficient, and widely applicable framework for enhancing reinforcement learning across diverse domains and applications.</p>
<p>Acknowledgement</p>
<p>This work is supported by the National Natural Science Foundation of China (62102241), the Shanghai Municipal Natural Science Foundation (23ZR-1425400).</p>
<p>Appendix A. Prompt demo</p>
<p>We provide several prompt statements for demonstration.Figures A.6 and A.7 depict the prompt statement designs for "incorporating prior knowledge" and "excluding prior knowledge", respectively.In Figure A.8, it is shown that when LLMs are not required to engage in CoT, they often neglect the provided prompt and produce a consistent output regardless of the circumstances.Figure A.9 exhibits that when LLMs are given a complete answer example in the prompt, they tend to generate nonsensical content by imitating the example.Figure A.10 illustrates the inquiries made to the Vicuna-30B model, with the generated results (highlighted in red) indicating that the weights of the Vicuna-30B model include significant prior knowledge related to Blackjack.</p>
<p>(a) Captioner.(b) End to End.</p>
<p>Figure 1 :
1
Figure 1: Schematic Representation of Diverse Approaches to Processing Environmental Information.It is evident that leveraging Visual Instruction Tuning in an end-to-end framework significantly enhances the capacity of LLMs to assimilate more pertinent information for informed decision-making, compared to the captioner-based approach.</p>
<p>Algorithm 1 : 3 for
13
Language Model Guided Tradeoffs Initialize: Initialize the Reinforcement Learning environment; Initialize the Q-value function or policy network; Initialize the experience replay buffer; Input : Total number of training episodes: N; Maximum number of steps per episode: M; Initial behavior policy: P; Target policy: P' ; Output : Learned policy or value function 1 for Episode = 1 to N do 2 Reset the environment and obtain the initial state s; Step = 1 to M do 4</p>
<p>Figure 3 :
3
Figure 3: Performance comparison on challenging exploration Atari environments.The figures present reward curves demonstrating our LMGT method's effectiveness in (a) Pitfall and (b) Montezuma's Revenge -two environments characterized by extremely sparse rewards and complex exploration requirements.Our approach shows significant performance gains over baseline methods in these notoriously difficult benchmarks where conventional RL algorithms typically struggle to make progress.</p>
<p>Figure 4 :
4
Figure 4: Results of experiments conducted across varying settings.It is important to note that all rewards in the Pendulum environment are negative.To enhance the visualization, each reward value shown in the graphs has been increased by an offset of 2000.</p>
<p>Figure 5 :
5
Figure 5: Results from comparative experiments conducted within the Housekeep environment.Correct arrangement success rates on 4 object-receptacle task sets.</p>
<p>Figure A. 6 :
6
Figure A.6: A salient example of a "prior-knowledge-inclusive prompt statement".</p>
<p>Figure A. 7 :
7
Figure A.7: A salient example of a "prior-knowledge-exclusive prompt statement".</p>
<p>Figure A. 8 :
8
Figure A.8: If the model is not required to engage in chain thinking, it will tend to simply provide a fixed result.</p>
<p>Figure A. 9 :
9
Figure A.9: After providing a few examples, the LLM produced some results with hallucinatory elements.</p>
<p>Figure A. 10 :
10
Figure A.10: The weights in the model already encompass knowledge about the game of Blackjack.</p>
<p>Table 1 :
1
Performance comparison in watch repair task.Evaluation of LMGT against baseline methods.All reported times include the inference time of the language model.
MethodEpisodes Time (sec)TD71,823427MC221,770530RUDDER2,029171LMGT+TD (ours)417114Lower values indicate better performance in both metrics.</p>
<p>Table 2 :
2
Performance of LMGT and baseline algorithms in hard-exploration Atari games.Game scores at different training stages (measured in frames ×10 10 ).LMGT is our proposed method.Best results are highlighted in bold.
Environment AlgorithmEarly TrainingLate Training00.51.01.52.02.53.03.5R2D20.0 103.595.3325.62786.33586.73548.63613.5PitfallNGU+R2D20.0 786.9 3233.3 4020.26400.05494.65120.05973.4LMGT+R2D2 (ours) 0.0 1535.5 5486.3 6348.26125.66354.36535.26503.5Montezuma's RevengeR2D2 NGU+R2D2 LMGT+R2D2 (ours) 0.0 5635.4 8957.3 11257.3 10985.7 12093.9 12463.6 12365.5 0.0 1235.6 2567.2 2465.8 2500.7 2754.6 2703.5 2687.2 0.0 4818.5 6369.2 7165.7 8236.5 9810.0 9966.7 9049.4</p>
<p>Table 3 :
3
Performance gains achieved by LMGT across different settings.Values represent the boosted reward compared to baseline methods.Negative values (shown in red) indicate performance degradation.
Observable Environmental State FormatEnvironment AlgorithmBoxHumann=100 n=1000 n=10000 n=100 n=1000 n=10000DQN1.051.902.600.000.751.70Cart PolePPO132.4511.8517.902.05-8.503.30A2C5.8038.200.000.30-1.404.20SAC358.73 1338.206.5133.48326.241.67PendulumTD3 PPO36.88 1177.35 302.75 47.0312.56 102.17 209.15 2.84569.88 14.682.73 12.16A2C222.4111.86-1.3533.733.12-12.32Higher values indicate greater improvement by LMGT over the baseline method.</p>
<p>Table 4 :
4
Impact of various prompt strategies on reinforcement learning performance.Higher values indicate better performance, with the best results in each column shown in bold.
Environment Prompt StrategyTime Stepsn=100 n=1000n=10000Baseline37.7042.70125.90CoT [49]42.1074.00126.00Zero-shot prompt [42]38.7068.90126.00Cart PoleFew-shot prompt [37]38.1065.00125.10Name [47]37.1042.90125.90CoT+Zero-shot prompt (w/o prior knowledge)42.0077.10126.10CoT+Zero-shot prompt (w/ prior knowledge)42.5078.90127.00Baseline-0.200.200.32CoT [49]0.100.280.45Zero-shot prompt [42]0.100.280.32BlackjackFew-shot prompt [37]-0.200.200.32Name [47]-0.200.200.33CoT+Zero-shot prompt (w/o prior knowledge)0.000.250.40CoT+Zero-shot prompt (w/ prior knowledge)0.120.300.45
CoT = Chain of Thought."w/ prior knowledge" indicates the inclusion of domain-specific knowledge in the prompt, while "w/o" indicates its exclusion.</p>
<p>Table 5 :
5
Ablation studies in the Blackjack environment.Performance comparison of different algorithms under varying state formats and training durations.Positive values indicate improvement, while negative values (in red) indicate performance degradation.
Observable Environmental State FormatBoxHumanAlgorithmn=100 n=1000 n=10000 n=100 n=1000 n=10000DQN0.080.030.180.010.00-0.01PPO0.050.200.15-0.010.12-0.10A2C0.320.120.130.01-0.03-0.01vironmental descriptions; b) Using learned descriptorsto characterize the environment, exemplifying the prac-tical performance of captioner-based methodologies.</p>
<p>Table 6 :
6
Impact of different LLMs on framework performance.Results across various time steps for the Blackjack environment, with higher values indicating better performance.
Model Family ConfigurationTime Stepsn=100 n=1000 n=100007B-4bit-0.200.180.327B-8bit-0.200.180.327B-16bit-0.200.180.327B-4bit-GPTQ-0.200.180.32Vicuna13B-4bit-0.200.180.3213B-8bit0.100.180.3413B-16bit0.100.180.3613B-4bit-GPTQ0.100.180.3430B-4bit-GPTQ0.120.300.457B-4bit-0.300.160.327B-8bit-0.300.160.327B-16bit-0.300.160.32Llama27B-4bit-GPTQ-0.300.160.3213B-4bit0.100.160.3213B-8bit0.100.160.3213B-16bit0.120.160.3413B-4bit-GPTQ0.120.160.34Best results are highlighted in bold. All experiments conducted inthe Blackjack environment.</p>
<p>Table 7 :
7
Performance comparison of LMGT framework and baseline SlateQ in recommendation tasks.Results show average rewards across different training episodes, with LMGT consistently outperforming the baseline method.
MethodMetricNumber of Episodesn=10n=50n=5000SlateQA.R.831.082913.528 1127.136LMGT (ours) A.R.933.624 1125.171 1150.251Perf. GainImprovement +102.542 +211.643+23.115A.R.: Average Reward, Perf. Gain: Performance Gain.</p>
<p>Reinforcement learning in swarm-robotics for multi-agent foraging-task domain. M Yogeswaran, S Ponnambalam, G Kanagaraj, 10.1109/SIS.2013.66151542013 IEEE Symposium on Swarm Intelligence. 201349</p>
<p>Exploration-exploitation dilemma in reinforcement learning under various form of prior knowledge. R Fruit, Sciences et Technologies; CRIStAL UMR. 91892019Université de Lille 1Ph.D. thesis</p>
<p>A comprehensive survey of multiagent reinforcement learning. L Busoniu, R Babuska, B De Schutter, IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews). 3822008</p>
<p>Reinforcement learning: a survey. L P Kaelbling, M L Littman, A W Moore, J. Artif. Int. Res. 411996</p>
<p>A model-based hybrid soft actor-critic deep reinforcement learning algorithm for optimal ventilator settings. S Chen, X Qiu, X Tan, Z Fang, Y Jin, 10.1016/j.ins.2022.08.028Inf. Sci. 611C2022</p>
<p>Reinforcement Learning: An Introduction. R S Sutton, A G Barto, 2018A Bradford BookCambridge, MA, USA</p>
<p>Is q-learning provably efficient?. C Jin, Z Allen-Zhu, S Bubeck, M I Jordan, Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS'18. the 32nd International Conference on Neural Information Processing Systems, NIPS'18Red Hook, NY, USACurran Associates Inc2018</p>
<p>A tutorial on thompson sampling. D J Russo, B Van Roy, A Kazerouni, I Osband, Z Wen, 10.1561/2200000070Found. Trends Mach. Learn. 1112018</p>
<p>Noveld: a simple yet effective exploration criterion. T Zhang, H Xu, X Wang, Y Wu, K Keutzer, J E Gonzalez, Y Tian, Proceedings of the 35th International Conference on Neural Information Processing Systems, NIPS '21. the 35th International Conference on Neural Information Processing Systems, NIPS '21Red Hook, NY, USACurran Associates Inc2021</p>
<p>Learning about progress from experts. J Bruce, A Anand, B Mazoure, R Fergus, The 11th International Conference on Learning Representations, ICLR'23, OpenReview.net. 2023</p>
<p>Leveraging the inductive bias of large language models for abstract textual reasoning. C M Rytting, D Wingate, Proceedings of the 35th International Conference on Neural Information Processing Systems, NIPS '21. the 35th International Conference on Neural Information Processing Systems, NIPS '21Red Hook, NY, USACurran Associates Inc2021</p>
<p>Language models represent space and time. W Gurnee, M Tegmark, 10.48550/arxiv.2310.02207arXiv:2310.022072024</p>
<p>Geo-fub: A method for constructing an operator-function knowledge base for geospatial code generation with large language models. S Hou, A Zhao, J Liang, Z Shen, H Wu, 10.1016/j.knosys.2025.113624Knowledge-Based Systems. 3191136242025</p>
<p>Agent lumos: Unified and modular training for open-source language agents. D Yin, F Brahman, A Ravichander, K Chandu, K.-W Chang, Y Choi, B Y Lin, 10.48550/arxiv.2311.05657arXiv:2311.056572024</p>
<p>Retroformer: Retrospective large language agents with policy gradient optimization. W Yao, S Heinecke, J C Niebles, Z Liu, Y Feng, L Xue, R R N , Z Chen, J Zhang, D Arpit, R Xu, P Mui, H Wang, C Xiong, S Savarese, The 12th International Conference on Learning Representations. 2024ICLR'24, OpenReview.net</p>
<p>Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory. X Zhu, Y Chen, H Tian, C Tao, W Su, C Yang, G Huang, B Li, L Lu, X Wang, Y Qiao, Z Zhang, J Dai, 10.48550/arxiv.2305.17144arXiv:2305.171442023</p>
<p>Y Deng, X Qiu, X Tan, J Pan, C Jue, Z Fang, Y Xu, W Chu, Y Qi, 10.48550/arxiv.2408.10608arXiv:2408.10608Promoting equality in large language models: Identifying and mitigating the implicit bias based on bayesian theory. 2024</p>
<p>Cognidual framework: Self-training large language models within a dual-system theoretical framework for improving cognitive tasks. Y Deng, X Qiu, X Tan, C Qu, J Pan, Y Cheng, Y Xu, W Chu, 10.1109/ICASSP49660.2025.10887899ICASSP 2025 -2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2025</p>
<p>Transfficformer: A novel transformer-based framework to generate evasive malicious traffic. W Du, J Xue, X Yang, W Guo, D Gu, W Han, 10.1016/j.knosys.2025.113546Knowledge-Based Systems. 3191135462025</p>
<p>Z Xi, W Chen, X Guo, W He, Y Ding, B Hong, M Zhang, J Wang, S Jin, E Zhou, R Zheng, X Fan, X Wang, L Xiong, Y Zhou, W Wang, C Jiang, Y Zou, X Liu, Z Yin, S Dou, R Weng, W Qin, Y Zheng, X Qiu, X Huang, Q Zhang, T Gui, 10.1007/s11432-024-4222-0The rise and potential of large language model based agents: a survey. Jan. 202568</p>
<p>Enhancing named entity recognition with external knowledge from large language model. Q Li, T Xie, J Zhang, K Ma, J Su, K Yang, H Wang, 10.1016/j.knosys.2025.113471Knowledge-Based Systems. 3181134712025</p>
<p>Visual instruction tuning. H Liu, C Li, Q Wu, Y J Lee, Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS '23. the 37th International Conference on Neural Information Processing Systems, NIPS '23Red Hook, NY, USACurran Associates Inc2023</p>
<p>Exploit reward shifting in value-based deep-rl: optimistic curiositybased exploration and conservative exploitation via linear reward shaping. H Sun, L Han, R Yang, X Ma, J Guo, B Zhou, Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS '22. the 36th International Conference on Neural Information Processing Systems, NIPS '22Red Hook, NY, USACurran Associates Inc2022</p>
<p>Y Kant, A Ramachandran, S Yenamandra, I Gilitschenski, D Batra, A Szot, H , 10.1007/978-3-031-19842-7_21Housekeep: Tidying virtual households using commonsense reasoning, in: Computer Vision -ECCV 2022: 17th European Conference. Tel Aviv, Israel; Berlin, HeidelbergSpringer-VerlagOctober 23-27, 2022. 2022Proceedings, Part XXXIX</p>
<p>Slateq: a tractable decomposition for reinforcement learning with recommendation sets. E Ie, V Jain, J Wang, S Narvekar, R Agarwal, R Wu, H.-T Cheng, T Chandra, C Boutilier, Proceedings of the 28th International Joint Conference on Artificial Intelligence, IJCAI'19. the 28th International Joint Conference on Artificial Intelligence, IJCAI'19Macao, ChinaAAAI Press2019</p>
<p>Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, A Zhou, P Abbeel, S Levine, 10.48550/arxiv.1801.01290arXiv:1801.012902018</p>
<p>Understanding the impact of entropy on policy optimization. Z Ahmed, N L Roux, M Norouzi, D Schuurmans, Proceedings of the 36th International Conference on Machine Learning, ICML 2019. K Chaudhuri, R Salakhutdinov, the 36th International Conference on Machine Learning, ICML 2019Long Beach, California, USAPMLR9-15 June 2019. 201997of Proceedings of Machine Learning Research</p>
<p>Curiosity-driven exploration by self-supervised prediction. D Pathak, P Agrawal, A A Efros, T Darrell, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningSydney, NSW, Australia201770</p>
<p>Large-scale study of curiosity-driven learning. Y Burda, H Edwards, D Pathak, A J Storkey, T Darrell, A A Efros, The 7th International Conference on Learning Representations, ICLR'19, OpenReview.net. 2019</p>
<p>A provably efficient sample collection strategy for reinforcement learning. J Tarbouriech, M Pirotta, M Valko, A Lazaric, Proceedings of the 35th International Conference on Neural Information Processing Systems, NIPS '21. the 35th International Conference on Neural Information Processing Systems, NIPS '21Red Hook, NY, USACurran Associates Inc2021</p>
<p>Exploration in reinforcement learning with deep covering options. Y Jinnai, J W Park, M C Machado, G D Konidaris, The 8th International Conference on Learning Representations, ICLR'20, OpenReview.net. 2020</p>
<p>Rank the episodes: A simple approach for exploration in procedurally-generated environments. D Zha, W Ma, L Yuan, X Hu, J Liu, The 9th International Conference on Learning Representations, ICLR'21, OpenReview.net. 2021</p>
<p>Plan-based relaxed reward shaping for goal-directed tasks. I Schubert, O S Oguz, M Toussaint, The 9th International Conference on Learning Representations, ICLR'21, OpenReview.net. 2021</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17. the 31st International Conference on Neural Information Processing Systems, NIPS'17Red Hook, NY, USACurran Associates Inc2017</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, 2018</p>
<p>10.48550/arxiv.2303.08774arXiv:2303.08774OpenAI, Gpt-4 technical report. 2023</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, 10.48550/arxiv.2302.13971arXiv:2302.13971Llama: Open and efficient foundation language models. 2023</p>
<p>. R Thoppilan, D D Freitas, J Hall, N Shazeer, A Kulshreshtha, H.-T Cheng, A Jin, T Bos, L Baker, Y Du, Y Li, H Lee, H S Zheng, A Ghafouri, M Menegali, Y Huang, M Krikun, D Lepikhin, J Qin, D Chen, Y Xu, Z Chen, A Roberts, M Bosma, V Zhao, Y Zhou, C.-C Chang, I Krivokon, W Rusch, M Pickett, P Srinivasan, L Man, K Meier-Hellstern, M R Morris, T Doshi, R D Santos, T Duke, J Soraker, B Zevenbergen, V Prabhakaran, M Diaz, B Hutchinson, K Olson, A Molina, E Hoffman-John, J Lee, L Aroyo, R Rajakumar, A Butryna, M Lamm, V Kuzmina, J Fenton, A Cohen, R Bernstein, R Kurzweil, B Aguera-Arcas, C Cui, M Croak, E Chi, Q Le, Lamda , 10.48550/arxiv.2201.08239arXiv:2201.082392022Language models for dialog applications</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E P Xing, H Zhang, J E Gonzalez, I Stoica, Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS '23. the 37th International Conference on Neural Information Processing Systems, NIPS '23Red Hook, NY, USACurran Associates Inc2023</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. Y Bai, A Jones, K Ndousse, A Askell, A Chen, N Das-Sarma, D Drain, S Fort, D Ganguli, T Henighan, N Joseph, S Kadavath, J Kernion, T Conerly, S El-Showk, N Elhage, Z Hatfield-Dodds, D Hernandez, T Hume, S Johnston, S Kravec, L Lovitt, N Nanda, C Olsson, D Amodei, T Brown, J Clark, S Mccandlish, C Olah, B Mann, J Kaplan, 10.48550/arxiv.2204.05862arXiv:2204.058622022</p>
<p>SaFER: A robust and efficient framework for fine-tuning BERT-based classifier with noisy labels. Z Qi, X Tan, C Qu, Y Xu, Y Qi, 10.18653/v1/2023.acl-industry.38doi:10.18653Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. S Sitaram, B Beigman Klebanov, J D Williams, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20235Industry Track)</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS '22. the 36th International Conference on Neural Information Processing Systems, NIPS '22Red Hook, NY, USACurran Associates Inc2022</p>
<p>RUDDER: return decomposition for delayed rewards. J A Arjona-Medina, M Gillhofer, M Widrich, T Unterthiner, J Brandstetter, S Hochreiter, 2019Curran Associates IncRed Hook, NY, USA</p>
<p>Recsim: A configurable simulation platform for recommender systems. E Ie, C Hsu, M Mladenov, V Jain, S Narvekar, J Wang, R Wu, C Boutilier, 10.48550/arxiv.1909.04847arXiv:1909.048472019</p>
<p>W.-L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, I Stoica, E P Xing, Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Mar. 2023</p>
<p>Gptq: Accurate post-training quantization for generative pre-trained transformers. E Frantar, S Ashkboos, T Hoefler, D Alistarh, 10.48550/arxiv.2210.17323arXiv:2210.173232023</p>
<p>Large language models in the workplace: A case study on prompt engineering for job type classification. B Clavié, A Ciceu, F Naylor, G Soulié, T Brightwell, 10.1007/978-3-031-35320-8_1Natural Language Processing and Information Systems: 28th International Conference on Applications of Natural Language to Information Systems, NLDB'23. Berlin, HeidelbergSpringer-Verlag2023</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, The 9th International Conference on Learning Representations, ICLR'21. 2021</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS '22. the 36th International Conference on Neural Information Processing Systems, NIPS '22Red Hook, NY, USACurran Associates Inc2022</p>
<p>Behavior from the void: unsupervised active pre-training. H Liu, P Abbeel, Proceedings of the 35th International Conference on Neural Information Processing Systems, NIPS '21. the 35th International Conference on Neural Information Processing Systems, NIPS '21Red Hook, NY, USACurran Associates Inc2021</p>
<p>Guiding pretraining in reinforcement learning with large language models. Y Du, O Watkins, Z Wang, C Colas, T Darrell, P Abbeel, A Gupta, J Andreas, International Conference on Machine Learning, ICML 2023. A Krause, E Brunskill, K Cho, B Engelhardt, S Sabato, J Scarlett, Honolulu, Hawaii, USAPMLR23-29 July 2023. 2023202of Proceedings of Machine Learning Research</p>            </div>
        </div>

    </div>
</body>
</html>
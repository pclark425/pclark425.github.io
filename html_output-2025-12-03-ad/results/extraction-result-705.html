<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-705 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-705</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-705</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-fbda91cfacd2b792794fb726e9417aef58480c72</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/fbda91cfacd2b792794fb726e9417aef58480c72" target="_blank">Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study</a></p>
                <p><strong>Paper Venue:</strong> PLoS Medicine</p>
                <p><strong>Paper TL;DR:</strong> Pneumonia-screening CNNs robustly identified hospital system and department within a hospital, which can have large differences in disease burden and may confound predictions.</p>
                <p><strong>Paper Abstract:</strong> Background There is interest in using convolutional neural networks (CNNs) to analyze medical imaging to provide computer-aided diagnosis (CAD). Recent work has suggested that image classification CNNs may not generalize to new data as well as previously believed. We assessed how well CNNs generalized across three hospital systems for a simulated pneumonia screening task. Methods and findings A cross-sectional design with multiple model training cohorts was used to evaluate model generalizability to external sites using split-sample validation. A total of 158,323 chest radiographs were drawn from three institutions: National Institutes of Health Clinical Center (NIH; 112,120 from 30,805 patients), Mount Sinai Hospital (MSH; 42,396 from 12,904 patients), and Indiana University Network for Patient Care (IU; 3,807 from 3,683 patients). These patient populations had an age mean (SD) of 46.9 years (16.6), 63.2 years (16.5), and 49.6 years (17) with a female percentage of 43.5%, 44.8%, and 57.3%, respectively. We assessed individual models using the area under the receiver operating characteristic curve (AUC) for radiographic findings consistent with pneumonia and compared performance on different test sets with DeLong’s test. The prevalence of pneumonia was high enough at MSH (34.2%) relative to NIH and IU (1.2% and 1.0%) that merely sorting by hospital system achieved an AUC of 0.861 (95% CI 0.855–0.866) on the joint MSH–NIH dataset. Models trained on data from either NIH or MSH had equivalent performance on IU (P values 0.580 and 0.273, respectively) and inferior performance on data from each other relative to an internal test set (i.e., new data from within the hospital system used for training data; P values both <0.001). The highest internal performance was achieved by combining training and test data from MSH and NIH (AUC 0.931, 95% CI 0.927–0.936), but this model demonstrated significantly lower external performance at IU (AUC 0.815, 95% CI 0.745–0.885, P = 0.001). To test the effect of pooling data from sites with disparate pneumonia prevalence, we used stratified subsampling to generate MSH–NIH cohorts that only differed in disease prevalence between training data sites. When both training data sites had the same pneumonia prevalence, the model performed consistently on external IU data (P = 0.88). When a 10-fold difference in pneumonia rate was introduced between sites, internal test performance improved compared to the balanced model (10× MSH risk P < 0.001; 10× NIH P = 0.002), but this outperformance failed to generalize to IU (MSH 10× P < 0.001; NIH 10× P = 0.027). CNNs were able to directly detect hospital system of a radiograph for 99.95% NIH (22,050/22,062) and 99.98% MSH (8,386/8,388) radiographs. The primary limitation of our approach and the available public data is that we cannot fully assess what other factors might be contributing to hospital system–specific biases. Conclusion Pneumonia-screening CNNs achieved better internal than external performance in 3 out of 5 natural comparisons. When models were trained on pooled data from sites with different pneumonia prevalence, they performed better on new pooled data from these sites but not on external data. CNNs robustly identified hospital system and department within a hospital, which can have large differences in disease burden and may confound predictions.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e705.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e705.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>frontal_view_label_mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inconsistent frontal/lateral view labels in radiology datasets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Metadata labels indicating frontal versus lateral chest radiographs were inconsistent across datasets, requiring an implemented CNN-based filter to enforce the preprocessing step; this mismatch was identified by manual review and corrected in code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>radiology data preprocessing pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline for preparing chest radiograph images for model training, including selection of frontal views and removal of lateral views.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>dataset documentation / metadata labels (frontal/lateral flags)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>CNN-based preprocessing filter (ResNet-50 classifier) implemented in PyTorch</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>inconsistent/missing metadata labels</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Datasets (NIH vs IU/MSH) had inconsistent or incorrect frontal/lateral labels. The paper's high-level description that NIH data contained only frontal radiographs conflicted with IU and MSH data containing mixed views with inconsistent labels, so the authors implemented a ResNet-50 classifier to identify frontal views rather than relying on dataset metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data preprocessing (view filtering)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>manual review of labels and sample images followed by training a ResNet-50 classifier to detect frontal views</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Classifier accuracy on held-out test splits: MSH test 187/190 correctly classified; IU test 102/102 correctly classified; final dataset filtered to 158,323 frontal radiographs (112,120 NIH, 42,396 MSH, 3,807 IU).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Required insertion of an automated filter step; ensured consistent input to downstream models and avoided training on lateral images. Without this, training/test splits could include mixed-view artifacts harming performance. Quantitatively, the preprocessing produced the final dataset of 158,323 images used for all experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Inconsistent labels observed in IU and MSH; the classifier validated on small manually labeled sets (IU: 402 labeled, MSH: 490 labeled) and achieved near-perfect test accuracy on those samples.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>inconsistent dataset metadata and labeling practices across contributing hospital systems (heterogeneous data annotation pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Train and apply a supervised frontal-view classifier to filter images; manual review to create small ground-truth sets for the classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>High — near-perfect test accuracies on the held-out frontal/lateral classification test sets (MSH 187/190; IU 102/102), enabling consistent preprocessing; effect on downstream generalization quantified indirectly by using the filtered dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep learning / medical imaging preprocessing</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e705.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e705.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>nlp_labeling_discrepancy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Heterogeneous NLP labeling pipelines and labeling thresholds across datasets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Different natural language processing and manual annotation methods were used to produce pathology labels for each dataset (proprietary parse-tree NLP for NIH, Lasso n-gram NLP trained on 405 reports for MSH, and manual curation for IU), producing large prevalence differences and label inconsistencies that affected model behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>label generation / annotation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The process and code used to turn radiology reports into binary image-level pathology labels via different NLP pipelines or manual labeling depending on the dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>dataset labeling description / report-to-label NLP specification</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>three label pipelines: proprietary parse-tree NLP (NIH), Lasso logistic regression on 1-2 gram bag-of-words (MSH), manual curator labels (IU)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>different algorithm variant / inconsistent labeling thresholds (implementation mismatch across sites)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Label-generation implementations differed across sites: NIH labels were generated by a proprietary parse-tree/MESH-rule NLP; MSH labels were produced by a smaller Lasso-based n-gram NLP trained on 405 hand-labeled reports; IU labels were manually curated. These differences (and MSH's apparently more liberal positive labeling behavior) produced major label prevalence disparities (MSH pneumonia 34.2% vs NIH 1.2% vs IU 1.0%), creating a mismatch between the assumed uniform label semantics described in text and the heterogeneous labeling code actually used.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>labeling / ground-truth generation</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>comparison of label prevalences across datasets, manual review, and evaluation of the MSH NLP on held-out reports (train/test split of the 405 reports); statistical tests (chi-squared) on prevalence differences.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Prevalence statistics (MSH 34.2%, NIH 1.2%, IU 1.0%); chi-square P<0.001 for differences; MSH NLP performance reported (AUC/sensitivity/specificity in S1 Table) on 122-report test split; effects on models measured via AUC and DeLong tests across internal vs external evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Major — produced strong site-revealing signals that models exploited, inflating internal performance but degrading external generalization. Illustrative numbers: a trivial model using only hospital-system prevalence achieved AUC 0.861 on the combined MSH-NIH test set; jointly trained MSH-NIH model had internal AUC 0.931 but external IU AUC 0.815 (P=0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed for these three datasets in this study; prevalence differences are large and statistically significant; the problem stems from per-site labeling pipelines rather than a rare occurrence in this corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>heterogeneous labeling implementations and thresholds, incomplete or inconsistent reporting of labeling rules across datasets, and implicit assumptions that 'positive' means the same across sites</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Manually label a common gold standard, refit or harmonize NLP across sites, perform external validation, and run engineered prevalence experiments (stratified subsampling and balanced cohorts) to test sensitivity to prevalence mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Balancing prevalence across training sites reduced the internal-vs-external disparity: the Balanced engineered cohort produced internal AUC 0.739 and external IU AUC 0.732 (P=0.88), indicating mitigation can be effective; extreme imbalances increased internal AUC but reduced external performance (e.g., MSH Severe internal AUC 0.899 vs external 0.641).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning labeling / medical NLP / dataset curation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e705.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e705.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>site_feature_confounding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Site-specific image artifacts and metadata (laterality tokens, scanner inversion) exploited by CNNs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>CNNs learned to identify hospital system and even department from image content (e.g., laterality tokens, image corners) with extremely high accuracy, allowing models to exploit site-related confounders correlated with disease prevalence rather than pathology.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>image classification / diagnostic CNN pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>DenseNet-121 based classifiers trained to predict pneumonia and separately to predict the originating hospital system and department from raw image pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods claim that models detect pathology from image features</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>DenseNet-121 models implemented in PyTorch, activation-map analysis following CAM/Grad-CAM-style approach</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>model exploiting unintended image-level confounders (implementation behavior differing from described pathology-only intent)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Although the project intention (as described) was pathology-driven classification, empirical code-level behavior showed CNNs detecting hospital system (NIH 99.95%, MSH 99.98%, IU 95.59% test accuracy) and MSH department (100% inpatient vs ED) from images alone. Activation maps highlighted metal laterality tokens and image corners as strong hospital-identifying features; many subregions independently predicted hospital with >=95% certainty (mean 35.7 out of 49 subregions). These site-specific signals enabled the model to calibrate predictions by site rather than purely by pathology.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model input features / learned representation (feature extraction and decision layers)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Train CNNs to predict hospital system and department; analyze activation heatmaps (7x7 subregions) and principal component analysis of bottleneck features; manual image inspection to identify metal tokens and inverted color schemes.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Hospital prediction accuracy (NIH 22,050/22,062 = 99.95%; MSH 8,386/8,388 = 99.98%; IU 737/771 = 95.59%); subregion prediction counts (mean 35.7/49 subregions >=95%); PCA showing hospital clusters; impact on diagnosis measured via AUC differences (joint model internal AUC 0.931 vs external IU 0.815).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantial: models leveraged site artifacts to boost internal evaluation metrics while failing to generalize externally. Example: a trivial prevalence-based model (ignoring images) achieved AUC 0.861 on combined MSH-NIH test set, demonstrating that site signals alone can largely predict the target and thus inflate apparent performance.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Site-identifying features were widespread: laterality tokens and corners contributed across many images; the hospital-detection model was nearly perfect on NIH and MSH test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>undocumented, site-specific acquisition and storage conventions (tokens, scanner models, image inversion, compression) that create image-level signals correlated with disease prevalence; these are not described in high-level methodology but are present in image data.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Explicit external validation across independent hospitals; activation-map based investigation to surface confounders; manual image review; stratified/engineered experiments to demonstrate sensitivity to prevalence; recommendation to remove or mask downstream confounding metadata/artifacts and design domain-specific models.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Identification was effective (activation maps and manual review surfaced tokens); mitigation by balancing prevalence reduced internal/external discrepancy (balanced cohort P=0.88), but direct removal of tokens and its quantitative effect on downstream AUC was not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep learning / medical imaging / model interpretability</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e705.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e705.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>prevalence_pooling_misalignment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Misalignment from pooling datasets with disparate disease prevalence (calibration exploitation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pooling training data from hospitals with substantially different pneumonia prevalences allowed the model to implicitly learn site-specific priors and calibrate predictions to hospital prevalence, improving internal pooled performance but degrading external generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>training data aggregation / model training protocol</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Experiments training models on individual sites and on joint pooled datasets (MSH + NIH) with additional engineered cohorts that varied disease prevalence intentionally.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>experimental protocol/assumption that pooling data improves generalizability</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>model training script implementing pooled training sets and stratified subsampling; evaluation scripts computing AUC and calibration</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / implicit calibration to dataset priors (misalignment between pooling assumption and implementation effect)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Although pooling is often described as increasing generalizability, the implementation (training on pooled data with uneven site-specific prevalences) enabled the CNN to learn and use hospital-specific prevalence priors to make predictions, rather than learning pathology-generalizable features. This discrepancy between the 'pooling helps' description and the actual trained model behavior was shown by both the trivial prevalence-only model (AUC 0.861 on pooled test) and by engineered experiments where increasing prevalence imbalance raised internal AUC but reduced external AUC.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training procedure / dataset construction / model calibration</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Engineered relative-risk experiments (stratified subsampling to create cohorts with controlled prevalence differences), comparative evaluation using AUC and DeLong tests, and calibration plots.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>AUC comparisons: Balanced cohort internal AUC 0.739 vs external IU AUC 0.732 (P=0.88); MSH Severe internal AUC 0.899 vs external IU AUC 0.641 (P<0.001). Calibration slopes reported with wide variation (min 0.047 to max 10.4) across train/predict pairings.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Pooling without accounting for prevalence heterogeneity led to over-optimistic internal performance estimates and poor external generalizability; exact quantitative examples provided above.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed consistently across experiments in this study (engineered cohorts and natural pooled MSH-NIH experiments); 3 out of 5 natural train/test comparisons showed internal > external performance.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>difference in disease prevalence across sites and the model's implicit ability to use site identity as a calibration signal (implicit prior learning), combined with insufficient reporting/awareness of per-site prevalence in pooling descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Construct balanced cohorts, stratified subsampling, and always report and evaluate models on external datasets; use engineered experiments to quantify sensitivity to prevalence differences.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Balancing prevalences across training sites produced models whose internal and external AUCs were not significantly different (Balanced cohort P=0.88), demonstrating that mitigation can be effective when applied appropriately.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning experimental design / dataset construction</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e705.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e705.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>transfer_downsampling_gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain mismatch from transfer learning with aggressive image downsampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of ImageNet-pretrained DenseNet-121 with images resized to 224x224 (standard for transfer learning) may omit important radiological detail; this pragmatic implementation choice differs from domain needs described in paper and can increase the model's reliance on confounders.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>model architecture / transfer learning pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>DenseNet-121 pretrained on ImageNet, fine-tuned on chest X-rays resized to 224x224, used to predict multiple pathologies including pneumonia.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>methodological description advocating transfer learning and specific image preprocessing</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>model implementation in PyTorch with resizing to 224x224 and ImageNet pretrained weights</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / domain-specific pre-processing trade-off (loss of domain information)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The paper documents that images were resized to 224x224 for DenseNet/ImageNet transfer learning. This implementation choice—while described—creates a mismatch between the natural language rationale (apply proven CNNs) and the domain requirement (high-resolution radiographs contain subtle findings). The downsampling and transfer approach can remove diagnostically-relevant detail and encourage reliance on larger-scale confounders that remain after resizing.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>input preprocessing / model architecture</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>conceptual analysis and comparison of domain requirements versus implementation; discussion of potential impacts rather than direct ablation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>No direct quantitative measurement in this paper; argumentation cites loss of radiographic findings and increased confounder reliance as likely consequences. Other referenced work shows high-resolution architectures perform better in some radiology tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Argued impact: may increase reliance on confounding site features and reduce sensitivity to subtle pathology; not directly quantified in experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common practice in many radiology deep-learning implementations to use ImageNet-pretrained models with 224x224 resizing; thus prevalence of this implementation choice is high.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>practical convenience of transfer learning with standard CNNs, lack of domain-specific architectures or explicit discussion of downsampling trade-offs in some descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Develop and use architectures tailored to high-resolution radiological images, preserve domain-specific spatial resolution, and explicitly discuss preprocessing trade-offs in the methods description.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not evaluated quantitatively in this paper; authors cite other studies showing promise for high-resolution, multi-view architectures but provide no direct numbers here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep learning / medical imaging</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e705.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e705.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>interpretability_reporting_gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Insufficient transparency in model internals and reporting due to model complexity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper highlights that massive parameter counts (DenseNet-121 ~6.96M parameters) and limited interpretability tools (heatmaps) make it difficult to identify which variables/implementation details drive predictions, creating a gap between described methodology and understanding of actual model behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>model reporting and interpretability pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The model architecture used (DenseNet-121) and post-hoc interpretability tools (activation heatmaps, PCA on bottleneck features) used to try to explain predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>reporting of model design and interpretability in research paper</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>large-scale CNN implementation with post-hoc analysis scripts for activation maps and PCA</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous description / incomplete specification of influential features (interpretability gap)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The natural-language methods and results report the architecture and some analysis tools but cannot fully specify which low-level image features drive model predictions; post-hoc tools (heatmaps) are inexact, and the high parameter count makes reproducible, human-readable mapping from described method to implementation behavior difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model analysis and reporting / interpretability</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Critical analysis in discussion: attempted to use heatmaps and PCA but acknowledge limitations; inability to enumerate features driving predictions despite many parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>No precise quantitative measure; authors remark on limitations of heatmaps and on the large number of parameters (6,963,081) as evidence of interpretability deficit.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Impedes diagnosis of implementation-level confounders and reduces transparency, complicating reproducibility and clinical trust; specific experimental impact not numerically quantified but discussed as major concern.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>A common issue for deep learning models across medical imaging; here specifically noted for DenseNet-121 implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>intrinsic complexity of deep networks, insufficient reporting granularity in methods sections, and limitations of current interpretability methods</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Adopt transparent reporting standards (e.g., TRIPOD), external validation at multiple sites, careful feature interrogation (activation maps, PCA) and consider simpler models or more constrained architectures when clinically appropriate.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not quantitatively evaluated in this study; authors recommend these practices but do not provide empirical evidence of improvement within the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning interpretability / medical AI reporting</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study', 'publication_date_yy_mm': '2018-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases <em>(Rating: 2)</em></li>
                <li>CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning <em>(Rating: 2)</em></li>
                <li>Do CIFAR-10 Classifiers Generalize to CIFAR-10? <em>(Rating: 1)</em></li>
                <li>Exploring the ChestXray14 dataset: problems <em>(Rating: 2)</em></li>
                <li>Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs <em>(Rating: 1)</em></li>
                <li>Preparing a collection of radiology examinations for distribution and retrieval <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-705",
    "paper_id": "paper-fbda91cfacd2b792794fb726e9417aef58480c72",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "frontal_view_label_mismatch",
            "name_full": "Inconsistent frontal/lateral view labels in radiology datasets",
            "brief_description": "Metadata labels indicating frontal versus lateral chest radiographs were inconsistent across datasets, requiring an implemented CNN-based filter to enforce the preprocessing step; this mismatch was identified by manual review and corrected in code.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "radiology data preprocessing pipeline",
            "system_description": "Pipeline for preparing chest radiograph images for model training, including selection of frontal views and removal of lateral views.",
            "nl_description_type": "dataset documentation / metadata labels (frontal/lateral flags)",
            "code_implementation_type": "CNN-based preprocessing filter (ResNet-50 classifier) implemented in PyTorch",
            "gap_type": "inconsistent/missing metadata labels",
            "gap_description": "Datasets (NIH vs IU/MSH) had inconsistent or incorrect frontal/lateral labels. The paper's high-level description that NIH data contained only frontal radiographs conflicted with IU and MSH data containing mixed views with inconsistent labels, so the authors implemented a ResNet-50 classifier to identify frontal views rather than relying on dataset metadata.",
            "gap_location": "data preprocessing (view filtering)",
            "detection_method": "manual review of labels and sample images followed by training a ResNet-50 classifier to detect frontal views",
            "measurement_method": "Classifier accuracy on held-out test splits: MSH test 187/190 correctly classified; IU test 102/102 correctly classified; final dataset filtered to 158,323 frontal radiographs (112,120 NIH, 42,396 MSH, 3,807 IU).",
            "impact_on_results": "Required insertion of an automated filter step; ensured consistent input to downstream models and avoided training on lateral images. Without this, training/test splits could include mixed-view artifacts harming performance. Quantitatively, the preprocessing produced the final dataset of 158,323 images used for all experiments.",
            "frequency_or_prevalence": "Inconsistent labels observed in IU and MSH; the classifier validated on small manually labeled sets (IU: 402 labeled, MSH: 490 labeled) and achieved near-perfect test accuracy on those samples.",
            "root_cause": "inconsistent dataset metadata and labeling practices across contributing hospital systems (heterogeneous data annotation pipelines)",
            "mitigation_approach": "Train and apply a supervised frontal-view classifier to filter images; manual review to create small ground-truth sets for the classifier.",
            "mitigation_effectiveness": "High — near-perfect test accuracies on the held-out frontal/lateral classification test sets (MSH 187/190; IU 102/102), enabling consistent preprocessing; effect on downstream generalization quantified indirectly by using the filtered dataset.",
            "domain_or_field": "deep learning / medical imaging preprocessing",
            "reproducibility_impact": true,
            "uuid": "e705.0",
            "source_info": {
                "paper_title": "Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "nlp_labeling_discrepancy",
            "name_full": "Heterogeneous NLP labeling pipelines and labeling thresholds across datasets",
            "brief_description": "Different natural language processing and manual annotation methods were used to produce pathology labels for each dataset (proprietary parse-tree NLP for NIH, Lasso n-gram NLP trained on 405 reports for MSH, and manual curation for IU), producing large prevalence differences and label inconsistencies that affected model behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "label generation / annotation pipeline",
            "system_description": "The process and code used to turn radiology reports into binary image-level pathology labels via different NLP pipelines or manual labeling depending on the dataset.",
            "nl_description_type": "dataset labeling description / report-to-label NLP specification",
            "code_implementation_type": "three label pipelines: proprietary parse-tree NLP (NIH), Lasso logistic regression on 1-2 gram bag-of-words (MSH), manual curator labels (IU)",
            "gap_type": "different algorithm variant / inconsistent labeling thresholds (implementation mismatch across sites)",
            "gap_description": "Label-generation implementations differed across sites: NIH labels were generated by a proprietary parse-tree/MESH-rule NLP; MSH labels were produced by a smaller Lasso-based n-gram NLP trained on 405 hand-labeled reports; IU labels were manually curated. These differences (and MSH's apparently more liberal positive labeling behavior) produced major label prevalence disparities (MSH pneumonia 34.2% vs NIH 1.2% vs IU 1.0%), creating a mismatch between the assumed uniform label semantics described in text and the heterogeneous labeling code actually used.",
            "gap_location": "labeling / ground-truth generation",
            "detection_method": "comparison of label prevalences across datasets, manual review, and evaluation of the MSH NLP on held-out reports (train/test split of the 405 reports); statistical tests (chi-squared) on prevalence differences.",
            "measurement_method": "Prevalence statistics (MSH 34.2%, NIH 1.2%, IU 1.0%); chi-square P&lt;0.001 for differences; MSH NLP performance reported (AUC/sensitivity/specificity in S1 Table) on 122-report test split; effects on models measured via AUC and DeLong tests across internal vs external evaluations.",
            "impact_on_results": "Major — produced strong site-revealing signals that models exploited, inflating internal performance but degrading external generalization. Illustrative numbers: a trivial model using only hospital-system prevalence achieved AUC 0.861 on the combined MSH-NIH test set; jointly trained MSH-NIH model had internal AUC 0.931 but external IU AUC 0.815 (P=0.001).",
            "frequency_or_prevalence": "Observed for these three datasets in this study; prevalence differences are large and statistically significant; the problem stems from per-site labeling pipelines rather than a rare occurrence in this corpus.",
            "root_cause": "heterogeneous labeling implementations and thresholds, incomplete or inconsistent reporting of labeling rules across datasets, and implicit assumptions that 'positive' means the same across sites",
            "mitigation_approach": "Manually label a common gold standard, refit or harmonize NLP across sites, perform external validation, and run engineered prevalence experiments (stratified subsampling and balanced cohorts) to test sensitivity to prevalence mismatch.",
            "mitigation_effectiveness": "Balancing prevalence across training sites reduced the internal-vs-external disparity: the Balanced engineered cohort produced internal AUC 0.739 and external IU AUC 0.732 (P=0.88), indicating mitigation can be effective; extreme imbalances increased internal AUC but reduced external performance (e.g., MSH Severe internal AUC 0.899 vs external 0.641).",
            "domain_or_field": "machine learning labeling / medical NLP / dataset curation",
            "reproducibility_impact": true,
            "uuid": "e705.1",
            "source_info": {
                "paper_title": "Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "site_feature_confounding",
            "name_full": "Site-specific image artifacts and metadata (laterality tokens, scanner inversion) exploited by CNNs",
            "brief_description": "CNNs learned to identify hospital system and even department from image content (e.g., laterality tokens, image corners) with extremely high accuracy, allowing models to exploit site-related confounders correlated with disease prevalence rather than pathology.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "image classification / diagnostic CNN pipeline",
            "system_description": "DenseNet-121 based classifiers trained to predict pneumonia and separately to predict the originating hospital system and department from raw image pixels.",
            "nl_description_type": "research paper methods claim that models detect pathology from image features",
            "code_implementation_type": "DenseNet-121 models implemented in PyTorch, activation-map analysis following CAM/Grad-CAM-style approach",
            "gap_type": "model exploiting unintended image-level confounders (implementation behavior differing from described pathology-only intent)",
            "gap_description": "Although the project intention (as described) was pathology-driven classification, empirical code-level behavior showed CNNs detecting hospital system (NIH 99.95%, MSH 99.98%, IU 95.59% test accuracy) and MSH department (100% inpatient vs ED) from images alone. Activation maps highlighted metal laterality tokens and image corners as strong hospital-identifying features; many subregions independently predicted hospital with &gt;=95% certainty (mean 35.7 out of 49 subregions). These site-specific signals enabled the model to calibrate predictions by site rather than purely by pathology.",
            "gap_location": "model input features / learned representation (feature extraction and decision layers)",
            "detection_method": "Train CNNs to predict hospital system and department; analyze activation heatmaps (7x7 subregions) and principal component analysis of bottleneck features; manual image inspection to identify metal tokens and inverted color schemes.",
            "measurement_method": "Hospital prediction accuracy (NIH 22,050/22,062 = 99.95%; MSH 8,386/8,388 = 99.98%; IU 737/771 = 95.59%); subregion prediction counts (mean 35.7/49 subregions &gt;=95%); PCA showing hospital clusters; impact on diagnosis measured via AUC differences (joint model internal AUC 0.931 vs external IU 0.815).",
            "impact_on_results": "Substantial: models leveraged site artifacts to boost internal evaluation metrics while failing to generalize externally. Example: a trivial prevalence-based model (ignoring images) achieved AUC 0.861 on combined MSH-NIH test set, demonstrating that site signals alone can largely predict the target and thus inflate apparent performance.",
            "frequency_or_prevalence": "Site-identifying features were widespread: laterality tokens and corners contributed across many images; the hospital-detection model was nearly perfect on NIH and MSH test sets.",
            "root_cause": "undocumented, site-specific acquisition and storage conventions (tokens, scanner models, image inversion, compression) that create image-level signals correlated with disease prevalence; these are not described in high-level methodology but are present in image data.",
            "mitigation_approach": "Explicit external validation across independent hospitals; activation-map based investigation to surface confounders; manual image review; stratified/engineered experiments to demonstrate sensitivity to prevalence; recommendation to remove or mask downstream confounding metadata/artifacts and design domain-specific models.",
            "mitigation_effectiveness": "Identification was effective (activation maps and manual review surfaced tokens); mitigation by balancing prevalence reduced internal/external discrepancy (balanced cohort P=0.88), but direct removal of tokens and its quantitative effect on downstream AUC was not reported.",
            "domain_or_field": "deep learning / medical imaging / model interpretability",
            "reproducibility_impact": true,
            "uuid": "e705.2",
            "source_info": {
                "paper_title": "Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "prevalence_pooling_misalignment",
            "name_full": "Misalignment from pooling datasets with disparate disease prevalence (calibration exploitation)",
            "brief_description": "Pooling training data from hospitals with substantially different pneumonia prevalences allowed the model to implicitly learn site-specific priors and calibrate predictions to hospital prevalence, improving internal pooled performance but degrading external generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "training data aggregation / model training protocol",
            "system_description": "Experiments training models on individual sites and on joint pooled datasets (MSH + NIH) with additional engineered cohorts that varied disease prevalence intentionally.",
            "nl_description_type": "experimental protocol/assumption that pooling data improves generalizability",
            "code_implementation_type": "model training script implementing pooled training sets and stratified subsampling; evaluation scripts computing AUC and calibration",
            "gap_type": "incomplete specification / implicit calibration to dataset priors (misalignment between pooling assumption and implementation effect)",
            "gap_description": "Although pooling is often described as increasing generalizability, the implementation (training on pooled data with uneven site-specific prevalences) enabled the CNN to learn and use hospital-specific prevalence priors to make predictions, rather than learning pathology-generalizable features. This discrepancy between the 'pooling helps' description and the actual trained model behavior was shown by both the trivial prevalence-only model (AUC 0.861 on pooled test) and by engineered experiments where increasing prevalence imbalance raised internal AUC but reduced external AUC.",
            "gap_location": "training procedure / dataset construction / model calibration",
            "detection_method": "Engineered relative-risk experiments (stratified subsampling to create cohorts with controlled prevalence differences), comparative evaluation using AUC and DeLong tests, and calibration plots.",
            "measurement_method": "AUC comparisons: Balanced cohort internal AUC 0.739 vs external IU AUC 0.732 (P=0.88); MSH Severe internal AUC 0.899 vs external IU AUC 0.641 (P&lt;0.001). Calibration slopes reported with wide variation (min 0.047 to max 10.4) across train/predict pairings.",
            "impact_on_results": "Pooling without accounting for prevalence heterogeneity led to over-optimistic internal performance estimates and poor external generalizability; exact quantitative examples provided above.",
            "frequency_or_prevalence": "Observed consistently across experiments in this study (engineered cohorts and natural pooled MSH-NIH experiments); 3 out of 5 natural train/test comparisons showed internal &gt; external performance.",
            "root_cause": "difference in disease prevalence across sites and the model's implicit ability to use site identity as a calibration signal (implicit prior learning), combined with insufficient reporting/awareness of per-site prevalence in pooling descriptions.",
            "mitigation_approach": "Construct balanced cohorts, stratified subsampling, and always report and evaluate models on external datasets; use engineered experiments to quantify sensitivity to prevalence differences.",
            "mitigation_effectiveness": "Balancing prevalences across training sites produced models whose internal and external AUCs were not significantly different (Balanced cohort P=0.88), demonstrating that mitigation can be effective when applied appropriately.",
            "domain_or_field": "machine learning experimental design / dataset construction",
            "reproducibility_impact": true,
            "uuid": "e705.3",
            "source_info": {
                "paper_title": "Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "transfer_downsampling_gap",
            "name_full": "Domain mismatch from transfer learning with aggressive image downsampling",
            "brief_description": "Use of ImageNet-pretrained DenseNet-121 with images resized to 224x224 (standard for transfer learning) may omit important radiological detail; this pragmatic implementation choice differs from domain needs described in paper and can increase the model's reliance on confounders.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "model architecture / transfer learning pipeline",
            "system_description": "DenseNet-121 pretrained on ImageNet, fine-tuned on chest X-rays resized to 224x224, used to predict multiple pathologies including pneumonia.",
            "nl_description_type": "methodological description advocating transfer learning and specific image preprocessing",
            "code_implementation_type": "model implementation in PyTorch with resizing to 224x224 and ImageNet pretrained weights",
            "gap_type": "incomplete specification / domain-specific pre-processing trade-off (loss of domain information)",
            "gap_description": "The paper documents that images were resized to 224x224 for DenseNet/ImageNet transfer learning. This implementation choice—while described—creates a mismatch between the natural language rationale (apply proven CNNs) and the domain requirement (high-resolution radiographs contain subtle findings). The downsampling and transfer approach can remove diagnostically-relevant detail and encourage reliance on larger-scale confounders that remain after resizing.",
            "gap_location": "input preprocessing / model architecture",
            "detection_method": "conceptual analysis and comparison of domain requirements versus implementation; discussion of potential impacts rather than direct ablation experiments.",
            "measurement_method": "No direct quantitative measurement in this paper; argumentation cites loss of radiographic findings and increased confounder reliance as likely consequences. Other referenced work shows high-resolution architectures perform better in some radiology tasks.",
            "impact_on_results": "Argued impact: may increase reliance on confounding site features and reduce sensitivity to subtle pathology; not directly quantified in experiments in this paper.",
            "frequency_or_prevalence": "Common practice in many radiology deep-learning implementations to use ImageNet-pretrained models with 224x224 resizing; thus prevalence of this implementation choice is high.",
            "root_cause": "practical convenience of transfer learning with standard CNNs, lack of domain-specific architectures or explicit discussion of downsampling trade-offs in some descriptions",
            "mitigation_approach": "Develop and use architectures tailored to high-resolution radiological images, preserve domain-specific spatial resolution, and explicitly discuss preprocessing trade-offs in the methods description.",
            "mitigation_effectiveness": "Not evaluated quantitatively in this paper; authors cite other studies showing promise for high-resolution, multi-view architectures but provide no direct numbers here.",
            "domain_or_field": "deep learning / medical imaging",
            "reproducibility_impact": null,
            "uuid": "e705.4",
            "source_info": {
                "paper_title": "Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study",
                "publication_date_yy_mm": "2018-07"
            }
        },
        {
            "name_short": "interpretability_reporting_gap",
            "name_full": "Insufficient transparency in model internals and reporting due to model complexity",
            "brief_description": "The paper highlights that massive parameter counts (DenseNet-121 ~6.96M parameters) and limited interpretability tools (heatmaps) make it difficult to identify which variables/implementation details drive predictions, creating a gap between described methodology and understanding of actual model behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "model reporting and interpretability pipeline",
            "system_description": "The model architecture used (DenseNet-121) and post-hoc interpretability tools (activation heatmaps, PCA on bottleneck features) used to try to explain predictions.",
            "nl_description_type": "reporting of model design and interpretability in research paper",
            "code_implementation_type": "large-scale CNN implementation with post-hoc analysis scripts for activation maps and PCA",
            "gap_type": "ambiguous description / incomplete specification of influential features (interpretability gap)",
            "gap_description": "The natural-language methods and results report the architecture and some analysis tools but cannot fully specify which low-level image features drive model predictions; post-hoc tools (heatmaps) are inexact, and the high parameter count makes reproducible, human-readable mapping from described method to implementation behavior difficult.",
            "gap_location": "model analysis and reporting / interpretability",
            "detection_method": "Critical analysis in discussion: attempted to use heatmaps and PCA but acknowledge limitations; inability to enumerate features driving predictions despite many parameters.",
            "measurement_method": "No precise quantitative measure; authors remark on limitations of heatmaps and on the large number of parameters (6,963,081) as evidence of interpretability deficit.",
            "impact_on_results": "Impedes diagnosis of implementation-level confounders and reduces transparency, complicating reproducibility and clinical trust; specific experimental impact not numerically quantified but discussed as major concern.",
            "frequency_or_prevalence": "A common issue for deep learning models across medical imaging; here specifically noted for DenseNet-121 implementations.",
            "root_cause": "intrinsic complexity of deep networks, insufficient reporting granularity in methods sections, and limitations of current interpretability methods",
            "mitigation_approach": "Adopt transparent reporting standards (e.g., TRIPOD), external validation at multiple sites, careful feature interrogation (activation maps, PCA) and consider simpler models or more constrained architectures when clinically appropriate.",
            "mitigation_effectiveness": "Not quantitatively evaluated in this study; authors recommend these practices but do not provide empirical evidence of improvement within the paper.",
            "domain_or_field": "machine learning interpretability / medical AI reporting",
            "reproducibility_impact": true,
            "uuid": "e705.5",
            "source_info": {
                "paper_title": "Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study",
                "publication_date_yy_mm": "2018-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases",
            "rating": 2
        },
        {
            "paper_title": "CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning",
            "rating": 2
        },
        {
            "paper_title": "Do CIFAR-10 Classifiers Generalize to CIFAR-10?",
            "rating": 1
        },
        {
            "paper_title": "Exploring the ChestXray14 dataset: problems",
            "rating": 2
        },
        {
            "paper_title": "Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs",
            "rating": 1
        },
        {
            "paper_title": "Preparing a collection of radiology examinations for distribution and retrieval",
            "rating": 1
        }
    ],
    "cost": 0.01931175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>RESEARCH ARTICLE</h1>
<h2>Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study</h2>
<p>John R. Zech ${ }^{\text {T }}$, Marcus A. Badgeley ${ }^{2 <em>}$, Manway Liu ${ }^{2}$, Anthony B. Costa ${ }^{3}$, Joseph J. Titano ${ }^{4}$, Eric Karl Oermann ${ }^{3 </em>}$<br>1 Department of Medicine, California Pacific Medical Center, San Francisco, California, United States of America, 2 Verily Life Sciences, South San Francisco, California, United States of America, 3 Department of Neurological Surgery, Icahn School of Medicine, New York, New York, United States of America, 4 Department of Radiology, Icahn School of Medicine, New York, New York, United States of America<br>$\cdot$ These authors contributed equally to this work.<br>* eric.oermann@mountsinai.org</p>
<h2>Abstract</h2>
<h2>Background</h2>
<p>There is interest in using convolutional neural networks (CNNs) to analyze medical imaging to provide computer-aided diagnosis (CAD). Recent work has suggested that image classification CNNs may not generalize to new data as well as previously believed. We assessed how well CNNs generalized across three hospital systems for a simulated pneumonia screening task.</p>
<h2>Methods and findings</h2>
<p>A cross-sectional design with multiple model training cohorts was used to evaluate model generalizability to external sites using split-sample validation. A total of 158,323 chest radiographs were drawn from three institutions: National Institutes of Health Clinical Center (NIH; 112,120 from 30,805 patients), Mount Sinai Hospital (MSH; 42,396 from 12,904 patients), and Indiana University Network for Patient Care (IU; 3,807 from 3,683 patients). These patient populations had an age mean (SD) of 46.9 years (16.6), 63.2 years (16.5), and 49.6 years (17) with a female percentage of $43.5 \%, 44.8 \%$, and $57.3 \%$, respectively. We assessed individual models using the area under the receiver operating characteristic curve (AUC) for radiographic findings consistent with pneumonia and compared performance on different test sets with DeLong's test. The prevalence of pneumonia was high enough at MSH (34.2\%) relative to NIH and IU ( $1.2 \%$ and $1.0 \%$ ) that merely sorting by hospital system achieved an AUC of 0.861 ( $95 \%$ CI $0.855-0.866$ ) on the joint MSH-NIH dataset. Models trained on data from either NIH or MSH had equivalent performance on IU ( $P$ values 0.580 and 0.273 , respectively) and inferior performance on data from each other relative to an internal test set (i.e., new data from within the hospital system used for training data; $P$ values both $&lt;0.001$ ). The highest internal performance was achieved by combining training and test data from MSH and NIH (AUC 0.931, 95\% CI 0.927-0.936), but this model</p>
<p>php). Retrospective data used in this study from Mount Sinai Health System cannot be released under the terms of our Institutional Review Board approval to protect patient confidentiality. Researchers interested in accessing Mount Sinai data through the Imaging Research Warehouse Initiative may contact Zahi Fayad, PhD at zahi. fayad@mssm.edu.</p>
<p>Funding: The Department of Radiology at the Icahn School of Medicine at Mount Sinai (http://icahn. mssm.edu/about/departments/radiology) supported this project financially via internal department funding (author JJT). No other authors received specific funding for this work. The funder had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p>
<p>Competing interests: I have read the journal's policy and the authors of this manuscript have the following competing interests: MAB and ML are currently employees at Verily Life Sciences, which played no role in the research and has no commercial interest in it. EKO and ABC receive funding from Intel for unrelated work.</p>
<p>Abbreviations: AUC, area under the receiver operating characteristic curve; CAD, computeraided diagnosis; CNN, convolutional neural network; ICU, intensive care unit; ILSVRC, ImageNet Large Scale Visual Recognition Competition; IU, Indiana University Network for Patient Care; MNIST, Modified National Institute of Standards and Technology; MSH, Mount Sinai Hospital; NIH, National Institutes of Health Clinical Center; NLP, natural language processing; NPV, negative predictive value; PACS, picture archiving and communication system; PPV, positive predictive value.
demonstrated significantly lower external performance at IU (AUC 0.815, 95\% CI 0.745$0.885, P=0.001$ ). To test the effect of pooling data from sites with disparate pneumonia prevalence, we used stratified subsampling to generate MSH-NIH cohorts that only differed in disease prevalence between training data sites. When both training data sites had the same pneumonia prevalence, the model performed consistently on external IU data ( $P=$ 0.88). When a 10 -fold difference in pneumonia rate was introduced between sites, internal test performance improved compared to the balanced model ( $10 \times$ MSH risk $P&lt;0.001 ; 10 \times$ NIH $P=0.002$ ), but this outperformance failed to generalize to IU (MSH $10 \times P&lt;0.001$; NIH $10 \times P=0.027$ ). CNNs were able to directly detect hospital system of a radiograph for $99.95 \%$ NIH $(22,050 / 22,062)$ and $99.98 \%$ MSH $(8,386 / 8,388)$ radiographs. The primary limitation of our approach and the available public data is that we cannot fully assess what other factors might be contributing to hospital system-specific biases.</p>
<h2>Conclusion</h2>
<p>Pneumonia-screening CNNs achieved better internal than external performance in 3 out of 5 natural comparisons. When models were trained on pooled data from sites with different pneumonia prevalence, they performed better on new pooled data from these sites but not on external data. CNNs robustly identified hospital system and department within a hospital, which can have large differences in disease burden and may confound predictions.</p>
<h2>Author summary</h2>
<h2>Why was this study done?</h2>
<ul>
<li>Early results in using convolutional neural networks (CNNs) on X-rays to diagnose disease have been promising, but it has not yet been shown that models trained on X-rays from one hospital or one group of hospitals will work equally well at different hospitals.</li>
<li>Before these tools are used for computer-aided diagnosis in real-world clinical settings, we must verify their ability to generalize across a variety of hospital systems.</li>
</ul>
<h2>What did the researchers do and find?</h2>
<ul>
<li>A cross-sectional design was used to train and evaluate pneumonia screening CNNs on 158,323 chest X-rays from the National Institutes of Health Clinical Center (NIH; $n=$ 112,120 from 30,805 patients), Mount Sinai Hospital ( 42,396 from 12,904 patients), and Indiana University Network for Patient Care ( $n=3,807$ from 3,683 patients).</li>
<li>In 3 out of 5 natural comparisons, performance on chest X-rays from outside hospitals was significantly lower than on held-out X-rays from the original hospital system.</li>
<li>CNNs were able to detect where a radiograph was acquired (hospital system, hospital department) with extremely high accuracy and calibrate predictions accordingly.</li>
</ul>
<h1>What do these findings mean?</h1>
<ul>
<li>The performance of CNNs in diagnosing diseases on X-rays may reflect not only their ability to identify disease-specific imaging findings on X-rays but also their ability to exploit confounding information.</li>
<li>Estimates of CNN performance based on test data from hospital systems used for model training may overstate their likely real-world performance.</li>
</ul>
<h2>Introduction</h2>
<p>There is significant interest in using convolutional neural networks (CNNs) to analyze radiology, pathology, or clinical imaging for the purposes of computer-aided diagnosis (CAD) [1-5]. These studies are generally performed utilizing CNN techniques that were pioneered on wellcharacterized computer vision datasets, including the ImageNet Large Scale Visual Recognition Competition (ILSVRC) and the Modified National Institute of Standards and Technology (MNIST) database of hand-drawn digits [6,7]. Training CNNs to classify images from these datasets is typically done by splitting the dataset into three subsets: train (data directly used to learn parameters for models), tune (data used to choose hyperparameter settings, also commonly referred to as "validation"), and test (data used exclusively for performance evaluation of models learned using train and tune data). CNNs are trained to completion with the first two, and the final set is used to estimate the model's expected performance on new, previously unseen data.</p>
<p>An underlying premise of the test set implying future generalizability to new data is that the test set is reflective of the data that will be encountered elsewhere. Recent work in computer vision has demonstrated that the true generalization performance of even classic CIFAR-10 photograph classification CNNs to new data may be lower than previously believed [8]. In the biomedical imaging context, we can contrast "internal" model performance on new, previously unseen data gathered from the same hospital system(s) used for model training with "external" model performance on new, previously unseen data from different hospital systems [9,10]. External test data may be different in important ways from internal test data, and this may affect model performance, particularly if confounding variables exist in internal data that do not exist in external data [11]. In a large-scale deep learning study of retinal fundoscopy, Ting and colleagues noted variation in the performance of CNNs trained to identify ocular disease across external hospital systems, with the areas under the receiver operating characteristic curve (AUCs) ranging from 0.889 to 0.983 and image-level concordance with human experts ranging from $65.8 \%$ to $91.2 \%$ on external datasets [4]. Despite the rapid push to develop deep learning systems on radiological data for academic and commercial purposes, to date, no study has assessed whether radiological CNNs actually generalize to external data. If external test performance of a system is inferior to internal test performance, clinicians may erroneously believe systems to be more accurate than they truly are in the deployed context, creating the potential for patient harm.</p>
<p>The primary aim of this study was to obtain data from three separate hospital systems and to assess how well deep learning models trained at one hospital system generalized to other external hospital systems. For the purposes of this assessment, we chose the diagnosis of pneumonia on chest X-ray for both its clinical significance as well as common occurrence and significant interest [2]. We reproduced the CheXNet model of Rajpurkar and colleagues, whose</p>
<p>internal performance on National Institutes of Health Clinical Center (NIH) data has previously been reported [2]. We extended upon this work to evaluate the model's internal performance when trained on data from a different hospital system and to demonstrate how this model generalized to external hospital systems not used for model training. By training and testing models on different partitions of data across three distinct institutions, we sought to establish whether a truly generalizable model could be learned, as well as which factors affecting external validity could be identified to aid clinicians when assessing models for potential clinical deployment.</p>
<h2>Methods</h2>
<h2>Datasets</h2>
<p>This study was approved by the Mount Sinai Health System Institutional Review Board; the requirement for patient consent was waived for this retrospective study that was deemed to carry minimal risk. Three datasets were obtained from different hospital groups: NIH (112,120 radiographs from 1992 to 2015), Indiana University Network for Patient Care (IU; 7,470 radiographs, date range not available), and Mount Sinai Hospital (MSH; 48,915 radiographs from 2009 to 2016) [1,12]. This study did not have a prospective analysis plan, and all analyses performed are subsequently described.</p>
<h2>CNNs</h2>
<p>Deep learning encompasses any algorithm that uses multiple layers of feed-forward neural networks to model phenomena [13]. Classification CNNs are a type of supervised deep learning model that take an image as input and predict the probability of predicted class membership as output. A typical use of CNNs is classifying photographs according to the animals or objects they contain: a chihuahua, a stove, a speedboat, etc. [6]. Many different CNN architectures have been proposed, including ResNet-50 and DenseNet-121 used in this paper, and improving the performance of these models is an active area of research [14,15]. In practice, CNNs are frequently pretrained on large computer vision databases, such as ImageNet, rather than being randomly initialized and trained de novo. After pretraining, the CNNs are then fine-tuned on the dataset of interest. This process of pretraining followed by fine-tuning reduces training time, promotes model convergence, and can regularize the model to reduce overfitting. A difficulty of using these models is that there are few formal guarantees as to their generalization performance [16]. In this paper, we use CNNs both to preprocess and to predict pneumonia in radiographs.</p>
<h2>Preprocessing: Frontal view filtering</h2>
<p>NIH data contained only frontal chest radiographs, whereas IU and MSH data contained both frontal and lateral chest radiographs and were found to contain inconsistent frontal and lateral labels on manual review. A total of 402 IU and 490 MSH radiographs were manually labeled as frontal/lateral and randomly divided into groups (IU: 200 train, 100 tune, 102 test; MSH: 200 train, 100 tune, 190 test) and used to train ResNet-50 CNNs to identify frontal radiographs [14]. A total of 187/190 MSH and 102/102 IU test radiographs were accurately classified. The datasets were then filtered to frontal radiographs using these CNNs, leaving a total of 158,323 radiographs (112,120 NIH, 42,396 MSH, and 3,807 IU) available for analysis (S1 Fig).</p>
<h2>Preprocessing: Generating labels for pathology</h2>
<p>IU radiographs were manually labeled by curators after review of the accompanying text radiology reports [12]. NIH radiographs were labeled automatically using a proprietary natural</p>
<p>language processing (NLP) system based on expanding sentences as parse trees and using handcrafted rules based on the MESH vocabulary to identify statements indicating positive pathology [1].</p>
<p>MSH radiographs did not initially include labels, so a subset of radiographic reports was manually labeled to train an NLP algorithm that could infer labels for the full dataset. A total of 405 radiographic reports were manually labeled for cardiomegaly, emphysema, effusion, hernia, nodule, atelectasis, pneumonia, edema, and consolidation. To evaluate the NLP algorithm's performance, these were split into train and test groups (283 and 122, respectively). A previously described NLP concept extraction model based on 1- and 2-gram bag-of-words features with Lasso logistic regression was trained to identify reports mentioning pneumonia [17]. AUC, sensitivity, and specificity at a 50% classification threshold are reported in S1 Table. The NLP model was then refit with all 405 manually labeled reports and used to process all unlabeled reports. As reports positive for hernia occurred too infrequently to use this NLP algorithm, reports were automatically labeled as positive for hernia if the word "hernia" appeared in the report.</p>
<h1>Preprocessing: Separation of patients across train, tune, and test groups</h1>
<p>As NIH and MSH data contained patient identifiers, all NIH and MSH patients were separated into fixed train (70\%), tune (10\%), and test (20\%) groups (S2 Fig). IU data did not contain patient identifiers. In the case of pneumonia detection, $100 \%$ of IU data was reserved for use as an external test set. IU data were used for training only to detect hospital system and in this case were separated into fixed train ( $70 \%$ ), tune ( $10 \%$ ), and test ( $20 \%$ ) groups using an identifier corresponding to accession number (e.g., which radiographs were obtained at the same time on the same patient). Test data were not available to CNNs during model training, and all results reported in this study are calculated exclusively on test data.</p>
<h2>Preprocessing: Identifying MSH portable scans from inpatient wards and the emergency department</h2>
<p>Of 42,396 MSH radiographs, 39,574 contained a label indicating whether they were portable radiographs; 31,838 were labeled as portable. We identified a subset of 31,076 MSH portable radiographs that documented the department of acquisition, with 28,841 from inpatient wards and 2,235 from the emergency department.</p>
<h2>Model training</h2>
<p>PyTorch 0.2.0 and torchvision were used for model training [18]. All images were resized to $224 \times 224$. CNNs used for experiments were trained with DenseNet-121 architecture with an additional dense layer $(n=15)$ attached to the original bottleneck layer and sigmoid activation (for binary classification) or a linear layer with output dimension equal to that of the classification label followed by softmax activation (for $n&gt;2$ multiclass prediction) [15]. This additional dense layer was added to facilitate extraction of bottleneck features in a reduced dimension. A DenseNet architecture with weights pretrained to ImageNet was chosen to facilitate comparison with recent work on pneumonia detection in radiographs by Rajpurkar and colleagues and for its state-of-the-art results on standard computer vision datasets [2]. All models were trained using a cross-entropy loss function with parameter update by stochastic gradient descent with momentum, initial learning rate 0.01 , momentum 0.9 , and weight decay 0.0001 . Learning rate was decayed by a factor of 10 after each epoch with no improvement in validation loss, and training was stopped after three epochs with no improvement in validation loss.</p>
<h1>Internal and external performance testing</h1>
<p>To assess how individual models trained using single datasets would generalize compared to a model trained simultaneously on multiple datasets, we trained CNNs to predict nine overlapping diagnoses (cardiomegaly, emphysema, effusion, hernia, nodule, atelectasis, pneumonia, edema, and consolidation) using three different train set combinations: NIH, MSH, and a joint NIH-MSH train set. We were interested only in the prediction of pneumonia and included other diagnoses to improve overall model training and performance. For each model, we calculated AUC, accuracy, sensitivity, and specificity for four different test sets: joint NIHMSH, NIH only, MSH only, and IU. We report differences in test AUC for all possible inter-nal-external comparisons. We consider the joint MSH-NIH test set the internal comparison set for the jointly trained model. We additionally report differences in test AUC between a jointly trained MSH-NIH model and individual MSH-NIH test sets. The classification threshold was set to ensure $95 \%$ sensitivity on each test set to simulate model use for a theoretical screening task. After external review of this analysis, a trivial model that ranked cases based only on the average pneumonia prevalence in each hospital system's training data and completely ignored radiographic findings was evaluated on the MSH-NIH test set to evaluate how hospital systems alone can predict pneumonia in the joint dataset. We include calibration plots across all reported comparisons as supporting information.</p>
<h2>Hospital system and department prediction</h2>
<p>After training models for pneumonia and evaluating their performance across sites, additional analysis was planned to better understand a CNN's ability to detect site and department and how that could affect pneumonia prediction. We trained a CNN to predict hospital systems from radiographs to assess whether location information was directly detectable from the radiograph alone. Radiographs from all three hospital systems were utilized to learn a model that could identify the hospital system from which a given radiograph was drawn. To develop this concept more granularly, for MSH radiographs, we further identified from which hospital unit individual radiographs were obtained (inpatient wards, emergency department). In all cases, we report the classification accuracy on a held-out test set.</p>
<h2>Sample activation maps</h2>
<p>We created $7 \times 7$ sample activation maps, following Zhou and colleagues, to attempt to understand which locations in chest radiographs provided strong evidence for hospital system [19]. For this experiment, we specifically identify radiographs from the NIH. For a sample of NIH test radiographs $(n=100)$, we averaged the softmax probability for each subregion calculated as</p>
<p>$$
\mathrm{P}\left(\text { hospital }=\mathrm{NIH} \mid \text { radiograph }<em NIH="NIH" _="{" _text="\text" i_="i," j_="j,">{\mathrm{i}, \mathrm{j}}\right)=\frac{e^{Y</em>
$$}}}}{e^{Y_{i, j, \text { NIH }}}+e^{Y_{i, j, \text { MSH }}}+e^{Y_{i, j, \text { IU }}}</p>
<p>where $i, j$ corresponds to the subregion at the ith row and jth column of the final convolutional layer ( $7 \times 7=49$ subregions), where each</p>
<p>$$
Y_{i, j \text { Hospital System }}=\sum\left(B_{k \text { Hospital System } *} X_{k, i, j}\right)+B_{0 \text { Hospital System }}
$$</p>
<p>where the sum is performed over the K final convolutional layers, and $\mathrm{X}_{\mathrm{k}, \mathrm{i}, \mathrm{j}}$ represents the activation at the ith row and jth column of the kth final convolutional layer. To characterize how many different subregions were typically involved in NIH hospital system classification, we report the mean, minimum, and maximum number of subregions that predicted NIH</p>
<p>decisively (probability $\geq 95 \%$ ). To illustrate the contribution of particularly influential features (e.g., laterality labels) to classification, we present several examples of heatmaps generated by calculating $\mathrm{Y}<em _mathrm_i="\mathrm{i">{\mathrm{i}, \mathrm{j} \text { NIH }}-\mathrm{Y}</em>$ subregions in an image and subtracting the mean. This additional calculation was necessary to distinguish their positive contribution in the context of many subregions contributing positively to classification probability.}, \mathrm{j} \text { MSH }}-\mathrm{Y}_{\mathrm{i}, \mathrm{j} \text { IU }}$ for all $\mathrm{i}, \mathrm{j</p>
<h1>Engineered relative risk experiment</h1>
<p>We wished to assess the hypothesis that a CNN could learn to exploit large differences in pathology prevalence between two hospital systems in training data by calibrating its predictions to the baseline prevalence at each hospital system rather than exclusively discriminating based on direct pathology findings. This would lead to strong performance on a test dataset consisting of imbalanced data from both hospital systems but would fail to generalize to data from an external hospital system. To test this hypothesis, we simulated experimental cohorts that differed only in relative disease prevalence and performed internal and external evaluations as described above. Five cohorts of 20,000 patients consisting of 10,000 MSH and 10,000 NIH patients were specifically sampled to artificially set different levels of pneumonia prevalence in each population while maintaining a constant overall prevalence: NIH Severe (NIH $9.9 \%$, MSH $0.1 \%$ ), NIH Mild (NIH 9\%, MSH 1\%), Balanced (NIH 5\%, MSH 5\%), MSH Mild (MSH 9\%, NIH 1\%), and MSH Severe (MSH 9.9\%, NIH 0.1\%). The sampling routine also ensured that males and females had an equal prevalence of pneumonia. We refer to these as "engineered prevalence cohorts." Train, tune, and test splits consistent with prior modeling were maintained for these experiments. CNNs were trained on each cohort in the fashion previously described, and test AUCs on internal joint MSH-NIH and external IU data were compared.</p>
<h2>Statistical methods</h2>
<p>To assess AUC differences between classification models, we used either the paired or unpaired version of DeLong's test for ROC curves as appropriate [20]. Comparisons between proportions were performed utilizing $\chi^{2}$ tests, and proportion CIs were calculated using the Clopper-Pearson interval. All $P$ values were assessed at an alpha of 0.05 . Statistical analysis was performed using R version 3.4 with the pROC package and scikit-learn 0.18.1 [21,22].</p>
<h2>Results</h2>
<h2>Datasets</h2>
<p>The average age of patients in the MSH cohort was 63.2 years (SD 16.5 years), compared to 49.6 years (SD 17 years) in the IU cohort and 46.9 years (SD 16.6 years) in the NIH cohort (Table 1).</p>
<p>Positive cases of pneumonia were remarkably more prevalent in MSH data (34.2\%) than in either NIH $\left(1.2 \%, \chi^{2} P&lt;0.001\right)$ or IU $(1.0 \%, P&lt;0.001)$ data.</p>
<h2>Internal and external performance testing</h2>
<p>Overall, the internal performance of pneumonia detection CNNs significantly exceeded external performance in 3 out of 5 natural comparisons (Fig 1, Table 2). CNNs trained to detect pneumonia at NIH had internal test AUC 0.750 ( $95 \%$ CI $0.721-0.778$ ), significantly worse external test AUC 0.695 at MSH ( $95 \%$ CI $0.683-0.706, P&lt;0.001$ ), and comparable external test AUC 0.725 at IU ( $95 \%$ CI $0.644-0.807, P=0.580$ ). CNNs trained to detect pneumonia at MSH had internal test AUC 0.802 ( $95 \%$ CI $0.793-0.812$ ), significantly worse external test AUC</p>
<p>Table 1. Baseline characteristics of datasets by site.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Characteristic</th>
<th style="text-align: center;">IU</th>
<th style="text-align: center;">MSH</th>
<th style="text-align: center;">NIH</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Patient demographics</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">No. patient radiographs</td>
<td style="text-align: center;">3,807</td>
<td style="text-align: center;">42,396</td>
<td style="text-align: center;">112,120</td>
</tr>
<tr>
<td style="text-align: left;">No. patients</td>
<td style="text-align: center;">3,683</td>
<td style="text-align: center;">12,904</td>
<td style="text-align: center;">30,805</td>
</tr>
<tr>
<td style="text-align: left;">Age, mean (SD), years</td>
<td style="text-align: center;">$49.6(17.0)$</td>
<td style="text-align: center;">$63.2(16.5)$</td>
<td style="text-align: center;">$46.9(16.6)$</td>
</tr>
<tr>
<td style="text-align: left;">No. females (\%)</td>
<td style="text-align: center;">$643(57.3 \%)$</td>
<td style="text-align: center;">$18,993(44.8 \%)$</td>
<td style="text-align: center;">$48,780(43.5 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Image diagnosis frequencies</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Pneumonia, No. (\%)</td>
<td style="text-align: center;">$39(1.0 \%)$</td>
<td style="text-align: center;">$14,515(34.2 \%)$</td>
<td style="text-align: center;">$1,353(1.2 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Emphysema, No. (\%)</td>
<td style="text-align: center;">$62(1.6 \%)$</td>
<td style="text-align: center;">$1,308(3.1 \%)$</td>
<td style="text-align: center;">$2,516(2.2 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Effusion, No. (\%)</td>
<td style="text-align: center;">$142(3.7 \%)$</td>
<td style="text-align: center;">$19,536(46.1 \%)$</td>
<td style="text-align: center;">$13,307(11.9 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Consolidation, No. (\%)</td>
<td style="text-align: center;">$26(0.7 \%)$</td>
<td style="text-align: center;">$25,318(59.7 \%)$</td>
<td style="text-align: center;">$4,667(4.2 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Nodule, No. (\%)</td>
<td style="text-align: center;">$104(2.7 \%)$</td>
<td style="text-align: center;">$569(1.3 \%)$</td>
<td style="text-align: center;">$6,323(5.6 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Atelectasis, No. (\%)</td>
<td style="text-align: center;">$307(8.1 \%)$</td>
<td style="text-align: center;">$16,713(39.4 \%)$</td>
<td style="text-align: center;">$11,535(10.3 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Edema, No. (\%)</td>
<td style="text-align: center;">$45(1.2 \%)$</td>
<td style="text-align: center;">$7,144(16.9 \%)$</td>
<td style="text-align: center;">$2,303(2.1 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Cardiomegaly, No. (\%)</td>
<td style="text-align: center;">$328(8.6 \%)$</td>
<td style="text-align: center;">$14,285(33.7 \%)$</td>
<td style="text-align: center;">$2,772(2.5 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Hernia, No. (\%)</td>
<td style="text-align: center;">$46(1.2 \%)$</td>
<td style="text-align: center;">$228(0.5 \%)$</td>
<td style="text-align: center;">$227(0.2 \%)$</td>
</tr>
</tbody>
</table>
<p>*Sex data available for 1,122 / 3,807 IU, 42,383 / 42,396 MSH; age data available for 112,077 / 112,120 NIH. Abbreviations: IU, Indiana University Network for Patient Care; MSH, Mount Sinai Hospital; NIH, National Institutes of Health Clinical Center; No., number.
https://doi.org/10.1371/journal.pmed. 1002683.t001
0.717 at NIH ( $95 \%$ CI $0.687-0.746, P&lt;0.001$ ), and comparable external test AUC 0.756 at IU ( $95 \%$ CI $0.674-0.838, P=0.273$ ). A jointly trained MSH-NIH model had internal test AUC 0.931 ( $95 \%$ CI $0.927-0.936$ ), significantly greater than external test AUC 0.815 at Indiana ( $95 \%$ CI $0.745-0.885, P=0.001$ ). The jointly trained model had stronger internal performance compared to either constituent site individually (MSH AUC $0.805,95 \%$ CI $0.796-0.814, P&lt;0.001$; NIH AUC $0.733,95 \%$ CI $0.703-0.762, P&lt;0.001$ ) (Table 2). A trivial model that ranked cases based only on the average pneumonia prevalence in each hospital system achieved AUC 0.861 ( $95 \%$ CI $0.855-0.866$ ) on the joint MSH-NIH test set. Calibration plots comparing model performance across hospital systems demonstrated a wide range of calibration slopes (minimum 0.047 for train MSH and predict NIH, maximum 10.4 for train NIH and predict MSH; S3-S14 Figs).</p>
<h1>Hospital system and department prediction</h1>
<p>A CNN trained to identify hospital systems accurately identified 22,050 / 22,062 (99.95\%, 95\% CI $0.9991-0.9997$ ) of NIH, 8,386 / 8,388 ( $99.98 \%, 95 \%$ CI $0.9991-1.0000$ ) of MSH, and 737 / 771 ( $95.59 \%, 95 \%$ CI $0.9389-0.9693$ ) of IU test radiographs, and hospital system was a larger source of variation in image data principal components than pneumonia (S15 Fig). To identify radiographs originating at a specific hospital system, such as NIH, CNNs used features from many different image regions (Fig 2A); the majority of image subregions were individually able to predict the hospital system with $\geq 95 \%$ certainty ( $35.7 / 49,72.9 \%$, minimum 21 , maximum $49, N=100$ NIH radiographs). Laterality labels were particularly influential (Fig 2B and 2C).</p>
<p>A CNN trained to identify individual departments within MSH accurately identified 5,805 / 5,805 ( $100 \%, 95 \%$ CI $0.9993-1.0000$ ) of inpatient radiographs and 449 / 449 ( $100 \%, 95 \%$ CI $0.9918-1.0000$ ) of emergency department radiographs. Patients who received portable</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig 1. Pneumonia models evaluated on internal and external test sets. A model trained using both MSH and NIH data (MSH + NIH) had higher performance on the combined MSH + NIH test set than on either subset individually or on fully external IU data. IU, Indiana University Network for Patient Care; MSH, Mount Sinai Hospital; NIH, National Institutes of Health Clinical Center.
https://doi.org/10.1371/journal.pmed.1002683.g001
radiographs on an inpatient floor had a higher prevalence of pneumonia than those in the emergency department ( $41.1 \%$ versus $32.8 \%$, respectively, $P&lt;0.001$ ).</p>
<h1>Engineered relative risk experiment</h1>
<p>Artificially increasing the difference in the prevalence of pneumonia between MSH and NIH led to CNNs that performed increasingly well on internal testing but not external testing (S2 Table). CNNs trained on engineered prevalence cohorts of NIH and MSH data showed stronger internal AUC on a joint NIH-MSH test set when the prevalence of pneumonia was imbalanced between the two hospital systems in the training dataset with MSH Severe AUC 0.899 ( $95 \%$ CI $0.885-0.914, P&lt;0.001$ ), MSH Mild AUC 0.860 ( $95 \%$ CI $0.839-0.882, P&lt;0.001$ ), NIH Mild AUC 0.807 ( $95 \%$ CI $0.778-0.836, P=0.002$ ), and NIH Severe AUC 0.849 ( $95 \%$ CI $0.826-0.871, P&lt;0.001$ ) than when it was balanced with AUC 0.739 ( $95 \%$ CI $0.707-0.772$ ) (Fig 3A and 3B).</p>
<p>Table 2. Internal and external pneumonia screening performance for all train, tune, and test hospital system combinations. Parentheses show $95 \%$ CIs.</p>
<table>
<thead>
<tr>
<th>Train/ Tune Site</th>
<th>Comparison Type*</th>
<th>Test Site (Images)</th>
<th>AUC</th>
<th>Accuracy</th>
<th>Sensitivity</th>
<th>Specificity</th>
<th>PPV</th>
<th>NPV</th>
</tr>
</thead>
<tbody>
<tr>
<td>NIH</td>
<td>Internal</td>
<td>NIH $(N=22,062)$</td>
<td>$\begin{gathered} 0.750(0.721- \ 0.778) \end{gathered}$</td>
<td>$\begin{gathered} 0.255(0.250- \ 0.261) \end{gathered}$</td>
<td>$\begin{gathered} 0.951(0.917- \ 0.973) \end{gathered}$</td>
<td>$\begin{gathered} 0.247(0.241- \ 0.253) \end{gathered}$</td>
<td>$\begin{gathered} 0.015(0.013- \ 0.017) \end{gathered}$</td>
<td>$\begin{gathered} 0.998(0.996- \ 0.999) \end{gathered}$</td>
</tr>
<tr>
<td></td>
<td>External</td>
<td>MSH $(N=8,388)$</td>
<td>$\begin{gathered} 0.695(0.683- \ 0.706) \end{gathered}$</td>
<td>$\begin{gathered} 0.476(0.465- \ 0.486) \end{gathered}$</td>
<td>$\begin{gathered} 0.950(0.942- \ 0.958) \end{gathered}$</td>
<td>$\begin{gathered} 0.212(0.201- \ 0.223) \end{gathered}$</td>
<td>$\begin{gathered} 0.401(0.390- \ 0.413) \end{gathered}$</td>
<td>$\begin{gathered} 0.884(0.866- \ 0.901) \end{gathered}$</td>
</tr>
<tr>
<td></td>
<td>External</td>
<td>IU $(N=3,807)$</td>
<td>$\begin{gathered} 0.725(0.644- \ 0.807) \end{gathered}$</td>
<td>$\begin{gathered} 0.190(0.178- \ 0.203) \end{gathered}$</td>
<td>$\begin{gathered} 0.974(0.865- \ 0.999) \end{gathered}$</td>
<td>$\begin{gathered} 0.182(0.170- \ 0.195) \end{gathered}$</td>
<td>$\begin{gathered} 0.012(0.009- \ 0.017) \end{gathered}$</td>
<td>$\begin{gathered} 0.999(0.992- \ 1.000) \end{gathered}$</td>
</tr>
<tr>
<td></td>
<td>Superset *</td>
<td>MSH + NIH</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>$(N=30,450)$</td>
<td>$\begin{gathered} 0.773(0.766- \ 0.780) \end{gathered}$</td>
<td>$\begin{gathered} 0.462(0.456- \ 0.467) \end{gathered}$</td>
<td>$\begin{gathered} 0.950(0.942- \ 0.957) \end{gathered}$</td>
<td>$\begin{gathered} 0.403(0.397- \ 0.409) \end{gathered}$</td>
<td>$\begin{gathered} 0.160(0.155- \ 0.166) \end{gathered}$</td>
<td>$\begin{gathered} 0.985(0.983- \ 0.987) \end{gathered}$</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Superset *</td>
<td>MSH + NIH + IU</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>$(N=34,257)$</td>
<td>$\begin{gathered} 0.787(0.780- \ 0.793) \end{gathered}$</td>
<td>$\begin{gathered} 0.470(0.464- \ 0.475) \end{gathered}$</td>
<td>$\begin{gathered} 0.950(0.942- \ 0.957) \end{gathered}$</td>
<td>$\begin{gathered} 0.418(0.413- \ 0.424) \end{gathered}$</td>
<td>$\begin{gathered} 0.148(0.144- \ 0.153) \end{gathered}$</td>
<td>$\begin{gathered} 0.987(0.985- \ 0.989) \end{gathered}$</td>
<td></td>
<td></td>
</tr>
<tr>
<td>MSH</td>
<td>Internal</td>
<td>MSH $(N=8,388)$</td>
<td>$\begin{gathered} 0.802(0.793- \ 0.812) \end{gathered}$</td>
<td>$\begin{gathered} 0.617(0.607- \ 0.628) \end{gathered}$</td>
<td>$\begin{gathered} 0.950(0.942- \ 0.958) \end{gathered}$</td>
<td>$\begin{gathered} 0.432(0.419- \ 0.446) \end{gathered}$</td>
<td>$\begin{gathered} 0.482(0.469- \ 0.495) \end{gathered}$</td>
<td>$\begin{gathered} 0.94(0.930- \ 0.949) \end{gathered}$</td>
</tr>
<tr>
<td></td>
<td>External</td>
<td>NIH $(N=22,062)$</td>
<td>$\begin{gathered} 0.717(0.687- \ 0.746) \end{gathered}$</td>
<td>$\begin{gathered} 0.184(0.179- \ 0.190) \end{gathered}$</td>
<td>$\begin{gathered} 0.951(0.917- \ 0.973) \end{gathered}$</td>
<td>$\begin{gathered} 0.175(0.170- \ 0.18) \end{gathered}$</td>
<td>$\begin{gathered} 0.014(0.012- \ 0.016) \end{gathered}$</td>
<td>$\begin{gathered} 0.997(0.994- \ 0.998) \end{gathered}$</td>
</tr>
<tr>
<td></td>
<td>External</td>
<td>IU $(N=3,807)$</td>
<td>$\begin{gathered} 0.756(0.674- \ 0.838) \end{gathered}$</td>
<td>$\begin{gathered} 0.099(0.089- \ 0.109) \end{gathered}$</td>
<td>$\begin{gathered} 0.974(0.865- \ 0.999) \end{gathered}$</td>
<td>$\begin{gathered} 0.090(0.081- \ 0.099) \end{gathered}$</td>
<td>$\begin{gathered} 0.011(0.008- \ 0.015) \end{gathered}$</td>
<td>$\begin{gathered} 0.997(0.984- \ 1.000) \end{gathered}$</td>
</tr>
<tr>
<td></td>
<td>Superset *</td>
<td>MSH + NIH</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>$(N=30,450)$</td>
<td>$\begin{gathered} 0.862(0.856- \ 0.868) \end{gathered}$</td>
<td>$\begin{gathered} 0.562(0.557- \ 0.568) \end{gathered}$</td>
<td>$\begin{gathered} 0.950(0.942- \ 0.957) \end{gathered}$</td>
<td>$\begin{gathered} 0.516(0.510- \ 0.522) \end{gathered}$</td>
<td>$\begin{gathered} 0.19(0.184- \ 0.197) \end{gathered}$</td>
<td>$\begin{gathered} 0.989(0.987- \ 0.990) \end{gathered}$</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Superset *</td>
<td>MSH + NIH + IU</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>$(N=34,257)$</td>
<td>$\begin{gathered} 0.871(0.865- \ 0.877) \end{gathered}$</td>
<td>$\begin{gathered} 0.577(0.572- \ 0.582) \end{gathered}$</td>
<td>$\begin{gathered} 0.950(0.942- \ 0.957) \end{gathered}$</td>
<td>$\begin{gathered} 0.537(0.532- \ 0.543) \end{gathered}$</td>
<td>$\begin{gathered} 0.180(0.174- \ 0.185) \end{gathered}$</td>
<td>$\begin{gathered} 0.990(0.989- \ 0.992) \end{gathered}$</td>
<td></td>
<td></td>
</tr>
<tr>
<td>MSH + NIH</td>
<td>Internal</td>
<td>MSH + NIH</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>$(N=30,450)$</td>
<td>$\begin{gathered} 0.931(0.927- \ 0.936) \end{gathered}$</td>
<td>$\begin{gathered} 0.732(0.727- \ 0.737) \end{gathered}$</td>
<td>$\begin{gathered} 0.950(0.942- \ 0.957) \end{gathered}$</td>
<td>$\begin{gathered} 0.706(0.700- \ 0.711) \end{gathered}$</td>
<td>$\begin{gathered} 0.279(0.271- \ 0.288) \end{gathered}$</td>
<td>$\begin{gathered} 0.992(0.990- \ 0.993) \end{gathered}$</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Subset $\dagger$</td>
<td>NIH $(N=22,062)$</td>
<td>$\begin{gathered} 0.733(0.703- \ 0.762) \end{gathered}$</td>
<td>$\begin{gathered} 0.243(0.237- \ 0.249) \end{gathered}$</td>
<td>$\begin{gathered} 0.951(0.917- \ 0.973) \end{gathered}$</td>
<td>$\begin{gathered} 0.234(0.229- \ 0.240) \end{gathered}$</td>
<td>$\begin{gathered} 0.015(0.013- \ 0.017) \end{gathered}$</td>
<td>$\begin{gathered} 0.997(0.996- \ 0.999) \end{gathered}$</td>
</tr>
<tr>
<td></td>
<td>Subset $\dagger$</td>
<td>MSH $(N=8,388)$</td>
<td>$\begin{gathered} 0.805(0.796- \ 0.814) \end{gathered}$</td>
<td>$\begin{gathered} 0.630(0.619- \ 0.640) \end{gathered}$</td>
<td>$\begin{gathered} 0.950(0.942- \ 0.958) \end{gathered}$</td>
<td>$\begin{gathered} 0.451(0.438- \ 0.465) \end{gathered}$</td>
<td>$\begin{gathered} 0.491(0.478- \ 0.504) \end{gathered}$</td>
<td>$\begin{gathered} 0.942(0.933- \ 0.951) \end{gathered}$</td>
</tr>
<tr>
<td></td>
<td>External</td>
<td>IU $(N=3,807)$</td>
<td>$\begin{gathered} 0.815(0.745- \ 0.885) \end{gathered}$</td>
<td>$\begin{gathered} 0.238(0.224- \ 0.252) \end{gathered}$</td>
<td>$\begin{gathered} 0.974(0.865- \ 0.999) \end{gathered}$</td>
<td>$\begin{gathered} 0.230(0.217- \ 0.244) \end{gathered}$</td>
<td>$\begin{gathered} 0.013(0.009- \ 0.018) \end{gathered}$</td>
<td>$\begin{gathered} 0.999(0.994- \ 1.000) \end{gathered}$</td>
</tr>
<tr>
<td></td>
<td>Superset *</td>
<td>MSH + NIH + IU</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>$(N=34,257)$</td>
<td>$\begin{gathered} 0.934(0.929- \ 0.938) \end{gathered}$</td>
<td>$\begin{gathered} 0.732(0.727- \ 0.737) \end{gathered}$</td>
<td>$\begin{gathered} 0.95(0.942- \ 0.957) \end{gathered}$</td>
<td>$\begin{gathered} 0.709(0.703- \ 0.714) \end{gathered}$</td>
<td>$\begin{gathered} 0.258(0.250- \ 0.266) \end{gathered}$</td>
<td>$\begin{gathered} 0.993(0.991- \ 0.994) \end{gathered}$</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>*Superset $=$ a test dataset containing data from the same distribution (hospital system) as the training data as well as external data. $\dagger$ Subset $=$ a test dataset containing data from fewer distributions (hospital systems) than the training data. Abbreviations: AUC, area under the receiver operating characteristic curve; IU, Indiana University Network for Patient Care; MSH, Mount Sinai Hospital; NIH, National Institutes of Health Clinical Center; NPV, negative predictive value; PPV, positive predictive value. https://doi.org/10.1371/journal.pmed. 1002683.t002</p>
<p>Internal MSH-NIH performance of all models trained on imbalanced cohorts was significantly better than their corresponding external performance on IU (external MSH Severe AUC $0.641,95 \%$ CI $0.552-0.730, P&lt;0.001$; MSH Mild AUC $0.650,95 \%$ CI $0.548-0.752$, $P&lt;0.001$; NIH Mild AUC $0.703,95 \%$ CI $0.616-0.790, P=0.027$; NIH Severe AUC $0.683,95 \%$ CI $0.591-0.775, P&lt;0.001$ ). Internal MSH-NIH performance did not significantly exceed external IU performance for Balanced ( $0.739,95 \%$ CI $0.707-0.772$ versus $0.732,95 \%$ CI $0.645-$ $0.819, P=0.880)$.</p>
<h1>Discussion</h1>
<p>We have demonstrated that pneumonia-screening CNNs trained on data from individual or multiple hospital systems did not consistently generalize to external sites, nor did they make predictions exclusively based on underlying pathology. Given the significant interest in using deep learning to analyze radiological imaging, our findings should give pause to those considering rapid deployment of such systems without first assessing their performance in a variety</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig 2. CNN to predict hospital system detects both general and specific image features. (A) We obtained activation heatmaps from our trained model and averaged over a sample of images to reveal which subregions tended to contribute to a hospital system classification decision. Many different subregions strongly predicted the correct hospital system, with especially strong contributions from image corners. (B-C) On individual images, which have been normalized to highlight only the most influential regions and not all those that contributed to a positive classification, we note that the CNN has learned to detect a metal token that radiology technicians place on the patient in the corner of the image field of view at the time they capture the image. When these strong features are correlated with disease prevalence, models can leverage them to indirectly predict disease. CNN, convolutional neural network.</p>
<p>https://doi.org/10.1371/journal.pmed.1002683.g002</p>
<p>of real-world clinical settings. To our knowledge, no prior studies have assessed whether radiological CNNs generalized to external datasets. We note that the issue of not generalizing externally is distinct from typical train/test performance degradation, in which overfitting to training data leads to lower performance on testing data: in our experiments, all results are reported on held-out test data exclusively in both internal and external comparisons. Performance of the jointly trained MSH–NIH model on the joint test set (AUC 0.931) was higher than performance on either individual dataset (AUC 0.805 and 0.733, respectively), likely because the model was able to calibrate to different prevalences across hospital systems in the joint test set but not individual test sets. A simple calibration-based non-CNN model that used hospital system pneumonia prevalence only to make predictions and ignored image features achieved AUC 0.861 because of the large difference in pneumonia prevalence between the MSH and NIH test sets. Calibration plots confirmed that a model trained on NIH data was poorly calibrated to MSH and vice versa.</p>
<p>By engineering cohorts of varying prevalence, we demonstrated that the more predictive a hospital system was of pneumonia, the more it was exploited to make predictions, which led to poor generalization on external datasets. We noted that metallic tokens indicating laterality often appeared in radiographs in a site-specific way, which made site identification trivial. However, CNNs did not require this indicator: most image subregions contained features indicative of a radiograph's origin. These results suggest that CNNs could rely on subtle differences in acquisition protocol, image processing, or distribution pipeline (e.g., image compression) and overlook pathology. Radiological imaging of the same modality from different hospital systems can have markedly different CNN image feature distributions, as we highlighted in S3 Fig. Many of these features encode hospital processes, and many (e.g., site, scanner) may be associated with the prevalence of disease. This can lead to strong internal performance that is not realized on data from new sites. Even in the absence of recognized confounders, we would caution, following Recht and colleagues, that "current accuracy numbers are brittle and susceptible to even minute natural variations in the data distribution" [8].</p>
<p>A difficulty of using deep learning models in medicine is that they use a massive number of parameters, making it difficult to identify the specific variables driving predictions and</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig 3. Assessing how prevalence differences in aggregated datasets encouraged confounder exploitation. (A) Five cohorts of 20,000 patients were systematically subsampled to differ only in relative pneumonia risk based on the clinical training data sites. Model performance was assessed on test data from the internal hospital systems (MSH, NIH) and from an external hospital system (IU). (B) Although models perform better in internal testing in the presence of extreme prevalence differences, this benefit is not seen when applied to data from new hospital systems. The natural relative risk of disease at MSH, indicated by a vertical line, is quite imbalanced. IU, Indiana University Network for Patient Care; MSH, Mount Sinai Hospital; NIH, National Institutes of Health Clinical Center; ROC, receiver operating characteristic; RR, relative risk.
https://doi.org/10.1371/journal.pmed.1002683.g003
complicating the traditional understanding of model overfitting. Best-practice guidelines within the biomedical literature put a heavy emphasis on identifying all features used and transparently reporting model coefficients to promote the development of reproducible, clinically relevant models [23,24]. This level of transparency is fundamentally incompatible with deep learning. The DenseNet-121 used in our analysis had 6,963,081 parameters, and the methods available to interrogate which features were driving model predictions remain inexact (e.g., heatmaps). With this large number of parameters, we note that the phenomenon of deep learning models failing to generalize externally is distinct from the traditional understanding of overfitting in the statistical literature. If allowed to train indefinitely, a CNN would overfit</p>
<p>in a traditional sense and would fail to generalize both internally and externally. We followed standard practices in training deep learning models to ensure internal generalization between the training data and held-out test data. Specifically, we used a tune set to assess internal generalization performance after every step of optimization, and we stopped training early at the point when the tune performance stopped improving. We note that in this process, each of the train, tune, and test sets were drawn from the same distribution. The ability of a radiological CNN to perform well in external testing on "plausibly related' populations," which may differ significantly in their underlying data distributions, will be critical for the real-world utility and adaptation of such models [24].</p>
<p>Even the development of customized deep learning models that are trained, tuned, and tested with the intent of deploying at a single site are not necessarily a solution that can control for potential confounding variables. At a finer level, we found that CNNs could separate portable radiographs from the inpatient wards and emergency department in MSH data with 100\% accuracy and that these patient groups had significantly different prevalences of pneumonia. It was determined after the fact that devices from different manufacturers had been used in the inpatient units (Konica Minolta) and emergency department (Fujifilm), and the latter were stored in the picture archiving and communication system (PACS) in an inverted color scheme (i.e., air appears white) along with distinctive text indicating laterality and use of a portable scanner. While these identifying features were prominent to the model, they only became apparent to us after manual image review. If certain scanners within a hospital are used to evaluate patients with different baseline disease prevalences (e.g., intensive care unit [ICU] versus outpatient), these may confound deep learning models trained on radiological data. Fully external testing—ideally on a collection of data gathered from a varied collection of hospitals—can reveal and account for such sampling biases that may limit the generalizability of a model.</p>
<p>The development of CNN architectures specifically adapted to accommodate radiological imaging is an important step towards building stronger models. Entire high-resolution radiological images are often aggressively downsampled (e.g., to $224 \times 224$ pixels) to facilitate "transfer learning," i.e., fine-tuning preexisting CNN architectures that have been pretrained to ImageNet [15]. While practically convenient, these models are not optimal for the radiological context, as they ignore essential domain information about the problem (e.g., pneumonia is present in the lungs), and the low-resolution images they require eliminate valuable radiographic findings. Both factors can lead to an increased reliance on confounding factors in making predictions. CNN architectures designed specifically to accommodate radiological imaging have demonstrated promising early results, and more work in this area is needed [25-27].</p>
<p>While our analysis found degradation of model performance on external test sets, we note that it is possible for external test set performance to be either better or worse than internal. Many different aspects of dataset construction (e.g., inclusion criteria, labeling procedure) and the underlying clinical data (pathology prevalence and severity, confounding protocolized variables) can affect performance. For example, a model trained on noisily labeled data that included all available imaging might reasonably be expected to have lower internal test performance than if tested externally on a similar dataset manually selected and labeled by a physician as clear examples of pathological and normal cases.</p>
<p>In addition to site-specific confounding variables that threaten generalizability, there are other factors related to medical management that may exist everywhere but undermine the clinical applicability of a model. As has been noted, chest tubes that treat pneumothorax frequently appear in studies positive for pneumothorax in NIH data; a CNN for pneumothorax may learn to detect obvious chest tubes rather than a subtler pneumothorax itself [28]. If such a model were deployed in an emergency department, it might inaccurately negatively diagnose patients presenting with pneumothorax because they lacked a chest tube. A CNN for</p>
<p>pneumonia could potentially learn that radiographs obtained with portable scanners were more likely to contain pneumonia and assign such radiographs a higher probability of disease. Models exploiting such confounding factors that reflect common medical practice may even generalize well but ultimately have limited clinical utility. If CNN-based systems are to be used for medical diagnosis, they must be tailored to carefully considered clinical questions, prospectively tested at a variety of sites in real-world-use scenarios, and carefully assessed to determine how they impact diagnostic accuracy.</p>
<p>There are several limitations to this study. Most notably, without more granular details on the underlying patient populations, we are unable to fully assess what factors might be contributing to the hospital system-specific biasing of the models. The extremely high incidence of pneumonia in the MSH dataset is also a point of concern; however, we attribute this to differences in the underlying patient populations and variability in classification thresholds for pathology. First, a majority of MSH radiographs were portable inpatient scans, ordered for patients too unstable to travel to the radiology department for a standard radiograph. In contrast, all IU radiographs were outpatient. While the inpatient/outpatient mix from NIH is not reported, we believe it likely contains a substantial outpatient percentage given that the incidence of pneumonia is similar to IU. Second, our NLP approach for MSH assigned positive ground truth labels more liberally than NIH or IU, marking a study as positive for pathology when a radiologist explicitly commented on it as a possibility in a report, indicating that the radiographic appearance was consistent with the finding. Different radiologists may have different thresholds at which they explicitly include a possible diagnosis in their reports. Researchers working in this area will continually have to make decisions about their classification threshold for labeling a study positive or negative. We believe that either of these two factors can drive large differences in prevalences of pathology across datasets, and this variation can confound diagnostic CNNs.</p>
<p>An additional limitation was that radiologic diagnoses are made in the context of a patient's history and clinical presentation, something not incorporated into our approach. Positive findings on chest radiograph are necessary but not sufficient for the diagnosis of pneumonia, which is only made when the patient also exhibits a "constellation of suggestive clinical features" [29]. Finally, the relatively small size and low number of pneumonia cases in IU data led to wide CIs in IU test AUC and may have limited our ability to detect external performance degradation in some cases. Nevertheless, many key comparisons achieved statistical significance with even this smaller external dataset.</p>
<h1>Conclusion</h1>
<p>Pneumonia-screening CNNs achieved better internal than external performance in 3 out of 5 natural comparisons. When models were trained on pooled data from sites with different pneumonia prevalence, they performed better on new pooled data from these sites but not on external data. CNNs robustly identified hospital system and department within a hospital, which can have large differences in disease burden and may confound predictions.</p>
<h2>Supporting information</h2>
<p>S1 Table. Performance of NLP algorithm on 30\% test data. NLP, natural language processing.
(XLSX)
S2 Table. Internal and external pneumonia screening performance with MSH-NIH training cohorts with varying engineered prevalences of pneumonia by hospital system. MSH,</p>
<p>Mount Sinai Hospital; NIH, National Institutes of Health Clinical Center. (XLSX)</p>
<p>S1 Fig. Preprocessing diagram.
(TIF)
S2 Fig. STARD cohort splitting diagram.
(TIF)
S3 Fig. Calibration plot (train MSH, predict IU). IU, Indiana University Network for Patient Care; MSH, Mount Sinai Hospital.
(TIF)
S4 Fig. Calibration plot (train MSH, predict MSH). MSH, Mount Sinai Hospital. (TIF)</p>
<p>S5 Fig. Calibration plot (train MSH, predict MSH-NIH). MSH, Mount Sinai Hospital; NIH, National Institutes of Health Clinical Center.
(TIF)
S6 Fig. Calibration plot (train MSH, predict NIH). MSH, Mount Sinai Hospital; NIH, National Institutes of Health Clinical Center.
(TIF)
S7 Fig. Calibration plot (train MSH-NIH, predict IU). IU, Indiana University Network for Patient Care; MSH, Mount Sinai Hospital; NIH, National Institutes of Health Clinical Center.
(TIF)
S8 Fig. Calibration plot (train MSH-NIH, predict MSH). MSH, Mount Sinai Hospital; NIH, National Institutes of Health Clinical Center.
(TIF)
S9 Fig. Calibration plot (train MSH-NIH, predict MSH-NIH). MSH, Mount Sinai Hospital; NIH, National Institutes of Health Clinical Center.
(TIF)
S10 Fig. Calibration plot (train MSH-NIH, predict NIH). MSH, Mount Sinai Hospital; NIH, National Institutes of Health Clinical Center.
(TIF)
S11 Fig. Calibration plot (train NIH, predict IU). IU, Indiana University Network for Patient Care; NIH, National Institutes of Health Clinical Center.
(TIF)
S12 Fig. Calibration plot (train NIH, predict MSH). MSH, Mount Sinai Hospital; NIH, National Institutes of Health Clinical Center.
(TIF)
S13 Fig. Calibration plot (train NIH, predict MSH-NIH). MSH, Mount Sinai Hospital; NIH, National Institutes of Health Clinical Center.
(TIF)
S14 Fig. Calibration plot (train NIH, predict NIH). NIH, National Institutes of Health Clinical Center.
(TIF)</p>
<p>S15 Fig. Distribution of images' embedded principal components. Principal component analysis was performed on each image's bottleneck features and colored to reveal the distribution of hospital systems and pneumonia. MSH images are largely separable from radiographs acquired at IU and NIH. IU, Indiana University Network for Patient Care; MSH, Mount Sinai Hospital; NIH, National Institutes of Health Clinical Center.
(TIF)</p>
<h1>Author Contributions</h1>
<p>Conceptualization: John R. Zech, Marcus A. Badgeley, Eric Karl Oermann.
Data curation: John R. Zech, Marcus A. Badgeley, Anthony B. Costa, Joseph J. Titano, Eric Karl Oermann.</p>
<p>Formal analysis: John R. Zech, Marcus A. Badgeley, Manway Liu.
Investigation: John R. Zech, Marcus A. Badgeley.
Methodology: John R. Zech, Marcus A. Badgeley, Eric Karl Oermann.
Project administration: Anthony B. Costa, Joseph J. Titano.
Resources: Anthony B. Costa.
Software: John R. Zech, Marcus A. Badgeley.
Supervision: Eric Karl Oermann.
Validation: John R. Zech.
Visualization: John R. Zech, Marcus A. Badgeley.
Writing - original draft: John R. Zech, Marcus A. Badgeley, Eric Karl Oermann.
Writing - review \&amp; editing: John R. Zech, Marcus A. Badgeley, Manway Liu, Anthony B. Costa, Joseph J. Titano, Eric Karl Oermann.</p>
<h2>References</h2>
<ol>
<li>Wang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases; 2017 [cited 1 July 2018]. Preprint. Available from: https://arxiv.org/abs/1705.02315.</li>
<li>Rajpurkar P, Irvin J, Zhu K, Yang B, Mehta H, Duan T, et al. CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning; 2017 [cited 1 July 2018]. Preprint. Available from: https://arxiv.org/abs/1711.05225.</li>
<li>Gulshan V, Peng L, Coram M, Stumpe MC, Wu D, Narayanaswamy A, et al. Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs. JAMA. 2016; 316: 2402-2410. https://doi.org/10.1001/jama.2016.17216 PMID: 27898976</li>
<li>Ting DSW, Cheung CY-L, Lim G, Tan GSW, Quang ND, Gan A, et al. Development and Validation of a Deep Learning System for Diabetic Retinopathy and Related Eye Diseases Using Retinal Images From Multiethnic Populations With Diabetes. JAMA. 2017; 318: 2211-2223. https://doi.org/10.1001/jama. 2017.18152 PMID: 29234807</li>
<li>Kermany DS, Goldbaum M, Cai W, Valentim CCS, Liang H, Baxter SL, et al. Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning. Cell. Elsevier; 2018; 172: 1122-1131. https://doi.org/10.1016/j.cell.2018.02.010 PMID: 29474911</li>
<li>Russakovsky O, Deng J, Su H, Krause J, Satheesh S, Ma S, et al. ImageNet Large Scale Visual Recognition Challenge; 2014 [cited 1 July 2018]. Preprint. Available from: https://arxiv.org/abs/1409.0575.</li>
<li>
<p>Lecun Y, Bottou L, Bengio Y, Haffner P. Gradient-based learning applied to document recognition. In Intelligent signal processing. IEEE Press. 2001. p. 306-351.</p>
</li>
<li>
<p>Recht B, Roelofs R, Schmidt L, Shankar V. Do CIFAR-10 Classifiers Generalize to CIFAR-10?; 2018 [cited 1 July 2018]. Preprint. Available from: https://arxiv.org/abs/1806.00451.</p>
</li>
<li>Rothwell PM. External validity of randomised controlled trials: "To whom do the results of this trial apply?" Lancet. 2005; 365: 82-93. https://doi.org/10.1016/S0140-6736(04)17670-8 PMID: 15639683</li>
<li>Pandis N, Chung B, Scherer RW, Elbourne D, Altman DG. CONSORT 2010 statement: extension checklist for reporting within person randomised trials. BMJ. 2017; 357: j2835. https://doi.org/10.1136/ bmj.j2835 PMID: 28667088</li>
<li>Cabitza F, Rasoini R, Gensini GF. Unintended Consequences of Machine Learning in Medicine. JAMA. 2017; 318: 517-518. https://doi.org/10.1001/jama.2017.7797 PMID: 28727867</li>
<li>Demner-Fushman D, Kohli MD, Rosenman MB, Shooshan SE, Rodriguez L, Antani S, et al. Preparing a collection of radiology examinations for distribution and retrieval. J Am Med Inform Assoc. 2016; 23: 304-310. https://doi.org/10.1093/jamia/ocv080 PMID: 26133894</li>
<li>LeCun Y, Bengio Y, Hinton G. Deep learning. Nature. 2015; 521: 436-444. https://doi.org/10.1038/ nature14539 PMID: 26017442</li>
<li>He K, Zhang X, Ren S, Sun J. Deep Residual Learning for Image Recognition; 2015 [cited 1 July 2018]. Preprint. Available from: https://arxiv.org/abs/1512.03385.</li>
<li>Huang G, Liu Z, Weinberger KQ, van der Maaten L. Densely Connected Convolutional Networks; 2016 [cited 1 July 2018]. Preprint. Available from: arXiv: https://arxiv.org/abs/1608.06993.</li>
<li>Zhang C, Bengio S, Hardt M, Recht B, Vinyals O. Understanding deep learning requires rethinking generalization; 2016 [cited 1 July 2018]. Preprint. Available from: https://arxiv.org/abs/1611.03530.</li>
<li>Zech J, Pain M, Titano J, Badgeley M, Schefflein J, Su A, et al. Natural Language-based Machine Learning Models for the Annotation of Clinical Radiology Reports. Radiology. 2018; 287(2): 570-580. https://doi.org/10.1148/radiol.2018171093 PMID: 29381109</li>
<li>Paszke A, Gross S, Chintala S, Chanan G, Yang E, DeVito Z, et al. Automatic differentiation in PyTorch; 2017 [cited 1 July 2018]. Preprint. Available from: https://openreview.net/forum?id=BJJsrmfCZ.</li>
<li>Zhou B, Khosla A, Lapedriza A, Oliva A, Torralba A. Learning Deep Features for Discriminative Localization; 2015. Preprint. Available from: https://arxiv.org/abs/1512.04150.</li>
<li>DeLong ER, DeLong DM, Clarke-Pearson DL. Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach. Biometrics. 1988; 44: 837-845. PMID: 3203132</li>
<li>Robin X, Turck N, Hainard A, Tiberti N, Lisacek F, Sanchez J-C, et al. pROC: an open-source package for R and S+ to analyze and compare ROC curves. BMC Bioinformatics. 2011; 12: 77. https://doi.org/ 10.1186/1471-2105-12-77 PMID: 21414208</li>
<li>Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, et al. Scikit-learn: Machine Learning in Python. J Mach Learn Res. 2011; 12: 2825-2830.</li>
<li>Collins GS; Reitsma JB, Altman DG, Moons KGM. Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis (TRIPOD): The TRIPOD Statement. Ann Intern Med 2015; 162:55-63. https://doi.org/10.7326/M14-0697 PMID: 25560714</li>
<li>Steyerberg EW and Vergouwe Y. Towards better clinical prediction models: seven steps for development and an ABCD for validation. European Heart Journal 2014; 35:1925-1931. https://doi.org/10. 1093/eurheartj/ehu207 PMID: 24898551</li>
<li>Gale W, Oakden-Rayner L, Carneiro G, Bradley AP, Palmer LJ. Detecting hip fractures with radiologistlevel performance using deep neural networks; 2017 [cited 1 July 2018]. Preprint. Available from: https://arxiv.org/abs/1711.06504.</li>
<li>Mutasa S, Chang PD, Ruzal-Shapiro C, Ayyala R. MABAL: a Novel Deep-Learning Architecture for Machine-Assisted Bone Age Labeling. J Digit Imaging. 2018; 31(4): 513-519. https://doi.org/10.1007/ s10278-018-0053-3 PMID: 29404850</li>
<li>Geras KJ, Wolfson S, Gene Kim S, Moy L, Cho K. High-Resolution Breast Cancer Screening with MultiView Deep Convolutional Neural Networks; 2017 [cited 1 July 2018]. Preprint. Available from: https:// arxiv.org/abs/1703.07047.</li>
<li>Oakden-Rayner L. Exploring the ChestXray14 dataset: problems. 18 Dec 2017 [cited 26 Jan 2018]. In: Luke Oakden-Rayner [Internet]. Available from: https://lukeoakdenrayner.wordpress.com/2017/12/18/ the-chestxray14-dataset-problems/.</li>
<li>Mandell LA, Wunderink RG, Anzueto A, Bartlett JG, Campbell GD, Dean NC, et al. Infectious Diseases Society of America/American Thoracic Society consensus guidelines on the management of community-acquired pneumonia in adults. Clin Infect Dis. 2007; 44 Suppl 2: S27-72.</li>
</ol>            </div>
        </div>

    </div>
</body>
</html>
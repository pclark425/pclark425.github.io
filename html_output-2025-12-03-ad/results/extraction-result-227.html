<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-227 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-227</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-227</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-11.html">extraction-schema-11</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <p><strong>Paper ID:</strong> paper-91206346edbe28abb606d7b3425cd455d4019d4f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/91206346edbe28abb606d7b3425cd455d4019d4f" target="_blank">Scaling Relationship on Learning Mathematical Reasoning with Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is found that pre-training loss is a better indicator of the model's performance than the model’s parameter count and that with augmented samples containing more distinct reasoning paths, RFT improves mathematical reasoning performance more for LLMs.</p>
                <p><strong>Paper Abstract:</strong> Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling relationship of it with respect to LLM capacity is under-explored. In this paper, we investigate how the pre-training loss, supervised data amount, and augmented data amount influence the reasoning performances of a supervised LLM. We find that pre-training loss is a better indicator of the model's performance than the model's parameter count. We apply supervised fine-tuning (SFT) with different amounts of supervised data and empirically find a log-linear relation between data amount and model performance, and we find better models improve less with enlarged supervised datasets. To augment more data samples for improving model performances without any human effort, we propose to apply Rejection sampling Fine-Tuning (RFT). RFT uses supervised models to generate and collect correct reasoning paths as augmented fine-tuning datasets. We find with augmented samples containing more distinct reasoning paths, RFT improves mathematical reasoning performance more for LLMs. We also find RFT brings more improvement for less performant LLMs. Furthermore, we combine rejection samples from multiple models which push LLaMA-7B to an accuracy of 49.3\% on GSM8K which outperforms the supervised fine-tuning (SFT) accuracy of 35.9\% significantly.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e227.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e227.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SFT (GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised Fine-Tuning on GSM8K (human-annotated chain-of-thought)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard supervised fine-tuning of pre-trained LLMs using the GSM8K dataset of human-annotated math word problems with chain-of-thought reasoning and final numeric answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scaling Relationship on Learning Mathematical Reasoning with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>SFT</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>math reasoning (math word problem QA)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Human-annotated chain-of-thought demonstrations and final answers (GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>7.4K examples (GSM8K training set)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>ground-truth chain-of-thought (r), final numeric answers (a); high-quality human demonstrations; one reasoning path per question in original dataset</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>maj1@1 accuracy (single greedy decode) and maj1@100 (majority over 100 samples)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>35.9% maj1@1 / 48.7% maj1@100 (LLaMA-7B SFT on GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>11.0% maj1@1 (ICL-8shot for LLaMA-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>+24.9 percentage points absolute (maj1@1 vs ICL-8shot for same model)</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Supervised fine-tuning on high-quality chain-of-thought examples (GSM8K) yields large absolute gains over in-context learning, but gains diminish for models with lower pre-training loss (better base models).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Relationship on Learning Mathematical Reasoning with Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e227.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e227.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RFT (k-sampled)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rejection sampling Fine-Tuning (RFT) using model-generated correct reasoning paths</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Augment SFT data by sampling multiple candidate chain-of-thought traces from an SFT model, filtering to keep only correct-answer traces, deduplicating by equation-list form, and fine-tuning on the expanded set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>SFT</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>math reasoning (math word problem QA)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Synthetic augmented training examples: model-generated correct chain-of-thoughts (filtered by correct answer and Python check), deduplicated by equation-list</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>RFT k=100 produces ~47K augmented samples total (~~distinct paths per question leads to ~47K samples for GSM8K training set); per-question distinct-path counts vary (see properties)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>correctness-filtered; emphasizes diversity of reasoning chains (distinct equation-lists); data originates from SFT models at decoding temperature 0.7; deduplication preserves different equation order/forms</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>maj1@1 and maj1@100</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>41.7% maj1@1 / 52.7% maj1@100 (LLaMA-7B, RFT k=100)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>35.9% maj1@1 / 48.7% maj1@100 (LLaMA-7B SFT on GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>+5.8 percentage points absolute (maj1@1) compared to SFT; +4.0 points (maj1@100)</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Augmenting SFT with model-generated correct but diverse reasoning chains (RFT) improves single-pass accuracy substantially over vanilla SFT; improvements scale with the number of distinct reasoning paths rather than raw sample count and show diminishing returns as k doubles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Relationship on Learning Mathematical Reasoning with Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e227.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e227.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RFT-U13B (aggregated)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aggregated Rejection-sampled Dataset from multiple 7B/13B SFT models (RFT-U13B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A combined augmented training set formed by aggregating rejection-sampled correct reasoning paths from multiple independent SFT models (LLaMA-7B, LLaMA2-7B, LLaMA-13B, LLaMA2-13B), deduplicated by equation-list.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scaling Relationship on Learning Mathematical Reasoning with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA (fine-tuned on aggregated dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 13B (reported per base)</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>SFT (with aggregated RFT-augmented data)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>math reasoning (math word problem QA)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Aggregated model-generated correct chain-of-thoughts from multiple SFT models; deduplicated by equation-list</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>RFT-U13B: ~104K augmented samples (reported aggregate dataset size)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>higher distinct reasoning-path diversity (more unique equation-lists per question than single-model RFT); cross-model diversity yields unique calculation processes (~15% uniquely from models ≤13B in their U33B aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>maj1@1 and maj1@100</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>49.3% maj1@1 / 61.8% maj1@100 (LLaMA-7B fine-tuned on RFT-U13B)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>35.9% maj1@1 / 48.7% maj1@100 (LLaMA-7B SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>+13.4 percentage points absolute (maj1@1) compared to SFT</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Aggregating rejection-sampled correct chains from multiple SFT models yields substantially more distinct reasoning paths and produces much larger gains than self-sampled RFT from a single model; cross-model aggregation is an efficient way to increase reasoning diversity and performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Relationship on Learning Mathematical Reasoning with Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e227.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e227.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pre-training loss effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of Pre-training Loss on Downstream SFT/ICL Gains</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Measured relationship that lower pre-training loss of the base model correlates with higher SFT and ICL accuracy on GSM8K, and that better pre-trained models benefit less from additional SFT/RFT data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scaling Relationship on Learning Mathematical Reasoning with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple (GPT-3, LLaMA, LLaMA2, GPT-4 reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>multiple</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>math reasoning (math word problem QA)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Pre-training corpora (aggregate quality/amount reflected in pre-training loss)</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>N/A (pre-training loss is a model property rather than a single dataset size in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>pre-training loss (proxy for pre-trained model capability); reflects dataset/tokenizer differences and different pretraining corpora</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>maj1@1 accuracy (SFT and ICL)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Reported monotonic relationship across models: e.g., pretrain loss 1.8 → lower accuracy; pretrain loss 1.62 (larger models) → higher accuracy (see paper Table 1 for model-specific numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Lower pre-training loss correlates with higher downstream accuracy; specific table shows SFT and ICL roughly negatively linearly correlated with pre-training loss in the observed interval.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Pre-training quality (lower loss) is the strongest root factor: better-pretrained models start higher and gain less from SFT/RFT; thus improving pre-training is fundamentally more effective, though more expensive, than augmenting fine-tuning data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Relationship on Learning Mathematical Reasoning with Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e227.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e227.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Distinct reasoning-paths (data property)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distinct Reasoning Path Count (per-question diversity)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The number of distinct correct equation-lists (distinct reasoning-process forms) per question in augmented data; identified as the key data property driving RFT gains rather than raw augmented sample counts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scaling Relationship on Learning Mathematical Reasoning with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SFT models used for sampling (e.g., LLaMA-7B, LLaMA-13B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>SFT → RFT (data-generation stage)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>math reasoning (math word problem QA)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Model-generated correct chain-of-thoughts deduplicated by equation-list to count distinct reasoning processes</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>Per-question distinct-path counts reported: e.g., for k=100 sampling, distinct paths per question ~5.25 (7B), ~5.26 (13B), ~2.78 (33B); aggregated datasets produce higher averages (U13B/U33B shown in tables)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>diversity of calculation processes (different equation forms and orders); correctness-filtered; deduplication by equation-list</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>maj1@1 accuracy after RFT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Higher distinct-path counts correlate with larger RFT gains; example: 7B with ~5.25 distinct paths (k=100) yields +5.8 pts over SFT; 33B had many correct paths but fewer distinct ones and saw little/no RFT gain</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>SFT performance on same model (varies by model size)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>RFT improvement magnitude scales with distinct-path count; diminishing returns as distinct-path increases slowly with k</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Distinct reasoning-path diversity (unique equation-lists) is the primary determinant of RFT effectiveness — more distinct correct reasoning chains per question yield larger gains, while simply increasing raw sampled candidates (k) gives diminishing returns because distinct-path growth is sublinear.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Relationship on Learning Mathematical Reasoning with Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e227.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e227.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dedup by equation-list</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Equation-list Deduplication of Sampled Reasoning Paths</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Deduplication procedure that groups generated reasoning chains by their extracted canonical equation-list form and keeps one representative per distinct equation-list, optionally selecting the most lexically dissimilar representative via Levenshtein distance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scaling Relationship on Learning Mathematical Reasoning with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>used during RFT data construction (models that generate samples, e.g., LLaMA-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>SFT → RFT (data preprocessing stage)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>math reasoning (math word problem QA)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Post-processing of model-generated chains: deduplicated synthetic examples by equation-list; selection may use Levenshtein distance to pick the most diverse path among duplicates</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>Reduces raw augmented sample counts to number of distinct equation-lists; e.g., RFT k=100 leads to ~47K raw augmented samples but dedup yields fewer (~per-question distinct counts as reported)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>keeps distinct calculation forms; reduces redundancy; preserves different equation orders/forms (considered different when they produce different equation string order)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>maj1@1 accuracy after RFT and training time (compute)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Dedup vs no-dedup: similar or slightly better accuracy in most cases and much lower training time; example paper reports similar performance between RFT k=100 and 'no dedup' variant but dedup uses less training compute</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Dedup maintains RFT gains while lowering compute; improves sample-efficiency of augmented data</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Deduplicating by equation-list preserves reasoning diversity while avoiding redundant training examples; dedup generally yields equal or better RFT performance with much less training time compared to using all sampled examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Relationship on Learning Mathematical Reasoning with Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e227.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e227.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self Query Augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self Query Augmentation (reverse-query generation from predicted reasoning chains)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-augmentation attempt that generates synthetic queries corresponding to model-produced reasoning chains (reverse of QA), then mixes or replaces original training data with these synthetic query–reasoning pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scaling Relationship on Learning Mathematical Reasoning with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>SFT (augmentation stage)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>math reasoning (math word problem QA)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Synthetic queries generated by a model trained to invert reasoning -> query mapping; paired with model-predicted reasoning chains</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>Not precisely quantified in final positive results; experiments mixed generated queries with original GSM8K or used generated-only training — both evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>synthetic (model-generated) queries; quality issues include incorrect arithmetic in reasoning chains or mismatched query-reasoning pairing</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>maj1@1 accuracy (GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>35.9% maj1@1 (LLaMA-7B SFT on original GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>negative: fine-tuning with self-query augmented data led to worst results; mixing original+generated still worse than original-only SFT</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Naive self-query augmentation performed worse than original human demonstrations — generated queries often misalign with reasoning chains or contain reasoning errors, reducing fine-tuning effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Relationship on Learning Mathematical Reasoning with Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e227.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e227.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self Revising Augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self Revising Augmentation (train a model to revise sampled chains into ground-truth chains)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Train a model to revise one sampled reasoning path (or one among K) into the ground-truth chain-of-thought, then apply that reviser at inference to improve predictions; selection of diverse training revisions improves effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scaling Relationship on Learning Mathematical Reasoning with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>SFT (augmentation / second-stage reviser)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>math reasoning (math word problem QA)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Pairs of (query + sampled reasoning path) → ground-truth reasoning chain; sampled paths produced by SFT model (temperature 0.7)</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>Varied by K (number of sampled candidates); experiments used K={1,3,6,12,...}; training via N-fold procedures also tested</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>relies on lexical diversity of sampled paths; selecting the sampled path with largest Levenshtein distance to ground-truth improved reviser training</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>maj1@1 accuracy (GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Small numeric example: baseline SFT 35.90% vs reviser K=1 gives 36.09% (marginal improvement). Larger K without diversity-selection degraded; using Levenshtein-based selection yielded uniform improvements above baseline (no single numeric aggregate provided).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>35.9% maj1@1 (LLaMA-7B SFT on GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>minor: ~+0.19 percentage points in a K=1 case; larger K required diversity-selection to avoid degradation</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Revising augmentation can yield small improvements if training pairs emphasize lexical/diverse sampled paths (e.g., Levenshtein-based selection); naive revising with many similar sampled paths can harm performance due to train/test distribution mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Relationship on Learning Mathematical Reasoning with Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e227.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e227.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Temperature / k sampling effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decoding Temperature and k (number of samples) impact on RFT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generation config choices (sampling temperature and number of candidate samples k) materially affect the number of correct and distinct reasoning paths obtained for RFT and thus downstream gains; increasing k increases distinct-paths sublinearly yielding diminishing RFT returns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scaling Relationship on Learning Mathematical Reasoning with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various SFT models used as samplers (7B, 13B, 33B reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>SFT (used to generate samples for RFT)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>math reasoning (math word problem QA)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>model-generated chain-of-thought samples under decoding settings (temperature e.g., 0.7, 1.0) and different k (1..400)</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>k tested in {1,3,6,12,25,50,100,400,500}; example: k=100 used widely producing ~47K augmented samples for GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>higher temperature and larger k can increase number of correct samples and distinct reasoning paths but growth in distinct-paths is sublinear; large models may need higher temperature to regain diversity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>maj1@1 after RFT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>RFT performance generally increases with k (e.g., LLaMA-7B: k=1→~37.6% maj1@1; k=100→41.7% maj1@1) but with diminishing returns on doubling k; 33B required temperature tuning to generate diverse paths and saw little RFT gain at default settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>SFT on same model (e.g., 35.9% maj1@1 for LLaMA-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Example: LLaMA-7B RFT k=1 ≈ +1.7 pts vs SFT; k=100 ≈ +5.8 pts vs SFT; per-doubling k yields smaller incremental gains.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Increasing k and adjusting temperature can produce more correct and distinct reasoning paths for augmentation, but distinct-path growth is sublinear so RFT gains diminish as k increases; large models may need different decoding configs to produce diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scaling Relationship on Learning Mathematical Reasoning with Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Solving math word problems with process- and outcome-based feedback <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>STar: Bootstrapping reasoning with reasoning <em>(Rating: 2)</em></li>
                <li>Let's verify step by step <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-227",
    "paper_id": "paper-91206346edbe28abb606d7b3425cd455d4019d4f",
    "extraction_schema_id": "extraction-schema-11",
    "extracted_data": [
        {
            "name_short": "SFT (GSM8K)",
            "name_full": "Supervised Fine-Tuning on GSM8K (human-annotated chain-of-thought)",
            "brief_description": "Standard supervised fine-tuning of pre-trained LLMs using the GSM8K dataset of human-annotated math word problems with chain-of-thought reasoning and final numeric answers.",
            "citation_title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
            "mention_or_use": "use",
            "model_name": "LLaMA",
            "model_size": "7B",
            "training_stage": "SFT",
            "task_type": "math reasoning (math word problem QA)",
            "is_scientific_domain": false,
            "data_type": "Human-annotated chain-of-thought demonstrations and final answers (GSM8K)",
            "data_size": "7.4K examples (GSM8K training set)",
            "data_properties": "ground-truth chain-of-thought (r), final numeric answers (a); high-quality human demonstrations; one reasoning path per question in original dataset",
            "performance_metric": "maj1@1 accuracy (single greedy decode) and maj1@100 (majority over 100 samples)",
            "performance_with_data": "35.9% maj1@1 / 48.7% maj1@100 (LLaMA-7B SFT on GSM8K)",
            "performance_baseline": "11.0% maj1@1 (ICL-8shot for LLaMA-7B)",
            "performance_lift": "+24.9 percentage points absolute (maj1@1 vs ICL-8shot for same model)",
            "compares_data_types": true,
            "key_finding": "Supervised fine-tuning on high-quality chain-of-thought examples (GSM8K) yields large absolute gains over in-context learning, but gains diminish for models with lower pre-training loss (better base models).",
            "uuid": "e227.0",
            "source_info": {
                "paper_title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "RFT (k-sampled)",
            "name_full": "Rejection sampling Fine-Tuning (RFT) using model-generated correct reasoning paths",
            "brief_description": "Augment SFT data by sampling multiple candidate chain-of-thought traces from an SFT model, filtering to keep only correct-answer traces, deduplicating by equation-list form, and fine-tuning on the expanded set.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA",
            "model_size": "7B",
            "training_stage": "SFT",
            "task_type": "math reasoning (math word problem QA)",
            "is_scientific_domain": false,
            "data_type": "Synthetic augmented training examples: model-generated correct chain-of-thoughts (filtered by correct answer and Python check), deduplicated by equation-list",
            "data_size": "RFT k=100 produces ~47K augmented samples total (~~distinct paths per question leads to ~47K samples for GSM8K training set); per-question distinct-path counts vary (see properties)",
            "data_properties": "correctness-filtered; emphasizes diversity of reasoning chains (distinct equation-lists); data originates from SFT models at decoding temperature 0.7; deduplication preserves different equation order/forms",
            "performance_metric": "maj1@1 and maj1@100",
            "performance_with_data": "41.7% maj1@1 / 52.7% maj1@100 (LLaMA-7B, RFT k=100)",
            "performance_baseline": "35.9% maj1@1 / 48.7% maj1@100 (LLaMA-7B SFT on GSM8K)",
            "performance_lift": "+5.8 percentage points absolute (maj1@1) compared to SFT; +4.0 points (maj1@100)",
            "compares_data_types": true,
            "key_finding": "Augmenting SFT with model-generated correct but diverse reasoning chains (RFT) improves single-pass accuracy substantially over vanilla SFT; improvements scale with the number of distinct reasoning paths rather than raw sample count and show diminishing returns as k doubles.",
            "uuid": "e227.1",
            "source_info": {
                "paper_title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "RFT-U13B (aggregated)",
            "name_full": "Aggregated Rejection-sampled Dataset from multiple 7B/13B SFT models (RFT-U13B)",
            "brief_description": "A combined augmented training set formed by aggregating rejection-sampled correct reasoning paths from multiple independent SFT models (LLaMA-7B, LLaMA2-7B, LLaMA-13B, LLaMA2-13B), deduplicated by equation-list.",
            "citation_title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
            "mention_or_use": "use",
            "model_name": "LLaMA (fine-tuned on aggregated dataset)",
            "model_size": "7B / 13B (reported per base)",
            "training_stage": "SFT (with aggregated RFT-augmented data)",
            "task_type": "math reasoning (math word problem QA)",
            "is_scientific_domain": false,
            "data_type": "Aggregated model-generated correct chain-of-thoughts from multiple SFT models; deduplicated by equation-list",
            "data_size": "RFT-U13B: ~104K augmented samples (reported aggregate dataset size)",
            "data_properties": "higher distinct reasoning-path diversity (more unique equation-lists per question than single-model RFT); cross-model diversity yields unique calculation processes (~15% uniquely from models ≤13B in their U33B aggregate)",
            "performance_metric": "maj1@1 and maj1@100",
            "performance_with_data": "49.3% maj1@1 / 61.8% maj1@100 (LLaMA-7B fine-tuned on RFT-U13B)",
            "performance_baseline": "35.9% maj1@1 / 48.7% maj1@100 (LLaMA-7B SFT)",
            "performance_lift": "+13.4 percentage points absolute (maj1@1) compared to SFT",
            "compares_data_types": true,
            "key_finding": "Aggregating rejection-sampled correct chains from multiple SFT models yields substantially more distinct reasoning paths and produces much larger gains than self-sampled RFT from a single model; cross-model aggregation is an efficient way to increase reasoning diversity and performance.",
            "uuid": "e227.2",
            "source_info": {
                "paper_title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Pre-training loss effect",
            "name_full": "Effect of Pre-training Loss on Downstream SFT/ICL Gains",
            "brief_description": "Measured relationship that lower pre-training loss of the base model correlates with higher SFT and ICL accuracy on GSM8K, and that better pre-trained models benefit less from additional SFT/RFT data.",
            "citation_title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
            "mention_or_use": "use",
            "model_name": "multiple (GPT-3, LLaMA, LLaMA2, GPT-4 reported)",
            "model_size": null,
            "training_stage": "multiple",
            "task_type": "math reasoning (math word problem QA)",
            "is_scientific_domain": false,
            "data_type": "Pre-training corpora (aggregate quality/amount reflected in pre-training loss)",
            "data_size": "N/A (pre-training loss is a model property rather than a single dataset size in this paper)",
            "data_properties": "pre-training loss (proxy for pre-trained model capability); reflects dataset/tokenizer differences and different pretraining corpora",
            "performance_metric": "maj1@1 accuracy (SFT and ICL)",
            "performance_with_data": "Reported monotonic relationship across models: e.g., pretrain loss 1.8 → lower accuracy; pretrain loss 1.62 (larger models) → higher accuracy (see paper Table 1 for model-specific numbers).",
            "performance_baseline": null,
            "performance_lift": "Lower pre-training loss correlates with higher downstream accuracy; specific table shows SFT and ICL roughly negatively linearly correlated with pre-training loss in the observed interval.",
            "compares_data_types": false,
            "key_finding": "Pre-training quality (lower loss) is the strongest root factor: better-pretrained models start higher and gain less from SFT/RFT; thus improving pre-training is fundamentally more effective, though more expensive, than augmenting fine-tuning data.",
            "uuid": "e227.3",
            "source_info": {
                "paper_title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Distinct reasoning-paths (data property)",
            "name_full": "Distinct Reasoning Path Count (per-question diversity)",
            "brief_description": "The number of distinct correct equation-lists (distinct reasoning-process forms) per question in augmented data; identified as the key data property driving RFT gains rather than raw augmented sample counts.",
            "citation_title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
            "mention_or_use": "use",
            "model_name": "SFT models used for sampling (e.g., LLaMA-7B, LLaMA-13B)",
            "model_size": null,
            "training_stage": "SFT → RFT (data-generation stage)",
            "task_type": "math reasoning (math word problem QA)",
            "is_scientific_domain": false,
            "data_type": "Model-generated correct chain-of-thoughts deduplicated by equation-list to count distinct reasoning processes",
            "data_size": "Per-question distinct-path counts reported: e.g., for k=100 sampling, distinct paths per question ~5.25 (7B), ~5.26 (13B), ~2.78 (33B); aggregated datasets produce higher averages (U13B/U33B shown in tables)",
            "data_properties": "diversity of calculation processes (different equation forms and orders); correctness-filtered; deduplication by equation-list",
            "performance_metric": "maj1@1 accuracy after RFT",
            "performance_with_data": "Higher distinct-path counts correlate with larger RFT gains; example: 7B with ~5.25 distinct paths (k=100) yields +5.8 pts over SFT; 33B had many correct paths but fewer distinct ones and saw little/no RFT gain",
            "performance_baseline": "SFT performance on same model (varies by model size)",
            "performance_lift": "RFT improvement magnitude scales with distinct-path count; diminishing returns as distinct-path increases slowly with k",
            "compares_data_types": true,
            "key_finding": "Distinct reasoning-path diversity (unique equation-lists) is the primary determinant of RFT effectiveness — more distinct correct reasoning chains per question yield larger gains, while simply increasing raw sampled candidates (k) gives diminishing returns because distinct-path growth is sublinear.",
            "uuid": "e227.4",
            "source_info": {
                "paper_title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Dedup by equation-list",
            "name_full": "Equation-list Deduplication of Sampled Reasoning Paths",
            "brief_description": "Deduplication procedure that groups generated reasoning chains by their extracted canonical equation-list form and keeps one representative per distinct equation-list, optionally selecting the most lexically dissimilar representative via Levenshtein distance.",
            "citation_title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
            "mention_or_use": "use",
            "model_name": "used during RFT data construction (models that generate samples, e.g., LLaMA-7B)",
            "model_size": null,
            "training_stage": "SFT → RFT (data preprocessing stage)",
            "task_type": "math reasoning (math word problem QA)",
            "is_scientific_domain": false,
            "data_type": "Post-processing of model-generated chains: deduplicated synthetic examples by equation-list; selection may use Levenshtein distance to pick the most diverse path among duplicates",
            "data_size": "Reduces raw augmented sample counts to number of distinct equation-lists; e.g., RFT k=100 leads to ~47K raw augmented samples but dedup yields fewer (~per-question distinct counts as reported)",
            "data_properties": "keeps distinct calculation forms; reduces redundancy; preserves different equation orders/forms (considered different when they produce different equation string order)",
            "performance_metric": "maj1@1 accuracy after RFT and training time (compute)",
            "performance_with_data": "Dedup vs no-dedup: similar or slightly better accuracy in most cases and much lower training time; example paper reports similar performance between RFT k=100 and 'no dedup' variant but dedup uses less training compute",
            "performance_baseline": null,
            "performance_lift": "Dedup maintains RFT gains while lowering compute; improves sample-efficiency of augmented data",
            "compares_data_types": true,
            "key_finding": "Deduplicating by equation-list preserves reasoning diversity while avoiding redundant training examples; dedup generally yields equal or better RFT performance with much less training time compared to using all sampled examples.",
            "uuid": "e227.5",
            "source_info": {
                "paper_title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Self Query Augmentation",
            "name_full": "Self Query Augmentation (reverse-query generation from predicted reasoning chains)",
            "brief_description": "A data-augmentation attempt that generates synthetic queries corresponding to model-produced reasoning chains (reverse of QA), then mixes or replaces original training data with these synthetic query–reasoning pairs.",
            "citation_title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
            "mention_or_use": "use",
            "model_name": "LLaMA",
            "model_size": "7B",
            "training_stage": "SFT (augmentation stage)",
            "task_type": "math reasoning (math word problem QA)",
            "is_scientific_domain": false,
            "data_type": "Synthetic queries generated by a model trained to invert reasoning -&gt; query mapping; paired with model-predicted reasoning chains",
            "data_size": "Not precisely quantified in final positive results; experiments mixed generated queries with original GSM8K or used generated-only training — both evaluated",
            "data_properties": "synthetic (model-generated) queries; quality issues include incorrect arithmetic in reasoning chains or mismatched query-reasoning pairing",
            "performance_metric": "maj1@1 accuracy (GSM8K)",
            "performance_with_data": null,
            "performance_baseline": "35.9% maj1@1 (LLaMA-7B SFT on original GSM8K)",
            "performance_lift": "negative: fine-tuning with self-query augmented data led to worst results; mixing original+generated still worse than original-only SFT",
            "compares_data_types": true,
            "key_finding": "Naive self-query augmentation performed worse than original human demonstrations — generated queries often misalign with reasoning chains or contain reasoning errors, reducing fine-tuning effectiveness.",
            "uuid": "e227.6",
            "source_info": {
                "paper_title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Self Revising Augmentation",
            "name_full": "Self Revising Augmentation (train a model to revise sampled chains into ground-truth chains)",
            "brief_description": "Train a model to revise one sampled reasoning path (or one among K) into the ground-truth chain-of-thought, then apply that reviser at inference to improve predictions; selection of diverse training revisions improves effectiveness.",
            "citation_title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
            "mention_or_use": "use",
            "model_name": "LLaMA",
            "model_size": "7B",
            "training_stage": "SFT (augmentation / second-stage reviser)",
            "task_type": "math reasoning (math word problem QA)",
            "is_scientific_domain": false,
            "data_type": "Pairs of (query + sampled reasoning path) → ground-truth reasoning chain; sampled paths produced by SFT model (temperature 0.7)",
            "data_size": "Varied by K (number of sampled candidates); experiments used K={1,3,6,12,...}; training via N-fold procedures also tested",
            "data_properties": "relies on lexical diversity of sampled paths; selecting the sampled path with largest Levenshtein distance to ground-truth improved reviser training",
            "performance_metric": "maj1@1 accuracy (GSM8K)",
            "performance_with_data": "Small numeric example: baseline SFT 35.90% vs reviser K=1 gives 36.09% (marginal improvement). Larger K without diversity-selection degraded; using Levenshtein-based selection yielded uniform improvements above baseline (no single numeric aggregate provided).",
            "performance_baseline": "35.9% maj1@1 (LLaMA-7B SFT on GSM8K)",
            "performance_lift": "minor: ~+0.19 percentage points in a K=1 case; larger K required diversity-selection to avoid degradation",
            "compares_data_types": true,
            "key_finding": "Revising augmentation can yield small improvements if training pairs emphasize lexical/diverse sampled paths (e.g., Levenshtein-based selection); naive revising with many similar sampled paths can harm performance due to train/test distribution mismatch.",
            "uuid": "e227.7",
            "source_info": {
                "paper_title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Temperature / k sampling effect",
            "name_full": "Decoding Temperature and k (number of samples) impact on RFT",
            "brief_description": "Generation config choices (sampling temperature and number of candidate samples k) materially affect the number of correct and distinct reasoning paths obtained for RFT and thus downstream gains; increasing k increases distinct-paths sublinearly yielding diminishing RFT returns.",
            "citation_title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
            "mention_or_use": "use",
            "model_name": "various SFT models used as samplers (7B, 13B, 33B reported)",
            "model_size": null,
            "training_stage": "SFT (used to generate samples for RFT)",
            "task_type": "math reasoning (math word problem QA)",
            "is_scientific_domain": false,
            "data_type": "model-generated chain-of-thought samples under decoding settings (temperature e.g., 0.7, 1.0) and different k (1..400)",
            "data_size": "k tested in {1,3,6,12,25,50,100,400,500}; example: k=100 used widely producing ~47K augmented samples for GSM8K",
            "data_properties": "higher temperature and larger k can increase number of correct samples and distinct reasoning paths but growth in distinct-paths is sublinear; large models may need higher temperature to regain diversity",
            "performance_metric": "maj1@1 after RFT",
            "performance_with_data": "RFT performance generally increases with k (e.g., LLaMA-7B: k=1→~37.6% maj1@1; k=100→41.7% maj1@1) but with diminishing returns on doubling k; 33B required temperature tuning to generate diverse paths and saw little RFT gain at default settings.",
            "performance_baseline": "SFT on same model (e.g., 35.9% maj1@1 for LLaMA-7B)",
            "performance_lift": "Example: LLaMA-7B RFT k=1 ≈ +1.7 pts vs SFT; k=100 ≈ +5.8 pts vs SFT; per-doubling k yields smaller incremental gains.",
            "compares_data_types": true,
            "key_finding": "Increasing k and adjusting temperature can produce more correct and distinct reasoning paths for augmentation, but distinct-path growth is sublinear so RFT gains diminish as k increases; large models may need different decoding configs to produce diversity.",
            "uuid": "e227.8",
            "source_info": {
                "paper_title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Solving math word problems with process- and outcome-based feedback",
            "rating": 2
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2
        },
        {
            "paper_title": "STar: Bootstrapping reasoning with reasoning",
            "rating": 2
        },
        {
            "paper_title": "Let's verify step by step",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 1
        }
    ],
    "cost": 0.01897175,
    "model_str": null
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Scaling Relationship on Learning Mathematical Reasoning with Large Language Models</h1>
<p>Zheng Yuan , Hongyi Yuan ${ }^{* \dagger}$, Chengpeng $\mathbf{L i}^{\dagger}$, Guanting Dong ${ }^{\dagger}$, Keming Lu Chuanqi Tan, Chang Zhou, Jingren Zhou<br>Alibaba DAMO Academy<br>{yuanzheng.yuanzhen, yuanhongyi.yhy}@alibaba-inc.com<br>{lichengpeng.lcp,dongguanting.dgt,lukeming.lkm}@alibaba-inc.com<br>{chuanqi.tcq,ericzhou.zc, jingren.zhou}@alibaba-inc.com</p>
<h4>Abstract</h4>
<p>Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling relationship of it with respect to LLM capacity is under-explored. In this paper, we investigate how the pre-training loss, supervised data amount, and augmented data amount influence the reasoning performances of a supervised LLM. We find that pre-training loss is a better indicator of the model's performance than the model's parameter count. We apply supervised fine-tuning (SFT) with different amounts of supervised data and empirically find a log-linear relation between data amount and model performance, and we find better models improve less with enlarged supervised datasets. To augment more data samples for improving model performances without any human effort, we propose to apply Rejection sampling Fine-Tuning (RFT). RFT uses supervised models to generate and collect correct reasoning paths as augmented fine-tuning datasets. We find with augmented samples containing more distinct reasoning paths, RFT improves mathematical reasoning performance more for LLMs. We also find RFT brings more improvement for less performant LLMs. Furthermore, we combine rejection samples from multiple models which push LLaMA-7B to an accuracy of $49.3 \%$ on GSM8K which outperforms the supervised fine-tuning (SFT) accuracy of $35.9 \%$ significantly. We release our codes and rejection sampling augmented data in https://github.com/OFA-Sys/gsm8k-ScRel.</p>
<h2>1 INTRODUCTION</h2>
<p>Large language models (LLMs) (Anil et al., 2023; Touvron et al., 2023b; OpenAI, 2023) have shown considerable abilities in various math reasoning tasks (Saxton et al., 2019; Cobbe et al., 2021; Lightman et al., 2023). It is of interest to understand, predict, and improve an LLM's math reasoning ability based on different pre-trained LLMs and supervised datasets. With this knowledge, we can better decide the effort we put into improving the LLM or augmenting the dataset. Many recent works are focusing on using different prompts (Wei et al., 2022b; Yao et al., 2023) or ensembling / reranking multiple times of inferences (Cobbe et al., 2021; Uesato et al., 2022; Wang et al., 2023; Lightman et al., 2023) to improve models' reasoning performances. While in-context learning (ICL) and performing multiple inferences can improve performance, it is computationally expensive and not suitable for online deployment scenarios. Therefore, we focus on the performance of the supervised LLMs with inference only once which is a setting closer to online deployment.</p>
<p>To this end, we empirically investigate the scaling relationship of factors that influence the math reasoning abilities of a supervised LLM, including pre-training losses, the amount of supervised data, and the amount of augmented data. Firstly, we analyze the supervised fine-tuning (SFT) and ICL performance of LLMs. We observe that the pre-training loss is approximately negatively linear correlated to the SFT and ICL accuracy in a given interval which is a better performance indicator than pre-trained model sizes or pre-trained token counts. Secondly, we analyze the relationship</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The key findings of scaling relationship on learning math reasoning ability with LLMs.
between SFT and different amounts of supervised data. We observe that the model performance has a log-linear relation versus the supervised data amount while the increase diminishes with the better pre-trained model. Thirdly, we want to leverage the model itself to generate more supervised data to reinforce its reasoning ability and analyze the scaling relationship of the augmented data amount. We apply rejection sampling on SFT models to sample and select correct reasoning paths as augmented dataset (Uesato et al., 2022; Zhu et al., 2023). We use these augmented datasets to fine-tune base LLMs which would achieve better performances compared to SFT and we denote it as rejection sampling fine-tuning (RFT). We find the key factor influencing RFT performance is the distinct reasoning path amount which can be increased by sampling more times or combing samples from multiple models. We apply RFT on several pre-trained LLMs and show larger improvement on less performant models. We discuss the reason RFT works is it provides multiple reasoning paths which makes LLMs have better reasoning generalization. We also discuss that RFT is much cheaper than pre-training in computational resources while training an LLM with lower pre-training loss is the fundamental solution.</p>
<p>The key findings of this paper are shown in Figure 1 and are summarized here:</p>
<ul>
<li>When the pre-training loss gets smaller (i.e. the pre-trained model gets better), the model reasoning performances of SFT and ICL increase linearly within a range. The SFT performance improves slower than ICL.</li>
<li>SFT improves in a log-linear manner with the increase of supervised data amount. The benefits of increasing data amount diminish as the pre-trained model gets better.</li>
<li>The model performance for RFT improves as the distinct reasoning path amount increases. The RFT performance improves slower than SFT.</li>
<li>The combination of rejection sampling samples from multiple models further enhances the RFT performance, resulting in an accuracy of 49.3 for LLaMA-7B (+13.4 compared to SFT), 50.3 for LLaMA2-7B (+8.7 compared to SFT), 52.1 for LLaMA-13B (+9.1 compared to SFT), and 55.4 for LLaMA2-13B (+5.4 compared to SFT).</li>
</ul>
<h1>2 Related Works</h1>
<p>Learning Math Reasoning with LLMs Recent research on LLMs has discovered the emergent ability to solve reasoning tasks beyond a certain model scale (Wei et al., 2022a). Such reasoning abilities in LLMs can be elicited by fine-tuning, few-shot prompting, or zero-shot prompting (Cobbe et al., 2021; Wei et al., 2021; Nye et al., 2021; Wei et al., 2022b; Kojima et al., 2022). A large</p>
<p>amount of research focuses on the reasoning tasks of math word problems (MWP), and methods are evaluated on the benchmarks spanning different levels of MWPs (Koncel-Kedziorski et al. (2016); Patel et al. (2021); Lan et al. (2021); Cobbe et al. (2021); Jie et al. (2022); Yuan et al. (2023a); Fu et al. (2023a), inter alia). The core idea of improving the mathematical reasoning ability of LLMs is to aggregate various sampled reasoning paths during either fine-tuning or inference. Cobbe et al. (2021) trained and devised a reasoning path verifier to select the correct results during inference. Wang et al. (2023) proposed to sample various reasoning paths during inference and then derive the final result by majority voting on the answers or through verifiers (Li et al., 2023). Several works applied the idea of rejection sampling along with other techniques to filter the diverse sampled reasoning paths for fine-tuning data augmentation (Huang et al., 2022; Zelikman et al., 2022; Ni et al., 2023; Zhu et al., 2023). Rejection sampling is a simple-yet-effective fine-tuning augmentation technique and is also used for LLM alignment with human preference (Bai et al., 2022; Yuan et al., 2023b; Dong et al., 2023; Touvron et al., 2023b; Song et al., 2023). Uesato et al. (2022) explored to use of reinforcement learning methods for improving the mathematical reasoning abilities of LLMs and they further discussed the difference between outcome-based and process-based reward modeling. Followed by Lightman et al. (2023), they collected large-scale process-based supervision signals through human annotation and verified that LLMs can benefit more from process-based reward modeling with human-annotated supervision than outcome-based reward modeling. There is also prior research that distilled the emergent reasoning ability of LLMs to small language models (Fu et al., 2023b; Shridhar et al., 2023). Compared to previous works (Zelikman et al., 2022; Uesato et al., 2022; Zhu et al., 2023; Ni et al., 2023), we are using a simpler way of generating augmented samples without any trained process-level reward models and we are focusing on researching the scaling relationship between LLMs and math reasoning ability.</p>
<p>Scaling Laws of Large Language Models It is important to understand and predict the performance gain as the language model scales up. Kaplan et al. (2020) first investigated and derived a predictable relationship on how the number of model parameters and data sizes contribute to the loss over many orders of magnitudes. Hoffmann et al. (2022) refined the scaling laws in (Kaplan et al., 2020) and found the scaling laws for computation-optimal training. Muennighoff et al. (2023) explored and extended the scaling laws under a data-constrained scenario. Besides investigating the scaling performance for pre-training, Gao et al. (2022) discussed the scaling laws for overparameterized reward models for alignment with human preference, and Hernandez et al. (2021) developed scaling laws for transferring performance from pre-trained models to downstream tasks. Henighan et al. (2020); Caballero et al. (2022) investigated scaling laws of math problems. In this paper, we are investigating the scaling relationships of large language models on learning math word problems with pre-training losses, supervised data amount, and augmented data amount.</p>
<h1>3 THE FACTORS OF MATH REASONING ABILITY IN SUPERVISED LLM</h1>
<p>The target of this paper is to try to understand the performances of supervised LLMs in math reasoning. We expect a pre-trained LLM $\rho$ to learn reasoning ability from a supervised reasoning dataset $\mathcal{D}$. The dataset is defined by $\mathcal{D}=\left{q_{i}, r_{i}, a_{i}\right}_{i}$, where $q$ is a question, $r$ is a chain-of-thought reasoning path, and $a$ is a numerical answer. We perform supervised fine-tuning on dataset $\mathcal{D}$ to obtain an SFT model $\pi$. We use $\pi$ to generate reasoning paths and answers in the test set by greedy decoding and report the accuracy (i.e. $a c c$ or maj1@1) as our metric here.</p>
<h3>3.1 MODEL ACCURACY VS. PRE-TRAINING LOSS</h3>
<p>Previous works state that the larger LLM shows better reasoning ability across the same series of models (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023a;b), and we find LLaMA outperforms GPT-3 which shows the model parameter counts should not be the only indicator of reasoning ability. While LLMs have different architectures, model parameters, and pre-training token numbers, we find the pre-training loss is a stable performance indicator of the math reasoning ability and we use it to represent the model instead of using their model parameters and pre-training token numbers.</p>
<p>We analyze the SFT and ICL (8-shot) performance of GPT-3 (Brown et al., 2020), LLaMA (Touvron et al., 2023a), LLaMA2 (Touvron et al., 2023b), and GPT-4 (OpenAI, 2023). The pre-training losses</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The performance of SFT (blue lines) and ICL (red lines) settings on GSM8K. GPT-4 states they use some part of the GSM8K data in pre-training, and suggest others consider its performance between SFT and ICL.
of these models are observed in their paper, we should notice that pre-training losses correspond to different pre-training datasets and different tokenizers which means they could not be compared strictly (and we cannot use it to do any sort of regression directly) while the tendency among these losses is still enlightening. We use the results of GPT-3 fine-tuning from (Cobbe et al., 2021) and we fine-tune LLaMA and LLaMA2 on the GSM8K training set (detailed in Appendix A.1). For in-context learning, we use the results from LLaMA (Touvron et al., 2023a) and LLaMA2 (Touvron et al., 2023b) paper.
In Figure 2, we can find that:</p>
<ul>
<li>The pre-training losses are approximately negatively linear correlated to the SFT and ICL accuracy during the given pre-training loss interval.</li>
<li>SFT outperforms ICL consistently, while the improvements diminish when the pre-training loss is lower.</li>
</ul>
<p>The linear relation of SFT and ICL accuracy may only work in the given interval. The reasons are (1) the slope of ICL is steeper than SFT, while the SFT performance should be greater than ICL performance; (2) the accuracy can not bigger than 1 or smaller than 0 . It should be using $-\log (a c c)$ instead of $a c c$ as the dependent variable theoretically while we find an apparent linear relationship among pre-training loss and $a c c$ and use $a c c$ as the dependent variable. LLaMA-2 7B(13B) can be viewed as an approximation of continue-training of LLaMA 7B(13B). As it trains longer, its ICL and SFT performance both improve without changing the parameter count. From the observations, one effective way to improve reasoning ability is to train a better base model with lower pre-training loss (Pre-training is all you need!). The models with lower pre-training loss improve less from the fine-tuning which may be due to the models having already obtained more reasoning abilities during pre-training and the supervised data can provide less signal to supervise them.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The performance of SFT with different amounts of supervised data on GSM8K.</p>
<h1>3.2 Model Accuracy vs. Supervised Data Count</h1>
<p>Supervised fine-tuning does improve LLMs’ reasoning ability, we want to know how the supervised data amount influences the model's improvement. We fine-tune LLaMA and LLaMA2 with ${1,1 / 2,1 / 4,1 / 8,1 / 16,1 / 32}$ amount of the training set from GSM8K (detailed in Appendix A.2). We want to use this experiment to extrapolate the model performances if we have more supervised data. In Figure 3, we plot the results of training with different amounts of supervised data. From this figure, we can observe that:</p>
<ul>
<li>The model performance has a log-linear relation versus data amount. When the data amount doubles, the performance increases by a unit.</li>
<li>Better model needs more amount of data to outperform its ICL performance.</li>
<li>Better model benefits less when supervised data amount doubles.</li>
</ul>
<p>The log-linear relation is stable during ${1,1 / 2,1 / 4,1 / 8}$ amount of the training data. From the observation, it is straightforward to enlarge the training dataset to improve the performance, especially for worse models. For better models, it benefits less which echoes that better models have learned more reasoning ability during pre-training.</p>
<h3>3.3 Model Accuracy vs. Augmented Data Count</h3>
<p>Increasing the amount of math reasoning labeled data is difficult, especially proposing a new question. It is easy for a well-educated student to solve hundreds of math word problems per day, but it is very hard to come up with diverse and educational math problems. So our direction changes to augment new data using existing resources. We have tried augmenting new queries (detailed in Appendix D.1) and augmenting revisions (detailed in Appendix D.2). These approaches have none to marginal improvements compared to SFT. We find a simplified version of rejection sampling (Zhu et al., 2023) is a naive and effective way to augment new reasoning paths and can improve the model performance. And we find the key factor influences fine-tuning on rejection sampling (RFT) augmented data is distinct reasoning path amount. Combining rejection sampling samples from multiple</p>
<table>
<thead>
<tr>
<th>Setting</th>
<th>7B</th>
<th>7B-2</th>
<th>13B</th>
<th>13B-2</th>
<th>33B</th>
</tr>
</thead>
<tbody>
<tr>
<td>Pretrain loss</td>
<td>1.8</td>
<td>1.75</td>
<td>1.73</td>
<td>1.68</td>
<td>1.62</td>
</tr>
<tr>
<td>ICL</td>
<td>11.0/18.1</td>
<td>14.6/-</td>
<td>17.8/29.3</td>
<td>28.7/-</td>
<td>35.6/53.1</td>
</tr>
<tr>
<td>SFT</td>
<td>35.9/48.7</td>
<td>41.6/55.4</td>
<td>43.0/55.2</td>
<td>50.0/61.7</td>
<td>54.6/-</td>
</tr>
<tr>
<td>RFT $k=100$</td>
<td>41.7/52.7</td>
<td>47.5/58.7</td>
<td>49.1/59.9</td>
<td>54.8/65.4</td>
<td>54.5/-</td>
</tr>
<tr>
<td>Correct paths per question</td>
<td>53.3</td>
<td>60.8</td>
<td>62.5</td>
<td>71.6</td>
<td>88.7</td>
</tr>
<tr>
<td>Distinct paths per question</td>
<td>5.25</td>
<td>5.19</td>
<td>5.26</td>
<td>5.29</td>
<td>2.78</td>
</tr>
</tbody>
</table>
<p>Table 1: The performance of RFT with $k=100$ on GSM8K compared with SFT and ICL. Distinct path amount means distinct equation list amount here.
models, we can further fine-tune a LLaMA-7B model to an accuracy of 49.3 (compared with SFT 35.9) and a LLaMA-13B model to an accuracy of 52.1 (compared with SFT 43.0).</p>
<p>Rejection Sampling Fine-tuning The SFT model $\pi$ obtains the ability to perform zero-shot chain-of-thought reasoning, and we use $\pi$ to generate more correct reasoning paths $r_{i j}$ to supply the training dataset. For each $q_{i}$, we generate $k$ candidate reasoning paths and answers $r, a$ with a temperature of 0.7 following (Cobbe et al., 2021). We first filter out reasoning paths with wrong answers $a \neq a_{i}$ or wrong calculations based on Python evaluation. Each reasoning path contains a list of equations $e_{j}$, and we select one reasoning path $r_{i j}$ for each distinct equation list as the augmented data and remove other reasoning paths with the same list of equations to deduplicate similar reasoning paths. Different order of elements (e.g. $3+4=7$ and $4+3=7$ ) or different order of equations (e.g. $1+2=3,3+4=7$ and $1+4=5,2+5=7$ ) are considered different. It is helpful for models to know these orders can be exchanged and is hard for models to learn this with only one reasoning path each problem. We define $\mathcal{D}<em i="i">{\pi}^{\prime}=\mathcal{D} \cup\left{q</em>\right}}, r_{i j}, a_{i<em _RFT="{RFT" _text="\text">{i, j}$ as the augmented dataset. We fine-tune $\mathcal{D}^{\prime}$ on pre-trained LLM $\rho$ to $\pi</em>$ as RFT, and we detail how we apply RFT in Appendix A.3. We list the results of RFT with sampling $k=100$ candidate reasoning paths on LLaMA and LLaMA-2 in Table 1. For ICL, SFT, and RFT, we list the maj1@1 (accuracy) and maj1@100 (sample 100 times and calculate accuracy based on majority voting) as metrics.}</p>
<p>In the case of 7B and 13B models, RFT yields an approximate increase of 5 to 6 points in maj1@1 and about 4 points increase in maj1@100. For 33B models, RFT does not improve performance compared to SFT. The main reason comes from the augmented samples from rejection sampling. We can find that better models generate more correct reasoning paths per question. For LLaMA-33B-SFT, it can generate an average of 88.7 correct paths per question. However, it overfits the training set and has difficulty generating more diverse paths on the training set questions. Rejection sampling with 33B is very time-consuming and we do not conduct a temperate grid search, we have tried using a larger temperate 1.0 for decoding LLaMA-33B-SFT models, it generates 82.4 correct paths and 4.77 distinct paths per question which is more diverse than using temperate 0.7 but still less diverse than 7B and 13B models. We admit there should be a temperate (or generation config) that can produce more distinct paths and generate good results for RFT in 33B and even larger models while it does need more computation resources for inference compared to sampling using 7B and 13B models. We will show we can use 7B and 13B models only for rejection sampling to improve the 33B model.</p>
<p>Model Accuracy vs Rejection Sampling Data Count To understand the performance of RFT, we vary $k$ among $1,3,6,12,25,50,100$ and apply RFT. We also have another setting of $k=100$ while not removing any reasoning paths denoted as no dedup. We list the RFT results with different $k$ on Figure 4. Comparing using RFT with $k=100$ and no dedup, the performance is similar and shows that it is better to estimate RFT performance based on distinct reasoning path amount instead of RFT augmented sample counts. Furthermore, using deduplication has better performances for 3 of 4 models and needs much less training time.
When using $k=3$, RFT outperforms SFT by 2 points stably. For most data points, using larger $k$ leads to better performances. However, the merits of RFT are decreasing when doubling $k$. We calculate different paths per question for different $k$ in Table 2. We can see that the amount of different reasoning paths is not growing quickly along $k$ growing. In Figure 3, we know doubling training samples can have a linear performance improvement. Doubling reasoning paths should</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The performance of RFT with different amounts of sampling count k on GSM8K.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">k</th>
<th style="text-align: center;">7B</th>
<th style="text-align: center;">7B-2</th>
<th style="text-align: center;">13B</th>
<th style="text-align: center;">13B-2</th>
<th style="text-align: center;">33B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1.17</td>
<td style="text-align: center;">1.19</td>
<td style="text-align: center;">1.15</td>
<td style="text-align: center;">1.18</td>
<td style="text-align: center;">1.06</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1.44</td>
<td style="text-align: center;">1.47</td>
<td style="text-align: center;">1.41</td>
<td style="text-align: center;">1.45</td>
<td style="text-align: center;">1.16</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">1.74</td>
<td style="text-align: center;">1.78</td>
<td style="text-align: center;">1.69</td>
<td style="text-align: center;">1.76</td>
<td style="text-align: center;">1.28</td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2.20</td>
<td style="text-align: center;">2.23</td>
<td style="text-align: center;">2.11</td>
<td style="text-align: center;">2.21</td>
<td style="text-align: center;">1.46</td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">2.93</td>
<td style="text-align: center;">2.93</td>
<td style="text-align: center;">2.88</td>
<td style="text-align: center;">2.94</td>
<td style="text-align: center;">1.77</td>
</tr>
<tr>
<td style="text-align: center;">50</td>
<td style="text-align: center;">3.94</td>
<td style="text-align: center;">3.91</td>
<td style="text-align: center;">3.90</td>
<td style="text-align: center;">3.94</td>
<td style="text-align: center;">2.19</td>
</tr>
<tr>
<td style="text-align: center;">100</td>
<td style="text-align: center;">5.25</td>
<td style="text-align: center;">5.19</td>
<td style="text-align: center;">5.26</td>
<td style="text-align: center;">5.29</td>
<td style="text-align: center;">2.78</td>
</tr>
<tr>
<td style="text-align: center;">400 (U13B)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">12.84</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">500 (U33B)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">13.65</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 2: Different reasoning paths per question generated by different SFT models with different k.
improve less than doubling training samples since obtaining different reasoning paths does not obtain any new questions. Therefore, doubling $k$ leads to diminished performance improvements.</p>
<p>Combining rejection sampling samples from multiple models The experiment results above demonstrate performance boosts in mathematical reasoning, benefitting from rejection sampling. Through case studies in 4.1, we show that rejection sampling can augment training data with reasoning paths of diverse calculation processes. However, the reasoning paths sampled from one single SFT model can be logically non-diverse. Therefore, we expect to further improve the mathematical reasoning performance by leveraging rejection sampled reasoning paths aggregated from different models. We denote two final datasets as $\mathcal{D}<em _mathrm_U="\mathrm{U">{\mathrm{U} 13 \mathrm{~B}}^{\prime}$ and $\mathcal{D}</em>} 33 \mathrm{~B}}^{\prime}$, which are aggregated from rejection sampling different models $\mathcal{D<em 7="7" _mathrm_B="\mathrm{~B">{\mathrm{U} 13 \mathrm{~B}}^{\prime}=\mathcal{D}</em>}}^{\prime} \oplus \mathcal{D<em 13="13" _mathrm_B="\mathrm{~B">{7 \mathrm{~B} 2}^{\prime} \oplus \mathcal{D}</em>}}^{\prime} \oplus \mathcal{D<em _mathrm_U="\mathrm{U">{13 \mathrm{~B} 2}^{\prime}$ and $\mathcal{D}</em>} 33 \mathrm{~B}}^{\prime}=\mathcal{D<em 33="33" _mathrm_B="\mathrm{~B">{\mathrm{U} 13 \mathrm{~B}}^{\prime} \oplus \mathcal{D}</em>$, where U means models under a certain size, 7B/13B/33B means LLaMA-7B/13B/33B and 7B2/13B2 means LLaMA2-7B/13B. $\oplus$ means an aggregation process in which all the reasoning paths from different sets are first combined and then Algorithm 1 is applied to deduplicate the reasoning paths with the same calculation process regarding the equation forms and orders.}}^{\prime</p>
<p>We can see, through the results visualized in Figure 5, that using the aggregated dataset $\mathcal{D}<em _mathrm_U="\mathrm{U">{\mathrm{U} 13 \mathrm{~B}}^{\prime}$ and $\mathcal{D}</em>} 33 \mathrm{~B}}^{\prime}$ can lead to uniformly better performance than fine-tuning with datasets from a single model across different model sizes. RFT on these two augmented datasets $\mathcal{D<em _mathrm_U="\mathrm{U">{\mathrm{U} 13 \mathrm{~B}}^{\prime}$ and $\mathcal{D}</em>$ decreases the performance gaps among the same size models in SFT and RFT $k=100$ which mean the combined augmented datasets provide enough reasoning supervision to fulfill the pre-training gap. We can assume with sufficient supervised data amounts, the performance indicator should be the model size but not the pre-training losses.} 33 \mathrm{~B}}^{\prime</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The performance of RFT with rejection sampling samples from multiple models.</p>
<p>We have stated that it is expensive to apply RFT $k=100$ on 33B models and it needs a temperate grid search to achieve an improvement compared to SFT. However fine-tuning on $\mathcal{D}_{\mathrm{U} 13 \mathrm{~B}}^{\prime}$ has similar rejection sampling computational cost compared with sampling 100 times on 33B and achieve better performance.</p>
<p>Another phenomenon is including $\mathcal{D}<em _mathrm_U="\mathrm{U">{\text {33B }}^{\prime}$ in aggregation barely influences the performance. To give a more comprehensive analysis of the results, we calculate the average reasoning path number per question in Table 2 and depict a Venn diagram to visualize the source of different reasoning paths shown in Figure 6. In Table 2, the average reasoning path numbers of $\mathcal{D}</em>} 13 \mathrm{~B}}^{\prime}$ and $\mathcal{D<em _mathrm_U="\mathrm{U">{\mathrm{U} 33 \mathrm{~B}}^{\prime}$ surpass those of a single model by large amounts, while $\mathcal{D}</em>} 33 \mathrm{~B}}^{\prime}$ only have slightly more reasoning paths than $\mathcal{D<em _mathrm_U="\mathrm{U">{\mathrm{U} 13 \mathrm{~B}}^{\prime}$ by 0.81 . In the meanwhile, as shown in Figure 6, the models under and including the size of 13B can contribute unique reasoning paths of similar proportion in $\mathcal{D}</em>$ around $15 \%$. However, only $6.5 \%$ of the reasoning paths can be exclusively acquired from LLaMA-33B-SFT model. This shows that the SFT model of 33B can provide limited reasoning diversity when sampling the training questions. This finding is consistent with the results above in Table 1, indicating the 33B model (and possibly 65B and 70B models) can well memorize the human-annotated reasoning paths.
For 65B models, we find using $\mathcal{D}_{\mathrm{U} 13 \mathrm{~B}}^{\prime}$ does not improve the performance compared to SFT. The reason can be better models benefit less from the supervised sample amounts while it has learnt more reasoning ability during pre-training.} 33 \mathrm{~B}}^{\prime</p>
<p>Overall, we can come to the conclusion that (1) RFT improves the mathematical reasoning performance of (worse) LLMs through diverse reasoning paths from rejection sampling of the SFT models, and aggregating more diverse reasoning paths can improve the performance further. (2) Different SFT models can contribute reasoning paths with different calculation processes from rejection sampling, leading to more diverse training data for RFT, and LLMs of larger parameter sizes may degrade in generating diversified reasoning paths as a result of overfitting the training questions. There may be a generation config or training config for large enough LMs not to overfit on the training dataset while it is not trivial to find them.</p>
<p>Comparing to other baselines We compare our RFT results of training on $\mathcal{D}_{\mathrm{U} 13 \mathrm{~B}}^{\prime}$ to several baselines and the results are detailed in Table 3. Although LLaMA and LLaMA2 are top-tier opensourced LLMs ${ }^{1}$, their mathematical reasoning performances still lag behind the current proprietary LLMs which are of larger parameter scales, such as GPT-4 and PaLM2. Compared to results on</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: The Venn diagram of the proportions of the reasoning calculation paths that each model provide to $\mathcal{D}<em _mathrm_U="\mathrm{U">{\mathrm{U} 33 \mathrm{~B}}^{\prime}$. For example, $15.5 \%$ (in the yellow part) of the reasoning calculation paths in $\mathcal{D}</em>$ can only be exclusively found in the rejection sampling results from LLaMA2-13B-SFT.
open-resourced models, our results on LLaMA present better performance than two recent state-of-the-art reasoning augmentation methods. Our RFT method is simpler compared to CoRE, since RFT does not require training verifier models and decoding with Monte Carlo Tree Search (MCTS). Compared to other open-sourced aligned language models, we can find that 7B models struggle at a level of 35 scores which are very similar to SFT performances of LLaMA-7B. We guess they use GSM8K during their pre-training phase following (OpenAI, 2023) or human alignment fine-tuning phase following (Qingyi et al., 2023). Using our augmented dataset $\mathcal{D}_{\mathrm{U} 13 \mathrm{~B}}^{\prime}$ to replace the original GSM8K can significantly boost their 7B models' performances.} 33 \mathrm{~B}}^{\prime</p>
<h1>4 DISCUSSION</h1>
<h3>4.1 DIFFERENT DISTRIBUTION OF REASONING PATHS</h3>
<p>In the aforementioned analysis of RFT training data, we observe that rejection sampling can augment the training question with diverse reasoning calculation paths. In this section, we investigate whether RFT models can learn to generate different reasoning paths to reach the correct answers. We finetune LLaMA and LLaMA2 of 7B and 13B on $\mathcal{D}<em _mathrm_U="\mathrm{U">{\mathrm{U} 13 \mathrm{~B}}^{\prime}$. During inference, we sample 100 different reasoning paths from each trained model for each test set question with a temperature of 0.7 . For each question, we compute the number of different calculation processes presented in 100 sampled reasoning paths that lead to the correct answer and draw histograms with respect to test set questions. SFT and RFT models on self-sampled datasets (RFT $\mathrm{k}=100$ ) are included for comparison.
As shown in Figure 7, the models trained by RFT on $\mathcal{D}</em>=100$ and SFT on the larger numbers of unique calculation processes. There are more question counts for SFT models where all the sampled reasoning paths only correspond to one single calculation process and SFT models can barely generate more than 8 different calculation} 13 \mathrm{~B}}^{\prime}$ exhibit more question counts than the models trained by RFT $\mathrm{k</p>
<table>
<thead>
<tr>
<th>Base Model</th>
<th>Training</th>
<th>maj1@1</th>
<th>maj1@K*</th>
</tr>
</thead>
<tbody>
<tr>
<td>Proprietary LLMs</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-4 (OpenAI, 2023)</td>
<td>5-shot ICL</td>
<td>92.0</td>
<td>-</td>
</tr>
<tr>
<td>GPT-3-175B (Brown et al., 2020)</td>
<td>SFT</td>
<td>34.0</td>
<td>-</td>
</tr>
<tr>
<td>PaLM2 (Anil et al., 2023)</td>
<td>8-shot ICL</td>
<td>80.7</td>
<td>91.0@K=40</td>
</tr>
<tr>
<td>PaLM-540B (Chowdhery et al., 2022)</td>
<td>8-shot ICL</td>
<td>56.5</td>
<td>74.4@K=40</td>
</tr>
<tr>
<td>Chinchilla-70B (Uesato et al., 2022)</td>
<td>5-shot ICL</td>
<td>43.7</td>
<td>58.6@K=96</td>
</tr>
<tr>
<td>Chinchilla-70B</td>
<td>SFT</td>
<td>58.9</td>
<td>77.7@K=96</td>
</tr>
<tr>
<td>Open-sourced LLMs</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-Neo-2.7B (Black et al., 2021)</td>
<td>FCS + PCS (Ni et al., 2023)</td>
<td>19.5</td>
<td>41.4</td>
</tr>
<tr>
<td>GPT-J-6B (Wang &amp; Komatsuzaki, 2021)</td>
<td>CoRE (Zhu et al., 2023)</td>
<td>34.9</td>
<td>63.2@K=40</td>
</tr>
<tr>
<td>ChatGLM2-6B (Zeng et al., 2022)</td>
<td>8-shot ICL</td>
<td>32.4</td>
<td>-</td>
</tr>
<tr>
<td>ChatGLM2-6B</td>
<td>Human Alignment</td>
<td>28.1</td>
<td>-</td>
</tr>
<tr>
<td>ChatGLM2-12B</td>
<td>8-shot ICL</td>
<td>40.9</td>
<td>-</td>
</tr>
<tr>
<td>ChatGLM2-12B</td>
<td>Human Alignment</td>
<td>38.1</td>
<td>-</td>
</tr>
<tr>
<td>InternLM-7B (Team, 2023)</td>
<td>4-shot ICL</td>
<td>31.2</td>
<td>-</td>
</tr>
<tr>
<td>InternLM-7B</td>
<td>Human Alignment</td>
<td>34.5</td>
<td>-</td>
</tr>
<tr>
<td>LLaMA-7B</td>
<td>SFT</td>
<td>35.9</td>
<td>48.7</td>
</tr>
<tr>
<td>Our RFT on open-sourced LLMs</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LLaMA-7B</td>
<td>RFT-U13B</td>
<td>49.3</td>
<td>61.8</td>
</tr>
<tr>
<td>LLaMA2-7B</td>
<td>RFT-U13B</td>
<td>50.3</td>
<td>65.6</td>
</tr>
<tr>
<td>LLaMA-13B</td>
<td>RFT-U13B</td>
<td>52.1</td>
<td>66.2</td>
</tr>
<tr>
<td>LLaMA2-13B</td>
<td>RFT-U13B</td>
<td>55.4</td>
<td>69.1</td>
</tr>
</tbody>
</table>
<p>Table 3: Compare GSM8K results with other baselines. RFT-U13B means models fine-tuned on $\mathcal{D}_{\mathrm{U} 13\mathrm{B}}^{\mathrm{r}}$. FCS and PCS represent fully-correct solutions and partially-correct solutions respectively. *K=100 if not specified.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: The histograms of question numbers solved with different numbers of unique reasoning calculation paths. We show the difference in question counts between SFT and RFT U13B in two cases where the numbers of unique reasoning calculation paths are 1 or more than 10.</p>
<p>processes for a question. This analysis demonstrates that diverse reasoning calculation paths in training data can equip the LLMs with finding diverse reasoning logic for solving math problems.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model size</th>
<th style="text-align: right;">7B</th>
<th style="text-align: right;">7B-2</th>
<th style="text-align: right;">13B</th>
<th style="text-align: right;">13B-2</th>
<th style="text-align: right;">33B</th>
<th style="text-align: right;">65B</th>
<th style="text-align: right;">70B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Pre-train FLOPs</td>
<td style="text-align: right;">$4.2 \times 10^{22}$</td>
<td style="text-align: right;">$8.4 \times 10^{22}$</td>
<td style="text-align: right;">$7.8 \times 10^{22}$</td>
<td style="text-align: right;">$1.6 \times 10^{23}$</td>
<td style="text-align: right;">$2.7 \times 10^{23}$</td>
<td style="text-align: right;">$5.5 \times 10^{23}$</td>
<td style="text-align: right;">$8.4 \times 10^{23}$</td>
</tr>
<tr>
<td style="text-align: left;">SFT FLOPs</td>
<td style="text-align: right;">$1.7 \times 10^{17}$</td>
<td style="text-align: right;">$3.3 \times 10^{17}$</td>
<td style="text-align: right;">$7.7 \times 10^{17}$</td>
<td style="text-align: right;">$1.3 \times 10^{18}$</td>
<td style="text-align: right;">$1.7 \times 10^{18}$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">RFT Inference FLOPs</td>
<td style="text-align: right;">$1.4 \times 10^{18}$</td>
<td style="text-align: right;">$2.6 \times 10^{18}$</td>
<td style="text-align: right;">$6.9 \times 10^{18}$</td>
<td style="text-align: right;">$1.4 \times 10^{19}$</td>
<td style="text-align: right;">$1.8 \times 10^{19}$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">RFT-U33B FLOPs</td>
<td style="text-align: right;">$3.0 \times 10^{18}$</td>
<td style="text-align: right;">$5.7 \times 10^{18}$</td>
<td style="text-align: right;">$1.3 \times 10^{19}$</td>
<td style="text-align: right;">$2.2 \times 10^{19}$</td>
<td style="text-align: right;">$3.0 \times 10^{19}$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Pre-train GPU hrs</td>
<td style="text-align: right;">82 k</td>
<td style="text-align: right;">184 k</td>
<td style="text-align: right;">135 k</td>
<td style="text-align: right;">368 k</td>
<td style="text-align: right;">530 k</td>
<td style="text-align: right;">1022 k</td>
<td style="text-align: right;">1720 k</td>
</tr>
<tr>
<td style="text-align: left;">SFT GPU hrs</td>
<td style="text-align: right;">0.6</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">40</td>
<td style="text-align: right;">74</td>
<td style="text-align: right;">80</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">RFT Inference GPU hrs</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">0.1 k</td>
<td style="text-align: right;">0.1 k</td>
<td style="text-align: right;">4.3 k</td>
<td style="text-align: right;">4.5 k</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">RFT-U33B GPU hrs</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">62</td>
<td style="text-align: right;">0.6 k</td>
<td style="text-align: right;">1 k</td>
<td style="text-align: right;">1.2 k</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">ICL Accuracy</td>
<td style="text-align: right;">11.0</td>
<td style="text-align: right;">14.6</td>
<td style="text-align: right;">17.8</td>
<td style="text-align: right;">28.7</td>
<td style="text-align: right;">35.6</td>
<td style="text-align: right;">50.9</td>
<td style="text-align: right;">56.8</td>
</tr>
<tr>
<td style="text-align: left;">SFT Accuracy</td>
<td style="text-align: right;">35.9</td>
<td style="text-align: right;">41.6</td>
<td style="text-align: right;">43.0</td>
<td style="text-align: right;">50.0</td>
<td style="text-align: right;">54.6</td>
<td style="text-align: right;">59.3</td>
<td style="text-align: right;">63.2</td>
</tr>
<tr>
<td style="text-align: left;">RFT-U33B Accuracy</td>
<td style="text-align: right;">49.1</td>
<td style="text-align: right;">51.2</td>
<td style="text-align: right;">51.4</td>
<td style="text-align: right;">55.3</td>
<td style="text-align: right;">57.9</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
</tr>
</tbody>
</table>
<p>Table 4: The statistics of FLOPs and GPU hours required for pre-training, SFT, RFT inference, and RFT. We take the pre-training GPU hours from <em>Touvron et al. (2023a, b)</em>. The GPU hours for RFT inference are calculated for 7,473 train set questions and 100 samples per question. To make the best of GPUs and properly fit models into the GPU memory, we tune the inference batch size. For 33B, 65B, and 70B models, we use DeepSpeed ZeRO3 <em>(Rasley et al., 2020)</em> for distributed training. All the GPU hours are based on NVIDIA A100 80GB GPU. Note we use non-embedding parameters to compute FLOPs in our experiments.</p>
<h1>4.2 Towards ExcelSior Mathematical Reasoning</h1>
<p>From our findings, there are two main factors that can improve mathematical reasoning abilities given a preset amount of human-annotated samples, including: (1) Pre-training the LLMs to lower losses; (2) Augmenting fine-tuning with rejection sampling. Through extensive experiments, we empirically verify the scaling relationships between the mathematical reasoning performance of LLM with both factors respectively. Out of the consideration of sustainable NLP, in this section, we investigate the possible computational resources required to extrapolate the mathematical performance of LLMs by both factors and discuss how to improve the performance more efficiently.</p>
<p>We estimate the pre-training, SFT, RFT inference, and RFT FLOPs following <em>Kaplan et al. (2020)</em> and GPU times in Table 4 which is detailed in Appendix E. We can find that the cost times of SFT $\left(\sim 1 \times 10^{-5}\right)$ and RFT $\left(\sim 1 \times 10^{-4}\right)$ are negligible compared to pre-training. One can always use SFT and RFT to improve models' performance. However, it could be hard to use RFT to further boost performance. Since we need much more sampling counts (at an exponential level) to increase distinct reasoning paths and there exists an upper bound of distinct reasoning path amount for a given math reasoning question.</p>
<p>We assume that performance follows RFT $&gt;$ SFT $&gt;$ ICL, from the findings in this paper we know the improvement speed follows RFT $&lt;$ SFT $&lt;$ ICL. And if we have an omnipotent language model which has a pre-training loss that is the same as the corpus randomness, it could have RFT $=$ SFT $=\mathrm{ICL}=100$. Thus when you pre-train a better language model (i.e. smaller pre-training loss), your model's performance still follows RFT $&gt;$ SFT $&gt;$ ICL but their performance gaps are diminishing. Since you can obtain an RFT model without too much effort (compared to pre-training), then the most important thing we should do is to decrease the model's pre-training loss. From LLaMA-7B to LLaMA2-7B, it needs to add $4.2 \times 10^{22}$ FLOPs to obtain a 2.1 improvement in the RFT-U33B setting with a 0.05 pre-training loss decrease. From LLaMA-7B to LLaMA-13B, it adds $3.6 \times 10^{22}$ FLOPs to obtain a 2.3 improvement in the RFT-U33B setting with a 0.07 pre-training loss decrease. While minimizing pre-training loss is expensive compared to SFT and RFT, we believe other abilities may follow a similar pattern and better pre-training can benefit all other tasks.</p>
<h2>5 CONCLUSIONS</h2>
<p>In this paper, we are investigating the scaling relationship in supervising math reasoning abilities with large language models. We find the relationship between math performance and pre-training</p>
<p>losses, supervised data amount, and distinct reasoning paths. We find that better language models benefit less with SFT and RFT, and the most important thing is to pre-train a better language model towards excellent math reasoning abilities.</p>
<h1>6 ACKNOWLEDGEMENT</h1>
<p>We would like to express our sincere appreciation to Tianhang Zhu, Runji Lin, Kai Dang, Keming Lu, Wei Wang, and Junyang Lin for their valuable insights and contributions to this paper.</p>
<h2>7 LIMITATIONS</h2>
<p>In this paper, we miss the following parts which are very important for building math reasoning abilities for LLMs and should be discussed in the revised version of this paper or future works.</p>
<ul>
<li>RFT for 65B and 70B LLaMA models.</li>
<li>Pre-training on the math-related corpus. This is obviously useful shown in Lewkowycz et al. (2022). While the pre-training loss obtained here cannot align with general domain pre-trained models' losses.</li>
<li>We do not regress any scaling laws in this paper since many numbers are estimated and pre-training losses, ICL prompts and SFT settings of various models may not be aligned.</li>
</ul>
<h2>REFERENCES</h2>
<p>Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/ paper/2017/file/453fadbd8a1a3af50a9df4df899537b5-Paper.pdf.</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli TranJohnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional ai: Harmlessness from ai feedback, 2022.</p>
<p>Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https://doi. org/10.5281/zenodo. 5297715 .</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020. URL https://api.semanticscholar.org/ CorpusID:218971783.</p>
<p>Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural scaling laws. arXiv preprint arXiv:2210.14891, 2022.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment, 2023.</p>
<p>Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance, 2023a.</p>
<p>Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. Specializing smaller language models towards multi-step reasoning. arXiv preprint arXiv:2301.12726, 2023b.</p>
<p>Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization, 2022.
Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701, 2020.</p>
<p>Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for transfer, 2021.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022.</p>
<p>Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve, 2022.</p>
<p>Zhanming Jie, Jierui Li, and Wei Lu. Learning to reason deductively: Math word problem solving as complex relation extraction. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5944-5955, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.410. URL https://aclanthology.org/2022.acl-long. 410.</p>
<p>Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. CoRR, abs/2001.08361, 2020. URL https://arxiv.org/abs/2001.08361.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=e2TBb5y0yFf.</p>
<p>Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1152-1157, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1136. URL https://aclanthology.org/N16-1136.</p>
<p>Yihuai Lan, Lei Wang, Qiyuan Zhang, Yunshi Lan, Bing Tian Dai, Yan Wang, Dongxiang Zhang, and Ee-Peng Lim. Mwptoolkit: An open-source framework for deep learning-based math word problem solvers, 2021.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models, 2022.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 53155333, Toronto, Canada, July 2023. Association for Computational Linguistics. URL https: //aclanthology.org/2023.acl-long. 291.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050, 2023.</p>
<p>Niklas Muennighoff, Alexander M. Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models, 2023.</p>
<p>Ansong Ni, Jeevana Priya Inala, Chenglong Wang, Alex Polozov, Christopher Meek, Dragomir Radev, and Jianfeng Gao. Learning math reasoning from self-sampled correct and partiallycorrect solutions. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=4D4TSJE6-K.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models, 2021.</p>
<p>OpenAI. Gpt-4 technical report, 2023.
Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 20802094, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. naacl-main.168. URL https://aclanthology.org/2021.naacl-main.168.</p>
<p>Si Qingyi, Wang Tong, Gu Naibin, Liu Rui, and Lin Zheng. Alpaca-cot: An instruction-tuning platform with unified interface of instruction collection, parameter-efficient methods, and large language models. https://github.com/PhoebusSi/alpaca-CoT, 2023.</p>
<p>Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, KDD '20, pp. 3505-3506, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450379984. doi: 10.1145/3394486.3406703. URL https://doi.org/10. $1145 / 3394486.3406703$.</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models, 2019.</p>
<p>Kumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan. Distilling reasoning capabilities into smaller language models. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 7059-7073, Toronto, Canada, July 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.findings-acl.441.</p>
<p>Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. Preference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492, 2023.</p>
<p>InternLM Team. Internlm: A multilingual language model with progressively enhanced capabilities. https://github.com/InternLM/InternLM, 2023.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023a.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b.</p>
<p>Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process- and outcome-based feedback, 2022.</p>
<p>Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=1PL1NIMdrw.</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. ArXiv, abs/2109.01652, 2021. URL https://api.semanticscholar.org/ CorpusID:237416585.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Trans. Mach. Learn. Res., 2022, 2022a. URL https://api.semanticscholar. org/CorpusID:249674500.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903, 2022b. URL https://api.semanticscholar.org/ CorpusID:246411621.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models, 2023.</p>
<p>Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. How well do large language models perform in arithmetic tasks? arXiv preprint arXiv:2304.02015, 2023a.</p>
<p>Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses to align language models with human feedback without tears, 2023b.</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STar: Bootstrapping reasoning with reasoning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=_3ELRdg2sgI.</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022.</p>
<p>Tianjun Zhang, Fangchen Liu, Justin Wong, Pieter Abbeel, and Joseph E. Gonzalez. The wisdom of hindsight makes language models better instruction followers, 2023.</p>
<p>Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Ruyi Gan, Jiaxing Zhang, and Yujiu Yang. Solving math word problems via cooperative reasoning induced language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 4471-4485, Toronto, Canada, July 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.acl-long. 245.</p>
<h1>A Detailed EXPERIMENT SETTING</h1>
<h2>A. 1 SFT ON GSM8K</h2>
<p>We fine-tune GSM8K with 3 epochs and a batch size of 128 on NVIDIA A100 GPUs. We use 8 GPUs for 7B and 13B models, 16 GPUs for 33B models, and 32 GPUs for 65B and 70B models during fine-tuning. We use a peak learning rate of $2 \mathrm{e}-5$ with a $3 \%$ learning rate warmup. We evaluate the results on the final epoch. We use greedy decode to calculate maj1@1 and decode with temperature 0.7 to calculate maj1@100.</p>
<h2>A. 2 SFT ON DOWNSAMPLED GSM8K</h2>
<p>We random downsample GSM8K dataset for fine-tuning. We find that using 3 epochs for little data will result in very poor results which are listed in Table 5. We search training epoch among $\left{3, \frac{3}{\text { data fraction }}\right}$ and evaluate the latest epoch. We report better test results among these two different epoch settings.</p>
<h2>A. 3 REJECTION SAMPLING FINE-TUNING ON GSM8K</h2>
<p>We use an SFT model $\pi$ to sample on training dataset for $k=100$ times with a temperature of 0.7. We extract the equation list in generated reasoning paths by finding $&lt;&lt;$ equation $&gt;&gt;$ first, removing all white spaces, and joining the equation string list by a special symbol to a string (called</p>
<p>it get_equation in our algorithm) for deduplication. We select the reasoning paths by this algorithm:
Algorithm 1: Reasoning Path Selection
Data: Reasoning paths for question $q, \mathcal{R}<em q="q">{q}$
Result: Selected reasoning paths for question $q, \mathcal{R}</em>$
1 Initialize selected reasoning paths, $\mathcal{R}}^{s<em q="q">{q}^{s}=\operatorname{list}()$
2 Initialize appeared equation set, $\mathcal{E}</em>()$
3 for $r$ in $\mathcal{R}}^{s}=\operatorname{set<em q="q">{q}$ do
4 if get_equation $(r) \notin \mathcal{E}</em>$ then
5
$\mathcal{R}}^{s<em q="q">{q}^{s} \cdot \operatorname{append}(r)$
$6 \quad \mathcal{E}</em>(r)])$
7 end
8 else
9
find $r^{s} \in \mathcal{R}}^{s} \cdot \operatorname{update([get_equation<em i:="i:" r__i="r_{i">{q}^{s}$ s.t. get_equation $\left(r^{s}\right)=$ get_equation $(r)$;
10
if $\sum</em>}^{s} \in \mathcal{E<em i="i">{q}^{s}, r</em>}^{s} \neq r^{s}} \operatorname{Levenstein} \cdot \operatorname{dist}\left(r, r_{i}^{s}\right)&gt;\sum_{i: r_{i}^{s} \in \mathcal{E<em i="i">{q}^{s}, r</em>\right)$ then
$r^{s}=r ;$
12
end
13
end
14 end
We are trying to find the most dissimilar reasoning paths based on Levenstein distances. The idea comes from we want diverse reasoning paths for better generalization.}^{s} \neq r^{s}} \operatorname{Levenstein} \cdot \operatorname{dist}\left(r^{s}, r_{i}^{s</p>
<h1>B Detailed Results of SFT and RFT</h1>
<p>We list detailed results of SFT and RFT in Table 5 and 6.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Data</th>
<th style="text-align: right;">Epoch</th>
<th style="text-align: right;">7B</th>
<th style="text-align: right;">7B-2</th>
<th style="text-align: right;">13B</th>
<th style="text-align: right;">13B-2</th>
<th style="text-align: right;">33B</th>
<th style="text-align: right;">65B</th>
<th style="text-align: right;">70B-2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ICL-8shot</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">11.0</td>
<td style="text-align: right;">14.6</td>
<td style="text-align: right;">17.8</td>
<td style="text-align: right;">28.7</td>
<td style="text-align: right;">35.6</td>
<td style="text-align: right;">50.9</td>
<td style="text-align: right;">56.8</td>
</tr>
<tr>
<td style="text-align: left;">SFT</td>
<td style="text-align: right;">$1 / 32$</td>
<td style="text-align: right;">96</td>
<td style="text-align: right;">9.5</td>
<td style="text-align: right;">10.1</td>
<td style="text-align: right;">8.6</td>
<td style="text-align: right;">17.1</td>
<td style="text-align: right;">18.6</td>
<td style="text-align: right;">25.2</td>
<td style="text-align: right;">27.4</td>
</tr>
<tr>
<td style="text-align: left;">SFT</td>
<td style="text-align: right;">$1 / 16$</td>
<td style="text-align: right;">48</td>
<td style="text-align: right;">14.3</td>
<td style="text-align: right;">15.5</td>
<td style="text-align: right;">14.2</td>
<td style="text-align: right;">23.9</td>
<td style="text-align: right;">25.9</td>
<td style="text-align: right;">28.9</td>
<td style="text-align: right;">33.6</td>
</tr>
<tr>
<td style="text-align: left;">SFT</td>
<td style="text-align: right;">$1 / 8$</td>
<td style="text-align: right;">24</td>
<td style="text-align: right;">17.9</td>
<td style="text-align: right;">20.8</td>
<td style="text-align: right;">18.4</td>
<td style="text-align: right;">28.5</td>
<td style="text-align: right;">31.6</td>
<td style="text-align: right;">35.8</td>
<td style="text-align: right;">38.9</td>
</tr>
<tr>
<td style="text-align: left;">SFT</td>
<td style="text-align: right;">$1 / 4$</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">21.6</td>
<td style="text-align: right;">27.7</td>
<td style="text-align: right;">26.7</td>
<td style="text-align: right;">36.3</td>
<td style="text-align: right;">38.4</td>
<td style="text-align: right;">45.6</td>
<td style="text-align: right;">46.9</td>
</tr>
<tr>
<td style="text-align: left;">SFT</td>
<td style="text-align: right;">$1 / 2$</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">29.0</td>
<td style="text-align: right;">33.1</td>
<td style="text-align: right;">35.2</td>
<td style="text-align: right;">43.7</td>
<td style="text-align: right;">48.6</td>
<td style="text-align: right;">50.5</td>
<td style="text-align: right;">57.5</td>
</tr>
<tr>
<td style="text-align: left;">SFT</td>
<td style="text-align: right;">$1 / 32$</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">7.8</td>
<td style="text-align: right;">14.2</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">5.9</td>
<td style="text-align: right;">25.3</td>
<td style="text-align: right;">28.9</td>
<td style="text-align: right;">15.8</td>
</tr>
<tr>
<td style="text-align: left;">SFT</td>
<td style="text-align: right;">$1 / 16$</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">12.7</td>
<td style="text-align: right;">16.2</td>
<td style="text-align: right;">7.4</td>
<td style="text-align: right;">27.7</td>
<td style="text-align: right;">29.2</td>
<td style="text-align: right;">39.5</td>
<td style="text-align: right;">52.8</td>
</tr>
<tr>
<td style="text-align: left;">SFT</td>
<td style="text-align: right;">$1 / 8$</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">16.5</td>
<td style="text-align: right;">21.8</td>
<td style="text-align: right;">19.5</td>
<td style="text-align: right;">33.4</td>
<td style="text-align: right;">39.3</td>
<td style="text-align: right;">46.0</td>
<td style="text-align: right;">57.8</td>
</tr>
<tr>
<td style="text-align: left;">SFT</td>
<td style="text-align: right;">$1 / 4$</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">22.7</td>
<td style="text-align: right;">28.1</td>
<td style="text-align: right;">27.4</td>
<td style="text-align: right;">37.5</td>
<td style="text-align: right;">44.6</td>
<td style="text-align: right;">50.4</td>
<td style="text-align: right;">57.8</td>
</tr>
<tr>
<td style="text-align: left;">SFT</td>
<td style="text-align: right;">$1 / 2$</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">30.9</td>
<td style="text-align: right;">34.6</td>
<td style="text-align: right;">36.1</td>
<td style="text-align: right;">45.3</td>
<td style="text-align: right;">50.8</td>
<td style="text-align: right;">55.6</td>
<td style="text-align: right;">61.0</td>
</tr>
<tr>
<td style="text-align: left;">SFT</td>
<td style="text-align: right;">7.4 K</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">35.9</td>
<td style="text-align: right;">41.6</td>
<td style="text-align: right;">43.0</td>
<td style="text-align: right;">50.0</td>
<td style="text-align: right;">54.6</td>
<td style="text-align: right;">59.3</td>
<td style="text-align: right;">63.2</td>
</tr>
<tr>
<td style="text-align: left;">RFT no dedup</td>
<td style="text-align: right;">$1 / 32$</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">37.5</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">RFT no dedup</td>
<td style="text-align: right;">$1 / 16$</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">38.3</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">RFT no dedup</td>
<td style="text-align: right;">$1 / 8$</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">41.1</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">RFT no dedup</td>
<td style="text-align: right;">$1 / 4$</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">41.2</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">RFT no dedup</td>
<td style="text-align: right;">$1 / 2$</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">43.9</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">RFT no dedup</td>
<td style="text-align: right;">400 K</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">43.6</td>
<td style="text-align: right;">46.7</td>
<td style="text-align: right;">46.9</td>
<td style="text-align: right;">53.7</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">RFT k=1</td>
<td style="text-align: right;">$\sim 12 \mathrm{~K}$</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">37.6</td>
<td style="text-align: right;">43.4</td>
<td style="text-align: right;">42.7</td>
<td style="text-align: right;">52.1</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">RFT k=3</td>
<td style="text-align: right;">$\sim 15 \mathrm{~K}$</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">39.0</td>
<td style="text-align: right;">45.3</td>
<td style="text-align: right;">45.2</td>
<td style="text-align: right;">51.9</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">RFT k=6</td>
<td style="text-align: right;">$\sim 18 \mathrm{~K}$</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">39.5</td>
<td style="text-align: right;">45.6</td>
<td style="text-align: right;">46.8</td>
<td style="text-align: right;">52.2</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">RFT k=12</td>
<td style="text-align: right;">$\sim 22 \mathrm{~K}$</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">41.6</td>
<td style="text-align: right;">45.3</td>
<td style="text-align: right;">48.0</td>
<td style="text-align: right;">53.1</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">RFT k=25</td>
<td style="text-align: right;">$\sim 28 \mathrm{~K}$</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">40.9</td>
<td style="text-align: right;">46.5</td>
<td style="text-align: right;">46.0</td>
<td style="text-align: right;">52.6</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">RFT k=50</td>
<td style="text-align: right;">$\sim 35 \mathrm{~K}$</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">40.7</td>
<td style="text-align: right;">47.0</td>
<td style="text-align: right;">49.4</td>
<td style="text-align: right;">54.5</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">RFT k=100</td>
<td style="text-align: right;">$\sim 47 \mathrm{~K}$</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">41.7</td>
<td style="text-align: right;">47.5</td>
<td style="text-align: right;">49.1</td>
<td style="text-align: right;">54.8</td>
<td style="text-align: right;">54.5</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">RFT-U13B</td>
<td style="text-align: right;">104 K</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">49.3</td>
<td style="text-align: right;">50.3</td>
<td style="text-align: right;">52.1</td>
<td style="text-align: right;">55.4</td>
<td style="text-align: right;">56.5</td>
<td style="text-align: right;">59.0</td>
<td style="text-align: right;">62.3</td>
</tr>
<tr>
<td style="text-align: left;">RFT-U33B</td>
<td style="text-align: right;">110 K</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">49.1</td>
<td style="text-align: right;">51.2</td>
<td style="text-align: right;">51.4</td>
<td style="text-align: right;">55.3</td>
<td style="text-align: right;">57.9</td>
<td style="text-align: right;">59.7</td>
<td style="text-align: right;">64.8</td>
</tr>
</tbody>
</table>
<p>Table 5: Detailed numerical results in this paper, some experiments are still under running. We report maj1@1 (accuracy) in this table.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Setting</th>
<th style="text-align: left;">7B</th>
<th style="text-align: left;">7B-2</th>
<th style="text-align: left;">13B</th>
<th style="text-align: left;">13B-2</th>
<th style="text-align: left;">33B</th>
<th style="text-align: left;">65B</th>
<th style="text-align: left;">70B-2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ICL-8shot</td>
<td style="text-align: left;">$11.0 / 18.1$</td>
<td style="text-align: left;">$14.6 /-$</td>
<td style="text-align: left;">$17.8 / 29.3$</td>
<td style="text-align: left;">$28.7 /-$</td>
<td style="text-align: left;">$35.6 / 53.1$</td>
<td style="text-align: left;">$50.9 / 69.7$</td>
<td style="text-align: left;">$56.8 /-$</td>
</tr>
<tr>
<td style="text-align: left;">SFT</td>
<td style="text-align: left;">$35.9 / 48.7$</td>
<td style="text-align: left;">$41.6 / 55.4$</td>
<td style="text-align: left;">$43.0 / 55.2$</td>
<td style="text-align: left;">$50.0 / 61.7$</td>
<td style="text-align: left;">$54.6 / 72.6$</td>
<td style="text-align: left;">$59.3 / 69.7$</td>
<td style="text-align: left;">$63.2 / 73.5$</td>
</tr>
<tr>
<td style="text-align: left;">RFT $\mathrm{k}=100$</td>
<td style="text-align: left;">$41.7 / 52.7$</td>
<td style="text-align: left;">$47.5 / 58.7$</td>
<td style="text-align: left;">$49.1 / 59.9$</td>
<td style="text-align: left;">$54.8 / 65.4$</td>
<td style="text-align: left;">$54.5 /-$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">RFT-U13B</td>
<td style="text-align: left;">$49.3 / 61.8$</td>
<td style="text-align: left;">$50.3 / 65.6$</td>
<td style="text-align: left;">$52.1 / 66.2$</td>
<td style="text-align: left;">$55.4 / 69.1$</td>
<td style="text-align: left;">$56.5 /-$</td>
<td style="text-align: left;">$59.0 /-$</td>
<td style="text-align: left;">$62.3 /-$</td>
</tr>
<tr>
<td style="text-align: left;">RFT-U33B</td>
<td style="text-align: left;">$49.1 / 61.6$</td>
<td style="text-align: left;">$51.2 / 64.1$</td>
<td style="text-align: left;">$51.4 / 66.3$</td>
<td style="text-align: left;">$55.3 / 69.1$</td>
<td style="text-align: left;">$57.9 / 71.3$</td>
<td style="text-align: left;">$59.7 /-$</td>
<td style="text-align: left;">$64.8 /-$</td>
</tr>
</tbody>
</table>
<p>Table 6: Detailed numerical results in this paper, some experiments are still under running. We report maj1@100 in this table.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Query A</th>
<th style="text-align: center;">Weng earns $\$ 12$ an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Path 1</td>
<td style="text-align: center;">Weng earned $12 / 60=\$(\langle 12 / 60=0.2\rangle) 0.2$ an hour for the 50 minutes that she worked yesterday. Therefore, she earned $\$ 0.2 \times 50=\$(\langle 0.2 * 50=10\rangle) 10$ for the 50 minutes that she worked yesterday. #### 10</td>
</tr>
<tr>
<td style="text-align: center;">Path 2</td>
<td style="text-align: center;">Weng earns $12 / 60=\$(\langle 12 / 60=0.2\rangle) 0.2$ per minute. So, Weng earned $50 * 0.2=$ $\$(\langle 50 * 0.2=10\rangle) 10$ for the 50 minutes of babysitting. #### 10</td>
</tr>
<tr>
<td style="text-align: center;">Path 3</td>
<td style="text-align: center;">Since there are 60 minutes in an hour, then Weng did $50 / 60=\langle\langle 50 / 60=0.8333\rangle\rangle 0.8333$ of an hour of babysitting. So, she earned $\$ 12 \times 0.8333=\$ \langle(12 * 0.8333=10\rangle) 10$. #### 10</td>
</tr>
<tr>
<td style="text-align: center;">Path 4</td>
<td style="text-align: center;">She earns $12 * 50 / 60=\$(\langle 12 * 50 / 60=10\rangle) 10$. #### 10</td>
</tr>
<tr>
<td style="text-align: center;">Path 5</td>
<td style="text-align: center;">Weng earns $12 <em>(50 / 60)=\$(\langle 12 </em>(50 / 60)=10\rangle) 10$. #### 10</td>
</tr>
<tr>
<td style="text-align: center;">Query B</td>
<td style="text-align: center;">A local college is offering German lessons and currently has 8 students enrolled. Through advertising, 8 more became interested but a fourth of these dropped out within a day. 2 more got frustrated and left. The class then rallied to show how simple the course actually is and increased enrollment by 5 times the amount of students already enrolled in the class, but 2 had to drop it because of scheduling conflicts. After one last rally, 6 more people enrolled. As the days passed, half of the class eventually dropped, and half of the remaining students graduated. How many are still enrolled?</td>
</tr>
<tr>
<td style="text-align: center;">Path 1</td>
<td style="text-align: center;">$8+8=\langle\langle 8+8=16\rangle\rangle 16$ students are enrolled in the German class. $8^{<em>} .25=\langle\langle 8^{</em>} .25=2\rangle\rangle 2$ dropped out from the new recruits. 2 more left, so the class had $16-2-2=\langle\langle 16-2-2=12\rangle\rangle 12$ students left. There were $12 * 5+12=\langle(12 * 5+12=72)\rangle 72$ total students in the class after the rally. Two people had to drop the class, so there were $72-2=\langle\langle 72-2=70\rangle\rangle 70$ left. $70+6=\langle\langle 70+6=76\rangle\rangle 76$ were enrolled after the final rally. $76 / 2=\langle\langle 76 / 2=38\rangle\rangle 38$ are left after half the class dropped. $38 / 2=\langle\langle 38 / 2=19\rangle\rangle 19$ are left after half the class graduated. #### 19</td>
</tr>
<tr>
<td style="text-align: center;">Path 2</td>
<td style="text-align: center;">$8+8=\langle\langle 8+8=16\rangle\rangle 16$ students are enrolled after the first rally. $16 / 4=\langle\langle 16 / 4=4\rangle\rangle 4$ dropped out. $16-4=\langle\langle 16-4=12\rangle\rangle 12$ remain. $12 * 5+12=\langle(12 * 5+12=72\rangle\rangle 72$ after the second rally. $72-2=\langle\langle 72-2=70\rangle\rangle 70$ after 2 more dropped. $70+6=\langle\langle 70+6=76\rangle\rangle 76$ after 6 more enrolled. $76 / 2=\langle\langle 76 / 2=38\rangle\rangle 38$ remain after half the class dropped. $38 / 2=\langle\langle 38 / 2=19\rangle\rangle 19$ remain after half the class graduated. #### 19</td>
</tr>
<tr>
<td style="text-align: center;">Path 3</td>
<td style="text-align: center;">Advertising brought in $8+8=\langle\langle 8+8=16\rangle\rangle 16$ students. Of these, $16 / 4=\langle\langle 16 / 4=4\rangle\rangle 4$ dropped out. $16-4-2=\langle\langle 16-4-2=10\rangle\rangle 10$ remained. Then $10 * 5=\langle\langle 10 * 5=50\rangle\rangle 50$ more enrolled. This brought the class to $50+10=\langle\langle 50+10=60\rangle\rangle 60$ students. Then $60+6=\langle\langle 60+6=66\rangle\rangle 66$ enrolled. So there were $66+10=\langle\langle 66+10=76\rangle\rangle 76$ students. Then $76 / 2=\langle\langle 76 / 2=38\rangle\rangle 38$ dropped. So $76-38=\langle\langle 76-38=38\rangle\rangle 38$ remained. Then $38 / 2=\langle\langle 38 / 2=19\rangle\rangle 19$ graduated. So $38-19=\langle\langle 38-19=19\rangle\rangle 19$ were left. #### 19</td>
</tr>
</tbody>
</table>
<p>Table 7: Cases of generated reasoning paths with different reasoning complexity from rejection sampling for RFT. The calculations are highlighted in red.</p>
<h1>C CASE Study OF RFT</h1>
<p>In this section, we present the cases of the training samples from rejection sampling. The case studies would shed light on how RFT potentially improves the mathematical reasoning performance of LLMs. The cases are shown in Table 7. As aforementioned, RFT considers the reasoning paths with different calculation processes regarding equation forms or orders, leading to the correct answers. In the cases from Table 7, all the reasoning paths from RFT result in the correct answer of 10 , while the calculation processes of reasoning are diverse. Path 1 and 2, as well as Path 4 and 5, are different in the equation forms as highlighted in red. Path 1 and 2 present a two-step calculation reasoning process while Path 4 and 5 alter to a one-step calculation reasoning process. The case demonstrates</p>
<p>that rejection sampling can potentially provide more supervision signals that improve mathematical reasoning performance. The filtered reasoning paths sampled from LLMs themselves are of similar quality to the reasoning demonstrations from human annotations.</p>
<h1>D Preliminary EXPERIMENTS</h1>
<h2>D. 1 Self Query Augmentation</h2>
<p>Through our preliminary experiments and case studies, the errors made by the fine-tuned LLMs are partly attributed to the incorrect reasoning chains where LLMs mistakenly understand the context information or fail to consider all the information in the queries. Although such incorrect reasoning chains lead to wrong answers to the original queries, the reasoning chains themselves represent reasonable logic. For example, for the query Josh decides to try flipping a house. He buys a house for $\$ 80,000$ and then puts in $\$ 50,000$ in repairs. This increased the value of the house by $150 \%$. How much profit did he make?, a fine-tuned LLaMA model predicts The value of the house increased by $80,000 * .15=\$ 12,000$. So the house was worth $80,000+12,000=\$ 92,000$. So he made a profit of $92,000-80,000-50,000=\$ 42,000$ where the model erroneously interprets $150 \%$ as $15 \%$, but the reasoning chain is reasonable if we ignore the error.</p>
<p>Therefore, such wrong predictions made by the LLMs may be correct under other queries (if we change $150 \%$ to $15 \%$ in the above example). We conduct experiments to generate queries for the predicted reasoning chains. This is a similar idea to the hindsight experience replay (Andrychowicz et al., 2017) in reinforcement learning where the method is designed to deal with the sparse reward problems by changing the original objectives for the failed samples to form samples with positive rewards. Such an idea was recently adopted by HIR (Zhang et al., 2023) to better align LLMs with instructions.</p>
<p>Concretely, we reformat GSM8K reversely by predicting the query given the corresponding groundtrue reasoning result and then we fine-tune a LLaMA model on the reversed task. We use this model to generate queries on the predicted reasoning chains by a normally fine-tuned LLaMA model on the training set of GSM8K, formalizing a training sample for augmentation. We experiment on the LLaMA 7B model and fine-tune models on the data mixing original and generated samples or solely on generated samples.</p>
<p>The results are shown in the left subfigure in Figure 8. We can see that fine-tuning with self query augmentation data leads to the worst results, and the performance of mixing the original data with self query augmented data still falls short of that of the original data. The fine-tuned performance for mathematical reasoning does not benefit from the naive idea of self query augmentation. Through several case studies of generated data, we find that there are two major defects in the generated data. The first one is some reasoning chains themselves are not logically reasonable, for example, there may be some calculation errors in the reasoning chains. The second one is that the generated query may not be suitable for a reasoning chain. The query generation model may still erroneously interpret the information in the reasoning chains. Both defects attribute to a mediocre augmented data quality, hence can be possible reasons for the failure of this data augmentation procedure.</p>
<h2>D. 2 Self ReVising Augmentation</h2>
<p>We also explore improving the mathematical reasoning abilities of LLMs through revising augmentation. To equip LLaMA with revising abilities, we generate a revising dataset by first sampling $K$ reasoning paths from a fine-tuned LLaMA model, then concatenating the query with one of the sampled reasoning paths using a template, and finally pairing with the ground-true reasoning path to form a training sample. We use a sampling temperature of 0.7 for generating reasoning paths. During inference, we use the fine-tuned revising model to revise the prediction from the normally fine-tuned model.</p>
<p>The results are shown in the middle subfigure of Figure 8. We can see that with $K=1$ the revising model improves the final accuracy marginally comparing $36.09 \%$ to $35.90 \%$. Surprisingly, as we increase $K$, the performances degrade. The possible defect of the revising model is that generated samples on the training set for revising training suffer from a distribution discrepancy with generated samples on the test set for revising inference. The sampled reasoning paths on the training set may</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Results for different methods of self data augmentation. GSM. and H. represent GSM8K and Hindsight respectively. The red dotted lines in the middle and right figures represent the results of vanilla fine-tuning on GSM8K.
have a larger lexical similarity to the ground true reasoning paths compared to those on the test set. Therefore we try two different procedures to alleviate such an issue.</p>
<ol>
<li>We use the sampled reasoning path with the largest Levenstein distance out of $K$ sampled paths with respect to the ground true path to form a training sample.</li>
<li>We split the train set to $N$ folds, and fine-tune a model on each $N-1$ folds and sampling reasoning path on the left fold.
The results are shown in the middle and right subfigures in Figure 8, we can see that when leveraging Levenstein distance for reasoning path selection, the fine-tuned revising model enjoys a performance boost, harvesting uniformly better performance than the fine-tuning baseline across different $K$ 's. The results demonstrate that for the revising performance, the lexical diversity of reasoning paths matters when constructing training samples. However, the revising performance does not benefit from the $N$-fold procedure.</li>
</ol>
<h1>E Estimating FLOPs of SFT and RFT</h1>
<p>We mainly follow the notations of (Kaplan et al., 2020) here.
Training FLOPs For each input sample of length $n_{c t x}$ in GSM8K dataset, we can split it into two parts:</p>
<p>$$
n_{c t x}=n_{Q}+n_{R}
$$</p>
<p>where $n_{Q}, n_{R}$ denotes the length of question and generated reasoning path and answers respectively.</p>
<p>$$
C_{\text {train }} \approx 6 N n_{c t x} N_{s}
$$</p>
<p>where $N_{s}$ denotes the numbers of samples.</p>
<p>Inference FLOPs We roughly computed the FLOPs of each token during the forward pass:</p>
<p>$$
C_{\text {forward }}\left(n_{\text {ctx }}\right)=2 N+2 n_{\text {layer }} n_{\text {ctx }} d_{\text {model }}
$$</p>
<p>To ensure the results were more accurate and reliable, we also took into account the Key-Value (KV) cache during the decoding procedure.</p>
<p>$$
K V_{\text {cache }} \approx 4 n_{\text {layer }} d_{\text {model }}^{2}
$$</p>
<p>Therefore, we obtain the FLOPs per token during the forward pass considering the KV cache.</p>
<p>$$
\begin{aligned}
C_{\text {forward }}^{\prime}\left(n_{c t x}\right) &amp; =2 N+2 n_{\text {layer }} n_{c t x} d_{\text {model }}-K V_{\text {cache }} \
&amp; =24 n_{\text {layer }} d_{\text {model }}^{2}+2 n_{\text {layer }} n_{c t x} d_{\text {model }}-4 n_{\text {layer }} d_{\text {model }}^{2} \
&amp; =20 n_{\text {layer }} d_{\text {model }}^{2}+2 n_{\text {layer }} n_{c t x} d_{\text {model }} \
&amp; \approx 1.66 N+2 n_{\text {layer }} n_{c t x} d_{\text {model }}
\end{aligned}
$$</p>
<p>The total inference FLOPs are computed as follows:</p>
<p>$$
C_{\text {total }}=N_{s} \cdot\left[n_{q} C_{\text {forward }}\left(n_{q}\right)+\sum_{i=n_{q}}^{n_{q}+n_{r}} i \cdot C_{\text {forward }}^{\prime}(i)\right]
$$</p>
<p>where $N_{s}$ denotes the numbers of samples. $n_{q}, n_{r}$ denotes the average length (tokens) of the user query and generated response respectively. In GSM8K dataset, $n_{q} \approx 66$ and $n_{r} \approx 130$.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
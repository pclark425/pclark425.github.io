<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7422 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7422</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7422</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-269149543</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.12973v2.pdf" target="_blank">Wisdom of Instruction-Tuned Language Model Crowds. Exploring Model Label Variation</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) exhibit remarkable text classification capabilities, excelling in zero- and few-shot learning (ZSL and FSL) scenarios. However, since they are trained on different datasets, performance varies widely across tasks between those models. Recent studies emphasize the importance of considering human label variation in data annotation. However, how this human label variation also applies to LLMs remains unexplored. Given this likely model specialization, we ask: Do aggregate LLM labels improve over individual models (as for human annotators)? We evaluate four recent instruction-tuned LLMs as “annotators” on five subjective tasks across four languages. We use ZSL and FSL setups and label aggregation from human annotation. Aggregations are indeed substantially better than any individual model, benefiting from specialization in diverse tasks or languages. Surprisingly, FSL does not surpass ZSL, as it depends on the quality of the selected examples. However, there seems to be no good information-theoretical strategy to select those. We find that no LLM method rivals even simple supervised models. We also discuss the tradeoffs in accuracy, cost, and moral/ethical considerations between LLM and human annotation.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7422.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7422.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Answer token in prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explicit answer field token ("Answer") in prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors add the literal token "Answer" (or <Answer>) after instruction text to mark the output field; they report this improves the LLMs' understanding and output format (reduces malformed outputs). This was used across all classification tasks in ZSL/FSL prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5, Flan-UL2, T0 / mT0, Tk-Instruct (instruction-tuned T5 family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder T5-family instruction-tuned models fine-tuned on multi-task instruction sets (FLAN, UL2, xP3/Super-NaturalInstructions).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Trustpilot tasks (SA, TD, AC-Gender, AC-Age) and HatEval HS</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Text classification tasks (sentiment 3-way, topic 5-way, demographic binary attributes, hate-speech binary).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language instruction with an explicit labelled output field (explicit 'Answer' prompt token)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot/few-shot natural-language instruction followed by the explicit token 'Answer' (or <Answer>) to indicate output region; used uniformly across tasks to improve output formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Macro-F1 and qualitative reduction in out-of-label responses</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Not quantified numerically in paper; authors state adding 'Answer' improves model understanding and output format (fewer malformed/OOL outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>ZSL and FSL prompting; default model parameters; used for all four LLMs and all tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wisdom of Instruction-Tuned Language Model Crowds. Exploring Model Label Variation', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7422.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7422.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tk-Instruct template fields</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tk-Instruct required prompt template with 'definition', 'input', 'output' fields</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Tk-Instruct requires a structured prompt template containing explicit 'definition', 'input', and 'output' fields; the authors used this format for Tk-Instruct and report it as part of the prompt engineering required by that model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Tk-Instruct (T5-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A T5-based instruction-following model fine-tuned on the SUPER-NATURALINSTRUCTIONS dataset (many declarative instructions).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HatEval HS and Trustpilot classification tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary/ternary/5-way text classification tasks across languages.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Structured prompt template with labeled fields ('definition', 'input', 'output')</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / template requirements</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Authors used a prompt with explicit <Definition> (instruction), <Input> (instance string), and <Response>/<Output> tokens for Tk-Instruct; example shown for HS: <Definition> Is this tweet expressing 'hate speech' or 'non-hate speech?' <Input> "I hate you" <Response>: {LM answer}.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Macro-F1; out-of-label rate reported elsewhere</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Not isolated numerically for template vs other prompt styles; included as required format for Tk-Instruct experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Applied when evaluating Tk-Instruct; ZSL and FSL contexts; default decoding parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wisdom of Instruction-Tuned Language Model Crowds. Exploring Model Label Variation', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7422.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7422.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Label-list (multiple-choice) prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Providing explicit candidate labels in the prompt (multiple-choice style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompts present the candidate classes inline (e.g., 'Is this review about "car lights", "fashion accessories", "pets", ...?') to constrain LLM outputs to the set of target labels; used across Topic Detection and other tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5, Flan-UL2, T0 / mT0, Tk-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned T5-family encoder-decoder models trained on many natural-language tasks and instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Topic Detection (5-way) and other classification tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assign the input text to one of a fixed set of candidate labels (topics, sentiment classes, demographic classes, hate vs non-hate).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language multiple-choice style: instance plus explicit enumeration of candidate labels</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>question type / prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Prompts include the text to classify and an explicit list of choices (quoted strings). For TD the list varied by language (5 labels). Authors note this presentation but also report a higher out-of-label (OOL) rate for TD despite explicit labels.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Out-of-label rate (OOL), Macro-F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Topic Detection (TD) shows much higher OOL (~13% overall in ZSL) compared to binary/ternary tasks (<1%); this occurred despite presenting enumerated candidate labels.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Binary/ternary classification prompts with explicit labels had OOL <1%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+~12 percentage points OOL for TD vs binary/ternary tasks (ZSL)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot prompting with candidate-label enumeration; default model parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wisdom of Instruction-Tuned Language Model Crowds. Exploring Model Label Variation', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7422.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7422.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Out-of-label (OOL) behavior by task and model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Out-of-label (invalid-class) predictions vary with task complexity and model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When the model output does not correspond to a valid class, the authors assign the most common class; OOL rates were very low (<1%) for binary/ternary tasks but substantially higher for descriptive multi-class Topic Detection (~13%). Flan models showed ~1% OOL, T0 and Tk-Instruct ~2% overall.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5, Flan-UL2, T0, Tk-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned encoder-decoder models varying by instruction-fine-tuning corpora (FLAN, UL2, xP3, SUPER-NATURALINSTRUCTIONS).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Topic Detection (TD) and binary/ternary classification tasks (SA, AC, HS)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict one of several descriptive topic labels (5-way) or binary/ternary labels for sentiment/demographics/hate.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language instruction with candidate labels enumerated (TD) or simple binary/ternary label prompts</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>label presentation / prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Descriptive, multi-word class labels (e.g., 'fashion accessories') in a list led to higher OOL in TD; simpler short labels and fewer classes yielded far fewer OOL outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Out-of-label percentage</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>TD OOL ≈ 13% (ZSL); binary/ternary tasks OOL < 1%. Model-level OOL: Flan models ≈ 1%; T0 and Tk-Instruct ≈ 2%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Binary/ternary tasks OOL < 1% (baseline for comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>TD shows ~+12 percentage points OOL relative to binary/ternary tasks; Flan models produce fewer OOL responses (~1%) than T0/Tk (~2%).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot prompting, counts aggregated across tasks and models; invalid outputs mapped to most frequent class.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wisdom of Instruction-Tuned Language Model Crowds. Exploring Model Label Variation', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7422.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7422.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ZSL vs FSL prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-Shot Learning (ZSL) versus Few-Shot Learning (FSL) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper compares zero-shot prompts (instruction only) to few-shot prompts (instruction plus exemplars); across subjective tasks (English Trustpilot) they find no consistent improvement from few-shot prompting — FSL showed higher variance and often did not outperform ZSL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5, Flan-UL2, T0 / mT0, Tk-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned T5-family models evaluated in both zero-shot and few-shot prompt settings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>English Trustpilot tasks (SA, TD, AC-Gender, AC-Age)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Subjective text classification tasks evaluated with macro-F1.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>ZSL: natural-language instruction only; FSL: same instruction with few-shot exemplars prepended (3 exemplars per class)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>few-shot vs zero-shot / prompt content</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>FSL used 3 exemplars per class (sampled from 4,000-instance pool); exemplar selection strategies included random, low-entropy, and max-entropy via MACE; exemplars were prepended to the instruction in prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Macro-F1 (ZSL vs FSL comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Authors report no statistically significant differences between ZSL and FSL prompting overall; FSL generally shows greater variance and does not consistently improve over ZSL across subjective tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>ZSL prompting (instruction-only) served as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>No consistent improvement; in aggregate FSL did not surpass ZSL and showed higher variance (no consistent absolute change reported).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>FSL exemplars: 3 examples per class chosen from 4,000-instance pool; selection strategies: Random, Low entropy, Max entropy (based on MACE); English Trustpilot; default decoding parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Authors state 'no statistically significant differences between the two prompting methods' (ZSL vs FSL).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wisdom of Instruction-Tuned Language Model Crowds. Exploring Model Label Variation', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7422.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7422.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Exemplar selection (entropy-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Entropy-based few-shot exemplar selection (MACE entropy: low vs max)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>For FSL, exemplars were chosen using MACE-derived example entropy to identify high-difficulty (max entropy) or low-difficulty (low entropy) seed examples; results showed no universal best strategy — effects were task-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5, Flan-UL2, T0 / mT0, Tk-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLMs; exemplar selection performed using MACE competence/entropy computed from LLM annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>English Trustpilot tasks (SA, TD, AC-Gender, AC-Age)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Subjective classification tasks; exemplars used in few-shot prompts to potentially improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot prompting with 3 exemplars per class selected by selection strategy</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>few-shot exemplar selection strategy</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Authors computed MACE entropy over a sample of 4,000 instances per task/label; from this pool they sampled 3 exemplars per class using strategies: Max-entropy (disagree/high difficulty), Low-entropy (agree/low difficulty), and Random. Those exemplars were prepended to prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Macro-F1; qualitative variance across strategies</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>No consistent trend across tasks: Max-entropy worked better for Sentiment Analysis and AC-Gender; Low-entropy worked better for AC-Age and Topic Detection; Random was inconsistent. Overall, exemplar-selection strategy effects were task-dependent and did not reliably beat ZSL.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Random exemplar selection and ZSL (no exemplars).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Task-dependent changes (no general absolute performance gain reported); overall no consistent FSL advantage.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Three exemplars per class from a 4,000-instance pool; selection via MACE entropy; English Trustpilot; default model params.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wisdom of Instruction-Tuned Language Model Crowds. Exploring Model Label Variation', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7422.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7422.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Aggregation of multiple LLM annotations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aggregating multiple LLM outputs (majority vote and MACE Bayesian aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors treat LLMs as annotators and aggregate their labels per example using majority voting and MACE (unsupervised Bayesian annotator competence estimation); aggregated labels typically outperform individual models on macro-F1.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5, Flan-UL2, T0 / mT0, Tk-Instruct (as annotator ensemble)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLMs whose per-example labels are combined by aggregation algorithms (majority vote and MACE) to produce final labels.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Trustpilot tasks and HatEval HS (all classification tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Ensemble annotation: produce a single label per instance by aggregating four LLM annotator outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Combining multiple independent LLM outputs into a single label via aggregation (majority vote; MACE computed competences)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>output aggregation / post-processing</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>For each example four model labels (one per model) were aggregated. Majority voting picks the most frequent label (ties random). MACE infers annotator competence and most likely labels unsupervisedly and also yields per-example entropy used in FSL exemplar selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Macro-F1 (compared to individual LLMs and baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Aggregated labels outperform individual LLMs on average: aggregated labels are on average 4.2 Macro-F1 points better than the average LLM; MACE was the best method in 6/14 tasks and aggregation methods were substantially more robust than any single model.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Individual LLM performance (per-model macro-F1) and simple baselines (most-frequent class, random choice).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+4.2 Macro-F1 points vs average individual LLM (average improvement); still below supervised models (supervised gap ≈ 10.5 F1 points on average).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Unsupervised aggregation over four LLM annotators; MACE competence correlated with actual performance (Spearman ρ=0.93, Pearson ρ=0.83); significance of individual model improvements over random evaluated with bootstrap p ≤ 0.01.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Individual model improvements over random baseline tested via 1,000 bootstrap samples (p ≤ 0.01) in Table 3; aggregated improvement statements reported but per-aggregation significance vs individuals not exhaustively given.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wisdom of Instruction-Tuned Language Model Crowds. Exploring Model Label Variation', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Scaling instruction-finetuned language models <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing <em>(Rating: 2)</em></li>
                <li>Multitask prompted training enables zero-shot task generalization <em>(Rating: 2)</em></li>
                <li>Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7422",
    "paper_id": "paper-269149543",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "Answer token in prompt",
            "name_full": "Explicit answer field token (\"Answer\") in prompt",
            "brief_description": "The authors add the literal token \"Answer\" (or &lt;Answer&gt;) after instruction text to mark the output field; they report this improves the LLMs' understanding and output format (reduces malformed outputs). This was used across all classification tasks in ZSL/FSL prompting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5, Flan-UL2, T0 / mT0, Tk-Instruct (instruction-tuned T5 family)",
            "model_description": "Encoder-decoder T5-family instruction-tuned models fine-tuned on multi-task instruction sets (FLAN, UL2, xP3/Super-NaturalInstructions).",
            "model_size": null,
            "task_name": "Trustpilot tasks (SA, TD, AC-Gender, AC-Age) and HatEval HS",
            "task_description": "Text classification tasks (sentiment 3-way, topic 5-way, demographic binary attributes, hate-speech binary).",
            "problem_format": "Natural-language instruction with an explicit labelled output field (explicit 'Answer' prompt token)",
            "format_category": "prompt style",
            "format_details": "Zero-shot/few-shot natural-language instruction followed by the explicit token 'Answer' (or &lt;Answer&gt;) to indicate output region; used uniformly across tasks to improve output formatting.",
            "performance_metric": "Macro-F1 and qualitative reduction in out-of-label responses",
            "performance_value": "Not quantified numerically in paper; authors state adding 'Answer' improves model understanding and output format (fewer malformed/OOL outputs).",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "ZSL and FSL prompting; default model parameters; used for all four LLMs and all tasks.",
            "statistical_significance": null,
            "uuid": "e7422.0",
            "source_info": {
                "paper_title": "Wisdom of Instruction-Tuned Language Model Crowds. Exploring Model Label Variation",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Tk-Instruct template fields",
            "name_full": "Tk-Instruct required prompt template with 'definition', 'input', 'output' fields",
            "brief_description": "Tk-Instruct requires a structured prompt template containing explicit 'definition', 'input', and 'output' fields; the authors used this format for Tk-Instruct and report it as part of the prompt engineering required by that model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Tk-Instruct (T5-based)",
            "model_description": "A T5-based instruction-following model fine-tuned on the SUPER-NATURALINSTRUCTIONS dataset (many declarative instructions).",
            "model_size": null,
            "task_name": "HatEval HS and Trustpilot classification tasks",
            "task_description": "Binary/ternary/5-way text classification tasks across languages.",
            "problem_format": "Structured prompt template with labeled fields ('definition', 'input', 'output')",
            "format_category": "prompt style / template requirements",
            "format_details": "Authors used a prompt with explicit &lt;Definition&gt; (instruction), &lt;Input&gt; (instance string), and &lt;Response&gt;/&lt;Output&gt; tokens for Tk-Instruct; example shown for HS: &lt;Definition&gt; Is this tweet expressing 'hate speech' or 'non-hate speech?' &lt;Input&gt; \"I hate you\" &lt;Response&gt;: {LM answer}.",
            "performance_metric": "Macro-F1; out-of-label rate reported elsewhere",
            "performance_value": "Not isolated numerically for template vs other prompt styles; included as required format for Tk-Instruct experiments.",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Applied when evaluating Tk-Instruct; ZSL and FSL contexts; default decoding parameters.",
            "statistical_significance": null,
            "uuid": "e7422.1",
            "source_info": {
                "paper_title": "Wisdom of Instruction-Tuned Language Model Crowds. Exploring Model Label Variation",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Label-list (multiple-choice) prompt",
            "name_full": "Providing explicit candidate labels in the prompt (multiple-choice style)",
            "brief_description": "Prompts present the candidate classes inline (e.g., 'Is this review about \"car lights\", \"fashion accessories\", \"pets\", ...?') to constrain LLM outputs to the set of target labels; used across Topic Detection and other tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5, Flan-UL2, T0 / mT0, Tk-Instruct",
            "model_description": "Instruction-tuned T5-family encoder-decoder models trained on many natural-language tasks and instructions.",
            "model_size": null,
            "task_name": "Topic Detection (5-way) and other classification tasks",
            "task_description": "Assign the input text to one of a fixed set of candidate labels (topics, sentiment classes, demographic classes, hate vs non-hate).",
            "problem_format": "Natural-language multiple-choice style: instance plus explicit enumeration of candidate labels",
            "format_category": "question type / prompt style",
            "format_details": "Prompts include the text to classify and an explicit list of choices (quoted strings). For TD the list varied by language (5 labels). Authors note this presentation but also report a higher out-of-label (OOL) rate for TD despite explicit labels.",
            "performance_metric": "Out-of-label rate (OOL), Macro-F1",
            "performance_value": "Topic Detection (TD) shows much higher OOL (~13% overall in ZSL) compared to binary/ternary tasks (&lt;1%); this occurred despite presenting enumerated candidate labels.",
            "baseline_performance": "Binary/ternary classification prompts with explicit labels had OOL &lt;1%",
            "performance_change": "+~12 percentage points OOL for TD vs binary/ternary tasks (ZSL)",
            "experimental_setting": "Zero-shot prompting with candidate-label enumeration; default model parameters.",
            "statistical_significance": null,
            "uuid": "e7422.2",
            "source_info": {
                "paper_title": "Wisdom of Instruction-Tuned Language Model Crowds. Exploring Model Label Variation",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Out-of-label (OOL) behavior by task and model",
            "name_full": "Out-of-label (invalid-class) predictions vary with task complexity and model",
            "brief_description": "When the model output does not correspond to a valid class, the authors assign the most common class; OOL rates were very low (&lt;1%) for binary/ternary tasks but substantially higher for descriptive multi-class Topic Detection (~13%). Flan models showed ~1% OOL, T0 and Tk-Instruct ~2% overall.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5, Flan-UL2, T0, Tk-Instruct",
            "model_description": "Instruction-tuned encoder-decoder models varying by instruction-fine-tuning corpora (FLAN, UL2, xP3, SUPER-NATURALINSTRUCTIONS).",
            "model_size": null,
            "task_name": "Topic Detection (TD) and binary/ternary classification tasks (SA, AC, HS)",
            "task_description": "Predict one of several descriptive topic labels (5-way) or binary/ternary labels for sentiment/demographics/hate.",
            "problem_format": "Natural-language instruction with candidate labels enumerated (TD) or simple binary/ternary label prompts",
            "format_category": "label presentation / prompt style",
            "format_details": "Descriptive, multi-word class labels (e.g., 'fashion accessories') in a list led to higher OOL in TD; simpler short labels and fewer classes yielded far fewer OOL outputs.",
            "performance_metric": "Out-of-label percentage",
            "performance_value": "TD OOL ≈ 13% (ZSL); binary/ternary tasks OOL &lt; 1%. Model-level OOL: Flan models ≈ 1%; T0 and Tk-Instruct ≈ 2%.",
            "baseline_performance": "Binary/ternary tasks OOL &lt; 1% (baseline for comparison)",
            "performance_change": "TD shows ~+12 percentage points OOL relative to binary/ternary tasks; Flan models produce fewer OOL responses (~1%) than T0/Tk (~2%).",
            "experimental_setting": "Zero-shot prompting, counts aggregated across tasks and models; invalid outputs mapped to most frequent class.",
            "statistical_significance": null,
            "uuid": "e7422.3",
            "source_info": {
                "paper_title": "Wisdom of Instruction-Tuned Language Model Crowds. Exploring Model Label Variation",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "ZSL vs FSL prompting",
            "name_full": "Zero-Shot Learning (ZSL) versus Few-Shot Learning (FSL) prompting",
            "brief_description": "The paper compares zero-shot prompts (instruction only) to few-shot prompts (instruction plus exemplars); across subjective tasks (English Trustpilot) they find no consistent improvement from few-shot prompting — FSL showed higher variance and often did not outperform ZSL.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5, Flan-UL2, T0 / mT0, Tk-Instruct",
            "model_description": "Instruction-tuned T5-family models evaluated in both zero-shot and few-shot prompt settings.",
            "model_size": null,
            "task_name": "English Trustpilot tasks (SA, TD, AC-Gender, AC-Age)",
            "task_description": "Subjective text classification tasks evaluated with macro-F1.",
            "problem_format": "ZSL: natural-language instruction only; FSL: same instruction with few-shot exemplars prepended (3 exemplars per class)",
            "format_category": "few-shot vs zero-shot / prompt content",
            "format_details": "FSL used 3 exemplars per class (sampled from 4,000-instance pool); exemplar selection strategies included random, low-entropy, and max-entropy via MACE; exemplars were prepended to the instruction in prompts.",
            "performance_metric": "Macro-F1 (ZSL vs FSL comparison)",
            "performance_value": "Authors report no statistically significant differences between ZSL and FSL prompting overall; FSL generally shows greater variance and does not consistently improve over ZSL across subjective tasks.",
            "baseline_performance": "ZSL prompting (instruction-only) served as baseline.",
            "performance_change": "No consistent improvement; in aggregate FSL did not surpass ZSL and showed higher variance (no consistent absolute change reported).",
            "experimental_setting": "FSL exemplars: 3 examples per class chosen from 4,000-instance pool; selection strategies: Random, Low entropy, Max entropy (based on MACE); English Trustpilot; default decoding parameters.",
            "statistical_significance": "Authors state 'no statistically significant differences between the two prompting methods' (ZSL vs FSL).",
            "uuid": "e7422.4",
            "source_info": {
                "paper_title": "Wisdom of Instruction-Tuned Language Model Crowds. Exploring Model Label Variation",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Exemplar selection (entropy-based)",
            "name_full": "Entropy-based few-shot exemplar selection (MACE entropy: low vs max)",
            "brief_description": "For FSL, exemplars were chosen using MACE-derived example entropy to identify high-difficulty (max entropy) or low-difficulty (low entropy) seed examples; results showed no universal best strategy — effects were task-dependent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5, Flan-UL2, T0 / mT0, Tk-Instruct",
            "model_description": "Instruction-tuned LLMs; exemplar selection performed using MACE competence/entropy computed from LLM annotations.",
            "model_size": null,
            "task_name": "English Trustpilot tasks (SA, TD, AC-Gender, AC-Age)",
            "task_description": "Subjective classification tasks; exemplars used in few-shot prompts to potentially improve performance.",
            "problem_format": "Few-shot prompting with 3 exemplars per class selected by selection strategy",
            "format_category": "few-shot exemplar selection strategy",
            "format_details": "Authors computed MACE entropy over a sample of 4,000 instances per task/label; from this pool they sampled 3 exemplars per class using strategies: Max-entropy (disagree/high difficulty), Low-entropy (agree/low difficulty), and Random. Those exemplars were prepended to prompts.",
            "performance_metric": "Macro-F1; qualitative variance across strategies",
            "performance_value": "No consistent trend across tasks: Max-entropy worked better for Sentiment Analysis and AC-Gender; Low-entropy worked better for AC-Age and Topic Detection; Random was inconsistent. Overall, exemplar-selection strategy effects were task-dependent and did not reliably beat ZSL.",
            "baseline_performance": "Random exemplar selection and ZSL (no exemplars).",
            "performance_change": "Task-dependent changes (no general absolute performance gain reported); overall no consistent FSL advantage.",
            "experimental_setting": "Three exemplars per class from a 4,000-instance pool; selection via MACE entropy; English Trustpilot; default model params.",
            "statistical_significance": null,
            "uuid": "e7422.5",
            "source_info": {
                "paper_title": "Wisdom of Instruction-Tuned Language Model Crowds. Exploring Model Label Variation",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Aggregation of multiple LLM annotations",
            "name_full": "Aggregating multiple LLM outputs (majority vote and MACE Bayesian aggregation)",
            "brief_description": "The authors treat LLMs as annotators and aggregate their labels per example using majority voting and MACE (unsupervised Bayesian annotator competence estimation); aggregated labels typically outperform individual models on macro-F1.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5, Flan-UL2, T0 / mT0, Tk-Instruct (as annotator ensemble)",
            "model_description": "Instruction-tuned LLMs whose per-example labels are combined by aggregation algorithms (majority vote and MACE) to produce final labels.",
            "model_size": null,
            "task_name": "Trustpilot tasks and HatEval HS (all classification tasks)",
            "task_description": "Ensemble annotation: produce a single label per instance by aggregating four LLM annotator outputs.",
            "problem_format": "Combining multiple independent LLM outputs into a single label via aggregation (majority vote; MACE computed competences)",
            "format_category": "output aggregation / post-processing",
            "format_details": "For each example four model labels (one per model) were aggregated. Majority voting picks the most frequent label (ties random). MACE infers annotator competence and most likely labels unsupervisedly and also yields per-example entropy used in FSL exemplar selection.",
            "performance_metric": "Macro-F1 (compared to individual LLMs and baselines)",
            "performance_value": "Aggregated labels outperform individual LLMs on average: aggregated labels are on average 4.2 Macro-F1 points better than the average LLM; MACE was the best method in 6/14 tasks and aggregation methods were substantially more robust than any single model.",
            "baseline_performance": "Individual LLM performance (per-model macro-F1) and simple baselines (most-frequent class, random choice).",
            "performance_change": "+4.2 Macro-F1 points vs average individual LLM (average improvement); still below supervised models (supervised gap ≈ 10.5 F1 points on average).",
            "experimental_setting": "Unsupervised aggregation over four LLM annotators; MACE competence correlated with actual performance (Spearman ρ=0.93, Pearson ρ=0.83); significance of individual model improvements over random evaluated with bootstrap p ≤ 0.01.",
            "statistical_significance": "Individual model improvements over random baseline tested via 1,000 bootstrap samples (p ≤ 0.01) in Table 3; aggregated improvement statements reported but per-aggregation significance vs individuals not exhaustively given.",
            "uuid": "e7422.6",
            "source_info": {
                "paper_title": "Wisdom of Instruction-Tuned Language Model Crowds. Exploring Model Label Variation",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Scaling instruction-finetuned language models",
            "rating": 2,
            "sanitized_title": "scaling_instructionfinetuned_language_models"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "rating": 2,
            "sanitized_title": "pretrain_prompt_and_predict_a_systematic_survey_of_prompting_methods_in_natural_language_processing"
        },
        {
            "paper_title": "Multitask prompted training enables zero-shot task generalization",
            "rating": 2,
            "sanitized_title": "multitask_prompted_training_enables_zeroshot_task_generalization"
        },
        {
            "paper_title": "Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks",
            "rating": 1,
            "sanitized_title": "supernaturalinstructions_generalization_via_declarative_instructions_on_1600_nlp_tasks"
        }
    ],
    "cost": 0.015048999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Wisdom of Instruction-Tuned Language Model Crowds. Exploring Model Label Variation</p>
<p>Flor Miriam Plaza-Del-Arco 
Department of Computing Sciences
Bocconi University
MilanItaly</p>
<p>Debora Nozza debora.nozza@unibocconi.it 
Department of Computing Sciences
Bocconi University
MilanItaly</p>
<p>Dirk Hovy Milanlp 
Department of Computing Sciences
Bocconi University
MilanItaly</p>
<p>Wisdom of Instruction-Tuned Language Model Crowds. Exploring Model Label Variation
0EA5E5A0C5B36A66338E48951B7CAD82model annotationmodel label variationsubjective taskslabel aggregationethics
Large Language Models (LLMs) exhibit remarkable text classification capabilities, excelling in zero-and few-shot learning (ZSL and FSL) scenarios.However, since they are trained on different datasets, performance varies widely across tasks between those models.Recent studies emphasize the importance of considering human label variation in data annotation.However, how this human label variation also applies to LLMs remains unexplored.Given this likely model specialization, we ask: Do aggregate LLM labels improve over individual models (as for human annotators)?We evaluate four recent instruction-tuned LLMs as "annotators" on five subjective tasks across four languages.We use ZSL and FSL setups and label aggregation from human annotation.Aggregations are indeed substantially better than any individual model, benefiting from specialization in diverse tasks or languages.Surprisingly, FSL does not surpass ZSL, as it depends on the quality of the selected examples.However, there seems to be no good information-theoretical strategy to select those.We find that no LLM method rivals even simple supervised models.We also discuss the tradeoffs in accuracy, cost, and moral/ethical considerations between LLM and human annotation.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have revolutionized many aspects of Natural Language Processing (NLP).Brown et al. (2020) showed that LLMs have few-shot (FSL) and even zero-shot learning (ZSL) capabilities in text classification due to their extensive pre-training.Subsequent iterations have further improved these capabilities.Those improvements have seemingly obviated one of the most time-and labor-intensive aspects of NLP: annotating enough data to train a supervised classification model.Instead, we can use LLMs to directly predict the labels via prompting.Indeed, various papers tested this hypothesis and found good performance on various NLP tasks (Zhao et al., 2023;Su et al., 2022;Wei et al., 2022;Brown et al., 2020;Plaza-del-Arco et al., 2023).However, upon closer inspection, these claims require some caveats: different models excel at different tasks, datasets, and formulations (Gilardi et al., 2023;Törnberg, 2023).</p>
<p>What if the answer is not to wait for one model to rule them all, though, but to treat their variation as specializations we can exploit, similar to the disagreement among human annotators?Different annotators have different strengths (or levels of reliability), and recent work (Basile et al., 2021;Plank, 2022) has suggested using this human label variation to our advantage.We test whether the same applies to LLMs if we treat them as "annotators".</p>
<p>We use four state-of-the-art open-source instruction-tuned LLMs to assess their capabilities as "annotators": Flan-T5 (Chung et al., 2022), Flan-UL2 (Tay et al., 2023), T0 (Sanh et al., 2022) SA Is the sentiment of this review "positive", "negative" or "neutral"?AC -Gender Is this review written by a "male" or a "female"?AC -Age Is this review written by a person "under 35", or "over 45"?TD Is this review about topic 1, topic 2, topic 3, topic 4, or topic 5? HS Is this tweet expressing "hate speech" or "non-hate speech"?(alongside its multilingual variant, mT0 (Muennighoff et al., 2022)), and Tk-Instruct (Sanh et al., 2022).(We use open models to mitigate potential concerns regarding data contamination during the evaluation process and to facilitate replication.)We evaluate them across five subjective prediction tasks (age, gender, topic, sentiment prediction, and hate speech detection) in four distinct languages: English, French, German, and Spanish.We use ZSL and FSL prompt instructions, similar to the ones we would give to human annotators.For FSL, we explore different entropy-based strategies to choose the seed examples.We then aggregate the LLM answers into a single label and evaluate their performance.</p>
<p>We find that different models indeed excel on some tasks or languages, but not on others.Some models even specialize on certain labels in a given task, but perform poorly on the others.Their behavior thus mimics human expertise in wisdom-of-the-crowd settings.</p>
<p>We then aggregate the model answers into a single label for each example.The simplest approach is majority voting: use the label that most LLMs suggested (ties are split randomly).However, the majority can still be wrong.Instead, we can use Bayesian models of annotation (Passonneau and Carpenter, 2014;Paun et al., 2018) to weigh the answers based on the inferred reliability of each annotator.This approach is similar to Bayesian classifier combination, but does not require gold labels to assign the scores.Instead, it is completely unsupervised.That distinction is crucial, as we want to work with unannotated data.</p>
<p>In most cases, the aggregated labels of either method outperform even the best individual LLM.On average, aggregated labels are 4.2 F1-points better than the average LLM.However, even the best-aggregated performance is still well below that of even simple supervised models trained on the same data, and substantially lower than Transformer-based supervised models (by over 10 F1 points on average).</p>
<p>In sum, aggregating several ZSL-prompted</p>
<p>LLMs is better than using a single LLM.Surprisingly, FSL-prompting is too varied to consistently improve performance.However, treating LLMs as annotators cannot rival using human annotators for fine-tuning or supervised learning.We also discuss what these results mean for the role of human annotation and supervised learning in NLP, with respect to performance, but also time, cost, bias, and ethics.</p>
<p>Contributions</p>
<p>(1) We explore the feasibility of four open-source instruction-tuned LLMs as "annotators" via ZSL and FSL prompting on five subjective classification tasks; (2) we compare them across four languages; (3) we analyze the robustness of two label aggregation methods to check whether we can benefit from model label variation in subjective tasks; and (4) we discuss the technical, moral, and ethical ramifications of this development for NLP and annotation.</p>
<p>Data</p>
<p>For our experiments, we use two datasets: Trustpilot (Hovy et al., 2015) and HatEval (Basile et al., 2019).Note that for most models, these datasets are "unseen", i.e., the data was not part of the LLMs' training.The one exception is HatEval in EN, which is included in Flan-T5 and Flan-UL2 models.We aim to evaluate their performance in a data contamination scenario, offering insights into models' generalization capabilities unaffected by such contamination.</p>
<p>Trustpilot (Hovy et al., 2015) is a multilingual dataset with demographic user information from various countries.It uses reviews from the user review website Trustpilot.To test a variety of languages commonly found in LLMs, we select data with English from the United States, German from Germany, and French from France for our experiments.The data includes labels for sentiment, the topic of the review, and two demographic dimensions of the review authors: self-declared gender and age (these two are not available for all data points).We use the same data splits as Hovy (2015) to ensure comparability and consistency.Given our ZSL setup, we evaluate on their evaluation sets for each language, which consists of the joint development and test sets.</p>
<p>HatEval (Basile et al., 2019) is a multilingual dataset for HS detection against immigrants and women on Twitter, part of a SemEval 2019 shared task.The dataset contains Spanish and English tweets manually annotated via crowdsourcing.We use the benchmark test set provided by the HatEval competition for both languages.</p>
<p>Tasks</p>
<p>We evaluate the performance of LLMs as annotators on five prediction tasks: four from the Trustpilot dataset and one from the HatEval corpus.These tasks involve sentiment analysis, topic detection, and predicting demographic attributes (gender and age).These two attribute classification (AC) tasks are binary: the gender of the text author (male or female)1 and the age of the text author (under 35 or above 45 years old).In the sentiment analysis (SA) task, reviews are classified into negative, neutral, and positive sentiments based on 1, 3, and 5-star ratings, respectively.The topic detection (TD) task uses the review categories of the texts to classify them into one of five topics.For this task, the exact topics vary across languages.</p>
<p>EN: Car lights, fashion accessories, pets, domestic appliances, and hotels.DE: Wine, car rental, drugs and pharmacy, flowers, and hotels.FR: Clothes and fashion, fashion accessories, pets, computer and accessories, and food and beverage.</p>
<p>For the hate speech detection (HS), the task is to classify a tweet as either hate speech or non-hate speech.</p>
<p>Models</p>
<p>We experiment with four state-of-the-art instructiontuned LLMs from the same model family, the T5 with an encoder-decoder (Raffel et al., 2020) architecture.We specifically select these models because they were fine-tuned on a diverse range of instructions.They use intuitive explanations of the downstream task to respond to natural language prompts, similar to the instructions provided to human annotators.Furthermore, these models are all open-source, letting us inspect the training data and examine data contamination.Our selection represents a realistic LLM annotator pool for a current NLP practitioner.As models evolve rapidly, though, this selection is likely to change.However, the results from using a diverse pool of LLM annotators should hold regardless.</p>
<p>In particular, we use the following models:</p>
<p>• Flan-T5 (Chung et al., 2022) is a sequence-tosequence transformer model built on the T5 architecture (Raffel et al., 2020).The model has been pre-trained with standard language modeling objectives and subsequent fine-tuning on the extensive FLAN collection (Longpre et al., 2023).The FLAN collection contains more than 1,800 NLP tasks in over 60 languages.We use the largest version2 of this model.</p>
<p>• Flan-UL2 (Tay et al., 2023) is the Flan version of the T5 and UL2 model.It has a similar architecture to T5, but with an upgraded pretraining procedure known as UL23 .</p>
<p>• T0 (Sanh et al., 2022) and the multilingual mT0 (Muennighoff et al., 2023).T04 is an encoderdecoder model based on T5 that is trained on a multi-task mixture of NLP datasets over different tasks.For the non-English languages, we use mT05 since T0 has been trained on English texts.mT0 is based on Google's mT5 (Xue et al., 2021) and has been fine-tuned on xP36 , which covers 13 training tasks across xP3 46 languages with English prompts.</p>
<p>• Tk-Instruct (Wang et al., 2022) is a generative model for transforming task inputs given declarative in-context instructions, like "Given an utterance and the past 3 utterances, output 'Yes' if the utterance contains the smalltalk strategy, otherwise output 'No'.Smalltalk is a cooperative negotiation strategy..." (adapted from Wang et al., 2022).It is also based on T5 but trained on all task instructions in a multi-task setup.It is fine-tuned on the SUPER-NATURALINSTRUCTIONS dataset (Triantafillou et al., 2020), a large benchmark of 1,616 NLP tasks and their natural language instructions.It covers 76 task types across 55 different languages7 .</p>
<p>Computing Infrastructure We run all experiments on a server with three NVIDIA RTX A6000 and 48GB of RAM.</p>
<p>Prompting</p>
<p>A prompt is an input that directs an LLM's text generation, ranging from a single sentence to a paragraph.It guides the model's comprehension and influences its output.Figure 1 depicts the task formulations (prompt instructions) we give to the LLMs, who act as our annotators, for every considered text classification task.We add "Answer" to mark the output field after the instruction to improve the LLMs' understanding and output format.For the TD tasks, the list of five topics varies by language.</p>
<p>For instance, the prompt for the English TD task is: "I love the earrings I bought," "Is this review about 'car lights,' 'fashion accessories,' 'pets,' 'domestic appliances,' or 'hotels'?"<Answer>: {LM answer}.Tk-Instruct requires a prompt template with specific fields: "definition", "input," and "output."The "definition" is used to specify the instruction or guidance, the "input" contains the instance to be classified, and the "output" is the output indicator.For instance, the prompt for the HS task is the following: <Definition> Is this tweet expressing "hate speech" or "non-hate speech?" <Input> "I hate you" <Response>: {LM answer}.</p>
<p>We use task-specific prompts to assess the model's performance on the resulting outputs for zero-and few-shot prediction.We used default parameters for the models.</p>
<p>If the output does not correspond to a valid class, we assign the most common class for that task.For instance, these out-of-label (OOL) predictions vary between tasks and models for the ZSL setup.Binary or ternary classification tasks (AC, SA, HS) exhibit a very low OOL percentage (&lt;1%).In contrast, TD shows a significantly higher percentage (13%) due to the larger number of classes and their more descriptive nature (e.g., "fashion accessories").At the model level, Flan models have a very low OOL percentage (1%), T0 and Tk-Instruct have a low OOL percentage (∼2%).</p>
<p>Baselines</p>
<p>We compare the LLMs across our five tasks to two baselines: the most frequent class and random choice.</p>
<p>The most frequent class baseline does not require any model.It always picks the most frequent label for a task as final prediction.This standard baseline method is very strong in unbalanced datasets.However, it requires knowledge of the label distribution.The random-choice method randomly picks a label from the set of labels for a task.It represents a lower bound.</p>
<p>Aggregation of Labels</p>
<p>For each example, we get four labels: one from each LLM annotator.We use two different methods to aggregate these four labels into a single label: majority voting, and a Bayesian model of annotation, Multi-Annotator Competence Estimation (MACE, Hovy et al., 2013).These methods use different aggregation methods.Both are common in the literature (Klie et al., 2023).</p>
<p>Majority-voting selects the label returned most by the four models.In case of a tie, it randomly chooses among the top candidates.This approach is common in many annotation projects, but has the drawback that the majority can still be wrong.</p>
<p>MACE is a Bayesian annotation tool that computes two scores: the competence (reliability) of each annotator (i.e., the probability an annotator chooses the "true" label based on their expertise instead of guessing one) and the most likely label.MACE uses variational Bayesian inference to infer both variables, and works on unlabeled data.The aggregated MACE labels are usually more accurate downstream than majority voting, and competence scores correlate strongly with actual annotator proficiency (Paun et al., 2018).Competence scores tend to correlate with annotators' actual expertise (Hovy et al., 2013), and can therefore be used to directly compare annotator quality in the absence of gold labels.As a probabilistic model, it also computes the entropy of each example, including both annotator competence and agreement.It is therefore a proxy for how "difficult" an example is to label.We use MACE to get the competence of each LLM and the entropy of each example, which we use to select seed examples for FSL.</p>
<p>Results</p>
<p>We compare the four models as annotators along several dimensions: How much do models agree with each other?This assesses the consensus among them and indicates specialization.How reliable is each model?This evaluates the consistency and trustworthiness of individual model predictions, key for label aggregation.How accurate are the predictions of the individual models versus their aggregations?This last question assesses the prediction quality of individual LLMs vis-a-vis aggregations to determine whether this approach is a viable alternative to supervised learning.</p>
<p>We first report ZSL results and then discuss the FSL setting separately (Section 4.4).</p>
<p>Inter-model Agreement</p>
<p>To assess the level of specialization among the LLMs as annotators, we evaluate their agreement.We use four common inter-annotator-agreement metrics: Cohen's κ (Cohen, 1960), Fleiss' κ (Fleiss, 1971), and Krippendorff's α (Krippendorff, 2011) (which all correct the observed agreement for expected agreement), as well as the unweighted raw agreement (i.e., the uncorrected level of agreement between LLMs).The results are shown in Table 1.Note that the number of labels does not factor into agreement, and that raw agreement is usually higher than chance-corrected inter-annotator agreement measures.</p>
<p>The results show a wide range of agreement values, but a few takeaways become apparent:</p>
<p>1) The scores suggest that the different models specialize on different tasks and labels.As we will see in the performance and reliability analysis, some models perform better on some tasks than others.Model specialization suggests that aggregation is likely beneficial (as the aggregation hopefully benefits from differing expertise).</p>
<p>2) Language does not factor into the differences.The models we test are all multi-lingual, and the languages we test are generally high-resource.The agreement difference between the different languages on the same task is negligible.</p>
<p>3) Some tasks show higher agreement than others: SA has higher scores than TD, and the others have little to no agreement.However, we do not know whether high-agreement tasks are inherently easier, or whether the models are all wrong in the same direction.</p>
<p>Reliability</p>
<p>When aggregating specialized annotators, we might want to trust more specialized ones more.We use the competence scores from MACE to assess the reliability of each model.Table 2 shows the competence scores.</p>
<p>The competence scores support the specialization hypothesis for the different models on different languages and tasks.No model is dominant in all settings, though the Flan models tend to have higher competence scores than the other models (reflected in their higher mean competence scores).</p>
<p>Model Performance and Robustness of Label Aggregation</p>
<p>Ultimately, we care about the predictive performance of the annotator method.Table 3 shows the macro-F1 scores of the LLMs on all tasks and languages.We compute the statistical difference of the individual results over the random-choice baseline, using a bootstrap sampling test with the bootsa 8 Python package.We use 1,000 bootstrap samples, a sample size of 20%, and p ≤ 0.01.Most models clearly and significantly outperform the random-choice and even mostfrequent-label baselines.Note though that Flan-T5 and Flan-UL2 included the HatEval dataset in their training.Consequently, they perform substantially better than the other models (with Flan-T5 receiving a very high competence score from MACE).</p>
<p>Aggregation When aggregating annotations into a single label, we implicitly assume that a) there is a single correct answer and b) the wisdom of the crowd will find it.The first assumption is up for debate (Basile et al., 2021), but the latter is clearly borne out by the results here.On average and in most individual cases, majority voting and MACE aggregation predictions are better than most models.In 6 out of 14 tasks, MACE was the best model.Note that for SA, Tk-Instruct performs better than the aggregation methods in all languages.For AC-Gender in English, Flan-UL2 is better, and in German, no method outperforms random choice (though MACE aggregation is close).</p>
<p>Overall, the two aggregation methods are substantially more robust than any one individual model across all languages and datasets (see the Mean results in Table 3).Presumably, they suffer less from the variance across tasks and languages and instead are able to exploit the specialization of each model as a source of information.The MACE competence score correlates with the actual performance of the models: 0.93 Spearman ρ and 0.83 Pearson ρ.This correlation suggests that MACE identified the model specializations correctly.A custom weighting of each model's prediction (for example, based on the actual performance) might perform even better.In practice, though, this weighting would of course be unknown.</p>
<p>Comparison to supervised learning ZSL holds a lot of promise for quick predictions, but to assess its worth, we need to compare it to supervised models based on human annotation.For the Trustpilot data, we compare our best ZSL result for each task and language (see Table 3  Except for 4 cases (AC-Gender and AC-Age in French, AC-Gender in English, HS in English), even the simple ML models beat the best ZSL result we achieved, be it from an LLM or aggregation method.Compared to the upper bounds from Hung et al. (2023), we see an average performance gap of 10.5 F1 points.Only for HS in English ZSL achieves better results, likely attributed to the data contamination found in the Flan models.</p>
<p>These results show that while ZSL might be a fast approximation for prediction tasks, it is still far from competitive with supervised learning.</p>
<p>Few-shot Learning</p>
<p>FSL has the potential to perform better than ZSL, so we ask whether using FSL models as annotators improves over our ZSL experiments.We investigate whether providing a limited set of examples enhances annotation capabilities, similar to instructing human annotators.The short answer is no.We apply this method to the English Trustpilot dataset.</p>
<p>To choose the seed examples, we compare three methods.Random selection and selection based on the MACE entropy scores9 .Entropy lets us identify two groups of examples: (1) maximum entropy indicates models were less confident or disagreed more, indicating higher difficulty, and</p>
<p>(2) low entropy indicates models were more confident or agreed more, indicating lower difficulty.For each task and label, we randomly choose 4,000 instances10 and use MACE to compute the entropy for each instance.From the initial pool of 4,000, we sample three exemplars per class based on the method (low entropy, max entropy) and use these as few-shot seeds, prepending them to the prompt.We compare these results to the random baseline.</p>
<p>Figure 2 shows a comparison between the ZSL and FSL approaches.Our analysis reveals that there are no statistically significant differences between the two prompting methods.In general, though, FSL does not perform as well as ZSL across subjective tasks.Within FSL, a prominent pattern emerges: it exhibits notably higher variance across tasks than ZSL.Presumably, exemplar quality heavily influences performance.</p>
<p>Regarding the two entropy-based selection methods, our results show no consistent trends between them.The choice is somewhat task-dependent: Max entropy seems to perform well for SA and AC-Gender tasks, while low entropy works best for AC-Age and TD tasks.The random strategy is less consistent across tasks.This discrepancy further underlines the inherent challenge in selecting 'good' exemplars for FSL.Our findings suggest that using no exemplars (ZSL) is generally more stable and consistent for aggregation.</p>
<p>Related Work</p>
<p>Generating human-annotated data is timeconsuming and costly, especially for complex or specialized tasks with limited available data.Instead, a possible solution is leveraging automatic annotation models, often using a small subset of labeled data (Smit et al., 2020;Rosenthal et al., 2021), known as 'weak supervision' (Stanford AI Lab Blog, 2019).Supervised learning has emerged as the dominant method, driven by the widespread adoption of traditional machine learning models and Transformer-based models like BERT (Devlin et al., 2019).</p>
<p>More recently, LLMs have shown zero-shot and few-shot learning capabilities (Brown et al., 2020).Researchers have further advanced these models with natural language instructions (Chung et al., 2022;Wang et al., 2022;Taori et al., 2023), enabling innovative techniques like prompting (Liu et al., 2023) without the need to train a supervised model.Several papers explored these new techniques with promising performance on various NLP tasks (Brown et al., 2020;Plaza-del-Arco et al., 2022;Su et al., 2022;Sottana et al., 2023).Recent work has focused on exploring the capabil-ities of LLMs as annotators.For instance, Lee et al. (2023) evaluate the performance and alignment between LLMs and humans, revealing that these models not only fall short in performing natural language inference tasks compared to humans but also struggle to capture the distribution of human disagreements accurately.Other studies have used ChatGPT as an annotator.Some report excellent capabilities (Huang et al., 2023;Gilardi et al., 2023;Törnberg, 2023;He et al., 2024), but Kuzman et al. (2023) found that ChatGPT's performance notably drops for less-resourced languages.Similarly, Kristensen-McLachlan et al. (2023) show that on two seemingly simple binary classification tasks, the performance of ChatGPT and open-source LLMs varies significantly and often unpredictably, and supervised models systematically outperform both types of models.</p>
<p>For annotation, it remains to be seen whether different instruction-tuned LLMs can generalize to any subjective text classification tasks in different languages, especially if these tasks and languages are not well-represented in the training data.Recent studies have shown the importance of considering human label variation (Basile et al., 2021;Plank, 2022), i.e., the disagreement between human annotators, as a source of information rather than a problem.However, how this human label variation also applies to LLMs remains unexplored.</p>
<p>Discussion</p>
<p>Our results indicate that treating LLMs as annotators and aggregating their responses is costeffective and quick.However, we also find that the overall performance is still well below that of even simple supervised models.</p>
<p>Human annotation still has a vital role if we focus on performance.As LLMs become more capable, this edge might diminish to the point where LLM annotation is equivalent to human annotation.As an aside, although all models are likely to improve across the board, we still expect specialization effects, meaning aggregation approaches will likely stay relevant for the foreseeable future.</p>
<p>But what about bias?Human label variation is not only due to different levels of expertise or diligence (Snow et al., 2008).It can also vary due to differing opinions, definitions, and biases (Shah et al., 2020).Specific tasks are subjective by nature (Basile et al., 2019;Rottger et al., 2022), but even seemingly objective tasks like part-of-speech tagging can have different interpretations (Plank et al., 2014).The current discussion around the moral and ethical alignment of LLMs (Liu et al., 2022) should make us cautious about using these models as annotators in subjective or sensitive tasks.Aggregation can overcome the biases of any one particular model, but it cannot safeguard against widespread biases.Lastly, annotation might be exploratory (the "descriptive" paradigm in Rottger et al., 2022), where the goal is to find the range of human responses.</p>
<p>However, replacing human annotators with LLMs has ramifications beyond performance and bias issues.While crowdsourced annotations can be problematic regarding worker exploitation (Fort et al., 2011), they often provide low to moderateincome earners with a way to supplement their living.Replacing this option with LLMs is a real-life example of automation making human jobs obsolete.Conversely, it may mitigate the mental health risks associated with annotating toxic or sensitive content, such as racist content or tasks related to mental disorders.Hybrid human and LLM annotation might offer a way forward here.</p>
<p>Conclusion</p>
<p>We use zero-and few-shot prompting to compare four current instruction-tuned LLMs as annotators on five subjective tasks in four languages.We find specialization across models and tasks.We leverage this variance similarly to human label variation by aggregating their predictions into a single label.This approach is, on average, substantially better than any individual model.This suggests that label aggregation consistently enhances performance compared to relying on a single LLM.Despite the rapid development of LLMs enhancing generalization to new tasks, aggregation remains a beneficial strategy.</p>
<p>Our findings suggest that practitioners aiming to label large amounts at minimal cost (both financially and time-wise) can benefit from the outlined aggregation approach.However, we also find that even the best models cannot compete with "traditional" supervised classification approaches.Furthermore, human annotation allows practitioners to encode a specific view or approach in a prescriptive manner or explore the range of responses descrip-tively (Rottger et al., 2022).Relying on LLMs while alignment and bias still need to be solved (Mökander et al., 2023) makes this approach unsuitable for sensitive applications.</p>
<p>Limitations</p>
<p>We were unable to compare to closed LLMs like GPT-4.While they are often state-of-the-art on many tasks, their training and setup change frequently and are, therefore, not replicable.Their pay-by-use nature also makes them less affordable for many practitioners than free open models.We do suspect, though, that including closed or generally better models will not change the overall conclusions of this paper.</p>
<p>Ethical Considerations</p>
<p>The data we use for AC-gender classification only makes a binary distinction (the Trustpilot website allowed users only to choose from two options).We do not assume this to be representative of gender identities and only use this data to test our hypotheses.</p>
<p>The languages we evaluate all come from the Indo-European branch of languages.The selection was due to data availability and our knowledge of languages.While we do not expect results to systematically differ from other languages, we do note that this is conjecture.</p>
<p>Figure 1 :
1
Figure 1: Instructions used to prompt the instruction-tuned LLMs for each classification task.</p>
<p>Figure 2 :
2
Figure 2: ZSL vs. FSL Macro-F1 scores on English Trustpilot tasks.FSL sample selection strategies: Low Entropy (↓ E), Max Entropy (↑ E), and Random (Rand).All FSL methods show much greater variance than ZSL.</p>
<p>Task/Language Cohen Fleiss Krip. Raw
EN 0.708 0.705 0.703 0.837SADE 0.636 0.633 0.630 0.792FR 0.665 0.662 0.660 0.809EN 0.299 0.279 0.229 0.615AC-GenderDE 0.271 0.136 -0.007 0.566FR 0.236 0.227 0.179 0.596EN 0.101 0.044 -0.154 0.428AC-AgeDE 0.068 0.040 -0.124 0.596FR 0.099 0.093 0.014 0.679EN 0.510 0.495 0.477 0.622TDDE 0.586 0.578 0.571 0.712FR 0.316 0.305 0.283 0.598HSEN 0.222 0.220 0.209 0.605 ES 0.155 0.099 -0.019 0.629</p>
<p>Table 1 :
1
Inter-model agreement scores.</p>
<p>Table 3 :
3
Hovy, 2015)pervised models.1)asimple Logistic Regression model (the baseline "agnostic" results reported inHovy, 2015)Zero-shot Macro-F1 results obtained by the LLMs on the Trustpilot and HatEval tasks, the baselines and the aggregation methods.Best result per language and task is shown in bold.Significant improvement over Random baseline ( ⋆ : p ≤ 0.01) with bootstrap sampling.For non-English languages, we use the multilingual mT0 model.
ModelsBaselinesAggregateTask/Lang.T0Flan-T5 Flan-UL2 Tk-Instruct Most Freq Random Majority MACEEN 0.453 ⋆ 0.532 ⋆0.482 ⋆0.553 ⋆0.1670.3340.503 ⋆ 0.514 ⋆SADE 0.469 ⋆ 0.495 ⋆0.433 ⋆0.517 ⋆0.1670.3310.480 ⋆ 0.484 ⋆FR 0.460 ⋆ 0.518 ⋆0.445 ⋆0.528 ⋆0.1670.3370.486 ⋆ 0.490 ⋆EN 0.516 0.594 ⋆0.624 ⋆0.541 ⋆0.3370.5010.617 ⋆ 0.623 ⋆AC-GenderDE 0.4560.4370.4470.4310.3340.4970.4580.485FR 0.428 0.573 ⋆0.566 ⋆0.563 ⋆0.3350.5030.577 ⋆ 0.579 ⋆EN 0.4950.4420.516 ⋆0.3970.3360.4970.569 ⋆ 0.572 ⋆AC-AgeDE 0.4580.3660.5030.3440.3340.5000.4220.499FR 0.4970.3750.550 ⋆0.3430.3350.5000.4430.542 ⋆EN 0.558 ⋆ 0.579 ⋆0.588 ⋆0.567 ⋆0.0850.1950.588 ⋆ 0.596 ⋆TDDE 0.506 ⋆ 0.514 ⋆0.513 ⋆0.493 ⋆0.1050.1930.516 ⋆ 0.520 ⋆FR 0.314 ⋆ 0.271 ⋆0.264 ⋆0.257 ⋆0.0960.1930.281 ⋆ 0.293 ⋆EN 0.5060.5370.5530.5150.2310.3820.5690.576MeanDE 0.4720.4530.4740.4460.2350.3800.4690.497FR 0.4250.4340.4560.4230.2330.3830.4470.476HSEN 0.621 ⋆ 0.726 ⋆ ES 0.601 ⋆ 0.532 ⋆0.670 ⋆ 0.5190.579 ⋆ 0.4490.367 0.3700.490 0.4920.717 ⋆ 0.726 ⋆ 0.533 ⋆ 0.603 ⋆and a recent Transformer-based model (the bestresults from Hung et al., 2023). Similarly, for Hat-Eval, we compare with 1) a simple linear SupportVector Machine based on a TF-IDF representation(the baseline results reported in Basile et al., 2019)and a fine-tuned multilingual Transformer model(Nozza, 2021). Table 4 shows the results. The twomethods approximate an upper and lower boundon supervised learning for these datasets.</p>
<p>best supervised Task/Language ZSL Standard ML Transformer
EN 0.5530.6100.680SADE 0.5170.6100.677FR 0.5280.6120.706EN 0.6240.6010.638AC-GenderDE 0.4970.5400.629FR 0.5790.5460.650EN 0.5720.6200.636AC-AgeDE 0.5030.6020.611FR 0.5500.5400.568EN 0.5960.6560.705TDDE 0.5200.6050.671FR 0.3140.3850.444HSEN 0.726 ES 0.6030.451 0.7010.416 0.752</p>
<p>Table 4 :
4
Macro-F1 results for best ZSL model (Table 3), compared to previous supervised results on the same datasets.</p>
<p>The data does not allow a more fine-grained classification of gender identities, as the original website only provided users with those two options. See Ethical Considerations for more discussion.
https://huggingface.co/google/flan-t5-xxl
https://huggingface.co/google/flan-ul2
https://huggingface.co/bigscience/T0
https://huggingface.co/bigscience/mt0-xxl
https://huggingface.co/datasets/bigscience/
https://huggingface.co/allenai/ tk-instruct-3b-def
https://github.com/fornaciari/boostsa
https://github.com/dirkhovy/MACE
We use 4,000 instances to make things as comparable as possible.
AcknowledgementsThis project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program (grant agreement No. 949944, INTEGRA-TOR).The research was made possible in part through an unrestricted Google research gift to explore variance in annotation.Flor Miriam Plaza-del-Arco, Debora Nozza, and Dirk Hovy are members of the MilaNLP group and the Data and Marketing Insights Unit of the Bocconi Institute for Data Science and Analysis.
SemEval-2019 task 5: Multilingual detection of hate speech against immigrants and women in Twitter. Bibliographical References, Valerio Basile, Cristina Bosco, Elisabetta Fersini, Debora Nozza, Viviana Patti, Francisco Manuel Rangel, Paolo Pardo, Manuela Rosso, Sanguinetti, 10.18653/v1/S19-2007Proceedings of the 13th International Workshop on Semantic Evaluation. the 13th International Workshop on Semantic EvaluationMinneapolis, Minnesota, USAAssociation for Computational Linguistics2019</p>
<p>We need to consider disagreement in evaluation. Michael Valerio Basile, Tommaso Fell, Dirk Fornaciari, Silviu Hovy, Barbara Paun, Massimo Plank, Alexandra Poesio, Uma, 10.18653/v1/2021.bppf-1.3Proceedings of the 1st Workshop on Benchmarking: Past, Present and Future. the 1st Workshop on Benchmarking: Past, Present and FutureOnline. Association for Computational Linguistics2021</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlishCurran Associates, Inc202033Language models are few-shot learners</p>
<p>Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Alex Chowdhery, Marie Castro-Ros, Kevin Pellat, Dasha Robinson, Sharan Valter, Gaurav Narang, Adams Mishra, Vincent Yu, Yanping Zhao, Huang, arXiv:2210.11416Scaling instruction-finetuned language models. Andrew Dai, Hongkun Yu, Slav Petrov, Ed H Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V Le, Jason Wei, 2022arXiv preprint</p>
<p>A coefficient of agreement for nominal scales. Jacob Cohen, Educational and Psychological Measurement. 201960</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, Minnesota20191Association for Computational Linguistics</p>
<p>Measuring nominal scale agreement among many raters. L Joseph, Fleiss, Psychological Bulletin. 761971</p>
<p>Karën Fort, Gilles Adda, K Bretonnel Cohen, 10.1162/COLI_a_00057Last words: Amazon Mechanical Turk: Gold mine or coal mine? Computational Linguistics. 201137</p>
<p>ChatGPT outperforms crowdworkers for text-annotation tasks. Fabrizio Gilardi, Meysam Alizadeh, Maël Kubli, arXiv:2303.150562023arXiv preprint</p>
<p>If in a crowdsourced data annotation pipeline. Zeyu He, Chieh-Yang Huang, Chien-Kuang Cornelia Ding, Shaurya Rohatgi, Ting-Hao' Kenneth, ' Huang, arXiv:2402.167952024a gpt-4. arXiv preprint</p>
<p>Demographic factors improve classification performance. Dirk Hovy, 10.3115/v1/P15-1073Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Long Papers. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingBeijing, ChinaAssociation for Computational Linguistics20151</p>
<p>Learning whom to trust with MACE. Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani, Eduard Hovy, Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAtlanta, GeorgiaAssociation for Computational Linguistics2013</p>
<p>User review sites as a resource for large-scale sociolinguistic studies. Dirk Hovy, Anders Johannsen, Anders Søgaard, 10.1145/2736277.2741141Proceedings of the 24th International Conference on World Wide Web, WWW '15. the 24th International Conference on World Wide Web, WWW '152015ternational World Wide Web Conferences Steering Committee</p>
<p>Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech. Fan Huang, Haewoon Kwak, Jisun An, 10.1145/3543873.3587368Companion Proceedings of the ACM Web Conference 2023. ACM2023</p>
<p>Can demographic factors improve text classification? revisiting demographic adaptation in the age of transformers. Chia-Chien Hung, Anne Lauscher, Dirk Hovy, Simone Paolo Ponzetto, Goran Glavaš, Findings of the Association for Computational Linguistics: EACL 2023. Dubrovnik, CroatiaAssociation for Computational Linguistics2023</p>
<p>Lessons learned from a citizen science project for natural language processing. Jan-Christoph Klie, Ji-Ung Lee, Kevin Stowe, Gözde Şahin, Nafise Sadat Moosavi, Luke Bates, Dominic Petrak, Richard Eckart De Castilho, Iryna Gurevych, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. the 17th Conference of the European Chapter of the Association for Computational LinguisticsDubrovnik, CroatiaAssociation for Computational Linguistics2023</p>
<p>Klaus Krippendorff, Computing krippendorff's alpha-reliability. Computing. 20111</p>
<p>Ross Deans Kristensen-Mclachlan, Miceal Canavan, Márton Kardos, Mia Jacobsen, Lene Aarøe, Chatbots are not reliable text annotators. 2023</p>
<p>ChatGPT: Beginning of an End of Manual Linguistic Data Annotation? Use Case of Automatic Genre Identification. Taja Kuzman, Igor Mozetič, Nikola Ljubešić, arXiv:2303.039532023arXiv preprint</p>
<p>Noah Lee, Na Min An, James Thorne, arXiv:2305.13788Can large language models infer and disagree like humans?. 2023arXiv preprint</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ACM Computing Surveys. 5592023</p>
<p>Aligning generative language models with human values. Ruibo Liu, Ge Zhang, Xinyu Feng, Soroush Vosoughi, 10.18653/v1/2022.findings-naacl.18Findings of the Association for Computational Linguistics: NAACL 2022. Seattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Barret Quoc V Le, Jason Zoph, Wei, arXiv:2301.13688The flan collection: Designing data and methods for effective instruction tuning. 2023arXiv preprint</p>
<p>Auditing large language models: a three-layered approach. Jakob Mökander, Jonas Schuett, Hannah Rose Kirk, Luciano Floridi, 10.1007/s43681-023-00289-2#citeasAI and Ethics. 2023</p>
<p>MTEB: Massive text embedding benchmark. Niklas Muennighoff, Nouamane Tazi, Proceedings of the 17th Conference of the European Chapter. the 17th Conference of the European ChapterDubrovnik, CroatiaAssociation for Computational Linguistics2023Loic Magne, and Nils Reimers</p>
<p>Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, arXiv:2211.01786Crosslingual generalization through multitask finetuning. Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, Colin Raffel, 2022arXiv preprint</p>
<p>Exposing the limits of zeroshot cross-lingual hate speech detection. Debora Nozza, 10.18653/v1/2021.acl-short.114Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20212Short Papers)</p>
<p>The benefits of a model of annotation. Rebecca J Passonneau, Bob Carpenter, 10.1162/tacl_a_00185Transactions of the Association for Computational Linguistics. 20142</p>
<p>Comparing Bayesian models of annotation. Bob Silviu Paun, Jon Carpenter, Dirk Chamberlain, Udo Hovy, Massimo Kruschwitz, Poesio, 10.1162/tacl_a_00040Transactions of the Association for Computational Linguistics. 62018</p>
<p>The "problem" of human label variation: On ground truth in data, modeling and evaluation. Barbara Plank, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Linguistically debatable or just plain wrong?. Barbara Plank, Dirk Hovy, Anders Søgaard, 10.3115/v1/P14-2083Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. the 52nd Annual Meeting of the Association for Computational LinguisticsBaltimore, MarylandAssociation for Computational Linguistics20142Short Papers)</p>
<p>Natural Language Inference Prompts for Zero-shot Emotion Classification in Text across Corpora. Flor Miriam, Plaza- Del-Arco, Roman María-Teresa Martín-Valdivia, Klinger, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational LinguisticsGyeongju, Republic of KoreaInternational Committee on Computational Linguistics2022</p>
<p>Respectful or Toxic? Using Zero-Shot Learning with Language Models to Detect Hate Speech. Flor Miriam, Plaza- Del-Arco, Debora Nozza, Dirk Hovy, 10.18653/v1/2023.woah-1.6The 7th Workshop on Online Abuse and Harms (WOAH). Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, 10.5555/3455716.3455856The Journal of Machine Learning Research. 2112020</p>
<p>SOLID: A large-scale semisupervised dataset for offensive language identification. Sara Rosenthal, Pepa Atanasova, Georgi Karadzhov, Marcos Zampieri, Preslav Nakov, 10.18653/v1/2021.findings-acl.80Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>Two contrasting data annotation paradigms for subjective NLP tasks. Paul Rottger, Bertie Vidgen, Dirk Hovy, Janet Pierrehumbert, 10.18653/v1/2022.naacl-main.13Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, Canwen Saiful Bari, Urmish Xu, Shanya Thakker, Eliza Sharma Sharma, Taewoon Szczechla, Gunjan Kim, Nihal Chhablani, Debajyoti Nayak, Jonathan Datta, Mike Chang, Tian-Jian, Han Jiang, Matteo Wang, Sheng Manica, Zheng Xin Shen, Harshit Yong, Rachel Pandey, Thomas Bawden, Trishala Wang, Jos Neeraj, Abheesht Rozen, Andrea Sharma, Thibault Santilli, Jason Fevry, Alan Fries, Ryan Teehan, Tali Bers, ; Alexander, M Rush, arXiv:2110.08207Multitask prompted training enables zero-shot task generalization. Stella Biderman, Leo Gao, Thomas Wolf2022arXiv preprint</p>
<p>Predictive biases in natural language processing models: A conceptual framework and overview. Deven Santosh Shah, H Andrew Schwartz, Dirk Hovy, 10.18653/v1/2020.acl-main.468Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Combining automatic labelers and expert annotations for accurate radiology report labeling using BERT. Akshay Smit, Saahil Jain, Pranav Rajpurkar, Anuj Pareek, Andrew Ng, Matthew Lungren, 10.18653/v1/2020.emnlp-main.117Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Cheap and fast -but is it good? evaluating non-expert annotations for natural language tasks. Rion Snow, O' Brendan, Daniel Connor, Andrew Jurafsky, Ng, Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing. the 2008 Conference on Empirical Methods in Natural Language ProcessingHonolulu, HawaiiAssociation for Computational Linguistics2008</p>
<p>Evaluation metrics in the era of GPT-4: Reliably evaluating large language models on sequence to sequence tasks. Andrea Sottana, Bin Liang, Kai Zou, Zheng Yuan, 10.18653/v1/2023.emnlp-main.543Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Weak supervision. Ai Lab Stanford, Blog, 2019</p>
<p>Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Tao Yu, arXiv:2209.01975Selective annotation makes language models better few-shot learners. 2022arXiv preprint</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Stanford Alpaca: An Instruction-following LLaMA model. 2023</p>
<p>Yi Tay, Mostafa Dehghani, Q Vinh, Xavier Tran, Jason Garcia, Xuezhi Wei, Hyung Won Wang, Siamak Chung, Dara Shakeri, Tal Bahri, Huaixiu Schuster, Denny Steven Zheng, Neil Zhou, Donald Houlsby, Metzler, arXiv:2205.05131Ul2: Unifying language learning paradigms. 2023arXiv preprint</p>
<p>ChatGPT-4 outperforms experts and crowd workers in annotating political twitter messages with zero-shot learning. Petter Törnberg, arXiv:2304.065882023arXiv preprint</p>
<p>Meta-dataset: A dataset of datasets for learning to learn from few examples. Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, Hugo Larochelle, International Conference on Learning Representations. 2020</p>
<p>Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Rohitha Phani, Pulkit Kaza, Ravsehaj Verma, Rushang Singh Puri, Savan Karia, Doshi, Keyur Shailaja, Siddhartha Sampat, Sujan Mishra, A Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022Kuntal Kumar Pal</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, arXiv:2206.07682Emergent abilities of large language models. 2022arXiv preprint</p>
<p>Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, 10.18653/v1/2021.naacl-main.41Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterOnline. Association for Computational Linguistics</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Yifan Dong, Chen Du, Yushuo Yang, Zhipeng Chen, Jinhao Chen, Jiang, arXiv:2303.18223A survey of large language models. Peiyu Liu, Jian-Yun Liu, Ji-Rong Nie, Wen, Ruiyang Ren, Yifan Li, Xinyu Tang2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
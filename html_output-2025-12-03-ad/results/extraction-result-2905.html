<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2905 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2905</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2905</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-72.html">extraction-schema-72</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-281673988</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.23979v1.pdf" target="_blank">ByteSized32Refactored: Towards an Extensible Interactive Text Games Corpus for LLM World Modeling and Evaluation</a></p>
                <p><strong>Paper Abstract:</strong> Simulating interactive world models remains a core challenge in Large Language Models(LLMs). In this work, we introduce the ByteSized32Refactored, a refactored, modular, and extensible implementation of the original ByteSized32 corpus to explore the task of text game generation. We further optimize the code structure of each text game and create the GameBasic.py foundation library, which centralizes common logic across all 32 games by abstracting 7 base classes (GameObject, etc.) into reusable modules, thereby reducing from 20k to 10k total lines of Python code compared to the original Bytesized32. Our refactored implementation enables extendability - with our centralized design, ByteSized32Refactored can be more efficiently extended to include text games of new scenarios and specifications by reusing the shared logic and functionalities. Extensive experiments with GPT-4o demonstrate a mix of performance - with Bytesized32Refactored, the generated text games for unseen scenarios showcase quality improvements on two of the four evaluation dimensions while decreases on the other two, indicating that the hierarchical structure of the refactored code presents new challenges for LLMs. Overall, we highlight that our extensible code structure, centered on the foundation library and the modular optimization, not only facilitates LLM adaptation to environment specifications but also establishes a scalable environment that supports future extensions.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2905.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2905.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o+Reflection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o with iterative self-reflection feedback loop</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o is used in the paper to generate and iteratively refine Python text-game implementations by receiving interpreter error outputs and performing up to three rounds of self-reflection; this feedback loop functions as an external, short-term memory-like mechanism improving game validity and winnability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-4o (with reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A prompting-driven LLM pipeline in which GPT-4o generates text-game code from a one-shot example and a task spec, then receives interpreter error traces and other diagnostics and performs iterative self-reflection/fix cycles (up to 3 rounds) to correct syntax, API, and logic problems in the generated game.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>ByteSized32Refactored (and original ByteSized32 in comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>A suite of 32 task-specific common-sense text games (e.g., washing dishes, boiling water) expressed as Python simulations; challenges include implementing object/container/device logic, respecting physical constraints, and producing winnable games.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>reflection-based (iterative feedback / short-term external memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Not a neural memory module — structured as an iterative external feedback loop: the model's previous generation (code) + interpreter error outputs and automated evaluation traces are returned to the model as part of the next prompt; the model conditions on that history across up to three reflection rounds to produce revised code. No persistent database/embedding index or retrieval store is described.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Direct prompt inclusion of prior generation and interpreter error messages (i.e., the model re-reads its previous outputs and the error traces supplied by the evaluation scripts); no semantic-index retrieval or attention-over-memory module is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Implicitly limited to the number of reflection rounds used (up to 3 rounds); capacity otherwise governed by the model prompt/context window (not specified numerically for the reflection buffer).</td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>The immediate previous code generation, interpreter error messages (syntax/API/runtime), automated evaluation outputs (valid-actions, initialization errors), and optionally playthrough traces used by the physical-reality/winnability evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>On ByteSized32Refactored (GPT-4o, before -> after 3 reflections): Game Initialization: 56.25% -> 85.42%; Valid Actions: 55.21% -> 70.83%; Runnable Game: 17.71% -> 61.46%; Winnability: 33.3% -> 54% (absolute changes shown in paper tables). Specification compliance for task-critical objects remained 100% (unchanged).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Initial (no reflection) generation (GPT-4o on ByteSized32Refactored, round 0): Game Initialization 56.25%; Valid Actions 55.21%; Runnable Game 17.71%; Winnability 33.3%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td>Runnable games +43.75 percentage points (61.46 - 17.71); Game Initialization +29.17 pp; Valid Actions +15.62 pp; Winnability +20.7 pp (all reported absolute increases from 0 to 3 reflection rounds).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Iterative self-reflection (feeding back interpreter errors and evaluation traces) substantially improves technical validity, runnable-ness, and winnability of LLM-generated text games; reflection closes many initial failures caused by hierarchical/abstracted code and increases compliance and winnability without sacrificing physical-reality alignment after multiple rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Memory is ephemeral and prompt-driven (no structured long-term store); reflection requires multiple rounds and can be slower/compute-intensive; hierarchical/refactored code initially confuses the model more than monolithic code, requiring more reflective iterations; the approach depends on having a functioning evaluation/execution pipeline to produce informative error traces.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ByteSized32Refactored: Towards an Extensible Interactive Text Games Corpus for LLM World Modeling and Evaluation', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2905.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2905.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4+Reflection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (including GPT-4-32k variant) with iterative self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 is evaluated as a baseline comparator to GPT-4o; it also uses the same reflection pipeline (interpreter error feedback) and shows measurable gains from reflection, but typically trails GPT-4o in absolute performance and benefits less from iterative correction in some metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-4 (with reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same prompting-and-reflection workflow as GPT-4o: generate a one-shot-derived text game, receive interpreter/evaluation errors, and iteratively refine the code across reflection rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td>GPT-4 (GPT-4-32k mentioned for some comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>Original ByteSized32 (comparison) and ByteSized32Refactored (reported comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>The 32-game ByteSized32 corpus (original and refactored variants) used as benchmarks for generating runnable, specification-compliant, physically-aligned text-game simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>reflection-based (iterative feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Same external iterative feedback loop: previous generation + interpreter/evaluation outputs are returned in the prompt for correction; no persistent or learned memory module described.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Direct prompt inclusion of prior outputs and error traces; no semantic retrieval mechanism described.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Limited by the number of reflection rounds (up to 3) and the model's prompt context window; exact capacity unspecified.</td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Previous code drafts, error messages, and evaluation traces supplied during reflection rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>On (original) ByteSized32 the paper reports that GPT-4 (example numbers): Game Initialization ~85.4% -> 89.6% after reflection; Valid Actions ~80.2% -> 88.5%; Runnable Game ~28.1% -> 57.3% (improvements reported in text comparing to GPT-4o results and across reflections).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Initial generation before reflection on the original corpus: Game Initialization ~85.4%; Valid Actions ~80.2%; Runnable Game ~28.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td>Runnable games improved by ~29.2 percentage points (57.3 - 28.1) in the reported comparison; other metrics improved modestly (e.g., Game Initialization +4.2 pp; Valid Actions +8.3 pp as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Reflection improves GPT-4's generated code quality and runnable-ness, but GPT-4 benefits less from iterative correction than GPT-4o in some alignment metrics; GPT-4's generations are more physically grounded initially but show smaller gains from reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Same limitations as for GPT-4o: prompt-limited, requires execution feedback, and lacks structured long-term memory; gains from reflection are model-dependent and smaller for GPT-4 than GPT-4o.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ByteSized32Refactored: Towards an Extensible Interactive Text Games Corpus for LLM World Modeling and Evaluation', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2905.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2905.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Reflection (iterative feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative self-reflection using interpreter/evaluation feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline-level memory-like mechanism used across models in the paper where generated code and playthrough/evaluation errors are fed back to the LLM in multiple rounds to correct and improve generated text games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflection pipeline (used with GPT-4o, GPT-4, attempted with GPT-5)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A procedural memory-like mechanism: after initial generation, automated evaluation (Python interpreter, trajectory-based checks for errors, physical reality alignment, and winnability tests) produces diagnostics; these diagnostics and error messages are included in subsequent prompts so the model can revise its code. Up to three reflection iterations are used in the main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td>Used with GPT-4o, GPT-4, attempted for GPT-5</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>ByteSized32Refactored and original ByteSized32 (applied to generated games across the corpora)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Same ByteSized32 family of task-specific Python text-game simulations; the reflection mechanism is applied to correct and improve generated games from these corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>reflection-based / short-term iterative feedback</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>External, prompt-mediated storage of previous outputs and interpreter diagnostics; structured as successive prompt contexts where each round includes the prior code and error traces. No embedding store or explicit indexing is used.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>The model re-reads prior code and error outputs because they are included in the prompt; retrieval is therefore prompt-access/recency-based rather than via a learned retrieval engine.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Configured to at most 3 reflection rounds in experiments; constrained by model context window for how much historical information can be included.</td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Previous code versions, interpreter error messages (syntax/API traces), automated evaluation judgments (technical validity, physical reality alignment outputs), and sampled playthrough traces used for diagnostics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Aggregated across models: consistent and substantial improvements in technical validity metrics and winnability (examples: GPT-4o runnable game 17.71% -> 61.46%; winnability 33.3% -> 54%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Initial (no-reflection) generations show much lower runnable and winnability rates (see numbers above for specific models).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td>See model-specific entries (e.g., GPT-4o runnable +43.75 pp; winnability +20.7 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Providing execution/evaluation feedback and allowing the LLM to iteratively correct its outputs is an effective short-term memory-like mechanism that substantially raises technical validity and winnability of generated text-game simulations; it compensates for difficulties the model has with hierarchical/refactored code structure.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Relies on a working evaluation/execution infrastructure; not robust across all LLM APIs (GPT-5 failed to reliably follow the evaluation output format and hit API/timeout limits); it is not a persistent or semantic memory and may not capture long-term experience or generalizable strategies beyond the corrected code snippets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ByteSized32Refactored: Towards an Extensible Interactive Text Games Corpus for LLM World Modeling and Evaluation', 'publication_date_yy_mm': '2025-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Starling: Self-supervised training of text-based reinforcement learning agent with large language models <em>(Rating: 2)</em></li>
                <li>Language guided exploration for rl agents in text environments <em>(Rating: 2)</em></li>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Graph constrained reinforcement learning for natural language action spaces <em>(Rating: 2)</em></li>
                <li>Byte-sized32: A corpus and challenge task for generating task-specific world models expressed as text games <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2905",
    "paper_id": "paper-281673988",
    "extraction_schema_id": "extraction-schema-72",
    "extracted_data": [
        {
            "name_short": "GPT-4o+Reflection",
            "name_full": "GPT-4o with iterative self-reflection feedback loop",
            "brief_description": "GPT-4o is used in the paper to generate and iteratively refine Python text-game implementations by receiving interpreter error outputs and performing up to three rounds of self-reflection; this feedback loop functions as an external, short-term memory-like mechanism improving game validity and winnability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GPT-4o (with reflection)",
            "agent_description": "A prompting-driven LLM pipeline in which GPT-4o generates text-game code from a one-shot example and a task spec, then receives interpreter error traces and other diagnostics and performs iterative self-reflection/fix cycles (up to 3 rounds) to correct syntax, API, and logic problems in the generated game.",
            "base_llm_model": "GPT-4o",
            "base_llm_size": null,
            "text_game_name": "ByteSized32Refactored (and original ByteSized32 in comparison)",
            "text_game_description": "A suite of 32 task-specific common-sense text games (e.g., washing dishes, boiling water) expressed as Python simulations; challenges include implementing object/container/device logic, respecting physical constraints, and producing winnable games.",
            "uses_memory": true,
            "memory_type": "reflection-based (iterative feedback / short-term external memory)",
            "memory_architecture": "Not a neural memory module — structured as an iterative external feedback loop: the model's previous generation (code) + interpreter error outputs and automated evaluation traces are returned to the model as part of the next prompt; the model conditions on that history across up to three reflection rounds to produce revised code. No persistent database/embedding index or retrieval store is described.",
            "memory_retrieval_mechanism": "Direct prompt inclusion of prior generation and interpreter error messages (i.e., the model re-reads its previous outputs and the error traces supplied by the evaluation scripts); no semantic-index retrieval or attention-over-memory module is reported.",
            "memory_capacity": "Implicitly limited to the number of reflection rounds used (up to 3 rounds); capacity otherwise governed by the model prompt/context window (not specified numerically for the reflection buffer).",
            "what_is_stored_in_memory": "The immediate previous code generation, interpreter error messages (syntax/API/runtime), automated evaluation outputs (valid-actions, initialization errors), and optionally playthrough traces used by the physical-reality/winnability evaluators.",
            "performance_with_memory": "On ByteSized32Refactored (GPT-4o, before -&gt; after 3 reflections): Game Initialization: 56.25% -&gt; 85.42%; Valid Actions: 55.21% -&gt; 70.83%; Runnable Game: 17.71% -&gt; 61.46%; Winnability: 33.3% -&gt; 54% (absolute changes shown in paper tables). Specification compliance for task-critical objects remained 100% (unchanged).",
            "performance_without_memory": "Initial (no reflection) generation (GPT-4o on ByteSized32Refactored, round 0): Game Initialization 56.25%; Valid Actions 55.21%; Runnable Game 17.71%; Winnability 33.3%.",
            "has_ablation_study": true,
            "memory_improvement_magnitude": "Runnable games +43.75 percentage points (61.46 - 17.71); Game Initialization +29.17 pp; Valid Actions +15.62 pp; Winnability +20.7 pp (all reported absolute increases from 0 to 3 reflection rounds).",
            "key_findings_about_memory": "Iterative self-reflection (feeding back interpreter errors and evaluation traces) substantially improves technical validity, runnable-ness, and winnability of LLM-generated text games; reflection closes many initial failures caused by hierarchical/abstracted code and increases compliance and winnability without sacrificing physical-reality alignment after multiple rounds.",
            "memory_limitations": "Memory is ephemeral and prompt-driven (no structured long-term store); reflection requires multiple rounds and can be slower/compute-intensive; hierarchical/refactored code initially confuses the model more than monolithic code, requiring more reflective iterations; the approach depends on having a functioning evaluation/execution pipeline to produce informative error traces.",
            "comparison_with_other_memory_types": null,
            "uuid": "e2905.0",
            "source_info": {
                "paper_title": "ByteSized32Refactored: Towards an Extensible Interactive Text Games Corpus for LLM World Modeling and Evaluation",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "GPT-4+Reflection",
            "name_full": "GPT-4 (including GPT-4-32k variant) with iterative self-reflection",
            "brief_description": "GPT-4 is evaluated as a baseline comparator to GPT-4o; it also uses the same reflection pipeline (interpreter error feedback) and shows measurable gains from reflection, but typically trails GPT-4o in absolute performance and benefits less from iterative correction in some metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GPT-4 (with reflection)",
            "agent_description": "Same prompting-and-reflection workflow as GPT-4o: generate a one-shot-derived text game, receive interpreter/evaluation errors, and iteratively refine the code across reflection rounds.",
            "base_llm_model": "GPT-4 (GPT-4-32k mentioned for some comparisons)",
            "base_llm_size": null,
            "text_game_name": "Original ByteSized32 (comparison) and ByteSized32Refactored (reported comparisons)",
            "text_game_description": "The 32-game ByteSized32 corpus (original and refactored variants) used as benchmarks for generating runnable, specification-compliant, physically-aligned text-game simulations.",
            "uses_memory": true,
            "memory_type": "reflection-based (iterative feedback)",
            "memory_architecture": "Same external iterative feedback loop: previous generation + interpreter/evaluation outputs are returned in the prompt for correction; no persistent or learned memory module described.",
            "memory_retrieval_mechanism": "Direct prompt inclusion of prior outputs and error traces; no semantic retrieval mechanism described.",
            "memory_capacity": "Limited by the number of reflection rounds (up to 3) and the model's prompt context window; exact capacity unspecified.",
            "what_is_stored_in_memory": "Previous code drafts, error messages, and evaluation traces supplied during reflection rounds.",
            "performance_with_memory": "On (original) ByteSized32 the paper reports that GPT-4 (example numbers): Game Initialization ~85.4% -&gt; 89.6% after reflection; Valid Actions ~80.2% -&gt; 88.5%; Runnable Game ~28.1% -&gt; 57.3% (improvements reported in text comparing to GPT-4o results and across reflections).",
            "performance_without_memory": "Initial generation before reflection on the original corpus: Game Initialization ~85.4%; Valid Actions ~80.2%; Runnable Game ~28.1%.",
            "has_ablation_study": true,
            "memory_improvement_magnitude": "Runnable games improved by ~29.2 percentage points (57.3 - 28.1) in the reported comparison; other metrics improved modestly (e.g., Game Initialization +4.2 pp; Valid Actions +8.3 pp as reported).",
            "key_findings_about_memory": "Reflection improves GPT-4's generated code quality and runnable-ness, but GPT-4 benefits less from iterative correction than GPT-4o in some alignment metrics; GPT-4's generations are more physically grounded initially but show smaller gains from reflection.",
            "memory_limitations": "Same limitations as for GPT-4o: prompt-limited, requires execution feedback, and lacks structured long-term memory; gains from reflection are model-dependent and smaller for GPT-4 than GPT-4o.",
            "comparison_with_other_memory_types": null,
            "uuid": "e2905.1",
            "source_info": {
                "paper_title": "ByteSized32Refactored: Towards an Extensible Interactive Text Games Corpus for LLM World Modeling and Evaluation",
                "publication_date_yy_mm": "2025-09"
            }
        },
        {
            "name_short": "Self-Reflection (iterative feedback)",
            "name_full": "Iterative self-reflection using interpreter/evaluation feedback",
            "brief_description": "A pipeline-level memory-like mechanism used across models in the paper where generated code and playthrough/evaluation errors are fed back to the LLM in multiple rounds to correct and improve generated text games.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Reflection pipeline (used with GPT-4o, GPT-4, attempted with GPT-5)",
            "agent_description": "A procedural memory-like mechanism: after initial generation, automated evaluation (Python interpreter, trajectory-based checks for errors, physical reality alignment, and winnability tests) produces diagnostics; these diagnostics and error messages are included in subsequent prompts so the model can revise its code. Up to three reflection iterations are used in the main experiments.",
            "base_llm_model": "Used with GPT-4o, GPT-4, attempted for GPT-5",
            "base_llm_size": null,
            "text_game_name": "ByteSized32Refactored and original ByteSized32 (applied to generated games across the corpora)",
            "text_game_description": "Same ByteSized32 family of task-specific Python text-game simulations; the reflection mechanism is applied to correct and improve generated games from these corpora.",
            "uses_memory": true,
            "memory_type": "reflection-based / short-term iterative feedback",
            "memory_architecture": "External, prompt-mediated storage of previous outputs and interpreter diagnostics; structured as successive prompt contexts where each round includes the prior code and error traces. No embedding store or explicit indexing is used.",
            "memory_retrieval_mechanism": "The model re-reads prior code and error outputs because they are included in the prompt; retrieval is therefore prompt-access/recency-based rather than via a learned retrieval engine.",
            "memory_capacity": "Configured to at most 3 reflection rounds in experiments; constrained by model context window for how much historical information can be included.",
            "what_is_stored_in_memory": "Previous code versions, interpreter error messages (syntax/API traces), automated evaluation judgments (technical validity, physical reality alignment outputs), and sampled playthrough traces used for diagnostics.",
            "performance_with_memory": "Aggregated across models: consistent and substantial improvements in technical validity metrics and winnability (examples: GPT-4o runnable game 17.71% -&gt; 61.46%; winnability 33.3% -&gt; 54%).",
            "performance_without_memory": "Initial (no-reflection) generations show much lower runnable and winnability rates (see numbers above for specific models).",
            "has_ablation_study": true,
            "memory_improvement_magnitude": "See model-specific entries (e.g., GPT-4o runnable +43.75 pp; winnability +20.7 pp).",
            "key_findings_about_memory": "Providing execution/evaluation feedback and allowing the LLM to iteratively correct its outputs is an effective short-term memory-like mechanism that substantially raises technical validity and winnability of generated text-game simulations; it compensates for difficulties the model has with hierarchical/refactored code structure.",
            "memory_limitations": "Relies on a working evaluation/execution infrastructure; not robust across all LLM APIs (GPT-5 failed to reliably follow the evaluation output format and hit API/timeout limits); it is not a persistent or semantic memory and may not capture long-term experience or generalizable strategies beyond the corrected code snippets.",
            "comparison_with_other_memory_types": null,
            "uuid": "e2905.2",
            "source_info": {
                "paper_title": "ByteSized32Refactored: Towards an Extensible Interactive Text Games Corpus for LLM World Modeling and Evaluation",
                "publication_date_yy_mm": "2025-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Starling: Self-supervised training of text-based reinforcement learning agent with large language models",
            "rating": 2,
            "sanitized_title": "starling_selfsupervised_training_of_textbased_reinforcement_learning_agent_with_large_language_models"
        },
        {
            "paper_title": "Language guided exploration for rl agents in text environments",
            "rating": 2,
            "sanitized_title": "language_guided_exploration_for_rl_agents_in_text_environments"
        },
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 2,
            "sanitized_title": "playing_textadventure_games_with_graphbased_deep_reinforcement_learning"
        },
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces",
            "rating": 2,
            "sanitized_title": "graph_constrained_reinforcement_learning_for_natural_language_action_spaces"
        },
        {
            "paper_title": "Byte-sized32: A corpus and challenge task for generating task-specific world models expressed as text games",
            "rating": 2,
            "sanitized_title": "bytesized32_a_corpus_and_challenge_task_for_generating_taskspecific_world_models_expressed_as_text_games"
        }
    ],
    "cost": 0.013679249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ByteSized32Refactored: Towards an Extensible Interactive Text Games Corpus for LLM World Modeling and Evaluation
28 Sep 2025</p>
<p>Haonan Wang 
Johns Hopkins University</p>
<p>Junfeng Sun 
Liaoning
Technical University</p>
<p>Xingdi Yuan 
Microsoft Research Montréal</p>
<p>Ruoyao Wang wangruoyao@cufe.edu.cn 
Central University of Finance and Economics</p>
<p>Ziang Xiao ziang.xiao@jhu.edu 
Johns Hopkins University</p>
<p>ByteSized32Refactored: Towards an Extensible Interactive Text Games Corpus for LLM World Modeling and Evaluation
28 Sep 2025B00B5EFF45858B059BEEDBB0248E3C1FarXiv:2509.23979v1[cs.CL]Game1.pyGame2.pyGame3.py……… Game32.pyGame1.pyGame2.pyGame3.py……… Game32.py20k lines of PYTHON code Bytesized 32 Validity Compliance Winnability Alignment Reflection Generated Game Auto Evaluation Validity Compliance Winnability Alignment Reflection Generated Game
Simulating interactive world models remains a core challenge in Large Language Models(LLMs).In this work, we introduce the ByteSized32Refactored 1 , a refactored, modular, and extensible implementation of the original ByteSized32 2 corpus to explore the task of text game generation.We further optimize the code structure of each text game and create the GameBasic.pyfoundation library, which centralizes common logic across all 32 games by abstracting 7 base classes (GameObject, etc.) into reusable modules, thereby reducing from 20k to 10k total lines of Python code compared to the original Bytesized32.Our refactored implementation enables extendability -with our centralized design, ByteSized32Refactored can be more efficiently extended to include text games of new scenarios and specifications by reusing the shared logic and functionalities.Extensive experiments with GPT-4o demonstrate a mix of performance -with Byte-sized32Refactored, the generated text games for unseen scenarios showcase quality improvements on two of the four evaluation dimensions while decreases on the other two, indicating that the hierarchical structure of the refactored code presents new challenges for LLMs.Overall, we highlight that our extensible code structure, centered on the foundation library and the modular optimization, not only facilitates LLM adaptation to environment specifications but also establishes a scalable environment that supports future extensions.</p>
<p>Introduction</p>
<p>Human intelligence and problem-solving are characterized by the ability to understand and interact with structured representations of the world, which support reasoning, planning, and generalization across complex environmental tasks (Gignac and Szodorai, 2024;Barsalou, 1999).Large language models (LLMs) have demonstrated impressive capabilities in natural language processing and zeroshot performance across task (Kojima et al., 2022).While these abilities indicate a nascent capacity for LLMs to possess the foundations of world modeling, their reasoning remains largely textual and lacking explicit formalization.Using LLMs to play text games primarily assesses their reasoning under constrained specifications, rather than whether they maintain consistent and structured models of the world (Jansen and Côté, 2022;Latitude Games, 2019;Hausknecht et al., 2019).To move beyond surface-level evaluation, we argue that generating interactive world simulators in text games provides a more suitable medium for examining world models in LLMs.It offers a structured, interpretable, and testable representation of environments as they formalize objects, states, constraints, and goals with natural language description (Wang et al., 2023(Wang et al., , 2022)).ByteSized32 (Wang et al., 2023(Wang et al., , 2022) ) marked an advanced step in formalizing world modeling, which focuses on the challenge of building text-based environments, where players interact with the world through natural language descriptions and commands.Such text-based environments enable agents to effectively operate within interactive text games to accomplish goals in complex scenarios through natural language input.Its highlights make it an ideal testing environment for evaluating the ability of an AI system in understanding, manipulating and constructing structured representations of the world.However, despite its significance, ByteSized32 also presents notable limitations in extendable code structure design: each game is implemented as a standalone file with logic, which hampers extensibility (adding new actions/objects/rules) and better input formulation for LLMs, i.e. flexible single-shot and multi-shot evaluation within a fixed context window, to generate Figure 1: An overview of text game generation and evaluation process between the ByteSized32Refactored and ByteSized32.GPT-4o and GPT-5 generate games using in-context learning with a prompt consisting of (1) a single template example game, and (2) the task specification for the target game to generate.Generated games are then self-reflected by providing the models with error outputs from a PYTHON interpreter that detects syntactic and API issues.Each generated game is subsequently evaluated by an automated evaluation for technical validity, specification compliance, physical reality alignment, and winnability.new text games.In this work, we present Byte-Sized32Refactored, a restructured, extensible text game corpus built upon the original ByteSized32, designed to evaluate the ability of multiple large language models(LLMs) like GPT-4o 3 to construct and test task-specific world environments.Specifically, compared with the original ByteSized32, by reorganizing the foundation library GameBasic.pythat abstracts the 7 base classes (e.g.GameObject, Container, World, Agent, and TextGame, etc.) and optimizing the code details of each game file.As a result, each game in RefactoredBytesized32 now only implements domain-specific objects and task logic, while all shared functionality is reused from GameBasic.py,ensuring modularity, extensibility, and reproducibility across the corpus.Moreover, the refactorization enhances the extendability of Bytesized32 to incorporate new text games with novel specifications, improves the utility of existing text games as demonstrations for in-context game generation, and presents a new challenge to LLMs for code generation with hierarchical structures.</p>
<p>The contributions of this work are as follows:</p>
<ol>
<li>We present ByteSized32Refactored, a modu-3 https://openai.com/index/gpt-4o-system-card/lar refactorization of the original ByteSized32 text game corpus (expressed as common-sense text games in Python).In contrast to the original 20k lines monolithic implementation, our refactored corpus further optimized each game by replacing verbose control flows with dictionary-based dispatch, streamlining string operations, and abstracting redundant logic to comprise 10k lines (subsubsection 3.2.2). 2. We develop GameBasic.py(subsubsection 3.2.1, Figure 2), a foundational library that provides a shared abstraction layer for all 32 text games, ensuring structural consistency, reducing redundant code, and enabling scalable extensions for future development.3. We evaluate GPT-4o on the refactored and original corpus, providing a fair basis for measuring progress over prior results (section 4).</li>
</ol>
<p>2 Related Work</p>
<p>Text Game</p>
<p>Text games, also known as interactive fiction (IF) environments are simulate interactive worlds in which both observations and actions are expressed entirely in natural language.These text games costs much less than for 2D or 3D games and provides natural language game descriptions of the current state (Jansen, 2021;Li et al., 2021;Nelson, 2006;Wang et al., 2025), which require users or agents to understand and generate natural language commands to interact with the game environment, which have been widely used as a challenging testbed for natural language processing to evaluate multiple capabilities of AI systems (Côté et al., 2018;Wang et al., 2023Wang et al., , 2022;;Jansen and Côté, 2022;Wang et al., 2024;Shridhar et al., 2020).Two dominant approaches have emerged in evaluating AI systems to navigate and complete text world games: reinforcement learning (RL)-based agents modeling the environment as a POMDP (Kaelbling et al., 1998) and large language model (LLM)based agents, where observations are input to the LLMs and the outputs are executed as actions (Cui et al., 2025).For (RL)-based agents, prior works have explored various techniques to enhance the learning process and performance for non-LLMbased agents.Subsequent efforts augmented RL agents with structured knowledge representations, such as knowledge graphs, to improve state tracking and filter irrelevant actions (Narasimhan et al., 2015;Hausknecht et al., 2020;Ammanabrolu and Riedl, 2018;Yuan et al., 2018;Murugesan et al., 2021;Ryu et al., 2023).More recent studies have introduced advanced mechanisms, including graph attention layers for more efficient action selection (Ammanabrolu and Hausknecht, 2020), roleplaying agents that intrinsically reward adherence to personas (Peng et al., 2023), and soft prompts to enable a single agent to adopt multiple personas (Cui et al., 2023) flexibly.In addition to these purely RL-based methods, recent work has explored hybrid approaches that integrate LLMs with RL agents.(Basavatia et al., 2024)employ LLMs to procedurally generate novel text game environments, thereby enabling RL agents to be tested on generalization across unseen tasks.Similarly, (Golchha et al., 2024) utilizes LLMs to provide decision-level guidance to RL agents, improving their reasoning and action selection.</p>
<p>LLMs for Code Generation</p>
<p>Large Language models (LMs) are rapidly being deployed in commercial applications, and several recent works for evaluated the ability of LLMs to generate executable program code (Dong et al., 2025;Huynh and Lin, 2025).The current standard benchmarks HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and APPS (Hendrycks et al., 2021) are in a longstanding pursuit of synthesizing code from natural language descriptions (Yu et al., 2018;Li et al., 2022;Zan et al., 2022).Similarly, SWE-bench (Jimenez et al., 2024) (Yin et al., 2025)extends evaluation to tasks such as code understanding and review.In addition to these benchmarks, several works have explored a variety of extensions have been proposed to broaden its coverage.These include multi-language variants (Cassano et al., 2022;Athiwaratkun et al., 2022;Orlanski et al., 2023), modifications to the edit scope and task granularity (Yu et al., 2024;Du et al., 2023), as well as novel code completion benchmarks (Muennighoff et al., 2023;Liu et al., 2023a).Other efforts introduce alternative coding paradigms (Yin et al., 2022;Yang et al., 2023).However, most of these benchmarks remain limited to short code snippets and fail to capture the complexity of executable programs.They rarely treat code as a fully interactive environment for simulating the world.</p>
<p>3 Bytesized32Refactored Corpus</p>
<p>Corpus Overview</p>
<p>Similar to the original BYTESIZED32, the Refac-toredByteSized32 also contains 32 common-sense task-specific text games like washing dishes, boiling water.Each text game is accompanied by a task specification, which provides a structured summary of the interactive environment and game objectives, including:</p>
<p>• Task Description: The natural language description of the task goal the agent must achieve, (e.g., washing dirty dishes using a dishwasher).• Task-Critical Objects: The set of objects indispensable for solving the task (e.g., dishes, dish soap, dishwasher).• Distractors: Objects or actions that are irrelevant to the task or deliberately increase its difficulty (adding food that creates additional dirty dishes).• Solution:A high-level procedural outline describing a canonical way to complete the task, such as opening the dishwasher.</p>
<p>Refactored Code Structure</p>
<p>Each task specification of the text game encodes a goal-oriented task that requires agents to manipulate objects, containers, and devices through naturalistic action sequences shown in Figure 5 and Table 1.It is worth noting that we show in Table 2 the number of examples that can fit in the context window of various LLMs.Compared with Byte-sized32, the Bytesized32Refactored corpus allows substantially more examples to be packed into the prompt as demonstrations under different context limits.</p>
<p>GameBasic.py Library</p>
<p>In the original ByteSized32 implementation, each game file embedded the complete implementation of multiple base classes such as GameObject, Container, World, Agent, and TextGame.While this approach ensures code independence, it results in significant maintenance costs.We developed GameBasic.py in RefacoredByteSized32, a centralized abstraction library that encapsulates common components into a modular code structure shown in Figure 2. Instead of these original classes in each game file, the GameBasic.pyprovides a unified interface and shared logic for core functionalities, which defines seven base class abstractions: GameObject (root class for all entities), Container and Device (interactive objects), Substance (physical state modeling), World and Agent (environment and player), and TextGame (template for task execution) by inheriting from these base classes.Compared to the code structure of the original ByteSized32, the GameBasic.pybrings two core contributions to the RefacoredByteSized32:</p>
<p>• Consistency: All games inherit from a unified parent class interface, ensuring structural uniformity.Developers can implement task-specific logic by simply extending these base classes, significantly reducing development complexity.These shared components in GameBasic.pyare centralized and maintained in a single location, drastically reducing repetitive code and improving maintainability.• Flexibility and Scalability: Not only provides generic class implementations but also reserves function interfaces (e.g., initializeWorld(), getTaskDescription()).This design enables each game to implement customized logic with minimal development overhead, thereby enhancing extensibility and adaptability for future tasks.</p>
<p>Experiment</p>
<p>Experiment Setup</p>
<p>We demonstrate the utility of the Byte-Sized32Refactored compared with the original ByteSized32 by evaluating the quality of games generated by SOTA large language model (i.e., GPT-4o) on both corpora.Specifically, each evaluation task provides the model with 1) a one-shot example of a game in the corpus; 2) a task specification drawn from an unseen evaluation set that asks the model to generate a new game following the specification.We follow the original ByteSized32 to randomly select the one-shot example and adopt the same prompt for task specification.However, while each example is presented as a single code snippet in the original corpus, in ByteSized32Refactored, we present the one-shot example with 1) the code and corresponding descriptions of GameBasic.pythat informs the model of the unified interface and shared logic for game construction, and 2) the actual game code of the example showcasing how to properly leverage the classes in GameBasic.pyto generate a game.We provide complete details of prompts and models in Figure F and Appendix A, respectively.</p>
<p>Evaluation Metrics</p>
<p>To ensure fair comparison with the original ByteSized32, we follow the original ByteSized32 to assess the quality of generated games from the following four dimensions: Technical Validity assesses whether core mechanisms of the game run error in Python, which includes environment initialization, valid action generation, and state updates.We explore a trajectory-based method that action sequences from the initial state up to depth three (max 100 actions each) to detect runtime errors and logic inconsistencies, thereby ensuring stability and systematic verification.Specification Compliance measures whether a generated game satisfies its task requirements by verifying the presence and correctness of required objects, actions, and other elements.Compliance is evaluated through automatic matching against the task specification, ensuring fidelity to the intended design.Physical Reality Alignment evaluates whether game actions respect basic physical constraints (e.g., opening a container before placing objects inside).Using trajectory-based exploration up to depth three with 100 sampled trajectories, we evaluate each step through binary judgments with justifications, ensuring logical grounding in real-world principles.Winnability focuses on whether the generated game is winnable, i.e., whether at least one complete sequence of actions can lead to successful task completion in the game.</p>
<p>Results and Analysis</p>
<p>We evaluate all generated games by GPT-4o on ByteSized32Refactored and the original Byte-sized32, reporting results before and after selfreflection to investigate whether our providing refactorization meaningfully change how LLMs build world modeling.Technical Validity The results of the technical validity are summarized in Table 3 and Table 4. On all of game initialization, valid actions and runnable game, the generated games demonstrate increasing quality with more founds of reflection on both Bytesized32Refactored and the original Bytesized32, showcasing the effectiveness of reflection in improving the quality of game generation across original and refactored codebase.However, on all of the three metrics, the game quality in Bytesized32Refactored down-performs the counter-part in the original Bytesized32 across all rounds of reflection.The degredation of quality reveals that the refactored code presents new challenges for LLM to reasoning over the hierarchical code structure and perform more complex classbased code generation.</p>
<p>Specification Compliance and Winnability</p>
<p>We report the specification compliance and winnablity performance of GPT-4o under the refactored and original corpus in Table 5 and Table 6.Specif-  ically, on both corpus, the compliance of taskcritical objects and actions and winnability increase with three rounds of reflection whereas the number of distractors slightly drops.Across inital generation and reflections, the generated game on Bytesized32Refactored consistently outperforms games on the original counterpart over object/action compliance and winnability, showcasing that GPT-4o is able to generate more reasonable games that better comply with specifications on Byte- with physical reality on both corpora.On average, comparing with performance on the original corpus, GPT-4o struggles to output physically-aligned games on Bytesized32Refactored on its initial generation but can attain a similar level of alignment after reflection.By comparing the two histograms, we observe that the higher abstraction and compressed structure of Refactored ByteSized32 initially make it more challenging for GPT-4o to capture physical consistency.This is evident in the distribution, which initially clusters near zero with few mid-or high-scoring samples.However, reflection significantly reduces low scores and gradually shifts the distribution toward the mid-and highrange.The results indicate that with reflection, the refactored corpus does not hinder GPT-4o from yielding physically-aligned text games compared to the original corpus.</p>
<p>Summary of results</p>
<p>In summary, our findings across the four evaluation dimensions highlight two key insights in the evaluation of LLMs for textgame generation: (1) The abstracted interfaces in GameBasic.pypose a greater challenge for GPT-4o than the redundant implementation in the original corpus, which embeds more explicit action cues that enables models to pass validity checks without deep reasoning.By contrast, abstraction increases reasoning demands and raises the generation barrier, making GPT-4o more prone to omitting nec-essary components or producing invalid structures during initial generation.</p>
<p>(2) On the other hand, our hierarchical abstractions enables LLMs to generate valid text games that 1) are much more complied with given specification; 2) possess high-quality underlying game logic to ensure winnability; and 3) do not sacrifice the alignment with physical reality under reflection.</p>
<p>Conclusion</p>
<p>In this work, we presented Byte-Sized32Refactored, a modular and extensible reimplementation of the original ByteSized32 corpus, designed to advance the exploration of text game generation with large language models (LLMs).GameBasic.pynot only reduces code redundancy in text-based games, but also decouples basic components, such as gameobject, container, and agent, from specific task logic, providing a unified framework for future text-based games.This design makes common text-based game features easy to extend and opens up the possibility of unlimited future expansion.With our refactored design, ByteSized32Refactored can be easily extended to new game scenarios by reusing common game logic and substantially increasing the number of examples in a limited LLM context window.Our experiments with GPT-4o demonstrate mixed performance of the generated games, revealing both the effectiveness of the refactored codebase and new challenges presented by the hierarchical design.Overall, our findings emphasize both the promise and limitations of modular design and foundational libraries in enabling LLMs to handle complex environment specifications, paving the way for future progress at the intersection of world modeling and code generation.</p>
<p>Limitations</p>
<p>Our work has several limitations that should be acknowledged.1.The ByteSized32Refactored corpus alters token distribution and code structure.This raises the possibility that observed improvements stem from structural biases rather than genuine advancements in modeling ability.</p>
<p>A Model Setting</p>
<p>In this work, we make extensive use of OpenAI's API.In all our experiments, we keep the following hyperparameters constant:In this work, we make extensive use of OpenAI's API.In all our experiments, we keep the following hyperparameters constant:</p>
<p>A.1 GPT-4o Model setting C GPT-5 Experiment error and analysis { "idx":0, "evaluation":"no", "short_justification":"The engine required holding the " "accessible butter to put it into/onto " "the pot, despite manipulation without "</p>
<p>"prior pickup being allowed, so the " "responses aren't physically accurate."} { "idx":0, "evaluation":"yes", "short_justification":"Requiring holding the butter before "</p>
<p>"placing it in/on the pot is physically "</p>
<p>"sensible, and all observations" { "idx": are0 consistent, " with theevaluation":"yes world", "short_justification":" stateIt." is } physically consistent to require holding the butter befor e placing it in or on the pot, and all observations align with a realistic state.</p>
<p>"}{" idx":0," evaluation":"yes","</p>
<p>short_justification":"</p>
<p>The engine sensibly requires holding the butter before placing it and returns no errors or physically impossible outcomes.</p>
<p>"}{" idx":0," evaluation":"yes","</p>
<p>short_justification":"</p>
<p>All responses are consistent with real-world constraints (you must handle/hold the butter to put it in/on the pot), and no impossible actions or errors occurred.</p>
<p>"}{" idx":0," evaluation":"</p>
<p>In [ ]:</p>
<p>Figure 6: GPT-5-Alignment-response-bug.</p>
<p>'''</p>
<p>A set of 'key': values is an object; all objects must be placed in generating multiple independent objects is prohibited.</p>
<p>output ONLY one JSON array:</p>
<p>{ "idx":0, "evaluation":"yes|no", "short_justification":"one sentence" } '''</p>
<p>In [ ]:</p>
<p>D GPT-5 Experiment Results and Discussion</p>
<p>Similarly, we keep GPT-5 constant to evaluate all generated games in ByteSized32Refactored and original ByteSized32.While we were able to obtain valid game generations from GPT-5, we encountered challenges during the evaluation and reflection process, as GPT-5 was unable to execute the provided evaluation and reflection code and prompt successfully.It is worth that rather than indicating a weakness of the RefactoredByteSized32 corpus, the incompatibility reflects GPT-5's different API constraints (e.g., output formatting, limited batch generation, timeout issues) and underscores the need for more robust, model-agnostic evaluation pipelines, the failure and experiment process for GPT-5 which highlights the practical limitations of our current evaluation methods when applied to LLMs and to supr futher development at the juncture of world modeling and code generation.Challenge 1:Output Format GPT-5 often deviated from the expected output format required by the evaluation scripts, leading to parsing errors and failed executions during the Physical Reality Alignment evaluation metrics.Despite prompt adjustments shown in new add Prompt 7, GPT-5 failed to produce outputs in the expected fixed JSON format with syntax errors and logical inconsistencies resulting in Figure 6.The error include JSON Structure Errors, such as missing or mismatched brackets (e.g., a "" without a corresponding ""), rendering the output unparsable; Syntax Irregularities, such as concatenated words (e.g., "theevaluation") without necessary spaces, causing readability and parsing issues; and Structural Disarray, where outputs exhibited syntactic and semantic incoherence, with poor sentence transitions, broken logical relationships, and contradictory semantics, resulting in outputs that lacked readability and consistency.Challenge 2: Timeouts and API Maximum Limitations.While we successfully used GPT-5 to generate all games, the reflection stage proved problematic.For several games (e.g., [specific games]), the multi-iteration reflection process exceeded the preset 30-minute timeout, and GPT-5's higher latency further disrupted the iterative improvement cycle.To proceed with later stages, these games had to be removed from the directory to continue evaluation and reflection.In addition, GPT-5's API enforced a strict maximum call limit of n ≤ 8, which prevented us from executing the compliancemajority-vote procedure that requires 31 generations.</p>
<p>E Discussion-LLMs Difference: Do</p>
<p>Newer LLMs Surpass GPT-4 in World Modeling?</p>
<p>In addition to examining the impact of code structure through our Refactored vs. Original comparison, we also investigated a complementary question: do newer LLMs surpass GPT-4 in world modeling when evaluated on the same original ByteSized32 corpus?While our main focus is on the structural effects introduced by Byte-Sized32Refactored, this second line of analysis provides insight into model-level differences independent of corpus design.For clarity, we present the results of this comparison in a separate subsection (and provide extended details in the Appendix).</p>
<p>E.1 GPT-4o vs GPT-4 on original Bytesized32</p>
<p>We keep the original Bytesized32 in constant and evaluate all generated games (N = 96) in GPT-4o and GPT-4, reporting results both before and after self-reflection to investigate whether the newer GPT-4o surpasses GPT-4 in building world models without Bytesized32Refactored.</p>
<p>The results of the technical validity are summarized in Table 4 and Table 7.</p>
<p>Table 7: Technical validity of GPT-4-32k on Byte-Sized32Refactored across reflection steps (0-3).On technical validity, GPT-4o consistently outperforms GPT-4 across all three metrics on the original ByteSized32 corpus.On game initialization, GPT-4o achieves 91.67% valid implementations before reflection, increasing to 95.83% after three reflections.In contrast, GPT-4 starts lower at 85.4% and rises to 89.6%, showing a +4.2% improvement but still trailing GPT-4o by a significant margin of 4.16%.On valid actions, GPT-4o begins at 75.00% and increases to 90.62%, while GPT-4 starts at a lower 80.2% and improves to 88.5%, resulting in a substantial gap of 8.3% after reflection.Finally, on runnable games, GPT-4o starts at 48.96% and rises to 82.29%, whereas GPT-4 begins at a much lower 28.1% and only reaches 57.3%, leaving a gap of 29.2% after reflection.</p>
<p>Similarly to the results of the physical reality alignment,specification compliance and winnability of games are summarized in Table 6, Table 8,  to 14.8% (+5.3%), while GPT-4 rises only from 30.25% to 31.97% (+1.72%).Score distributions show GPT-4o produces more mid-to high-range results, whereas GPT-4 clusters at lower scores.This indicates GPT-4o benefits more from reflection and achieves a more balanced alignment.</p>
<p>These results indicate that GPT-4o has a stronger initial capability and benefits more from self-reflection compared to GPT-4.GPT-4-32K achieves a higher initial alignment score (0.3025 vs. 0.095) but gains little from reflection (+0.0172), whereas GPT-4o starts much lower but improves more substantially (+0.053).This indicates that GPT-4-32K's generations are already physically grounded, yet its reasoning benefits less from iterative correction, while GPT-4o, despite its weaker initial grounding, shows stronger reflection-driven reasoning.</p>
<p>F RefactoredBYTESIZED32 PYTHON TEMPLATE in GPT-4o Here is a specification of the task that your code should simulate.# Task: Create a micro−simulation that models how to heat milk to a temperature that is suitable for a baby using a stove.</p>
<p>GPT Physical Reality Alignment Prompt</p>
<p>in the playthrough of the text game below, I would like you to describe whether the game engine (i.e. the observations it returns in response to actions) are physically accurate models of the world or whether they don't make sense.</p>
<p>An example of not making sense would be being able to take an action from a container (like a fridge) without having opened it first.In addition, if an action produces an error from the game, then it automatically fails to accurately model the world and does not make sense.</p>
<p>Please restrict your evaluation only to the short playthrough, and the specific actions chosen, without speculating about other Actions.</p>
<p>Note: Objects can be manipulated by the agent without first being explicitly picked up, as long as they are in the environment, and readily accessible (e.g.not in a closed container).</p>
<p>The evaluation should be binary ("yes" or "no"), except in the cases where the code generated an error, when the evaluation should be "error".</p>
<p>Here is an example output format: {"evaluation":"no", "short_justification": "could take an object (banana) from the closed fridge without having to first open the fridge"} Game Task: {GAME_TASK} Here is the playthrough to evaluate: {PATH}</p>
<p>GPT Reflection Prompt</p>
<p>You are DeveloperGPT, the most advanced AI developer tool on the planet.You answer any coding question, and provide real useful example code using code blocks.Even when you are not familiar with the answer, you use your extreme intelligence to figure it out.Your task is to correct a program that is a text−based simulation.Here is the code of the simulation <code>{ GAME_CODE}</code>H ere is the error message from a Python interpreter.{ERROR_MESSAGE} You should respond all the code with your fix.Do not respond anything else.</p>
<p>GPT Winnability Prompt</p>
<p>You are playing a text−based games.Interact with the environment to solve a task.</p>
<p>Here is an example.{IN_CONTEXT_EXAMPLE}</p>
<p>The game you are about to play only understands one command at a time from the following list of commands: {POSSIBLE_ACTIONS} Prepend your thoughts with 'think:' when planning your next steps.When you think the task is completed, say 'done'.If you believe the task truly cannot be completed, or if you are repeating failed actions with no progress, or if all actions only result in errors, then say 'bug'.</p>
<p>The IN_CONTEXT_EXAMPLE used in the GPT Winnability Prompt</p>
<p>Task Description: Your task is to light the LED by building a circuit.Initial Observation: You find yourself in a workshop.In the workshop, you see: yourself a LED which is off a red wire a black wire a blue wire a battery Type 'help' for a list of possible actions.</p>
<blockquote>
<p>think: To light the LED, I need to create a complete circuit.This will involve connecting the LED to the battery using the wires.I will need to connect one terminal of the LED to the positive terminal (anode) of the battery and the other terminal of the LED to the negative terminal (cathode) of the battery.I will use the red wire for the positive connection and the black wire for the negative connection.The blue wire may not be necessary, but I will keep it in mind in case it is needed.I will start by examining the items in the workshop.&gt; look around Observation: You find yourself in a workshop.In the workshop, you see: yourself a LED which is off a red wire a black wire a blue wire a battery &gt; think: I need to gather the necessary components to build the circuit.I will start by taking the LED, the red wire, the black wire, and the battery into my inventory.</p>
<p>take LED</p>
</blockquote>
<p>The LED is removed from the workshop.You put the LED in your inventory.</p>
<blockquote>
<p>think: I will now take the battery into my inventory, as it is the final necessary component for building the circuit.</p>
<p>take battery</p>
</blockquote>
<p>The battery is removed from the workshop.You put the battery in your inventory.</p>
<blockquote>
<p>think: I now have all the necessary components in my inventory: the LED, the red wire, the black wire, and the battery.I will proceed to build the circuit by connecting the components.First, I will connect one terminal of the LED to the red wire, which will later connect to the positive terminal (anode) of the battery.&gt; connect LED terminal1 to red wire terminal1 Successfully connected LED terminal1 to red wire terminal1 &gt; think: I have successfully connected one terminal of the LED to the red wire.Next, I will connect the other terminal of the LED to the black wire, which will later connect to the negative terminal (cathode) of the battery.&gt; connect LED terminal2 to black wire terminal1 Successfully connected LED terminal2 to black wire terminal1 &gt; think: I have successfully connected the LED to both the red wire and the black wire.Now, I need to complete the circuit by connecting the red wire to the positive terminal (anode) of the battery and the black wire to the negative terminal (cathode) of the battery.I will start by connecting the red wire to the battery's anode.&gt; connect red wire terminal2 to battery anode Successfully connected red wire terminal2 to battery anode &gt; think: I have successfully connected the red wire to the battery's anode.Now, I need to complete the circuit by connecting the black wire to the battery's cathode.Once this connection is made, the circuit should be complete, and the LED should light up.&gt; connect black wire terminal2 to battery cathode Successfully connected black wire terminal2 to battery cathode Current step: 7 Score: 1 Reward: 1 Game Over: True Game Won: True</p>
</blockquote>
<p>Figure 2 :
2
Figure 2: Layered architecture of the GameBasic library.Lower layers provide unified abstractions and reusable modules, while the Derived Layer defines the world and the agent.At the top, TextGame orchestrates these entities to execute the game.Arrows denote inheritance or dependency, and specifically a control relation from TextGame to World and Agent.</p>
<p>Figure 3 :Figure 4 :
34
Figure 3: Histogram of physical reality alignment scores for GPT-4o on ByteSized32Refactoredbefore(red) and after reflection(blue).Asterisk represent average scores(0.055pre-reflection,0.147post-reflection)</p>
<p>Figure 5 :
5
Figure 5: An example of Playthrough.</p>
<p>Figure</p>
<p>Figure 7: GPT5Alignmentpromptincrease.</p>
<p>Figure 8: Histogram of physical reality alignment scores before and after reflection.</p>
<h1>Environment: kitchen # Task−critical Objects: Stove, Pot, Milk, Fridge, Thermometer # High−level object classes: Device (Stove, Fridge), Container ( Stove, Pot, Fridge) # Critical properties: temperature (Milk), temperature_increase_per_tick (Stove), temperature_decrease_per_tick (fridge), max_temperature (Stove) , min_temperature (fridge) # Actions: look, inventory, examine, take/put object, open/close container, turn on/off device, use thermometer on object, feed baby with milk # Distractor Items: None # Distractor Actions: drink milk # High−level solution procedure: open fridge, take pot containing milk, put the pot on the stove, turn on the stove, use the thermometer to moniter the milk temperature till the temperature is suitable for a baby to drink, feed baby ```GPT Specification Compliance PromptYou are DeveloperGPT, the most advanced AI developer tool on the planet.You answer any coding question, and provide real useful example code using code blocks.Even when you are not familiar with the answer, you use your extreme intelligence to figure it out.Your task is to evaluate a program that is a text−based Simulation.Here is a specification of the simulation: {GAME_SPEC} Here is the GameBasic library used by the simulation: {GameBasic_CODE} Here is the code of the simulation: {GAME_CODE} Answer the following question based on the given specification and the simulation code: {EVAL_QUESTION} Answer 'Yes' or 'No' first and briefly explain your answer.</h1>
<p>Layer Derived Layer Extension Layer Core Abstraction Layer TextGame</p>
<p>• Actions:The valid operations available to the agent, such as opening/closing containers.
Control Initialize world actions Parse / execute step()Update state &amp; scoreWorldAgentRoot of object treePlayer &amp; mventoryDescribe room contentsReferent = 'yourself'DeviceContainerSubstanceIs on propertyOpen / ClosePhase change by temperatureTurn on() / Turn off()Place / Take objectsTick updates stateGameObjectUnified properties (aict)Core ops:add / remove,tickInterface:makeDescriptionStr()</p>
<p>Table 1 :
1
Comparison of BYTESIZED32 and Refactored Game Statistics (per game)
StatisticBYTESIZED32 BYTESIZED32Refactored∆Lines of Python code618.1303.19-50.9Lines of comments198.1198.10Tokens per Game67922896-3896Action verbs9.89.80Valid actions306.6306.60Object classes5.45.6+0.2Object instances7.46.5−0.9Expert path length12.812.803.2.2 Code Modularity OptimizationIn addition to the development of GameBasic.py,our refactoring emphasized structured logic andloop mechanisms, leveraging higher-level functionencapsulation and abstraction to replace repetitiveimplementations, underwent further code optimiza-tion like object initialization and action handling:
• Action Parsing with action_map: In the original ByteSized32, the step() function relied on lengthy if/elif chains to dispatch actions.After the refactored code replaces this with an action_map dictionary, significantly improving extensibility and simplicity by mapping actions directly to their corresponding handlers.•Efficient String Construction: In the original ByteSized32, makeDescriptionStr() relied on verbose string concatenation, whereas the refactored version uses join() and conditional expressions for cleaner and more efficient code.</p>
<p>Table 2 :
2
Number of examples that can fit in the context window of various LLMs.Compared with Bytesized32, Bytesized32Refractored can fit more examples into the prompt as demonstrations under various context limits.
GPT-4o Qwen3-32B Llama3.2-1b-QContext Length128k32k8kBytesized321841Bytesized32Refactored4492</p>
<p>Table 3 :
3
Technical validity of GPT-4o on Byte-Sized32Refactored across reflection steps (0-3)
Technical Validity MeasurementNumber of Reflections0123Game Initialization56.25% 81.25% 85.42% 85.42%Vaild Actions55.21% 62.50% 69.79% 70.83%Runnable Game17.71% 39.58% 55.21% 61.46%</p>
<p>Table 4 :
4
Technical validity of GPT-4o on Bytesized32 across reflection steps (0-3)
Technical Validity MeasurementNumber of Reflections0123Game Initialization91.67% 94.79% 95.83% 95.83%Vaild Actions75.00% 90.62% 90.62% 90.62%Runnable Game48.96% 75.00% 80.21% 82.29%</p>
<p>Table 5 :
5
Specification compliance and winnability for GPT-4o on ByteSized32Refactored before/after reflection.
MeasurementReflection∆Before AfterSpecification ComplianceTask-critical objects100.0% 100.0% 0.0%Task-critical actions93.75% 93.75% 0.0%Distractors31.25% 28.12% -3.13%Winnability33.3%54% +20.7%</p>
<p>Table 6 :
6
Specification compliance and winnability for GPT-4o on ByteSized32 before/after reflection.
MeasurementReflection∆Before AfterSpecification ComplianceTask-critical objects100.0% 100.0% 0.0%Task-critical actions90.62% 90.62% 0.0%Distractors56.25% 53.12% -3.13%Winnability20%34%+14%</p>
<p>Table 8 :
8
Specification compliance and winnability for GPT-4-32k on Original ByteSized32 before/after reflection.
MeasurementReflection∆Before AfterSpecification ComplianceSpecification ComplianceTask-critical objects100.0% 100.0% 0.0%Task-critical actions93.8% 93.8%0.0%Distractors21.9% 18.8% -3.1%Winnability30.2% 37.5% +7.3%</p>
<p>Agent, and a base implementation of TextGame.In your game code, you must derive new classes from these basic classes to build your game logic.Your code must contain a class named TextGame.The TextGame class should have the following member functions: Your code must contain a derived class of TextGame that has the following member functions: <strong>init</strong>(self, randomSeed), getTaskDescription(self), generatePossibleActions(self), step(self, actionStr), calculateScore(self).
GPT Game Generation PromptYou are DeveloperGPT, the most advanced AI developer tool on the planet.You answer any coding question, and provide real useful example codeusing code blocks. Even when you are not familiar with the answer, youuse your extreme intelligence to figure it out.Your task is to write a program that: is a text−based simulation. Theprogram should be written in Python. It should be challenging to the user,testing their common−sense knowledge, and take multiple steps tocomplete. If possible, there should be distractor objects and actions that donot help progress, to measure whether the user really knows what they'redoing. You should name all target objects and distractor objects withcommon−sense names.<code>`python GameBasic.py\n"{GameBasic_CODE}</code>GameBasic.py includes the classes GameObject, Container, Device,Substance, World,
ByteSized32Refactored:https://github.com/ isle-dev/BYTESIZED32-Refactored
ByteSized32:https://github.com/cognitiveailab/ BYTESIZED32</p>
<p>Graph constrained reinforcement learning for natural language action spaces. Prithviraj Ammanabrolu, Matthew Hausknecht, arXiv:2001.088372020arXiv preprint</p>
<p>Playing text-adventure games with graph-based deep reinforcement learning. Prithviraj Ammanabrolu, Mark O Riedl, arXiv:1812.016282018arXiv preprint</p>
<p>Qing Sun, Mingyue Shang, and 1 others. 2022. Multi-lingual evaluation of code generation models. Ben Athiwaratkun, Krishna Sanjay, Zijian Gouda, Xiaopeng Wang, Yuchen Li, Ming Tian, Tan, Uddin Wasi, Shiqi Ahmad, Wang, arXiv:2210.14868arXiv preprint</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, arXiv:2108.07732Quoc Le, and 1 others. 2021. Program synthesis with large language models. arXiv preprint</p>
<p>Perceptual symbol systems. Lawrence W Barsalou, Behavioral and brain sciences. 2241999</p>
<p>Starling: Self-supervised training of text-based reinforcement learning agent with large language models. Shreyas Basavatia, Keerthiram Murugesan, Shivam Ratnakar, arXiv:2406.058722024arXiv preprint</p>
<p>Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn , Jane Anderson, arXiv:2208.08227Molly Q Feldman, and 1 others. 2022. Multiple: A scalable and extensible approach to benchmarking neural code generation. arXiv preprint</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De, Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, arXiv:2107.03374Greg Brockman, and 1 others. 2021. Evaluating large language models trained on code. arXiv preprint</p>
<p>Textworld: A learning environment for text-based games. Marc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, and 1 others. Springer2018Workshop on Computer Games</p>
<p>Christopher Cui, Xiangyu Peng, Mark Riedl, arXiv:2308.01872Thespian: Multi-character text role-playing game agents. 2023arXiv preprint</p>
<p>Christopher Zhang Cui, Xingdi Yuan, Ziang Xiao, Prithviraj Ammanabrolu, Marc-Alexandre Côté, arXiv:2504.14128Tales: Text adventure learning environment suite. 2025arXiv preprint</p>
<p>A survey on code generation with llm-based agents. Yihong Dong, Xue Jiang, Jiaru Qian, Tian Wang, Kechi Zhang, Zhi Jin, Ge Li, arXiv:2508.000832025Preprint</p>
<p>Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng, Yiling Lou, arXiv:2308.01861Classeval: A manually-crafted benchmark for evaluating llms on class-level code generation. 2023arXiv preprint</p>
<p>Defining intelligence: Bridging the gap between human and artificial perspectives. E Gilles, Eva T Gignac, Szodorai, Intelligence. 1041018322024</p>
<p>Language guided exploration for rl agents in text environments. Hitesh Golchha, Sahil Yerawar, Dhruvesh Patel, Soham Dan, Keerthiram Murugesan, arXiv:2403.031412024arXiv preprint</p>
<p>Interactive fiction games: A colossal adventure. Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, Xingdi Yuan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Interactive fiction games: A colossal adventure. Matthew J Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, Xingdi Yuan, AAAI Conference on Artificial Intelligence. 2019</p>
<p>Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, arXiv:2105.09938Dawn Song, and 1 others. 2021. Measuring coding challenge competence with apps. arXiv preprint</p>
<p>Large language models for code generation: A comprehensive survey of challenges, techniques, evaluation, and applications. Nam Huynh, Beiyu Lin, arXiv:2503.012452025Preprint</p>
<p>Livecodebench: Holistic and contamination free evaluation of large language models for code. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, arXiv:2403.079742024arXiv preprintArmando Solar-Lezama, Koushik Sen, and Ion Stoica</p>
<p>Peter A Jansen, arXiv:2107.04132A systematic survey of text worlds as embodied natural language environments. 2021arXiv preprint</p>
<p>Textworldexpress: Simulating text games at one million steps per second. Alexander Peter, Marc-Alexandre Jansen, Côté, ArXiv, abs/2208.011742022</p>
<p>Kexin Pei, Ofir Press, and Karthik Narasimhan. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, arXiv:2310.06770Swe-bench: Can language models resolve real-world github issues? Preprint. 2024</p>
<p>Planning and acting in partially observable stochastic domains. Leslie Pack, Kaelbling Michael L Littman, Anthony R Cassandra, Artificial intelligence. 1011-21998</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Belinda Z Li, Maxwell Nye, Jacob Andreas, arXiv:2106.00737Implicit representations of meaning in neural language models. 2021arXiv preprint</p>
<p>Agustin Dal Lago, and 1 others. 2022. Competition-level code generation with alphacode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Science. 3786624</p>
<p>Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, Lingming Zhang, arXiv:2305.012102023aPreprint</p>
<p>Repobench: Benchmarking repositorylevel code auto-completion systems. Tianyang Liu, Canwen Xu, Julian Mcauley, arXiv:2306.030912023barXiv preprint</p>
<p>Octopack: Instruction tuning code large language models. Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro Von Werra, Shayne Longpre, NeurIPS 2023 workshop on instruction tuning and instruction following. 2023</p>
<p>Efficient text-based reinforcement learning by jointly leveraging state and commonsense graph representations. Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Kartik Talamadupula, Mrinmaya Sachan, Murray Campbell, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics20212</p>
<p>Karthik Narasimhan, Tejas Kulkarni, Regina Barzilay, arXiv:1506.08941Language understanding for text-based games using deep reinforcement learning. 2015arXiv preprint</p>
<p>Natural language, semantic analysis, and interactive fiction. Graham Nelson, IF Theory Reader. 141991042006</p>
<p>Measuring the impact of programming language distribution. Gabriel Orlanski, Kefan Xiao, Xavier Garcia, Jeffrey Hui, Joshua Howland, Jonathan Malmaud, Jacob Austin, Rishabh Singh, Michele Catasta, International Conference on Machine Learning. PMLR2023</p>
<p>Story shaping: Teaching agents human-like behavior with stories. Xiangyu Peng, Christopher Cui, Wei Zhou, Renee Jia, Mark Riedl, Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment. the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment202319</p>
<p>A minimal approach for natural language action space in textbased games. Kelvin Dongwon, Meng Ryu, Shirui Fang, Gholamreza Pan, Ehsan Haffari, Shareghi, arXiv:2305.040822023arXiv preprint</p>
<p>Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, arXiv:2010.03768Alfworld: Aligning text and embodied environments for interactive learning. 2020arXiv preprint</p>
<p>Design and optimization of reinforcement learning-based agents in text-based games. Haonan Wang, Mingjia Zhao, Junfeng Sun, Wei Liu, arXiv:2509.034792025arXiv preprint</p>
<p>Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu, arXiv:2203.07540Scienceworld: Is your agent smarter than a 5th grader? Preprint. 2022</p>
<p>Ruoyao Wang, Graham Todd, Ziang Xiao, Xingdi Yuan, Marc-Alexandre Côté, Peter Clark, Peter Jansen, arXiv:2406.06485Can language models serve as text-based world simulators?. 2024arXiv preprint</p>
<p>Byte-sized32: A corpus and challenge task for generating task-specific world models expressed as text games. Ruoyao Wang, Graham Todd, Eric Yuan, Ziang Xiao, Marc-Alexandre Côté, Peter Jansen, arXiv:2305.148792023Preprint</p>
<p>Intercode: Standardizing and benchmarking interactive coding with execution feedback. John Yang, Akshara Prabhakar, Karthik Narasimhan, Shunyu Yao, arXiv:2306.148982023Preprint</p>
<p>Natural language to code generation in interactive data science notebooks. Pengcheng Yin, Wen-Ding Li, Kefan Xiao, Abhishek Rao, Yeming Wen, Kensen Shi, Joshua Howland, Paige Bailey, Michele Catasta, Henryk Michalewski, Alex Polozov, Charles Sutton, arXiv:2212.092482022Preprint</p>
<p>Wenjing Yin, Tianze Sun, Yijiong Yu, Jiawei Fang, Guangyao Su, Jiancheng Wang, Zekun Wang, Wei Wang, Ran Chen, arXiv:2504.20673Ziyun Dai, and 1 others. 2025. Coco-bench: A comprehensive code benchmark for multi-task large language model evaluation. arXiv preprint</p>
<p>Codereval: A benchmark of pragmatic code generation with generative pre-trained models. Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Qianxiang Wang, Tao Xie, Proceedings of the 46th IEEE/ACM International Conference on Software Engineering. the 46th IEEE/ACM International Conference on Software Engineering2024</p>
<p>Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, arXiv:1809.08887Shanelle Roman, and 1 others. 2018arXiv preprint</p>
<p>Xingdi Yuan, Marc-Alexandre Côté, Alessandro Sordoni, Romain Laroche, Remi Tachet Des Combes, Matthew Hausknecht, Adam Trischler, arXiv:1806.11525Counting to explore and generalize in text-based games. 2018arXiv preprint</p>
<p>Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Yongji Wang, Jian-Guang Lou, arXiv:2212.09420Large language models meet nl2code: A survey. 2022arXiv preprint</p>
<p>Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur, Bani Yusuf, Haolan Zhan, arXiv:2406.15877Junda He, Indraneil Paul, and 1 others. 2024. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
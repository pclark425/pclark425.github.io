<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7437 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7437</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7437</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-267750721</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.11493v2.pdf" target="_blank">Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation</a></p>
                <p><strong>Paper Abstract:</strong> In recent years, substantial advancements have been made in the development of large language models, achieving remarkable performance across diverse tasks. To evaluate the knowledge ability of language models, previous studies have proposed lots of benchmarks based on question-answering pairs. We argue that it is not reliable and comprehensive to evaluate language models with a fixed question or limited paraphrases as the query, since language models are sensitive to prompt. Therefore, we introduce a novel concept named knowledge boundary to encompass both prompt-agnostic and prompt-sensitive knowledge within language models. Knowledge boundary avoids prompt sensitivity in language model evaluations, rendering them more dependable and robust. To explore the knowledge boundary for a given model, we propose projected gradient descent method with semantic constraints, a new algorithm designed to identify the optimal prompt for each piece of knowledge. Experiments demonstrate a superior performance of our algorithm in computing the knowledge boundary compared to existing methods. Furthermore, we evaluate the ability of multiple language models in several domains with knowledge boundary.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7437.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7437.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot vs P-zero (single fixed prompt vs any paraphrase)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot single-prompt evaluation versus P-zero (paraphrase-aggregated) evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper reports that evaluating LLMs with a single fixed question (zero-shot) is unreliable because model scores vary across paraphrases; aggregating over paraphrases (P-zero: treat knowledge as known if any paraphrase elicits the correct answer) produces different rankings and coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2, GPT-J, LLaMA2, Vicuna</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer LMs used in experiments: GPT-2 (pretrained decoder-only transformer), GPT-J (6B autoregressive transformer), LLaMA2 (7B open foundation model), Vicuna (7B chat fine-tuned from LLaMA).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>774M, 6B, 7B, 7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>PARAREL, KAssess (common-knowledge QA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Knowledge QA tasks where each knowledge tuple has multiple paraphrased textual prompts; task measures whether the model can generate the correct object given a subject+relation expression.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language question prompts (single fixed paraphrase for 'zero') versus aggregating over multiple paraphrases (P-zero).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>question type / prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot: present a single hand-curated question template; P-zero: consider a knowledge item 'known' if any of its paraphrase templates is answered correctly. No few-shot examples in these comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Coverage / success rate (percentage of knowledge items considered inside model's boundary)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Not reported as a single number; paper reports that results differ substantially across paraphrases and that inter-system rankings change when using single prompts vs paraphrase-aggregated evaluation (qualitative/aggregate claim).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Varies by query and model; the difference between zero and P-zero reflects prompt sensitivity (qualitative statement; no single absolute change reported).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot querying using the dataset templates; P-zero aggregates correctness across available paraphrases per knowledge item.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7437.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7437.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cloze-style vs Choice-style (MMLU)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cloze-style conversion of multiple-choice questions versus original choice-style prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors convert MMLU multiple-choice questions to cloze (fill-in-the-blank) format and report that cloze-style prompts give more reliable and stable assessments but produce much lower absolute scores (~20%), suggesting choice-style benchmarks can overestimate model knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2, GPT-J, LLaMA2, Vicuna, Mistral</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same set of generative transformer LLMs evaluated on domain subsets of MMLU after conversion to cloze format.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>774M, 6B, 7B, 7B, 7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU (converted to cloze-style)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Domain-specialized exam questions (multiple choice originally) converted to cloze: the model must produce the correct answer phrase (fill-in) rather than select among given options.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Cloze / fill-in-the-blank (converted from multiple-choice); single-answer generation (no choices provided).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>question type</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Choice-style questions were stripped to only the text of the correct option as the target answer; cloze format allows free-generation and is said to be more reliable/stable; the paper filtered out computation/reasoning items and used 30 subject areas.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (points) on cloze-style MMLU</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Approximately 20% (paper states 'scores are quite low (around 20 points)' for cloze-style MMLU evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Choice-style MMLU scores (not numerically reported in this paper) — described as higher, i.e., choice-style likely yields inflated scores relative to cloze.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Substantially lower absolute scores when using cloze-style vs choice-style (qualitative; no exact absolute delta provided).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Cloze conversion of MMLU; same PGDC hyperparameters; decoding settings not specifically enumerated in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7437.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7437.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Discrimination format vs Cloze-style</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Judgment/discriminator prompt format (true/false) versus cloze-style fill-in format</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper finds that discrimination (asking the model to judge a statement as true/false) is less reliable: P-dis (paraphrase-aggregated discriminator) produces a large proportion of 'true' answers, reflecting bias, whereas cloze-style prompts are more stable.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2, GPT-J, LLaMA2, Vicuna</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generative transformer models evaluated with a discriminator-style prompt (statement + ask true/false) as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>774M, 6B, 7B, 7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>PARAREL, KAssess, COUNTERFACT, ALCUNA (used as evaluation suites where discriminator baseline is applied)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assess whether a model 'knows' a fact by providing a factual statement and asking the model to judge its correctness (binary), as opposed to eliciting the object via cloze completion.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Discrimination/judgment prompt (true/false) vs cloze (fill-in-the-blank)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / question type</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Discriminator baseline presents a statement and asks the model to judge correctness; P-dis aggregates across paraphrases to see if any paraphrase leads to correct judgment. Observed model preference toward 'true' affects reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Proportion of 'true' judgments / apparent accuracy in discriminator format</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: P-dis yields a similar proportion of responses labeled 'true' across common and unanswerable benchmarks, indicating bias; exact numeric rates are not provided in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Discrimination format appears less reliable (higher false positives) compared to cloze-style prompts (qualitative statement).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Discriminator prompts used as one baseline (dis, P-dis); cloze-style used in PGDC and other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7437.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7437.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot vs Zero-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot in-context learning versus zero-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Few-shot prompting (providing exemplars) generally induces more knowledge and improves retrieval of knowledge compared to pure zero-shot prompting, but designing good few-shot exemplars is difficult and not guaranteed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2, GPT-J, LLaMA2, Vicuna</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer LLMs evaluated under few-shot (4 exemplars used in experiments) and zero-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>774M, 6B, 7B, 7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>PARAREL, KAssess and related knowledge benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Knowledge elicitation where exemplars are retrieved from the dataset and prepended to the prompt (few-shot) versus supplying only the test question (zero-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot in-context exemplars (4 examples) vs zero-shot single prompt</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / in-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Few-shot baseline uses 4 retrieved exemplars per query (the paper's experimental default); P-few aggregates across paraphrases similarly. Authors note manual selection of good examples is hard.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Coverage / success rate (knowledge detected)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: Few-shot > Zero-shot (few-shots induce more knowledge than zero-shots); no single numeric delta is provided in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Improvement with few-shot relative to zero-shot (not quantified here).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Few-shot baseline: 4 exemplars; retrieval from dataset as exemplars; same decoding/model settings as other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7437.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7437.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autoprompt vs PGDC (CFACT counterfactual)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoPrompt (HotFlip-based trigger token method) compared to PGDC (Projected Gradient Descent with Constraints) on the COUNTERFACT (CFACT) counterfactual dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Autoprompt (an adversarial trigger-based optimizer) induces models to output counterfactual target answers at very high rates, whereas PGDC — constrained to preserve semantics — induces far fewer hallucinations on counterfactual data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2, GPT-J, LLaMA2, Vicuna</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer LLMs tested for susceptibility to adversarial trigger prompts (AutoPrompt) versus semantics-preserving prompt optimization (PGDC).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>774M, 6B, 7B, 7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>COUNTERFACT (CFACT) — counterfactual knowledge elicitation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Dataset of counterfactual (false) knowledge records; measuring whether optimized prompts cause models to output the counterfactual target answer (i.e., hallucinate).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Optimized trigger-based prompts (Autoprompt) vs semantics-constrained optimized prompts (PGDC), applied to cloze-style queries</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt optimization method / adversarial vs constrained</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Autoprompt: extend the question with five trigger tokens initialized from last token and updated for three rounds (HotFlip style). PGDC: optimize in embedding space with semantic loss and proximal projection to preserve meaning; tested on CFACT to measure spurious induction of false knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate of constructing prompts that elicit the (counterfactual) target answer (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Autoprompt success rates on CFACT — GPT-2: 92.38% ; GPT-J: 85.67% ; LLaMA2: 88.35% ; Vicuna: 33.09%. PGDC success rates on CFACT — GPT-2: 2.81% ; GPT-J: 4.82% ; LLaMA2: 3.41% ; Vicuna: 3.50%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Autoprompt (adversarial) vs PGDC (semantics-preserving) — numbers above provide direct contrast.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Absolute differences: GPT-2 +89.57 percentage points (Autoprompt vs PGDC); GPT-J +80.85 pp; LLaMA2 +84.94 pp; Vicuna +29.59 pp (Autoprompt higher).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Autoprompt: 5 trigger tokens, 3 update rounds. PGDC: projected gradient descent in embedding space with semantic constraint and δ regularizer; same model decoding as other experiments. Dataset: CFACT (21,919 counterfactual records).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7437.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7437.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PGDC vs Baselines on common knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PGDC (Projected Gradient Descent with Constraints) compared to baselines (zero, few, dis, and paraphrase-aggregated variants) on common-knowledge benchmarks (PARAREL, KAssess)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PGDC — a semantics-preserving prompt optimization algorithm — achieves the highest detection of knowledge boundaries on common-knowledge benchmarks across most evaluated LLMs, indicating that prompt format/presentation (optimized cloze prompts) substantially changes measured model capability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2, GPT-J, LLaMA2, Vicuna</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generative transformer LLMs evaluated with PGDC-optimized cloze prompts versus baseline prompting styles (zero-shot, few-shot, discriminator).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>774M, 6B, 7B, 7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>PARAREL, KAssess (common knowledge benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate whether the model can produce the correct object for a knowledge triple when queried with paraphrased textual prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Cloze-style prompts; PGDC-optimized prompt text (may project to discrete tokens if within projection threshold) versus human-created templates and few/zero-shot baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / prompt optimization</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>PGDC searches for semantically similar optimized paraphrases via embedding-space gradient descent with semantic loss R(X,Q) and projection δ; baselines include zero-shot using one template, few-shot with 4 exemplars, and discriminator judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Coverage / success rate (proportion of knowledge items found within model's boundary)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports PGDC achieves the highest performance on common-knowledge benchmarks for almost all LLMs (exact per-model numeric tables appear in appendix/tables but main-text emphasizes relative superiority).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baselines: zero, few, dis and paraphrase-aggregated variants; PGDC outperforms Pfew, P-dis and typically P-zero.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>PGDC increases detected knowledge boundary size relative to baselines (absolute numbers not summarized as a single delta in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Datasets: PARAREL (328 paraphrases for 38 relations), KAssess (3,488 templates for 600 relations). PGDC hyperparameters as per Appendix B (learning rate 1e-2, iterations 25, λ values, projection threshold c).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7437.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7437.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Semantic preservation of optimized prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human-evaluated semantic preservation rate of PGDC-optimized prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human annotation indicates that PGDC-optimized prompts generally preserve the original semantics: measured semantic-preservation rates ranged from ~80.5% to 86.2% across evaluated models, supporting the claim that format changes found by PGDC do not usually alter meaning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2, GPT-J, LLaMA2, Vicuna</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same generative transformer models; the evaluation is human annotation comparing original question phrasing and PGDC-optimized prompt text.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>774M, 6B, 7B, 7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>PARAREL semantic-preservation human evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Human annotators judge whether the semantics of the PGDC-optimized prompt remain consistent with the original question (binary label).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Comparison of original textual prompt versus PGDC-optimized prompt (both in natural language, possibly not fully fluent).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / semantic-preservation evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>200 random samples from PARAREL; three trained annotators; annotators passed qualification tests; instructions allowed minor nonfluency; Fleiss' Kappa = 0.64.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Semantic preservation rate (%) (human annotation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GPT-2: 80.5% ; GPT-J: 85.1% ; LLaMA2: 83.3% ; Vicuna: 86.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Not applicable (this measures semantic fidelity rather than model accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>200 PARAREL samples; three annotators with CET-6 level English; annotators required ≥90% accuracy on qualification; Fleiss' Kappa = 0.64.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7437.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7437.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model preference and prompt sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-specific prompt preference and sensitivity (qualitative)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Authors note that different LLMs prefer different prompt formulations; e.g., GPT-2 is 'overly sensitive' to prompt variations and thus scores lower under fixed-prompt evaluations, while LLaMA2 and others show different prompt preferences affecting measured performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2, GPT-J, LLaMA2, Vicuna</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Observed differences in how stable each model's outputs are across paraphrases and prompt styles during the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>774M, 6B, 7B, 7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>PARAREL, KAssess, MMLU and other benchmark evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Knowledge elicitation and domain evaluation tasks where prompt formulation influences correctness and measured knowledge boundary.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Various prompt formulations (single-template, paraphrases, cloze, discrimination, few-shot, optimized PGDC prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / model behavior</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Paper discusses qualitative model differences: some models respond better to certain stop-words, formats, or paraphrases; PGDC can automatically discover format/stop-word tweaks that improve elicitation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Model scores / coverage and qualitative rank differences across prompting methods</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: GPT-2 appears highly sensitive and scores lower on fixed-template evaluations; LLaMA2 tends to perform much better in certain domains (e.g., engineering) when prompts are optimized. No single numeric summary provided for this qualitative observation.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Prompt-sensitive ranking changes observed across systems; e.g., P-zero vs zero differences reveal sensitivity (not quantified as a single delta).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Comparative experiments across baselines and PGDC on multiple datasets; inspection of per-model behavior and case studies.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts <em>(Rating: 2)</em></li>
                <li>Statistical knowledge assessment for large language models <em>(Rating: 2)</em></li>
                <li>Measuring and improving consistency in pretrained language models <em>(Rating: 1)</em></li>
                <li>COUNTERFACT <em>(Rating: 2)</em></li>
                <li>PARAREL <em>(Rating: 2)</em></li>
                <li>LAMA: Language models are knowledge bases? <em>(Rating: 1)</em></li>
                <li>Style over substance: Evaluation biases for large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7437",
    "paper_id": "paper-267750721",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "Zero-shot vs P-zero (single fixed prompt vs any paraphrase)",
            "name_full": "Zero-shot single-prompt evaluation versus P-zero (paraphrase-aggregated) evaluation",
            "brief_description": "The paper reports that evaluating LLMs with a single fixed question (zero-shot) is unreliable because model scores vary across paraphrases; aggregating over paraphrases (P-zero: treat knowledge as known if any paraphrase elicits the correct answer) produces different rankings and coverage.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2, GPT-J, LLaMA2, Vicuna",
            "model_description": "Autoregressive transformer LMs used in experiments: GPT-2 (pretrained decoder-only transformer), GPT-J (6B autoregressive transformer), LLaMA2 (7B open foundation model), Vicuna (7B chat fine-tuned from LLaMA).",
            "model_size": "774M, 6B, 7B, 7B",
            "task_name": "PARAREL, KAssess (common-knowledge QA)",
            "task_description": "Knowledge QA tasks where each knowledge tuple has multiple paraphrased textual prompts; task measures whether the model can generate the correct object given a subject+relation expression.",
            "problem_format": "Natural-language question prompts (single fixed paraphrase for 'zero') versus aggregating over multiple paraphrases (P-zero).",
            "format_category": "question type / prompt style",
            "format_details": "Zero-shot: present a single hand-curated question template; P-zero: consider a knowledge item 'known' if any of its paraphrase templates is answered correctly. No few-shot examples in these comparisons.",
            "performance_metric": "Coverage / success rate (percentage of knowledge items considered inside model's boundary)",
            "performance_value": "Not reported as a single number; paper reports that results differ substantially across paraphrases and that inter-system rankings change when using single prompts vs paraphrase-aggregated evaluation (qualitative/aggregate claim).",
            "baseline_performance": null,
            "performance_change": "Varies by query and model; the difference between zero and P-zero reflects prompt sensitivity (qualitative statement; no single absolute change reported).",
            "experimental_setting": "Zero-shot querying using the dataset templates; P-zero aggregates correctness across available paraphrases per knowledge item.",
            "statistical_significance": null,
            "uuid": "e7437.0",
            "source_info": {
                "paper_title": "Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Cloze-style vs Choice-style (MMLU)",
            "name_full": "Cloze-style conversion of multiple-choice questions versus original choice-style prompts",
            "brief_description": "The authors convert MMLU multiple-choice questions to cloze (fill-in-the-blank) format and report that cloze-style prompts give more reliable and stable assessments but produce much lower absolute scores (~20%), suggesting choice-style benchmarks can overestimate model knowledge.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2, GPT-J, LLaMA2, Vicuna, Mistral",
            "model_description": "Same set of generative transformer LLMs evaluated on domain subsets of MMLU after conversion to cloze format.",
            "model_size": "774M, 6B, 7B, 7B, 7B",
            "task_name": "MMLU (converted to cloze-style)",
            "task_description": "Domain-specialized exam questions (multiple choice originally) converted to cloze: the model must produce the correct answer phrase (fill-in) rather than select among given options.",
            "problem_format": "Cloze / fill-in-the-blank (converted from multiple-choice); single-answer generation (no choices provided).",
            "format_category": "question type",
            "format_details": "Choice-style questions were stripped to only the text of the correct option as the target answer; cloze format allows free-generation and is said to be more reliable/stable; the paper filtered out computation/reasoning items and used 30 subject areas.",
            "performance_metric": "Accuracy (points) on cloze-style MMLU",
            "performance_value": "Approximately 20% (paper states 'scores are quite low (around 20 points)' for cloze-style MMLU evaluations).",
            "baseline_performance": "Choice-style MMLU scores (not numerically reported in this paper) — described as higher, i.e., choice-style likely yields inflated scores relative to cloze.",
            "performance_change": "Substantially lower absolute scores when using cloze-style vs choice-style (qualitative; no exact absolute delta provided).",
            "experimental_setting": "Cloze conversion of MMLU; same PGDC hyperparameters; decoding settings not specifically enumerated in main text.",
            "statistical_significance": null,
            "uuid": "e7437.1",
            "source_info": {
                "paper_title": "Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Discrimination format vs Cloze-style",
            "name_full": "Judgment/discriminator prompt format (true/false) versus cloze-style fill-in format",
            "brief_description": "The paper finds that discrimination (asking the model to judge a statement as true/false) is less reliable: P-dis (paraphrase-aggregated discriminator) produces a large proportion of 'true' answers, reflecting bias, whereas cloze-style prompts are more stable.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2, GPT-J, LLaMA2, Vicuna",
            "model_description": "Generative transformer models evaluated with a discriminator-style prompt (statement + ask true/false) as a baseline.",
            "model_size": "774M, 6B, 7B, 7B",
            "task_name": "PARAREL, KAssess, COUNTERFACT, ALCUNA (used as evaluation suites where discriminator baseline is applied)",
            "task_description": "Assess whether a model 'knows' a fact by providing a factual statement and asking the model to judge its correctness (binary), as opposed to eliciting the object via cloze completion.",
            "problem_format": "Discrimination/judgment prompt (true/false) vs cloze (fill-in-the-blank)",
            "format_category": "prompt style / question type",
            "format_details": "Discriminator baseline presents a statement and asks the model to judge correctness; P-dis aggregates across paraphrases to see if any paraphrase leads to correct judgment. Observed model preference toward 'true' affects reliability.",
            "performance_metric": "Proportion of 'true' judgments / apparent accuracy in discriminator format",
            "performance_value": "Qualitative: P-dis yields a similar proportion of responses labeled 'true' across common and unanswerable benchmarks, indicating bias; exact numeric rates are not provided in the main text.",
            "baseline_performance": null,
            "performance_change": "Discrimination format appears less reliable (higher false positives) compared to cloze-style prompts (qualitative statement).",
            "experimental_setting": "Discriminator prompts used as one baseline (dis, P-dis); cloze-style used in PGDC and other baselines.",
            "statistical_significance": null,
            "uuid": "e7437.2",
            "source_info": {
                "paper_title": "Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Few-shot vs Zero-shot prompting",
            "name_full": "Few-shot in-context learning versus zero-shot prompting",
            "brief_description": "Few-shot prompting (providing exemplars) generally induces more knowledge and improves retrieval of knowledge compared to pure zero-shot prompting, but designing good few-shot exemplars is difficult and not guaranteed.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2, GPT-J, LLaMA2, Vicuna",
            "model_description": "Autoregressive transformer LLMs evaluated under few-shot (4 exemplars used in experiments) and zero-shot settings.",
            "model_size": "774M, 6B, 7B, 7B",
            "task_name": "PARAREL, KAssess and related knowledge benchmarks",
            "task_description": "Knowledge elicitation where exemplars are retrieved from the dataset and prepended to the prompt (few-shot) versus supplying only the test question (zero-shot).",
            "problem_format": "Few-shot in-context exemplars (4 examples) vs zero-shot single prompt",
            "format_category": "prompt style / in-context learning",
            "format_details": "Few-shot baseline uses 4 retrieved exemplars per query (the paper's experimental default); P-few aggregates across paraphrases similarly. Authors note manual selection of good examples is hard.",
            "performance_metric": "Coverage / success rate (knowledge detected)",
            "performance_value": "Qualitative: Few-shot &gt; Zero-shot (few-shots induce more knowledge than zero-shots); no single numeric delta is provided in main text.",
            "baseline_performance": null,
            "performance_change": "Improvement with few-shot relative to zero-shot (not quantified here).",
            "experimental_setting": "Few-shot baseline: 4 exemplars; retrieval from dataset as exemplars; same decoding/model settings as other baselines.",
            "statistical_significance": null,
            "uuid": "e7437.3",
            "source_info": {
                "paper_title": "Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Autoprompt vs PGDC (CFACT counterfactual)",
            "name_full": "AutoPrompt (HotFlip-based trigger token method) compared to PGDC (Projected Gradient Descent with Constraints) on the COUNTERFACT (CFACT) counterfactual dataset",
            "brief_description": "Autoprompt (an adversarial trigger-based optimizer) induces models to output counterfactual target answers at very high rates, whereas PGDC — constrained to preserve semantics — induces far fewer hallucinations on counterfactual data.",
            "citation_title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
            "mention_or_use": "use",
            "model_name": "GPT-2, GPT-J, LLaMA2, Vicuna",
            "model_description": "Autoregressive transformer LLMs tested for susceptibility to adversarial trigger prompts (AutoPrompt) versus semantics-preserving prompt optimization (PGDC).",
            "model_size": "774M, 6B, 7B, 7B",
            "task_name": "COUNTERFACT (CFACT) — counterfactual knowledge elicitation",
            "task_description": "Dataset of counterfactual (false) knowledge records; measuring whether optimized prompts cause models to output the counterfactual target answer (i.e., hallucinate).",
            "problem_format": "Optimized trigger-based prompts (Autoprompt) vs semantics-constrained optimized prompts (PGDC), applied to cloze-style queries",
            "format_category": "prompt optimization method / adversarial vs constrained",
            "format_details": "Autoprompt: extend the question with five trigger tokens initialized from last token and updated for three rounds (HotFlip style). PGDC: optimize in embedding space with semantic loss and proximal projection to preserve meaning; tested on CFACT to measure spurious induction of false knowledge.",
            "performance_metric": "Success rate of constructing prompts that elicit the (counterfactual) target answer (percentage)",
            "performance_value": "Autoprompt success rates on CFACT — GPT-2: 92.38% ; GPT-J: 85.67% ; LLaMA2: 88.35% ; Vicuna: 33.09%. PGDC success rates on CFACT — GPT-2: 2.81% ; GPT-J: 4.82% ; LLaMA2: 3.41% ; Vicuna: 3.50%.",
            "baseline_performance": "Autoprompt (adversarial) vs PGDC (semantics-preserving) — numbers above provide direct contrast.",
            "performance_change": "Absolute differences: GPT-2 +89.57 percentage points (Autoprompt vs PGDC); GPT-J +80.85 pp; LLaMA2 +84.94 pp; Vicuna +29.59 pp (Autoprompt higher).",
            "experimental_setting": "Autoprompt: 5 trigger tokens, 3 update rounds. PGDC: projected gradient descent in embedding space with semantic constraint and δ regularizer; same model decoding as other experiments. Dataset: CFACT (21,919 counterfactual records).",
            "statistical_significance": null,
            "uuid": "e7437.4",
            "source_info": {
                "paper_title": "Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "PGDC vs Baselines on common knowledge",
            "name_full": "PGDC (Projected Gradient Descent with Constraints) compared to baselines (zero, few, dis, and paraphrase-aggregated variants) on common-knowledge benchmarks (PARAREL, KAssess)",
            "brief_description": "PGDC — a semantics-preserving prompt optimization algorithm — achieves the highest detection of knowledge boundaries on common-knowledge benchmarks across most evaluated LLMs, indicating that prompt format/presentation (optimized cloze prompts) substantially changes measured model capability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2, GPT-J, LLaMA2, Vicuna",
            "model_description": "Generative transformer LLMs evaluated with PGDC-optimized cloze prompts versus baseline prompting styles (zero-shot, few-shot, discriminator).",
            "model_size": "774M, 6B, 7B, 7B",
            "task_name": "PARAREL, KAssess (common knowledge benchmarks)",
            "task_description": "Evaluate whether the model can produce the correct object for a knowledge triple when queried with paraphrased textual prompts.",
            "problem_format": "Cloze-style prompts; PGDC-optimized prompt text (may project to discrete tokens if within projection threshold) versus human-created templates and few/zero-shot baselines.",
            "format_category": "prompt style / prompt optimization",
            "format_details": "PGDC searches for semantically similar optimized paraphrases via embedding-space gradient descent with semantic loss R(X,Q) and projection δ; baselines include zero-shot using one template, few-shot with 4 exemplars, and discriminator judgments.",
            "performance_metric": "Coverage / success rate (proportion of knowledge items found within model's boundary)",
            "performance_value": "Paper reports PGDC achieves the highest performance on common-knowledge benchmarks for almost all LLMs (exact per-model numeric tables appear in appendix/tables but main-text emphasizes relative superiority).",
            "baseline_performance": "Baselines: zero, few, dis and paraphrase-aggregated variants; PGDC outperforms Pfew, P-dis and typically P-zero.",
            "performance_change": "PGDC increases detected knowledge boundary size relative to baselines (absolute numbers not summarized as a single delta in main text).",
            "experimental_setting": "Datasets: PARAREL (328 paraphrases for 38 relations), KAssess (3,488 templates for 600 relations). PGDC hyperparameters as per Appendix B (learning rate 1e-2, iterations 25, λ values, projection threshold c).",
            "statistical_significance": null,
            "uuid": "e7437.5",
            "source_info": {
                "paper_title": "Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Semantic preservation of optimized prompts",
            "name_full": "Human-evaluated semantic preservation rate of PGDC-optimized prompts",
            "brief_description": "Human annotation indicates that PGDC-optimized prompts generally preserve the original semantics: measured semantic-preservation rates ranged from ~80.5% to 86.2% across evaluated models, supporting the claim that format changes found by PGDC do not usually alter meaning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2, GPT-J, LLaMA2, Vicuna",
            "model_description": "Same generative transformer models; the evaluation is human annotation comparing original question phrasing and PGDC-optimized prompt text.",
            "model_size": "774M, 6B, 7B, 7B",
            "task_name": "PARAREL semantic-preservation human evaluation",
            "task_description": "Human annotators judge whether the semantics of the PGDC-optimized prompt remain consistent with the original question (binary label).",
            "problem_format": "Comparison of original textual prompt versus PGDC-optimized prompt (both in natural language, possibly not fully fluent).",
            "format_category": "prompt style / semantic-preservation evaluation",
            "format_details": "200 random samples from PARAREL; three trained annotators; annotators passed qualification tests; instructions allowed minor nonfluency; Fleiss' Kappa = 0.64.",
            "performance_metric": "Semantic preservation rate (%) (human annotation)",
            "performance_value": "GPT-2: 80.5% ; GPT-J: 85.1% ; LLaMA2: 83.3% ; Vicuna: 86.2%.",
            "baseline_performance": null,
            "performance_change": "Not applicable (this measures semantic fidelity rather than model accuracy).",
            "experimental_setting": "200 PARAREL samples; three annotators with CET-6 level English; annotators required ≥90% accuracy on qualification; Fleiss' Kappa = 0.64.",
            "statistical_significance": null,
            "uuid": "e7437.6",
            "source_info": {
                "paper_title": "Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Model preference and prompt sensitivity",
            "name_full": "Model-specific prompt preference and sensitivity (qualitative)",
            "brief_description": "Authors note that different LLMs prefer different prompt formulations; e.g., GPT-2 is 'overly sensitive' to prompt variations and thus scores lower under fixed-prompt evaluations, while LLaMA2 and others show different prompt preferences affecting measured performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2, GPT-J, LLaMA2, Vicuna",
            "model_description": "Observed differences in how stable each model's outputs are across paraphrases and prompt styles during the experiments.",
            "model_size": "774M, 6B, 7B, 7B",
            "task_name": "PARAREL, KAssess, MMLU and other benchmark evaluations",
            "task_description": "Knowledge elicitation and domain evaluation tasks where prompt formulation influences correctness and measured knowledge boundary.",
            "problem_format": "Various prompt formulations (single-template, paraphrases, cloze, discrimination, few-shot, optimized PGDC prompts).",
            "format_category": "prompt style / model behavior",
            "format_details": "Paper discusses qualitative model differences: some models respond better to certain stop-words, formats, or paraphrases; PGDC can automatically discover format/stop-word tweaks that improve elicitation.",
            "performance_metric": "Model scores / coverage and qualitative rank differences across prompting methods",
            "performance_value": "Qualitative: GPT-2 appears highly sensitive and scores lower on fixed-template evaluations; LLaMA2 tends to perform much better in certain domains (e.g., engineering) when prompts are optimized. No single numeric summary provided for this qualitative observation.",
            "baseline_performance": null,
            "performance_change": "Prompt-sensitive ranking changes observed across systems; e.g., P-zero vs zero differences reveal sensitivity (not quantified as a single delta).",
            "experimental_setting": "Comparative experiments across baselines and PGDC on multiple datasets; inspection of per-model behavior and case studies.",
            "statistical_significance": null,
            "uuid": "e7437.7",
            "source_info": {
                "paper_title": "Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
            "rating": 2,
            "sanitized_title": "autoprompt_eliciting_knowledge_from_language_models_with_automatically_generated_prompts"
        },
        {
            "paper_title": "Statistical knowledge assessment for large language models",
            "rating": 2,
            "sanitized_title": "statistical_knowledge_assessment_for_large_language_models"
        },
        {
            "paper_title": "Measuring and improving consistency in pretrained language models",
            "rating": 1,
            "sanitized_title": "measuring_and_improving_consistency_in_pretrained_language_models"
        },
        {
            "paper_title": "COUNTERFACT",
            "rating": 2,
            "sanitized_title": "counterfact"
        },
        {
            "paper_title": "PARAREL",
            "rating": 2
        },
        {
            "paper_title": "LAMA: Language models are knowledge bases?",
            "rating": 1,
            "sanitized_title": "lama_language_models_are_knowledge_bases"
        },
        {
            "paper_title": "Style over substance: Evaluation biases for large language models",
            "rating": 1,
            "sanitized_title": "style_over_substance_evaluation_biases_for_large_language_models"
        }
    ],
    "cost": 0.01714325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation
29 May 2024</p>
<p>Xunjian Yin xjyin@pku.edu.cn 
Wangxuan Institute of Computer Technology
Peking University</p>
<p>Xu Zhang zhangxu@pku.edu.cn 
Wangxuan Institute of Computer Technology
Peking University</p>
<p>Jie Ruan ruanjie@stu.pku.edu.cn 
Wangxuan Institute of Computer Technology
Peking University</p>
<p>Xiaojun Wan wanxiaojun@pku.edu.cn 
Wangxuan Institute of Computer Technology
Peking University</p>
<p>Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation
29 May 2024496F2E3D2F71106D801075504418BD6BarXiv:2402.11493v2[cs.CL]
In recent years, substantial advancements have been made in the development of large language models, achieving remarkable performance across diverse tasks.To evaluate the knowledge ability of language models, previous studies have proposed lots of benchmarks based on question-answering pairs.We argue that it is not reliable and comprehensive to evaluate language models with a fixed question or limited paraphrases as the query, since language models are sensitive to prompt.Therefore, we introduce a novel concept named knowledge boundary to encompass both prompt-agnostic and promptsensitive knowledge within language models.Knowledge boundary avoids prompt sensitivity in language model evaluations, rendering them more dependable and robust.To explore the knowledge boundary for a given model, we propose a projected gradient descent method with semantic constraints, a new algorithm designed to identify the optimal prompt for each piece of knowledge.Experiments demonstrate a superior performance of our algorithm in computing the knowledge boundary compared to existing methods.Furthermore, we evaluate the ability of multiple language models in several domains with knowledge boundary.</p>
<p>Introduction</p>
<p>Recently, large language models (LLMs) have made significant advancements in a variety of tasks (Brown et al., 2020;Thoppilan et al., 2022;Bubeck et al., 2023).In order to gain deeper insights into the knowledge capabilities of different LLMs to help select appropriate LLM in practice, numerous studies have proposed various benchmarks for LLM evaluation (Guo et al., 2023;Zhong et al., 2023).The majority of previous research on model evaluation constructs a test dataset sourced from standardized examinations, such as college * These authors contributed equally to this work.</p>
<p>Knowledge</p>
<p>Mastery of LLM Unknown Sensitive Agnostic</p>
<p>Baseline Model Evaluation</p>
<p>Real Knowledge Boundary</p>
<p>Figure 1: Illustration of three classes of knowledge based on the model's mastery of knowledge in different textual forms.Existing evaluation methods suffer from sensitivity to input prompt.Therefore, the knowledge ability depicted by these methods is irregularly shaped.We propose to evaluate the knowledge capacity with a knowledge boundary containing both Prompt-Agnostic Knowledge and Prompt-Sensitive Knowledge.</p>
<p>entrance exams and law school admission tests (Hendrycks et al., 2021).Subsequently, the questions are fed to LLMs as prompts, eliciting responses that are then scored for evaluation (Yu et al., 2023;Zhang et al., 2023).</p>
<p>However, each piece of knowledge embodies abstract concept that can be expressed in a nearly infinite number of textual forms (Phenix, 1967).When evaluating a specific piece of knowledge, existing work only evaluated LLMs with one or several textual forms randomly sampled from the semantic space of the knowledge.However, existing LLMs are notorious for being sensitive to prompt, thereby undermining the reliability of such evaluations (Ji et al., 2023;Maharana et al., 2023;Chang and Bergen, 2023;Chen et al., 2023).Consequently, current studies on model evaluation are reasonably considered to be insufficiently robust.</p>
<p>As shown in Figure 1, from the perspective of the model's mastery of the textual form of knowl-edge, knowledge can be divided into three classes: 1) Prompt-Agnostic Knowledge that can be correctly answered for any textual form; 2) Prompt-Sensitive Knowledge that is sensitive to the form of the prompt fed into the model; 3) Unanswerable Knowledge that is unable to be answered by the model, regardless of the prompt employed.The majority of previous research on model evaluation ignored the presence of Prompt-Sensitive Knowledge, resorting to oversimplified binary evaluations, classifying the model's knowledge mastery merely as true or false.Dong et al. (2023) attempts to assess LLM through diverse paraphrases, yet these evaluations remain confined to limited textual forms of knowledge.We give strict definitions of three types of knowledge in Section 2.1.</p>
<p>In this paper, we aim to reduce the contingency when evaluating LLMs.Different from previous paradigms of LLM evaluation, we attempt to explore the Unanswerable Knowledge of the model to be evaluated, thereby illuminating the knowledge boundaries of LLMs.How can we find Unanswerable knowledge for the model?It is obvious that trying all prompts for the knowledge to query the model is too resource-intensive.Therefore, we choose to make efforts to search for the optimal prompt.We formalize optimal prompt searching as a discrete optimization problem: given some question paraphrases, we search for a prompt to maximize the probability of generating the correct answer.We propose the Projected Gradient Descent method with Constraints (PGDC), a new algorithm that updates prompt with gradient descent and implements proximal projection to search discrete prompts.To ensure that the optimized prompt has the same semantics as the original prompt, we introduce semantic loss, which is a measure of the distance between the semantic representations of the optimized prompt and the original prompt.</p>
<p>Experimental results demonstrate that our proposed PGDC can outperform baselines in depicting knowledge boundaries.In addition, results on counterfactual datasets demonstrate that our approach is reasonable and robust.Human evaluation also reveals that our optimized prompts generally have the same semantics as the original questions.Moreover, we delineate models' knowledge boundaries in different domains using PGDC to evaluate LLMs.The size of the model's domain knowledge boundaries is strongly associated with the performance of downstream tasks in the domain.The optimal prompts also have some patterns that can give some inspiration for designing prompts when using corresponding LLMs.</p>
<p>In summary, our contributions are: (1) We propose a new evaluation paradigm for benchmarking knowledge boundaries to compare models' capabilities, which can reduce the randomness in current evaluations.(2) We design PGDC, a projected gradient descent method with constraints, to optimize prompts and obtain knowledge boundaries of LLMs which achieves the best results on four datasets.(3) We evaluate five models using knowledge boundaries and obtain some valuable findings.</p>
<p>Our code and data are released to facilitate future research1 .</p>
<p>Preliminaries</p>
<p>Knowledge Boundary Definition</p>
<p>First, we provide a strict definition of the three types of knowledge in LLMs, Prompt-Agnostic Knowledge, Prompt-Sensitive Knowledge and Unanswerable Knowledge.</p>
<p>Formally, let k denote a piece of knowledge, and let Θ represent an LM.We assess whether Θ "possesses" knowledge of k by calculating P (a i k |q i k , Θ), where q i k ∈ Q k , a i k ∈ A k represents a pair of inputoutput tokens to verify the knowledge k.</p>
<p>For the universe of all conceivable knowledge U , and a threshold ϵ within the range (0.5, 1], the three types of knowledge for model Θ are defined as follows:</p>
<p>• Prompt-Agnostic Knowledge:
K A = {k ∈ U |P (a i k |q i k , Θ) &gt; ϵ, ∀q i k ∈ Q k , ∀a i k ∈ A k } • Unanswerable Knowledge: K U = {k ∈ U |P (a i k |q i k , Θ) &lt; ϵ, ∀q i k ∈ Q k , ∀a i k ∈ A k } • Prompt-Sensitive Knowledge: K S = {k ∈ U |k / ∈ K U ∧ k / ∈ K A }
In short, for a piece of knowledge k, if the model Θ is able to answer the question about k correctly under at least one expression, k is inside the knowledge boundary of Θ.If Θ is unable to answer the question about k correctly under any expressions, k is outside the knowledge boundary of the model.</p>
<p>Knowledge Boundary Requirements</p>
<p>We attempt to calculate the knowledge boundary of LLM by automatically constructing the optimal</p>
<p>Language Model</p>
<p>Input Embedding Generated Text</p>
<p>Language Model</p>
<p>Input Embedding eos token</p>
<p>Hidden Representation</p>
<p>Original Representation</p>
<p>Sim</p>
<p>Semantic Loss</p>
<p>Logits</p>
<p>Max</p>
<p>Target Loss</p>
<p>Regularization</p>
<p>+</p>
<p>Loss Func</p>
<p>Embedding Projection</p>
<p>PGDC Optimization</p>
<p>Unprojectable Region</p>
<p>Semantic Space Prompt in Embedding Form</p>
<p>Model Embedding</p>
<p>Prompt in Text Form</p>
<p>Optimized Prompt</p>
<p>Figure 2: An illustration of our PGDC method, projected gradient descent method with constraints.The left part of the image shows the overall framework of our method: we start from a few labeled prompts, perform gradient descent with the target answer as the optimization goal and try to project the embedding into text form, while ensuring that the whole search process is in the same semantic space of the expression of the target knowledge.The right side of the image shows how our loss function is calculated at each step of gradient descent.</p>
<p>prompt.As various methods for prompt engineering have been proposed to obtain better prompt as query (Dong et al., 2022;Wei et al., 2023), not all of them are suitable for calculating the knowledge boundary of LLM.In this section, we propose four basic requirements for the algorithm applied to the calculation of knowledge boundaries: Universality, Truthfulness, Robustness and Optimality.</p>
<p>Universality When searching for an optimal prompt, the method should work for most current LLMs, regardless of its size and architecture.</p>
<p>Truthfulness The constructed prompt should share the same semantics as the original question, and not be allowed to change subject or relation.</p>
<p>Robustness When searching for the optimal prompt for a piece of knowledge, the effectiveness of the method should be relevant to the knowledge capacity of LLM.In other words, the algorithm should tend not to find appropriate prompt for unanswerable knowledge.</p>
<p>Optimality The algorithm should search for as much prompt-sensitive knowledge in the LLM as possible.</p>
<p>Problem Formulation</p>
<p>In this section, we give a formal problem formulation of searching for the optimal prompt.For a given piece of knowledge, assume we have an LLM that models next-token probability P (x i |x 1 , x 2 , ..., x i−1 ) with an input sequence (x 1 , x 2 , ..., x i−1 ).The piece of knowledge is expressed in various textual forms to construct a QA set of multiple questions and answers.Different questions in the QA set are paraphrases, while answers are aliases.We believe that if the model is able to answer one of the questions correctly, it is possible for the model to "know" this piece of knowledge.Therefore, if the model is able to generate one of the correct answers with prompt semantically similar to one of the questions, we consider the knowledge within its knowledge boundary.</p>
<p>To illustrate the problem, we start from the simpler case with only one question Q = {q 1 , q 2 , ..., q n } and one answer A = {a 1 , a 2 , ..., a m }.Prompt X is initialized with Q and optimized to maximize the probability of generating A while remaining semantically similar to Q.We formalize optimal prompt searching as the problem:
min X Φ(X) = L(X, A) + λR(X, Q),(1)
where L(•) denotes the loss function to penalize unsuccessful generation.R(•) indicates the semantic distance between the optimized prompt and the initial prompt while λ is the penalty factor.</p>
<p>Method</p>
<p>To obtain a better knowledge boundary for LLM, our effort is directed toward identifying the optimal prompt within the semantic space.As illustrated in Figure 2, PGDC optimizes prompt in the neighbor semantic space of the original question.The prompt in text form is first mapped to prompt in embedding form as continuous text embedding.</p>
<p>During PGDC optimization, the text embedding is updated through gradient descent with direction of the loss function Φ(•).After each update, if the text embedding is close to a discrete prompt, it is projected to the discrete prompt through embedding projection.To avoid the text embedding from entering unprojectable region where there are no close discrete prompts to project, we introduce a regularization to force the embedding not to enter these regions.After multiple iterations of the update, we get the final optimized prompt.</p>
<p>PGDC Optimization</p>
<p>In PGDC algorithm, we do not specify the positions of the answer in our LLM-generated output, which relaxes constraints in the model output and leaves space for the LLM to generate reasoning process and do inference.Therefore, we define the target loss of generating a specific answer A with a slicing window method:
L = min j&lt;=t−k i +1 − log P (O j:j+k i = A),(2)
where O = {o 1 , o 2 , ..., o t } denotes the output of the LLM given X as the input.In this way, PGDC automatically searches for the target position in the model output and optimizes the probability of generating answer.When there exist multiple answers in the answer set A * , we optimize the answer with the highest probability to be generated:
L = min A∈A * min j&lt;=t−k i +1 − log P (O j:j+k i = A). (3)
We separately optimize prompts with PGDC if there are multiple paraphrases of questions in the piece of knowledge.</p>
<p>Since PGDC optimizes prompts in the continuous embedding space while text space is discrete, it is hard for methods of automatically searching for prompts to constrain semantic information (Shin et al. (2020); Jones et al. (2023)).To combat the challenge, we introduce a semantic constraint to the loss function, which is defined as:
R(X, Q) = ||h(X) − h(Q)|| 2 ,(4)
where h(•) is the hidden representation of prompt and || • || 2 denotes the L2 distance between two items.As illustrated in Figure 2, the hidden representation is obtained with the last hidden layer output of the LLM given the concatenation of prompt and a <eos> character.</p>
<p>As the optimization process is implemented in the continuous space, it is necessary to project the embedding into discrete tokens.The optimized embedding obtained might enter the unprojectable region shown in Figure 2, which makes the projection hard.Therefore, we add a regularization in the loss function to punish prompt embedding far from discrete tokens:
δ(X) = Σ N i=1 min v∈V ||x i − W v|| 2 ,(5)
where V denotes the vocabulary of LLM and W is the projection from vocab to embedding space.</p>
<p>The lowercase letters such as x represent tokens while x represents its embedding.N in Equation 5denotes the length of the prompt X which varies in different iterations.</p>
<p>In general, the final loss function of PGDC is formulated as:
Φ(X) = L(X, A) + λ 1 R(X, Q) + λ 2 δ(X). (6)</p>
<p>Proximal Projection</p>
<p>Instead of projecting the prompt into text space after the overall optimization (Guo et al., 2021) or conducting projection after each iteration (Cheng et al., 2020), PGDC achieves flexible transformation of embedding space to text space with a threshold of the vector distance.Formally, the transformation can be written as:
xi = W v, min v∈V ||x i − W v|| 2 &lt; c xi , min v∈V ||x i − W v|| 2 &gt;= c,(7)
where c represents the threshold of the L2 distance.</p>
<p>As illustrated in Figure 2, the dashed line shows the proximal projection process.</p>
<p>Algorithm Summary</p>
<p>In general, PGDC iteratively optimizes prompt in the embedding space with gradient descent to minimize the loss function in Equation 5and do proximal projection after each iteration.A detailed pseudocode is shown in Appendix C.</p>
<p>Experiments</p>
<p>In this section, we perform comparisons between our method and baseline methods which are commonly used in model evaluation on common knowledge benchmarks and unanswerable knowledge benchmarks.We also conduct a manual evaluation to check whether the semantics of the prompts we obtained are consistent with the original question.</p>
<p>Dataset</p>
<p>Datasets and Models</p>
<p>Common Knowledge Benchmarks In order to evaluate the performance of different methods on common knowledge, we choose to use KAssess (Dong et al., 2023) and PARAREL (Elazar et al., 2021) for our evaluation.Both of them consist of knowledge tuples and hand-curated prompt templates, where all subjects, relations, and objects exist as entities in WikiData.</p>
<p>Unanswerable Knowledge Benchmarks To test whether our optimized prompts leak answers or induce hallucinations that cause LLMs to answer knowledge that they could not answer originally, we perform evaluations on two counterfactual datasets, COUNTERFACT (denoted as CFACT) (Meng et al., 2022) and ALCUNA (Yin et al., 2023).CFACT contains 20K counterfactual knowledge records with a diverse set of subjects, relations, and linguistic variations.ALCUNA is a biological dataset for evaluating the ability of the model in face of new knowledge.The above datasets have multiple expressions for each knowledge query, except for ALCUNA.</p>
<p>Models Our experiments use GPT-2 (774M) (Radford et al., 2019), GPT-J (6B) (Wang and Ko-matsuzaki, 2021), LLaMA2 (7B) (Touvron et al., 2023b), and Vicuna (7B) (Chiang et al., 2023).</p>
<p>Baseline Methods</p>
<p>There are several common methods of assessing the model's mastery of knowledge, and we use the following as baselines:</p>
<p>Zero-Shot (zero) Zero shot prompting is the simplest and most common approach used in previous evaluation work.We directly query models using questions from benchmarks.</p>
<p>Few-Shots (few) Few shots prompting is commonly used to enhance model performance by utilizing the contextual learning capabilities of LLMs.We retrieve similar knowledge in the dataset as exemplars to feed to the model.</p>
<p>Discriminator (dis)</p>
<p>We can also use the judgment question format to assess whether a model knows one piece of knowledge.So we provide LLM with one knowledge statement and let it determine whether this statement is correct or incorrect.</p>
<p>Since there are several paraphrases for each knowledge query in the benchmarks, for each above baseline, we will use two different metrics to simulate previous work on model evaluation: 1) For each knowledge query, we will randomly select one of its expressions for evaluation.2) For each knowledge query, as long as one of its paraphrases can be answered correctly, the knowledge is considered to be inside the boundaries, and the baseline method using this metric is denoted as P-baseline.</p>
<p>A more detailed description of the dataset as well as the implementation of the baseline methods, are shown in Appendix A. The hyperparameter settings for the PGDC are shown in Appendix B.</p>
<p>Results and Analysis</p>
<p>Table 1 summarizes the experimental results on four different LLMs.Our proposed PGDC achieves the highest performance on common knowledge benchmarks on almost all LLMs.The results indicate that the knowledge boundary found by our method is more comprehensive than baseline methods, which shows the Optimality and Universality of PGDC.The experimental results on unanswerable knowledge benchmarks including CFACT and ALCUNA reflect the Robustness of different prompt methods.PGDC only slightly raises the amount of unanswerable knowledge over zero-shot baseline, which shows that our proposed method will only introduce relatively limited fake knowledge and meets the Robustness requirement.PGDC considers knowledge to be within the boundaries if any of its paraphrases can be answered correctly for each knowledge query and shows comparative results with P-zero while outperforming Pfew and P-dis.Moreover, the prompts generated by PGDC are generally consistent semantically with the original questions (as shown in Section 4.5).Therefore, PGDC meets all four fundamental criteria (Universality, Truthfulness, Robustness, and Optimality) in calculating knowledge boundaries.</p>
<p>In addition, we can observe that:</p>
<p>Evaluating LLMs with a fixed question or limited paraphrases as the query is not reliable and comprehensive According to the zero and P-zero results of the PARAREL and the KAssess dataset, we can see that different queries yield different results, and different prompting methods may result in different inter-system rankings.This suggests that assessing LLMs using a predetermined question or restricted paraphrases as the query lacks reliability and comprehensiveness.Evaluating LLMs with a fixed question or limited paraphrases may lead to the selection of suboptimal LLMs for prac- tical applications, demonstrating the necessity of optimizing prompt design to seek more realistic knowledge boundaries.</p>
<p>Discrimination format is much less reliable than cloze-style format P-dis has a similar proportion of responses as true on common and unanswerable knowledge benchmarks, which correlates with the model's preference for true.This observation aligns with previous findings by Wu and Aji (2023); Wang et al. (2023).</p>
<p>Different models prefer different prompts</p>
<p>Since traditional model evaluation methods use fixed queries, the model's preference for prompt affects the score.The difference between P-zero and zero then reflects the fact that the model is sensitive to prompt.Even for queries with the same meaning, different ways of asking can produce different results.GPT-2 also acquires a fair amount of knowledge, but is overly sensitive and thus scores lower on traditional assessment methods.</p>
<p>Manual design of a good prompt is difficult Few shots prompting induces more knowledge than zero-shots.However, it is difficult to verify how to select good examples and whether a good enough prompt has been designed.PGDC, on the other hand, uses cloze-style problem and automatically searches for the optimal prompt for different models, so it is a much better approach to model evaluation.</p>
<p>We also analyze the knowledge detected by PGDC as well as the knowledge detected by baselines on KAssess.We categorize relations according to KAssess, and analyze the accuracy of PGDC and baseline methods on various relation cate- gories.The results of PGDC and the strong baseline method P-few on the strong LLaMA2 model are shown in Figure 3, while the coverage results of other baseline methods are presented in the Appendix D. We find that the knowledge boundaries we obtained can almost cover baselines.Moreover, we also record the iterations on KAssess to find the optimized prompt using PGDC in Figure 4. We observe that PGDC can find the optimal prompt for the majority of queries within 15 iterations.</p>
<p>Comparison with Prompt Optimization Method</p>
<p>Since our method is a prompt optimization type of method, we conduct experiments to compare the robustness of PGDC and Autoprompt (Shin et al., 2020), a representative method of prompt optimization.Autoprompt is a Hotflip-based algorithm (Ebrahimi et al., 2018) in optimizing prompt, which employs several trigger tokens to elicit the target output.The exact experimental setup is shown in Appendix E. As shown in Table 3, we can find that Autoprompt induces the model to output target answers on counterfactual datasets in a large percentage.This result suggests that Autoprompt is more similar to an adversarial attack algorithm that is committed to getting the target answer, while PGDC optimizes the prompt within the semantic constraint.</p>
<p>Semantic Preservation Evaluation</p>
<p>In order to check whether the prompt obtained by PGDC is semantically consistent with the original questions (Truthfulness), we perform a manual evaluation.We randomly select 200 samples from the PARAREL dataset.Specifically, we enlisted three college students who hold English qualification certificates.Initially, they were given an evaluation guideline, which is detailed in Appendix F. Each evaluator underwent a training process to improve their comprehension of the annotation procedure.Prior to annotation, we administered a qualification test comprising 10 samples; only annotators who passed this test were deemed qualified and permitted to proceed with annotation.</p>
<p>The human evaluation results show that the semantic preservation rates of GPT-2, GPT-J, LLaMA2, and Vicuna are respectively 80.5%, 85.1%, 83.3%, and 86.2%.This indicates that the prompts generated by PGDC are generally semantic consistent with the original questions, which demonstrates the general Truthfulness of PGDC.More details about the human evaluation are shown in Appendix F.</p>
<p>Case Study</p>
<p>To understand how PGDC steers question prompts to generate desired answers, we manually study cases in which PGDC successfully updates the prompt.We summarize the advantages of PGDC into three aspects and provide cases in Table 2: 1) Finding the optimal paraphrase of the original prompt.Due to human resource constraints, it is impossible to enumerate all paraphrases of the original question.PGDC automatically searches for the optimal paraphrase that elicits correct answers.2) Leaving space for LLM to reason and infer.PGDC allows LLMs to generate some tokens to assist their reasoning and inference to achieve the answer.3) Changing the format and stop words in the original prompt.Some special tokens and stop words vary in different LLMs, which can be hard for humans to detect.PGDC is able to optimize format and stop words on the basis of gradient.</p>
<p>Assessments of LLMs</p>
<p>The above experiments have demonstrated the effectiveness of PGDC in detecting knowledge and the reasonableness of the obtained optimal prompt.In this section, we apply PGDC on MMLU (Hendrycks et al., 2021) to evaluate LLMs.</p>
<p>Experimental Settings</p>
<p>We evaluate GPT-2 (774M), GPT-J (6B), LLaMA2 (7B), Vicuna (7B) and Mistral (7B) (Jiang et al., 2023) from the perspective of 30 refined domain knowledge using MMLU2 .</p>
<p>[Original Prompt] The associated item of source code is; [Answer]     To be consistent with our approach, we modify the questions in MMLU from choice questions to a cloze format, which yields more reliable and stable assessment results.Following previous work (Anil et al., 2023;Touvron et al., 2023a), we categorize the questions in MMLU into six types of topics: natural sciences, medical and biological sciences, computer science and logic, social sciences, humanities, and others.More details of the experiment are shown in Appendix G.1.</p>
<p>Results</p>
<p>The results 3 of our model evaluation on each of the broad categories are demonstrated in Figure 5, and more detailed scores are shown in Appendix G.2.</p>
<p>We can find that Mistral has the largest knowledge boundaries overall.LLaMA2 exceeds the other models by a lot in the engineering domain.It may be because LLaMA2 uses a lot more new 3 For display purposes, our radar graph ranges 1-25%.labeled code data for training.Vicuna performs not far behind LLaMA2 on other topics.GPT-2 has very small knowledge boundaries and performs poorly in the more specialized medical domain.By identifying more reliable knowledge boundaries, we help select the appropriate LLM in practice.</p>
<p>However, it is worth noting that these scores are quite low (around 20 points).This is due to the fact that we use more difficult cloze-style questions for reliability.It also reflects the fact that the results obtained from the choice-style benchmark may be too high.The problems in MMLU are relatively specialized, and the current general-purpose models do not have a lot of knowledge in the relevant areas.</p>
<p>Discussion of Randomness in Model Evaluation</p>
<p>The inherent randomness in model evaluation presents significant challenges to the reliability of evaluation results and the process of model selection.This randomness can be seen in several key areas:</p>
<p>Benchmark Question Selection Randomness is inherent in the selection of benchmark questions for testing, as the objective is to choose representative questions that accurately demonstrate the model's abilities in specific areas or domains (Guo et al., 2023;Zhong et al., 2023).</p>
<p>Prompt Expression Formulating the selected questions into prompts introduces additional randomness, including aspects such as the phrasing of questions, system instructions, the order of options, and other details.Numerous studies have demonstrated that models are sensitive to variations in prompts (Ji et al., 2023;Maharana et al., 2023;Chang and Bergen, 2023;Chen et al., 2023).</p>
<p>Decoding Settings</p>
<p>The choice of decoding parameters (e.g., temperature) during evaluation can introduce randomness into the model's output, potentially raising concerns about fairness.</p>
<p>Output Evaluation</p>
<p>The evaluation of the model's generated content also involves randomness.Some studies use exact matching (Yin et al., 2023), while others employ GPT-4 for evaluation (Chiang et al., 2023); however, neither method is flawless.</p>
<p>Given the various sources of randomness in model evaluation, developing reliable, stable, and informative evaluation methods remains a significant challenge and warrants further research.To address this concern, we introduce the concept of the "knowledge boundary," which employs optimized prompts to test the upper limits of a model's capabilities, thereby minimizing the randomness associated with prompt selection.</p>
<p>Related Work</p>
<p>Model Evaluation Several benchmarks have been proposed to evaluate Large Language Models (LLMs) on human exams like college entrance and law school admission tests (Suzgun et al., 2022;Srivastava et al., 2023;Choudhary and Reddy, 2023;Zhong et al., 2023).In terms of knowledge assessment, LAMA (Petroni et al., 2019) evaluates whether models can correctly predict masked object entities in a cloze-style prompt.Some studies (Onoe et al., 2021;Mallen et al., 2023;Arodi et al., 2023;Yu et al., 2023) focus on measuring LLMs' understanding and mastery of world knowledge.These benchmarks do not take into account that LLMs are sensitive to different prompts.Some works (Elazar et al., 2021;Dong et al., 2023) focus on estimating and measuring the consistency of LLMs given diverse prompts.All previous studies on model evaluation use fixed prompts, and our work pioneers prompt optimization for evaluating LLMs' knowledge boundaries.</p>
<p>Prompt Optimization Due to the sensitivity of language models to prompts, better prompts can help achieve higher performance in specific tasks (Deng et al. (2022); Wei et al. (2023); Yang et al. (2023)).Prompt engineering like in-context learning greatly improves the performance of prompt methods (Dong et al., 2022).Another related line of work attempts to formalize prompt searching as a discrete optimization task to achieve better performance in specific tasks (Shin et al., 2020).Some studies adopt Hotflip-based algorithms (Ebrahimi et al., 2018) to automatically construct prompts (Wallace et al. (2019); Shin et al. (2020); Jones et al. (2023);).In addition, several work tries to optimize prompts in continuous embedding space with Gumbel-softmax trick (Guo et al., 2021) and projection (Cheng et al. (2020); Wen et al. (2023)).</p>
<p>Conclusion</p>
<p>The sensitivity of LLMs to prompt leads to the unreliability of the results obtained from traditional model evaluation works that use fixed queries to evaluate the model.To address this problem, we propose semantics-preserving prompt optimization methods, PGDC, to find the knowledge boundaries of models for model evaluation.Our experiments demonstrate shortcomings of previous model evaluation methods and the fact that the prompt we find is superior to the fixed prompt.At the same time, the prompt found by our method maintains the original semantics and does not induce knowledge that is not captured by the model, which outperforms previous prompt optimization efforts.Moreover, we conduct experiments exploring the boundaries of the model's different domain knowledge and compare and analyze the LLM's capabilities.</p>
<p>Limitations</p>
<p>According to our definition, one can say that a model knows this knowledge when it can answer the corresponding question with the optimal prompt.In this paper, we only aim to find the Unanswerable Knowledge of the model as the knowledge boundary.In fact, for Prompt-sensitive knowledge, the model's sensitivity also reflects the model's mastery of it (the knowledge in the color gradient in Figure 1).At this stage we would like to have a clear boundary, so we have not considered this part for now.But exploring this part of knowledge is an interesting and important future work.</p>
<p>Additionally, despite our efforts to ensure the truthfulness of optimized prompts, there remains a small probability that the semantics of the prompt may change, especially for weaker language models such as GPT-2.</p>
<p>Ethics Statement</p>
<p>A potential negative impact of our approach is that malicious attackers could use our method to attack public large pre-trained language models, leading to fake knowledge generation.As pre-trained language models advance in many tasks, addressing safety concerns becomes increasingly necessary and imperative.Analyses in our paper can help enhance the evaluation of pre-trained language models.</p>
<p>A Datasets and Baselines</p>
<p>A.1 Datasets</p>
<p>We conduct comparative experiments between our method and the baseline on four datasets.The datasets are described and detailed below: KAssess KAssess (Dong et al., 2023) is a largescale assessment suite with 994,123 entities, 600 relations, and their text aliases which are obtained from T-REx knowledge graph (Elsahar et al., 2018).KAssess constructs multiple paraphrased templates for each relation.In total, there are 3,488 templates for 600 relations, with an average of 5.82 paraphrased templates per relation.</p>
<p>PARAREL PARAREL (Elazar et al., 2021) is also a manually curated resource that provides patterns-short textual prompts-that are paraphrases of one another, with 328 paraphrases describing 38 binary relations.</p>
<p>COUNTERFACT COUNTERFACT (Meng et al., 2022) is an evaluation dataset for evaluating counterfactual edits in language models which contains 21,919 records with a diverse set of subjects, relations, and linguistic variations.We use its target knowledge as counterfactual knowledge to query LLMs.</p>
<p>ALCUNA ALCUNA (Yin et al., 2023) is used for evaluating the ability of LLMs in the face of new knowledge which consists of a total of 84351 questions about 3554 artificial entities.We only select the cloze-style portion of the questions to be used for the experiments.</p>
<p>A.2 Baselines</p>
<p>We slightly adapted the dataset to the characteristics of the generative model, and examples of the inputs are shown in Table 4.</p>
<p>B Hyperparameter setting for PGDC</p>
<p>Hyperparameter settings are shown in Table 5.</p>
<p>C Pseudocode for our algorithm</p>
<p>We provide pseudocode for ASRA is in Algorithm 1.</p>
<p>D Coverage Analysis</p>
<p>We also examined the knowledge identified by PGDC, as well as the knowledge identified by baseline methods based on KAssess.
L ← Φ(z) 5: p ← p − γ∇ p L 6: for p j ∈ p do 7: t ← arg min k≤|V| ||p j − v k || 2 8: if ||p j − v t || 2 ≤ c then 9: p j ← v t 10: end if 11:
end for 12: end for 13: return p</p>
<p>Human Evaluation Guideline Task Overview</p>
<p>Thank you for participating in this task!We are currently working on a project focused on benchmarking knowledge boundary for Large Language Model (LLM).You will be randomly presented with two texts.Your task is to determine whether the semantics of two texts are consistent, which means judging whether the two texts possess consistent semantics and can both be used to inquire about the same knowledge.Note that the texts do not need to be fluent or grammatically correct, and the presence of non-disruptive gibberish that does not affect the recognition of semantics is allowed.If the semantics of the two texts are consistent, label 1; if not, label 0. Please maintain high quality in your annotations.</p>
<p>Emphasis and Caution</p>
<p>Support and Reference: If you encounter any confusion regarding professional knowledge or context while performing this task, please feel free to reach out to us for clarification.You may also refer to Wikipedia or other reliable sources to gain further understanding.Feedback Mechanism: You can directly submit your queries, concerns, or suggestions to us. of PGDC and the baseline methods P-few, few, Pzero, zero, P-dis and dis on the strong LLaMA2 model are shown in Figure 6.Our analysis reveals that the knowledge boundaries we derived can effectively encompass those of the baseline methods.</p>
<p>E Autoprompt for Model Evaluation</p>
<p>We implement Autoprompt by extending the question with five trigger tokens initialized with the last token in original prompt.The trigger tokens are updated for three rounds according to the algorithm described in Shin et al. (2020).</p>
<p>F Human Evaluation</p>
<p>We provide our human evaluation guideline furnished to participants for manually evaluating the semantic preservation task, as presented in Table 6.</p>
<p>We recruited three college students, all possessing College English Test-6 certificates, demonstrating fluency in English.We first distribute the evaluation guidelines to the evaluators.Subsequently, we conduct training sessions for the evaluators, explaining the evaluation guidelines to ensure a better understanding of the task requirements and addressing any questions or concerns they may have.Before commencing formal annotation tasks, we administered a qualification test.Ten samples were randomly selected.These samples were evaluated by the participants, and subsequently, we assessed  the accuracy of each annotator's evaluations.A higher accuracy score reflects a more consistent understanding of our guidelines.Evaluators who achieved at least 90 % accuracy were deemed qualified to proceed with the evaluation task.We employed Fleiss's Kappa statistic (Fleiss et al., 1981) to assess the agreement between the three annotators, yielding a score of 0.64.</p>
<p>G Model Evaluation on MMLU G.1 Experimental Settings</p>
<p>Based on the conclusion of our earlier experiments that cloze-style questions are more reliable, we converted the choice-style questions in MMLU to a cloze format.We remove the other options and only keep the contents of the correct option as the answer to the cloze question.</p>
<p>Since the topic of our paper is about knowledge and some of the questions in MMLU are about computation and reasoning, we filter them out The remaining 30 subjects are grouped into six larger subjects, as shown in Table 7.</p>
<p>The PGDC method in this experiment uses the same hyperparameters as in Appendix B.</p>
<p>G.2 Detailed Results</p>
<p>In the main article we report the results in the broad categories, and the results in each subcategory are shown in Table 7</p>
<p>Figure 3 :
3
Figure 3: Knowledge boundaries of PGDC and baseline method P-few on KAssess using LLaMA2 model.</p>
<p>Figure 4 :
4
Figure 4: Iterations on KAssess to find the optimized prompt using PGDC with LLaMA2 model.</p>
<p>Figure 5 :
5
Figure 5: Knowledge boundaries of different domains of models on MMLU.</p>
<p>(a) Knowledge boundaries of PGDC and baseline P-few.(b) Knowledge boundaries of PGDC and baseline few.(c) Knowledge boundaries of PGDC and baseline P-zero.(d) Knowledge boundaries of PGDC and baseline zero.(e)Knowledge boundaries of PGDC and baseline P-dis.(f)Knowledge boundaries of PGDC and baseline dis.</p>
<p>Figure 6 :
6
Figure 6: Knowledge boundaries of the proposed PGDC and baseline methods on KAssess using LLaMA2 model.</p>
<p>Table 1 :
1
The success rate of constructing prompts to elicit specific knowledge on four Datasets.We conduct experiments on four different LLMs to illustrate the performance of our proposed PGDC.Dataset PARAREL and KAssess provide true knowledge to characterize the ability of different methods to obtain knowledge boundary while the pieces of knowledge in dataset CFACT and ALCUNA are fake which shows the robustness of PGDC.</p>
<p>program [PGDC Prompt] The early item of source code is a simple; [Optimal Paraphrase] [Original Prompt] Isatis tinctoria is a source of; [Answer] indigo [PGDC Prompt] Isatis tinctoria, a source of the natural dye; [Reasoning and Inference] [Original Prompt] The host country of Australian Capital Territory is; [Answer] Australia [PGDC Prompt] <s> host country of Australian Capital Territory is; [Format and Stop Words]</p>
<p>Table 2 :
2
Cases that PGDC successfully updates the prompt.We summarize the advantages of PGDC into three aspects : 1) Optimal Paraphrase; 2) Reasoning and Inference; 3) Format and Stop Words.
DatasetCFACT↓ModelGPT-2GPT-J LLaMA2 VicunaAutoprompt 92.38% 85.67% 88.35% 33.09%PGDC2.81% 4.82%3.41%3.50%</p>
<p>Table 3 :
3
Comparison of PGDC and AutoPrompt on CFACT dataset.
2024/2/16 13:12127.0.0.1:44999</p>
<p>Table 4 :
4
Demonstration of baselines.Answers are in '[]'.Each query has multiple textual expressions and each answer has multiple aliases.The number of examples for few-shots in our experiments is 4. Due to space constraints, we do not show these comprehensively.
HyperparameterLearning rate1e-2OptimizerAdamSchedulerExponentialLRSchedule Step5Iteration Rounds25λ 20.01</p>
<p>Table 5 :
5
Hyperparameter settings of PGDC.</p>
<p>Algorithm 1 PGDC Algorithm Input: LLM θ, Embedding TableE|V| , Input Question q = {q 1 , q 2 , ..., q n }, Answer a, Loss Function Φ, Optimization Step T , Learning Rate γ, Projection Ceil c1: p ← E |V| [q] 2: for i = 1, 2, ...T do 3:Generate z = {z 1 , z 2 , ..., z m } with p as an input into θ 4:</p>
<p>Table 6 :
6
Human evaluation guideline.</p>
<p>Table 7 :
7
. Our categorization of subjects in MMLU and detailed scores.
Broader Subject Subject in MMLUGPT-2 GPT-J LLaMA2 Vicuna Mistralastronomy_test5.086.786.7810.1715.25college_biology_test2.5610.2614.1010.2616.67college_chemistry_test1.891.8911.329.435.67Nature Scienceconceptual_physics_test14.47 19.3023.2523.2523.25high_school_physics_test0.000.001.721.721.72high_school_biology_test1.418.4514.7911.9715.49high_school_chemistry_test3.064.087.146.128.16high_school_government_politics10.34 12.6418.3916.0914.94high_school_macroeconomics_test0.931.873.744.675,61high_school_microeconomics_test2.502.505.004.175.00management_test1.080.005.385.382.15Social Scienceprofessional_accounting_test1.670.001.671.670.00sociology_test0.003.496.983.495.81us_foreign_policy_test5.0810.1710.176.7810.17world_religions_test3.614.2119.8815.0618.07high_school_psychology_test10.08 14.4719.9017.0522.22Engineeringelectrical_engineering_test college_computer_science_test6.02 0.005.26 4.7610.53 4.766.02 0.006.77 14.29clinical_knowledge_test0.002.684.702.686.71college_medicine_test2.862.8610.002.868.57Medicinemedical_genetics_test nutrition_test0.00 2.534.11 3.169.59 10.1313.70 7.599.59 8,87virology_test0.792.383.973.976.35anatomy_test1.239.8820.9919.7525.93global_facts_test1.143.417.959.0912.50Humanitiesmoral_disputes_test3.522.017.044.526.03miscellaneous_test10.30 11.6123.3722.7928.45high_school_geography_test4.858.4816.3612.7313.94Otherslogical_fallacies_test0.000.982.943.923.92human_aging_test29.273.905.379.768.29
https://github.com/pkulcwmzx/knowledge boundary
MMLU covers 57 subjects. To fit the theme of our paper, here we have selected
topics related to knowledge, dropping topics such as computation and reasoning, for analysis.
AcknowledgementsThis work was supported by Beijing Science and Technology Program (Z231100007423011), National Key R&amp;D Program of China (2021YFF0901502), National Science Foundation of China (No. 62161160339) and Key Laboratory of Science, Technology and Standard in Press Industry (Key Laboratory of Intelligent Press Media Technology).We would like to thank Zhidong Jia, Yezhen Chen, Baizhou Huang, Junzhe Zhang and members of the group for their valuable feedback and discussions.We appreciate the anonymous reviewers for their helpful comments.Xiaojun Wan is the corresponding author.
. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Erica Moreira, Mark Omernick, Kevin Robinson,; Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barhamet al. 2023. Palm 2 technical report</p>
<p>Akshatha Arodi, Martin Pömsl, Kaheer Suleman, Adam Trischler, Alexandra Olteanu, and Jackie Chi Kit Cheung. 2023. The kitmus test: Evaluating knowledge integration from multiple sources in natural language understanding systems. </p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, 2020</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, Sparks of artificial general intelligence: Early experiments with gpt-4. 2023</p>
<p>Say what you mean! large language models speak too positively about negative commonsense knowledge. Tyler A Chang, Benjamin K Bergen ; Jiangjie Chen, Wei Shi, Ziquan Fu, Sijie Cheng, Lei Li, Yanghua Xiao, 2023. 2023Language model behavior: A comprehensive survey</p>
<p>Seq2sick: Evaluating the robustness of sequence-to-sequence models with adversarial examples. Minhao Cheng, Jinfeng Yi, Pin-Yu Chen, Huan Zhang, Cho-Jui Hsieh, 2020</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. </p>
<p>Complex logical reasoning over knowledge graphs using large language models. Nurendra Choudhary, K Chandan, Reddy, 2023</p>
<p>Prompt-based conservation learning for multi-hop question answering. Zhenyun Deng, Yonghua Zhu, Yang Chen, Qianqian Qi, Michael Witbrock, Patricia Riddle, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational LinguisticsGyeongju, Republic of Korea2022International Committee on Computational Linguistics</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Zhifang Sui, arXiv:2301.00234A survey for in-context learning. 2022arXiv preprint</p>
<p>Statistical knowledge assessment for large language models. Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Zhifang Sui, Lei Li, 2023</p>
<p>HotFlip: White-box adversarial examples for text classification. Javid Ebrahimi, Anyi Rao, Daniel Lowd, Dejing Dou, 10.18653/v1/P18-2006Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics20182Short Papers)</p>
<p>Measuring and improving consistency in pretrained language models. Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schütze, Yoav Goldberg, 10.1162/tacl_a_00410Transactions of the Association for Computational Linguistics. 20219</p>
<p>T-REx: A large scale alignment of natural language with knowledge base triples. Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare, Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)Miyazaki, Japan2018Frederique Laforest, and Elena Simperl. European Language Resources Association (ELRA</p>
<p>Bruce Joseph L Fleiss, Myunghee Levin, Cho Paik, The measurement of interrater agreement. Statistical methods for rates and proportions. 19812</p>
<p>. Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, Yupeng Wu, 2023How close is chatgpt to human experts? comparison corpus, evaluation, and detection</p>
<p>Gradient-based adversarial attacks against text transformers. Chuan Guo, Alexandre Sablayrolles, Hervé Jégou, Douwe Kiela, 10.18653/v1/2021.emnlp-main.464Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, 2021</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, 10.1145/3571730ACM Computing Surveys. 55122023</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023</p>
<p>Automatically auditing large language models via discrete optimization. Erik Jones, Anca Dragan, Aditi Raghunathan, Jacob Steinhardt, 2023</p>
<p>Adyasha Maharana, Amita Kamath, Christopher Clark, Mohit Bansal, Aniruddha Kembhavi, Exposing and addressing cross-task inconsistency in unified vision-language models. 2023</p>
<p>When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, Hannaneh Hajishirzi, 2023</p>
<p>Locating and editing factual associations in GPT. Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov, Advances in Neural Information Processing Systems. 202236</p>
<p>Creak: A dataset for commonsense reasoning over entity knowledge. Yasumasa Onoe, J Q Michael, Eunsol Zhang, Greg Choi, Durrett, 2021</p>
<p>Language models as knowledge bases?. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, 10.18653/v1/D19-1250Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>The architectonics of knowledge. H Philip, Phenix, 10.1080/00336297.1967.10702784Quest. 911967</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2019</p>
<p>AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. Taylor Shin, Yasaman Razeghi, Robert L Logan, I V , Eric Wallace, Sameer Singh, 10.18653/v1/2020.emnlp-main.346Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, et al. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, Jason Wei, 2022</p>
<p>Lamda: Language models for dialog applications. Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Yaguang Du, Hongrae Li, Huaixiu Lee, Steven Zheng, 2022</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. </p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Universal adversarial triggers for attacking and analyzing NLP. Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, Sameer Singh, 10.18653/v1/D19-1221Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, China2019Association for Computational Linguistics</p>
<p>Ben Wang, Aran Komatsuzaki, GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. 2021</p>
<p>Large language models are not fair evaluators. Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui, 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery. Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, Tom Goldstein, 2023</p>
<p>Style over substance: Evaluation biases for large language models. Minghao Wu, Alham Fikri, Aji , 2023</p>
<p>Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, Xinyun Chen, Large language models as optimizers. 2023</p>
<p>ALCUNA: Large language models meet new knowledge. Xunjian Yin, Baizhou Huang, Xiaojun Wan, 10.18653/v1/2023.emnlp-main.87Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao, Daniel Zhang-Li, Xin Lv, Hao Peng, Zijun Yao, Xiaohan Zhang, Hanming Li, Carefully benchmarking world knowledge of large language models. Kola2023</p>
<p>Benchmarking large language models for news summarization. Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen Mckeown, Tatsunori B Hashimoto, 2023</p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
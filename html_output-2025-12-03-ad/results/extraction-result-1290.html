<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1290 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1290</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1290</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-588be1711731acfc4f7dea1e6ae99bc8f9333449</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/588be1711731acfc4f7dea1e6ae99bc8f9333449" target="_blank">Active Exploration via Experiment Design in Markov Chains</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Artificial Intelligence and Statistics</p>
                <p><strong>Paper TL;DR:</strong> This work proposes an algorithm -- Markov-design -- that efficiently selects policies whose measurement allocation reasonably converges to the optimal one, and is sequential in nature, adapting its choice of policies (experiments) informed by past measurements.</p>
                <p><strong>Paper Abstract:</strong> A key challenge in science and engineering is to design experiments to learn about some unknown quantity of interest. Classical experimental design optimally allocates the experimental budget to maximize a notion of utility (e.g., reduction in uncertainty about the unknown quantity). We consider a rich setting, where the experiments are associated with states in a {\em Markov chain}, and we can only choose them by selecting a {\em policy} controlling the state transitions. This problem captures important applications, from exploration in reinforcement learning to spatial monitoring tasks. We propose an algorithm -- \textsc{markov-design} -- that efficiently selects policies whose measurement allocation \emph{provably converges to the optimal one}. The algorithm is sequential in nature, adapting its choice of policies (experiments) informed by past measurements. In addition to our theoretical analysis, we showcase our framework on applications in ecological surveillance and pharmacology.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1290.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1290.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MARKOV-DESIGN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MARKOV-DESIGN (Adaptive Experiment Design in Markov Chains)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptive experiment-design algorithm that sequentially selects episode policies in a known Markov chain to allocate measurement effort so that empirical trajectory visitations converge to the optimal information-maximizing allocation; relies on convex optimization over state-action visitation polytopes and a convex-RL oracle.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MARKOV-DESIGN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A sequential policy-selection algorithm that maintains the empirical trajectory distribution eta_t and at each episode solves a convex optimization (via a Convex RL subroutine) to find a policy whose visitation distribution d minimizes a one-step objective G_t(d)=U((t/(t+1)) Z eta_t + (1/(t+1)) d). Implements mixture policies (Frank-Wolfe style), density estimation, line search, and marginalization to actual executable policies.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Information-gain-driven adaptive experimental design implemented as convex experimental design (scalarized information objectives) solved via Convex RL and Frank-Wolfe incremental allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each episode the algorithm: (1) updates empirical visitation Z eta_t from past executed trajectories; (2) forms a one-step objective G_t that measures the benefit of adding a new visitation distribution; (3) uses a convex-RL linearization (gradient of U) and a linear-minimization oracle to obtain a new policy / trajectory distribution to add with step-size 1/(1+t); (4) executes the selected policy, observes the trajectory, and updates eta_t. Variants differ in whether the convex subproblem is solved exactly (EXACT) or only one Frank-Wolfe step is taken (ONE-STEP).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multiple (Synthetic gridworlds; Species monitoring (Beilschmiedia) spatial Poisson sensing; Group pharmacokinetics blood-draw planning)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Known Markov chain transition model, unknown target function f(x,a) in an RKHS observed via noisy evaluations; observation noise may be heteroscedastic or Poisson (variance tied to mean); environments exhibit stochastic transitions (variable levels), discrete state/action spaces in experiments, and structural similarity (kernel) across state-actions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Discrete gridworld: 4 actions (up/down/left/right), episode length H = 20 in experiments, varying stochasticity parameter p for action noise; species monitoring: spatial sectors as states, drone trajectories of length H (unspecified H), Poisson counts per sector; pharmacokinetics: each episode = one patient, up to 5 blood draws per episode with minimum separation constraints (3 timesteps).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Adaptive MARKOV-DESIGN (ONE-STEP/EXACT) empirically converges substantially faster than non-adaptive baselines: in synthetic gridworlds adaptive methods quickly reduce suboptimality and often show empirical rates up to O(1/T^2) (faster than theoretical guarantees). In species monitoring and pharmacokinetics adaptive variants dominate baselines in the reported objectives (plots show median and 10/90% quantiles over 20 runs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Non-adaptive resampling (repeatedly executing a fixed optimal mixture policy) shows slower convergence with empirical rate O(1/√T) and can suffer from coupon-collector effects (resampling same trajectories repeatedly); random selection fails to converge in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Empirically much more sample-efficient than non-adaptive baselines: adaptive variants reach low suboptimality within far fewer episodes (figures indicate rapid early progress); reported empirical convergence often outperforms theoretical bounds, e.g., observed O(1/T^2) vs proven O(1/√T) high-probability rates.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>The objective is pure information-driven (exploration) rather than reward exploitation: the algorithm greedily minimizes a scalarized information objective given past data with diminishing step-size 1/(1+t), thereby controlling how much new exploration is added; mixture policies allow controlled stochastic exploration across episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>NON-ADAPTIVE resampling of the found mixture policy; TRACKING (tracking mixture components to equalize empirical counts); RANDOM policy selection; marginalization (execute marginalized policy repeatedly).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>The adaptive MARKOV-DESIGN algorithm is provably convergent: Theorems establish high-probability and expected suboptimality bounds (e.g., high-probability bound scaling ∝ (1/T) sqrt(sum_t ||∇F(η_t)||_∞^2 log(T/δ)) and expectation bounds involving local smoothness constants). Empirically, ONE-STEP and EXACT outperform non-adaptive baselines across synthetic gridworlds, species monitoring, and pharmacokinetics; ONE-STEP often dominates EXACT for short horizons, and adaptively updating unknown variance/confidence sets improves speed in heteroscedastic/Poisson cases. The method effectively constructs visitation distributions matching the optimal allocation over states and actions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires knowledge of the Markov transition model P (simulation/density oracle) for density estimation or access to a simulator; theoretical rates depend on gradient norms and local smoothness constants that are hard to bound generally; smoothing needed for guarantees can hamper practical speed (authors do not use smoothing empirically); TRACKING degrades under high transition stochasticity; NON-ADAPTIVE resampling suffers coupon-collector inefficiencies; EXACT variant can be computationally heavier and only shows linear convergence in limited cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Exploration via Experiment Design in Markov Chains', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1290.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1290.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ONE-STEP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ONE-STEP variant of MARKOV-DESIGN</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical, adaptive variant of MARKOV-DESIGN that runs one Convex RL Frank–Wolfe step per episode and then executes the first component of the mixture policy as the next policy; often empirically fastest in early episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ONE-STEP (MARKOV-DESIGN variant)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A greedy adaptive agent that at each episode runs one iteration of the Convex RL (Frank–Wolfe) oracle to obtain a mixture policy and executes the first (greedy) base policy; updates empirical visitation and repeats.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Greedy Frank–Wolfe-style convex experimental design (one-step Convex RL)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts by using current empirical visitation Z eta_t to form G_t, runs a single Frank–Wolfe linearization + RL solver to get a candidate policy, and immediately executes that policy (no inner-loop convergence); uses observed trajectory to update eta_t.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same as MARKOV-DESIGN experiments (Synthetic gridworlds; Species monitoring; Pharmacokinetics)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Known transitions, unknown RKHS function f, noisy observations, variable stochasticity in transitions; discrete state/action in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Gridworlds with H=20 and 4 actions; species monitoring with spatial sectors and drone trajectory constraints; pharmacokinetics with 5 sampling actions per episode and timing constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>ONE-STEP empirically converges fastest in early-stage learning across tasks; dominates EXACT for short horizons in reported experiments and reaches low suboptimality very quickly (see Fig. 2 and 3). Observed empirical convergence rates up to O(1/T^2) in some gridworlds.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Compared to NON-ADAPTIVE resampling and random policies, ONE-STEP shows markedly faster reduction in the information-objective suboptimality; numerical baselines in paper show non-adaptive O(1/√T) behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>High: achieves large information gains in few episodes relative to non-adaptive baselines (figures indicate rapid early progress; authors note ONE-STEP's empirical superiority for short horizons).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Purely exploration-focused: picks the next policy to maximally reduce uncertainty given past empirical allocation with a single-step greedy update; diminishing step-size controls amount of change over time.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>EXACT variant of MARKOV-DESIGN, NON-ADAPTIVE resampling, TRACKING, RANDOM</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>ONE-STEP is a computationally simple adaptive strategy that often outperforms the theoretically stronger EXACT variant in early episodes; it yields rapid convergence to near-optimal visitation in deterministic and low-noise settings, and empirically shows faster-than-theoretical rates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>As a greedy one-step method, it may be suboptimal asymptotically compared to EXACT in some problems; relies on reliable gradient estimation and transition model; performance can degrade if gradient/smoothness constants are unfavorable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Exploration via Experiment Design in Markov Chains', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1290.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1290.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EXACT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EXACT variant of MARKOV-DESIGN</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adaptive MARKOV-DESIGN variant that, at each episode, solves the one-step convex subproblem to (near) optimality over trajectory distributions and then selects the corresponding policy to execute; theoretically stronger but computationally heavier.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>EXACT (MARKOV-DESIGN variant)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>At each episode EXACT computes q_t = argmin_{q in P} F( (t/(t+1)) eta_t + (1/(t+1)) Z q ) (solved to near-optimality), then executes a policy corresponding to q_t and updates eta_t; inner optimization solved with Convex RL to high accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Exact one-step convex experimental design (solve the episode-level minimization exactly/near-exact)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Uses full solution of the one-step optimization that accounts for how adding any trajectory distribution q will change the future empirical allocation; executes the resulting policy and updates empirical counts.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same experimental environments (gridworlds, species monitoring, pharmacokinetics)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Known Markov chain, unknown RKHS target, noisy observations, possibly heteroscedastic noise or unknown variances.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Comparable to ONE-STEP; experiments show behavior across different stochasticity levels and application domains.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>EXACT achieves strong convergence and in some settings exhibits linear convergence in later stages (reported in select figures); however, for short horizons ONE-STEP sometimes outperforms EXACT empirically.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>EXACT outperforms non-adaptive baselines in all presented experiments but may be slower in wall-clock or early-episode performance due to heavier inner optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>High asymptotic efficiency with provable convergence guarantees; empirically competitive but with higher computational cost per episode than ONE-STEP.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Fully exploration-driven by optimizing a scalarized information objective; exact subproblem deliberately selects the trajectory distribution that best reduces uncertainty given past data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>ONE-STEP, NON-ADAPTIVE resampling, TRACKING, RANDOM</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>EXACT yields provable convergence to the optimal allocation (Theorems 1 and 2) and sometimes achieves linear convergence empirically; however, empirical evidence shows it can be outperformed by the simpler ONE-STEP early on despite stronger asymptotic behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Higher computational cost due to solving subproblems to near-optimality; empirical early-stage performance can be inferior to ONE-STEP; theoretical guarantees rely on smoothness and gradient-norm conditions that can be large in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Exploration via Experiment Design in Markov Chains', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1290.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1290.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NON-ADAPTIVE (Resampling)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Non-adaptive resampling / marginalization baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline strategy that computes an optimal (convex-RL) mixture/marginalized policy once (non-adaptively) and then repeatedly executes components sampled from that mixture or executes the marginalized stationary policy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>NON-ADAPTIVE resampling / marginalization</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Compute a single optimal mixture policy via Convex RL and then either sample components from that mixture each episode (resampling) or marginalize to a stationary policy and execute it repeatedly. Also includes TRACKING variant that attempts to balance empirical counts across mixture components.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Non-adaptive design (single-shot convex experimental design, then repetition) — i.e., no per-episode adaptation based on observed trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>None (post-optimization execution only); TRACKING variant tries to equalize empirical counts to match mixture weights by selecting components with largest remaining deficit.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same experimental environments in paper</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Known Markov chain, unknown target f, stochastic transitions possible.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same as above (gridworld H=20; other domain specifics).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Non-adaptive resampling exhibits slower convergence (empirical O(1/√T)) and suffers from the coupon-collector problem (resampling same trajectories), making it sample-inefficient; TRACKING can mitigate redundant resampling in low-stochasticity settings but degrades with higher transition stochasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Lower than adaptive methods: requires many more episodes to achieve comparable reductions in the information objective; coupon-collector effects can require O(d log d) samples to cover d distinct useful trajectories in degenerate settings.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Not adaptive; exploration distribution fixed by initial optimization; TRACKING tries to enforce coverage across mixture components but does not use observed measurement values to re-optimize.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared directly as baseline against MARKOV-DESIGN (ONE-STEP and EXACT) and RANDOM</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Serves as the primary baseline; authors demonstrate adaptive methods significantly outperform NON-ADAPTIVE resampling in sample efficiency and final suboptimality. Resampling has slower theoretical and empirical convergence due to sampling redundancy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Inefficient under large/structured trajectory spaces due to repeated resampling of the same trajectories; poor empirical performance in high-stochasticity environments for TRACKING.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Exploration via Experiment Design in Markov Chains', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1290.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1290.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Convex RL (oracle)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Convex Reinforcement Learning (Convex RL) subroutine</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A subroutine that, given a convex objective over state-action visitation distributions U(d), constructs a stationary (or non-stationary) policy via Frank–Wolfe style iteration by solving linearized RL problems (standard planning / MDP solvers) and mixing base policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Provably efficient maximum entropy exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Convex RL oracle</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Implements the Frank–Wolfe algorithm on the visitation-polytope: repeatedly linearizes the convex objective U(d), solves a linear minimization oracle (classical RL with linear reward equal to ∇U(d)) using standard RL/planning solvers to get a new base policy, and updates a mixture policy via line search; used as inner solver in MARKOV-DESIGN.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Convex RL (convex objective minimization via Frank–Wolfe; uses planning/MDP solvers as linear oracle)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Not an episodic adaptive agent itself but an optimization oracle: given current mixture visitation it returns a new base policy corresponding to the linearized objective; used iteratively to build up the desired visitation distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Used as oracle across the paper's environments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Assumes known transition model for planning; can be applied to discrete or sample-based estimation settings (requires simulator or transition model).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Scales with state/action space; Frank–Wolfe requires O(L/ε log(1/ε)) inner iterations to ε-optimality under assumptions; density estimation for mixtures may require simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Enables computation of policies whose visitation d* minimizes U(d); complexity depends on smoothness L and desired accuracy; used successfully inside MARKOV-DESIGN to produce policies that reduce information-objective suboptimality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Computational/sample tradeoff: the oracle itself is computationally heavier than single-policy execution, but it produces policies that improve sample-efficiency of the outer adaptive loop; convergence in number of mixture components n is O(L/ε log(1/ε)).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Solves purely exploration (information) objectives via linearized MDP planning; exploration emerges through the policies produced by solving linear reward MDPs derived from ∇U(d).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Cited references: Hazan et al. (2019), Zahavy et al. (2021) for convex RL framework; paper uses this as an internal oracle and compares outer adaptive strategies (ONE-STEP/EXACT) against non-adaptive baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Using Convex RL as an oracle reduces the complex experiment-design optimization over exponentially many trajectories to a sequence of classical RL planning problems; this makes optimization tractable and enables provable convergence of MARKOV-DESIGN.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Relies on knowledge of transition model or simulator for density estimation; the number of inner iterations (mixture components) can be large when objective smoothness L is large.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Exploration via Experiment Design in Markov Chains', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Provably efficient maximum entropy exploration. <em>(Rating: 2)</em></li>
                <li>Instancedependent policy learning for linear mdps via online experiment design. <em>(Rating: 2)</em></li>
                <li>Active exploration in markov decision processes. <em>(Rating: 1)</em></li>
                <li>Bandit optimization with upper-confidence frank-wolfe <em>(Rating: 1)</em></li>
                <li>No-regret algorithms for capturing events in poisson point processes. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1290",
    "paper_id": "paper-588be1711731acfc4f7dea1e6ae99bc8f9333449",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "MARKOV-DESIGN",
            "name_full": "MARKOV-DESIGN (Adaptive Experiment Design in Markov Chains)",
            "brief_description": "An adaptive experiment-design algorithm that sequentially selects episode policies in a known Markov chain to allocate measurement effort so that empirical trajectory visitations converge to the optimal information-maximizing allocation; relies on convex optimization over state-action visitation polytopes and a convex-RL oracle.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MARKOV-DESIGN",
            "agent_description": "A sequential policy-selection algorithm that maintains the empirical trajectory distribution eta_t and at each episode solves a convex optimization (via a Convex RL subroutine) to find a policy whose visitation distribution d minimizes a one-step objective G_t(d)=U((t/(t+1)) Z eta_t + (1/(t+1)) d). Implements mixture policies (Frank-Wolfe style), density estimation, line search, and marginalization to actual executable policies.",
            "adaptive_design_method": "Information-gain-driven adaptive experimental design implemented as convex experimental design (scalarized information objectives) solved via Convex RL and Frank-Wolfe incremental allocation.",
            "adaptation_strategy_description": "At each episode the algorithm: (1) updates empirical visitation Z eta_t from past executed trajectories; (2) forms a one-step objective G_t that measures the benefit of adding a new visitation distribution; (3) uses a convex-RL linearization (gradient of U) and a linear-minimization oracle to obtain a new policy / trajectory distribution to add with step-size 1/(1+t); (4) executes the selected policy, observes the trajectory, and updates eta_t. Variants differ in whether the convex subproblem is solved exactly (EXACT) or only one Frank-Wolfe step is taken (ONE-STEP).",
            "environment_name": "Multiple (Synthetic gridworlds; Species monitoring (Beilschmiedia) spatial Poisson sensing; Group pharmacokinetics blood-draw planning)",
            "environment_characteristics": "Known Markov chain transition model, unknown target function f(x,a) in an RKHS observed via noisy evaluations; observation noise may be heteroscedastic or Poisson (variance tied to mean); environments exhibit stochastic transitions (variable levels), discrete state/action spaces in experiments, and structural similarity (kernel) across state-actions.",
            "environment_complexity": "Discrete gridworld: 4 actions (up/down/left/right), episode length H = 20 in experiments, varying stochasticity parameter p for action noise; species monitoring: spatial sectors as states, drone trajectories of length H (unspecified H), Poisson counts per sector; pharmacokinetics: each episode = one patient, up to 5 blood draws per episode with minimum separation constraints (3 timesteps).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Adaptive MARKOV-DESIGN (ONE-STEP/EXACT) empirically converges substantially faster than non-adaptive baselines: in synthetic gridworlds adaptive methods quickly reduce suboptimality and often show empirical rates up to O(1/T^2) (faster than theoretical guarantees). In species monitoring and pharmacokinetics adaptive variants dominate baselines in the reported objectives (plots show median and 10/90% quantiles over 20 runs).",
            "performance_without_adaptation": "Non-adaptive resampling (repeatedly executing a fixed optimal mixture policy) shows slower convergence with empirical rate O(1/√T) and can suffer from coupon-collector effects (resampling same trajectories repeatedly); random selection fails to converge in reported experiments.",
            "sample_efficiency": "Empirically much more sample-efficient than non-adaptive baselines: adaptive variants reach low suboptimality within far fewer episodes (figures indicate rapid early progress); reported empirical convergence often outperforms theoretical bounds, e.g., observed O(1/T^2) vs proven O(1/√T) high-probability rates.",
            "exploration_exploitation_tradeoff": "The objective is pure information-driven (exploration) rather than reward exploitation: the algorithm greedily minimizes a scalarized information objective given past data with diminishing step-size 1/(1+t), thereby controlling how much new exploration is added; mixture policies allow controlled stochastic exploration across episodes.",
            "comparison_methods": "NON-ADAPTIVE resampling of the found mixture policy; TRACKING (tracking mixture components to equalize empirical counts); RANDOM policy selection; marginalization (execute marginalized policy repeatedly).",
            "key_results": "The adaptive MARKOV-DESIGN algorithm is provably convergent: Theorems establish high-probability and expected suboptimality bounds (e.g., high-probability bound scaling ∝ (1/T) sqrt(sum_t ||∇F(η_t)||_∞^2 log(T/δ)) and expectation bounds involving local smoothness constants). Empirically, ONE-STEP and EXACT outperform non-adaptive baselines across synthetic gridworlds, species monitoring, and pharmacokinetics; ONE-STEP often dominates EXACT for short horizons, and adaptively updating unknown variance/confidence sets improves speed in heteroscedastic/Poisson cases. The method effectively constructs visitation distributions matching the optimal allocation over states and actions.",
            "limitations_or_failures": "Requires knowledge of the Markov transition model P (simulation/density oracle) for density estimation or access to a simulator; theoretical rates depend on gradient norms and local smoothness constants that are hard to bound generally; smoothing needed for guarantees can hamper practical speed (authors do not use smoothing empirically); TRACKING degrades under high transition stochasticity; NON-ADAPTIVE resampling suffers coupon-collector inefficiencies; EXACT variant can be computationally heavier and only shows linear convergence in limited cases.",
            "uuid": "e1290.0",
            "source_info": {
                "paper_title": "Active Exploration via Experiment Design in Markov Chains",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "ONE-STEP",
            "name_full": "ONE-STEP variant of MARKOV-DESIGN",
            "brief_description": "A practical, adaptive variant of MARKOV-DESIGN that runs one Convex RL Frank–Wolfe step per episode and then executes the first component of the mixture policy as the next policy; often empirically fastest in early episodes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ONE-STEP (MARKOV-DESIGN variant)",
            "agent_description": "A greedy adaptive agent that at each episode runs one iteration of the Convex RL (Frank–Wolfe) oracle to obtain a mixture policy and executes the first (greedy) base policy; updates empirical visitation and repeats.",
            "adaptive_design_method": "Greedy Frank–Wolfe-style convex experimental design (one-step Convex RL)",
            "adaptation_strategy_description": "Adapts by using current empirical visitation Z eta_t to form G_t, runs a single Frank–Wolfe linearization + RL solver to get a candidate policy, and immediately executes that policy (no inner-loop convergence); uses observed trajectory to update eta_t.",
            "environment_name": "Same as MARKOV-DESIGN experiments (Synthetic gridworlds; Species monitoring; Pharmacokinetics)",
            "environment_characteristics": "Known transitions, unknown RKHS function f, noisy observations, variable stochasticity in transitions; discrete state/action in experiments.",
            "environment_complexity": "Gridworlds with H=20 and 4 actions; species monitoring with spatial sectors and drone trajectory constraints; pharmacokinetics with 5 sampling actions per episode and timing constraints.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "ONE-STEP empirically converges fastest in early-stage learning across tasks; dominates EXACT for short horizons in reported experiments and reaches low suboptimality very quickly (see Fig. 2 and 3). Observed empirical convergence rates up to O(1/T^2) in some gridworlds.",
            "performance_without_adaptation": "Compared to NON-ADAPTIVE resampling and random policies, ONE-STEP shows markedly faster reduction in the information-objective suboptimality; numerical baselines in paper show non-adaptive O(1/√T) behavior.",
            "sample_efficiency": "High: achieves large information gains in few episodes relative to non-adaptive baselines (figures indicate rapid early progress; authors note ONE-STEP's empirical superiority for short horizons).",
            "exploration_exploitation_tradeoff": "Purely exploration-focused: picks the next policy to maximally reduce uncertainty given past empirical allocation with a single-step greedy update; diminishing step-size controls amount of change over time.",
            "comparison_methods": "EXACT variant of MARKOV-DESIGN, NON-ADAPTIVE resampling, TRACKING, RANDOM",
            "key_results": "ONE-STEP is a computationally simple adaptive strategy that often outperforms the theoretically stronger EXACT variant in early episodes; it yields rapid convergence to near-optimal visitation in deterministic and low-noise settings, and empirically shows faster-than-theoretical rates.",
            "limitations_or_failures": "As a greedy one-step method, it may be suboptimal asymptotically compared to EXACT in some problems; relies on reliable gradient estimation and transition model; performance can degrade if gradient/smoothness constants are unfavorable.",
            "uuid": "e1290.1",
            "source_info": {
                "paper_title": "Active Exploration via Experiment Design in Markov Chains",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "EXACT",
            "name_full": "EXACT variant of MARKOV-DESIGN",
            "brief_description": "Adaptive MARKOV-DESIGN variant that, at each episode, solves the one-step convex subproblem to (near) optimality over trajectory distributions and then selects the corresponding policy to execute; theoretically stronger but computationally heavier.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "EXACT (MARKOV-DESIGN variant)",
            "agent_description": "At each episode EXACT computes q_t = argmin_{q in P} F( (t/(t+1)) eta_t + (1/(t+1)) Z q ) (solved to near-optimality), then executes a policy corresponding to q_t and updates eta_t; inner optimization solved with Convex RL to high accuracy.",
            "adaptive_design_method": "Exact one-step convex experimental design (solve the episode-level minimization exactly/near-exact)",
            "adaptation_strategy_description": "Uses full solution of the one-step optimization that accounts for how adding any trajectory distribution q will change the future empirical allocation; executes the resulting policy and updates empirical counts.",
            "environment_name": "Same experimental environments (gridworlds, species monitoring, pharmacokinetics)",
            "environment_characteristics": "Known Markov chain, unknown RKHS target, noisy observations, possibly heteroscedastic noise or unknown variances.",
            "environment_complexity": "Comparable to ONE-STEP; experiments show behavior across different stochasticity levels and application domains.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "EXACT achieves strong convergence and in some settings exhibits linear convergence in later stages (reported in select figures); however, for short horizons ONE-STEP sometimes outperforms EXACT empirically.",
            "performance_without_adaptation": "EXACT outperforms non-adaptive baselines in all presented experiments but may be slower in wall-clock or early-episode performance due to heavier inner optimization.",
            "sample_efficiency": "High asymptotic efficiency with provable convergence guarantees; empirically competitive but with higher computational cost per episode than ONE-STEP.",
            "exploration_exploitation_tradeoff": "Fully exploration-driven by optimizing a scalarized information objective; exact subproblem deliberately selects the trajectory distribution that best reduces uncertainty given past data.",
            "comparison_methods": "ONE-STEP, NON-ADAPTIVE resampling, TRACKING, RANDOM",
            "key_results": "EXACT yields provable convergence to the optimal allocation (Theorems 1 and 2) and sometimes achieves linear convergence empirically; however, empirical evidence shows it can be outperformed by the simpler ONE-STEP early on despite stronger asymptotic behavior.",
            "limitations_or_failures": "Higher computational cost due to solving subproblems to near-optimality; empirical early-stage performance can be inferior to ONE-STEP; theoretical guarantees rely on smoothness and gradient-norm conditions that can be large in practice.",
            "uuid": "e1290.2",
            "source_info": {
                "paper_title": "Active Exploration via Experiment Design in Markov Chains",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "NON-ADAPTIVE (Resampling)",
            "name_full": "Non-adaptive resampling / marginalization baseline",
            "brief_description": "Baseline strategy that computes an optimal (convex-RL) mixture/marginalized policy once (non-adaptively) and then repeatedly executes components sampled from that mixture or executes the marginalized stationary policy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "NON-ADAPTIVE resampling / marginalization",
            "agent_description": "Compute a single optimal mixture policy via Convex RL and then either sample components from that mixture each episode (resampling) or marginalize to a stationary policy and execute it repeatedly. Also includes TRACKING variant that attempts to balance empirical counts across mixture components.",
            "adaptive_design_method": "Non-adaptive design (single-shot convex experimental design, then repetition) — i.e., no per-episode adaptation based on observed trajectories",
            "adaptation_strategy_description": "None (post-optimization execution only); TRACKING variant tries to equalize empirical counts to match mixture weights by selecting components with largest remaining deficit.",
            "environment_name": "Same experimental environments in paper",
            "environment_characteristics": "Known Markov chain, unknown target f, stochastic transitions possible.",
            "environment_complexity": "Same as above (gridworld H=20; other domain specifics).",
            "uses_adaptive_design": false,
            "performance_with_adaptation": null,
            "performance_without_adaptation": "Non-adaptive resampling exhibits slower convergence (empirical O(1/√T)) and suffers from the coupon-collector problem (resampling same trajectories), making it sample-inefficient; TRACKING can mitigate redundant resampling in low-stochasticity settings but degrades with higher transition stochasticity.",
            "sample_efficiency": "Lower than adaptive methods: requires many more episodes to achieve comparable reductions in the information objective; coupon-collector effects can require O(d log d) samples to cover d distinct useful trajectories in degenerate settings.",
            "exploration_exploitation_tradeoff": "Not adaptive; exploration distribution fixed by initial optimization; TRACKING tries to enforce coverage across mixture components but does not use observed measurement values to re-optimize.",
            "comparison_methods": "Compared directly as baseline against MARKOV-DESIGN (ONE-STEP and EXACT) and RANDOM",
            "key_results": "Serves as the primary baseline; authors demonstrate adaptive methods significantly outperform NON-ADAPTIVE resampling in sample efficiency and final suboptimality. Resampling has slower theoretical and empirical convergence due to sampling redundancy.",
            "limitations_or_failures": "Inefficient under large/structured trajectory spaces due to repeated resampling of the same trajectories; poor empirical performance in high-stochasticity environments for TRACKING.",
            "uuid": "e1290.3",
            "source_info": {
                "paper_title": "Active Exploration via Experiment Design in Markov Chains",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Convex RL (oracle)",
            "name_full": "Convex Reinforcement Learning (Convex RL) subroutine",
            "brief_description": "A subroutine that, given a convex objective over state-action visitation distributions U(d), constructs a stationary (or non-stationary) policy via Frank–Wolfe style iteration by solving linearized RL problems (standard planning / MDP solvers) and mixing base policies.",
            "citation_title": "Provably efficient maximum entropy exploration.",
            "mention_or_use": "use",
            "agent_name": "Convex RL oracle",
            "agent_description": "Implements the Frank–Wolfe algorithm on the visitation-polytope: repeatedly linearizes the convex objective U(d), solves a linear minimization oracle (classical RL with linear reward equal to ∇U(d)) using standard RL/planning solvers to get a new base policy, and updates a mixture policy via line search; used as inner solver in MARKOV-DESIGN.",
            "adaptive_design_method": "Convex RL (convex objective minimization via Frank–Wolfe; uses planning/MDP solvers as linear oracle)",
            "adaptation_strategy_description": "Not an episodic adaptive agent itself but an optimization oracle: given current mixture visitation it returns a new base policy corresponding to the linearized objective; used iteratively to build up the desired visitation distribution.",
            "environment_name": "Used as oracle across the paper's environments",
            "environment_characteristics": "Assumes known transition model for planning; can be applied to discrete or sample-based estimation settings (requires simulator or transition model).",
            "environment_complexity": "Scales with state/action space; Frank–Wolfe requires O(L/ε log(1/ε)) inner iterations to ε-optimality under assumptions; density estimation for mixtures may require simulation.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Enables computation of policies whose visitation d* minimizes U(d); complexity depends on smoothness L and desired accuracy; used successfully inside MARKOV-DESIGN to produce policies that reduce information-objective suboptimality.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Computational/sample tradeoff: the oracle itself is computationally heavier than single-policy execution, but it produces policies that improve sample-efficiency of the outer adaptive loop; convergence in number of mixture components n is O(L/ε log(1/ε)).",
            "exploration_exploitation_tradeoff": "Solves purely exploration (information) objectives via linearized MDP planning; exploration emerges through the policies produced by solving linear reward MDPs derived from ∇U(d).",
            "comparison_methods": "Cited references: Hazan et al. (2019), Zahavy et al. (2021) for convex RL framework; paper uses this as an internal oracle and compares outer adaptive strategies (ONE-STEP/EXACT) against non-adaptive baselines.",
            "key_results": "Using Convex RL as an oracle reduces the complex experiment-design optimization over exponentially many trajectories to a sequence of classical RL planning problems; this makes optimization tractable and enables provable convergence of MARKOV-DESIGN.",
            "limitations_or_failures": "Relies on knowledge of transition model or simulator for density estimation; the number of inner iterations (mixture components) can be large when objective smoothness L is large.",
            "uuid": "e1290.4",
            "source_info": {
                "paper_title": "Active Exploration via Experiment Design in Markov Chains",
                "publication_date_yy_mm": "2022-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Provably efficient maximum entropy exploration.",
            "rating": 2
        },
        {
            "paper_title": "Instancedependent policy learning for linear mdps via online experiment design.",
            "rating": 2
        },
        {
            "paper_title": "Active exploration in markov decision processes.",
            "rating": 1
        },
        {
            "paper_title": "Bandit optimization with upper-confidence frank-wolfe",
            "rating": 1
        },
        {
            "paper_title": "No-regret algorithms for capturing events in poisson point processes.",
            "rating": 1
        }
    ],
    "cost": 0.021303499999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Active Exploration via Experiment Design in Markov Chains</h1>
<p>Mojmír Mutný<br>mojmir.mutny@inf.ethz.ch<br>ETH Zürich</p>
<p>Tadeusz Janik<br>tjanik@student.ethz.ch<br>ETH Zürich</p>
<h2>Andreas Krause</h2>
<p>krausea@ethz.ch
ETH Zürich</p>
<h2>Abstract</h2>
<p>A key challenge in science and engineering is to design experiments to learn about some unknown quantity of interest. Classical experimental design optimally allocates the experimental budget to maximize a notion of utility (e.g., reduction in uncertainty about the unknown quantity). We consider a rich setting, where the experiments are associated with states in a Markov chain, and we can only choose them by selecting a policy controlling the state transitions. This problem captures important applications, from exploration in reinforcement learning to spatial monitoring tasks. We propose an algorithm - MARKOV-DESIGN - that efficiently selects policies whose measurement allocation provably converges to the optimal one. The algorithm is sequential in nature, adapting its choice of policies (experiments) informed by past measurements. In addition to our theoretical analysis, we showcase our framework on applications in ecological surveillance and pharmacology.</p>
<h2>1 Introduction</h2>
<p>The optimal design of experiments (Pukelsheim, 2006; Chaloner and Verdinelli, 1995) is a ubiquitous challenge in science and engineering. The key goal is to utilize the limited budget (time, resources) to gain as much information about some unknown quantity of interest. Classical experiment design assumes that an experiment measures a single value with specific conditions. Motivated by applications illustrated in more detail below, we assume that experiments are associated with policies executed in a known Markov chain. There are three major challenges in finding the set of best policies. Firstly, the space of policies can be combinatorial in the size of the state-action space, and searching over it directly would lead to intractable optimization problems with classical methods. Secondly, the feedback from a policy is stochastic, due to the randomness of the Markov chain, which needs to be taken into account. Lastly, it is unclear how classical experimental design objectives can be formulated as a function of the policies. We address
these challenges with convex optimization techniques.
Quantity of interest The general goal of experiment design is to estimate some aspects of an unknown quantity $f$ of interest. Here, we assume $f$ is a function of states and actions of a Markov chain $f: \mathcal{X} \times \mathcal{A} \rightarrow \mathbb{R}$. We further assume $f$ belongs to a reproducing kernel Hilbert space (RKHS) $\left(f \in \mathcal{H}<em _mathcal_H="\mathcal{H">{k}\right)$ with a known kernel $k\left((x, a),\left(x^{\prime}, a^{\prime}\right)\right)=$ $\Phi(x, a)^{\top} \Phi\left(x^{\prime}, a^{\prime}\right)^{\top}$, where $x, x^{\prime} \in \mathcal{X}$ and $a, a^{\prime} \in \mathcal{A}$, and a known bound $|f|</em><em k="k">{k}} \leq \frac{1}{5}$. As a concrete example, the function $f$ can model the distribution of species over a certain geographical location, where the kernel incorporates spatial features such as access to water, soil salinity, etc., and $x$ corresponds to the spatial location. While $f$ is unknown altogether, we are often interested in estimating only a linear functional of it $\mathbf{C} f$, where $\mathbf{C}: \mathcal{H}</em>} \rightarrow \mathbb{R}^{p}$. In the context of biological surveillance, this can be, e.g., a spatial average over certain locations. Another example are values at specific locations, where the rows of $\mathbf{C<em i="i">{i:}=\Phi\left(x</em>$ may even be the identity (i.e., the goal is to estimate $f$ completely). We observe $f$ via noisy evaluations at specific states $x$ while performing an action $a$,}, \cdot\right)^{\top}$, with $\left{x_{i}\right}_{i}$ being the locations of interest. For simpler models (finite-dimensional $f$ ), $\mathbf{C</p>
<p>$$
y=f^{\top} \Phi(x, a)+\epsilon
$$</p>
<p>where $\epsilon$ is random noise realization such that $\mathbb{E}[\epsilon]=0$.
Exploration Consider an example, where we want to deploy an agent (e.g., a drone) that explores the environment efficiently to learn $\mathbf{C} f$ from observations at $(x, a)$. Due to the kernel regularity assumption (RKHS), we know that similar $(x, a)$ lead to similar values of $f$, and to understand $\mathbf{C} f$, intuitively, we should explore diverse landscape features instead of evaluating similar $(x, a)$. In the species distribution example, these are illustrated via the pictograms in Fig. 1a and Fig. 1b. We would like to pick a small subset of states covering all pictograms there, however, we cannot choose states arbitrarily: the only way we can choose them is by following the Markov transition rule, which then generates trajectories as in Fig. 1b. In fact, to learn $\mathbf{C} f$ effectively, we need to visit the states in proportion to the heat map in Fig. 1a - optimal visitation of the states. In this work, we develop a method that picks policies sequentially such that their trajectories</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>(as in Fig. 1b) converge to the optimal visitation distribution over states and actions as in Fig. 1a, detailed below.
Contribution We design a novel algorithm: MARKOVDESIGN for adaptive experiment design, where we collect data by deploying policies in a known Markov chain for a fixed horizon. We notice that classical experimental design objectives can be reformulated such that they depend on state-action visitations and show how they can be efficiently optimized using convex reward reinforcement learning which reduces to a sequence of classical planning problems. The resulting algorithm relies on convex optimization techniques, and is simple to implement. We prove that our algorithm converges to the optimal allocation over trajectories generated by selected policies and propose several variants of the algorithm with varying levels of approximation and analyze their performance. Additionally, we demonstrate our algorithm on two real-world experimental design problems with Markov chain structure.</p>
<h2>2 Background and Related Work</h2>
<p>Environment We assume a known set of environment states $\mathcal{X}$ and actions $\mathcal{A}$, and a known Markov transition model $p\left(x^{\prime} \mid x, a\right): \mathcal{X} \times \mathcal{A} \rightarrow \mathcal{X}$. We interact with the environment by executing a policy, where actions are sampled $a_{h} \sim \pi_{h}\left(a \mid x_{h}\right)$, for horizon $H$, resulting in trajectories $\tau=$ $\left{x_{0}, a_{0}, x_{1}, a_{1}, \ldots x_{H-1}, a_{H-1}\right}$, where $\tau \in \mathcal{T}$ is the set of all possible trajectories where the horizon is $H$. The space of all policies is denoted $\Pi$. A policy induces a probability distribution over trajectories $\eta$, belonging to the feasible distribution set $\mathcal{P} \subseteq \Delta_{p}$, a subset of all possible distributions over trajectories. A distribution supported only on one trajectory $\tau$ is denoted $\delta_{\tau}$. As in Fig. 1c, note that a policy $\pi$ is related to a specific $\eta \in \mathcal{P}$, but it is generally impossible to associate an arbitrary distribution over trajectories to a policy.</p>
<h3>2.1 Related Work</h3>
<p>Experiment Design Optimal experimental design (OED) (Pukelsheim, 2006) has a rich history, with modern applications in protein design (Romero et al., 2013), Poisson sensing (Mutný and Krause, 2022b), and many others (cf., Chaloner and Verdinelli, 1995). It is closely related to active learning (Settles, 2009) and bandit optimization (Szepesvari and Lattimore, 2020). Traditionally, in OED, the action (experiment) can be chosen deterministically, and the space of actions is not large. Scaling to many actions has been addressed in the bandit literature in the context of combinatorial bandits (Zou et al., 2014; Talebi et al., 2013; Jourdan et al., 2021), where the action structures are, e.g., paths in a graph, or spanning trees. While the goal is optimization (i.e., finding $\arg \max _{x} f(x)$ ), the techniques are similar in spirit. They circumvent the large action set problem by having an efficient oracle for incrementally increasing the allocation, e.g., the Dijkstra algorithm. Similarly, we provide a way to incrementally increase the visitations of state-actions, and our oracle relies on efficient Bellman optimality solvers.
Convex RL Our algorithms utilize the recent framework of reinforcement learning with convex reward functions due to Hazan et al. (2019) and Zahavy et al. (2021). These works propose a way to find a stationary policy $\pi^{<em>}$ which minimizes a certain convex reward function, which, in light of our work, can be uncertainty about $f$. Exploration with convex rewards in the context of MDPs is investigated by Tarbouriech and Lazaric (2019) in non-episodic context with ergodicity assumption without the notion of similarity as studied here. Different from their work, we consider more complex objectives which incorporate the structure of RKHS, exploiting a connection with the field of experiment design. Shortly after the publication of the first version of this work, Wagenmaker and Jamieson (2022) introduced a similar experiment design problem on Markov chains with the aim of developing instance-optimal RL algorithms. Unlike other works, we focus also on how the optimal policy $\pi^{</em>}$ should be executed. Replicating the policy $\pi^{<em>}$ for multiple episodes leads to potentially the same states being visited and a suboptimal convergence to the optimal choice of state-actions as we show. Instead, we incrementally construct a sequence of policies that take the history of executed trajectories into account and such that convergence of the empirical visitation distribution to the optimal visitation distribution of policy $\pi^{</em>}$ is faster than resampling. If $f$ is interpreted as a reward function, our work can be seen as a reward learning problem similar to Lindner et al. (2021). The difference here is that our exploration is planned for an entire episode instead of assuming an access to arbitrary state-action oracle (simulator) as in their work, which makes our approach more widely applicable. Episodic planning for reward learning with different assumptions on $f$ and methods is considered by Belogolovsky et al. (2021).</p>
<h3>2.2 Estimation</h3>
<p>We estimate $\mathbf{C} f$ using a regularized least squares estimator, where the estimated $\hat{f}<em t="t">{t}$ after $t$ episodes is the solution to $\hat{f}</em>=\arg \min <em k="k">{f \in \mathcal{H}</em>+\lambda|f|}} \sum_{i=1}^{t} \sum_{(x, a) \in \tau_{i}}\left(f^{\top} \Phi(x, a)-\right.$ $\left.y_{a, x, i}\right)^{2<em k="k">{\mathcal{H}</em>$. This estimator is motivated by the famed Gauss-Markov theorem, as it minimizes the second moment of residuals (see Mutný and Krause, 2022a, for details). Due to the representer theorem (cf., Schölkopf et al., 2001), the above problem can be solved even if the RKHS is infinite-dimensional.
We aim to understand the uncertainty of the estimate $\mathbf{C} \hat{f}$ as a function of trajectories taken. To do so, as common in optimal experiment design, we consider the second moment of the residuals $\mathbf{C}(\hat{f}-f)$ denoted as $\mathbf{E}}}^{2}$. In order to estimate the functional $\mathbf{C} f$, we simply use $\mathbf{C} \hat{f<em _epsilon="\epsilon">{T}=\mathbb{E}</em>, i \in[T]$ :}[\mathbf{C}(f-$ $\hat{f})(f-\hat{f})^{\top} \mathbf{C}^{\top}]$, where the expectation is understood over the noise realization $\epsilon$ (see Eq. (1)). Evaluating it, we can see its dependence on the executed trajectories $\tau_{i</p>
<p>$$
\mathbf{E}<em i="1">{T} \preceq \mathbf{C}\left(\left(\sum</em>
$$}^{T} I\left(\tau_{i}\right)+\lambda \mathbf{I}\right)^{-1}\right) \mathbf{C}^{\top</p>
<p>where we call $I(\tau): \mathcal{H}<em k="k">{k} \rightarrow \mathcal{H}</em>$ the information} \times \mathcal{H}_{k</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: a) Optimal state-action visitation distribution that maximizes the information about species distribution as a function of state features (indicated via pictograms). Different pictograms represent sectors, and hence different values of the unknown $f$. Note that some sectors are not covered frequently, as the information about the function value in these sectors is already extracted from different sectors with the same pictogram. b) Displayed are three trajectories from three different policies in three episodes, seeking to match the optimal state-action distribution depicted in b). Note that the trajectories move from the initial (lower left) to the terminal state (upper right) and overlap in the purple-colored states. c) Relation between the sets of objects we work with. Given a policy $\pi$, we can calculate state-action visitations and probabilities over trajectories it generates. Not every state-action distribution nor probability over trajectories can be realized by a policy. Also, state-action visitations $d$ and probability over trajectories $\eta$ are related via a linear conversion map $\mathbf{Z}$. A single executed trajectory defines a Dirac delta on the probability space $\mathcal{P}, \delta_{\tau}$.</p>
<p>matrix for a trajectory $\tau$,</p>
<p>$$
I(\tau)=\sum_{(a, x) \in \tau} \frac{1}{\sigma_{a, x}^{2}} \Phi(x, a) \Phi(x, a)^{\top}
$$</p>
<p>and $\sigma_{a, x}$ denotes the variance of random variable $\epsilon$. The derivation follows a straightforward application of optimality conditions, representer theorem, and use of bounded norm assumption $|f|<em k="k">{\mathcal{M}</em>$. A very detailed derivation is given by Mutný and Krause (2022a).}} \leq \frac{1}{4</p>
<p>By appropriate choice of trajectories $\tau_{i}$, we can minimize the second moment of residuals in Eq. (2). As alluded to in the beginning, unless the system is completely deterministic, we cannot directly choose trajectories $\tau_{i}$. Even if we could, this space of trajectories may grow exponentially in $|S|$ and $|A|$. Instead, we first reformulate the objective via a fractional allocation over trajectories $\eta \in \Delta_{p}$, essentially by inserting $1 / T$ into Eq. (2),</p>
<p>$$
\mathbf{E}<em T="T">{T}\left(\eta</em>}\right) \preceq \underbrace{\frac{1}{T} \mathbf{C}\left(\left(\sum_{\tau \in T} I(\tau) \eta_{T}(\tau)+\frac{\lambda}{T} \mathbf{I}\right)^{-1}\right)<em T="T">{\text {covariance matrix } \boldsymbol{\Sigma}\left(\eta</em>
$$}\right)} \mathbf{C}^{\top</p>
<p>where we identified $\eta_{T}=\frac{1}{T} \sum_{i=1}^{T} \delta_{\tau_{i}}$. Now the goal of experiment design is to invest the $T$ samples (trajectories) in such a way that the matrix in Eq. (4) is as small as possible. Since the RHS is matrix-valued, a classical approach in OED is to scalarize the objective by one of the well-known scalarization functions (see 1). A scalarization function $s: \mathbb{S}^{p \times p} \rightarrow \mathbb{R}$ acts on the space of PSD matrices, and is convex. These objectives usually correspond</p>
<p>Table 1: Selected design objectives with their interpretations and name (Fedorov and Hackl, 1997).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Design</th>
<th style="text-align: left;">Represents</th>
<th style="text-align: left;">$s(\boldsymbol{\Sigma})$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">D</td>
<td style="text-align: left;">information</td>
<td style="text-align: left;">$-\log \operatorname{det}\left(\boldsymbol{\Sigma}^{-1}\right)$</td>
</tr>
<tr>
<td style="text-align: left;">A</td>
<td style="text-align: left;">parameter error ${ }^{2}$</td>
<td style="text-align: left;">$\operatorname{Tr}[\boldsymbol{\Sigma}]$</td>
</tr>
<tr>
<td style="text-align: left;">E</td>
<td style="text-align: left;">worst projection error ${ }^{2}$</td>
<td style="text-align: left;">$\lambda_{\max }[\boldsymbol{\Sigma}]$</td>
</tr>
</tbody>
</table>
<p>to a specific type of uncertainty, such as the squared error, entropy, or predictive error. Upon scalarization we have $s\left(\mathbf{E}_{T}\right) \leq \frac{1}{T} s(\boldsymbol{\Sigma}(\eta))$, where $s(\boldsymbol{\Sigma}(\eta))$ represents the constant with which the budget $T$ is invested. We seek to have the lowest possible constant with $\eta^{<em>}, s\left(\boldsymbol{\Sigma}\left(\eta^{</em>}\right)\right)$.</p>
<h2>3 Experiment Design: Problem Statement</h2>
<p>Our goal is to design and execute a sequence of $T$ policies $\pi_{i}, i \in[T]$, each one for a single episode, for a fixed number of steps $H$ (episode length), such that we reduce the uncertainty of $\mathbf{C} \hat{f}$ efficiently. Directly optimizing probability distributions over trajectories in $F(\eta)$ is intractable as their description size is $(|\mathcal{X}||\mathcal{A}|)^{H}$ in the worst case. Instead, we focus on the building blocks of trajectories: states and actions, and their visitations. We design policies that visit the states and actions such that uncertainty is maximally reduced as measured by the function $F$ and show that for experiment design objectives the complexity of the problem can be reduced to optimization over distributions with size $|\mathcal{X}||\mathcal{A}| H$.</p>
<h3>3.1 State-action polytope</h3>
<p>To optimize the choice of policies (and hence trajectories), we work with an object that accumulates the information about state-visitations: the state-action visitation distribution. For ease of exposition, assume that $\mathcal{X}$ and $\mathcal{A}$</p>
<p>are discrete sets. The space of all possible state-action visitation distributions with a known initial distribution $d_{0}$ is the state-action visitation polytope,</p>
<p>$$
\begin{aligned}
&amp; \mathcal{D}<em h="h">{h}:=\left{d</em>(a, x)=1\right. \
&amp; \left.\sum_{a} d_{h}\left(x^{\prime}, a\right)=\sum_{x, a} d_{h-1}(x, a) p\left(x^{\prime} \mid x, a\right)\right}
\end{aligned}
$$} \mid d(x, a) \geq 0, \sum_{a, x} d_{h</p>
<p>For reference see Puterman (2014) or Neu and Pike-Burke (2020). The average state-action visitation polytope over a time horizon $H$ is the central constraint set used in this work, which we denote by $\mathcal{D}:=\left{d=\frac{1}{H} \sum_{h=1}^{H} d_{h} \mid d_{i} \in \mathcal{D}<em h="h">{i} \forall i \in\right.$ $[H]}$. Notice that for fixed horizon, the state-action visitation distribution $d</em>$ and policy $\pi$,}$ for each timestep $h$ is different. A useful approximation is to let $H$ be very large. In that case, a reasonable approximation to this polytope is the average-case polytope, where $\overline{\mathcal{D}}=\left{d \mid d(x, a) \geq 0, \sum_{a, x} d(a, x)=\right.$ $\left.1, \sum_{a} d\left(x^{\prime}, a\right)=\sum_{x, a} d(x, a) p\left(x^{\prime} \mid x, a\right)\right}$, and the distribution $d$ for each $h$ is the same. The distributions $d_{h}$ can be generated via transition operator $P_{\pi_{h}</p>
<p>$$
P_{\pi_{h}}\left(x, x^{\prime}\right):=\sum_{a} p(x \mid a, x) \pi_{h}(a \mid x)
$$</p>
<p>to get $d_{h}(x, a)=\left(\prod_{i=1}^{h} P_{\pi_{h}} d_{0}(x)\right)$. Conversely, to match the state-action visitation by executing a policy, we can obtain a policy by marginalization:</p>
<p>$$
\pi_{h}(a \mid x)=\frac{d_{h}(a, x)}{\sum_{a} d_{h}(a, x)} \text { and } \bar{\pi}(a \mid x)=\frac{d(a, x)}{\sum_{a} d(a, x)}
$$</p>
<p>where the second case corresponds to the average case. In the latter case, the induced policy $\pi$ is stationary, while the former is non-stationary. We will drop the subscript $h$ from $d$ and $\pi$, and instead refer to them as $d_{\pi}$ and $\pi$, as the treatment for average and fixed horizon polytopes is essentially the same - they differ only in the form of marginalization.
Visitations and Trajectories Executing a policy $\pi$ for $H$ steps leads to a trajectory $\tau=\left{x_{0}, a_{0}, \ldots x_{H-1}, a_{H-1}\right}$. Since both the policy and/or the environment can be stochastic, $\pi$ induces a distribution over trajectories of length $H$, which we denote by $\eta_{\pi} \in \mathcal{P}$. We see that $\pi$ induces both $\eta_{\pi}$ and $d_{\pi}$, and in turn $d_{\pi}$ can be matched by a specific $\pi$. The distributions $d_{\pi}$ and $\eta_{\pi}$ can be related using a linear map $\mathbf{Z}$ that we refer to as a conversion map (see Fig. 1c) as follows,</p>
<p>$$
\begin{aligned}
d_{\pi}(a, x) &amp; =\sum_{\tau \in \mathcal{T}} \eta_{\pi}(\tau) \sum_{a^{\prime}, x^{\prime} \in \tau} \delta_{a=a^{\prime}, x=x^{\prime}} \
&amp; =\sum_{a, x \in \mathcal{A} \times \mathcal{X}} \sum_{\tau \in \mathcal{T}} #<em _pi="\pi">{(a, x \in \tau)} \eta</em>}(\tau)=\mathbf{Z<em _pi="\pi">{a, x} \eta</em>
\end{aligned}
$$</p>
<p>where $#_{(a, x \in \tau)}$ refers to the number of times a state combination $(x, a)$ appears in $\tau$. The relation will be important as it allows us to directly optimize the distribution over trajectories (and hence policies) by optimizing state-action visitations.</p>
<h3>3.2 Loss Function</h3>
<p>Using the conversion map $\mathbf{Z}$, we can relate the objectives $F$ in terms of the empirical distribution of trajectories as $\eta$ with objective $U$ depending only on state-action visitations:</p>
<p>$$
\min <em _pi="\pi">{\eta</em>\right):=\min } \in \mathcal{P}} F\left(\eta_{\pi<em _pi="\pi">{\eta</em>\right)=\min } \in \mathcal{P}} U\left(\mathbf{Z} \eta_{\pi<em _pi="\pi">{d</em>\right)
$$} \in \mathcal{D}} U\left(d_{\pi</p>
<p>where $d_{\pi}$ corresponds to the average state-action visitation of the policy $\pi$ (which needs to be neither Markovian nor stationary). Functions $U$ and $F$ are essentially the same; the only difference is in terms of the decision variables involved. These variables are related (in one way) via a linear map $\mathbf{Z}$. This is possible due to the definition of $F\left(\eta_{\pi}\right)$ via the information matrix $I(\tau)$, which is additive in terms of state-action pairs in a trajectory $\tau$. For a formal derivation please see Lemma 1 in Appendix A.1.
Optimum We want that the empirical distribution over executed trajectories $\eta_{T}=\frac{1}{T} \sum_{t=1}^{T} \delta_{\tau_{t}}$ converges to the optimum of (8), $\eta^{<em>} \in \arg \min <em _tau__t="\tau_{t">{\eta \in \mathcal{P}} F(\eta)$, where $\delta</em>$. The optimum distribution over trajectories $\eta^{}}$ is a delta-function supported on the trajectory $\tau_{t</em>}$ corresponds to a fixed non-stationary (or stationary for the average case treatment) policy which comes from the set of optimal policies $\Pi^{*}$ equivalent in terms of their value of $F$ (any one of them can be chosen).</p>
<p>By picking multiple different policies, each different in each episode, one might hope that one can perform better than a single optimal policy $\pi^{<em>}$, that induces $\eta^{</em>}$. However, in expectation, it cannot be improved upon due to convexity of $U$, as</p>
<p>$$
\begin{aligned}
&amp; \mathbb{E}<em i="i">{\tau</em>} \sim \pi_{i}}\left[F\left(\eta_{T}\right)\right] \stackrel{(5)}{=} \mathbb{E<em i="i">{\tau</em>} \sim \pi_{i}}\left[U\left(\mathbf{Z} \eta_{T}\right)\right] \stackrel{\text { len. }}{=} U\left(\mathbf{Z} \mathbb{E<em i="i">{\tau</em>\right]\right) \
&amp; =U\left(\frac{\mathbf{Z}}{T} \sum_{t=1}^{T} \mathbb{E}} \sim \pi_{i}}\left[\eta_{T<em t="t">{\tau</em>\right)
\end{aligned}
$$} \sim \pi_{t}}\left[\delta_{\tau_{t}}\right]\right) \geq U\left(\frac{\mathbf{Z}}{T} \sum_{t=1}^{T} q_{t}\right) \stackrel{\text { opt. }}{=} U\left(\mathbf{Z} \eta^{*</p>
<p>where $q_{t}$ is the induced probability over trajectories due to the policy $\pi_{t}$. The last inequality follows as $\frac{1}{T} \sum_{t=1}^{T} \mathbf{Z} q_{t}$ is a convex combination of elements inside the state-action polytope, which is convex, and hence there exists a unique $\mathbf{Z} \eta$ which can replicate it. Note that given $q_{t}$, we can find $d_{\pi_{t}}=\mathbf{Z} q_{t}$ that in turns corresponds to the marginalized $\pi_{t}$. As $\eta^{<em>}$ is the optimum over $\mathcal{P}, U(\mathbf{Z} \eta)$ is larger than $U\left(\mathbf{Z} \eta^{</em>}\right)$. The above calculation reveals that the value $F\left(\eta^{*}\right)$ is a good benchmark in expectation, and cannot be improved upon in expectation.</p>
<h2>4 Convex RL: Non-adaptive Design</h2>
<p>Given a convex objective $U(d)$ (related to $F(\eta)=U(\mathbf{Z} \eta)$ ) over polytope $d \in \mathcal{D}$ (either the average or fixed horizon average polytope), we can solve for a policy $\pi^{<em>}$ which achieves the optimum of $d^{</em>}$ due to seminal works of Hazan et al. (2019) and Zahavy et al. (2021). We refer to it as convex Reinforcement Learning (RL). It proceeds by solving a sequence of classical RL problems with a linear reward</p>
<p>function that corresponds to the gradient of $U(d)$. We use it as a subroutine in our adaptive algorithm that we explain in Section 5 Using this method, we can find a fixed policy $\pi^{<em>}$ that matches the optimum value $U\left(d^{</em>}\right)=F\left(\eta^{*}\right)$. It does so by constructing a convex combination of base policies.
Mixture Policy We refer to a convex combination of policies as mixture policy. It is a tuple consisting of a set $n$ base policies, and set of $n$ positive weights $\alpha_{i}$ that sum to one, $\pi_{\text {mix }, n}=\left{\left(\alpha_{i}, \pi_{i}\right)\right}<em j="j">{i=1}^{n}$. Such a mixture can be executed by first sampling an index $j \in[n]$ with probability equal to $\alpha</em>$. Note that like any other policy, a mixture policy can be summarized by a single policy via marginalization as in Eq. (6).}$, and then evaluating policy $\pi_{j}$ for $H$ rounds. A property of mixture policies is that the state-action probabilities follow the convex combination of the policies such as $d_{\pi_{\text {mix }, n}}=\sum_{i=1}^{n} \alpha_{i} d_{\pi_{i}</p>
<h3>4.1 Convex RL as Frank-Wolfe</h3>
<p>Convex RL can be solved via the Frank-Wolfe algorithm - it incrementally constructs a mixture policy whose stateaction distribution $d_{\pi_{\text {mix }, n}}$ converges to $d^{*}$ as the number of mixture components $n$ increases. There are two distinct steps. The first is called density estimation, corresponding to the evaluation of the gradient in Frank-Wolfe, and the second is policy search, corresponding to the linear minimization oracle in Frank-Wolfe.
Density Estimation Given a mixture policy, the density estimation oracle needs to estimate $d_{\pi_{\text {mix }}}$. For discrete Markov chains (i.e., tabular MDPs), this amounts to a straightforward application of the operator $P_{\pi}\left(x^{\prime}, x\right)$ to $d_{0}$ as in (5) for every component of the mixture policy. In particular for the fixed horizon setting $d_{\pi_{i}}(x)=\frac{1}{H} \sum_{h=1}^{H} \prod_{j=1}^{h} P_{\left{\pi_{i}\right}<em 0="0">{j}} d</em>(x)$ where subscript $j$ denotes the iteration within the episode as $\pi$ is nonstationary for a fixed horizon. The overall mixture state-action visitation is the convex combination of all mixture components. Beyond discrete Markov chains, any state-visitation density can be estimated via sampling. Note that due to knowledge of transition operator $P$, this means simulation, not interaction with the environment.
Policy Search Having estimated $d_{\pi_{\text {mix }, n}}$ with $n$ elements, we can now add an element $\pi_{n+1}$ such that the objective $U$ decreases. We linearize the objective $\nabla U(d)$ and solve the linear minimization oracle:</p>
<p>$$
d_{\pi_{n+1}}=\underset{d_{\pi} \in \mathcal{D}}{\arg \min } \sum_{x, a} \nabla U\left(d_{\pi_{\text {mix }}}\right)(x, a) d(x, a)
$$</p>
<p>This is a classical reinforcement learning problem, where $\nabla U\left(d_{\pi_{\text {mix }}}\right)$ plays the role of the reward function (Puterman, 2014). Hence, it can be solved by any RL solver such as value/policy iteration or linear programming. The newly found $d_{\pi_{n+1}}$ defines $\pi_{n+1}$ or vice versa depending on the RL solver used. The weight of new policy $\pi_{n+1}$, $\alpha_{n+1}$, is found via a line search as in Algorithm 1 or any other convergent step-size scheme for the Frank-Wolfe algorithm (Jaggi, 2013). The new mixture policy is then $\pi_{\text {mix }, n+1}=\left{\left(\left(1-\alpha_{n+1}\right) \alpha_{i}, \pi_{i}\right)\right}<em n_1="n+1">{i=1}^{n} \cup\left{\alpha</em>$}, \pi_{n+1}\right}$. This algorithm is summarized in Alg. 1 as Convex RL. Hazan et al. (2019) prove that in order to converge to $\epsilon$ optimality in terms of $U$, under the regularity conditions as in Assumption 1 (see Sec. 6), one needs $n \geq \mathcal{O}\left(\frac{L}{\epsilon} \log (1 / \epsilon)\right)$ steps with step size $\alpha_{n}=\frac{\epsilon}{L}$ for all $n .{ }^{2</p>
<h2>5 Markov-Design: Adaptive Design</h2>
<p>In the previous section, we discussed how to find a single policy such that its state-action visitation probability minimizes a certain convex functional $U$. However, ultimately our objective depends on the empirical distribution $\eta_{T}=\frac{1}{T} \sum_{t=1}^{T} \delta_{\tau_{t}}$ of the executed trajectories. We now consider ways to generate trajectories: first the straightforward NON-ADAPTIVE methods that execute a single policy multiple times, and then we describe the ADAPTIVE methods, which lie at the core of the contribution.</p>
<h3>5.1 Resampling from mixture $\pi^{*}$</h3>
<p>Variant: NON-ADAPTIVE As our final density is in the form of a mixture policy, we can either sample a component for each episode or summarize the policy by marginalization and execute it multiple times resulting into empirical eta $<em T="T">{T}$. The value $F\left(\eta</em>\right)\right|}\right) \rightarrow F\left(\eta^{*}\right)$ with probability $1-\delta$, depending on the gradient norm $B=\left|\nabla F\left(\eta_{T<em j="j">{\infty}$, as $\mathcal{O}(B \log (1 / \delta) / \sqrt{T})$. In general, this strategy suffers from the coupon collector problem, where the same trajectories are resampled with non-zero probability. Namely this manifest itself in number of $T$ needed such that $B$ is well-behaved. For more details and formal statements, see Appendix B. 2 and B.3.
Variant: TRACKING The NON-ADAPTIVE variant is wasteful in that there exists a non-zero probability that the same base policy is executed multiple times, leading to similar trajectories (and hence redundant experiments). This can be avoided via tracking - closely following the empirical distribution of executed policies to the mixture found by convex RL. Namely, we choose the base policy $\pi</em>$ such that $j=\arg \max <em i="i">{i}\left(\alpha</em>}-\hat{\alpha<em i="i">{i}\right)$, where $\hat{\alpha}</em>$ corresponds to the empirical distribution of executed policies. This, however, can be wasteful, as it depends on how the mixture policy is decomposed. If the mixture contains a lot of dissimilar policies, then this method can be very competitive. On the other hand, if the components are all very similar, then this is as wasteful as the NON-ADAPTIVE variant.</p>
<h3>5.2 Adaptive Optimization on $\mathcal{P}$</h3>
<p>A more elegant way to avoid resampling is to inform the choice of the next policy with the information about the executed trajectories from previous steps. To do this, we incrementally estimate the empirical distribution of the visited states from past trajectory distributions as $\mathbf{Z} \eta_{t}$,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>where $\eta_{t}=\frac{1}{t} \sum_{i=1}^{t} \delta_{\tau_{i}}$. We then seek an addition to this empirical measure $d=\mathbf{Z} q_{t}$ as $\frac{1}{1+t} d+\frac{t}{1+t} \mathbf{Z} \eta_{t}$ which minimizes the objective $G_{t}(d)=U\left(\frac{1}{1+t} d+\frac{t}{1+t} \mathbf{Z} \eta_{t}\right)$. Notice that the weighting $\frac{1}{1+t}$ is chosen such that the new empirical distribution over trajectories $\eta_{t+1}=\frac{1}{1+t} \delta_{t}+\frac{t}{1+t} \eta_{t}$ where $\delta_{t} \sim q_{t}$ remains still the average allocation over the executed trajectories. It is in a sense a greedy one-step change in the allocation that brings us closer to the optimal allocation.
Variants: EXACT and ONE-STEP Depending on how we identify the distribution over trajectories $q_{t}$ (and the associated $d$ ), we distinguish two variants. The EXACT variant finds the exact element such that $q_{t}=\arg \min <em t="t">{q \in \mathcal{P}} U\left(\frac{1}{1+t} \mathbf{Z} q</em>$ as summarized in Algorithm 1. The ONE-STEP can be seen as a poor man's version of EXACT, although there seems to be empirical benefits for the former as we will see.
Uncertain Objectives In some cases, the objective $F$ depends on an unknown quantity such as unknown variance $\sigma_{\pi, \alpha}^{2}$. We can then optimize the worst case over the unknown compact parameter set $\Gamma$, as $F(\eta)=\sup }+\frac{t}{1+t} \mathbf{Z} \eta_{t}\right)$. The ONE-STEP variant simply runs the Convex MDP framework for one step, and plays the first component in the mixture policy $\pi_{\text {mix }<em _alpha_="\alpha," s="s">{\sigma</em>(\eta)$. Since $F$ is convex and $\Gamma$ compact, the objective remains convex. Further details are given in Appendix B.5.} \in \Gamma} F_{\sigma_{\alpha, s}</p>
<h2>6 Convergence Theory</h2>
<p>The ONE-STEP variant is closely related to the Frank-Wolfe algorithm on the space of trajectory distributions and the theory we develop for its convergence is largely based on it. The convergence cannot be linear unless step sizes are adjusted (Lacoste-Julien and Jaggi, 2015), however, our $\frac{1}{1+t}$ step-sizes are determined by the one-step update specific to this setting, and cannot be changed.
We utilize the same convergence proof for the EXACT and ONE-STEP variant, although the two have different convergence behavior on the real problems. The regularity conditions under which we show convergence are summarized bellow.</p>
<p>Assumption 1 (Regularity). Let $F: \Delta_{p} \rightarrow \mathbb{R}$ (and likewise $U$ ) be convex, differentiable, locally Lipschitz continuous in $|\cdot|_{\infty}$, and locally smooth as,</p>
<p>$$
F(\eta+\alpha h) \leq F(\eta)+\nabla F(\eta)^{\top} h+\frac{L_{\eta, \alpha}}{2}|h|_{2}^{2}
$$</p>
<p>for $\alpha \in(0,1)$ and $\eta, h \in \Delta_{p}, L:=\max <em _alpha="\alpha" _eta_="\eta,">{\eta, \alpha} L</em>$.
Note that the above differs from classical smoothness assumption, which we refer to as global smoothness. On its own is not sufficient to prove the desired rate of convergence we observe in practice. The problem is that the experiment design objectives can have global smoothness $L$ proportional to $\frac{T}{k}$, which does not suffice to prove convergence. However, the usual behavior is that after a few initial steps (eg. ca. $\mathcal{O}(p)$ ), the local smoothness constants
drops to a small number. On top of that some objectives might not be smooth at all like E-design. To remedy both of these, one can apply classical smoothing technique due to Nesterov (2005), where the smoothed function $F_{\mu}$ is $\mu$ close to $F$, but with smoothness $L_{\mu}=L /(1+\mu L) \leq \frac{1}{\mu}$. Applying our algorithm on objective $F_{\mu}$ gives order $T$ optimal convergence rate with high probability.
Theorem 1 (Convergence EXACT and ONE-STEP). Under Assumption 1, with the smoothed objective using $\mu=$ $\sqrt{\log T / T}$, the EXACT and ONE-STEP variants satisfy,</p>
<p>$$
F\left(\eta_{T}\right)-F\left(\eta^{*}\right) \leq \mathcal{O}\left(\frac{1}{T} \sqrt{\sum_{t=1}^{T}\left|\nabla F\left(\eta_{t}\right)\right|_{\infty}^{2} \log (T / \delta)}\right)
$$</p>
<p>with $1-\delta$ probability over transition model and policy.
Proofs are postponed to the Appendix B.4, and Nesterov (2005) smoothing technique is reviewed in Appendix A. 2 for completness. With a reasonable upper bound on the gradient of $F$, the convergence is as $O\left(\frac{1}{\sqrt{T}}\right)$ even for non-smooth objectives.</p>
<p>However, the smoothing is not necessary as our experimental results point to. Particularly its use hampers the potentially fast convergence in expectation, which is the driving term in practice, especially for determinisitic Markov chains. Therefore, we state the next result in terms of the local Lipschitz constant $L_{\eta_{t}, 1 / t}$, which we conjecture, depend only logarithmically on $T$, leading to convergence $\mathcal{O}\left(\frac{\log T}{T}\right)$.
Theorem 2 (Convergence EXACT and ONE-STEP). Under Assumption 1, the EXACT and ONE-STEP variants satisfy</p>
<p>$$
\mathbb{E}\left[F\left(\eta_{t}\right)\right]-F\left(\eta^{*}\right) \leq \mathcal{O}\left(\frac{1}{t} \sum_{k=1}^{T} \frac{L_{\eta_{k}, 1 / k}}{1+k}\right)
$$</p>
<p>for $t \leq T$ with expectation over transition model and policy.
Showing non-trivial bounds on the smoothness and gradient remains a challenging problem not addressed even in the context of classical experiment design (Zhao and Freund, 2022). The only existing approaches that address this follow a particular initialization scheme which is infeasible for kernelized regime, and specialized for D-design (Todd, 2016). Nevertheless, we conjecture that for example for D-design, the gradient and smoothness are quickly of order polylog $(T)$. Even with a proper initialization, one of the main challenges hampering the analysis of the algorithm is that the algorithm has fixed step-size $1 /(1+t)$ and is not monotonically decreasing like other analysis of Frank-Wolfe (Carderera et al., 2021).
The consequence of the above theorems is that the suboptimality decreases as $F\left(\eta_{T}\right) \leq F\left(\eta^{<em>}\right)+T^{-1 / 2}$, which in turn means that the second moment of residuals in scalarization (see Eq. 4), $s\left(\mathbf{E}_{L}\right) \leq \frac{F\left(\eta^{</em>}\right)}{T}+\mathcal{O}\left(T^{-3 / 2}\right)$, where the leading term in terms of $T$ depends on the optimal constant. Analog statements hold in expectation albeit with $\mathcal{O}\left(T^{-2}\right)$.</p>
<h1>Algorithm 1 MARKOV-DESIGN</h1>
<p>Require: known Markov chain, $p\left(x^{\prime} \mid x, a\right)$, Objective $F(U)$, Number of episodes $T$
1: while $t \leq T$ do
2: $\quad$ Convex RL: solving $\min <em t="t">{d \in \mathcal{D}} G</em> d\right)$
3: $\quad \pi_{\text {mix }, 1}=\pi_{t} ; i=1, d_{\pi_{\text {mix }, 1}}=\mathbf{Z} \eta_{t}$
4: repeat $i=i+1$
5: $\quad \varpi_{i}=\arg \min }(d):=U\left(\mathbf{Z} \eta_{t} \frac{t}{t+1}+\frac{1}{t+1<em a="a" x_="x,">{\pi \in \Pi} \sum</em>(x, a)\right) \quad \triangleright$ RL problem
6: $\quad d_{\varpi_{i}}=$ Density Estimation $\left(\varpi_{i}\right) \quad \triangleright$ keep track of visitations
7: $\quad \alpha_{i}=\arg \min } d_{\pi}(x, a) \nabla_{x, a} G_{t}\left(d_{\pi_{\text {mix }, i}<em t="t">{\alpha \in \mathbb{R}} G</em>\right) \quad \triangleright$ line search
8: $\quad \pi_{\text {mix }, i+1}=\left(1-\alpha_{i}\right) \pi_{\text {mix }, i} \cup\left{\left(\alpha_{i}, \varpi_{i}\right)\right} \quad \triangleright$ update mixture policy
9: Update $d_{\pi_{\text {mix }, i+1}}=\alpha_{i} d_{\pi_{\text {mix }, i}}+\left(1-\alpha_{i}\right) d_{\varpi_{i}} \quad \triangleright$ update mixture policy visitations
10: until convergence
11: $\quad$ Choose $\pi_{t}= \begin{cases}\text { Marginalize } &amp; \pi_{\text {mix }} \ \varpi_{1} &amp; \text { if VARIANT = EXACT } \ &amp; \text { if VARIANT = ONE-STEP }\end{cases}$
12: Interaction
13: Sample trajectory from $\tau_{t} \sim \pi_{t}$ (also as $\delta_{\tau_{t}} \sim q_{t}$ )
14: $\quad \mathbf{Z} \eta_{t+1}=\mathbf{Z} \frac{t}{t+1} \eta_{t}+\mathbf{Z} \frac{t}{t+1} \delta_{\tau_{t}} \quad \triangleright$ keep track of visited states
15: end while
}\left(\alpha d_{\pi_{\text {mix }, i}}+(1-\alpha) d_{\varpi_{i}<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Gridworlds experiment: From a) to c) we increase the stochasticity of the environment. We plot the median with $10 \%$ and $90 \%$ quantiles over 20 reruns of the method. Notice that the adaptive methods perform much better than the non-adaptive ones, and very quickly optimize the objective. For deterministic systems, the ONE-STEP method is deterministic as it coincides with the greedy method over a set of deterministic policies. Surprisingly, the ONE-STEP method dominates the EXACT method for short horizons despite EXACT demonstrating linear convergence in later stages. The non-adaptive method exhibits slow convergence, while random selection does not converge.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Beilschmiedia, and pharmacokinetics experiment: We report median of 20 reruns with $10 \%$ and $90 \%$ quantiles. In a) and b) we report the objective value as a function of episodes $t$ for $\sigma_{x, a}$ with a global upper bound, and one where we update the confidence set over $\sigma_{x, a}$ after each episode, respectively. Notice that we can converge to the objective faster in b) than a) due to better exploration policy (as it is more informed about values of variance). Note that ONE-STEP and EXACT are dominating the convergence. In c) we report the same objective for the pharmacokinetic example. TRACKING works very well for this problem, most likely due to a favorable mixture decomposition.</p>
<h2>7 Applications \&amp; Experiments</h2>
<p>We present three applications of the proposed framework. We plot the suboptimality gap $F\left(\eta_{t}\right)-F\left(\eta_{*}\right)$. Further details of the experiments can be found in Appendix C. Overall, the empirical convergence tends to have two phases. First slow, but then quickly, the rate of convergence is of the order $\mathcal{O}\left(\frac{1}{T}\right)$ to $\mathcal{O}\left(\frac{1}{T^{2}}\right)$. In very limited cases the EXACT variant exhibits linear convergence (see Figs. 2 and 3). Note that for statistical estimation the suboptimality at $t=T$ is decisive.
Synthetic Gridworlds Consider a grid of a fixed height and width as in Fig. 1, where possible actions are to move up, down, left, and right by one cell. We consider three levels of increasing stochasticity. We assume that with probability $1-p$, the action we take is executed as expected, and with probability $p$ an arbitrary valid action (up, down, left, right) is played instead. The unknown target $f$ corresponds to a linear function and the features $\Phi(x, a)$ correspond to a unit vector in a different direction for each state type (cf., pictogram). Thus, the states of the same type are completely correlated while different ones are not at all correlated. Fig. 2 shows that the convergence of our algorithm is of order $\mathcal{O}\left(1 / T^{2}\right)$ faster than our theory predicts, and $\mathcal{O}(1 / \sqrt{T})$ for the NON-ADAPTIVE variant as we expect. With increasing stochasticity, the performance of the TRACKING variant reduces as it cannot adapt to executed trajectories from prior steps, while our adaptive methods can.
Species monitoring: path planning Suppose we want to estimate the rate of occurrence of a particular species. To model this application, we use occurrence data of Beilschmiedia, a tree genus native to Africa, from Baddeley et al. (2015). We model the occurrence rate $f$ as a positive valued RKHS function that determines the rate of the spatio-temporal Poisson point process. The states in this work are sectors $X$ which are sensed. (See the map Fig. 4a in Appendix C). We adopt the approach from Mutný and Krause (2021), where $f$ is estimated with a penalized least squares estimator from count observations. The number of counts in region $X$ is distributed according to $y \mid X \sim \operatorname{Poisson}\left(\int_{x \in X} f(x) d x\right)$. A peculiar property of the Poisson distribution is that its variance and mean are the same. Hence, the values of $\sigma_{a, X}^{2}=\int_{x \in X} f(x) d x$, are unknown due to the unknown $f$. To model $f$, we use a squared exponential kernel that takes the slope $s_{x, y}$ and height $h_{x, y}$ of a point $(x, y)$ as inputs, as these are predictive of the habitat of Beilschmiedia. We assume that a drone can cover a certain trajectory of length $H$ before it has to return to the starting position. We use the D-design objective to maximize the information about $f$ everywhere in the domain. As $\sigma_{X, a}$ is unknown, we either run our algorithm with a known absolute upper bound on $\sigma_{X, a}$ (due to norm bound on $\left|f\right|<em k="k">{\mathcal{H}</em>$ after each episode, and we take the upper bound of those. Results are shown in Fig.3b).}}$ ) as in Fig. 3a) or, alternatively, use confidence sets from Mutný and Krause (2021) on $f$ for adaptively collected data points. These allow us to construct confidence sets on $\sigma_{X, a</p>
<p>In both cases, the adaptive variants work well and their convergence rate is consistent with the theory we proved.
Group pharmacokinetics The goal of pharmacokinetics is to identify the rate of drug transport between the digestion system and other organ systems such as the circulatory system (blood). Such studies are designed for any new drug candidates to understand their absorption rates. The experiments are performed by drawing blood at specific time intervals, and inferring the medication concentrations over time. From this, the experimenters find the corresponding parameters $\gamma$ for the differential equation, which generate these concentration trajectories. We assume that the more accurately we can estimate the drug concentration trajectory over time for each patient, the more accurately we can estimate the parameters of the differential equation that generated these drug concentrations - and hence focus our design on estimating the trajectories. More realistically, we also assume that the concentration in the blood has two components $f_{i}(t)=c_{b}(t)+g_{i}(t)+\epsilon$ where $\epsilon \sim \mathcal{N}\left(0, \sigma^{2}\right)$ and $t$ is time. Hereby, $c_{b}(t)$ is the blood concentration of interest following a differential equation with parameters $\gamma$, and $g_{i}(t)$ is a patient-specific random variation that contaminates our measurements and we are not interested in inferring it per se. The differential equation is linear and hence forms a linear constraint on the estimation as $L_{\gamma}\left(c_{b}\right)=0$, where $L_{\gamma}$ is the linear differential operator. Note that this means that $c_{b}$ is in the null space of operator $L_{\gamma}$, which we denote $\mathbf{C}<em b="b">{\gamma}$ and the uncertainty of $c</em>$ for a fixed $\gamma$ is then due to initial conditions only. While we set the initial concentration in our experiment by specifying dosage, we will model the initial conditions as unknown with little uncertainty, and then design an objective that reduces this uncertainty for a fixed $\gamma$. Since $\gamma$ is unknown, we will use a robust designs of the experimental design objective, considering the supremum (resp. infimum if negative) over reasonable values of $\gamma \in \Gamma$ as $F(\eta)=$ $\sup <em _gamma="\gamma">{\gamma \in \Gamma} \operatorname{Tr}\left[\left(\mathbf{C}</em>\right)\right]$ to define $F$. The policy, in this case, is a medical plan when to draw blood with a constraint that blood can be drawn only 5 times per patient (episode) with reasonable separations ( 3 time steps). Each patient corresponds to an episode. We report the results in Fig. 3c).}\left(\sum_{\tau \in T} \eta(\tau) I(\tau)+(\lambda / T) \mathbf{I}\right)^{-1} \mathbf{C}_{\gamma}^{\top</p>
<h2>8 Conclusion</h2>
<p>We introduced a novel algorithm MARKOV-DESIGN for experiment design in Markov chains, capable of finding a set of exploratory policies that converge to an optimal allocation over trajectories to learn an unknown function of the states and actions in a known Markov chain. The algorithm solves a sequence of convex RL problems, which are informed by the previous trajectories of the agent. We proved the convergence rate of the method and its superiority to other approaches. We demonstrated its empirical performance in real-world problems and hope that this work will open a new avenue to study controlled Markov chains from an experimental design perspective.</p>
<h2>References</h2>
<p>Baddeley, A., E., R., and R., T. (2015). Spatial Point Patterns: Methodology and Applications with R. Chapman and Hall/CRC Press.</p>
<p>Beck, A. and Teboulle, M. (2012). Smoothing and first order methods: A unified framework. SIAM Journal on Optimization, 22(2):557-580.</p>
<p>Belogolovsky, S., Korsunsky, P., Mannor, S., Tessler, C., and Zahavy, T. (2021). Inverse reinforcement learning in contextual mdps. Machine Learning.</p>
<p>Berthet, Q. and Perchet, V. (2017). Bandit optimization with upper-confidence frank-wolfe. arXiv preprint arXiv:1702.06917.</p>
<p>Borsos, Z., Mutný, M., and Krause, A. (2020). Coresets via bilevel optimization for continual learning and streaming. Neural and Information Processing Systems (NeurIPS) 2020.</p>
<p>Borwein, J. and Zhu, Q. (2005). Techniques of Variational Analysis. Springer-Verlag New York.</p>
<p>Carderera, A., Besançon, M., and Pokutta, S. (2021). Simple steps are all you need: Frank-wolfe and generalized self-concordant functions. In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W., editors, Advances in Neural Information Processing Systems, volume 34, pages 5390-5401. Curran Associates, Inc.</p>
<p>Chaloner, K. and Verdinelli, I. (1995). Bayesian experimental design: A review. Statist. Sci., 10(3):273-304.</p>
<p>Fedorov, V. V. and Hackl, P. (1997). Model-Oriented Design of Experiments | Valerii V. Fedorov | Springer. SpringerVerlag New York.</p>
<p>Foster, A., Ivanova, D. R., Malik, I., and Rainforth, T. (2021). Deep adaptive design: Amortizing sequential bayesian experimental design. In Meila, M. and Zhang, T., editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 3384-3395. PMLR.</p>
<p>Gabrielsson, J. and Weiner, D. (1995). Pharmacokinetic and pharmacodynamic data analysis. Trends in Pharmacological Sciences, 16(4):143.</p>
<p>Golovin, D. and Krause, A. (2011). Adaptive submodularity: Theory and applications in active learning and stochastic optimization. J. Artif. Int. Res., 42(1):427-486.</p>
<p>Hazan, E., Kakade, S., Singh, K., and Van Soest, A. (2019). Provably efficient maximum entropy exploration. In International Conference on Machine Learning, pages 26812691. PMLR.</p>
<p>Jaggi, M. (2013). Revisiting Frank-Wolfe: Projectionfree sparse convex optimization. In Dasgupta, S. and
McAllester, D., editors, Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 427-435, Atlanta, Georgia, USA. PMLR.</p>
<p>Jin, C., Yang, Z., Wang, Z., and Jordan, M. I. (2020). Provably efficient reinforcement learning with linear function approximation. In Abernethy, J. and Agarwal, S., editors, Proceedings of Thirty Third Conference on Learning Theory, volume 125 of Proceedings of Machine Learning Research, pages 2137-2143. PMLR.</p>
<p>Jourdan, M., Mutný, M., Kirschner, J., and Krause, A. (2021). Efficient pure exploration for combinatorial bandits with semi-bandit feedback. In Proceedings of the 32nd International Conference on Algorithmic Learning Theory (ALT).</p>
<p>Kirschner, J. and Krause, A. (2018). Information directed sampling and bandits with heteroscedastic noise. COLT.</p>
<p>Krause, A. and Golovin, D. (2014). Submodular function maximization.</p>
<p>Krause, A. and Guestrin, C. (2005). Optimal nonmyopic value of information in graphical models: efficient algorithms and theoretical limits. AAAI 2005.</p>
<p>Lacoste-Julien, S. and Jaggi, M. (2015). On the global linear convergence of frank-wolfe optimization variants. Advances in neural information processing systems, 28.</p>
<p>Lindner, D., Turchetta, M., Tschiatschek, S., Ciosek, K., and Krause, A. (2021). Information directed reward learning for reinforcement learning. In Proc. Neural Information Processing Systems (NeurIPS).</p>
<p>Lugosi, G. (2009). Concentration of measure inequalities. Lecture Notes.</p>
<p>Mutný, M. and Krause, A. (2018). Efficient high dimensional bayesian optimization with additivity and quadrature fourier features. In Neural and Information Processing Systems (NeurIPS) 2018.</p>
<p>Mutný, M. and Krause, A. (2021). No-regret algorithms for capturing events in poisson point processes. In Meila, M. and Zhang, T., editors, Proceedings of the 38th International Conference on Machine Learning (ICML 2021), volume 139 of Proceedings of Machine Learning Research, pages 7894-7904. PMLR.</p>
<p>Mutný, M. and Krause, A. (2022a). Experimental design for linear functionals in reproducing kernel hilbert spaces: Methods, confidence sets and applications. submitted.</p>
<p>Mutný, M. and Krause, A. (2022b). Sensing cox processes via posterior sampling and positive bases. Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS 2022).</p>
<p>Nemhauser, G. L., Wolsey, L. A., and Fisher, M. L. (1978). An analysis of approximations for maximizing submodular set functions-i. Mathematical programming, 14(1):265-294.</p>
<p>Nesterov, Y. (2005). Smooth minimization of non-smooth functions. Math. Program., 103:127-152.</p>
<p>Neu, G. and Pike-Burke, C. (2020). A unifying view of optimism in episodic reinforcement learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., editors, Advances in Neural Information Processing Systems, volume 33, pages 1392-1403. Curran Associates, Inc.</p>
<p>Pukelsheim, F. (2006). Optimal Design of Experiments. SIAM.</p>
<p>Puterman, M. L. (2014). Markov decision processes: discrete stochastic dynamic programming. John Wiley Sons.</p>
<p>Romero, P. A., Krause, A., and Arnold, F. H. (2013). Navigating the protein fitness landscape with gaussian processes. Proceedings of the National Academy of Sciences, 110(3):E193-E201.</p>
<p>Schölkopf, B., Herbrich, R., and Smola, A. (2001). A generalized representer theorem. In Computational learning theory, pages 416-426. Springer.</p>
<p>Settles, B. (2009). Active learning literature survey. University of Wisconsin-Madison Department of Computer Sciences.</p>
<p>Szepesvari, C. and Lattimore, T. (2020). Bandit Algorithms. Cambridge University Press.</p>
<p>Talebi, M. S., Zou, Z., Combes, R., Proutiere, A., and Johansson, M. (2013). Stochastic online shortest path routing: The value of feedback. arXiv preprint arXiv:1309.7367.</p>
<p>Tarbouriech, J. and Lazaric, A. (2019). Active exploration in markov decision processes. In Chaudhuri, K. and Sugiyama, M., editors, Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics, volume 89 of Proceedings of Machine Learning Research, pages 974-982. PMLR.</p>
<p>Todd, M. J. (2016). Minimum-Volume Ellipsoids: Theory and Algorithms. SIAM, mos-siam series on optimization edition.</p>
<p>Wagenmaker, A. and Jamieson, K. (2022). Instancedependent policy learning for linear mdps via online experiment design. In NeurIPS.</p>
<p>Wainwright, M. J. (2019). High-Dimensional StatisticsA Non-Asymptotic Viewpoint. Cambridge University Press.</p>
<p>Zahavy, T., O’Donoghue, B., Desjardins, G., and Singh, S. (2021). Reward is enough for convex mdps. In 35th</p>
<p>Conference on Neural Information Processing Systems (NeurIPS 2021).</p>
<p>Zhang, F. (2011). Matrix Theory: Basic Results and Techniques. Springer Science \&amp; Business Media.</p>
<p>Zhao, R. and Freund, R. (2022). nalysis of the frank-wolfe method for convex composite optimization involving a logarithmically-homogeneous barrier. Math. Program.</p>
<p>Zou, Z., Proutiere, A., and Johansson, M. (2014). Online shortest path routing: The value of information. In American Control Conference (ACC), 2014, pages 2142-2147. IEEE.</p>
<h1>Supplementary Material: Active Exploration via Experiment Design in Markov Chains</h1>
<h2>A Additional Results</h2>
<h2>A. 1 Relating state-visitations and trajectories</h2>
<p>We constructively prove that $F(\eta)$ and $U(d)$ can be related easily for scalarized information matrix objectives.
Lemma 1 (Conversion Mapping). Let $F(\eta)=s(\mathbf{S}(\eta))$, where $\mathbf{S}(\eta) \in \mathbb{S}<em k="k">{+}$positive semi-definite cone of operators $\mathcal{H}</em>} \rightarrow \mathcal{H<em _in="\in" _mathcal_T="\mathcal{T" _tau="\tau">{k}$ s.t. $\mathbf{S}(\eta)=\sum</em>}} \eta(\tau) I(\tau)+\lambda \mathbf{I}$ and and $s: \mathbb{S<em _in="\in" _mathcal_A="\mathcal{A" a_="a," x="x">{+} \rightarrow \mathbb{R}$, then there exists $U(d)=U(\mathbf{Z} \eta):=F(\eta)$ which is equal to $U(d)=s\left(\sum</em>\right)$.} \times \mathcal{X}} d(a, x) \Phi(x, a) \Phi(x, a)^{\top}+\lambda \mathbf{I</p>
<p>Proof. The proof goes by construction, where we first construct $U$ and then verify it satisfies the desired property. The whole proof relies only on the additive property of information operator $I(\tau)$.</p>
<p>$$
\begin{aligned}
U\left(d_{\pi}\right) &amp; =s\left(\sum_{a, x \in \mathcal{A} \times \mathcal{X}} d_{\pi}(a, x) \Phi(x, a) \Phi(x, a)^{\top}+\lambda \mathbf{I}\right) \
&amp; \stackrel{(1)}{=} s\left(\sum_{a, x \in \mathcal{A} \times \mathcal{X}} \sum_{\tau \in \mathcal{T}} \mathbf{Z}(a, s, \tau) \eta_{\pi}(\tau) \Phi(x, a) \Phi(x, a)^{\top}+\lambda \mathbf{I}\right) \
&amp; =s\left(\sum_{a, x \in \mathcal{A} \times \mathcal{X}} \sum_{\tau \in \mathcal{T}} #(a, x \in \tau) \eta_{\pi}(\tau) \Phi(x, a) \Phi(x, a)^{\top}+\lambda \mathbf{I}\right) \
&amp; =s\left(\sum_{\tau \in \mathcal{T}} \eta_{\pi}(\tau) \sum_{a, x \in \mathcal{A} \times \mathcal{X}} #(a, x \in \tau) \Phi(x, a) \Phi(x, a)^{\top}+\lambda \mathbf{I}\right) \
&amp; \stackrel{(3)}{=} s\left(\sum_{\tau \in \mathcal{T}} \eta_{\pi}(\tau) I(\tau)+\lambda \mathbf{I}\right)=F\left(\eta_{\pi}\right)
\end{aligned}
$$</p>
<h2>A. 2 Smoothing Technique for Convex Optimization</h2>
<p>In this section, we will briefly review, by now, the classical technique of convex optimization for non-smooth functions by Nesterov (2005). The development outlined here is inspired by Beck and Teboulle (2012). Suppose the function $F(\eta)$ is either smooth with a large constant $L$ or non-smooth. In what follows, the non-smooth case is recovered by letting $L \rightarrow \infty$. A central object component in defining the smoothing is the convex conjugate of $F, F^{*}$,</p>
<p>$$
F^{*}(\zeta):=\max _{\eta \in \mathcal{P}} \eta^{\top} \zeta-F(\eta)
$$</p>
<p>In order to define a smoothed function $F_{\eta}$, we perform the reverse operation with added regularization</p>
<p>$$
F_{\mu}(\eta):=\max _{\zeta} \zeta^{\top} \eta-F^{*}(\zeta)-\frac{\mu}{2}|\zeta|^{2}
$$</p>
<p>where the domain of $\zeta$ is everywhere where $F^{*}$ is finite. Equivalently it can be represented via dual reformulation as</p>
<p>$$
F_{\mu}(\eta)=\inf <em 2="2">{\zeta \in \mathcal{P}} F(\zeta)+\frac{1}{2 \mu}|\zeta-\eta|</em>
$$</p>
<p>which is sometimes referred to as Moreau proximal smoothing (Beck and Teboulle, 2012). Due to the above definition it is clear that $F_{\mu}(\eta) \leq F(\eta) \leq F_{\mu}(\eta)+\mu \max <em 2="2">{x, y \in \mathcal{P}}|x-y|</em>(\eta)+2 \mu$. Hence by choosing sufficiently small $\mu$, we can} \leq F_{\mu</p>
<p>ensure that minimizing $F_{\mu}$ will closely minimize $F$. In addition, $F_{\mu}$ is smooth, differentiable, and smooth as summarized in the following lemma.</p>
<p>Lemma 2 (Bounded smoothness). Let $F_{\mu}$ be smoothing of $F$ as in Eq. (16). The function $F_{\mu}$ is $\frac{L}{1+\mu L}$-smooth.</p>
<p>Proof. In order to show this we will use the definition in Eq. (15) and the fact that convex conjugate has the property that a conjugate of $s$-strongly convex function is $1 / s$-smooth and vice versa (Borwein and Zhu, 2005).</p>
<p>Notice that $F^{<em>}$ is $1 / L$ strongly-convex, then $F^{</em>}+\mu / 2|\cdot|$ is $1 / L+\mu$ strongly-convex. Conjugating these cause the function to be $\frac{1}{1 / L+\mu}=\frac{L}{1+\mu L}$ smooth. As $L \rightarrow \infty$, the smoothness constant is $1 / \mu$.</p>
<p>As a corollary of the above definition, we also have a bound on the gradient that we will utilize later.
Lemma 3 (Bounded gradient ${ }^{1}$ ). Let $F_{\mu}$ be smoothing of $F$ as in Eq. (16), then</p>
<p>$$
\left|\nabla F_{\mu}(\eta)\right|<em p="p">{p} \leq|\nabla F(\eta)|</em>
$$</p>
<p>for any $\eta \in \mathcal{P}$, and $p \in[1, \infty]$.</p>
<p>Proof. To prove this relation, we use representation in Eq. (16). Note that $\nabla F_{\mu}(\eta)=\frac{1}{\mu}\left(\eta-\zeta^{<em>}\right)$, where $\zeta^{</em>}$ is where the infimum is realized. In particular, it holds that $\nabla F\left(\zeta^{<em>}\right)+\frac{1}{\mu}\left(\zeta^{</em>}-\eta\right)=0$, i.e. $\zeta^{*}=(\mathbf{I}+\mu \nabla F)^{-1}(\eta)$. The $\mathbf{I}$ designates the identity operator.</p>
<p>Now,</p>
<p>$$
\begin{aligned}
\left|\nabla F_{\mu}(\eta)\right|<em p="p">{p} &amp; =\frac{1}{\mu}\left|\eta-\zeta^{*}\right|</em> \
&amp; =\frac{1}{\mu}\left|\eta-(\mathbf{I}+\mu \nabla F)^{-1} \eta\right|<em p="p">{p} \
&amp; =\frac{1}{\mu}\left|(\mathbf{I}+\mu \nabla F)^{-1}((\mathbf{I}+\mu \nabla F) \eta-\eta)\right|</em> \
&amp; \leq \frac{1}{\mu}\left|((\mathbf{I}+\mu \nabla F) \eta-\eta)\right|<em p="p">{p} \
&amp; =|\nabla F(\eta)|</em>
\end{aligned}
$$</p>
<p>where in the fourth line we use that resolvent is non-expansive.</p>
<h1>A. 3 A-design and D-design regularity</h1>
<p>The Assumption 1 is satisfied for the objectives in Table 1 with the exception of E-design which is non-smooth. Using the smoothing technique above one can make sure the conditions are satisfied for the optimization algorithm used in this work.</p>
<p>The proofs of the smoothness and convexity of A and V-design can be found in Appendix, Lemma 8 of Borsos et al. (2020). The convexity of D-design is clear from Pukelsheim (2006), the objective is also smooth in the regularized form, however, the constant has poor scaling. The following following lemma demonstrates it.</p>
<p>Lemma 4. The objective $-\log \operatorname{det}\left(\sum_{x \in \mathcal{X}} x x^{\top} \eta(x)+\lambda \mathbf{I}\right)$ is $L$-smooth in terms of $\eta \in \Delta_{|\mathcal{X}|}$, with $L=\frac{1}{\lambda^{2}} \lambda_{\max }\left(\left(\mathbf{X X}^{\top}\right) \circ\right.$ $\left.\left(\mathbf{X X}^{\top}\right)\right)$, and additionally the $\lambda_{\max }(\mathbf{H}(\eta)) \leq|\nabla F(\eta)|_{2}^{2}$, where $\mathbf{H}$ is the Hessian.</p>
<p>Proof. The Hessian of the above objective is $\mathbf{H}=\left(\mathbf{X V}^{-1} \mathbf{X}^{\top}\right) \circ\left(\mathbf{X V}^{-1} \mathbf{X}^{\top}\right)$, where $\mathbf{V}=\mathbf{X}^{\top} \mathbf{D}(\eta) \mathbf{X}+\lambda \mathbf{I}$ and $\circ$ refers to the Hadamard product. This can be shown by taking the derivative, linearity of trace, and $\partial \mathbf{A}^{-1}=\mathbf{A}^{-1}(\partial \mathbf{A}) \mathbf{A}^{-1}$ for symmetric matrix.</p>
<p>The last part is by noting that all elements of $\mathbf{H}<em i="i">{i j}$ are positive, and strictly smaller than $\nabla</em> F$. The result follows by using Theorem 5.22 from (Zhang, 2011).} F \nabla_{j</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>B Formal Results and Proofs</h1>
<h2>B. 1 Concentration Results</h2>
<p>First, we show that empirical measures constructed via sampling from a probability distribution can closely track its probability distributions using Azuma inequality. We will use this result in the convergence proofs later in Appendix B.4.
Lemma 5 (Empirical measure concentration (linear)). Let $\left{\eta_{t}\right}<em t-1="t-1">{t=1}$ be an adapted sequence of probability distributions on $\mathcal{X}, \mathcal{P}(\mathcal{X})$ with respect to filtration $\mathcal{F}</em>\right}}$. Likewise let $\left{f_{t<em t="t">{t=1}$ be an adapted sequence of linear functionals $f</em>\right|}: \mathcal{P}(\mathcal{X}) \rightarrow \mathbb{R}$ s.t. $\left|f_{t<em t="t">{\infty} \leq B</em>}$. Also, let $x_{t} \sim \eta_{t}$, and $\delta_{t}(x)=\mathbf{1<em t="t">{x</em>$, then}=x</p>
<p>$$
\mathrm{P}\left(\left|\sum_{t=1}^{T} f_{t}\left(\delta_{t}-\eta_{t}\right)\right| \geq \sqrt{2 \sum_{t=1}^{T} B_{t}^{2} \log \left(\frac{2}{\delta}\right)}\right) \leq \delta
$$</p>
<p>Proof. Let $Q_{t}=f_{t}\left(\delta_{t}-\eta_{t}\right)$. In other words $Q_{t}=\int_{x \in \mathcal{X}} f_{t}(x)\left(\delta_{t}(x)-\eta_{t}(x)\right) d x$. Since,</p>
<p>$$
\begin{aligned}
\mathbb{E}\left[Q_{t} \mid \mathcal{F}<em t="t">{t-1}\right] &amp; =\mathbb{E}\left[f</em>}\left(\delta_{t}(x)-\eta_{t}(x)\right) \mid \mathcal{F<em t="t">{t-1}\right] \stackrel{\text { fin. }}{=} f</em>}\left(\mathbb{E}\left[\delta_{t}(x)-\eta_{t}(x) \mid \mathcal{F<em t="t">{t-1}\right]\right) \
&amp; =f</em>(0)=0
\end{aligned}
$$}\left(\eta_{t}(x)-\eta_{t}(x)\right)=f_{t</p>
<p>and each $Q_{t}$ is bounded as $\left|Q_{t}\right| \leq\left|f_{t}\right|<em t="t">{\infty}\left|\delta</em>\right|}-\eta_{t<em t="t">{1} \leq\left|f</em>$ it is martingale difference sequence. Consequently, we can use the generalized Azuma-Hoeffding (Wainwright, 2019, Corr. 2.20) inequality to control the sum,}\right| \leq B_{t}, Q_{t</p>
<p>$$
\mathrm{P}\left(\left|\sum_{t=1}^{T} Q_{t}\right| \geq \epsilon\right) \leq 2 \exp \left(-\frac{2 \epsilon^{2}}{4 \sum_{t=1}^{T} B_{t}^{2}}\right)
$$</p>
<p>Setting $\delta / 2=\exp \left(-\frac{2 \epsilon^{2}}{4 \sum_{t=1}^{T} B_{t}^{2}}\right)$, gives $\epsilon^{2}=2 \log (2 / \delta) \sum_{t=1}^{T} B_{t}^{2}$.
Lemma 6 (Hoeffding on Hilbert space (Lugosi, 2009)). Let $X_{i} \in L_{2}$ be zero mean s.t. $\left|X_{i}\right|_{2} \leq s$, then for any $t \geq \sqrt{N s^{2}}$</p>
<p>$$
P\left(\left|S_{N}\right| \geq t\right) \leq \exp \left(-\left(t-\sqrt{N s^{2}}\right)^{2} /\left(2\left(N s^{2}\right)\right)\right)
$$</p>
<p>Pick $t=2 \sqrt{N s^{2} \log (1 / \delta)}$, then $P\left(\left|S_{N}\right| \geq \sqrt{2 N s^{2} \log (1 / \delta)}\right) \leq \exp (-\log (1 / \delta))=\delta$</p>
<h2>B. 2 Resampling convergence: Proofs</h2>
<p>First, we prove the saturation of the resampling, NON-ADAPTIVE, algorithm. Namely, given a policy $\pi^{<em>}$ which induces the optimal $d^{</em>}$ and hence optimal $\eta^{<em>}$, how quickly does the empirical $\eta_{T}=\frac{1}{T} \sum_{t=1}^{T} \delta_{\tau_{t}}$ converge to $\eta^{</em>}$ ? The proof trivially uses Lemma 5 and 6 , to show concentration.
Proposition 1. Under Assumption 1, for resampling oracle with $T$ resamplings, and $\eta_{T}=\frac{1}{T} \sum_{t=1}^{T} \delta_{\tau_{t}}$,</p>
<p>$$
F\left(\eta_{T}\right)-F\left(\eta^{*}\right) \leq\left|\nabla F\left(\eta_{T}\right)\right|_{\infty} \sqrt{\frac{2 \log (2 / \delta)}{T}}
$$</p>
<p>with probability $1-\delta$. For D-design, we can show that</p>
<p>$$
\mathbb{E}\left[F\left(\eta_{T}\right)\right]-F\left(\eta^{*}\right) \leq \frac{4\left|\nabla F\left(\eta_{T}\right)\right|_{\infty}^{2}}{T} \log (T)+2
$$</p>
<p>where the expectation is over the sampling process.
Proof.</p>
<p>$$
F\left(\eta_{T}\right)-F\left(\eta^{<em>}\right) \stackrel{\text { convexity }}{\leq} \nabla F\left(\eta_{T}\right)^{\top}\left(\eta^{</em>}-\eta_{T}\right)=\frac{1}{T} \sum_{i=1}^{T} \nabla F\left(\eta_{T}\right)^{\top}\left(\eta^{*}-\delta_{\tau_{t}}\right)
$$</p>
<p>Using $\left|\nabla F\left(\eta_{T}\right)\right|<em t="t">{\infty}=B$, and Lemma 5 , with $B</em>=B / T$, we get</p>
<p>$$
F\left(\frac{1}{T} \sum_{t=1}^{T} \delta_{t}\right)-F\left(\eta^{*}\right) \leq \sqrt{4 \sum_{t=1}^{T} \frac{B^{2}}{T^{2}} \log (2 / \delta)}
$$</p>
<p>To show the bound in expectation,</p>
<p>$$
\begin{aligned}
&amp; F\left(\eta_{T}\right)-F\left(\eta^{<em>}\right) \leq \nabla F\left(\eta^{</em>}\right)^{\top}\left(\eta_{T}-\eta^{<em>}\right)+\frac{L_{t}}{2}\left|\left(\frac{1}{T} \sum_{t=1}^{T} \delta_{t}\right)-\eta^{</em>}\right|^{2} \
&amp; \stackrel{\text { Lemma } 4}{\leq}\left|\nabla F\left(\eta^{<em>}\right)^{\top}\left(\eta_{T}-\eta^{</em>}\right)\right|+\frac{|\nabla F(z)|<em t="1">{2}^{2}}{2}\left|\left(\frac{1}{T} \sum</em>\right)-\eta^{}^{T} \delta_{t<em>}\right|^{2} \
&amp; \mathbb{E}\left[F\left(\eta_{T}\right)\right]-F\left(\eta^{</em>}\right) \leq \frac{B^{2}}{T^{2}} \mathbb{E}\left[\left|\sum_{t=1}^{T}\left(\delta_{t}-\eta^{*}\right)\right|^{2}\right] \
&amp; \stackrel{\text { Lemma } 6}{\leq} B^{2} T^{2}(2 \sqrt{T \log (1 / \delta}))^{2}(1-\delta)+\delta 2 T \
&amp; \leq \frac{4 B^{2}}{T} \log (T)+2
\end{aligned}
$$</p>
<p>where in the last step we used $\delta=1 / T$, and in the second to last we used that $\mathbb{E}\left[\eta_{T}\right]=\eta^{*}$.
Note that the value depends on the constant $\left|\nabla F\left(\eta_{T}\right)\right|_{\infty}$. We cannot globally control the value of the gradient, at least with a satisfactory constant, but for sufficiently large $T$, this value will be well-behaved. The following section B. 3 provides intuition when this number becomes well-behaved.</p>
<h1>B. 3 Discussion on the regularity of the objective</h1>
<p>The convergence rates in Theorems 1 and 2 depend on the norm of the gradient $\left|\nabla F\left(\eta_{t}\right)\right|<em t="t">{\infty}$ and local smoothness parameter $L</em>$ depends on it as in Proposition 1. In this section, we would like to provide some intuition about these quantities.}$. Likewise, after $t$ resamplings the suboptimality of $\eta_{t</p>
<p>To explore these quantities, we will use a completely degenerate example, where the Markov chain has $d$ states and $d$ action, where playing $i$ th action leads to $i$ th state, and the agent has to stay there as $H=1$. This corresponds to the classical experimental design problem where the Markov chain is trivial. On top of this, we assume that each $\Phi\left(x_{i}, a_{i}\right)=e_{i}$ where $e_{i}$ is unit vector in $\mathbb{R}^{d}$, and we focus on D-design objective.</p>
<p>In this case, the optimal policy $\eta^{*}$ puts equal mass on all trajectories, i.e. $\mathrm{P}\left(\tau=\tau_{i}\right)=1 / d$ for all $i \in[d]$.
Resampling The gradient in this case is equal to:</p>
<p>$$
\left|\nabla F\left(\eta_{t}\right)\right|<em _in_d_="\in[d]" i="i">{\infty}=\max </em>
$$} e_{i}^{\top}\left(\sum_{j=1}^{t} e_{\tau_{j}} e_{\tau_{j}}^{\top}+\frac{\lambda}{T} \mathbf{1}\right)^{-1} e_{i</p>
<p>In this case, since the feature of each state-action is orthogonal to each other, the above is proportional linearly to $T$ only if $\left{\tau_{j}\right}<em _all="{all" _text="\text">{j=1}^{t}$ does not span the whole basis. In order to do so, we need to sample each trajectory at least once. However, since the sampling is with replacement, we know due to classical coupon collector problem we need to perform in expectation $d \log (d)$ resamplings in order to have each of them single. One can also express this using a high-probability event. Namely, let $T</em> \leq d \log (d)+d \log (1 / \delta)\right) \geq 1-\delta$. Notice that $d \log (d)$ appears regardless of confidence score $\delta \in(0,1)$.
Adaptive method Notice in contrast that e.g. ONE-STEP or EXACT algorithm would always sample a different $e_{\tau_{j}}$ than previously collected in their next episode since they would always pick $\tau_{j}$ which maximizes the gradient (or any of them). Hence, already with $d$ steps, deterministically, in this example, would lead to a bound on the gradient as $\frac{d}{1+\lambda / T} \leq d$. In subsequent steps, the value of the gradient can grow, however, it does not grow linearly with $T$. Notice that since the smoothness constant of the D-objective is related to the gradient squared due to Lemma 4 , after $d$ steps even the local smoothness constant is bounded by $d^{2}$ in this example.}}$ be the time after all are sampled at least once. The probability $\mathrm{P}\left(T_{\text {all }</p>
<p>This reveals that while the resampling and adaptive optimization have the same convergence in terms of $T$, in expectation or with high probability, their actual performance depends significantly on the auxiliarly constants such as gradient norm or local Lipschitz constant.</p>
<p>Generalizing this procedure to more complicated feature spaces and Markov chain structure is a challenging problem. The above simplification suggests that adaptive methods perform some form of efficient initialization scheme. The closest in literature is the work of Todd (2016) which provides an initialization scheme for D-design such that the gradient is bounded. They consider only objectives without regularization. Extending this technique to a kernelized setting is non-trivial and perhaps not as practically appealing as the methods suggested here, especially in the regularized scenario that we consider. Detailed analysis and bounding of these constants in more general settings is an open problem for future work.</p>
<h1>B. 4 Adaptive Method: Proofs</h1>
<p>In this section, we give proof of our adaptive methods. The proofs are related and essentially the same as those of Berthet and Perchet (2017) for Frank-Wolfe UCB albeit the source of error in gradients is different and handled differently. Due to the connection to Franke-Wolfe, we also know that the rate in expectation cannot be improved with this step size (Lacoste-Julien and Jaggi, 2015). We present ONE-STEP and EXACT variant in the following two theorems.</p>
<p>Theorem 3 (Convergence high probability). Under Assumption 1, using Nesterov (2005) smoothing technique as in Appendix A. 2 with $\mu=\sqrt{\log T / T}$, the suboptimality of ONE-STEP and EXACT variant can be bounded as,</p>
<p>$$
F\left(\eta_{T}\right)-F\left(\eta^{<em>}\right) \leq \frac{1}{T}\left(F\left(\eta_{1}\right)-F\left(\eta^{</em>}\right)\right)+2 \sqrt{\frac{\log T}{T^{3}}}+\frac{1}{T} \sqrt{2 \sum_{t=1}^{T}\left|\nabla F\left(\eta_{t}\right)\right|<em t="1">{\infty}^{2} \log \left(\frac{2}{\delta}\right)}+3 \sqrt{\frac{\log T}{T}}+\frac{1}{T} \sum</em>
$$}^{T} \Delta_{t</p>
<p>with probability $(1-\delta)$ over the stochasticity of policies and the Markov chain. For ONE-STEP variant $\Delta_{t}=0$ for all $t$, and for EXACT variant each subproblem is solved to near-optimality s.t. $\hat{v}_{t}$ approximates</p>
<p>$$
v_{t}=\arg \min <em _mu="\mu">{q \in \mathcal{P}} F</em>}\left(\eta_{t}+\frac{1}{t+1}\left(q-\eta_{t}\right)\right), \text { and } F_{\mu}\left(\hat{v<em _mu="\mu">{t}\right)-F</em>
$$}\left(v_{t}\right) \leq \Delta_{t</p>
<p>Proof. Let us refresh the definition of the following terms,</p>
<p>$$
\begin{gathered}
q_{t}=\underset{q \in \mathcal{P}}{\arg \min } \nabla F_{\mu}\left(\eta_{t}\right)^{\top} q \
\eta_{\mu}^{<em>}=\underset{q \in \mathcal{P}}{\arg \min } F_{\mu}(q), \quad \text { and } \quad \eta^{</em>}=\underset{q \in \mathcal{P}}{\arg \min } F(q)
\end{gathered}
$$</p>
<p>We will first show the proof of ONE-STEP variant and later show how EXACT can be reduced such that the same analysis applies.</p>
<p>$$
\begin{aligned}
&amp; F_{\mu}\left(\eta_{t+1}\right) \quad=\quad F_{\mu}\left(\eta_{t}+\frac{1}{t+1}\left(\delta_{t}-\eta_{t}\right)\right) \
&amp; \stackrel{L_{\mu} \text {-smooth }}{\leq} \quad F_{\mu}\left(\eta_{t}\right)+\frac{1}{t+1} \nabla F_{\mu}\left(\eta_{t}\right)^{\top}\left(\delta_{t}-\eta_{t}\right)+\frac{L_{\mu}}{2(1+t)^{2}}\left|\delta_{t}-\eta_{t}\right|<em _mu="\mu">{2}^{2} \
&amp; F</em> \
&amp; =\quad F_{\mu}\left(\eta_{t}\right)+\frac{1}{t+1} \nabla F_{\mu}\left(\eta_{t}\right)^{\top}\left(q_{t}-\eta_{t}\right)+\frac{1}{t+1} \nabla F_{\mu}\left(\eta_{t}\right)^{\top}\left(-q_{t}+\delta_{t}\right)+\frac{L_{\mu}}{(1+t)^{2}} \
&amp; \stackrel{(21)}{\leq} \quad F_{\mu}\left(\eta_{t}\right)+\frac{1}{t+1} \nabla F_{\mu}\left(\eta_{t}\right)^{\top}\left(\eta_{\mu}^{}\left(\eta_{t+1}\right) \quad \stackrel{\text { bounded }}{\leq} \quad F_{\mu}\left(\eta_{t}\right)+\frac{1}{t+1} \nabla F_{\mu}\left(\eta_{t}\right)^{\top}\left(\delta_{t}-\eta_{t}\right)+\frac{L_{\mu}}{(1+t)^{2}<em>}-\eta_{t}\right)+\frac{1}{t+1} \nabla F_{\mu}\left(\eta_{t}\right)^{\top}\left(-q_{t}+\delta_{t}\right)+\frac{L_{\mu}}{(1+t)^{2}} \
&amp; \stackrel{\text { convexity }}{\leq} \quad F_{\mu}\left(\eta_{t}\right)-\frac{1}{t+1}\left(F_{\mu}\left(\eta_{t}\right)-F_{\mu}\left(\eta_{\mu}^{</em>}\right)\right)+\frac{1}{t+1} \nabla F_{\mu}\left(\eta_{t}\right)^{\top}\left(-q_{t}+\delta_{t}\right)+\frac{L_{\mu}}{(1+t)^{2}} \
&amp; F_{\mu}\left(\eta_{t+1}\right)-F_{\mu}\left(\eta_{\mu}^{<em>}\right) \quad \leq \quad F_{\mu}\left(\eta_{t}\right)-F_{\mu}\left(\eta_{\mu}^{</em>}\right)-\frac{1}{t+1}\left(F_{\mu}\left(\eta_{t}\right)-F_{\mu}\left(\eta_{\mu}^{<em>}\right)\right)+\frac{1}{t+1} \nabla F_{\mu}\left(\eta_{t}\right)^{\top}\left(-q_{t}+\delta_{t}\right)+\frac{L_{\mu}}{(1+t)^{2}} \
&amp; \leq \quad \frac{t}{1+t}\left(F_{\mu}\left(\eta_{t}\right)-F_{\mu}\left(\eta_{\mu}^{</em>}\right)\right)+\frac{1}{t+1} \nabla F_{\mu}\left(\eta_{t}\right)^{\top}\left(-q_{t}+\delta_{t}\right)+\frac{L_{\mu}}{(1+t)^{2}}
\end{aligned}
$$</p>
<p>where we used the shorthand $\epsilon_{t}=\nabla F_{\mu}\left(\eta_{t}\right)^{\top}\left(-q_{t}+\delta_{t}\right)$. Using shorthand $\rho_{t+1}=F_{\mu}\left(\eta_{t+1}\right)-F_{\mu}\left(\eta_{\mu}^{*}\right)$ :</p>
<p>$$
\left(F_{\mu}\left(\eta_{t+1}\right)-F_{\mu}\left(\eta_{\mu}^{<em>}\right)\right)(t+1) \leq t\left(F_{\mu}\left(\eta_{t}\right)-F_{\mu}\left(\eta_{\mu}^{</em>}\right)\right)+\nabla F_{\mu}\left(\eta_{t}\right)^{\top}\left(-q_{t}+\delta_{t}\right)+\frac{L_{\mu}}{(1+t)}
$$</p>
<p>$$
\begin{aligned}
\rho_{t+1}(t+1) &amp; \leq\left(t \rho_{t}+\epsilon_{t}+\frac{L_{\mu}}{1+t}\right) \
\rho_{t+1}(t+1)-\rho_{t} t &amp; \leq \epsilon_{t}+\frac{L_{\mu}}{1+t} \
\sum_{t=1}^{T-1} \rho_{t+1}(t+1)-\rho_{t} t &amp; \leq \sum_{t=1}^{T-1} \epsilon_{t}+L_{\mu} \log T \
T \rho_{T}-\rho_{1} &amp; \leq \sum_{t=1}^{T-1} \epsilon_{t}+L \log T \
\rho_{T} &amp; \leq \frac{1}{T} \rho_{1}+\frac{1}{T} \sum_{t=1}^{T-1} \epsilon_{t}+\frac{L_{\mu} \log T}{T}
\end{aligned}
$$</p>
<p>Lastly as, we are interested in suboptimality of the actual function $\varrho_{t+1}=F\left(\eta_{t+1}\right)-F\left(\eta^{*}\right)$, which can be bound as</p>
<p>$$
\begin{aligned}
&amp; \varrho_{T} \quad \leq \quad \rho_{T}+2 \mu \leq 2 \mu+\frac{1}{T} \rho_{1}+\frac{1}{T} \sum_{t=1}^{T-1} \epsilon_{t}+\frac{L_{\mu} \log T}{T} \
&amp; \stackrel{\text { Lemma }}{ } 2 \quad \frac{1}{T} \rho_{1}+\frac{1}{T} \sum_{t=1}^{T-1} \epsilon_{t}+\frac{\log T}{T} \frac{1}{\mu}+2 \mu \
&amp; \stackrel{\text { Lemma }}{ } 5 \quad \frac{1}{T} \rho_{1}+\frac{1}{T} \sqrt{2 \sum_{t=1}^{T}\left|\nabla F_{\mu}\left(\eta_{t}\right)\right|<em 1="1">{\infty}^{2} \log \left(\frac{2}{\delta}\right)}+\frac{\log T}{T} \frac{1}{\mu}+2 \mu \
&amp; \stackrel{\mu=\sqrt{\log T / T}}{\leq} \quad \frac{1}{T} \rho</em>\right)\right|}+\frac{1}{T} \sqrt{2 \sum_{t=1}^{T}\left|\nabla F_{\mu}\left(\eta_{t<em 1="1">{\infty}^{2} \log \left(\frac{2}{\delta}\right)}+3 \sqrt{\frac{\log T}{T}} \
&amp; \stackrel{\text { Lemma }}{ } 3 \quad \frac{1}{T} \rho</em>\right)\right|}+\frac{1}{T} \sqrt{2 \sum_{t=1}^{T}\left|\nabla F\left(\eta_{t<em 1="1">{\infty}^{2} \log \left(\frac{2}{\delta}\right)}+3 \sqrt{\frac{\log T}{T}} \
&amp; \leq \quad \frac{1}{T} \varrho</em>
\end{aligned}
$$}+2 \sqrt{\frac{\log T}{T^{3}}}+\frac{1}{T} \sqrt{2 \sum_{t=1}^{T}\left|\nabla F\left(\eta_{t}\right)\right|_{\infty}^{2} \log \left(\frac{2}{\delta}\right)}+3 \sqrt{\frac{\log T}{T}</p>
<p>This proves the theorem for ONE-STEP variant. Now we will focus on the EXACT variant. The exact variant uses,</p>
<p>$$
v_{t}=\arg \min <em t="t">{q \in \mathcal{P}} G</em>(q):=\arg \min <em _mu="\mu">{q \in \mathcal{P}} F</em>\right)\right)
$$}\left(\eta_{t}+\frac{1}{t+1}\left(q-\eta_{t</p>
<p>First, notice that due to the convexity of $F_{\mu}, G_{t}$ is convex as well. Also, note that $\frac{1}{1+t} \nabla F_{\mu}\left(\eta_{t}\right)=\nabla G_{t}\left(\eta_{t}\right)$. Using convexity,</p>
<p>$$
\frac{1}{1+t} \nabla F_{\mu}\left(\eta_{t}\right)^{\top}\left(v_{t}-\eta_{t}\right)=\nabla G_{t}(\eta)^{\top}\left(v_{t}-\eta_{t}\right) \leq G_{t}\left(v_{t}\right)-G_{t}\left(\eta_{t}\right)=G_{t}\left(v_{t}\right)-F_{\mu}\left(\eta_{t}\right)
$$</p>
<p>Following the similar analysis notice,</p>
<p>$$
\begin{aligned}
F_{\mu}\left(\eta_{t+1}\right) &amp; =F_{\mu}\left(\eta_{t}+\frac{1}{t+1}\left(\delta_{t}-\eta_{t}\right)\right) \
&amp; \leq F_{\mu}\left(\eta_{t}\right)+\frac{1}{1+t} \nabla F_{\mu}\left(\eta_{t}\right)^{\top}\left(\delta_{t}-\eta_{t}\right)+\frac{L_{\mu}}{(1+t)^{2}} \
&amp; \leq F_{\mu}\left(\eta_{t}\right)+\frac{1}{1+t} \nabla F_{\mu}\left(\eta_{t}\right)^{\top}\left(v_{t}-\eta_{t}\right)+\frac{1}{1+t} \nabla F_{\mu}\left(\eta_{t}\right)^{\top}\left(\delta_{t}-v_{t}\right)+\frac{L_{\mu}}{(1+t)^{2}} \
&amp; \stackrel{(54)}{\leq} F_{\mu}\left(\eta_{t}+\frac{1}{t+1}\left(v_{t}-\eta_{t}\right)\right)+\frac{1}{1+t} \nabla F_{\mu}\left(\eta_{t}\right)^{\top}\left(\delta_{t}-v_{t}\right)+\frac{L_{\mu}}{(1+t)^{2}}
\end{aligned}
$$</p>
<p>$$
\stackrel{(23)}{\leq} F_{\mu}\left(\eta_{t}+\frac{1}{t+1}\left(q_{t}-\eta_{t}\right)\right)+\frac{1}{1+t} \nabla F_{\mu}\left(\eta_{t}\right)^{\top}\left(\delta_{t}-v_{t}\right)+\frac{L_{\mu}}{(1+t)^{2}}
$$</p>
<p>where the last line follows due to $v_{t}$ being a minimizer. The rest of the analysis is identical to the ONE-STEP variant.</p>
<p>The consequence of this theorem is that if the gradient $\left|\nabla F\left(\eta_{t}\right)\right|<em t="t">{\infty} \approx \mathcal{O}(1)$ and $\Delta</em>\right)$ limited by the concentration event. Now, in order not to be limited by the concentration event, we will look at the convergence in expectation.} \propto \mathcal{O}\left(t^{-1 / 2}\right)$, or zero, the overall complexity is of order $\mathcal{O}\left(\sqrt{\frac{\log T}{T}</p>
<p>Theorem 4 (Convergence expectation). Under Assumption 1, the convergence of ONE-STEP and EXACT variant satisfies</p>
<p>$$
\mathbb{E}\left[F\left(\eta_{t}\right) \mid \mathcal{F}<em k="1">{t-1}\right]-F\left(\eta^{<em>}\right) \leq \frac{1}{t}\left(F\left(\eta_{1}\right)-F\left(\eta^{</em>}\right)\right)+\frac{1}{t} \sum</em>
$$}^{t-1} \frac{L_{\eta_{k}, 1 / k}}{(1+k)</p>
<p>for $t \leq T$, where the expectation over stochasticity of policies $\left{\pi_{k}\right}<em t-1="t-1">{k=1}^{t}$, and the Markov chain, and, where $\mathcal{F}</em>$ approximates}$ designates filtration up to time $t-1$. For ONE-STEP variant $\Delta_{k}=0$ for all $k$, and for EXACT variant each subproblem is solved to near-optimality s.t. $\tilde{v}_{k</p>
<p>$$
v_{k}=\arg \min <em k="k">{q \in \mathcal{P}} F\left(\eta</em>}+\frac{1}{k+1}\left(q-\eta_{k}\right)\right), \text { and } F\left(\tilde{v<em k="k">{k}\right)-F\left(v</em>
$$}\right) \leq \Delta_{k</p>
<p>Proof. Following the proof of Theorem 3, we note that by taking expectation over $\mathbb{E}\left[\epsilon_{k}\right]=\nabla F\left(\eta_{k}\right)^{\top}\left(-q_{k}+\mathbb{E}\left[\delta_{k}\right]\right)=0$. Notice that we apply the smoothness under Assumption 2 not due to Nesterov (2005) smoothing technique.</p>
<p>$$
\begin{aligned}
&amp; F\left(\eta_{k+1}\right)=F\left(\eta_{k}+\frac{1}{k+1}\left(\delta_{k}-\eta_{k}\right)\right) \
&amp; \stackrel{(0)}{\leq} F\left(\eta_{k}\right)+\frac{1}{k+1} \nabla F\left(\eta_{k}\right)^{\top}\left(\delta_{k}-\eta_{k}\right)+\frac{L_{\eta_{k}, 1 / k}}{2(1+k)^{2}}\left|\delta_{k}-\eta_{k}\right|<em k="k">{2}^{2} \
&amp; \leq F\left(\eta</em> \
&amp; \mathbb{E}\left[F\left(\eta_{k+1}\right)\right] \quad \leq \quad F\left(\eta_{t}\right)+\frac{1}{k+1} \nabla F\left(\eta_{k}\right)^{\top}\left(q_{k}-\eta_{k}\right)+\frac{L_{\eta_{k}, 1 / k}}{(1+k)^{2}} \
&amp; \leq \quad F\left(\eta_{t}\right)+\frac{1}{k+1} \nabla F\left(\eta_{k}\right)^{\top}\left(\eta^{}\right)+\frac{1}{k+1} \nabla F\left(\eta_{k}\right)^{\top}\left(\delta_{k}-\eta_{k}\right)+\frac{1}{k+1} \nabla F\left(\eta_{k}\right)^{\top}\left(-q_{k}+\delta_{k}\right)+\frac{L_{\eta_{k}, 1 / k}}{(1+k)^{2}<em>}-\eta_{k}\right)+\frac{L_{\eta_{k}, 1 / k}}{(1+k)^{2}} \
&amp; \text { convexity } \quad F\left(\eta_{t}\right)-\frac{1}{k+1}\left(F\left(\eta_{k}\right)-F\left(\eta^{</em>}\right)\right)+\frac{L_{\eta_{k}, 1 / k}}{(1+k)^{2}} \
&amp; \mathbb{E}\left[F\left(\eta_{k+1}\right)\right]-F\left(\eta^{<em>}\right) \quad \leq \quad F\left(\eta_{t}\right)-F\left(\eta^{</em>}\right)-\frac{1}{k+1}\left(F\left(\eta_{k}\right)-F\left(\eta^{*}\right)\right)+\frac{L_{\eta_{k}, 1 / k}}{(1+k)^{2}}
\end{aligned}
$$</p>
<p>Using shorthand $\rho_{k+1}=\mathbb{E}\left[F\left(\eta_{k+1}\right)\right]-F\left(\eta^{*}\right)$,</p>
<p>$$
\begin{aligned}
\rho_{k+1}(k+1) &amp; \leq k \rho_{k}+\frac{L_{\eta_{k}, 1 / k}}{(1+k)} \
\sum_{k=1}^{t-1} \rho_{k+1}(k+1)-k \rho_{k} &amp; \leq \sum_{k=1}^{t-1} \frac{L_{\eta_{k}, 1 / k}}{(1+k)} \
\rho_{t} \leq \frac{1}{t} \rho_{1}+\frac{1}{t} \sum_{k=1}^{t-1} \frac{L_{\eta_{k}, 1 / k}}{(1+k)}
\end{aligned}
$$</p>
<p>The proof for the exact variant follows analogically as in the proof of Theorem 3.
Notice that if $L_{\eta_{k}, 1 / k}$ could be globally bounded the suboptimality decreases as $\mathcal{O}(\log T / T)$.</p>
<h1>B. 5 Robust and Uncertain objectives</h1>
<p>The functional $\mathbf{C}$ studied in the paper can itself depend on an unknown quantity, which we designate as $\gamma$ as $\mathbf{C}<em t="t">{\gamma}$, where $\gamma \in \Gamma</em>$ as in Eq. (4) might not be known in advance. To deal with this complication, there are two approaches one can take:}, t \in[T]$. For example, the value of $\sigma_{a, x</p>
<ul>
<li>Robust design: take a supremum over the set $\Gamma$, and have a design that takes into account any possible values of $\gamma$. If $F$ is convex then so is the supremum over the compact set.</li>
<li>Sequential design: amend the objective $F$ with the new value of the supremum if by executing a trajectory we can reduce the size of the set $\Gamma_{t}$. In the context of the example (4), we can learn the variance from repeated samples and update over confidence set over them $\Gamma_{t}$.</li>
</ul>
<p>We give examples for both of these design approaches in the experimental Section 7.</p>
<h2>B. 6 Linear MDPs: Density Oracle</h2>
<p>If the system is not tabular, the formula given in Sec.4.1 to calculate $d$ cannot be used. However, one can always estimate $d$ via sampling which converges at a rate $N^{-1 / 2}$ with a number of trajectory samples $N$ given that we know the 'simulator' $P$.</p>
<p>We provide a specific way to calculate the density for recently introduced class of Markov chains called linear MDPs (Jin et al., 2020), which stipulate that $P\left(x^{\prime} \mid x, a\right)=\mu\left(x^{\prime}\right)^{\top} \psi(x, a)$, where $\cdot^{\top}$ designates an inner product in an Euclidean space, and $\psi(x, a) \in \mathbb{R}^{m}$, where $m$ is the dimension of the feature space.</p>
<p>Linear MDPs provide a way to improve the scalability when $\mathcal{X}$ is larger or even infinite. In this case, the transition matrix $P_{\pi}$ with policy $\pi$ is equal to $P_{\pi}\left(x^{\prime}, x\right)=\sum_{a} \mu\left(x^{\prime}\right)^{\top} \psi(x, a) \pi(a \mid x)=\mu\left(x^{\prime}\right)^{\top} z_{\pi}(x)$, where $z_{\pi}(x) \in \mathbb{R}^{m}$ is the mean embedding of $\psi(x, a)$ with probability distribution $\pi(a \mid x)$. Lets us define an operator $\mathbf{U}: \mathcal{X} \rightarrow \mathbb{R}^{m}$ and $\mathbf{V}<em _left_pi__i="\left(\pi_{i">{h}: \mathcal{X} \rightarrow \mathbb{R}^{m}$, then $P</em>\right)<em h_="h," i="i">{h}}=\mathbf{U} \mathbf{V}</em>\right)}^{\top}$. Using that $d_{0}=\delta_{x_{0}}$ and the definition from above $d_{\pi_{i}}(x)=\frac{1}{H} \sum_{h=1}^{H} \prod_{i=1}^{h} P_{\left(\pi_{i<em 0="0">{j}} d</em>}(x)=$ $\frac{1}{H} \sum_{h=1}^{H} \mu(x)\left(\prod_{i=2}^{h} \mathbf{V<em 0="0">{h, i}^{\top} \mathbf{U}\right) z\left(x</em>}\right)=\mu \frac{1}{H} \sum_{h=1}^{H} \mu(x) \mathbf{M<em 0="0">{h, i} z\left(x</em>|$ infinite.}\right)$, where $\mathbf{M} \in \mathbb{R}^{m \times m}$. Hence in order to estimate $d_{\pi}(x)$ we only need to multiply $m \times m$ matrices despite $|\mathcal{X</p>
<h2>B. 7 Relationship to Submodular optimization</h2>
<p>The objective $F$ can be reformulated as set function $H(S): 2^{\mathcal{T}} \rightarrow \mathbb{R}$ defined on subsets $S=\left{\tau_{1}, \ldots, \tau_{t}\right}$ of the set of all possible trajectories. In addition, if $H$ is submodular, as is the case with $\log$ det objective (Krause and Guestrin, 2005), we can benefit from the $\left(1-e^{-1}\right)$ approximation guarantee for the greedy algorithm discovered by Nemhauser et al. (1978) when solving cardinality constrained maximization $\max <em S="S" _in="\in" _tau="\tau">{|S| \leq T} H(S)$. The objective $H$ upon reformulation can be related to the objective studied in this work as $H(S)=\log \operatorname{det}\left(\sum</em> /|S|\right)$. The objective values are only are directly relatable only when $|S|=T$ due to different way of handling regularization.} I(\tau)+\lambda \mathbf{I}\right)=d \log (|S|)-F\left(\frac{1}{|S|} \sum_{i=1}^{|S|} \delta_{\tau_{i}}\right)$, where $F(\eta)=-\log \operatorname{det}\left(\sum_{\tau \in \mathcal{T}} \eta(\tau) I(\tau)+\lambda \mathbf{I</p>
<p>Nevertheless, we can show that greedy selection from the ground formed by the power set of $\mathcal{T}, 2^{\mathcal{T}}$ defines as:</p>
<p>$$
\tau_{t}=\underset{\tau \in \mathcal{T}}{\arg \max } H(S \cup{\tau})
$$</p>
<p>leads to the same update as if we optimized the convex-relaxation used within our framework,</p>
<p>$$
\underset{\tau \in \mathcal{T}}{\arg \min } F\left(\frac{1}{|S|+1}\left(\sum_{i=1}^{|S|} \delta_{\tau_{i}}+\delta_{\tau}\right)\right)
$$</p>
<p>with the specially chosen $\frac{1}{1+t}$ step-size (as used in this work). The objectives cannot be easily compared, and hence greedy or convex optimization guarantees are different with the two viewpoints, but both frameworks lead to the same solutions, i.e. same selection of trajectories, in the end when convex-RL is run with the varying regularization constant $\lambda_{t}=\lambda / t$; a minor technicality.</p>
<p>Stochastic set cover However, in our case, we cannot pick trajectories exactly (unless the system is deterministic). We pick distributions over $\mathcal{T}, q \in \mathcal{P}$. In particular a trajectory $\tau \sim q$ is generated from $q$ (or likewise associated $\pi$ ), and hence the objective we study is $\tilde{H}\left(\left{\tau_{1}, \ldots\right}\right)=\mathbb{E}<em i="i">{\tau</em>$ as a function of policies is still submodular. In fact, this formulation exactly coincides with the stochastic set cover problem} \sim \pi_{i}}\left[H\left(\left{\tau_{i}, \ldots, \tau_{T}\right}\right)\right]$ where the ground set changes to the set of all policies. Submodularity is preserved upon nonnegative linear combinations (see Krause and Golovin (2014)), hence the objective $\tilde{H</p>
<p>(Golovin and Krause, 2011). However the parallel to convex allocation used in this work is more complicated now. Notice that the greedy step of choosing a policy $\pi_{t+1}$ (equivalently $q_{t}$ ) to sample as an addition to already chosen trajectories $\left{\tau_{1}, \ldots, \tau_{t}\right}$ due to $\left{q_{1} \ldots, q_{t}\right}$ is solving the following objective</p>
<p>$$
q_{t+1}=\underset{q \in \mathcal{P}}{\arg \max }=\mathbb{E}<em t_1="t+1">{\tau</em>\right}\right)\right]
$$} \sim q}\left[H\left(\left{\tau_{1}, \ldots\right} \cup\left{\tau_{t+1</p>
<p>If we were to look at the corresponding greedy update rule, where the convex measures are augmented by a single step (with the step size $1 /(1+t))$ leading to the same update in terms of elements we would arrive at:</p>
<p>$$
q_{t+1}=\arg \min <em _tau__t_1="\tau_{t+1">{q \in \mathcal{P}} \mathbb{E}</em>\right)\right)\right]
$$} \sim q}\left[F\left(\frac{1}{t+1}\left(\sum_{i=1}^{t} \delta_{\tau_{i}}+\delta_{\tau_{t+1}</p>
<p>Again the objective values are different but the sequence of elements chosen is the same. This objective is, however, different to the oracle we are using in Alg. 1 step 2. Notice that our oracle looks for the best $q$ that minimizes</p>
<p>$$
F\left(\frac{t}{t+1} \eta_{t}+\frac{1}{1+t} q\right)=F\left(\frac{1}{1+t}\left(\sum_{i=1}^{t} \delta_{\tau_{i}}+q\right)\right)=F\left(\frac{1}{1+t}\left(\sum_{i=1}^{t} \delta_{\tau_{i}}+\mathbb{E}<em t_1="t+1">{\tau</em>\right]\right)\right)
$$} \sim q}\left[\delta_{\tau_{t+1}</p>
<p>which is related to the above greedy marginal gain via Jensen inequality. Unfortunately, the gap between these two can be large, and hence our method can be seen as a heuristic approximation to the greedy algorithm without an explicit guarantee on the greedy oracle. Only in the case when $q \in \mathcal{P}$ where $\mathcal{P}$ is restricted to trajectory probabilities due to the deterministic policies, do the two methods coincide (as in the previous paragraph). This is because the expectation contains only one term and the Jensen gap is zero. This suggests that greedy formulation as in Eq. (30) is more powerful, however as we Sec. 3, we still can be competitive with respect to the objective where the expectation is taken outside.</p>
<h1>B. 8 Sequential Design and Uncertain Objectives</h1>
<p>It turns out that we can be competitive to the true value of $\gamma^{*} \in \Gamma_{t}$ in the set $\Gamma_{t}$ if the set decreases with time $t$. Its decrease influences the rate and the convergence can be guaranteed if it decreases at least as $1 / \sqrt{t}$ if the size of $\Gamma_{t}$ is measured as Euclidean diameter and the set $\Gamma_{t} \subset \mathbb{R}^{m}$ as we show in the following assumption. We do not directly show that $\Gamma_{t}$ decreases as this are different for each application and it might not be true in general.</p>
<p>Assumption 2 (Regularity with respect to the unknown). Under Assumption 1 suppose further that either</p>
<ol>
<li>$F_{\gamma}(\eta)$ is Lipschitz in $\gamma$ for EXACT variant:</li>
</ol>
<p>$$
\left|F_{\gamma}(\eta)-F_{\gamma^{\prime}}(\eta)\right| \leq U\left|\gamma-\gamma^{\prime}\right|_{2}
$$</p>
<p>where $U$ is independent of $\eta$.
2. $\nabla F_{\gamma}(\eta)$ is Lipschitz in $\gamma$ for ONE-STEP variant:</p>
<p>$$
\left|\nabla F_{\gamma}(\eta)-\nabla F_{\gamma^{\prime}}(\eta)\right|<em 2="2">{2} \leq U\left|\gamma-\gamma^{\prime}\right|</em>
$$</p>
<p>where $U$ is independent of $\eta$.
The proof of the following theorem is very much inspired by Berthet and Perchet (2017), where this is essentially a Frank-Wolfe UCB algorithm. Note also that for the example of Poisson sensing from Sec. 7 we know that the set $\left|\gamma^{*}-\gamma_{t}\right|_{2}$ decreases at least as $1 / \sqrt{t}$ since the objective is exactly designed to reduce the uncertainty of the $f$ and hence $\gamma$ and $f$ are the same this must be true.</p>
<p>Theorem 5 (Convergence for unknown). Under Assumptions 1 and 2 suppose there exists $F_{\gamma^{<em>}}$, where $\gamma^{</em>} \in \Gamma_{t}$ for all $t \in[T]$ is unknown, then using smoothed target of $F_{t}(\eta)=\sup <em t="t">{\gamma \in \Gamma</em>$ in each round $t \in[T]$ of EXACT algorithm we can show}} F_{\gamma}(\eta)$, denoted as $F_{t, \mu}$, with iteration varying smoothing parameter $\mu=\sqrt{\log T / T</p>
<p>$$
F_{\gamma^{<em>}}\left(\eta_{T}\right)-F_{\gamma^{</em>}}\left(\eta^{*}\right) \leq \mathcal{O}\left((U+B) \sqrt{\frac{\log \left(\frac{3}{2}\right)+\log T}{T}}\right)
$$</p>
<p>Using oracle $\min <em _Gamma__t="\Gamma_{t" _gamma="\gamma" _in="\in">{q \in \mathcal{P}} \inf </em>$ in ONE-STEP algorithm we can show,}} \nabla F_{\gamma, \mu}\left(\eta_{t}\right)^{\top} q$, where $F_{\gamma, \mu}$ is the smoothed objective with $\mu=\sqrt{\log T / T</p>
<p>$$
F_{\gamma^{<em>}}\left(\eta_{T}\right)-F_{\gamma^{</em>}}\left(\eta^{*}\right) \leq \mathcal{O}\left((U+B) \sqrt{\frac{\log \left(\frac{2}{2}\right)+\log T}{T}}\right)
$$</p>
<p>as long as $\left|\gamma^{<em>}-\bar{\gamma}<em t="t">{t}\right| \leq \mathcal{O}(1 / \sqrt{t})$, where $\bar{\gamma}</em>\left|\nabla F_{\gamma^{}$ achieves the supremum value in $\Gamma_{t}$. The value $B=\sqrt{\frac{1}{T} \sum_{t=1}^{T</em>}}\left(\eta_{t}\right)\right|_{\infty}^{2}}$ denotes the average gradient of the objective. The statements hold with $1-\delta$ probability.</p>
<p>Proof.</p>
<p>EXACT In what follows we utilizing nearly identical proof technique as in Theorem 3. Namely, we continue where Eq. (29) left. Due to notation clutter, we drop the $\mu$ dependence on in $F$,</p>
<p>$$
\begin{aligned}
&amp; F_{\gamma^{<em>}}\left(\eta_{t+1}\right) \quad \leq \quad F_{\gamma^{</em>}}\left(\eta_{t}+\frac{1}{1+t}\left(v_{t}-\eta_{t}\right)\right)+\frac{1}{t+1} \nabla F_{\gamma^{<em>}}\left(\eta_{t}\right)^{\top}\left(-v_{t}+\delta_{t}\right)+\frac{1 / \mu}{(1+t)^{2}} \
&amp; \text { due to } \sup <em _bar_gamma="\bar{\gamma">{\gamma} \quad F</em><em t="t">{t}}\left(\eta</em>-\gamma^{}+\frac{1}{1+t}\left(v_{t}-\eta_{t}\right)\right)+U\left|\bar{\gamma}_{t</em>}\right|<em>{2}+\frac{1}{t+1} \nabla F</em>{\gamma^{<em>}}\left(\eta_{t}\right)^{\top}\left(-v_{t}+\delta_{t}\right)+\frac{1 / \mu}{(1+t)^{2}} \
&amp; \stackrel{(23)}{\leq} \quad F_{\bar{\gamma}<em t="t">{t}}\left(\eta</em>-\gamma^{}+\frac{1}{1+t}\left(q_{t}-\eta_{t}\right)\right)+U\left|\bar{\gamma}_{t</em>}\right|<em>{2}+\frac{1}{t+1} \nabla F</em>{\gamma^{<em>}}\left(\eta_{t}\right)^{\top}\left(-v_{t}+\delta_{t}\right)+\frac{1 / \mu}{(1+t)^{2}} \
&amp; \stackrel{(9)}{\leq} \quad F_{\bar{\gamma}<em t="t">{t}}\left(\eta</em>}\right)+\frac{1}{1+t} \nabla F_{\bar{\gamma<em t="t">{t}}\left(\eta</em>-\gamma^{}\right)^{\top}\left(q_{t}-\eta_{t}\right)+U\left|\bar{\gamma}_{t</em>}\right|<em>{2}+\frac{1}{t+1} \nabla F</em>{\gamma^{<em>}}\left(\eta_{t}\right)^{\top}\left(-v_{t}+\delta_{t}\right)+\frac{2 / \mu}{(1+t)^{2}} \
&amp; \stackrel{(21)}{\leq} \quad F_{\bar{\gamma}<em t="t">{t}}\left(\eta</em>}\right)+\frac{1}{1+t} \nabla F_{\bar{\gamma<em t="t">{t}}\left(\eta</em>\left(\eta^{}\right)^{\top</em>}-\eta_{t}\right)+U\left|\bar{\gamma}<em t="t">{t}-\gamma^{<em>}\right|<em>{2}+\frac{1}{t+1} \nabla F</em>{\gamma^{</em>}}\left(\eta</em> \
&amp; \stackrel{(9),(31)}{\leq} \quad F_{\gamma^{}\right)^{\top}\left(-v_{t}+\delta_{t}\right)+\frac{2 / \mu}{(1+t)^{2}<em>}}\left(\eta_{t}\right)-\frac{1}{1+t}\left(F_{\gamma^{</em>}}\left(\eta_{t}\right)-F_{\gamma^{<em>}}\left(\eta^{</em>}\right)\right)+2 U\left(1+(1+t)^{-1}\right)\left|\bar{\gamma}<em t="t">{t}-\gamma^{<em>}\right|<em>{2} \
&amp; +\frac{1}{t+1} \nabla F</em>{\gamma^{</em>}}\left(\eta</em> \
&amp; \leq \quad F_{\gamma^{}\right)^{\top}\left(-v_{t}+\delta_{t}\right)+\frac{2 / \mu}{(1+t)^{2}<em>}}\left(\eta_{t}\right)-\frac{1}{1+t}\left(F_{\gamma^{</em>}}\left(\eta_{t}\right)-F_{\gamma^{<em>}}\left(\eta^{</em>}\right)\right)+4 U\left|\bar{\gamma}<em t="t">{t}-\gamma^{<em>}\right|<em>{2} \
&amp; +\frac{1}{t+1} \nabla F</em>{\gamma^{</em>}}\left(\eta</em>
\end{aligned}
$$}\right)^{\top}\left(-v_{t}+\delta_{t}\right)+\frac{2 / \mu}{(1+t)^{2}</p>
<p>The rest of the proof is similar to Thm. 3, where $\rho_{t}=F_{\gamma^{<em>}}\left(\eta_{t}\right)-F_{\gamma^{</em>}}\left(\eta^{<em>}\right), \epsilon_{t}=\nabla F_{\gamma^{</em>}}\left(\eta_{t}\right)^{\top}\left(-v_{t}+\delta_{t}\right)$,</p>
<p>$$
\rho_{t+1}(t+1) \leq t \rho_{t}+4 U\left|\bar{\gamma}<em 2="2">{t}-\gamma^{*}\right|</em>
$$}+\epsilon_{t}+\frac{2 / \mu}{(1+t)</p>
<p>Summing both sides on $t=1$ to $T-1$,</p>
<p>$$
\begin{aligned}
T \rho_{T}-\rho_{1} &amp; \leq \sum_{t=1}^{T-1} 4 U\left|\bar{\gamma}<em t="t">{t}-\gamma^{*}\right|+\epsilon</em> \
\rho_{T} &amp; \leq \frac{1}{T} \rho_{1}+\frac{4 U}{\sqrt{T}}+B \sqrt{\frac{2 \log \left(\frac{2}{2}\right)}{T}}+\sqrt{\frac{\log T}{T}}
\end{aligned}
$$}+\sqrt{\frac{\log T}{T}</p>
<p>where in the last step we used the fact that the distance $\left|\bar{\gamma}<em t="1">{t}-\gamma^{*}\right| \leq 1 / \sqrt{t}$, and the Lemma 5 to bound the deviation of $\sum</em>$ which does correspond to the leading term.
ONE-STEP Dropping $\mu$ from $F$ as above and utilizing nearly identical proof technique as in Thm. 3 we can show the following,}^{T} \epsilon_{t}$. Again, notice that due to to the smoothing the value is off by $\sqrt{\frac{\log T}{T}</p>
<p>$$
F_{\gamma^{<em>}}\left(\eta_{t+1}\right) \quad \stackrel{(21)}{\leq} \quad F_{\gamma^{</em>}}\left(\eta_{t}\right)+\frac{1}{t+1} \nabla F_{\gamma^{<em>}}\left(\eta_{t}\right)^{\top}\left(q_{t}-\eta_{t}\right)+\frac{1}{t+1} \nabla F_{\gamma^{</em>}}\left(\eta_{t}\right)^{\top}\left(-q_{t}+\delta_{t}\right) \frac{1 / \mu}{(1+t)^{2}}
$$</p>
<p>$$
\begin{aligned}
= &amp; F_{\gamma^{<em>}}\left(\eta_{t}\right)+\frac{1}{t+1}\left(\nabla F_{\gamma^{</em>}}\left(\eta_{t}\right)-\nabla F_{\bar{\gamma}<em t="t">{t}}\left(\eta</em>}\right)+\nabla F_{\bar{\gamma<em t="t">{t}\left(\eta</em>\right) \
&amp; +\frac{1}{t+1} \nabla F_{\gamma^{}\right)}\right)^{\top}\left(q_{t}-\eta_{t<em>}}\left(\eta_{t}\right)^{\top}\left(-q_{t}+\delta_{t}\right)+\frac{1 / \mu}{(1+t)^{2}} \
\stackrel{(32)}{\leq} &amp; F_{\gamma^{</em>}}\left(\eta_{t}\right)+\frac{1}{t+1}\left(\nabla F_{\bar{\gamma}<em t="t">{t}\left(\eta</em>\right)+U\left|\gamma^{}\right)}\right)^{\top}\left(q_{t}-\eta_{t<em>}-\bar{\gamma}<em 2="2">{t}\right|</em>\right|}\left|q_{t}-\eta_{t<em>{2} \
&amp; +\frac{1}{t+1} \nabla F</em>{\gamma^{</em>}}\left(\eta_{t}\right)^{\top}\left(-q_{t}+\delta_{t}\right)+\frac{1 / \mu}{(1+t)^{2}} \
\text { update rule } &amp; F_{\gamma^{<em>}}\left(\eta_{t}\right)+\frac{1}{t+1} \nabla F_{\gamma^{</em>}}\left(\eta_{t}\right)^{\top}\left(\eta^{<em>}-\eta_{t}\right)+U\left|\gamma^{</em>}-\bar{\gamma}<em 2="2">{t}\right|</em>\right|}\left|q_{t}-\eta_{t<em _gamma_="\gamma^{*">{2} \
&amp; +\frac{1}{t+1} \nabla F</em>
\end{aligned}
$$}}\left(\eta_{t}\right)^{\top}\left(-q_{t}+\delta_{t}\right)+\frac{1 / \mu}{(1+t)^{2}</p>
<p>where in the last step, we used the update rule $q_{t}=\arg \min <em _Gamma__t="\Gamma_{t" _gamma="\gamma" _in="\in">{q \in \mathcal{P}} \inf </em> q$. The problem as appears now is reduced to the proof of Theorem 3, as all terms feature only $\gamma^{}} \nabla F_{\gamma}\left(\eta_{t}\right)^{\top<em>}$, and the only additional factor is $\sum_{t=1}^{t-1} U\left|\gamma^{</em>}-\bar{\gamma}<em 2="2">{t}\right|</em>)$ due to the assumption in the theorem, which finished the proof.}\left|q_{t}-\eta_{t}\right|_{2} \leq 2 U / \sqrt{t}$ upon summing as in Theorem 3, which is $\mathcal{O}(U / \sqrt{T</p>
<h1>B. 9 Relationship to multi-agent systems</h1>
<p>If we had $T$ agents that we were to release jointly at the same time, or equivalently we would like to plan jointly for $T$ episodes in one optimization step, we could increase the state-action space by assuming the new action space be $(\mathcal{X} \times \mathcal{A}) \times(\mathcal{X} \times \mathcal{A}) \ldots$, where we do the product $T$ times. This increases the action-space exponentially in $T$, and we denote it $\tilde{\mathcal{X}} \times \tilde{\mathcal{A}}$. Visiting the state $x_{i}^{(t)}, a_{i}^{(t)}$ in episode $t$ does not provide any different information and hence the observations for $\Phi\left(x_{i}^{(t)}, a_{i}^{(t)}\right)=\Phi\left(x_{i}^{\left(t^{\prime}\right)}, a_{i}^{\left(t^{\prime}\right)}\right)$ for all $t, t o \in[T]$. Hence, let us just drop the time superscript.
Thus, if we were to consider the information matrix due to observing a states $\left{\left(x_{i}^{(t)}, a_{i}^{(t)}\right)\right}_{t=1}^{T}$ as a function of state-action visitation over $\tilde{d}$ over $\tilde{\mathcal{X}} \times \tilde{\mathcal{A}}$ :</p>
<p>$$
U(\tilde{d})=f\left(\sum_{x_{i}, a_{i} \in \mathcal{X} \times \mathcal{A}} \Phi\left(x_{i}, a_{i}\right) \Phi\left(x_{i}, a_{i}\right)^{\top}\left(\sum_{t=1}^{T} \tilde{d}\left(x_{i}^{(t)}, a_{i}^{(t)}\right)\right)+\mathbf{1} \lambda\right) \quad \tilde{d} \in \tilde{\mathcal{D}}
$$</p>
<p>where $f$ is the scalarization and $\tilde{\mathcal{D}}$ is the average state-action polytope on $\tilde{\mathcal{X}} \times \tilde{\mathcal{A}}$. The equivalent solution is to pick $d\left(x_{i}^{(t)}, a_{i}^{(t)}\right)=d\left(x_{i}, a_{i}\right)$ fixed for all $t$, which does not arise when optimizing jointly. We would arrive at an improved solution over the solution in Sec. 4.1. However, optimizing for multiple reruns jointly has two disadvantages a) blows up the state-action space combinatorially and b) does not adapt to the prior executed trajectories.</p>
<h2>C Experiments: Further Information</h2>
<p>In this section, we provide details of the experiments that we introduced in Sec. 7. Before we do so a couple of general comments. When optimizing over the polytope $\mathcal{D}$, we always use an average-case polytope due to simpler implementations. This, from our experience, does not reduce the performance but provides a significant simplification in the code, and is a commonly made simplification found in other works such as Hazan et al. (2019). Secondly, we never use Nesterov (2005) smoothing technique, which is strictly not necessary since all our objectives are smooth only with globally large smoothness constants. Despite this, the algorithm performs very well suggesting the analysis is pessimistic in nature.</p>
<p>To run the experiments we used a smaller server-class machine with 28 CPU cores that we utilized for no more than 20 hours of active time. In general, this is a methodological paper and does not rely on any heavy calculation.</p>
<p>Synthetic grid In this experiment we use $H=20$. The exact location of different unit vectors as described in Sec. 7 is visualized in Fig. 1 with different pictograms. The optimization is run with EXACT-method such that when the duality gap is below $\epsilon=0.05$ the optimization terminates. We used the exact line search with bisections to solve the line search problem. We always marginalize the policy before execution; except for TRACKING variant. The initial state is in the lower-left corner while the final is in the upper-right corner as in Fig. 1.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Special thanks to XY for the proof of this lemma.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
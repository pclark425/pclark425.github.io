<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3306 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3306</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3306</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-9e12539d92088001e08b1e903c490127c479de4c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9e12539d92088001e08b1e903c490127c479de4c" target="_blank">Transformers as Soft Reasoners over Language</a></p>
                <p><strong>Paper Venue:</strong> International Joint Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> This work trains transformers to reason (or emulate reasoning) over natural language sentences using synthetically generated data, thus bypassing a formal representation and suggesting a new role for transformers, namely as limited "soft theorem provers" operating over explicit theories in language.</p>
                <p><strong>Paper Abstract:</strong> Beginning with McCarthy's Advice Taker (1959), AI has pursued the goal of providing a system with explicit, general knowledge and having the system reason over that knowledge. However, expressing the knowledge in a formal (logical or probabilistic) representation has been a major obstacle to this research. This paper investigates a modern approach to this problem where the facts and rules are provided as natural language sentences, thus bypassing a formal representation. We train transformers to reason (or emulate reasoning) over these sentences using synthetically generated data. Our models, that we call RuleTakers, provide the first empirical demonstration that this kind of soft reasoning over language is learnable, can achieve high (99%) accuracy, and generalizes to test data requiring substantially deeper chaining than seen during training (95%+ scores). We also demonstrate that the models transfer well to two hand-authored rulebases, and to rulebases paraphrased into more natural language. These findings are significant as it suggests a new role for transformers, namely as limited "soft theorem provers" operating over explicit theories in language. This in turn suggests new possibilities for explainability, correctability, and counterfactual reasoning in question-answering.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3306.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3306.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa (RuleTaker)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa-large fine-tuned as RuleTaker</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RoBERTa-large transformer fine-tuned to classify whether a statement deductively follows from a context of natural-language facts and rules; used as the primary model in the paper to emulate deductive reasoning over linguistic rulebases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RoBERTa: a robustly optimized BERT pretraining approach</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-large (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A pretrained transformer (masked-language-model objective) further fine-tuned as a binary classifier on inputs of the form [CLS] context [SEP] statement [SEP] to predict whether the statement follows from the context (T/F). Training used synthetic rule/fact datasets of varying inference depth and paraphrased variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['deductive emulation (soft theorem proving)', 'forward-chaining emulation (as learned I/O behavior)', 'negation-as-failure handling (NAF) in context of stratified logic', 'depth-limited/chained reasoning (inference depth up to 5)', 'explanation via critical-sentence identification (perturbation-based)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The model is trained to emulate deductive inference: given a context of facts+rules expressed in (synthetic or paraphrased) English and a candidate statement, it outputs True/False under closed-world semantics. The training data was generated by forward-chaining a formal logic program (including stratified negation/NAF) to produce provable and non-provable statements annotated with inference depth; the transformer learns this mapping end-to-end. Explanation behavior is probed by removing sentences and observing prediction flips to identify 'critical' sentences used by the model.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single, consistent method (deductive emulation) across experiments. The paper does not apply multiple distinct internal reasoning styles (e.g., chain-of-thought vs. scratchpad vs. retrieval); instead it fine-tunes a single transformer architecture to emulate formal deductive behavior. Diversity arises only in dataset variants (depth, negation, paraphrase) and in comparing architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>RuleTaker synthetic rule reasoning (D=0, D<=1, D<=2, D<=3, DMax), ParaRules, Birds, Electricity</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Synthetic datasets of small theories (facts+rules) expressed in English; questions ask whether a statement deductively follows (T/F) under closed-world semantics. Datasets are stratified by required inference depth up to 5 (DMax). Additional tests: zero-shot hand-authored rulebases (Birds, Electricity) and crowdsourced paraphrases (ParaRules).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>On held-out synthetic test sets: near-perfect own-test accuracy (Mod0..Mod3, MMax own test ~99%); generalization to deeper depth (tested on DMax): Mod0 53.5%, Mod1 63.5%, Mod2 83.9%, Mod3 98.9% (rows from Table 1). On paraphrased rules (zero-shot): MMax 66.6% overall on ParaRules (higher for lower depth: Depth0~85.8-100 depending on training), and when fine-tuned on ParaRules + D<=3 (Mod3+Para) achieved 98.8% on ParaRules test. Explanation/critical-sentence identification: macro P=98.7, R=86.9, F1=92.4; removing a critical sentence caused the model's correct T->F flip 81% of the time (not 100% as a formal prover would).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared architectures (all trained in the same deductive-emulation setup): RoBERTa outperforms other models (BERT, ESIM, DECOMP) on both own-test and out-of-distribution depth generalization. Pretraining matters: when words are replaced by random tokens (removing real-word pretraining signal), RoBERTa's accuracy on D<=3 test drops to 83.3% from 99.3%, indicating reliance on pretrained linguistic knowledge. Fine-tuning on paraphrases gives large gains (zero-shot paraphrase performance 66.6% -> 98.8% after fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Transformers (RoBERTa) can be trained to emulate deductive reasoning over rules expressed in English with high accuracy and generalize to greater inference depth than seen in training; they learn a single emulated-deductive style rather than multiple distinct internal reasoning strategies. Pretraining and architecture (transformer) improve performance and ease of learning; fine-tuning on paraphrases yields near-perfect performance on paraphrased natural-language rulebases.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>A purely shallow model (Mod0 trained only on lookup) fails badly on deeper inference and even scores below random for some depths. The learned model is not strictly sound/complete like a theorem prover: removing a critical sentence flips the model only 81% of the time. Zero-shot transfer to crowdsourced paraphrases is partial (MMax 66.6%), requiring fine-tuning for near-perfect performance. Some blind spots observed (e.g., poorer performance on Electricity4 for deeper-trained models due to rare training patterns).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformers as Soft Reasoners over Language', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3306.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3306.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT (RuleTaker)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT fine-tuned as a rule reasoning classifier</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BERT-based transformer tested as an alternative architecture to RoBERTa for the RuleTaker deductive-emulation tasks; achieves strong but somewhat lower performance than RoBERTa.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained transformer (BERT) fine-tuned on the same synthetic datasets (input: [CLS] context [SEP] statement [SEP]) to predict T/F; trained in the same experimental regime to emulate deductive inference over linguistic rules.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['deductive emulation (same as RoBERTa)', 'forward-chaining emulation (learned I/O)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>BERT was used identically to RoBERTa as a binary classifier mapping context+statement to True/False, hence implementing the same learned deductive-emulation behavior rather than a different internal reasoning style.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single method (same deductive emulation as RoBERTa); diversity is across datasets and depth, not reasoning paradigms.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>RuleTaker synthetic rule reasoning (D-level datasets) and DMax generalization</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same synthetic fact+rule T/F tasks as used for RoBERTa.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Own-test accuracies (Table 6): Mod0 100, Mod1 99.3, Mod2 98.2, Mod3 97.0, MMax 96.9. DMax generalization row: Mod0 53.5, Mod1 64.1, Mod2 90.6, Mod3 95.3. These are slightly below RoBERTa but still strong, especially when trained to D<=2/3.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>BERT performs slightly worse than RoBERTa on both in-distribution and out-of-distribution depth generalization, indicating transformer pretraining and/or RoBERTa's optimization give modest gains. No evidence BERT used a different style of reasoning—differences are architectural/optimization-related.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Transformer architectures other than RoBERTa can learn deductive-emulation behavior, achieving high accuracy and depth generalization, though RoBERTa yielded the best results in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>While strong, BERT underperforms RoBERTa by a few percentage points on deeper-generalization tests (e.g., Mod3 on DMax: BERT 95.3% vs RoBERTa 98.9%).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformers as Soft Reasoners over Language', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3306.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3306.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ESIM (LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Enhanced LSTM (ESIM) model for natural language inference</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LSTM-based NLI model (ESIM) evaluated on the RuleTaker datasets; it can learn the mapping but with substantially lower accuracy than transformer models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Enhanced lstm for natural language inference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ESIM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A bidirectional LSTM-based architecture with attention mechanisms (ESIM) trained from scratch (no large-scale pretrained transformer initialization) as a T/F classifier on the same synthetic rule/fact datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['deductive emulation (learned)', 'forward-chaining emulation (learned I/O)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>ESIM was trained in the same end-to-end manner to emulate deductive inference; unlike pretrained transformers, ESIM did not benefit from large-scale language model pretraining in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single method (same deductive-emulation objective). No distinct internal reasoning styles were tested.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>RuleTaker synthetic rule reasoning (D-level datasets) and DMax generalization</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same synthetic T/F rule reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Own-test accuracies (Table 6): Mod0 100, Mod1 90.3, Mod2 87.8, Mod3 84.2, MMax 80.0. DMax generalization row: Mod0 53.5, Mod1 66.4, Mod2 73.2, Mod3 79.6.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>ESIM learns the task but substantially underperforms pretrained transformers, likely due to lack of large-scale pretraining and architectural limits for the kinds of systematic chaining required. The gap indicates transformers + pretraining learn the emulation more effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Non-transformer architectures can learn deductive-emulation behavior but with lower accuracy; pretraining and transformer architectures give an advantage for these datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>ESIM's best accuracy (MMax own test 80%) is far below transformer results (~99%), suggesting that LSTM-based models without pretraining were insufficient for near-perfect emulation in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformers as Soft Reasoners over Language', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3306.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3306.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DECOMP (Decomposable Attention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decomposable Attention Model for Natural Language Inference</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lightweight attention-based NLI model (Decomposable Attention) evaluated on RuleTaker; performs poorly on the task, near-random accuracy in many configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A decomposable attention model for natural language inference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Decomposable Attention (DECOMP)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A simple attention-based NLI architecture (Parikh et al.) trained as a binary classifier on the synthetic RuleTaker data; lacks the parameterization or pretraining to capture deep chained deductive patterns here.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['deductive emulation (learned, but limited)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Like other models, DECOMP was trained to map context+statement to T/F labels; however, its representational capacity and lack of pretraining limits its ability to learn chained deductive patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single method; same end-to-end classification objective, no alternative reasoning styles.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>RuleTaker synthetic rule reasoning (D-level datasets) and DMax generalization</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same synthetic T/F rule reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Own-test accuracies (Table 6): Mod0 72.5, Mod1 68.2, Mod2 58.6, Mod3 57.8, MMax 64.1. DMax generalization row: Mod0 56.5, Mod1 58.1, Mod2 56.4, Mod3 57.4 (near random 50%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>DECOMP fails to learn robust deductive emulation compared to transformers and even ESIM, indicating the task requires more representational power and/or pretraining than DECOMP provides.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Simple attention-based NLI architectures without strong pretraining or capacity do not attain useful performance on these synthetic deductive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>DECOMP often performs close to random, showing the datasets are not trivially solvable by lightweight NLI models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformers as Soft Reasoners over Language', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>RoBERTa: a robustly optimized BERT pretraining approach <em>(Rating: 2)</em></li>
                <li>Enhanced lstm for natural language inference <em>(Rating: 2)</em></li>
                <li>A decomposable attention model for natural language inference <em>(Rating: 2)</em></li>
                <li>Towards AI-Complete question answering: A set of prerequisite toy tasks <em>(Rating: 2)</em></li>
                <li>Nlprolog: Reasoning with weak unification for question answering in natural language <em>(Rating: 2)</em></li>
                <li>Learning a SAT solver from single-bit supervision <em>(Rating: 1)</em></li>
                <li>oLMpics - on what language model pre-training captures <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3306",
    "paper_id": "paper-9e12539d92088001e08b1e903c490127c479de4c",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "RoBERTa (RuleTaker)",
            "name_full": "RoBERTa-large fine-tuned as RuleTaker",
            "brief_description": "RoBERTa-large transformer fine-tuned to classify whether a statement deductively follows from a context of natural-language facts and rules; used as the primary model in the paper to emulate deductive reasoning over linguistic rulebases.",
            "citation_title": "RoBERTa: a robustly optimized BERT pretraining approach",
            "mention_or_use": "use",
            "model_name": "RoBERTa-large (fine-tuned)",
            "model_description": "A pretrained transformer (masked-language-model objective) further fine-tuned as a binary classifier on inputs of the form [CLS] context [SEP] statement [SEP] to predict whether the statement follows from the context (T/F). Training used synthetic rule/fact datasets of varying inference depth and paraphrased variants.",
            "model_size": null,
            "reasoning_methods": [
                "deductive emulation (soft theorem proving)",
                "forward-chaining emulation (as learned I/O behavior)",
                "negation-as-failure handling (NAF) in context of stratified logic",
                "depth-limited/chained reasoning (inference depth up to 5)",
                "explanation via critical-sentence identification (perturbation-based)"
            ],
            "reasoning_methods_description": "The model is trained to emulate deductive inference: given a context of facts+rules expressed in (synthetic or paraphrased) English and a candidate statement, it outputs True/False under closed-world semantics. The training data was generated by forward-chaining a formal logic program (including stratified negation/NAF) to produce provable and non-provable statements annotated with inference depth; the transformer learns this mapping end-to-end. Explanation behavior is probed by removing sentences and observing prediction flips to identify 'critical' sentences used by the model.",
            "diversity_of_methods": "single, consistent method (deductive emulation) across experiments. The paper does not apply multiple distinct internal reasoning styles (e.g., chain-of-thought vs. scratchpad vs. retrieval); instead it fine-tunes a single transformer architecture to emulate formal deductive behavior. Diversity arises only in dataset variants (depth, negation, paraphrase) and in comparing architectures.",
            "reasoning_task_name": "RuleTaker synthetic rule reasoning (D=0, D&lt;=1, D&lt;=2, D&lt;=3, DMax), ParaRules, Birds, Electricity",
            "reasoning_task_description": "Synthetic datasets of small theories (facts+rules) expressed in English; questions ask whether a statement deductively follows (T/F) under closed-world semantics. Datasets are stratified by required inference depth up to 5 (DMax). Additional tests: zero-shot hand-authored rulebases (Birds, Electricity) and crowdsourced paraphrases (ParaRules).",
            "performance_by_method": "On held-out synthetic test sets: near-perfect own-test accuracy (Mod0..Mod3, MMax own test ~99%); generalization to deeper depth (tested on DMax): Mod0 53.5%, Mod1 63.5%, Mod2 83.9%, Mod3 98.9% (rows from Table 1). On paraphrased rules (zero-shot): MMax 66.6% overall on ParaRules (higher for lower depth: Depth0~85.8-100 depending on training), and when fine-tuned on ParaRules + D&lt;=3 (Mod3+Para) achieved 98.8% on ParaRules test. Explanation/critical-sentence identification: macro P=98.7, R=86.9, F1=92.4; removing a critical sentence caused the model's correct T-&gt;F flip 81% of the time (not 100% as a formal prover would).",
            "comparison_of_methods": "Compared architectures (all trained in the same deductive-emulation setup): RoBERTa outperforms other models (BERT, ESIM, DECOMP) on both own-test and out-of-distribution depth generalization. Pretraining matters: when words are replaced by random tokens (removing real-word pretraining signal), RoBERTa's accuracy on D&lt;=3 test drops to 83.3% from 99.3%, indicating reliance on pretrained linguistic knowledge. Fine-tuning on paraphrases gives large gains (zero-shot paraphrase performance 66.6% -&gt; 98.8% after fine-tuning).",
            "key_findings": "Transformers (RoBERTa) can be trained to emulate deductive reasoning over rules expressed in English with high accuracy and generalize to greater inference depth than seen in training; they learn a single emulated-deductive style rather than multiple distinct internal reasoning strategies. Pretraining and architecture (transformer) improve performance and ease of learning; fine-tuning on paraphrases yields near-perfect performance on paraphrased natural-language rulebases.",
            "counter_examples_or_negative_results": "A purely shallow model (Mod0 trained only on lookup) fails badly on deeper inference and even scores below random for some depths. The learned model is not strictly sound/complete like a theorem prover: removing a critical sentence flips the model only 81% of the time. Zero-shot transfer to crowdsourced paraphrases is partial (MMax 66.6%), requiring fine-tuning for near-perfect performance. Some blind spots observed (e.g., poorer performance on Electricity4 for deeper-trained models due to rare training patterns).",
            "uuid": "e3306.0",
            "source_info": {
                "paper_title": "Transformers as Soft Reasoners over Language",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "BERT (RuleTaker)",
            "name_full": "BERT fine-tuned as a rule reasoning classifier",
            "brief_description": "BERT-based transformer tested as an alternative architecture to RoBERTa for the RuleTaker deductive-emulation tasks; achieves strong but somewhat lower performance than RoBERTa.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BERT (fine-tuned)",
            "model_description": "Pretrained transformer (BERT) fine-tuned on the same synthetic datasets (input: [CLS] context [SEP] statement [SEP]) to predict T/F; trained in the same experimental regime to emulate deductive inference over linguistic rules.",
            "model_size": null,
            "reasoning_methods": [
                "deductive emulation (same as RoBERTa)",
                "forward-chaining emulation (learned I/O)"
            ],
            "reasoning_methods_description": "BERT was used identically to RoBERTa as a binary classifier mapping context+statement to True/False, hence implementing the same learned deductive-emulation behavior rather than a different internal reasoning style.",
            "diversity_of_methods": "single method (same deductive emulation as RoBERTa); diversity is across datasets and depth, not reasoning paradigms.",
            "reasoning_task_name": "RuleTaker synthetic rule reasoning (D-level datasets) and DMax generalization",
            "reasoning_task_description": "Same synthetic fact+rule T/F tasks as used for RoBERTa.",
            "performance_by_method": "Own-test accuracies (Table 6): Mod0 100, Mod1 99.3, Mod2 98.2, Mod3 97.0, MMax 96.9. DMax generalization row: Mod0 53.5, Mod1 64.1, Mod2 90.6, Mod3 95.3. These are slightly below RoBERTa but still strong, especially when trained to D&lt;=2/3.",
            "comparison_of_methods": "BERT performs slightly worse than RoBERTa on both in-distribution and out-of-distribution depth generalization, indicating transformer pretraining and/or RoBERTa's optimization give modest gains. No evidence BERT used a different style of reasoning—differences are architectural/optimization-related.",
            "key_findings": "Transformer architectures other than RoBERTa can learn deductive-emulation behavior, achieving high accuracy and depth generalization, though RoBERTa yielded the best results in these experiments.",
            "counter_examples_or_negative_results": "While strong, BERT underperforms RoBERTa by a few percentage points on deeper-generalization tests (e.g., Mod3 on DMax: BERT 95.3% vs RoBERTa 98.9%).",
            "uuid": "e3306.1",
            "source_info": {
                "paper_title": "Transformers as Soft Reasoners over Language",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "ESIM (LSTM)",
            "name_full": "Enhanced LSTM (ESIM) model for natural language inference",
            "brief_description": "An LSTM-based NLI model (ESIM) evaluated on the RuleTaker datasets; it can learn the mapping but with substantially lower accuracy than transformer models.",
            "citation_title": "Enhanced lstm for natural language inference",
            "mention_or_use": "use",
            "model_name": "ESIM",
            "model_description": "A bidirectional LSTM-based architecture with attention mechanisms (ESIM) trained from scratch (no large-scale pretrained transformer initialization) as a T/F classifier on the same synthetic rule/fact datasets.",
            "model_size": null,
            "reasoning_methods": [
                "deductive emulation (learned)",
                "forward-chaining emulation (learned I/O)"
            ],
            "reasoning_methods_description": "ESIM was trained in the same end-to-end manner to emulate deductive inference; unlike pretrained transformers, ESIM did not benefit from large-scale language model pretraining in these experiments.",
            "diversity_of_methods": "single method (same deductive-emulation objective). No distinct internal reasoning styles were tested.",
            "reasoning_task_name": "RuleTaker synthetic rule reasoning (D-level datasets) and DMax generalization",
            "reasoning_task_description": "Same synthetic T/F rule reasoning tasks.",
            "performance_by_method": "Own-test accuracies (Table 6): Mod0 100, Mod1 90.3, Mod2 87.8, Mod3 84.2, MMax 80.0. DMax generalization row: Mod0 53.5, Mod1 66.4, Mod2 73.2, Mod3 79.6.",
            "comparison_of_methods": "ESIM learns the task but substantially underperforms pretrained transformers, likely due to lack of large-scale pretraining and architectural limits for the kinds of systematic chaining required. The gap indicates transformers + pretraining learn the emulation more effectively.",
            "key_findings": "Non-transformer architectures can learn deductive-emulation behavior but with lower accuracy; pretraining and transformer architectures give an advantage for these datasets.",
            "counter_examples_or_negative_results": "ESIM's best accuracy (MMax own test 80%) is far below transformer results (~99%), suggesting that LSTM-based models without pretraining were insufficient for near-perfect emulation in this setup.",
            "uuid": "e3306.2",
            "source_info": {
                "paper_title": "Transformers as Soft Reasoners over Language",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "DECOMP (Decomposable Attention)",
            "name_full": "Decomposable Attention Model for Natural Language Inference",
            "brief_description": "A lightweight attention-based NLI model (Decomposable Attention) evaluated on RuleTaker; performs poorly on the task, near-random accuracy in many configurations.",
            "citation_title": "A decomposable attention model for natural language inference",
            "mention_or_use": "use",
            "model_name": "Decomposable Attention (DECOMP)",
            "model_description": "A simple attention-based NLI architecture (Parikh et al.) trained as a binary classifier on the synthetic RuleTaker data; lacks the parameterization or pretraining to capture deep chained deductive patterns here.",
            "model_size": null,
            "reasoning_methods": [
                "deductive emulation (learned, but limited)"
            ],
            "reasoning_methods_description": "Like other models, DECOMP was trained to map context+statement to T/F labels; however, its representational capacity and lack of pretraining limits its ability to learn chained deductive patterns.",
            "diversity_of_methods": "single method; same end-to-end classification objective, no alternative reasoning styles.",
            "reasoning_task_name": "RuleTaker synthetic rule reasoning (D-level datasets) and DMax generalization",
            "reasoning_task_description": "Same synthetic T/F rule reasoning tasks.",
            "performance_by_method": "Own-test accuracies (Table 6): Mod0 72.5, Mod1 68.2, Mod2 58.6, Mod3 57.8, MMax 64.1. DMax generalization row: Mod0 56.5, Mod1 58.1, Mod2 56.4, Mod3 57.4 (near random 50%).",
            "comparison_of_methods": "DECOMP fails to learn robust deductive emulation compared to transformers and even ESIM, indicating the task requires more representational power and/or pretraining than DECOMP provides.",
            "key_findings": "Simple attention-based NLI architectures without strong pretraining or capacity do not attain useful performance on these synthetic deductive tasks.",
            "counter_examples_or_negative_results": "DECOMP often performs close to random, showing the datasets are not trivially solvable by lightweight NLI models.",
            "uuid": "e3306.3",
            "source_info": {
                "paper_title": "Transformers as Soft Reasoners over Language",
                "publication_date_yy_mm": "2020-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "RoBERTa: a robustly optimized BERT pretraining approach",
            "rating": 2
        },
        {
            "paper_title": "Enhanced lstm for natural language inference",
            "rating": 2
        },
        {
            "paper_title": "A decomposable attention model for natural language inference",
            "rating": 2
        },
        {
            "paper_title": "Towards AI-Complete question answering: A set of prerequisite toy tasks",
            "rating": 2
        },
        {
            "paper_title": "Nlprolog: Reasoning with weak unification for question answering in natural language",
            "rating": 2
        },
        {
            "paper_title": "Learning a SAT solver from single-bit supervision",
            "rating": 1
        },
        {
            "paper_title": "oLMpics - on what language model pre-training captures",
            "rating": 1
        }
    ],
    "cost": 0.01365725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Transformers as Soft Reasoners over Language</h1>
<p>Peter Clark, Oyvind Tafjord and Kyle Richardson<br>Allen Institute for AI, Seattle, WA<br>{peterc,oyvindt,kyler}@allenai.org</p>
<h4>Abstract</h4>
<p>Beginning with McCarthy's Advice Taker (1959), AI has pursued the goal of providing a system with explicit, general knowledge and having the system reason over that knowledge. However, expressing the knowledge in a formal (logical or probabilistic) representation has been a major obstacle to this research. This paper investigates a modern approach to this problem where the facts and rules are provided as natural language sentences, thus bypassing a formal representation. We train transformers to reason (or emulate reasoning) over these sentences using synthetically generated data. Our models, that we call RuleTakers, provide the first empirical demonstration that this kind of soft reasoning over language is learnable, can achieve high ( $99 \%$ ) accuracy, and generalizes to test data requiring substantially deeper chaining than seen during training ( $95 \%+$ scores). We also demonstrate that the models transfer well to two hand-authored rulebases, and to rulebases paraphrased into more natural language. These findings are significant as it suggests a new role for transformers, namely as limited "soft theorem provers" operating over explicit theories in language. This in turn suggests new possibilities for explainability, correctability, and counterfactual reasoning in question-answering. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>AI has long pursued the goal of giving a system explicit knowledge, and having it reason over that knowledge to reach conclusions, dating back to the earliest years of the field, e.g., McCarthy's Advice Taker (1959), and Newell and Simon's Logic Theorist (1956). While this has resulted in impressive applications (e.g., [Metaxiotis et al., 2002]), building and reasoning over the required formal representations has also proved challenging [Musen and Van der Lei, 1988]. In this work, we explore a modern approach to this goal, and ask whether transformers can be trained to reason (or emulate reasoning) using rules expressed in language, thus bypassing a</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>(Input Facts:) Alan is blue. Alan is rough. Alan is young. Bob is big. Bob is round. Charlie is big. Charlie is blue. Charlie is green. Dave is green. Dave is rough. (Input Rules:) Big people are rough. If someone is young and round then they are kind. If someone is round and big then they are blue. All rough people are green.</p>
<p>Q1: Bob is green. True/false? [Answer: T] Q2: Bob is kind. True/false? [F] Q3: Dave is blue. True/false? [F]</p>
<p>Figure 1: Questions in our datasets involve reasoning with rules. The inputs to the model are the context (facts + rules) and a question. The output is the T/F answer to the question. Here the underlying reasoning for the true fact (Q1) is: Bob is big, therefore rough (rule1) therefore green (rule4). Note that the facts + rules themselves change for different questions in the datasets.
formal representation. If so, new opportunities for questionanswering, explainability, correctability, and counterfactual reasoning may become possible.</p>
<p>This goal is quite distinct from question-answering as selecting an answer span in a passage, today's prevailing paradigm, e.g., [Rajpurkar et al., 2016]. Rather, we want the system to reason over the provided rules to find conclusions that follow. Our goal is also distinct from that of inducing rules from examples, e.g., given instances of family relationships, inducing that a parent's parent is a grandparent [Sinha et al., 2019], something that transformers are already known to do well. Rather, here we provide rules explicitly, and wish transformers to draw appropriate conclusions, as illustrated in Figure 1. Here, rather than inducing rules from examples, our task involves learning to emulate a reasoning algorithm.</p>
<p>We provide the first demonstration that this is possible, i.e., that transformers can reason with rules expressed in language. Our approach uses a broadly applicable training regimen: Characterize the desired behavior in a formal way, synthesize formal examples, generate linguistic equivalents, and train a model. The result suggests a new role for transformers, namely as a kind of limited "soft theorem prover" over language (Figure 2). This in turn may allow inspection and control of the knowledge that the model is manipulating, with</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: (a) Traditional formal reasoning applies a theorem prover to axioms in order to answer a question. (b) Our work here strives for a linguistic analog, where a transformer serves as a "soft theorem prover" over knowledge expressed linguistically.</p>
<p>potential benefits for explanation, correctability, and counterfactual reasoning.</p>
<p>Our investigations here are in a limited setting: Rules are linguistic expressions of conjunctive implications <em>condition</em> [∧ <em>condition</em>] <em>→ conclusion</em>, with the semantics of logic programs with negation [Apt <em>et al.</em>, 1988]; and reasoning is the deduction of a statement's truth according to these semantics. However, although there is still a potentially large gap to natural language inference (NLI),<sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup> our approach also suggests a path to teaching machines to reason over broader language, with similar potential benefits.</p>
<p>We leave open the question of whether the transformer is actually "reasoning", and even what that might mean in a neural setting. Rather, we show that transformers can reliably emulate the i/o behavior of a formal reasoner, including applied to test data requiring more reasoning than at training time, two hand-authored rulebases, and rulebases rephrased into more natural (crowdsourced) language.</p>
<p>The paper is organized to address the following questions, and contributes the following results:</p>
<ol>
<li>Can transformers learn to reason with rules? We train and test on rules expressed in (synthetic) language, and find high (99%) accuracy, including on test questions requiring a greater depth of reasoning than seen during training (scoring up to 95%, Table 1).</li>
<li>Can the trained model solve hand-authored reasoning problems? We find the trained models are able to solve five of six variants of two independently authored rule-based problems, zero shot (90%+ scores, Table 4).</li>
<li>Do the results transfer to theories expressed in more natural language? Models also perform well when trained and tested on theories paraphrased into more natural (crowdsourced) language (98% score). The best earlier model can even partially solve these problems zero-shot (66% accuracy, Table 5).</li>
<li>Can the model identify which facts an answer depends on? We show that the model is largely able to do this (94% F1), including perfect identification for over 70% of the questions. This is a first step towards having a model create an explanation for its conclusions. (Sec-</li>
</ol>
<p>tion 4.5 and Figure 8).</p>
<p>[^5]. Can other neural architectures learn to reason? Our experiments show a particular transformer (RoBERTa) is sufficient for our tasks, but is it necessary? We show that two other systems, BERT and ESIM (an LSTM-based model) [Chen <em>et al.</em>, 2017], are also able to learn these tasks, albeit with lower scores (95% and 80% respectively, vs. 98%). This suggests that our results are not specific to RoBERTa or transformers, although transformers learn the tasks more easily (Table 6).</p>
<h2>2 Related Work</h2>
<p>While our work is, to the best of our knowledge, the first systematic study of transformers directly reasoning with rules in language, there are several datasets that make a first step towards this by testing whether neural systems can apply a single rule in a particular situation. Task 15 in the bAbI dataset [Weston <em>et al.</em>, 2016] tests whether a rule of the form "Xs are afraid of Ys" can be correctly applied, e.g., "Sheep are afraid of wolves. Gertrude is a sheep. What is Gertrude afraid of? A:wolves". Similarly, the synthetic, conditional probes in [Richardson <em>et al.</em>, 2020] test single rule application. In addition, the datasets QuaRTz [Tafjord <em>et al.</em>, 2019] and ROPES [Lin <em>et al.</em>, 2019] involve applying general statements to a situation, but also require many other reading comprehension skills, rather than specifically testing reasoning.</p>
<p>Although our core datasets may seem similar to the bAbI dataset [Weston <em>et al.</em>, 2016] in using synthetic data, our probes are qualitatively different. Specifically, apart from bAbI Task 15 (above), the underlying rules needed to infer an answer in the bAbI tasks are <em>implicit</em>, while our concern here is reasoning with explicit rule sets, potentially different for each example (Figure 1).</p>
<p>Our approach contrasts with prior efforts that attempt to semantically parse language into a formal form, so that a formal reasoner can then be applied [Kamath and Das, 2019]. Despite substantial research, semantic parsing remains challenging, with few examples of systems that can reliably convert multi-sentence text into formal theories. Instead, we explore reasoning with language directly, bypassing the semantic parsing task.</p>
<p>Our work can be seen as evaluating transformers for (a subset of) Natural Logic [MacCartney and Manning, 2014], i.e., formal inference over statements expressed in language. It is also related to textual entailment and Natural Language Inference (NLI) [Manning and MacCartney, 2009], but with the important difference that NLI also allows <em>unsupported</em> inferences that "a person would typically infer" [Dagan <em>et al.</em>, 2013]. We discuss bridging the gap between our work and NLI in Section 5.3.</p>
<p>Several researchers have developed methods for Neural Theorem Proving (NTP), combining symbolic and neural methods to reason step-wise over language-derived structures, e.g., [Weber <em>et al.</em>, 2019]. Similarly, there has been work on SAT solving [Selsam <em>et al.</em>, 2019], approximate (DNF) model counting [Abboud <em>et al.</em>, 2020], and formula embedding [Abdelaziz <em>et al.</em>, 2020] to help solve formal reasoning problems. While our goals are similar, we do not im-</p>
<p>pose any structure on the neural reasoning process, instead wanting to know if the (i/o of the) reasoning process itself is learnable, using knowledge expressed in language.</p>
<p>Our task can perhaps best be viewed as one of algorithm emulation, here for systematic reasoning with rules. There have been numerous other demonstrations that transformers either already know [Talmor et al., 2019; Richardson and Sabharwal, 2019] or can learn to emulate other algorithms, including for semantic parsing [He and Choi, 2019], machine translation [Wang et al., 2019], integration [Lample and Charton, 2019], and math [Saxton et al., 2019]. Here we investigate a transformer's ability to learn rule-based reasoning.</p>
<h2>3 Dataset Generation</h2>
<p>To investigate a transformer's ability to emulate rule-based reasoning, we generate five datasets requiring various depths of inference to answer the questions. Each example in a dataset is a triple (context,statement,answer), where context has the form (fact<em>, rule</em>), statement is the question, namely a declarative sentence to prove, and answer is either T (true) if statement deductively follows from the context, or F if it does not (false under a closed-world assumption, CWA). Facts, rules, and the question statements are expressed in (synthetic) English. Each example is essentially a (linguistic) standalone logical theory with an "Is it true?" question posed against it.</p>
<h3>3.1 Overview</h3>
<p>To generate each example, we first generate a small theory (facts + rules) in logic, perform forward inference to derive all its implications, then select question statements from those implications (answer=true), and from unproven (positive) facts (answer=false, under the CWA). We generate five datasets, each constrained by the maximum depth of inference required to prove the facts used in its questions (up to depths $\mathrm{D}=0, \mathrm{D} \leq 1, \mathrm{D} \leq 2, \mathrm{D} \leq 3$ and $\mathrm{D} \leq 5$ respectively). Depth $\mathrm{D}=0$ means the true facts can be "proved" by simple lookup in the context (no inference). The fifth dataset, called DMax, contains questions up to depth 5, and is used to test generalization to depths unseen in training on the other four datasets.</p>
<h3>3.2 Theory Generation</h3>
<p>Theories contain two types of facts:</p>
<ul>
<li>attributes is $\left(e_{i}, a_{j}\right)$ e.g., is(Alan,Big).</li>
<li>relations $r_{k}\left(e_{i}, e_{k}\right)$ e.g., eats(Dog,Rabbit).</li>
</ul>
<p>The is() predicate assigns attributes to entities, while the $r_{k}()$ predicates relate two entities. Like people names, the symbols Dog, Rabbit, etc. also denote specific entities, i.e., denote "the dog", "the rabbit", etc. Rules are of the form:
condition [ $\wedge$ condition]* $\rightarrow$ conclusion.
The first condition is a predicate whose first argument is a variable, ${ }^{3}$ and second argument is an attribute or entity. For each subsequent condition and the conclusion, they are also predicates whose first argument is either the same variable or a previously mentioned entity, and the second argument is a</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>The bald eagle does not eat the dog. The cat chases the dog. The cat eats the bald eagle. The cat is nice. The cat likes the dog. The cat likes the rabbit. The dog is furry.
The rabbit chases the bald eagle. The rabbit eats the bald eagle.
If someone does not eat the cat then they do not eat the dog. If someone likes the bald eagle then they do not like the rabbit. If someone eats the bald eagle and they do not eat the rabbit then they are furry.
If someone is furry then they like the cat.
Q1. The bald eagle likes the cat. True/false? [F]
Q2. The rabbit likes the cat. True/false? [T]
Q3. The bald eagle is furry. True/false? [F]</p>
<p>Figure 3: An example of a rulebase and 3 questions using relations with negation. The reasoning for the $[\mathbf{T}]$ answer is: The rabbit eats the bald eagle (given), therefore the rabbit is furry (rule3), therefore the rabbit likes the cat (rule4).
new attribute or entity. (In this way, rules are constrained to have at most one variable. Rules are implicitly universally quantified over that variable). For example, the formal form of the first rule in Figure 1 looks:
// If someone is young and round then they are kind. is(?X,Young) $\wedge$ is(?X,Round) $\rightarrow$ is(?X,Kind).
Each theory contains 1-16 facts and 1-9 rules generated at random. We generate two types of theory:</p>
<ol>
<li>Type 1 uses only the is() predicate, with 4 entities {Alan,Bob,...} and 7 (non-mutually-exclusive) attributes {Blue,Rough,Young,...}, drawn randomly from pools of 10 names and 14 attributes respectively.</li>
<li>Type 2 uses is() and 3 other predicates {likes(), chases(), ...}, 4 entities ${$ Cat,Dog,BaldEagle,...}, and 5 attributes ${$ Big,Furry,...}, drawn randomly from pools of size 6,10 , and 10 respectively.
We also generate a version of each that adds negation (not) in the facts and rule conditions/conclusions (negation-as-failure for conditions, strong negation for conclusions). Figure 1 is an example of Type 1, without negation. Figure 3 is an example of Type 2, with negation. Each dataset contains 100 k examples ( 25 k of each Type $\times$ without/with negation). Data is randomly split 70/10/20 into train/dev/test partitions, ensuring no overlap of theories between each partition.</li>
</ol>
<h3>3.3 Forward Inference</h3>
<p>Given a randomly generated theory (facts+rules), we perform exhaustive forward inference to find all its implications, noting their proof(s). (As the domains are finite, the number of implications are finite too). For semantics, we treat the rulebase as a logic program, and infer the minimal, supported answer set implied by the program [Apt et al., 1988]. Negations in the rules' conditions are treated as negation as failure (NAF), and we ensure that the rulebase is stratified to avoid ambiguity and cycles [Bidoit and Froidevaux, 1991]. Inference is performed layerwise to find the minimal supported model, and inconsistent and unstratified rulebases are discarded. We also check that inference proceeds to the depth required, e.g., for the $\mathrm{D} \leq 3$ dataset, at last one fact must require depth 3 inference to infer it for all its theories.</p>
<h3>3.4 Question Generation and English Synthesis</h3>
<p>For each theory, we generate several questions with answer 'true' by selecting from the inferred facts, one at each depth of inference from 0 to the dataset's target depth (e.g., for the $\mathrm{D} \leq 2$ dataset, we generate 3 'true' questions at depths $d=0,1$, and 2 for each theory). For each 'true' question we also generate a 'false' question by negating a conclusion proven at the same depth. We then generate the same number of questions using facts that are unproven (false under a closed-world assumption), drawing equally from unproven, instantiated positive rule conclusions or other unproven positive facts. Half are used as questions labeled as false (via the CWA), and for diversity, half are flipped by negating the fact and changing the label to true (i.e., " $f$ ? False" becomes "Not $f$ ? True"). Thus a theory for depth $d$ has (up to) $4(\mathrm{~d}+1)$ questions, with an equal balance of true and false answers. Each question is also annotated with the inference depth needed to answer it.</p>
<p>Finally the theories and questions are converted into (synthetic) English, using simple natural language templates plus rules to improve fluency (e.g., using pronouns). We use three templates (randomly selected per rule): "If condition [and condition]" then conclusion.", "All attribute<em> people|things are attribute.", and "attribute</em> people|things are attribute.", the last two only applicable to rules involving just attributes. Examples are shown in Figures 1 and 3.</p>
<h2>4 Experiments</h2>
<h3>4.1 Models</h3>
<p>We conduct all our experiments (bar Section 4.6) using RoBERTa-large, additionally fine-tuned on the RACE dataset [Lai et al., 2017]. We use fixed hyperparameters (learning rate etc), inheriting the settings from RoBERTa on RACE [Liu et al., 2019].</p>
<p>We train RoBERTa to predict true/false (i.e., binary classification) for each question statement. Questions are supplied to RoBERTa as: [CLS] context [SEP] statement [SEP], where context is the theory (facts+rules, expressed in language) and statement is the fact to try and prove. The [CLS] output token is projected to a single logit. A logit score of $&gt;0$ is treated as predicting true, otherwise the answer is false. Training is performed using cross-entropy loss. For evaluation, we measure accuracy. (The test data has an equally balance of TRUE/FALSE answers, hence the baseline of random guessing is $50 \%$ ).</p>
<h3>4.2 Can RoBERTa Answer Reasoning Questions?</h3>
<p>We train and test RoBERTa models on each of our datasets $\mathrm{D}=0, \mathrm{D} \leq 1, \mathrm{D} \leq 2, \mathrm{D} \leq 3$, and DMax, containing problems requiring reasoning up to depths $0,1,2,3$, and 5 respectively. We then test the models on the DMax dataset, that includes problems at depths greater than the other datasets. The results are shown in Table 1. The results suggest the following findings:</p>
<ol>
<li>RoBERTa is able to master the test data almost perfectly ( $99 \%$ accuracy, row 1) even though the specific reasoning problems (facts+rules) in each test question are distinct from those in the training set.</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: left;">Training</th>
<th style="text-align: center;">Num Q</th>
<th style="text-align: center;">Mod0 <br> $D=0$</th>
<th style="text-align: center;">Mod1 <br> $D&lt;=1$</th>
<th style="text-align: center;">Mod2 <br> $D&lt;=2$</th>
<th style="text-align: center;">Mod3 <br> $D&lt;=3$</th>
<th style="text-align: center;">MMax <br> DMax</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Test (own)</td>
<td style="text-align: center;">$\sim 20000$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">99.2</td>
</tr>
<tr>
<td style="text-align: left;">Test (DMax)</td>
<td style="text-align: center;">20192</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">99.2</td>
</tr>
<tr>
<td style="text-align: left;">Depth=0</td>
<td style="text-align: center;">6299</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">Depth=1</td>
<td style="text-align: center;">4434</td>
<td style="text-align: center;">57.9</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">98.5</td>
<td style="text-align: center;">98.4</td>
</tr>
<tr>
<td style="text-align: left;">Depth=2</td>
<td style="text-align: center;">2915</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">36.8</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">98.4</td>
</tr>
<tr>
<td style="text-align: left;">Depth=3</td>
<td style="text-align: center;">2396</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">98.5</td>
<td style="text-align: center;">98.8</td>
</tr>
<tr>
<td style="text-align: left;">Depth=4</td>
<td style="text-align: center;">2134</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">99.2</td>
</tr>
<tr>
<td style="text-align: left;">Depth=5</td>
<td style="text-align: center;">2003</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">12.3</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">97.6</td>
<td style="text-align: center;">99.8</td>
</tr>
</tbody>
</table>
<p>Out-of-distribution tests (reasoning depth unseen in training)</p>
<p>Table 1: Accuracy of models (Mod0,...) trained and tested on the five datasets ("Test (own)" row), and tested on all, and different slices, of the DMax test set. The boxed area indicates test problems at depths unseen during training.
2. The Depth $=0$ model, Mod0, only trained on lookup questions, is (unsurprisingly) unable to answer questions requiring reasoning (column Mod0). ${ }^{4}$
3. As we train with increasingly deep inference, the models' ability to generalize improves. The $\mathrm{D} \leq 2$ model (questions involving problems up to depth 2) achieves $71.1 \%$ on Depth $=3$ problems, while the $\mathbf{D} \leq 3$ model generalizes well right up to the maximum depth tested (e..g, $97.6 \%$ for Depth=5 problems).</p>
<p>We additionally test the robustness of the models' answers by perturbing the original theories. Specifically, for each test fact $f$ that is true, we test whether removing a sentence that is part of the proof of $f$ causes the prediction to (desirably) flip from true to false. We call these sentences in the proof tree critical sentences, as the truth of $f$ depends on them. Conversely, removing an irrelevant sentence should cause no change to the model's prediction. As we know the original proof trees for each fact $f$ in the dataset, we can identify the critical and irrelevant sentences by simple inspection of those trees. ${ }^{5}$ Typically, 1-6 sentences of the $\approx 15-20$ sentences are critical for proving each provable fact.</p>
<p>We test this using the no-negation ${ }^{6}$ half of the DMax test set ( $\approx 10 \mathrm{k}$ questions). In this partition, 5904 questions have proofs (are true). (The remaining questions are false under the CWA). For each of these questions, we remove each of the theory sentences $s_{i}$ in turn, and measure the prediction accuracy on each result. As there are about 19 sentences/theory on average, this results in 113978 "sentence removed" probes (of which 20746 have a critical sentence removed, and 93232 have an irrelevant sentence removed). Ideally, removing a sentence critical to a question $f$ should flip the model's pre-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Original</th>
<th style="text-align: center;">Remove <br> Irrelevant</th>
<th style="text-align: center;">Remove <br> Critical</th>
<th style="text-align: center;">Remove <br> Any</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Accuracy (test)</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">96.3</td>
</tr>
</tbody>
</table>
<p>Table 2: Accuracy on the DMax (no negation) subset, and all its (113k) perturbed (one context sentence removed) variants. The overall accuracy (Remove Any, last column) is largely unchanged, but with a drop for the subset where a critical sentence was removed.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Original predictions for true (positive) facts:</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\mathbf{T}$</td>
</tr>
<tr>
<td style="text-align: left;">New</td>
<td style="text-align: center;">3895 (should have flipped) 10 (incorrectly flips)</td>
</tr>
<tr>
<td style="text-align: left;">Pred.</td>
<td style="text-align: center;">16654 (correct flips) 187 (becomes correct)</td>
</tr>
</tbody>
</table>
<p>Table 3: On the true questions that were originally answered correctly (column 1), the predicted T answer should flip to predicted F when a critical sentence is removed. In practice, we observe this happens $81 \%$ of the time (16654/(16654+3895)).
diction from T to F, while removing a noncritical sentence should leave the prediction unchanged as T. We also measure overall performance on the entire dataset of questions with perturbed theories.</p>
<p>The results are shown in Tables 2 and 3. We observe:</p>
<ol>
<li>The overall accuracy is largely unchanged on the full collection of questions with perturbed theories, suggesting robustness to these variants (last column, Table 2).</li>
<li>For the (20k) questions where the prediction is expected to flip from true to false, we see this flip occurs $81 \%$ of the time, Table 3. This suggests moderate robustness to this specific type of perturbation, although notably less than for a formal theorem prover (that would make this flip $100 \%$ of the time). For the remaining (93k) questions, the prediction (correctly) stays true over $99 \%$ of the time (no Table).</li>
</ol>
<h3>4.3 Performance on Hand-Authored Problems</h3>
<p>To further test robustness and out-of-distribution performance, we test the trained models on two hand-authored reasoning problems, both including reasoning with negation, written independently of our datasets. Note that these new datasets are used purely as test sets (no training on them, i.e., zero-shot performance); their vocabulary of entities, attributes, and predicates (except for is()) are all new to the models at test time. The two test datasets are as follows:
Birds. The "birds" rulebase is a well-known logic problem illustrating the use of "abnormality" predicates [McCarthy, 1984]. We entered Sergot's formulation of it ${ }^{7}$ verbatim (bar syntax), and generated a series of test questions using the same procedure as earlier. Figure 4 illustrates the problem (in restricted English, exactly as presented to our model) and four example questions. We created two linguistic expressions of the formal theory, Birds1 and Birds2. Birds2 is shown in Figure 4, while Birds1 is identical except "can/cannot fly" is replaced with "is/is not flying" to make the negation ("not") more explicit (this turns out not to matter). Questions require reasoning up to depth 1.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>If someone is a bird and not abnormal then they can fly. If someone is an ostrich then they are a bird. If someone is an ostrich then they are abnormal. If someone is an ostrich then they cannot fly. If someone is a bird and wounded then they are abnormal. If someone is wounded then they cannot fly.
Arthur is a bird. Arthur is not wounded. Bill is an ostrich. Colin is a bird. Colin is wounded.
Dave is not an ostrich. Dave is wounded.
Q1. Arthur can fly. True/false?[T] Q2.Bill can fly. True/false?[F] Q3. Colin can fly. True/false?[F] Q4.Dave can fly. True/false?[F]</p>
<p>Figure 4: Sergot's "birds" puzzle includes reasoning about abnormality predicates. The dataset contains these and other questions about the single theory.</p>
<p>The circuit has a switch.
The switch is on.
The circuit has a light bulb.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>If a circuit has a switch and the switch is on then the circuit is complete.
If a circuit does not have a switch then the circuit is complete.
If a circuit is complete then a current runs through the circuit.
If a current runs through a circuit and the circuit has a light bulb then the light bulb is glowing.
If a current runs through a circuit and the circuit has a bell then the bell is ringing.
If a current runs through a circuit and the circuit has a radio then the radio is playing.
Q1. The circuit is not complete. True/false? [F]
Q2. The light bulb is glowing. True/false? [T]
Q3. The radio is playing. True/false? [F]</p>
<p>Figure 5: The simple Electricity2 rulebase, an example circuit, and 3 questions about the circuit. (Circuit diagram is for illustration only).</p>
<p>Electricity. We also created a small rulebase about an electrical circuit, describing the conditions for an appliance to function. We created 4 variants of increasing complexity, containing 5, 6, 11, and 12 rules respectively. For each rulebase, we generate different scenarios (the facts) by randomly selecting from possible ground facts. Questions are then generated against each scenario using the same procedure as earlier, resulting in 4 test sets. Figure 5 shows the Electricity2 rulebase with an example scenario plus three questions. Questions against the four rulebases require inference up to depth $2,3,3$, and 4 respectively.</p>
<h2>Results</h2>
<p>The results are in Table 4, tested using the earlier trained models. Note that these new problems and vocabularies were unseen during training (i.e., are zero-shot). We observe:</p>
<ol>
<li>The "birds" problems are solved (almost) perfectly by all but the non-reasoning (Mod0) model (MMax gets one question wrong on Birds1).</li>
<li>The MMax model (trained on DMax) solves all but one of these datasets with $90 \%+$ scores.
These are two point demonstrations that the trained models</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">Num Q</th>
<th style="text-align: right;">Mod0</th>
<th style="text-align: right;">Mod1</th>
<th style="text-align: right;">Mod2</th>
<th style="text-align: right;">Mod3</th>
<th style="text-align: right;">MMax</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Test $\downarrow$; Train $\rightarrow$</td>
<td style="text-align: right;">$D=0$</td>
<td style="text-align: right;">$D&lt;=1$</td>
<td style="text-align: right;">$D&lt;=2$</td>
<td style="text-align: right;">$D$</td>
<td style="text-align: right;">$&lt;=3$</td>
<td style="text-align: right;">DMax</td>
</tr>
<tr>
<td style="text-align: left;">Birds1</td>
<td style="text-align: right;">40</td>
<td style="text-align: right;">80.0</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">97.5</td>
</tr>
<tr>
<td style="text-align: left;">Birds2</td>
<td style="text-align: right;">40</td>
<td style="text-align: right;">80.0</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">100</td>
</tr>
<tr>
<td style="text-align: left;">Electricity1</td>
<td style="text-align: right;">162</td>
<td style="text-align: right;">77.8</td>
<td style="text-align: right;">88.9</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">96.9</td>
</tr>
<tr>
<td style="text-align: left;">Electricity2</td>
<td style="text-align: right;">180</td>
<td style="text-align: right;">70.0</td>
<td style="text-align: right;">80.0</td>
<td style="text-align: right;">97.2</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">98.3</td>
</tr>
<tr>
<td style="text-align: left;">Electricity3</td>
<td style="text-align: right;">624</td>
<td style="text-align: right;">80.8</td>
<td style="text-align: right;">93.9</td>
<td style="text-align: right;">92.8</td>
<td style="text-align: right;">90.5</td>
<td style="text-align: right;">91.8</td>
</tr>
<tr>
<td style="text-align: left;">Electricity4</td>
<td style="text-align: right;">4224</td>
<td style="text-align: right;">91.9</td>
<td style="text-align: right;">97.5</td>
<td style="text-align: right;">93.6</td>
<td style="text-align: right;">86.0</td>
<td style="text-align: right;">76.7</td>
</tr>
</tbody>
</table>
<p>All results are zero-shot (these rulebases completely unseen during training)
Table 4: Accuracy of the earlier models tested on hand-crafted rulebases (zero shot, no fine-tuning). Note that the models were only trained on the earlier datasets (e.g., Figures 1 and 3), and thus the new rulebases' entities, attributes, and predicates (bar $t s(t)$ ) are completely unseen until test time.
can be used to solve novel reasoning problems with high reliability ( $90 \%+$ in all but one case).</p>
<p>We see one surprising anomaly also: the models trained with deeper reasoning depths do slightly worse on Electricity4 than the depth 1 model, Mod1. From investigation, we find almost all failing questions at higher depths are those where the queried fact $f$ is an unsatisfied rule conclusion (hence should be false), in particular when the first argument of $f$ is not the first argument of one of the rule's conditions. Because of the way the original dataset was generated, examples similar to this are very rare in the training data, possibly causing this anomaly. More generally this illustrates that even when trained on a diversity of problems, the trained model can have unanticipated blind spots.</p>
<h3>4.4 Reasoning with Paraphrased Rules</h3>
<p>Our experiments so far have been with synthetic language, but our ultimate goal is to reason over full natural language. To test transfer to more natural linguistic forms, we generated a new dataset of 40 k examples, using crowdworkers to paraphrase our theories. Of course, this only tests robustness to paraphrasing, not to abitrary natural language. Nevertheless, it is a small first step in this direction.</p>
<p>To generate our data, we follow a similar approach to [Sinha et al., 2019]. For this experiment, we used Type 1 theories without negation, i.e., the same form as in Figure 1.</p>
<h2>Dataset Generation</h2>
<p>To generate the new dataset, called ParaRules, we first generated a novel collection of 10k theories (facts+rules) expressed in synthetic language, as before, then extracted the "fact groups" and rules from each. A "fact group" is all the facts in a theory about a particular person, e.g., (from Figure 1) "Alan is blue. Alan is rough. Alan is young.", while a rule is just the original "If...then..." sentence. We then asked crowdworkers to creatively re-express the fact-groups and rules, shown to them in English, in their own words. For example, the earlier fact-group might be rewritten as: "Alan is on the young side, but rough. He often feels rather blue.". Rewritten fact-groups were then turned into templates by variabilizing the person name. Turkers also rephrased each rule (no variabilization needed). Rephrasings were automatically checked to make sure that all the key attributes were mentioned (and no others included), and rejected otherwise.</p>
<p>Alan, who is round, red, kind, and also green, tends to be rather blue. In the snow sits Bob, crying from being cold. Charlie has green teeth and rough skin. People also notice his blue eyes.
A quite nice person who is red and green is also big.
Any big, kind person that turns red is cold to the touch.
Young, kind people have a habit of being nice.
A kind person will certainly be young.
Q1. Dave is nice. True/false? [F]
Q2. Charlie is big. True/false? [F]
Q3. Alan is nice. True/false? [T]</p>
<p>Figure 6: A paraphrased theory in the ParaRules dataset. The reasoning for the true answer here is: Alan is kind (given), therefore young (rule4), therefore nice (rule3).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Mod0</th>
<th style="text-align: center;">Mod1</th>
<th style="text-align: center;">Mod2</th>
<th style="text-align: center;">Mod3</th>
<th style="text-align: center;">MMax</th>
<th style="text-align: center;">Mod3+Para</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Training</td>
<td style="text-align: center;">$D=0$</td>
<td style="text-align: center;">$D&lt;=1$</td>
<td style="text-align: center;">$D&lt;=2$</td>
<td style="text-align: center;">$D&lt;=3$</td>
<td style="text-align: center;">DMax</td>
<td style="text-align: center;">$D&lt;=3+$ Para</td>
</tr>
<tr>
<td style="text-align: left;">Para test</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">60.1</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">98.8</td>
</tr>
<tr>
<td style="text-align: left;">Depth=0</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">86.2</td>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">99.8</td>
</tr>
<tr>
<td style="text-align: left;">Depth=1</td>
<td style="text-align: center;">59.9</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">99.3</td>
</tr>
<tr>
<td style="text-align: left;">Depth=2</td>
<td style="text-align: center;">33.2</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">40.7</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">98.2</td>
</tr>
<tr>
<td style="text-align: left;">Depth=3</td>
<td style="text-align: center;">6.9</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">26.6</td>
<td style="text-align: center;">96.7</td>
</tr>
<tr>
<td style="text-align: left;">Depth=4</td>
<td style="text-align: center;">4.2</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">90.1</td>
</tr>
</tbody>
</table>
<p>Zero-shot tests (no fine-tuning on the paraphrased rule set)
Table 5: Accuracy with rules paraphrased into more natural language (ParaRules), without fine-tuning (zero shot) and with (last column only). The strongest zero-shot model (MMax) partially solves $(66.6 \%)$ this problem zero-shot, with strongest performance for depth 0 and 1 inferences.</p>
<p>We use these to assemble the new ParaRules dataset of 40k questions against $\approx 2 \mathrm{k}$ theories expressed in the paraphrased language. To build each theory, facts were collected by randomly sampling and instantiating fact-group templates with people's names, and rules were randomly sampled. An example is shown in Figure 6. The train, dev, and test sets were generated using different partitions of the templates, to ensure that no templates were shared between partitions.</p>
<p>As we kept track of the corresponding logic underlying each fact group and rule, we can then generate questions as before: Exhaustively forward-chain on the (logic version of) the theory, discard if a contradiction is hit or reasoning is of insufficient depth (we require at least depth 3 reasoning), and then for each depth select inferred and non-inferred facts as true/false questions as before.</p>
<h2>Results</h2>
<p>We ran the earlier trained models on the ParaRules test partition (no fine-tuning, i.e., zero shot). The results are shown in Table 5. The strongest model, MMax, partially solves this dataset with a score of $66.6 \%$, higher for questions requiring less inference, and lower for questions requiring more inference. (The below-random scores for $\mathrm{D}=0$ reflect the same artifact as earlier, namely predicting everything as false except for facts explicitly given. See Footnote 4).</p>
<p>Note that these results are for zero-shot, with no model exposure to the paraphrased data during training. In contrast, we also trained a model using both of the $\mathrm{D} \leq 3$ and ParaRules training partitions. The resulting model (last column Table 5) has an accuracy of $98.8 \%$ on ParaRules test (even though the</p>
<p>Statement: The lion visits the rabbit. (TRUE). Depth: 2
Context: If something visits the lion then it chases the rabbit. The lion is red. If something sees the squirrel and the squirrel is young then the squirrel chases the rabbit .The lion is cold. The squirrel sees the rabbit. The rabbit chases the squirrel. The lion chases the cat. Red things are young. The lion sees the rabbit. The cat is young. If something is cold and young then it visits the rabbit. The squirrel is big.</p>
<p>Figure 7: In this (abbreviated) example, the model has correctly identified the sentences critical to the answer (shown in green). Perfect identification occurs for over $70 \%$ of the provable answers (See Figure 8 for a full histogram).
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 8: Counts of the F1 scores for predicting which sentences are critical to the proofs of questions in DMax (test, no negation subset). For over $70 \%$ of the questions, the model predicts critical sentences perfectly $(\mathrm{F} 1=1.0)$, with high F1 in the remaining case.</p>
<p>ParaRules test rewordings are distinct from train and dev), showing near-perfect performance is learnable. Although a limited study, this suggests that our findings may extend to rulebases expressed in more natural language.</p>
<h3>4.5 Generating Explanations</h3>
<p>In Section 4.2, we tested (for the no-negation theories) whether removing a theory sentence $s_{i}$ caused the prediction for a true fact $f$ to flip to false, and found that sentences causing a flip were very often ( $98 \%$ ) part of the original proof of $f$ (i.e., critical sentences), while sentences that did not were not ( $97 \%$ ). Using that data about which removed sentences caused a flip, we can build a map of the theory paragraph showing which sentences the model considers critical to a conclusion, a potentially first step to providing an explanation for the model's answers (see Figure 7).</p>
<p>We can quantify this "explanatory" performance by measuring the per-proof scores of predicted vs. actual critical sentences for each question, measuring the precision, recall, and F1 scores for each question in turn. The (macro)average $\mathrm{P} / \mathrm{R} / \mathrm{F} 1$ scores are $\mathrm{P}=98.7, \mathrm{R}=86.9$, and $\mathrm{F} 1=92.4$, suggesting a high degree of reliability in predicting sentences critical to a proof. (This is essentially an alternative view on the earlier robustness data, viewed from a per-proof perspective). A histogram of the F1 scores is shown in Figure 8, indicating perfect critical sentence identification for over $70 \%$ of the questions, and high F1 for the remaining questions. This suggests the model has some knowledge of the dependencies between the context sentences and a particular conclusion.</p>
<h3>4.6 Other Architectures</h3>
<p>To what extent are our results specific to RoBERTa? To explore this, we also trained BERT and ESIM (an LSTM-based model for natural language inference) [Chen et al., 2017] on our datasets. As a sanity check we also ran the decomposable attention model (DECOMP) on our data [Parikh et al., 2016]. The results are shown in Table 6.</p>
<p>We observe that the strongest BERT model trained up to depth 3 (Mod3) masters the dataset that includes higher inference depths (DMax) with $95 \%+$ accuracy, while ESIM's</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Mod0</th>
<th style="text-align: center;">Mod1</th>
<th style="text-align: center;">Mod2</th>
<th style="text-align: center;">Mod3</th>
<th style="text-align: center;">MMax</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Training</td>
<td style="text-align: center;">$D=0$</td>
<td style="text-align: center;">$D&lt;=1$</td>
<td style="text-align: center;">$D&lt;=2$</td>
<td style="text-align: center;">$D&lt;=3$</td>
<td style="text-align: center;">DMax</td>
</tr>
<tr>
<td style="text-align: left;">Test (own):</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">99.2</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">98.2</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">96.9</td>
</tr>
<tr>
<td style="text-align: left;">ESIM</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">90.3</td>
<td style="text-align: center;">87.8</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">80.0</td>
</tr>
<tr>
<td style="text-align: left;">DECOMP</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">64.1</td>
</tr>
<tr>
<td style="text-align: left;">Test (DMax):</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ESIM</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">66.4</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">DECOMP</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>(Includes questions at depths unseen during training)
Table 6: Transformers (RoBERTa,BERT) are sufficient but not strictly necessary for this task, although other architectures (ESIM) do not score as well.
scores are lower ( $\approx 80 \%$ ). Note that unlike RoBERTa and BERT, ESIM was not pre-trained on large amounts of text, perhaps contributing to its lower scores. This suggests that our results are not specific to RoBERTa or transformers, although transformers seem to learn the tasks more easily. As expected, DECOMP does not do well (random score is $50 \%$ ), suggesting the datasets are not trivially solvable.</p>
<p>Finally, to explore the role of pretraining, we generated a version of the $\mathrm{D} \leq 3$ dataset in which every word was (systematically) replaced by a random word, so that there was no grammaticality in the theories. After training, RoBERTa scores $83.3 \%$ on the test partition, substantially below the original $99.3 \%$, suggests that pretrained knowledge is playing an important role.</p>
<h2>5 Discussion and Future Work</h2>
<p>Although our demonstrations have been in a limited setting, the implications of being able to predictably reason with language are significant. With further advances, we may potentially be able to:</p>
<ul>
<li>author theories in English (e.g., Figure 5), thus sidestepping the intricacies of formal languages and offering new opportunities for easy creation and maintenance of knowledge.</li>
<li>have the machine apply general knowledge, e.g., from Wikipedia, to explainably solve novel problems</li>
<li>teach our AI when it makes a mistake, by providing the missing facts and/or correcting the erroneous ones it used ("instructable systems").</li>
<li>reason about counterfactual situations. For example, we might describe a world in which plastic is a type of metal, and see how the conductivity of objects change. This useful capability has previously been out of scope for transformers.</li>
</ul>
<p>Our RuleTaker models demonstrate these capabilities in a narrow setting. We now discuss additional steps needed to achieve these goals more broadly.</p>
<h3>5.1 Extending The Theory Language</h3>
<p>While we have shown that transformers can emulate a form of deductive reasoning, our demonstrations have been with</p>
<p>small theory sizes ( $&lt;20$ facts, $&lt;10$ rules), small domains ( $&lt;$ 100 possible ground facts), and with a limited rule language (at most one variable that is universally quantified over). Expanding the expressiveness of the rule language would enhance the model's utility. For example, we have not yet explored using multi-variable rules such as "If a person's father is a second person, and the second person's father is a third person, then the first person's grandfather is the third person," limiting what can be stated (e.g., rules of transitivity). Similarly there are other forms of reasoning we would like to train the model to handle, e.g., taxonomic inheritance, reasoning with disjunctive conclusions, and handling functional relations ("A country has exactly one capital"). This again requires characterizing the semantics of such statements, and generating training data showing the valid conclusions.</p>
<p>More generally, there are many natural language statements whose formal meaning is less clear (e.g., "Most birds fly", "It often rains in Seattle in winter."). To apply our methodology to statements with more complex semantics would require new training data, either synthesized from a richer formal representation and model of inference, ${ }^{8}$ or collected from people.</p>
<h3>5.2 Generating Training Data</h3>
<p>We assume that our synthetic training data is sufficiently representative of the real problems that the model will eventually be used for. However, it is possible that the generation procedure under-represents or misses some important types of theory, potentially giving the model a "blind spot" on novel problems if it is unable to fully generalize. (A minor example of this was the MMax results on Electricity4, last paragraph of Section 4.3). It would be valuable to find ways to characterize the different types of inference problems in the space, and design training curricula to ensure they are systematically covered and/or the model is able to generalize to them. Adversarial approaches to generation, where the generator learns to create theories that are hard for a partially trained model, may be useful in this context, e.g., [Kalyan et al., 2019].</p>
<h3>5.3 Natural Language Inference (NLI)</h3>
<p>We have shown that transformers can perform deductive inference over English statements. However, human reasoning over language - natural language inference (NLI) - is not always deductive. In particular, NLI allows for unsupported inferences that "a person would typically infer" [Dagan et al., 2013], while we have used a precise model of inference in which all of a rule's conditions need to be proven true in order for the conclusion to follow. Our model may still be quite far from that required for fully natural reasoning over language. For example, we would like our model to still proceed if there are gaps in the explicitly provided knowledge, providing the missing knowledge is "obvious" (and not contradicted by the explicitly provided facts), perhaps by leveraging its pretrained knowledge. Similarly, our model's treatment of negation as failure (NAF) sometimes clashes with intuitions about NLI, for example given (just) "If my car does not have</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>gas then it is not working." our model will conclude (given nothing else) that "My car is not working." as it cannot prove that "My car has gas.".</p>
<p>This raises a fundamental tension about the nature of the reasoning we ultimately desire: We want reasoning to be rigorous (conclusions justified by the information provided), but also "soft" (tolerant of phrasing differences and commonsense knowledge gaps), and strictly speaking these two goals are in conflict. Our experiments with Turk-authored language illustrates tolerance of phrasing differences, which we view as desirable, although in a strict deductive sense it is unjustified to conclude (say) "A person is green" from "Charlie has green teeth" (Figure 6). Similarly we would like the model to tolerate minor, unstated taxonomic gaps, for example given "Buildings have roofs" conclude "My house has a roof", even if "Houses are buildings" is not explicitly stated (but not conclude that result if it is explicitly stated that "Houses are not buildings"). Characterizing which inferences should be deductive vs. which can be assumed in NLI, and training a model to combine explicitly stated knowledge with implicit (pretrained) knowledge, remain significant open challenges.</p>
<h2>6 Conclusion</h2>
<p>Just as McCarthy advocated 60 years ago for machines reasoning ("taking advice") in logic, we have shown (in a restricted setting) that machines can by trained to reason over language. While we have assumed a particular semantics of inference, the methodology we have used is general: Characterize the desired behavior in a formal way, synthesize examples, generate linguistic equivalents, and train a model. The result, at least within our experiments, appears to be both natural and robust, in a way distinct from working with the original formalization.</p>
<p>The ability to reason (or emulate reasoning) over rules expressed in language has potentially far-reaching implications. For example, rules might be easily authored by a person, sidestepping some of the intricacies of a formal language (a simple kind of "programming in English"); or they could be retrieved from natural sources (e.g., science texts, Wikipedia). Similarly, if the answer is wrong, the user may be able to directly teach the system by providing general missing knowledge (or correcting erroneous knowledge) that can then also be used for new problems - a step towards instructable algorithms. Finally, the mechanism opens the door to neural counterfactual reasoning. For example, we can modify the earlier "birds" rulebase to describe a world in which birds typically don't fly, but where ostriches can fly, and see the consequences. To encourage further progress, an interactive demo and all our datasets are available at https://allenai.org/data/ruletaker</p>
<h2>Acknowledgements</h2>
<p>Thanks to Chitta Baral, Jonathan Berant, Oren Etzioni, Matt Gardner, Ashish Sabharwal, and Alon Talmor for comments on earlier drafts.</p>
<h2>References</h2>
<p>[Abboud et al., 2020] R. Abboud, I. Ceylan, and T. Lukasiewicz. Learning to reason: Leveraging neural networks for approximate dnf counting. In AAAI, 2020.
[Abdelaziz et al., 2020] Ibrahim Abdelaziz, Veronika Thost, Maxwell Crouse, and Achille Fokoue. An experimental study of formula embeddings for automated theorem proving in first-order logic. arXiv, 2002.00423, 2020.
[Apt et al., 1988] K. Apt, H. Blair, and A. Walker. Towards a theory of declarative knowledge. In Foundations of Deductive Databases and Logic Programming., 1988.
[Bidoit and Froidevaux, 1991] N. Bidoit and C. Froidevaux. General logical databases and programs: Default logic semantics and stratification. Inf. Comput., 91:15-54, 1991.
[Chen et al., 2017] Qian Chen, Xiao-Dan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. Enhanced lstm for natural language inference. In ACL, 2017.
[Dagan et al., 2013] Ido Dagan, Dan Roth, Mark Sammons, and Fabio Zanzotto. Recognizing Textual Entailment: Models and Applications. Morgan and Claypool, 2013.
[He and Choi, 2019] Han He and Jinho D. Choi. Establishing strong baselines for the new decade: Sequence tagging, syntactic and semantic parsing with bert. ArXiv, abs/1908.04943, 2019.
[Kalyan et al., 2019] Ashwin Kalyan, Oleksandr Polozov, and Adam Kalai. Adaptive generation of programming puzzles. Technical report, Georgia Tech, 2019. (https://openreview.net/forum?id=HJeRveHKDH).
[Kamath and Das, 2019] Aishwarya Kamath and Rajarshi Das. A survey on semantic parsing. In $A K B C^{\prime} 19,2019$.
[Lai et al., 2017] G. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy. RACE: Large-scale reading comprehension dataset from examinations. In EMNLP, 2017.
[Lample and Charton, 2019] G. Lample and F. Charton. Deep learning for symbolic mathematics. In $I C L R, 2019$.
[Lin et al., 2019] Kevin Lin, Oyvind Tafjord, Peter Clark, and Matt Gardner. Reasoning over paragraph effects in situations. In Proc. MRQA Workshop (EMNLP'19), 2019. also arXiv:1908.05852.
[Liu et al., 2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: a robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019.
[MacCartney and Manning, 2014] Bill MacCartney and Chris Manning. Natural logic and natural language inference. Computing Meaning, 47:129-147, 2014.
[Manning and MacCartney, 2009] Christopher D. Manning and Bill MacCartney. Natural language inference. Stanford University, 2009.
[McCarthy, 1959] John W. McCarthy. Programs with common sense. In Proc. Tedding Conf. on the Mechanization of Thought Processes, pages 75-91, 1959.
[McCarthy, 1984] J. McCarthy. Applications of circumscription to formalizing commonsense. In NMR, 1984.
[Metaxiotis et al., 2002] Kostas S Metaxiotis, Dimitris Askounis, and John Psarras. Expert systems in production planning and scheduling: A state-of-the-art survey. Journal of Intelligent Manufacturing, 13(4):253-260, 2002.
[Musen and Van der Lei, 1988] Mark A Musen and Johan Van der Lei. Of brittleness and bottlenecks: Challenges in the creation of pattern-recognition and expert-system models. In Machine Intelligence and Pattern Recognition, volume 7, pages 335-352. Elsevier, 1988.
[Newell and Simon, 1956] A. Newell and H. Simon. The logic theory machine-a complex information processing system. IRE Trans. Information Theory, 2:61-79, 1956.
[Parikh et al., 2016] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model for natural language inference. In EMNLP, 2016.
[Rajpurkar et al., 2016] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. SQuAD: 100,000+ questions for machine comprehension of text. In EMNLP, 2016.
[Richardson and Sabharwal, 2019] Kyle Richardson and Ashish Sabharwal. What does my qa model know? devising controlled probes using expert knowledge. ArXiv, abs/1912.13337, 2019.
[Richardson et al., 2020] Kyle Richardson, Hai Hu, Lawrence S Moss, and Ashish Sabharwal. Probing natural language inference models through semantic fragments. In AAAI'20, 2020.
[Saxton et al., 2019] David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. In $I C L R, 2019$.
[Selsam et al., 2019] Daniel Selsam, Matthew Lamm, Benedikt Bünz, Percy Liang, Leonardo de Moura, and David L. Dill. Learning a SAT solver from single-bit supervision. In $I C L R, 2019$.
[Sinha et al., 2019] K. Sinha, S. Sodhani, J. Dong, J. Pineau, and W. Hamilton. CLUTRR: a diagnostic benchmark for inductive reasoning from text. In EMNLP, 2019.
[Tafjord et al., 2019] Oyvind Tafjord, Matt Gardner, Kevin Lin, and Peter Clark. Quartz: An open-domain dataset of qualitative relationship questions. In EMNLP, 2019.
[Talmor et al., 2019] A. Talmor, Y. Elazar, Y. Goldberg, and J. Berant. oLMpics - on what language model pre-training captures. ArXiv, abs/1912.13283, 2019.
[Wang et al., 2019] Q. Wang, B. Li, T. Xiao, J. Zhu, C. Li, D. Wong, and L. Chao. Learning deep transformer models for machine translation. In $A C L, 2019$.
[Weber et al., 2019] Leon Weber, Pasquale Minervini, Jannes Münchmeyer, Ulf Leser, and Tim Rocktäschel. Nlprolog: Reasoning with weak unification for question answering in natural language. In $A C L, 2019$.
[Weston et al., 2016] J. Weston, A. Bordes, S. Chopra, and T. Mikolov. Towards AI-Complete question answering: A set of prerequisite toy tasks. In $I C L R, 2016$.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ If one even exists - formal reasoning is still far from modeling all of natural language inference.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>NLI is informally defined as making inferences from language that "a person would typically infer" [Dagan <em>et al.</em>, 2013], and includes use of many linguistic forms, unstated background knowledge, and sometimes unsound inference steps.&#160;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
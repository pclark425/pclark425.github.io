<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8895 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8895</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8895</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-869cadc07bd9b8d9e702349530ec60d510eff2e4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/869cadc07bd9b8d9e702349530ec60d510eff2e4" target="_blank">CogMol: Target-Specific and Selective Drug Design for COVID-19 Using Deep Generative Models</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> An end-to-end framework for designing new drug-like small molecules targeting novel viral proteins with high affinity and off-target selectivity, which combines adaptive pre-training of a molecular SMILES Variational Autoencoder (VAE) and an efficient multi-attribute controlled sampling scheme that uses guidance from attribute predictors trained on latent features.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8895.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8895.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UniRep embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pre-trained protein sequence embeddings (UniRep)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Unsupervised protein sequence embeddings learned from a large unlabeled corpus (UniRef50) and used as protein representations to enable binding-affinity prediction and targeted molecule generation for unseen proteins.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unified rational protein engineering with sequence-based deep representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UniRep (pre-trained protein embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>unsupervised sequence embedding / protein language model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Unlabeled corpus of ~24 million UniRef50 protein sequences (as reported in the cited UniRep work)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery: protein-molecule binding affinity prediction and target-conditioned molecule generation (COVID-19 targets)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Used as fixed protein context embeddings input to a binding-affinity regressor that guides conditional molecule generation (CLaSS) without target-specific retraining</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Enables generation for novel/unseen protein targets (no direct novelty metric tied to UniRep itself); CogMol reports generation of novel molecular scaffolds when using these embeddings in the pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Embedding captures sequence/structural/functional relationships so the affinity predictor can estimate binding for unseen targets and thus steer generation toward target-specific ligands</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Improved binding-affinity predictor RMSE when using UniRep embeddings (RMSE = 0.8426) compared to LSTM baseline (RMSE = 1.0104); used downstream in CLaSS to raise fraction of high-affinity generated molecules</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Using UniRep embeddings in the affinity predictor yielded better predictive RMSE and enabled CogMol to generate target-specific compounds for novel SARS-CoV-2 proteins without retraining; allowed higher proportion of generated molecules passing affinity/selectivity/QED controls.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperformed an LSTM-based embedding baseline for binding-affinity prediction in this work (lower RMSE) and was highlighted as superior to small BindingDB-trained embeddings cited in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Not a panacea — success depends on how related the novel target is to training coverage; affinity predictor still imperfect (RMSE ~0.84) and downstream generation quality depends on that predictor's accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CogMol: Target-Specific and Selective Drug Design for COVID-19 Using Deep Generative Models', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8895.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8895.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Molecular Transformer (retrosynthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Molecular Transformer (used within retrosynthesis predictor)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based sequence-to-sequence model for chemical reaction prediction that is used here as the basis of a retrosynthesis algorithm to estimate synthetic feasibility of generated molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Molecular Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based sequence-to-sequence model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Patent chemical reaction data (as reported in the retrosynthesis implementation used)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Synthetic route prediction / retrosynthesis planning to assess synthetic feasibility of generated drug-like molecules</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Retrosynthetic planning: model predicts reaction steps/precursors to estimate number of steps and feasibility from commercially available materials</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not used to generate novel molecules per se, but used to assess synthesizability of CogMol-generated molecules; novelty metrics in paper pertain to VAE outputs, not retrosynthesis model.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Applied to evaluate synthetic accessibility of CogMol candidates for specific SARS-CoV-2 targets and HDAC1, estimating how many steps and whether precursors are commercially available</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Fraction of molecules predicted synthetically feasible; distribution of number of retrosynthesis steps required (1,2,3,...); comparison to FDA-approved drug set</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>CogMol-generated sets for SARS-CoV-2 targets had high predicted synthesizability (>85-90%) (better than an FDA-drug baseline of ~78%); HDAC1 set was lower (~67%), indicating dataset- and chemistry-class dependence.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Used as a practical synthetic-feasibility filter in the CogMol pipeline; direct performance comparisons to other retrosynthesis methods are not provided, but results show practical utility relative to FDA baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Coverage depends on reaction knowledge in patent training data; some chemical classes (e.g., HDAC1 set) show lower predicted feasibility, reflecting gaps in reaction training data or patent coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CogMol: Target-Specific and Selective Drug Design for COVID-19 Using Deep Generative Models', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8895.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8895.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Seq2Seq Transformer metabolite predictor</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Seq2Seq Transformer for human metabolite prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequence-to-sequence Transformer (pre-trained on chemical reaction data and fine-tuned on metabolite reaction data) used to predict likely human metabolites of generated molecules for downstream toxicity screening.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prediction of drug metabolites using neural machine translation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Seq2Seq Transformer metabolite predictor</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based sequence-to-sequence (neural machine translation) model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pre-trained on chemical reaction data and fine-tuned on human metabolite reaction data (as reported in the metabolite predictor work)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Predicting human metabolic products of small molecules to assess metabolite toxicity during in silico screening</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Treats metabolite prediction as sequence translation: input SMILES -> predicted metabolite SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Produces predicted metabolites (not designed novel parent molecules); novelty metrics in main paper refer to parent molecule generation, not metabolite model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Enables evaluation of toxicity risk of both parent molecules and their metabolites by producing candidate metabolites for scoring with the toxicity classifier</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Downstream toxicity endpoints (MT-DNN predictions across 12 in vitro endpoints + ClinTox); reported percentage of metabolites predicted toxic in 0-1 endpoints (~80% for metabolites)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Majority (~80%) of predicted metabolites were flagged as toxic in 0-1 endpoints (out of 13), comparable to FDA-approved drugs, supporting that CogMol-generated candidates have metabolite-toxicity profiles similar to known drugs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Used as an automated metabolite-prediction component within CogMol; no head-to-head benchmarking versus alternative metabolite predictors is provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Accuracy of metabolite predictions depends on available metabolite reaction data and fine-tuning; downstream toxicity conclusions are conditional on both metabolite prediction fidelity and toxicity predictor accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CogMol: Target-Specific and Selective Drug Design for COVID-19 Using Deep Generative Models', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8895.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8895.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM embeddings (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LSTM-based sequence embeddings for SMILES/proteins (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recurrent (LSTM) embedding of SMILES (and proteins as a baseline) used to build binding-affinity predictors; served as a baseline against which UniRep-based models were compared.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Recurrent neural network (LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>BindingDB compound-protein affinity dataset for downstream predictor training (SMILES and proteins embedded with LSTMs as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Binding-affinity prediction baseline for drug design</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Embeddings used as inputs to a binding-affinity regressor; SMILES were embedded by LSTM for an x-level predictor used in in silico screening</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not applicable — LSTM used as representation for prediction rather than generative model for novel molecule design in this work</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Baseline predictor using LSTM embeddings yielded higher RMSE (worse) than the UniRep-initialized predictor, demonstrating less accurate affinity estimates for unseen targets</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Binding-affinity predictor RMSE: LSTM baseline RMSE = 1.0104 (binding affinity x-level baseline) versus UniRep-based model RMSE = 0.8426</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LSTM-based embeddings provided a working baseline for affinity prediction but were outperformed by UniRep-based protein embeddings in RMSE, motivating use of large unsupervised protein embeddings for generation targeting unseen proteins.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly within this paper to UniRep-based embeddings for binding-affinity prediction; UniRep produced better predictive performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>LSTM embeddings were less accurate for affinity prediction on the available data and appear to generalize less well to unseen protein targets compared to large pre-trained protein embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CogMol: Target-Specific and Selective Drug Design for COVID-19 Using Deep Generative Models', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8895.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8895.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer protein->drug (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer neural network for protein-specific de novo drug generation (mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recently proposed approach that frames target-specific de novo drug generation as a translation problem from amino-acid 'language' to molecular SMILES using a Transformer; cited as related work and contrasted with CogMol's use of large pre-trained protein embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transformer neural network for protein specific de novo drug generation as machine translation problem</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer for protein-to-ligand translation</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (sequence-to-sequence / translation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Reported to be trained on BindingDB protein-ligand pairs with ~1100 human protein sequences (as discussed in the paper's related-work commentary)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Target-specific de novo drug generation (drug discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Treats generation as machine-translation from protein sequence representation to ligand SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not quantified in this paper; limitation noted that limited protein training coverage (~1100 proteins) constrains generalization to unseen targets</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Designed to produce ligands conditioned on protein sequence, but generalization is limited by restricted protein training corpus</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not detailed in this paper (only cited in related work); primary critique here is limited generalizability due to small protein set</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as a promising translation-based approach, but criticized in this work for being trained on a small protein set (BindingDB human proteins) and thus limited for unseen targets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Contrasted with CogMol's approach which leverages large unsupervised protein embeddings (UniRep) trained on ~24M sequences to improve generalization to novel targets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Paper notes the approach's limitation: protein embeddings learned from only ~1100 human proteins limit model generalization to novel or unrelated targets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CogMol: Target-Specific and Selective Drug Design for COVID-19 Using Deep Generative Models', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Unified rational protein engineering with sequence-based deep representation learning <em>(Rating: 2)</em></li>
                <li>Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction <em>(Rating: 2)</em></li>
                <li>Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy <em>(Rating: 2)</em></li>
                <li>Prediction of drug metabolites using neural machine translation <em>(Rating: 2)</em></li>
                <li>Transformer neural network for protein specific de novo drug generation as machine translation problem <em>(Rating: 1)</em></li>
                <li>DeepAffinity: interpretable deep learning of compound-protein affinity through unified recurrent and convolutional neural networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8895",
    "paper_id": "paper-869cadc07bd9b8d9e702349530ec60d510eff2e4",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "UniRep embeddings",
            "name_full": "Pre-trained protein sequence embeddings (UniRep)",
            "brief_description": "Unsupervised protein sequence embeddings learned from a large unlabeled corpus (UniRef50) and used as protein representations to enable binding-affinity prediction and targeted molecule generation for unseen proteins.",
            "citation_title": "Unified rational protein engineering with sequence-based deep representation learning",
            "mention_or_use": "use",
            "model_name": "UniRep (pre-trained protein embeddings)",
            "model_type": "unsupervised sequence embedding / protein language model",
            "model_size": null,
            "training_data": "Unlabeled corpus of ~24 million UniRef50 protein sequences (as reported in the cited UniRep work)",
            "application_domain": "Drug discovery: protein-molecule binding affinity prediction and target-conditioned molecule generation (COVID-19 targets)",
            "generation_method": "Used as fixed protein context embeddings input to a binding-affinity regressor that guides conditional molecule generation (CLaSS) without target-specific retraining",
            "novelty_of_chemicals": "Enables generation for novel/unseen protein targets (no direct novelty metric tied to UniRep itself); CogMol reports generation of novel molecular scaffolds when using these embeddings in the pipeline.",
            "application_specificity": "Embedding captures sequence/structural/functional relationships so the affinity predictor can estimate binding for unseen targets and thus steer generation toward target-specific ligands",
            "evaluation_metrics": "Improved binding-affinity predictor RMSE when using UniRep embeddings (RMSE = 0.8426) compared to LSTM baseline (RMSE = 1.0104); used downstream in CLaSS to raise fraction of high-affinity generated molecules",
            "results_summary": "Using UniRep embeddings in the affinity predictor yielded better predictive RMSE and enabled CogMol to generate target-specific compounds for novel SARS-CoV-2 proteins without retraining; allowed higher proportion of generated molecules passing affinity/selectivity/QED controls.",
            "comparison_to_other_methods": "Outperformed an LSTM-based embedding baseline for binding-affinity prediction in this work (lower RMSE) and was highlighted as superior to small BindingDB-trained embeddings cited in related work.",
            "limitations_and_challenges": "Not a panacea — success depends on how related the novel target is to training coverage; affinity predictor still imperfect (RMSE ~0.84) and downstream generation quality depends on that predictor's accuracy.",
            "uuid": "e8895.0",
            "source_info": {
                "paper_title": "CogMol: Target-Specific and Selective Drug Design for COVID-19 Using Deep Generative Models",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "Molecular Transformer (retrosynthesis)",
            "name_full": "Molecular Transformer (used within retrosynthesis predictor)",
            "brief_description": "A transformer-based sequence-to-sequence model for chemical reaction prediction that is used here as the basis of a retrosynthesis algorithm to estimate synthetic feasibility of generated molecules.",
            "citation_title": "Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction",
            "mention_or_use": "use",
            "model_name": "Molecular Transformer",
            "model_type": "Transformer-based sequence-to-sequence model",
            "model_size": null,
            "training_data": "Patent chemical reaction data (as reported in the retrosynthesis implementation used)",
            "application_domain": "Synthetic route prediction / retrosynthesis planning to assess synthetic feasibility of generated drug-like molecules",
            "generation_method": "Retrosynthetic planning: model predicts reaction steps/precursors to estimate number of steps and feasibility from commercially available materials",
            "novelty_of_chemicals": "Not used to generate novel molecules per se, but used to assess synthesizability of CogMol-generated molecules; novelty metrics in paper pertain to VAE outputs, not retrosynthesis model.",
            "application_specificity": "Applied to evaluate synthetic accessibility of CogMol candidates for specific SARS-CoV-2 targets and HDAC1, estimating how many steps and whether precursors are commercially available",
            "evaluation_metrics": "Fraction of molecules predicted synthetically feasible; distribution of number of retrosynthesis steps required (1,2,3,...); comparison to FDA-approved drug set",
            "results_summary": "CogMol-generated sets for SARS-CoV-2 targets had high predicted synthesizability (&gt;85-90%) (better than an FDA-drug baseline of ~78%); HDAC1 set was lower (~67%), indicating dataset- and chemistry-class dependence.",
            "comparison_to_other_methods": "Used as a practical synthetic-feasibility filter in the CogMol pipeline; direct performance comparisons to other retrosynthesis methods are not provided, but results show practical utility relative to FDA baseline.",
            "limitations_and_challenges": "Coverage depends on reaction knowledge in patent training data; some chemical classes (e.g., HDAC1 set) show lower predicted feasibility, reflecting gaps in reaction training data or patent coverage.",
            "uuid": "e8895.1",
            "source_info": {
                "paper_title": "CogMol: Target-Specific and Selective Drug Design for COVID-19 Using Deep Generative Models",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "Seq2Seq Transformer metabolite predictor",
            "name_full": "Seq2Seq Transformer for human metabolite prediction",
            "brief_description": "A sequence-to-sequence Transformer (pre-trained on chemical reaction data and fine-tuned on metabolite reaction data) used to predict likely human metabolites of generated molecules for downstream toxicity screening.",
            "citation_title": "Prediction of drug metabolites using neural machine translation",
            "mention_or_use": "use",
            "model_name": "Seq2Seq Transformer metabolite predictor",
            "model_type": "Transformer-based sequence-to-sequence (neural machine translation) model",
            "model_size": null,
            "training_data": "Pre-trained on chemical reaction data and fine-tuned on human metabolite reaction data (as reported in the metabolite predictor work)",
            "application_domain": "Predicting human metabolic products of small molecules to assess metabolite toxicity during in silico screening",
            "generation_method": "Treats metabolite prediction as sequence translation: input SMILES -&gt; predicted metabolite SMILES",
            "novelty_of_chemicals": "Produces predicted metabolites (not designed novel parent molecules); novelty metrics in main paper refer to parent molecule generation, not metabolite model outputs.",
            "application_specificity": "Enables evaluation of toxicity risk of both parent molecules and their metabolites by producing candidate metabolites for scoring with the toxicity classifier",
            "evaluation_metrics": "Downstream toxicity endpoints (MT-DNN predictions across 12 in vitro endpoints + ClinTox); reported percentage of metabolites predicted toxic in 0-1 endpoints (~80% for metabolites)",
            "results_summary": "Majority (~80%) of predicted metabolites were flagged as toxic in 0-1 endpoints (out of 13), comparable to FDA-approved drugs, supporting that CogMol-generated candidates have metabolite-toxicity profiles similar to known drugs.",
            "comparison_to_other_methods": "Used as an automated metabolite-prediction component within CogMol; no head-to-head benchmarking versus alternative metabolite predictors is provided here.",
            "limitations_and_challenges": "Accuracy of metabolite predictions depends on available metabolite reaction data and fine-tuning; downstream toxicity conclusions are conditional on both metabolite prediction fidelity and toxicity predictor accuracy.",
            "uuid": "e8895.2",
            "source_info": {
                "paper_title": "CogMol: Target-Specific and Selective Drug Design for COVID-19 Using Deep Generative Models",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "LSTM embeddings (baseline)",
            "name_full": "LSTM-based sequence embeddings for SMILES/proteins (baseline)",
            "brief_description": "Recurrent (LSTM) embedding of SMILES (and proteins as a baseline) used to build binding-affinity predictors; served as a baseline against which UniRep-based models were compared.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LSTM embeddings",
            "model_type": "Recurrent neural network (LSTM)",
            "model_size": null,
            "training_data": "BindingDB compound-protein affinity dataset for downstream predictor training (SMILES and proteins embedded with LSTMs as baseline)",
            "application_domain": "Binding-affinity prediction baseline for drug design",
            "generation_method": "Embeddings used as inputs to a binding-affinity regressor; SMILES were embedded by LSTM for an x-level predictor used in in silico screening",
            "novelty_of_chemicals": "Not applicable — LSTM used as representation for prediction rather than generative model for novel molecule design in this work",
            "application_specificity": "Baseline predictor using LSTM embeddings yielded higher RMSE (worse) than the UniRep-initialized predictor, demonstrating less accurate affinity estimates for unseen targets",
            "evaluation_metrics": "Binding-affinity predictor RMSE: LSTM baseline RMSE = 1.0104 (binding affinity x-level baseline) versus UniRep-based model RMSE = 0.8426",
            "results_summary": "LSTM-based embeddings provided a working baseline for affinity prediction but were outperformed by UniRep-based protein embeddings in RMSE, motivating use of large unsupervised protein embeddings for generation targeting unseen proteins.",
            "comparison_to_other_methods": "Compared directly within this paper to UniRep-based embeddings for binding-affinity prediction; UniRep produced better predictive performance.",
            "limitations_and_challenges": "LSTM embeddings were less accurate for affinity prediction on the available data and appear to generalize less well to unseen protein targets compared to large pre-trained protein embeddings.",
            "uuid": "e8895.3",
            "source_info": {
                "paper_title": "CogMol: Target-Specific and Selective Drug Design for COVID-19 Using Deep Generative Models",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "Transformer protein-&gt;drug (related work)",
            "name_full": "Transformer neural network for protein-specific de novo drug generation (mentioned in related work)",
            "brief_description": "A recently proposed approach that frames target-specific de novo drug generation as a translation problem from amino-acid 'language' to molecular SMILES using a Transformer; cited as related work and contrasted with CogMol's use of large pre-trained protein embeddings.",
            "citation_title": "Transformer neural network for protein specific de novo drug generation as machine translation problem",
            "mention_or_use": "mention",
            "model_name": "Transformer for protein-to-ligand translation",
            "model_type": "Transformer (sequence-to-sequence / translation)",
            "model_size": null,
            "training_data": "Reported to be trained on BindingDB protein-ligand pairs with ~1100 human protein sequences (as discussed in the paper's related-work commentary)",
            "application_domain": "Target-specific de novo drug generation (drug discovery)",
            "generation_method": "Treats generation as machine-translation from protein sequence representation to ligand SMILES",
            "novelty_of_chemicals": "Not quantified in this paper; limitation noted that limited protein training coverage (~1100 proteins) constrains generalization to unseen targets",
            "application_specificity": "Designed to produce ligands conditioned on protein sequence, but generalization is limited by restricted protein training corpus",
            "evaluation_metrics": "Not detailed in this paper (only cited in related work); primary critique here is limited generalizability due to small protein set",
            "results_summary": "Mentioned as a promising translation-based approach, but criticized in this work for being trained on a small protein set (BindingDB human proteins) and thus limited for unseen targets.",
            "comparison_to_other_methods": "Contrasted with CogMol's approach which leverages large unsupervised protein embeddings (UniRep) trained on ~24M sequences to improve generalization to novel targets.",
            "limitations_and_challenges": "Paper notes the approach's limitation: protein embeddings learned from only ~1100 human proteins limit model generalization to novel or unrelated targets.",
            "uuid": "e8895.4",
            "source_info": {
                "paper_title": "CogMol: Target-Specific and Selective Drug Design for COVID-19 Using Deep Generative Models",
                "publication_date_yy_mm": "2020-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Unified rational protein engineering with sequence-based deep representation learning",
            "rating": 2
        },
        {
            "paper_title": "Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction",
            "rating": 2
        },
        {
            "paper_title": "Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy",
            "rating": 2
        },
        {
            "paper_title": "Prediction of drug metabolites using neural machine translation",
            "rating": 2
        },
        {
            "paper_title": "Transformer neural network for protein specific de novo drug generation as machine translation problem",
            "rating": 1
        },
        {
            "paper_title": "DeepAffinity: interpretable deep learning of compound-protein affinity through unified recurrent and convolutional neural networks",
            "rating": 1
        }
    ],
    "cost": 0.01605125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CogMol: Target-Specific and Selective Drug Design for COVID-19 Using Deep Generative Models</h1>
<p>Vijil Chenthamarakshan, Payel Das, Samuel C. Hoffman, Hendrik Strobelt ${ }^{\dagger}$, Inkit Padhi, Kar Wai Lim<em>, Benjamin Hoover ${ }^{\dagger}$, Matteo Manica ${ }^{\ddagger}$, Jannis Born ${ }^{\ddagger}$, Teodoro Laino ${ }^{\ddagger}$, Aleksandra Mojsilovic<br>IBM Research, Yorktown Heights, New York; ${ }^{</em>}$ IBM Research, Singapore<br>${ }^{\dagger}$ IBM Research, MIT-IBM Watson AI Lab, Cambridge; ${ }^{\ddagger}$ IBM Research Europe<br>{ecvijil, daspa, aleksand}@us.ibm.com,<br>{shoffman, hendrik.strobelt}@ibm.com,<br>{inkpad, kar.wai.lim, benjamin.hoover}@ibm.com,<br>{tte, jab,teo}@zurich.ibm.com</p>
<h4>Abstract</h4>
<p>The novel nature of SARS-CoV-2 calls for the development of efficient de novo drug design approaches. In this study, we propose an end-to-end framework, named CogMol (Controlled Generation of Molecules), for designing new drug-like small molecules targeting novel viral proteins with high affinity and off-target selectivity. CogMol combines adaptive pre-training of a molecular SMILES Variational Autoencoder (VAE) and an efficient multi-attribute controlled sampling scheme that uses guidance from attribute predictors trained on latent features. To generate novel and optimal drug-like molecules for unseen viral targets, CogMol leverages a protein-molecule binding affinity predictor that is trained using SMILES VAE embeddings and protein sequence embeddings learned unsupervised from a large corpus. We applied the CogMol framework to three SARS-CoV-2 target proteins: main protease, receptor-binding domain of the spike protein, and non-structural protein 9 replicase. The generated candidates are novel at both the molecular and chemical scaffold levels when compared to the training data. CogMol also includes in silico screening for assessing toxicity of parent molecules and their metabolites with a multi-task toxicity classifier, synthetic feasibility with a chemical retrosynthesis predictor, and target structure binding with docking simulations. Docking reveals favorable binding of generated molecules to the target protein structure, where $87-95 \%$ of high affinity molecules showed docking free energy $&lt;-6 \mathrm{kcal} / \mathrm{mol}$. When compared to approved drugs, the majority of designed compounds show low parent molecule and metabolite toxicity and high synthetic feasibility. In summary, CogMol can handle multi-constraint design of synthesizable, low-toxic, drug-like molecules with high target specificity and selectivity, even to novel protein target sequences, and does not need target-dependent fine-tuning of the framework or target structure information.</p>
<h2>1 Introduction</h2>
<p>Generating novel drug molecules is a daunting task that aims to create new molecules (or optimize known molecules) with multiple desirable properties that often compete and tightly interact with each other. For example, optimal drug molecules should have binding affinity to the target protein of interest (target specificity), low binding affinity to other targets (off-target selectivity), be easy to synthesize, and also exhibit high drug likeliness (QED). This makes drug discovery a costly (2-3 billion USD) and time-consuming process (more than a decade) with a low success rate ( $&lt;10 \%$ ) [1].</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Workflow of the drug candidate generation pipeline
Traditional in silico molecule design and screening rely on rational design methods that need physicsbased simulations, heuristic search algorithms, and considerable domain knowledge. However, optimizing over the discrete, unstructured and sparse molecular space remains an intrinsically difficult challenge. Therefore, there is much interest in developing automated machine learning techniques to efficiently discover sizeable numbers of plausible, diverse and novel candidate molecules in the vast $\left(10^{23}-10^{60}\right)$ space of molecules [2]. Bayesian optimization, Reinforcement Learning, and gradientbased optimization methods have been proposed for automating drug molecule design with desired properties (e.g., high drug-likeliness, synthetic accessibility, or solubility) [3-5]. These methods either optimize directly on the high-dimensional input space or on the low dimensional representation learned using a latent variable model such as a probabilistic autoencoder.
One crucial aspect of designing drug candidates is to account for the right context, e.g., protein, gene, metabolic or disease pathway information. For example, in target protein-specific drug design, the goal is to generate molecules with high binding affinity to a specific target protein. This requires fine-tuning a generative model on a small library of ligands to enable target-specific sampling. For novel or unrelated proteins, such as the SARS-CoV-2 viral proteins involved in the recent COVID-19 pandemic, binding affinity data is unavailable. At the same time, these novel target proteins are not related to the proteins in existing binding affinity databases. Thus, handling novel targets in the current generative frameworks becomes non-trivial.
Designing drug candidates for novel targets gets even more challenging, as the drug designed for the novel target can bind to other undesired targets. Small molecule drugs have been shown to bind on average to a minimum of 6-11 distinct targets in addition to their intended target [6]. This molecular "promiscuity" of drugs causes unintended therapeutic effects or multiple drug-target interactions leading to off-target toxicities and decreased effectiveness [7, 8]. Accounting for this important aspect of off-target selectivity becomes non-trivial if the generative model is trained only on a small ligand library optimized for a single target or only on good binder molecules for a limited set of targets.</p>
<h1>2 CogMol - Molecule Generation Pipeline</h1>
<p>To address the challenges stated above, we propose an alternative method named Controlled Generation of Molecules (CogMol) for designing small molecule drugs in a context, i.e. target protein in this study, -dependent manner. CogMol accounts for both target specificity and selectivity, even for novel or low-coverage target sequences. As depicted in Figure 1, CogMol includes the following components:</p>
<ol>
<li>A Variational Autoencoder (VAE), first trained unsupervised and then jointly with a set of attribute regressors (QED and Synthetic Accessibility, SA), that learns a disentangled latent space of the molecules using the SMILES representation.</li>
<li>A protein-molecule binding affinity regressor trained on the VAE latent features of molecules and protein sequence embeddings trained on a large unlabeled corpus, which is used for estimating target specificity and off-target selectivity.</li>
<li>An efficient sampling scheme to generate molecules with desired attributes from the model of the VAE latent space, using guidance from a set of attribute (affinity, selectivity, QED) predictors.
Instead of training the binding affinity regressor on the sequence embeddings of a few thousand target proteins reported in the binding affinity database, CogMol uses pre-trained protein sequence embeddings, [9] learned on an unlabeled corpus of 24 M Uniprot protein sequences from UniRef50 database, to train the affinity predictor. Since these pre-trained protein embeddings are capable of better capturing sequence, structural, and functional relationships [9, 10], using them in CogMol allows targeted generation of molecules even for new/unseen targets and does not require model retraining for every individual target. Finally, CogMol proposes an efficient way of modeling off-</li>
</ol>
<p>target selectivity and using this as a control for targeted generation, leveraging the same trained protein-ligand binding affinity predictor.
CogMol is also empowered with an in silico screening protocol for generated molecules, which involves: (i) toxicity prediction on several in vitro and clinical endpoints for parent molecules and their predicted metabolites using a multi-task deep learning-based classifier, (ii) synthetic feasibility prediction using a chemical retrosynthesis predictor; and (iii) blind docking simulations to estimate binding of the generated molecules to the target protein structure. We hope that accounting for multiple important properties that play a role in the efficacy or viability of a drug such as target affinity, offtarget selectivity, toxicity of parent molecules and their metabolites, and synthetic practicality, within an AI framework will help the in silico drug design process to be faster and less costly, leading to shorter discovery pipelines with high success rate.
CogMol for COVID-19 Antiviral Molecule Design. Given the urgency with the ongoing COVID19 pandemic, we apply CogMol to generate candidate molecules that bind to three relevant target proteins of the novel SARS-CoV-2 virus, namely NSP9 Replicase (NSP9), Main Protease ( $\mathrm{M}^{\mathrm{pro}}$ ), and the Receptor-Binding Domain (RBD) of the SARS-CoV-2 S protein, with high affinity. Note, these targets are not present in the binding affinity database, and both NSP9 and RBD are more novel than $\mathrm{M}^{\text {pro }}$ (See Supp. Mat. A). We also used CogMol to generate molecules for human Histone deacetylase 1 (HDAC1) protein implicated in cancer, for which the number of molecules with desired criteria in the training database is very low.
Our contributions in this work are: (i) An end-to-end framework for drug-like small molecule design that accounts for multiple relevant and critical factors such as target affinity, off-target selectivity, toxicity of the parent molecules and their metabolites across different endpoints, target structure binding, and synthetic practicality. (ii) This is, to our knowledge, the first deep generative approach that generates novel, specific, and selective drug-like small molecules for a unseen target sequence without model retraining. (iii) A system capable of generating drug-like molecules with high target affinity and selectivity for selected targets that are either relatively novel or have a low ligand coverage. (iv) Although our framework learns from 1D protein sequence information only, generated molecules are still capable of binding to the druggable binding pockets of the 3D target structure with favorable binding free energy (BFE). (v) For three novel and very relevant COVID-19 targets (as well as for a cancer target with low coverage of optimal ligands), we are able to identify a set of generated novel and optimal drug-like molecules with high target affinity and selectivity, that binds well to the target structure, is synthetically practical and has low predicted parent and metabolite toxicity with respect to FDA-approved drugs.</p>
<h1>3 Related Work</h1>
<p>Earlier approaches to generate molecules involved recurrent neural networks (RNN) [11, 12], whereas recent works employ deep generative frameworks, such as the Variational Autoencoder (VAE) [3, 1315] and the Generative Adversarial Network (GAN) [16, 17]. Most of those works employ SMILES representation. Generating syntactically valid molecules under SMILES grammar is challenging and there have been attempts to ensure validity via constraints [18, 19]. Recently, there has been increasing interest in molecular graph-based generative methods [20-25]. Unfortunately, graph-based models are not permutation-invariant of their node labels, the training has a quadratic complexity concerning the number of nodes, and generating semantically valid graphs is challenging. [26] is considered as a state-of-the-art architecture in this context, which represents a molecular graph as fragments connected in a tree structure.
For targeted generation of molecules with a set of desired attributes, Reinforcement Learning (RL) and Bayesian Optimization (BO) methods have often been employed on top of a SMILES- or Graph-based molecule generator [27-30, 4, 3, 31, 5], but typically incur high computational cost. Semi-supervised learning has also been used [15, 14, 21], which involves optimizing complicated loss objectives. CogMol is instead inspired by the Controlled Latent attribute Space Sampling (CLaSS) method [32]. Our proposed methodology aims at computationally efficient targeted generation with multiple constraints from the latent space, requiring minimal model training and no supervision.
To generate drugs specific to a particular target, generative models in existing works [4, 21, 33] are typically fine-tuned on the subset of molecules that bind to that specific target sequence or structure and hence cannot generalize to unseen targets. Recently, target-specific de novo drug design has been defined as a translation problem between amino acid "language" and molecular</p>
<p>SMILES [34], where the latent code $z$ of a protein is considered as a "context" to generate a binding ligand. However, the protein embeddings are learned only from the $\sim 1100$ human protein sequences captured in BindingDB, which limits the model's generalization capabilities. In contrast, CogMol uses protein embeddings from Unirep trained on an unsupervised corpus of $\sim 24$ million UniRef50 amino acid sequences [9]. This approach has been demonstrated to improve performance in downstream prediction tasks [10, 9] as well as in generative modeling [32].</p>
<h1>4 Model and Methods</h1>
<p>Data. We used the Moses benchmarking dataset [35] for the unsupervised VAE training, which include 1.6 M molecules in the training set and 176 K molecules in the test and scaffold test sets respectively from the ZINC database [36].
For target-specific compound design, we used a curated IC50-labeled compound-protein binding data from BindingDB [37], as reported in DeepAffinity [38]. The DeepAffinity models use a separate held-out set with four different protein classes to test the generalizability of their predictor. Since our objective is to build the best binding affinity (pIC50) regression model using available data, we also added the four excluded classes into our training data.
Variational Autoencoder for Molecule Generation. A Variational Autoencoder (VAE) [39] frames an autoencoder in a probabilistic formalism that constrains the expressivity of the latent space, $z$. Each sample defines an encoding distribution $q_{\phi}(z \mid x)$ and for each sample, this encoder distribution is constrained to be close to a simple prior distribution $p(z)$. We consider the case of the encoder specifying a diagonal Gaussian distribution only, i.e. $q_{\phi}(z \mid x)=$ $N(z ; \mu(x), \Sigma(x))$ with $\Sigma(x)=\operatorname{diag}\left[\exp \left(\log \left(\sigma_{d}^{2}\right)(x)\right)\right)$. The encoder neural network produces the $\log$ variances $\log \left(\sigma_{d}^{2}\right)(x)$. The marginal posterior is $q_{\phi}(\mathbf{z})=\frac{1}{N} \sum_{j=1}^{N} q_{\phi}\left(\mathbf{z} \mid \mathbf{x}<em _mathrm_KL="\mathrm{KL">{j}\right)$. The standard VAE objective is defined as follows (where $D</em>}}$ is the Kullback-Leibler divergence), $\mathcal{L<em p_x_="p(x)">{\mathrm{VAE}}(\theta, \phi)=$ $\mathbb{E}</em>}\left{\mathbb{E<em _phi="\phi">{q</em>(z \mid x) | p(z)\right)\right}$.
We also jointly trained two property predictors, one for QED and one for SA, each parameterized by a feed-forward network, along with the VAE, to predict $y(x)$ from the latent embedding of $x$. As shown in Supp. Mat. K, the BindingDB molecules have a different distribution of QED when compared to molecules in ZINC. To better reflect this diversity in the latent embeddings of the VAE, we continued training of the VAE model with QED and SA predictors on BindingDB molecules. We report the architecture and performance of the final VAE model in Supp. Mat. B.
Attribute Predictors. We train multiple property predictors for controlling generation. The architecture and performance of these predictors are reported in Supp. Mat. F. First, to test the information content of the VAE latent space, we trained multiple attribute (QED, $\log \mathrm{P}$, and SA) predictors on the latent embeddings. These models show low root-mean-square-error (RMSE) on test data for all three attribute predictors.
Next, we trained a binding affinity regression model using the pIC50 $(=-\log (I C 50))$ data from BindingDB. This model takes a representation of a target protein sequence and latent embedding, $\mathbf{z}$, of a molecule as input, and predicts the binding affinity between the protein-molecule pair. We used pre-trained protein embeddings from [9] to initialize the weights for proteins. This model, along with the model for QED is used in the controlled generation pipeline. We also trained a binding affinity predictor using SMILES (x) instead of latent (z) embedding as the input molecular representation. This model was used during the in silico screening process as it has a higher accuracy than the model trained on the latent embeddings, comparable to a recent model described in [38].
Selectivity Modeling. Selectivity to a particular target is often modeled only in the later stages of a drug development pipeline. It has been suggested that improvement in the early accounting of offtarget interactions represents an opportunity to reduce safety-related attrition rates during pre-clinical and clinical development [8]. Given the novel nature of COVID-19, it is even more important to account for off-target selectivity in the early design state in order to minimize undesired interactions with host targets. Thus, we believe that incorporating selectivity during the candidate generation stage will contribute to a reduction in the failure rate of drug candidates. We define selectivity as the excess binding affinity (BA) of a molecule $(m)$ to a target of interest $(T)$ over its average binding affinities to a random selection of $k$ targets [40]: $\operatorname{Sel}}(z \mid x)}\left[\log p_{\theta}(x \mid z)\right]-D_{\mathrm{KL}}\left(q_{\phi<em i="1">{T, m}=B A(T, m)-\frac{1}{k} \sum</em>, m\right)$.}^{k} B A\left(T_{i</p>
<p>Controlled Generation. Our objective is to generate molecules that simultaneously satisfy multiple (often conflicting) objectives. Specifically, we want generated molecules controlled by high binding affinity to a selected novel SARS-CoV-2 target, high drug-likeliness, and high off-target selectivity.
For this purpose, we performed conditional generation using Conditional Latent (attribute) Space Sampling - CLaSS proposed recently in [32]. In short, CLaSS leverages the attribute predictors trained on the latent features and uses a rejection sampling scheme to generate samples with desired attributes from a density model of the latent space. Since the goal is to sample conditionally $p(\mathbf{x} \mid \mathbf{a})$, where $\mathbf{a} \in \mathbb{R}^{n}=\left[a_{1}, a_{2}, \ldots, a_{n}\right]$, a set of independent attributes, CLaSS approaches this task through conditional sampling in latent space: $p(\mathbf{x} \mid \mathbf{a})=\mathbb{E}<em _mathbf_z="\mathbf{z">{\mathbf{z}}[p(\mathbf{z} \mid \mathbf{a}) p(\mathbf{x} \mid \mathbf{z})] \approx \mathbb{E}</em>}}\left[\hat{p<em _theta="\theta">{\xi}(\mathbf{z} \mid \mathbf{a}) p</em>}(\mathbf{x} \mid \mathbf{z})\right]$. Where $\hat{p<em _xi="\xi">{\xi}(\mathbf{z} \mid \mathbf{a})$ uses rejection sampling from parametric approximations to $p(\mathbf{z} \mid \mathbf{a})$. The term $\hat{p}</em>)&gt;0$ and probabilities from all predictors are $&gt;0$, samples will be accepted in this scheme. Consequently, CLaSS can sample from the targeted region of the autoencoder latent space, which was trained unsupervised. Learning to control for one or more attribute(s) in CLaSS is computationally efficient, as it does not require a surrogate model or policy learning and neither adds complicated loss terms to the original objective.
In Silico Screening. Molecular toxicity or side effect testing is conventionally carried out via different endpoint experiments, e.g., in vitro molecular assays, in vivo animal testing, clinical trials, and adverse effect reports. However, these experiments are costly and time-consuming. We instead used a multitask deep neural network (MT-DNN) for binary (yes/no) toxicity prediction as an early screening tool to prioritize the testing of molecules that are less likely to be harmful and to speed up the process of finding a COVID-19 therapeutic (For details see Supp. Mat. G). A multitask model is expected to improve the prediction by exploiting the correlation between different endpoints. The MT-DNN was used to predict the toxicity of 12 in vitro endpoints from the Tox21 challenge [41]. We also predicted whether the generated molecules would fail clinical trials, using the ClinTox data [42].
The generated molecules were screened further for target affinity and selectivity using the $\mathbf{x}$-level binding affinity predictor (See Supp. Mat. F). To investigate the possible binding modes of the generated molecules with the target protein structure, we performed 5 independent runs of blind docking of the generated achiral molecules with the target structure using Autodock Vina [43]. To evaluate the synthetic accessibility, the generated molecules were analyzed using a retrosynthetic algorithm [44] based on the Molecular Transformer [45] trained on patent chemical reaction data.}(\mathbf{z} \mid \mathbf{a})$ is approximated using a density model $Q_{\xi}(\mathbf{z})$, such as a Gaussian mixture model and per-attribute classifier model $q_{\xi}\left(a_{i} \mid \mathbf{z}\right)$. This is approached by using Bayes' rule and then conditional independence of the attributes [32]. Rejection sampling is then performed through the proposal distribution: $g(\mathbf{z})=$ $Q_{\xi}(\mathbf{z})$ that can be directly sampled. Since we impose multiple attribute constraints for sampling, intuitively, the acceptance probability is equal to the product of the attribute predictors' scores, while sampling from explicit density $Q_{\xi}(z)$. As long as there is a region in $\mathbf{z}$ space where $Q_{\xi}(\mathbf{z</p>
<h1>5 Results and Discussion</h1>
<h3>5.1 Benchmarking Molecular VAE Model</h3>
<p>The architecture and performance metrics of the final VAE model that is adaptively pre-trained from ZINC to BindingDB with SA and QED supervision are provided in Supp. Mat. B, along with a comparison to a number of baseline models. The majority of the generated molecules are chemically valid ( $90 \%$ ), unique ( $99 \%$ ), pass relevant filters ( $95 \%$ ), and show a slightly higher diversity (Table B.1). One interesting observation from Table B. 2 is that the generated molecular ensemble has a higher Fréchet ChemNet Distance (FCD) [46] with respect to chemical scaffolds present in both ZINC and BindingDB training molecules. This implies that adaptive pre-training from ZINC to BindingDB enables the discovery of novel chemical scaffolds, which is further confirmed by comparing the Tanimoto Similarity between generated scaffolds and reference scaffolds (Supp. Mat. Figure B.1). A few synthetically plausible and novel scaffolds from the generated set are shown in Supp. Mat. Figure B. 2</p>
<h3>5.2 Attributes of COVID-Targeted Molecules</h3>
<p>CogMol-Controlled Attributes - Target Affinity, Selectivity, and QED. Table 1 reports higher proportion of molecules with desired attributes in the set accepted in CLaSS, when compared to a randomly sampled set, implying that CLaSS does generate a more optimal set than random sampling</p>
<p>Table 1: Normalized fraction of molecules that are accepted in CLaSS with different set of controls (Affinity, QED, and Selectivity). The values of controls are normalized between 0 and 1. As we increase the extent of controls, a relatively higher proportion of molecules meeting all criteria are in the accepted set compared to a randomly sampled set.</p>
<table>
<thead>
<tr>
<th></th>
<th>Aff $&gt;0.5$</th>
<th></th>
<th>Aff $&gt;0.5 \&amp;$ QED $&gt;0.8$</th>
<th></th>
<th>Aff $&gt;0.5 \&amp;$ QED $&gt;0.8 \&amp;$ Sel $&gt;0.5$</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Accepted</td>
<td>Random</td>
<td>Accepted</td>
<td>Random</td>
<td>Accepted</td>
<td>Random</td>
</tr>
<tr>
<td>NSP9</td>
<td>0.567</td>
<td>0.355</td>
<td>0.45</td>
<td>0.211</td>
<td>0.069</td>
<td>0.007</td>
</tr>
<tr>
<td>RBD</td>
<td>0.546</td>
<td>0.369</td>
<td>0.429</td>
<td>0.217</td>
<td>0.09</td>
<td>0.009</td>
</tr>
<tr>
<td>$\mathrm{M}^{\text {pro }}$</td>
<td>0.603</td>
<td>0.366</td>
<td>0.472</td>
<td>0.216</td>
<td>0.104</td>
<td>0.011</td>
</tr>
</tbody>
</table>
<p>Table 2: CogMol-generated SMILES found in PubChem and their predicted affinity (pIC50), lowest docking free energy ( $\mathrm{kcal} / \mathrm{mol}$ ), PubChem Compound ID (CID), and reported biological activity.</p>
<table>
<thead>
<tr>
<th>Target</th>
<th>Pred. Affinity</th>
<th>Docking Energy</th>
<th>CID</th>
<th>Biological Activity</th>
</tr>
</thead>
<tbody>
<tr>
<td>NSP9</td>
<td>6.51</td>
<td>-7.7</td>
<td>12042753</td>
<td>Antagonist of rat mGluR</td>
</tr>
<tr>
<td>Dimer</td>
<td>7.06</td>
<td>-5.6</td>
<td>44397285</td>
<td>Active to human S6 kinase</td>
</tr>
<tr>
<td></td>
<td>7.18</td>
<td>-6.4</td>
<td>10570770</td>
<td>Matrix metalloproteinase inhibitor</td>
</tr>
<tr>
<td>Main</td>
<td>7.24</td>
<td>-6.1</td>
<td>10608757</td>
<td>Dihydrofolate reductase inhibitor</td>
</tr>
<tr>
<td>Protease</td>
<td>6.91</td>
<td>-6.9</td>
<td>872399</td>
<td>Shiga toxin inhibitor</td>
</tr>
<tr>
<td>RBD</td>
<td>7.82</td>
<td>-7.5</td>
<td>76332092</td>
<td>Plasmepsin inhibitor</td>
</tr>
</tbody>
</table>
<p>from the latent space, and the success depends on the target context. We further selected around 1000 CogMol-generated molecules for each target as explained in Supp. Mat. C. The density plots in Figure 2 of the selected set indicate that generating high-affinity ligands is more challenging for NSP9 (Figure 2a), while $\mathrm{M}^{\text {pro }}$ ligands are more selective in general (Figure 2b), which is likely due to relative novelty of the target sequences with respect to BindingDB training sequences (see Supp. Mat. A). The QED distribution also highlights target sequence dependence of the generated molecules, as the molecules targeting RBD show a peak at a lower QED value in the distribution. Several randomly chosen samples for each SARS-CoV-2 target are shown in Supp. Mat. Figure D.1. Novelty. The novelty distributions, as estimated using the Tanimoto Similarity [47] between molecular fingerprints, of the generated molecules with respect to both the PubChem [48] database and our training set are shown in Supp. Mat. Figures E. 1 and E.2. When compared with the training database of size $\sim 1.9 \mathrm{M}$, we find that the likelihood of generating molecules with a novelty value of 0 is $\leq 2 \%$. With respect to the larger PubChem database consisting of $\sim 103 \mathrm{M}$ molecules, the majority of which were not included in model training, we find the percentage of generated molecules with novelty value of 0 is $9.5 \%, 3.7 \%$, and $8.3 \%$ generated molecules for $\mathrm{M}^{\text {pro }}$, RBD, and NSP9, respectively. Higher FCD of those generated molecules with respect to test scaffolds in ZINC/BindingDB (Supp. Mat. Table E.1) further confirms presence of novel chemical scaffolds in them. CogMol Identifies PubChem Molecules with Potential Anti-COVID Activity. Only 19, 5, and 15 of the generated molecules match exactly with an existing SMILES string in PubChem, for $\mathrm{M}^{\text {pro }}$, RBD, and NSP9, respectively. Some of these SMILES are reported with biological activity in PubChem, as shown in Table 2, which calls for further investigation. For example, the molecule with PubChem Compound ID (CID) 76332092 is a known Plasmepsin-2 and Plasmepsin-4 inhibitor and has also shown antimalarial activity against chloroquine-sensitive Plasmodium falciparum. As</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Density plots of (a) binding affinity, (b) off-target selectivity, (c) QED for selected molecules.</p>
<p>Table 3: Docking analysis: Size, average $(E)$ ( $\pm$ standard deviation) binding free energy (BFE), minimum BFE, fraction of generated molecules with $\mathrm{BFE}&lt;-6 \mathrm{kcal} / \mathrm{mol}$ for each cluster. In parentheses after target name: $\%$ of generated molecules for the respective target that had a $\mathrm{BFE}&lt;-6$ $\mathrm{kcal} / \mathrm{mol}$. Only top 2 clusters are shown (see Supp. Mat. H.1).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Target</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Size (\%)</th>
<th style="text-align: center;">$E(\mathrm{kcal} / \mathrm{mol})$</th>
<th style="text-align: center;">Min (kcal/mol)</th>
<th style="text-align: center;">Low Energy (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">NSP9 Dimer (87\%)</td>
<td style="text-align: center;">cluster 0</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">$-6.8 \pm 0.7$</td>
<td style="text-align: center;">$-8.6$</td>
<td style="text-align: center;">88</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">cluster 1</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">$-6.9 \pm 0.9$</td>
<td style="text-align: center;">$-8.8$</td>
<td style="text-align: center;">85</td>
</tr>
<tr>
<td style="text-align: center;">Main Protease (91\%)</td>
<td style="text-align: center;">cluster 0</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">$-7.2 \pm 0.8$</td>
<td style="text-align: center;">$-9.5$</td>
<td style="text-align: center;">93</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">cluster 1</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">$-6.9 \pm 0.8$</td>
<td style="text-align: center;">$-9.2$</td>
<td style="text-align: center;">86</td>
</tr>
<tr>
<td style="text-align: center;">RBD (95\%)</td>
<td style="text-align: center;">cluster 0</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">$-6.9 \pm 0.6$</td>
<td style="text-align: center;">$-8.3$</td>
<td style="text-align: center;">93</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">cluster 1</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">$-7.2 \pm 0.6$</td>
<td style="text-align: center;">$-9.1$</td>
<td style="text-align: center;">97</td>
</tr>
</tbody>
</table>
<p>RBD from S protein binding to angiotensin-converting enzyme-2 (ACE-2) receptor is needed for the viral entry to the host cells [49], both RBD and ACE-2 receptor are being actively investigated as COVID-19 targets. Chloroquine (and its hydroxy derivative) is a known ACE-2 inhibitor and has been already considered as a promising COVID-19 drug [50]. CID 76332092 deserves further investigation in the context of SARS-CoV-2 as it shows a predicted pIC50 of 7.82 and lowest docking binding free energy (BFE) of $-6.80 \mathrm{kcal} / \mathrm{mol}$ (Figure H.3) to the ACE-2 binding pocket of RBD (Table 2). The generated molecule with highest predicted affinity for RBD (with a pIC50 of 10.49 and a docking BFE of $-6.9 \mathrm{kcal} / \mathrm{mol}$ in the top binding mode with ACE-2 binding pocket) also shares a strong maximum common subgraph similarity [51] with Telavancin, an approved skin infection and Pneumonia drug, as shown in Figure E.3. These results indicate that CogMol can generate promising and biologically relevant drug candidates beyond the training dataset.
Docking with Target Structure. Table 3 summarizes these results. In the best (lowest BFE) docking pose, $87 \%, 91 \%$, and $95 \%$ of generated molecules show a minimum BFE of $&lt;-6 \mathrm{kcal} / \mathrm{mol}$ for NSP9 dimer, $\mathrm{M}^{\text {pro }}$, and RBD, respectively. For each target, we classified each molecule by its binding location, fitting the geometric centers of docked molecules drawn from a larger set of 875 K samples to a mixture of 4,5 , and 6 Gaussian models, respectively (see Supp. Mat. H). We also report the average and minimum BFE, as well as the fraction of generated molecules with a BFE of $&lt;-6 \mathrm{kcal} / \mathrm{mol}$ for each cluster (Table 3). Results show that even though CLaSS used only target sequence information for controlled generation, generated molecules do identify the relevant and known druggable binding pockets within the 3D target structure and bind to those favorably.</p>
<h1>5.3 CogMol-generated Molecules Targeting Human HDAC1</h1>
<p>Human HDAC1 plays key role in eukaryotic gene expression and is implicated in cancer. Though it is present in BindingDB, there are only a handful of molecules with high QED and high pIC50, see Table K.1. We applied CogMol to generate optimal ligands targeting HDAC1. Table K. 1 shows that CogMol-generated molecules comprise a larger proportion of molecules satisfying high pIC50 and QED criteria, implying CogMol can discover novel and optimal molecules even in a low-data regime.</p>
<h3>5.4 Synthesizability and Toxicity of Generated Molecules</h3>
<p>The number of steps/reactions needed to complete the synthesis (synthetic design) provides an estimate of the complexity of the molecules respect to commercially available materials (more details and the parameters adopted, can be found in the Supp. Mat. I). In Figure 3a (legend), we report the percentage of feasibility for 4 sets of generated molecules, each targeting a different protein NSP9, RBD, $\mathrm{M}^{\text {pro }}$, and HDAC1. For SARS-CoV-2 targets, molecules were selected by considering the top-100 molecules based on SA. For HDAC1, calculation was done on a set of 100 generated molecules. We also estimated feasibility for a selection of FDA-approved and commercially available drugs [52] (FDA), used as a baseline. For the FDA set, the fraction of molecules predicted as feasible is $\sim 78 \%$. This is expected, since most of these molecules have been protected with patents and are therefore chemically accessible with the reaction knowledge available in patents. The molecules generated by CogMol for the three COVID-19 targets perform better than the FDA with successful rates $&gt;85-90 \%$. The HDAC1 set instead shows a value of $\sim 67 \%$. The higher success rate for the COVID target sets indicates that the molecules in these classes are easier to synthesize from commercially available materials than the molecules belonging to the FDA class. The HDAC1 set,</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: (a) Bar plots describe the percentage of molecules synthesizable for the exact number of retrosynthesis steps. Legend reports the fraction of molecules for each set marked as synthetically accessible. (b) Percentage of parent molecules or their metabolites as a function of number of endpoints in which predicted to be toxic.
while still showing a relatively high synthesizability rate, demonstrates additional need of chemical knowledge uncovered in patents. The distribution of the number of steps needed for each set reveals two interesting observations, as shown in Figure 3a: FDA, $\mathrm{M}^{\text {pro }}$ and NSP9 molecules show a peak at 1 step and HDAC1 and RBD showing a peak respectively at 2 and 3 steps. In comparison, $&gt;$ $80 \%$ of the successfully synthesizable FDA approved molecules can be made in a single step from commercially available molecules, likely because their precursors are also made commercially available after approval. The $\mathrm{M}^{\text {pro }}$ and NSP9 sets are similar to the FDA approved drugs. They are characterized by a lower degree of complexity, indicating close relation to commercially available molecules, when compared to HDAC1 and RBD. Overall, the retrosynthetic analysis of the generative model outcomes clearly shows that the generated structures are chemically relevant and synthetically feasible. Additional results revealing correlations between number of synthesis steps and properties of molecules for the COVID-19 targets can be found in Supp. Mat. I.
Figure 3b shows toxicity analyses of the CogMol-generated molecules, their metabolites, as well as of FDA-approved drugs by using the MT-DNN model. A molecule was considered toxic if it was predicted to be toxic in $\geq 2$ endpoints. The metabolites were predicted by using a recently proposed work that models the human metabolite prediction task of small molecules as a sequence translation problem and uses a Seq2Seq Transformer model originally pre-trained on chemical reaction data and further fine-tuned on metabolite reaction data to predict the outcome of human metabolic reactions [53]. Results in Figure 3b show that majority ( $\sim 70 \%$ ) of the generated molecules, as well as their predicted metabolites ( $\sim 80 \%$ of them) are predicted toxic only in $0-1$ endpoints out of a total of 13, which is comparable to the FDA-approved drugs.</p>
<h1>5.5 Sharing and Visualization of Generated Molecules</h1>
<p>We share around $\sim 3500$ generated molecules under an open license for the research community to download and evaluate. In order to help domain experts, we also created a publicly available Molecule Explorer tool to facilitate screening and filtering of the molecules, perform novelty analysis, and identify closest molecules in PubChem. A screen cast of the Molecule Explorer tool is provided. ${ }^{1}$</p>
<h2>6 Conclusions and Future Work</h2>
<p>In this paper, we proposed CogMol, a framework for Controlled Generation of Molecules with a set of desired attributes. Our framework can handle targeted and novel compound generation for multiple proteins using the same trained model, can generalize to unseen viral proteins, and accounts explicitly for off-target selectivity. Additionally, we provide an in silico screening method that accounts for target structure binding, in vitro and clinical toxicity of parent molecules and their metabolites, and synthesis feasibility. When applied to COVID-19 novel viral protein sequences, CogMol generated novel molecules that were able to bind favorably to the relevant druggable pockets of the target structure. The generated compounds are also comparable to FDA-approved drugs in parent molecule, metabolite toxicity, and synthetic feasibility. In summary, our framework provides an efficient and</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>viable computational framework for de novo multi-objective design and filtering of optimal drug compounds that are specific and selective to novel/unseen targets. Future work will address accounting for additional contexts (on top of target protein), adding other pharmacologically relevant controls, and also weigh those according to their relative importance to make CogMol framework more efficient in term of generating promising drug candidates.</p>
<h1>7 Statement of Broader Impact</h1>
<p>We discuss the broader impact of our work from the following perspectives.
Benefits To date, SARS-CoV-2 has infected millions and killed hundreds of thousands around the globe and continues to cause a severe economical crisis [54]. No approved therapeutics is available against any coronavirus [55], no SARS-CoV-2 vaccine has successfully completed phase II human trails [56], and drugs for repurposing are still undergoing investigation [57]. Therefore, it is timely to explore for efficient de novo drug design approaches to combat COVID-19 and future pandemics. The CogMol framework is adapative, generic, and could pave the road for accelerated discovery of new antivirals optimized against specific SARS-CoV-2 (or other novel virus) targets. This could have a major impact on our global effort against COVID-19 and future novel pandemics and save human lives.</p>
<p>We demonstrated that our framework can generate target-specific and selective compounds for unseen protein targets, a novel property that may be key for swift reactions to possible SARS-CoV-2 mutants. We further provide early assessment of novel AI-generated compounds on target structure binding, and synthetic feasibility and toxicity in the context of FDA-approved drugs, in order to identify a list of promising compounds that is of reasonable size and can be immediately sent to wet lab for synthesis and validation. We showed the efficiency of the framework in terms of handling multiple constraints at once and can be easily extended to adding more controls to account for additional factors considered crucial in drug discovery such as ADME properties. Thus, our approach systematically bridges biology and machine learning to accelerate drug discovery.
We further share with the community a list of CogMol-generated compounds (and their attributes) designed for three novel SARS-Cov-2 targets, as well as a molecular explorer tool to visualize, experience, and provide feedback on these molecules. This sets our vision for an open community of discovery that facilitates interactions between AI researchers and medicinal scientists.
Risks and the Potential to Cause Harm While our approach offers enormous potential to speed up the development of new drugs, it must be realized that drug candidate generation and in silico screening are merely first steps in the development of viable therapeutics. The ability of the public to order these novel compounds online, poses a risk that it might be tried by people who are not sufficiently educated about the dangers of exposing themselves to these molecules in an uncontrolled setting. The public must be educated to not to treat these candidates as approved drugs or miracle cures. Further, since our framework allows generation of molecules satisfying arbitrary objectives, this capability can be misused by bad actors to design potentially harmful chemicals.
Consequences of Failure It is possible that our framework will not be able to generate molecules with desired properties either because of bias in training data or because of the inaccuracy of the predictors used for controlled generation. In this case, the properties of these molecules should be independently validated by using multiple independent mechanisms.</p>
<h1>References</h1>
<p>[1] H. Matthews, J. Hanison, and N. Nirmalan, ""Omics"-informed drug and biomarker discovery: opportunities, challenges and future perspectives," Proteomes, vol. 4, no. 3, p. 28, 2016.
[2] P. G. Polishchuk, T. I. Madzhidov, and A. Varnek, "Estimation of the size of drug-like chemical space based on GDB-17 data," Journal of Computer-Aided Molecular Design, vol. 27, no. 8, pp. 675-679, 2013.
[3] R. Gómez-Bombarelli, J. N. Wei, D. Duvenaud, J. M. Hernández-Lobato, B. Sánchez-Lengeling, D. Sheberla, J. Aguilera-Iparraguirre, T. D. Hirzel, R. P. Adams, and A. Aspuru-Guzik, "Automatic chemical design using a data-driven continuous representation of molecules," ACS Central Science, vol. 4, no. 2, pp. 268-276, 2018.
[4] A. Zhavoronkov, Y. A. Ivanenkov, A. Aliper, M. S. Veselov, V. A. Aladinskiy, A. V. Aladinskaya, V. A. . Terentiev, D. A. Polykovskiy, M. D. Kuznetsov, A. Asadulaev, Y. Volkov, A. Zholus, R. R. Shayakhmetov, A. Zhebrak, L. I. Minaeva, B. A. Zagribelnyy, L. H. Lee, R. Soll, D. Madge, L. Xing, G. Tao, and A. Aspuru-Guzik, "Deep learning enables rapid identification of potent DDR1 kinase inhibitors," Nature Biotechnology, vol. 37, pp. 1038—1040, 2019.
[5] Z. Zhou, S. Kearnes, L. Li, R. N. Zare, and P. Riley, "Optimization of molecules via deep reinforcement learning," Scientific Reports, vol. 9, no. 1, p. 10752, 2019.
[6] A. Peón, S. Naulaerts, and P. J. Ballester, "Predicting the reliability of drug-target interaction predictions with maximum coverage of target space," Scientific reports, vol. 7, no. 1, pp. 1-11, 2017.
[7] F. Cheng, I. A. Kovács, and A.-L. Barabási, "Network-based prediction of drug combinations," Nature communications, vol. 10, no. 1, pp. 1-11, 2019.
[8] F. Miljković and J. Bajorath, "Data-driven exploration of selectivity and off-target activities of designated chemical probes," Molecules, vol. 23, no. 10, p. 2434, 2018.
[9] E. C. Alley, G. Khimulya, S. Biswas, M. AlQuraishi, and G. M. Church, "Unified rational protein engineering with sequence-based deep representation learning," Nature Methods, vol. 16, p. 1315-1322, Oct 2019.
[10] R. Rao, N. Bhattacharya, N. Thomas, Y. Duan, P. Chen, J. Canny, P. Abbeel, and Y. Song, "Evaluating protein transfer learning with TAPE," in Advances in Neural Information Processing Systems, pp. 9686-9698, 2019.
[11] M. H. Segler, T. Kogej, C. Tyrchan, and M. P. Waller, "Generating focused molecule libraries for drug discovery with recurrent neural networks," ACS Central Science, vol. 4, no. 1, pp. 120-131, 2017.
[12] A. Gupta, A. T. Müller, B. J. Huisman, J. A. Fuchs, P. Schneider, and G. Schneider, "Generative recurrent networks for de novo drug design," Molecular Informatics, vol. 37, no. 1-2, p. 1700111, 2018.
[13] T. Blaschke, M. Olivecrona, O. Engkvist, J. Bajorath, and H. Chen, "Application of generative autoencoder in de novo molecular design," Molecular Informatics, vol. 37, no. 1-2, p. 1700123, 2018.
[14] S. Kang and K. Cho, "Conditional molecular design with deep generative models," Journal of Chemical Information and Modeling, vol. 59, no. 1, pp. 43-52, 2018.
[15] J. Lim, S. Ryu, J. W. Kim, and W. Y. Kim, "Molecular generative model based on conditional variational autoencoder for de novo molecular design," Journal of Cheminformatics, vol. 10, no. 1, p. 31, 2018.
[16] G. L. Guimaraes, B. Sanchez-Lengeling, C. Outeiral, P. L. C. Farias, and A. Aspuru-Guzik, "Objective-reinforced generative adversarial networks (ORGAN) for sequence generation models," arXiv preprint arXiv:1705.10843, 2017.</p>
<p>[17] N. De Cao and T. Kipf, "MolGAN: An implicit generative model for small molecular graphs," arXiv preprint arXiv:1805.11973, 2018.
[18] M. J. Kusner, B. Paige, and J. M. Hernández-Lobato, "Grammar variational autoencoder," in Proceedings of the 34th International Conference on Machine Learning, pp. 1945-1954, JMLR.org, 2017.
[19] H. Dai, Y. Tian, B. Dai, S. Skiena, and L. Song, "Syntax-directed variational autoencoder for structured data," arXiv preprint arXiv:1802.08786, 2018.
[20] Y. Li, O. Vinyals, C. Dyer, R. Pascanu, and P. Battaglia, "Learning deep generative models of graphs," arXiv preprint arXiv:1803.03324, 2018.
[21] Y. Li, L. Zhang, and Z. Liu, "Multi-objective de novo drug design with conditional graph generative model," Journal of Cheminformatics, vol. 10, no. 1, p. 33, 2018.
[22] M. Simonovsky and N. Komodakis, "GraphVAE: Towards generation of small graphs using variational autoencoders," in International Conference on Artificial Neural Networks, pp. 412422, Springer, 2018.
[23] B. Samanta, D. Abir, G. Jana, P. K. Chattaraj, N. Ganguly, and M. G. Rodriguez, "NeVAE: a deep generative model for molecular graphs," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, pp. 1110-1117, 2019.
[24] T. Ma, J. Chen, and C. Xiao, "Constrained generation of semantically valid graphs via regularizing variational autoencoders," in Advances in Neural Information Processing Systems, pp. 7113-7124, 2018.
[25] H. Kajino, "Molecular hypergraph grammar with its application to molecular optimization," arXiv preprint arXiv:1809.02745, 2018.
[26] W. Jin, R. Barzilay, and T. Jaakkola, "Junction tree variational autoencoder for molecular graph generation," arXiv preprint arXiv:1802.04364, 2018.
[27] M. Popova, O. Isayev, and A. Tropsha, "Deep reinforcement learning for de novo drug design," Science Advances, vol. 4, no. 7, p. eaap7885, 2018.
[28] M. Olivecrona, T. Blaschke, O. Engkvist, and H. Chen, "Molecular de-novo design through deep reinforcement learning," Journal of Cheminformatics, vol. 9, no. 1, p. 48, 2017.
[29] N. Jaques, S. Gu, D. Bahdanau, J. M. Hernández-Lobato, R. E. Turner, and D. Eck, "Sequence tutor: Conservative fine-tuning of sequence generation models with KL-control," in Proceedings of the 34th International Conference on Machine Learning, pp. 1645-1654, JMLR.org, 2017.
[30] E. Putin, A. Asadulaev, Y. Ivanenkov, V. Aladinskiy, B. Sanchez-Lengeling, A. Aspuru-Guzik, and A. Zhavoronkov, "Reinforced adversarial neural computer for de novo molecular design," Journal of Chemical Information and Modeling, vol. 58, no. 6, pp. 1194-1204, 2018.
[31] J. Born, M. Manica, A. Oskooei, J. Cadow, and M. R. Martínez, "PaccMann ${ }^{\mathrm{RL}}$ : Designing anticancer drugs from transcriptomic data via reinforcement learning," in International Conference on Research in Computational Molecular Biology, pp. 231-233, Springer, 2020.
[32] P. Das, T. Sercu, K. Wadhawan, I. Padhi, S. Gehrmann, F. Cipcigan, V. Chenthamarakshan, H. Strobelt, C. d. Santos, P.-Y. Chen, et al., "Accelerating antimicrobial discovery with controllable deep generative models and molecular dynamics," arXiv preprint arXiv:2005.11248, 2020.
[33] M. Skalic, D. Sabbadin, B. Sattarov, S. Sciabola, and G. De Fabritiis, "From target to drug: Generative modeling for the multimodal structure-based ligand design," Molecular pharmaceutics, vol. 16, no. 10, pp. 4282-4291, 2019.
[34] D. A. Grechishnikova, "Transformer neural network for protein specific de novo drug generation as machine translation problem," BioRxiv, p. 863415, 2019.</p>
<p>[35] D. Polykovskiy, A. Zhebrak, B. Sanchez-Lengeling, S. Golovanov, O. Tatanov, S. Belyaev, R. Kurbanov, A. Artamonov, V. Aladinskiy, M. Veselov, A. Kadurin, S. Nikolenko, A. AspuruGuzik, and A. Zhavoronkov, "Molecular sets (MOSES): A benchmarking platform for molecular generation models," arXiv preprint arXiv:1811.12823, 2018.
[36] J. J. Irwin and B. K. Shoichet, "ZINC-a free database of commercially available compounds for virtual screening," Journal of Chemical Information and Modeling, vol. 45, no. 1, pp. 177-182, 2005.
[37] M. K. Gilson, T. Liu, M. Baitaluk, G. Nicola, L. Hwang, and J. Chong, "BindingDB in 2015: a public database for medicinal chemistry, computational chemistry and systems pharmacology," Nucleic Acids Research, vol. 44, no. D1, pp. D1045-D1053, 2015.
[38] M. Karimi, D. Wu, Z. Wang, and Y. Shen, "DeepAffinity: interpretable deep learning of compound-protein affinity through unified recurrent and convolutional neural networks," arXiv preprint arXiv:1806.07537, 2018.
[39] D. P. Kingma and M. Welling, "Auto-encoding variational Bayes," arXiv preprint arXiv:1312.6114, 2013.
[40] N. Bosc, C. Meyer, and P. Bonnet, "The use of novel selectivity metrics in kinase research," BMC Bioinformatics, vol. 18, no. 1, p. 17, 2017.
[41] R. Huang, M. Xia, D.-T. Nguyen, T. Zhao, S. Sakamuru, J. Zhao, S. A. Shahane, A. Rossoshek, and A. Simeonov, "Tox21 challenge to build predictive models of nuclear receptor and stress response pathways as mediated by exposure to environmental chemicals and drugs," Frontiers in Environmental Science, vol. 3, p. 85, 2016.
[42] Z. Wu, B. Ramsundar, E. N. Feinberg, J. Gomes, C. Geniesse, A. S. Pappu, K. Leswing, and V. Pande, "MoleculeNet: a benchmark for molecular machine learning," Chemical Science, vol. 9, no. 2, pp. 513-530, 2018.
[43] O. Trott and A. J. Olson, "AutoDock Vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading," Journal of Computational Chemistry, vol. 31, no. 2, pp. 455-461, 2010.
[44] P. Schwaller, R. Petraglia, V. Zullo, V. H. Nair, R. A. Haeuselmann, R. Pisoni, C. Bekas, A. Iuliano, and T. Laino, "Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy," Chemical Science, vol. 11, no. 12, pp. 3316-3325, 2020.
[45] P. Schwaller, T. Laino, T. Gaudin, P. Bolgar, C. A. Hunter, C. Bekas, and A. A. Lee, "Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction," ACS Central Science, vol. 5, no. 9, pp. 1572-1583, 2019.
[46] K. Preuer, P. Renz, T. Unterthiner, S. Hochreiter, and G. Klambauer, "Fréchet ChemNet distance: a metric for generative models for molecules in drug discovery," Journal of Chemical Information and Modeling, vol. 58, no. 9, pp. 1736-1741, 2018.
[47] P. Willett, J. M. Barnard, and G. M. Downs, "Chemical similarity searching," Journal of Chemical Information and Computer Sciences, vol. 38, no. 6, pp. 983-996, 1998.
[48] S. Kim, J. Chen, T. Cheng, A. Gindulyte, J. He, S. He, Q. Li, B. A. Shoemaker, P. A. Thiessen, B. Yu, L. Zaslavsky, J. Zhang, and E. E. Bolton, "PubChem 2019 update: improved access to chemical data," Nucleic Acids Research, vol. 47, pp. D1102-D1109, 102018.
[49] M. Hoffmann, H. Kleine-Weber, S. Schroeder, N. Krüger, T. Herrler, S. Erichsen, T. S. Schiergens, G. Herrler, N.-H. Wu, A. Nitsche, et al., "SARS-CoV-2 cell entry depends on ACE2 and TMPRSS2 and is blocked by a clinically proven protease inhibitor," Cell, 2020.
[50] J. Liu, R. Cao, M. Xu, X. Wang, H. Zhang, H. Hu, Y. Li, Z. Hu, W. Zhong, and M. Wang, "Hydroxychloroquine, a less toxic derivative of chloroquine, is effective in inhibiting SARS-CoV-2 infection in vitro," Cell Discovery, vol. 6, no. 1, pp. 1-4, 2020.</p>
<p>[51] Y. Cao, T. Jiang, and T. Girke, "A maximum common substructure-based algorithm for searching and predicting drug-like compounds," Bioinformatics, vol. 24, no. 13, pp. i366-i374, 2008.
[52] EnamineStore, FDA approved Drugs, 2020 (accessed May, 2020).
[53] E. Litsa, P. Das, and L. Kavraki, "Prediction of drug metabolites using neural machine translation," May 2020.
[54] N. Fernandes, "Economic effects of coronavirus outbreak (COVID-19) on the world economy," Available at SSRN 3557504, 2020.
[55] M. L. Agostini, E. L. Andres, A. C. Sims, R. L. Graham, T. P. Sheahan, X. Lu, E. C. Smith, J. B. Case, J. Y. Feng, R. Jordan, et al., "Coronavirus susceptibility to the antiviral remdesivir (GS-5734) is mediated by the viral polymerase and the proofreading exoribonuclease," MBio, vol. 9, no. 2, pp. e00221-18, 2018.
[56] T. T. Le, Z. Andreadakis, A. Kumar, R. G. Roman, S. Tollefsen, M. Saville, and S. Mayhew, "The COVID-19 vaccine development landscape," Nat Rev Drug Discov, vol. 19, no. 5, pp. 305-6, 2020.
[57] Y. Wang, D. Zhang, G. Du, R. Du, J. Zhao, Y. Jin, S. Fu, L. Gao, Z. Cheng, Q. Lu, et al., "Remdesivir in adults with severe COVID-19: a randomised, double-blind, placebo-controlled, multicentre trial," The Lancet, 2020.
[58] J. L. Durant, B. A. Leland, D. R. Henry, and J. G. Nourse, "Reoptimization of MDL keys for use in drug discovery," Journal of Chemical Information and Computer Sciences, vol. 42, no. 6, pp. 1273-1280, 2002.
[59] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, "Dropout: a simple way to prevent neural networks from overfitting," The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929-1958, 2014.
[60] D. Rogers and M. Hahn, "Extended-connectivity fingerprints," Journal of Chemical Information and Modeling, vol. 50, no. 5, pp. 742-754, 2010.
[61] S. Liu, M. F. Demirel, and Y. Liang, "N-gram graph: Simple unsupervised representation for graphs, with applications to molecules," in Advances in Neural Information Processing Systems, pp. 8464-8476, 2019.
[62] L. Zhang, D. Lin, X. Sun, U. Curth, C. Drosten, L. Sauerhering, S. Becker, K. Rox, and R. Hilgenfeld, "Crystal structure of SARS-CoV-2 main protease provides a basis for design of improved $\alpha$-ketoamide inhibitors," Science, vol. 368, no. 6489, pp. 409-412, 2020.
[63] R. Krivák and D. Hoksza, "P2Rank: machine learning based tool for rapid and accurate prediction of ligand binding sites from protein structure," Journal of Cheminformatics, vol. 10, no. 1, p. 39, 2018.
[64] L. Jendele, R. Krivak, P. Skoda, M. Novotny, and D. Hoksza, "PrankWeb: a web server for ligand binding site prediction and visualization," Nucleic Acids Research, vol. 47, no. W1, pp. W345-W349, 2019.
[65] eMolecules, eMolecules Plus Database Download, 2020 (accessed May, 2020).</p>
<h1>Supplementary Materials</h1>
<h2>A Protein Targets Chosen for Generation</h2>
<p>Figure A. 1 shows the amino acid sequences corresponding to the three SARS-CoV-2 targets.
We also computed the similarity of these targets with respect to our training data from BindingDB using NCBI-BLAST tool ${ }^{2}$. The BLAST tools computes Expect value (E-value) - a measure of statistical significance of the match between the query sequence and database sequences. Larger E-value indicates a higher chance that the similarity between the hit (from the database) and the query is merely a coincidence, i.e. the query is not homologous or related to the hit. The lowest Expect value with respect to BindingDB protein sequences using the default parameters of BLAST are: $\mathrm{M}^{\text {pro }}$ $=0.51$ (query coverage $=40 \%$ ), RBD $=1.9$ (query coverage $=26 \%$ ), NSP9 $=3.2$ (query coverage $=$ $10 \%)$.</p>
<div class="codehilite"><pre><span></span><code>&gt; SARS-CoV-2 Main Protease
SGFRKMAFPSGKVEGCMVQVTCGTTTLNGLWLDDVVYCPRHVICTSEDMLNPNYEDLLIRKSNHNFLVQAGNVQLRVIGINSMQNCVLKLK
VDTANPKTPKYKFVRIQPGQTFSVLACYNGSPSGVYQCAMRPNFTIKGSFLNGSCGSVGFNIDYDCVSFCYMHHMELPTGVHAGTDLEGN
FYGPFVDRQTAQAAGTDTTITVNVLAWLYAAVINGDRWFLNRFTTTLNDFNLVAMKYNYEPLTQDHVDILGPLSAQTGIAVLDMCASLKE
LLQNGMNQRTILGSALLEDEFTPFDVVRQCSGVTFQ
&gt; SARS-COV-2 NSP9 Replicase
SNAMNNELSPVALRQMSCAAGTTQTACTDDNALAYYNTTKGGRFVLALLSDLQDLKWARFPKSDGTGTIYTELEPPCRFVTDTPKGPKVK
YLYFIKGLNNLNRGMVLGSLAATVRLQ
&gt; Receptor-Binding Domain (RBD) of SARS-COV-2 S protein
RVVPSGDVVRFPNITNLCPPGEVFNATKFPSVYAWERKKISNCVADYSVLYNSTFFSTFKCYGVSATKLNDLCFSNVYADSFVVKGDDVR
QIAPGQTGVIADYNYKLPDDFMGCVLAWNTRNIDATSTGNYNYKYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQ
PTNGVGYQPYRVVVLSFELLNAPATVCGPKLSTDLIK
</code></pre></div>

<p>Figure A.1: Sequences of SARS-CoV-2 targets</p>
<h2>B Variational Autoencoder</h2>
<p>We used a bidirectional Gated Recurrent Unit (GRU) with a linear output layer as an encoder. The decoder is a 3-layer GRU RNN of 512 hidden dimensions with intermediate dropout layers with dropout probability 0.2 . The performance characteristics of the VAE model with respect to various metrics are given below.</p>
<p>Table B.1: The performance metrics of the generative model: Fraction of valid molecules, Fraction of unique molecules from a sample of 1,000 and 10,000 molecules, Internal Diversity (IntDiv1 and IntDiv2), Fraction of molecules passing filters (MCF, PAINS, ring sizes, charges, atom types)</p>
<table>
<thead>
<tr>
<th style="text-align: right;">Model</th>
<th style="text-align: center;">Valid</th>
<th style="text-align: center;">Unique@1k</th>
<th style="text-align: center;">Unique@10k</th>
<th style="text-align: center;">IntDiv1</th>
<th style="text-align: center;">IntDiv2</th>
<th style="text-align: center;">Filters</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">ZINC (Unsupervised)</td>
<td style="text-align: center;">0.9553</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.9996</td>
<td style="text-align: center;">0.8568</td>
<td style="text-align: center;">0.8510</td>
<td style="text-align: center;">0.9889</td>
</tr>
<tr>
<td style="text-align: right;">ZINC (Supervised)</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.999</td>
<td style="text-align: center;">0.8578</td>
<td style="text-align: center;">0.8521</td>
<td style="text-align: center;">0.9888</td>
</tr>
<tr>
<td style="text-align: right;">BindingDB (Supervised)</td>
<td style="text-align: center;">0.904</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.9993</td>
<td style="text-align: center;">0.8717</td>
<td style="text-align: center;">0.8665</td>
<td style="text-align: center;">0.9482</td>
</tr>
<tr>
<td style="text-align: right;">CharRNN</td>
<td style="text-align: center;">0.809</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.855</td>
<td style="text-align: center;">0.849</td>
<td style="text-align: center;">0.975</td>
</tr>
<tr>
<td style="text-align: right;">AAE</td>
<td style="text-align: center;">0.997</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.995</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">0.997</td>
</tr>
<tr>
<td style="text-align: right;">VAE</td>
<td style="text-align: center;">0.969</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.999</td>
<td style="text-align: center;">0.856</td>
<td style="text-align: center;">0.851</td>
<td style="text-align: center;">0.996</td>
</tr>
<tr>
<td style="text-align: right;">JT-VAE</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.999</td>
<td style="text-align: center;">0.851</td>
<td style="text-align: center;">0.845</td>
<td style="text-align: center;">0.978</td>
</tr>
<tr>
<td style="text-align: right;">Training</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.851</td>
<td style="text-align: center;">1.0</td>
</tr>
</tbody>
</table>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table B.2: Performance evaluation of the generative model using scaffold split metrics: Fréchet ChemNet Distance (FCD), Similarity to the nearest neighbour (SNN), Fragment similarity (Frag), Scaffold similarity (Scaff). The model trained on BindingDB is evaluated on both BindingDB and ZINC Scaffolds.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>FCD</th>
<th></th>
<th>SNN</th>
<th></th>
<th>Frag</th>
<th></th>
<th>Scaff</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Test</td>
<td>TestSF</td>
<td>Test</td>
<td>TestSF</td>
<td>Test</td>
<td>TestSF</td>
<td>Test</td>
<td>TestSF</td>
</tr>
<tr>
<td>ZINC(Unsupervised)</td>
<td>0.166</td>
<td>0.603</td>
<td>0.560</td>
<td>0.533</td>
<td>0.999</td>
<td>0.997</td>
<td>0.905</td>
<td>0.128</td>
</tr>
<tr>
<td>ZINC(Supervised)</td>
<td>0.2051</td>
<td>0.6222</td>
<td>0.5526</td>
<td>0.5267</td>
<td>0.999</td>
<td>0.998</td>
<td>0.8907</td>
<td>0.1319</td>
</tr>
<tr>
<td>BindingDB (BindingDB Scaff.)</td>
<td>0.7322</td>
<td>9.535</td>
<td>0.4335</td>
<td>0.3732</td>
<td>0.998</td>
<td>0.8493</td>
<td>0.5382</td>
<td>0.0764</td>
</tr>
<tr>
<td>BindingDB (ZINC Scaff.)</td>
<td>7.3416</td>
<td>7.7179</td>
<td>0.4089</td>
<td>0.4002</td>
<td>0.9593</td>
<td>0.9576</td>
<td>0.3196</td>
<td>0.0869</td>
</tr>
<tr>
<td>CharRNN</td>
<td>0.355</td>
<td>0.899</td>
<td>0.536</td>
<td>0.514</td>
<td>0.999</td>
<td>0.996</td>
<td>0.882</td>
<td>0.14</td>
</tr>
<tr>
<td>AAE</td>
<td>0.395</td>
<td>1.0</td>
<td>0.62</td>
<td>0.575</td>
<td>0.995</td>
<td>0.994</td>
<td>0.866</td>
<td>0.1</td>
</tr>
<tr>
<td>VAE</td>
<td>0.084</td>
<td>0.541</td>
<td>0.623</td>
<td>0.677</td>
<td>1.0</td>
<td>0.998</td>
<td>0.993</td>
<td>0.062</td>
</tr>
<tr>
<td>JT-VAE</td>
<td>0.422</td>
<td>0.996</td>
<td>0.556</td>
<td>0.527</td>
<td>0.996</td>
<td>0.995</td>
<td>0.892</td>
<td>0.1</td>
</tr>
<tr>
<td>Training</td>
<td>0.008</td>
<td>0.476</td>
<td>0.642</td>
<td>0.586</td>
<td>1.0</td>
<td>0.999</td>
<td>0.991</td>
<td>0.0</td>
</tr>
</tbody>
</table>
<p>Scaffold Novelty Relative to Training Set
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure B.1: The novelty of the scaffold of each generated molecule compared to the most similar scaffold in the training set. 26 k molecules were generated and their scaffolds compared to the scaffolds of every molecule in the Zinc and BindingDB datasets. The results indicate that while many molecules have scaffolds which are present in the dataset (indicated by the spike at novelty $=0$ ), there are many molecules that contain scaffolds not at all present in the training data.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure B.2: Comparing example scaffolds in the 26 k generated molecules to the scaffolds within the training data. The most similar scaffold in the training dataset, as calculated by the Tanimoto Similarity of the fingerprints, is shown next to the scaffold of each generated molecule. These novel scaffolds were determined by a synthetic organic chemist to be synthetically plausible.</p>
<h1>C Selection of Molecules for Further Analysis</h1>
<p>We selected around 1000 molecules for each target based on binding affinity ( $p I C 50&gt;6$ ), QED ( $&gt;0.4$ ), Synthetic Accessibility ( $&lt;5$ ), number of toxic endpoints ( $&lt;2$ ), $\log \mathrm{P}(&lt;5$ ) and Mol. Wt $(&lt;500)$ for further analysis. The off-target selectivity was chosen to be higher than $1.15,0.75$ and 0.7 for Main Protease, RBD of S Protein and NSP9 Replicase respectively.</p>
<h2>D Random Examples of Generated Molecules</h2>
<p>We show a representative set of molecules generated for each target in Figure D. 1
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure D.1: Representative molecules generated for (top to bottom): NSP9 Replicase, ReceptorBinding Domain (RBD) of S protein, and Main Protease of SARS-CoV-2</p>
<h2>E Novelty Analysis</h2>
<p>To assess the novelty of generated molecules, we assigned to each molecule $m$ a score $\operatorname{Nov}<em _95_="_">{m}$ representing its minimal distance (maximal similarity) to all registered compounds $p$ in the training database or another reference database of known compounds $P: \operatorname{Nov} _m=1-\max _p \in P{\operatorname{sim}\left(k</em>$. Note that a novelty of 0 means that the molecule's fingerprint matches exactly the fingerprint of a compound in the reference database; however, the final structure of the generated molecule can still be different.
The distribution of novelty scores for each of the targets with respect to the training set and a larger set of molecules from PubChem is given in Figures E. 1 and E.2.
We further compute the FCD of the generated molecules (and scaffolds) for each target with respect to the ZINC and BindingDB datasets (See Table E.1. We note that novel scaffolds emerge in the generated molecules with respect to both ZINC and BindingDB.
It is also interesting to note that the CogMol generated molecule with the highest binding affinity to RBD has maximum subgraph similarity to a commercially available drug Telavancin (See Figure E.3). Telavancin is a semi-synthetic derivative of vancomycin. It is used to treat complicated skin and skin} m, k_{_} p\right)}$. MACCS keys [58] were used as structural fingerprints to determine the similarity for each pair of molecule $\left(k_{m}\right)$ and compound $\left(k_{p}\right)$. The Tanimoto [47] coefficient between two fingerprints expresses the similarity: $\operatorname{sim}\left(k_{x}, k_{y}\right)=\frac{\left|k_{x} \cap k_{y}\right|}{\left|k_{x} \cup k_{y}\right|</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure E.1: Novelty of the generated molecules for each target relative to the molecules in the training set, confirming that the model is indeed creating new molecules with novel fingerprints.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure E.2: Novelty relative to PubChem Dataset. Tends to be lower than novelty compared to all the training molecules, likely because there are significantly more molecules recorded in PubChem than the training subset.
structure infections, and hospital-acquired and ventilator-associated bacterial pneumonia caused by Staphylococcus aureus.</p>
<h1>F Property Predictors</h1>
<p>Property predictors for QED, $\log \mathrm{P}$ and SA were trained on the latent embeddings of the VAE. These regression models have 4 hidden layers with 50 units each and ReLU nonlinearity. We further train a binding affinity predictor using the latent embeddings of the VAE and pretrained protein embeddings [9]. The protein embeddings and the molecular embeddings are concatenated and passed through a single hidden layer with 2048 hidden units and ReLU nonlinearity. In order to build a predictor for binding affinity trained directly on the smiles sequences (x), we first embed them using</p>
<p>Table E.1: FCD of the generated molecules for each target</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Target</th>
<th style="text-align: center;">FCD/Test (ZINC)</th>
<th style="text-align: center;">FCD/Test (BindingDB)</th>
<th style="text-align: center;">FCD/TestSF (ZINC)</th>
<th style="text-align: center;">FCD/TestSF(BindingDB)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">NSP9</td>
<td style="text-align: center;">7.106</td>
<td style="text-align: center;">1.007</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">9.25</td>
</tr>
<tr>
<td style="text-align: center;">RBD</td>
<td style="text-align: center;">7.072</td>
<td style="text-align: center;">1.004</td>
<td style="text-align: center;">7.472</td>
<td style="text-align: center;">9.202</td>
</tr>
<tr>
<td style="text-align: center;">MPro</td>
<td style="text-align: center;">7.107</td>
<td style="text-align: center;">0.995</td>
<td style="text-align: center;">7.523</td>
<td style="text-align: center;">9.278</td>
</tr>
<tr>
<td style="text-align: center;">HDAC1</td>
<td style="text-align: center;">8.028</td>
<td style="text-align: center;">1.924</td>
<td style="text-align: center;">8.47</td>
<td style="text-align: center;">10.167</td>
</tr>
</tbody>
</table>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure E.3: Maximum Common Subgraph Similarity of the CogMol-generated molecule with highest binding affinity to RBD (left) and Telavancin (right)</p>
<p>LSTMs. When Protein sequences are embedded using LSTM, it serve as a baseline, and we get a RMSE of 1.0104 on test data. Our best model on Binding Affinity, with RMSE of 0.8426, uses pre-trained protein embeddings from [9].</p>
<p>Table F.1: Performance of the attribute predictors for QED, $\log \mathrm{P}, \mathrm{SA}$, and binding affinities. Binding affinity $(z)$ is trained on the latent space of the VAE, while binding affinity ( x ) is trained on the actual SMILES sequences.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Attribute</th>
<th style="text-align: center;">RMSE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Binding Affinity (x) Baseline</td>
<td style="text-align: center;">1.0104</td>
</tr>
<tr>
<td style="text-align: center;">QED</td>
<td style="text-align: center;">0.0281</td>
</tr>
<tr>
<td style="text-align: center;">$\log \mathrm{P}$</td>
<td style="text-align: center;">0.3307</td>
</tr>
<tr>
<td style="text-align: center;">SA</td>
<td style="text-align: center;">0.0973</td>
</tr>
<tr>
<td style="text-align: center;">Binding Affinity (z)</td>
<td style="text-align: center;">1.2820</td>
</tr>
<tr>
<td style="text-align: center;">Binding Affinity (x)</td>
<td style="text-align: center;">0.8426</td>
</tr>
</tbody>
</table>
<h1>G Toxicity Prediction Model</h1>
<p>The MT-DNN model contains a total of four hidden layers: two are shared across all toxicity endpoints and two are private for each of the endpoints. We used a dropout [59] probability of 0.5 , and a ReLU activation function for all layers except for the last layers, in which the sigmoid activation was used. Morgan Fingerprints [60] were used as the input features to the model.</p>
<p>The ROC AUC, Accuracy (ACC), Balanced Accuracy (BAC), True Negative (TN), True Positive (TP), Precision (PR), Recall (RC), and the F1 score of the MT-DNN model on Tox21 and ClinTox test data are reported in Table G. 1 in the Appendix. Although the AUC values are slightly worse than the existing work of [61, see Table S14], the precision (and thus true positive rate) achieved by the MT-DNN is much higher. ${ }^{3}$ For comparison, we also report the results from a random forest (RF) model in Table G.2, showing that the MT-DNN significantly outperforms the RF model in terms of true positive rate, recall, and F1 score. Therefore, the MT-DNN model was used for assessing the generated molecules for toxicity.
Tables G. 1 and G. 2 show the performance of toxicity prediction using the MT-DNN and the random forest as the baseline. The MT-DNN significantly outperforms the RF model in terms of true positive rate, recall, and F1 score, while incurring a small penalty in ROC AUC and precision. Table G. 3 displays the proportion of molecules being predicted toxic in a number of endpoints. We can see that the predicted toxicity of the generated molecules in all three targets are similar to that of the FDA approved drugs.</p>
<p>Table G.1: Performance on toxicity prediction using MT-DNN for all 12 Tox21 tasks and ClinTox task (CT-TOX). The reported metrics are ROC AUC, accuracy, balanced accuracy, true negative rate, true positive rate, precision, recall, and the F1 score. Refer to Table G. 2 for a comparison with random forest, the MT-DNN achieves much better true positive, recall and F1 score with slight penalty on AUC.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">AUC</th>
<th style="text-align: center;">ACC</th>
<th style="text-align: center;">BAC</th>
<th style="text-align: center;">TN</th>
<th style="text-align: center;">TP</th>
<th style="text-align: center;">PR</th>
<th style="text-align: center;">RC</th>
<th style="text-align: center;">F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">NR-AR</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.55</td>
</tr>
<tr>
<td style="text-align: center;">NR-Aromatase</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.41</td>
</tr>
<tr>
<td style="text-align: center;">NR-PPAR- $\gamma$</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.31</td>
</tr>
<tr>
<td style="text-align: center;">SR-HSE</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.32</td>
</tr>
<tr>
<td style="text-align: center;">NR-AR-LBD</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.66</td>
</tr>
<tr>
<td style="text-align: center;">NR-ER</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.40</td>
</tr>
<tr>
<td style="text-align: center;">SR-ARE</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.47</td>
</tr>
<tr>
<td style="text-align: center;">SR-MMP</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.62</td>
</tr>
<tr>
<td style="text-align: center;">NR-AhR</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.55</td>
</tr>
<tr>
<td style="text-align: center;">NR-ER-LBD</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.48</td>
</tr>
<tr>
<td style="text-align: center;">SR-ATAD5</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.32</td>
</tr>
<tr>
<td style="text-align: center;">SR-p53</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.40</td>
</tr>
<tr>
<td style="text-align: center;">CT-TOX</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.31</td>
</tr>
<tr>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">$\mathbf{0 . 6 7}$</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">$\mathbf{0 . 3 6}$</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">$\mathbf{0 . 3 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 5}$</td>
</tr>
</tbody>
</table>
<p>Table G.2: Performance on toxicity prediction for Random Forest on all 12 Tox21 tasks and ClinTox task (CT-TOX). The reported metrics are ROC AUC, accuracy, balanced accuracy, true negative rate, true positive rate, precision, recall, and the F1 score.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">AUC</th>
<th style="text-align: center;">ACC</th>
<th style="text-align: center;">BAC</th>
<th style="text-align: center;">TN</th>
<th style="text-align: center;">TP</th>
<th style="text-align: center;">PR</th>
<th style="text-align: center;">RC</th>
<th style="text-align: center;">F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">NR-AR</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.60</td>
</tr>
<tr>
<td style="text-align: center;">NR-Aromatase</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.32</td>
</tr>
<tr>
<td style="text-align: center;">NR-PPAR-gamma</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.20</td>
</tr>
<tr>
<td style="text-align: center;">SR-HSE</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.22</td>
</tr>
<tr>
<td style="text-align: center;">NR-AR-LBD</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.66</td>
</tr>
<tr>
<td style="text-align: center;">NR-ER</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.32</td>
</tr>
<tr>
<td style="text-align: center;">SR-ARE</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.36</td>
</tr>
<tr>
<td style="text-align: center;">SR-MMP</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.49</td>
</tr>
<tr>
<td style="text-align: center;">NR-AhR</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.45</td>
</tr>
<tr>
<td style="text-align: center;">NR-ER-LBD</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.45</td>
</tr>
<tr>
<td style="text-align: center;">SR-ATAD5</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.18</td>
</tr>
<tr>
<td style="text-align: center;">SR-p53</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.22</td>
</tr>
<tr>
<td style="text-align: center;">CT-TOX</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.19</td>
</tr>
<tr>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">$\mathbf{0 . 8 1}$</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.36</td>
</tr>
</tbody>
</table>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table G.3: Proportion of molecules being predicted to have a number of toxic endpoints. There are 200k molecules for each of the targets NSP9, RBD, and Main Protease. While there are only 680 molecules in the FDA database. Note that there are negligible amount of molecules or metabolites having more than 10 toxic endpoints and thus they are omitted here.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Targets</th>
<th style="text-align: center;">0</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">6</th>
<th style="text-align: center;">7</th>
<th style="text-align: center;">8</th>
<th style="text-align: center;">9</th>
<th style="text-align: center;">10</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Molecules</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">NSP9</td>
<td style="text-align: center;">0.502</td>
<td style="text-align: center;">0.223</td>
<td style="text-align: center;">0.121</td>
<td style="text-align: center;">0.076</td>
<td style="text-align: center;">0.042</td>
<td style="text-align: center;">0.021</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.004</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">RBD</td>
<td style="text-align: center;">0.505</td>
<td style="text-align: center;">0.223</td>
<td style="text-align: center;">0.121</td>
<td style="text-align: center;">0.075</td>
<td style="text-align: center;">0.041</td>
<td style="text-align: center;">0.021</td>
<td style="text-align: center;">0.009</td>
<td style="text-align: center;">0.004</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Main Protease</td>
<td style="text-align: center;">0.507</td>
<td style="text-align: center;">0.221</td>
<td style="text-align: center;">0.123</td>
<td style="text-align: center;">0.073</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.021</td>
<td style="text-align: center;">0.009</td>
<td style="text-align: center;">0.004</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">FDA</td>
<td style="text-align: center;">0.569</td>
<td style="text-align: center;">0.157</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.056</td>
<td style="text-align: center;">0.038</td>
<td style="text-align: center;">0.034</td>
<td style="text-align: center;">0.022</td>
<td style="text-align: center;">0.013</td>
<td style="text-align: center;">0.006</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.001</td>
</tr>
<tr>
<td style="text-align: center;">Metabolites</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">NSP9</td>
<td style="text-align: center;">0.642</td>
<td style="text-align: center;">0.191</td>
<td style="text-align: center;">0.075</td>
<td style="text-align: center;">0.038</td>
<td style="text-align: center;">0.023</td>
<td style="text-align: center;">0.014</td>
<td style="text-align: center;">0.009</td>
<td style="text-align: center;">0.005</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">RBD</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.206</td>
<td style="text-align: center;">0.085</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.019</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">0.005</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Main Protease</td>
<td style="text-align: center;">0.652</td>
<td style="text-align: center;">0.192</td>
<td style="text-align: center;">0.071</td>
<td style="text-align: center;">0.038</td>
<td style="text-align: center;">0.021</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">0.008</td>
<td style="text-align: center;">0.004</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">FDA</td>
<td style="text-align: center;">0.656</td>
<td style="text-align: center;">0.138</td>
<td style="text-align: center;">0.068</td>
<td style="text-align: center;">0.045</td>
<td style="text-align: center;">0.037</td>
<td style="text-align: center;">0.025</td>
<td style="text-align: center;">0.013</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.004</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.001</td>
</tr>
</tbody>
</table>
<h1>H Docking Analysis</h1>
<p>First, we removed chiral molecules from consideration for docking, as handling of chiral molecules in silico and in wet lab is tricky. We performed docking simulations using AutoDock Vina [43] with exhaustiveness $=8$ and a search space encompassing the entire protein target. We used the best result from 5 independent runs. Using a large set of approximately 875,000 molecules (generated with only affinity constraints) for each target, we form clusters from the geometric centers of the top docking poses. We perform only 1 run of docking for these ligands. We use these cluster locations to approximate common binding sites for each target. We observe some correspondence with known binding pockets from literature - e.g., cluster 0 for $\mathrm{M}^{\text {pro }}$ corresponds closely to the substrate-binding pocket [62] — as well as with pockets identified with PrankWeb [63, 64] (see Table H.3).</p>
<p>Table H.1: Docking analysis of screened molecules: Size (\%), average ( $\pm$ standard deviation) binding free energy, minimum binding free energy, the fraction of generated molecules with binding free energy $&lt;-6 \mathrm{kcal} / \mathrm{mol}$ for each cluster. In parentheses after target name: \% of generated molecules that showed a binding free energy $&lt;-6 \mathrm{kcal} / \mathrm{mol}$. Table 3 shows a condensed version of this table.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Target</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Size (\%)</th>
<th style="text-align: center;">Mean (kcal/mol)</th>
<th style="text-align: center;">Min (kcal/mol)</th>
<th style="text-align: center;">Low Energy (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">NSP9 Dimer (87\%)</td>
<td style="text-align: center;">cluster 0</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">$-6.8 \pm 0.7$</td>
<td style="text-align: center;">$-8.6$</td>
<td style="text-align: center;">88</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">cluster 1</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">$-6.9 \pm 0.9$</td>
<td style="text-align: center;">$-8.8$</td>
<td style="text-align: center;">85</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">cluster 2</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">$-7.0 \pm 0.8$</td>
<td style="text-align: center;">$-8.8$</td>
<td style="text-align: center;">80</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">cluster 3</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$-6.5 \pm 0.9$</td>
<td style="text-align: center;">$-8.1$</td>
<td style="text-align: center;">73</td>
</tr>
<tr>
<td style="text-align: center;">Main Protease (91\%)</td>
<td style="text-align: center;">cluster 0</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">$-7.2 \pm 0.8$</td>
<td style="text-align: center;">$-9.5$</td>
<td style="text-align: center;">93</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">cluster 1</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">$-6.9 \pm 0.8$</td>
<td style="text-align: center;">$-9.2$</td>
<td style="text-align: center;">86</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">cluster 2</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$-7.0 \pm 0.5$</td>
<td style="text-align: center;">$-7.8$</td>
<td style="text-align: center;">94</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">cluster 3</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$-7.0 \pm 1.0$</td>
<td style="text-align: center;">$-8.4$</td>
<td style="text-align: center;">75</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">cluster 4</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$-6.8 \pm 1.3$</td>
<td style="text-align: center;">$-8.2$</td>
<td style="text-align: center;">67</td>
</tr>
<tr>
<td style="text-align: center;">RBD (95\%)</td>
<td style="text-align: center;">cluster 0</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">$-6.9 \pm 0.6$</td>
<td style="text-align: center;">$-8.3$</td>
<td style="text-align: center;">93</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">cluster 1</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">$-7.2 \pm 0.6$</td>
<td style="text-align: center;">$-9.1$</td>
<td style="text-align: center;">97</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">cluster 2</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">$-6.8 \pm 0.7$</td>
<td style="text-align: center;">$-8.3$</td>
<td style="text-align: center;">84</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">cluster 3</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">$-7.3 \pm 0.5$</td>
<td style="text-align: center;">$-9.1$</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">cluster 4</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$-6.9 \pm 0.7$</td>
<td style="text-align: center;">$-8.0$</td>
<td style="text-align: center;">86</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">cluster 5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$-6.8 \pm 0.4$</td>
<td style="text-align: center;">$-7.3$</td>
<td style="text-align: center;">100</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ The average precision from [61] over all 13 tasks is 0.45 , which was obtained by running their code available through Github.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
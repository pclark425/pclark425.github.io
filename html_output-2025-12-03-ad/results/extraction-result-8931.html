<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8931 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8931</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8931</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-258865576</p>
                <p><strong>Paper Title:</strong> <a href="https://aclanthology.org/2023.findings-emnlp.762.pdf" target="_blank">Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement</a></p>
                <p><strong>Paper Abstract:</strong> To enhance the multi-step reasoning capabilities of large language models, researchers have extensively explored prompting methods, notably the Chain-of-Thought (CoT) method which explicitly elicits human-like rationales. However, they have inadvertently overlooked the potential of enhancing model reasoning performance by formulating higher-quality problems. In this work, we start from the problem side and propose Self-Polish (SP), a novel method that facilitates the model's reasoning by guiding it to progressively refine the given problems to be more comprehensible and solvable. We also explore several automatic prompting varients and propose the Self-Polish prompt bank for the community. SP is orthogonal to all other prompting methods of answer/reasoning side like CoT, allowing for seamless integration with state-of-the-art techniques for further improvement. Thorough experiments show that the proposed method attains notable and consistent effectiveness on five reasoning benchmarks across different models. Furthermore, our method also showcases impressive performance on robustness evaluation. Codes and prompts are available at https://github.com/WooooDyy/Self-Polish.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8931.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8931.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Polish</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting-based, training-free method that has the model iteratively rewrite/clarify the input problem (problem-side refinement) to make it more concise, focused, and solvable, then solve the refined problem; can be used with CoT-style answer-side prompting or with self-consistency voting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002, text-davinci-003, GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-series models accessed via OpenAI API as reported in the paper; the paper does not provide parameter counts or training details for these models.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Polish (iterative problem refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The model is prompted to rewrite the original problem into a clearer, concise, well-focused formulation (via zero-shot instruction, in-context demonstrations, or automatic demonstration selection). The pipeline alternates problem-refining and problem-solving: generate a refined problem, generate an answer (optionally with Chain-of-Thought), then repeat refinement+answering until a stopping condition (two consecutive identical answers) or a maximum iteration count T (default T=2). Variants include zero-shot SP, in-context SP, Auto-SP (clustering-based demo selection), and Complex-SP (complexity-based demo selection).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple math/reasoning benchmarks (GSM8K, AQuA, SVAMP, MultiArith, MathQA, GSM-IC robustness splits)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Standard multi-step math and arithmetic reasoning benchmarks: GSM8K (grade school math), AQuA (algebraic QA), SVAMP (simple arithmetic with adversarial perturbations), MultiArith (arithmetic word problems), MathQA (math word problems); GSM-IC is an adversarial arithmetic dataset with distracting information used for robustness evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Paper reports consistent and notable improvements across five benchmarks and on robustness splits when applying Self-Polish. Example numeric results reported for MATH (random 200 samples, CoT answer-side): No refinement 21.0% → Zero-shot SP 23.5% → In-context SP 24.5% (percent accuracy). For other benchmarks, the paper reports qualitative/aggregate gains (figures and tables referenced) but specific per-benchmark numeric values are not reproduced in the main text excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline few-shot or CoT prompting without problem refinement (standard prompting / CoT / Auto-CoT / Complex-CoT) as used in prior work; example numeric baseline for MATH (same sample) is 21.0% accuracy (No refinement). For other datasets, explicit baseline numbers are not provided in the text excerpt but comparisons are reported in tables referenced by the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering / in-context learning: instructions and demonstrations to rewrite problems; automatic demo selection via k-means clustering (Auto-SP) or problem complexity (Complex-SP); iterative loop implemented by alternating a problem-refine prompt and an answer/reasoning prompt. Decoding used greedy when evaluating SP alone (temperature=0), and temperature > 0 (0.7) when combining with Self-Consistency to sample multiple reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>2</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Empirical evaluations across GSM8K, AQuA, SVAMP, MultiArith, MathQA show consistent improvements when applying SP; robustness evaluations on GSM-IC show improved resistance to distracting information; ablations show increasing max iterations T raises convergence accuracy up to a point; combining SP with reasoning-side methods (CoT, Auto-CoT, Complex-CoT) further increases accuracy. Quantitatively, on a 200-sample MATH subset, accuracy rose from 21.0% (no refinement) to 24.5% (in-context SP).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations include (1) convergence criterion is based on identical successive answers rather than assessment of problem optimality; (2) excessive refinement can alter semantics or drop important information and worsen performance (a failure case shown for MultiArith where second generated problem deviated semantically and caused incorrect answer); (3) designed refinement patterns do not cover all real-world scenarios; (4) automatic prompt construction explored but not exhaustive; (5) on some simple tasks (e.g., MultiArith combined with CoT), SP yields limited or negative improvements due to possible information loss from unnecessary rewriting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>SP is explicitly orthogonal to answer-side methods like Chain-of-Thought (CoT), Least-to-Most (LtM), Auto-CoT and Complex-CoT and can be combined with them; combining SP with these methods typically yields substantial additional gains. Complex-based prompting (Complex-SP) performed best among SP variants. On simpler tasks already well-handled by CoT, SP gives limited improvement or can harm performance if refinement is excessive.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Ablations explored max iteration T and final-answer selection strategy: larger T increases convergence accuracy ('Converge') but too large T can decrease overall performance (excessive rewriting harms quality); T_actual distribution shows most examples converge in few iterations (long tail beyond T); selection strategies: 'Last One' works better with small T, while with more iterations voting/majority strategies can outperform 'Last One'. The paper sets T=2 as a trade-off between compute and performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8931.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8931.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProgressiveRefine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Progressively Refining Framework (within Self-Polish)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative control loop used by Self-Polish that alternates problem refinement and problem solving until a return condition (converged answers or max iterations) is met, with multiple strategies to select the final answer if convergence is not reached.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002, text-davinci-003, GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-series models accessed via OpenAI API as reported in the paper; model internals/size not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Progressive problem-refine + solve iterations (convergence-based stopping)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Start by generating an answer to the original problem; then loop: (1) generate a refined version of the problem using the refine prompt; (2) answer the refined problem; (3) compare current answer to previous answer; stop and return when two consecutive answers match (convergence) or when iteration count exceeds T (default T=2). If exceeding max iterations, final-answer selection strategies include returning the original answer, the first generated answer, the last generated answer (default), or using majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same multi-step reasoning benchmarks as above; additionally used for robustness evaluation (GSM-IC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks for multi-step arithmetic/mathematical reasoning and adversarial robustness datasets with distracting information.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Progressive refining (adding the iterative loop) improves consistency and reliability of answers and brings further improvement across prompt-construction approaches (reported qualitatively; specific per-task numeric deltas not present in text except aggregate improvements described).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Single-step problem refining (i.e., rewrite once then answer) or baseline prompting without iterative convergence checking; paper reports iterative progressive refining adds further gains over single-step SP.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineered iterative loop implemented within the prompt+API call sequence (see Algorithm 1 in Appendix B); uses answer comparison to detect convergence. Greedy decoding used for stability in SP-only runs; sampling (temperature > 0) used when combining with self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>2</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Ablation: increasing max iterations T increases 'converge' accuracy metric; distribution plots show most examples converge in few iterations; using progressive refining on top of the several SP prompting approaches yields additional performance gains (paper reports this in tables/figures).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>If T is set too high, excessive rewriting can degrade problem quality and performance; the return condition only checks answer equality (not problem optimality); when convergence not reached, selection strategy matters and can change outcomes (the paper reports different strategies perform differently depending on T).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>The progressive refining loop is a control strategy layered on top of SP and is complementary to reasoning-side methods; the paper compared different final-answer selection rules and showed dependence on iteration count (e.g., 'Last One' better for small T, voting better for larger T).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Varying T: larger T increased convergence accuracy but too large T decreased overall performance; actual iteration counts T_actual are typically small (long-tail distribution). Final-answer selection strategies: 'Last One' favored with small T; other strategies (e.g., voting) can outperform when iteration count is larger.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8931.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8931.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously proposed answer-side method that samples multiple reasoning paths/chain-of-thoughts and aggregates answers (majority vote) to improve reasoning reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Selfconsistency improves chain of thought reasoning in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002, text-davinci-003, GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-series models accessed via OpenAI API as reported in the paper; no size details provided in this text.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency (voting over sampled reasoning paths)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate multiple diverse reasoning chains by sampling (temperature > 0); collect the answers produced by each sampled chain and select the most consistent answer via majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K and AQuA (experiments combining SC with Self-Polish); general multi-step reasoning benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K and AQuA are multi-step mathematical reasoning datasets used to evaluate chain-of-thought and sampling-based aggregation strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>When combined with Self-Polish, Self-Consistency (SC) yields substantial additional gains in Auto-CoT and Complex-CoT manners on GSM8K and AQuA (exact numerical improvements referenced in Table 3 but not included in the text excerpt). Increasing the number of sampled reasoning paths increases performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Self-Consistency alone (without Self-Polish) is an existing strong baseline; the paper shows SP provides further gains on top of SC, but specific baseline numeric values are not present in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Sampling-based ensemble of multiple reasoning outputs (prompt engineering to elicit CoT and sampling via temperature); aggregation by majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Experiments combining SP with SC (temperature=0.7 for diversity) on GSM8K and AQuA demonstrate that SP gives substantial performance gains for SC under Auto-CoT and Complex-CoT setups; moreover, more sampled reasoning paths led to improved accuracy (reported qualitatively and referenced in Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No detailed failure cases specific to SC presented in the excerpt; paper emphasizes SP is orthogonal and complementary to SC, and combination improves performance. The excerpt notes sampling (temperature) and number of paths affect performance, consistent with known SC sensitivities.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>SC is compared as an answer-side method; SP is orthogonal and improves SC results when combined. The paper contrasts problem-side SP with reasoning-side methods such as CoT and LtM and demonstrates synergy.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>The paper studies combining SP with SC and varying number of reasoning paths (increasing paths improved results), but precise ablation numbers are referenced in Table 3 and not reproduced in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Selfconsistency improves chain of thought reasoning in language models. <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Least-to-most prompting enables complex reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Automatic chain of thought prompting in large language models. <em>(Rating: 1)</em></li>
                <li>Complexity-based prompting for multi-step reasoning. <em>(Rating: 1)</em></li>
                <li>Large language models can be easily distracted by irrelevant context. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8931",
    "paper_id": "paper-258865576",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "SP",
            "name_full": "Self-Polish",
            "brief_description": "A prompting-based, training-free method that has the model iteratively rewrite/clarify the input problem (problem-side refinement) to make it more concise, focused, and solvable, then solve the refined problem; can be used with CoT-style answer-side prompting or with self-consistency voting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-002, text-davinci-003, GPT-3.5-Turbo",
            "model_description": "GPT-series models accessed via OpenAI API as reported in the paper; the paper does not provide parameter counts or training details for these models.",
            "reflection_method_name": "Self-Polish (iterative problem refinement)",
            "reflection_method_description": "The model is prompted to rewrite the original problem into a clearer, concise, well-focused formulation (via zero-shot instruction, in-context demonstrations, or automatic demonstration selection). The pipeline alternates problem-refining and problem-solving: generate a refined problem, generate an answer (optionally with Chain-of-Thought), then repeat refinement+answering until a stopping condition (two consecutive identical answers) or a maximum iteration count T (default T=2). Variants include zero-shot SP, in-context SP, Auto-SP (clustering-based demo selection), and Complex-SP (complexity-based demo selection).",
            "task_name": "Multiple math/reasoning benchmarks (GSM8K, AQuA, SVAMP, MultiArith, MathQA, GSM-IC robustness splits)",
            "task_description": "Standard multi-step math and arithmetic reasoning benchmarks: GSM8K (grade school math), AQuA (algebraic QA), SVAMP (simple arithmetic with adversarial perturbations), MultiArith (arithmetic word problems), MathQA (math word problems); GSM-IC is an adversarial arithmetic dataset with distracting information used for robustness evaluation.",
            "performance_with_reflection": "Paper reports consistent and notable improvements across five benchmarks and on robustness splits when applying Self-Polish. Example numeric results reported for MATH (random 200 samples, CoT answer-side): No refinement 21.0% → Zero-shot SP 23.5% → In-context SP 24.5% (percent accuracy). For other benchmarks, the paper reports qualitative/aggregate gains (figures and tables referenced) but specific per-benchmark numeric values are not reproduced in the main text excerpt.",
            "performance_without_reflection": "Baseline few-shot or CoT prompting without problem refinement (standard prompting / CoT / Auto-CoT / Complex-CoT) as used in prior work; example numeric baseline for MATH (same sample) is 21.0% accuracy (No refinement). For other datasets, explicit baseline numbers are not provided in the text excerpt but comparisons are reported in tables referenced by the paper.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering / in-context learning: instructions and demonstrations to rewrite problems; automatic demo selection via k-means clustering (Auto-SP) or problem complexity (Complex-SP); iterative loop implemented by alternating a problem-refine prompt and an answer/reasoning prompt. Decoding used greedy when evaluating SP alone (temperature=0), and temperature &gt; 0 (0.7) when combining with Self-Consistency to sample multiple reasoning paths.",
            "number_of_iterations": 2,
            "evidence_for_improvement": "Empirical evaluations across GSM8K, AQuA, SVAMP, MultiArith, MathQA show consistent improvements when applying SP; robustness evaluations on GSM-IC show improved resistance to distracting information; ablations show increasing max iterations T raises convergence accuracy up to a point; combining SP with reasoning-side methods (CoT, Auto-CoT, Complex-CoT) further increases accuracy. Quantitatively, on a 200-sample MATH subset, accuracy rose from 21.0% (no refinement) to 24.5% (in-context SP).",
            "limitations_or_failure_cases": "Reported limitations include (1) convergence criterion is based on identical successive answers rather than assessment of problem optimality; (2) excessive refinement can alter semantics or drop important information and worsen performance (a failure case shown for MultiArith where second generated problem deviated semantically and caused incorrect answer); (3) designed refinement patterns do not cover all real-world scenarios; (4) automatic prompt construction explored but not exhaustive; (5) on some simple tasks (e.g., MultiArith combined with CoT), SP yields limited or negative improvements due to possible information loss from unnecessary rewriting.",
            "comparison_to_other_methods": "SP is explicitly orthogonal to answer-side methods like Chain-of-Thought (CoT), Least-to-Most (LtM), Auto-CoT and Complex-CoT and can be combined with them; combining SP with these methods typically yields substantial additional gains. Complex-based prompting (Complex-SP) performed best among SP variants. On simpler tasks already well-handled by CoT, SP gives limited improvement or can harm performance if refinement is excessive.",
            "ablation_study_results": "Ablations explored max iteration T and final-answer selection strategy: larger T increases convergence accuracy ('Converge') but too large T can decrease overall performance (excessive rewriting harms quality); T_actual distribution shows most examples converge in few iterations (long tail beyond T); selection strategies: 'Last One' works better with small T, while with more iterations voting/majority strategies can outperform 'Last One'. The paper sets T=2 as a trade-off between compute and performance.",
            "uuid": "e8931.0",
            "source_info": {
                "paper_title": "Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ProgressiveRefine",
            "name_full": "Progressively Refining Framework (within Self-Polish)",
            "brief_description": "An iterative control loop used by Self-Polish that alternates problem refinement and problem solving until a return condition (converged answers or max iterations) is met, with multiple strategies to select the final answer if convergence is not reached.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-002, text-davinci-003, GPT-3.5-Turbo",
            "model_description": "GPT-series models accessed via OpenAI API as reported in the paper; model internals/size not provided.",
            "reflection_method_name": "Progressive problem-refine + solve iterations (convergence-based stopping)",
            "reflection_method_description": "Start by generating an answer to the original problem; then loop: (1) generate a refined version of the problem using the refine prompt; (2) answer the refined problem; (3) compare current answer to previous answer; stop and return when two consecutive answers match (convergence) or when iteration count exceeds T (default T=2). If exceeding max iterations, final-answer selection strategies include returning the original answer, the first generated answer, the last generated answer (default), or using majority voting.",
            "task_name": "Same multi-step reasoning benchmarks as above; additionally used for robustness evaluation (GSM-IC)",
            "task_description": "Benchmarks for multi-step arithmetic/mathematical reasoning and adversarial robustness datasets with distracting information.",
            "performance_with_reflection": "Progressive refining (adding the iterative loop) improves consistency and reliability of answers and brings further improvement across prompt-construction approaches (reported qualitatively; specific per-task numeric deltas not present in text except aggregate improvements described).",
            "performance_without_reflection": "Single-step problem refining (i.e., rewrite once then answer) or baseline prompting without iterative convergence checking; paper reports iterative progressive refining adds further gains over single-step SP.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-engineered iterative loop implemented within the prompt+API call sequence (see Algorithm 1 in Appendix B); uses answer comparison to detect convergence. Greedy decoding used for stability in SP-only runs; sampling (temperature &gt; 0) used when combining with self-consistency.",
            "number_of_iterations": 2,
            "evidence_for_improvement": "Ablation: increasing max iterations T increases 'converge' accuracy metric; distribution plots show most examples converge in few iterations; using progressive refining on top of the several SP prompting approaches yields additional performance gains (paper reports this in tables/figures).",
            "limitations_or_failure_cases": "If T is set too high, excessive rewriting can degrade problem quality and performance; the return condition only checks answer equality (not problem optimality); when convergence not reached, selection strategy matters and can change outcomes (the paper reports different strategies perform differently depending on T).",
            "comparison_to_other_methods": "The progressive refining loop is a control strategy layered on top of SP and is complementary to reasoning-side methods; the paper compared different final-answer selection rules and showed dependence on iteration count (e.g., 'Last One' better for small T, voting better for larger T).",
            "ablation_study_results": "Varying T: larger T increased convergence accuracy but too large T decreased overall performance; actual iteration counts T_actual are typically small (long-tail distribution). Final-answer selection strategies: 'Last One' favored with small T; other strategies (e.g., voting) can outperform when iteration count is larger.",
            "uuid": "e8931.1",
            "source_info": {
                "paper_title": "Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SC",
            "name_full": "Self-Consistency",
            "brief_description": "A previously proposed answer-side method that samples multiple reasoning paths/chain-of-thoughts and aggregates answers (majority vote) to improve reasoning reliability.",
            "citation_title": "Selfconsistency improves chain of thought reasoning in language models.",
            "mention_or_use": "use",
            "model_name": "text-davinci-002, text-davinci-003, GPT-3.5-Turbo",
            "model_description": "GPT-series models accessed via OpenAI API as reported in the paper; no size details provided in this text.",
            "reflection_method_name": "Self-Consistency (voting over sampled reasoning paths)",
            "reflection_method_description": "Generate multiple diverse reasoning chains by sampling (temperature &gt; 0); collect the answers produced by each sampled chain and select the most consistent answer via majority voting.",
            "task_name": "GSM8K and AQuA (experiments combining SC with Self-Polish); general multi-step reasoning benchmarks",
            "task_description": "GSM8K and AQuA are multi-step mathematical reasoning datasets used to evaluate chain-of-thought and sampling-based aggregation strategies.",
            "performance_with_reflection": "When combined with Self-Polish, Self-Consistency (SC) yields substantial additional gains in Auto-CoT and Complex-CoT manners on GSM8K and AQuA (exact numerical improvements referenced in Table 3 but not included in the text excerpt). Increasing the number of sampled reasoning paths increases performance.",
            "performance_without_reflection": "Self-Consistency alone (without Self-Polish) is an existing strong baseline; the paper shows SP provides further gains on top of SC, but specific baseline numeric values are not present in the excerpt.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Sampling-based ensemble of multiple reasoning outputs (prompt engineering to elicit CoT and sampling via temperature); aggregation by majority voting.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Experiments combining SP with SC (temperature=0.7 for diversity) on GSM8K and AQuA demonstrate that SP gives substantial performance gains for SC under Auto-CoT and Complex-CoT setups; moreover, more sampled reasoning paths led to improved accuracy (reported qualitatively and referenced in Table 3).",
            "limitations_or_failure_cases": "No detailed failure cases specific to SC presented in the excerpt; paper emphasizes SP is orthogonal and complementary to SC, and combination improves performance. The excerpt notes sampling (temperature) and number of paths affect performance, consistent with known SC sensitivities.",
            "comparison_to_other_methods": "SC is compared as an answer-side method; SP is orthogonal and improves SC results when combined. The paper contrasts problem-side SP with reasoning-side methods such as CoT and LtM and demonstrates synergy.",
            "ablation_study_results": "The paper studies combining SP with SC and varying number of reasoning paths (increasing paths improved results), but precise ablation numbers are referenced in Table 3 and not reproduced in the excerpt.",
            "uuid": "e8931.2",
            "source_info": {
                "paper_title": "Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Selfconsistency improves chain of thought reasoning in language models.",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2
        },
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models.",
            "rating": 2
        },
        {
            "paper_title": "Automatic chain of thought prompting in large language models.",
            "rating": 1
        },
        {
            "paper_title": "Complexity-based prompting for multi-step reasoning.",
            "rating": 1
        },
        {
            "paper_title": "Large language models can be easily distracted by irrelevant context.",
            "rating": 2
        }
    ],
    "cost": 0.01427925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement</p>
<p>Zhiheng Xi 
School of Computer Science
Fudan University
ShanghaiChina</p>
<p>Senjie Jin 
School of Computer Science
Fudan University
ShanghaiChina</p>
<p>Yuhao Zhou 
School of Computer Science
Fudan University
ShanghaiChina</p>
<p>Rui Zheng rzheng20@fudan.edu.cn 
School of Computer Science
Fudan University
ShanghaiChina</p>
<p>Songyang Gao 
School of Computer Science
Fudan University
ShanghaiChina</p>
<p>Tao Gui tgui@fudan.edu.cn 
Institute of Modern Languages and Linguistics
Fudan University
ShanghaiChina</p>
<p>Qi Zhang 
School of Computer Science
Fudan University
ShanghaiChina</p>
<p>Xuanjing Huang xjhuang@fudan.edu.cn 
School of Computer Science
Fudan University
ShanghaiChina</p>
<p>Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement
11127BFE1B421866A91724C3A196811D
To enhance the multi-step reasoning capabilities of large language models, researchers have extensively explored prompting methods, notably the Chain-of-Thought (CoT) method which explicitly elicits human-like rationales.However, they have inadvertently overlooked the potential of enhancing model reasoning performance by formulating higher-quality problems 1 .In this work, we start from the problem side and propose Self-Polish (SP), a novel method that facilitates the model's reasoning by guiding it to progressively refine the given problems to be more comprehensible and solvable.We also explore several automatic prompting varients and propose the Self-Polish prompt bank for the community.SP is orthogonal to all other prompting methods of answer/reasoning side like CoT, allowing for seamless integration with state-of-the-art techniques for further improvement.Thorough experiments show that the proposed method attains notable and consistent effectiveness on five reasoning benchmarks across different models.Furthermore, our method also showcases impressive performance on robustness evaluation.Codes and prompts are available at https://github.com/WooooDyy/Self-Polish.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have achieved impressive performance on a variety of NLP tasks (Brown et al., 2020;Otter et al., 2021;Chowdhery et al., 2022), but their capability to perform multistep reasoning is considered a limitation, which can not be tackled solely by scaling up the model size (Rae et al., 2021;Srivastava et al., 2022).To address this challenge, many prompting methods have been proposed to elicit reasoning in LLMs, and Figure 1: Schematic comparison between Self-Polish and other representative approaches for reasoning with prompting.Previous paradigms enhance the reasoning capability of LLMs from the aspect of the answer side/reasoning side, while our method starts from the problem side, and refines problems to be simpler and more comprehensible for models.</p>
<p>have demonstrated significant effectiveness (Wei et al., 2022b;Fu et al., 2022;Zhou et al., 2022a).</p>
<p>Chain-of-Thought (CoT) is a breakthrough method that teaches a language model to imitate the step-by-step reasoning process of humans to solve a reasoning task (Wei et al., 2022b).Many following work has explored variants of CoT to improve the quality of rationales of LLMs (Kojima et al., 2022;Fu et al., 2022;Zhou et al., 2022a).There is also a line of work that optimizes the rationales for better consistency and continuity (Wang et al., 2022;Li et al., 2022;Zelikman et al., 2022;Zheng et al., 2023), and a representative one is Self-Consistency (SC).SC generates diverse reasoning paths and answers, and then leverages the majority vote strategy to get the most consistent answer (Wang et al., 2022).Despite the boosted reasoning performance of the aforementioned methods, they focus on the answer/reasoning side, and little emphasis has been placed on the problems side.problem description are crucial factors for human understanding and model comprehension (Shou and Smithson, 2015;Faruqui and Das, 2018;Chu et al., 2020).LLMs often exhibit poor reasoning performance when confronted with low-quality real-world reasoning problems, which may be excessively long, ambiguous, unclear in focus, or contain irrelevant information (Zellers et al., 2018;Shi et al., 2023;Ye and Durrett, 2022).To tackle this challenge, we consider refining problems into a better formulation.</p>
<p>In this work, we propose Self-Polish (Figure 1 right) that leverages LLMs themselves to refine reasoning problems without training for better reasoning performance.We first present several principles for refined problems: concise, clear, well-focused, and absent of irrelevant information.To achieve our goal, we propose the Self-Polish Prompt Bank which includes several feasible solutions as outlined in the following text.An intuitive strategy is to reformulate problems via instruction-following (Sanh et al., 2022;Ouyang et al., 2022), and we call it zero-shot problem refining.Next, we include demonstrations in the prompts (Brown et al., 2020;Chowdhery et al., 2022) to enable models to better internalize and apply the principles, which is defined as in-context problem refining.During the construction of the demonstrations, we incorporated a curated collection of problem-refining patterns, e.g., eliminating irrelevant information, rearranging the logic structure, and organizing local conditions into new ones in parallel.Moreover, we explore automatic prompting methods to construct enhanced prompts and mitigate manual efforts, based on the criteria of complexity (complexitybased Self-Polish) or diversity (automatic Self-Polish).To further enhance the reliability and consistency of the generated problems, we propose to progressively refine problems until obtaining a convergent answer.</p>
<p>Experiments show that our method consistently improves the reasoning performance of various models (i.e., Text-davinci-002, Text-davinci-003, GPT-3.5-Turbo) on five benchmarks (Table 1 &amp; Figure 3).Moreover, the proposed method is orthogonal to all other reasoning-side state-of-theart prompting methods, making it convenient to be combined with them for further improvement.Detailed experiments demonstrate that the performance of reasoning-side methods can be significantly boosted when integrated with SP (Table 2 &amp;   Table 3).Self-Polish also showcases exceptional performance on robustness evaluation (Figure 4).</p>
<p>In summary, we make the following contributions:</p>
<ol>
<li>
<p>We propose a novel method, Self-Polish, to improve the reasoning performance and robustness of LLMs.</p>
</li>
<li>
<p>We demonstrate the effectiveness of our method when applied alone or combined with other prompting approaches on five benchmarks with different models.</p>
</li>
<li>
<p>We believe that the proposed Self-Polish represents an important step in enhancing LLMs' reasoning capabilities by shifting the perspective from the answer/reasoning side to the problem side.We hope it could inspire future research in this field.</p>
</li>
</ol>
<p>Related Work</p>
<p>Multi-step reasoning.Multi-step reasoning tasks have posed significant challenges for language models (Rae et al., 2021;Bommasani et al., 2021;Qiao et al., 2022), and it is considered as an emergent ability of LLMs (Schaeffer et al., 2023).It is in these tasks that the effectiveness of few-shot prompting begins to surpass that of full training set fine-tuning (Lewkowycz et al., 2022).Moreover, such capability is considered important in building more complex artificial intelligence such as large language model-based agents (LLM-based agents) (Xi et al., 2023).Our work represents a significant stride in enhancing the ability of language models to perform multi-step reasoning tasks, through the facilitation of models' comprehension and processing of given reasoning problems.</p>
<p>Reasoning with prompting.Prompting strategies have substantially improved the reasoning ability of LLMs by a large margin (Qiao et al., 2022;Lewkowycz et al., 2022).An important line of work in this area is Chain-of-Thought (CoT) prompting which elicits the reasoning ability of models by prompting them to imitate the stepby-step reasoning process of humans (Wei et al., 2022b;Kojima et al., 2022;Fu et al., 2022;Zhou et al., 2022a).Another line of work focuses on optimizing the rationales for better consistency and continuity (Wang et al., 2022;Li et al., 2022;Zelikman et al., 2022;Zheng et al., 2023).A representative one is Self-Consistency (SC), which samples Begin, Problem Refine (Remove Irrelevant Information)</p>
<p>[Refined Problem 1 ] : Kylie makes 12 beaded necklaces, 5 beaded bracelets.Each beaded necklace needs 20 beads.Each beaded bracelet requires 10 beads.How many beads does Kylie use in total to make her jewelry?</p>
<p>[Answer 1 ] : 155.</p>
<p>[Refined Problem 2 ] : Kylie makes 12 beaded necklaces, and each beaded necklaces needs 20 beads.She also makes 5 beaded bracelets, and each beaded bracelet needs 10 beads.How many beads does Kylie use in total to make her jewelry?</p>
<p>[Answer 2 ] : 290.</p>
<p>[Refined Problem 3 ] : Kylie requires 240 beads to make beaded necklaces.She also requires 50 beads to make beaded bracelets.How many beads does Kylie use in total to make her jewelry?</p>
<p>[Answer 3 ] : 290.</p>
<p>A 1 !=A 0 , Continue Refine (Reorder Conditions)
A 2 != A 1 , Continue Refine (Summary Local Conditions) A 3 == A 2 , Return A 3
Refine example 人造 Figure 2: An example illustrating the framework and problem-refining patterns of Self-Polish.In the first refining iteration, the irrelevant information "Ada bought 2000 tomatoes from the grocery store." is removed.In the second iteration, the conditions are reordered for easier calculation of the number of beads required for each type of beaded product.In the third iteration, the local conditions were parallelly combined to form new conditions (the total number of beads required for necklaces and bracelets).</p>
<p>multiple reasoning paths and generate the most consistent answer by majority vote (Wang et al., 2022).Different from Self-Polish, the aforementioned strategies emphasize improving the quality of rationales from the answer/reasoning side.Our method is a problem-side method, so it is orthogonal to all of them and can be combined with them for further improvement.</p>
<p>See Appendix A for more related work and the detailed differences between Self-Polish and Leastto-Most (Zhou et al., 2022a).</p>
<p>Self-Polish Prompting</p>
<p>In this section, we first revisit previous prompting paradigms aiming at solving reasoning problems.Next, we describe the proposed Self-Polish method detailedly.</p>
<p>Revisiting Paradigms of Reasoning Problem Solving</p>
<p>In the context of enhancing the capabilities of LLMs, the prompting technique has emerged as one of the most popular approaches owing to its training-free nature and effectiveness (Qiao et al., 2022;Lewkowycz et al., 2022).Here, we formalize several representative paradigms.See Figure 1 for a schematic comparison between them and our method.</p>
<p>Standard.The prompt contains k× [Problem, Answer] pairs, followed by the test problem.</p>
<p>Chain-of-Thought (Wei et al., 2022b).The prompt contains k× [Problem, Rationale, Answer] tuples, followed by the test problem.This method teaches models to generate rationales and answers, achieving significant improvement in reasoning.Auto-CoT (Fu et al., 2022) and Complex-CoT (Zhou et al., 2022a) are two automatic varients that constructs CoT demonstrations according to the criteria of problem diversity and reasoning complexity, respecticely.</p>
<p>Least-to-Most (Zhou et al., 2022a making it evident what the question is asking; (4) absence of irrelevant information: the problems should be free from extraneous details that could cause confusion or distractions.</p>
<p>Construction of Refining Prompts</p>
<p>Zero-shot Self-Polish.It is difficult to internalize the aforementioned principles within the model via training due to the tedious process of constructing a corresponding dataset and potential catastrophic forgetting problems (Goodfellow et al., 2014;Parisi et al., 2019).So we turn to training-free strategies.</p>
<p>As LLMs demonstrate emergent abilities of instruction-following (Schaeffer et al., 2023;Sanh et al., 2022;Wei et al., 2022a), a simple and intuitive strategy to refine problems is prompting LLMs with an instruction.In the instruction, we guide the model to rewrite new versions of the original reasoning problem to be more understandable and easy to answer, and never omit any useful information.The prompt contains [Instruction, Original Problem] and the model responds with a newly generated problem.Next, we can adopt any prompting method in Section 3.1 to get the answer to the new problem, and we take this answer as the final one.We conduct preliminary validation experiments and the results are illustrated in Table 1.Zero-shot refining can consistently improve reasoning performance on various benchmarks.</p>
<p>In-context Self-Polish.As empirical results show that zero-shot refining can only provide limited performance gain, especially on difficult datasets, we then add demonstrations to the prompt to enable models to better internalize and apply design principles.Specifically, demonstrations are formulated as [Original Problem, New Problem] pairs, and we incorporate a curated collection of problem-refining patterns in the demonstrations:</p>
<p>(1) remove irrelevant information, as the first iteration in Figure 2; (2) rearrange the logic structure and group relevant conditions together to better match the reasoning logic of the model, as the second iteration in Figure 2; (3) summarize local conditions into new ones in parallel, as the third iteration in Figure 2. 2 Results in Table 1 show that in-context problem refining yields more performance gain than zero-shot refining.</p>
<p>Automatic Self-Polish.This is an automatic variant of the in-context problem-refining.We draw inspiration from Zhang et al. (2022) and construct the refining prompt according to the diverse semantics of problems with the technique of k-means clustering.The underlying hypothesis is that a diverse set of demonstrations can cover a broad semantic space of problems, thereby the model can locate relevant reference demonstrations for more test examples.Table 1 shows that Auto-SP also yields significant improvement.</p>
<p>Complexity-based Self-Polish.This is another variant of the in-context problem-refining for automatically selecting refining demonstrations.We draw inspiration from Fu et al. (2022) and construct the refining prompt according to the complexity of each problem.The underlying hypothesis is that the refining ability of the model can generalize from complex problems to simpler ones.Table 1 demonstrates that Complex-SP can also yield substantial performance gain.</p>
<p>Progressively Refining Framework</p>
<p>To enhance the consistency and reliability of the refined problems, we propose a progressive framework that has two stages: the problem-solving stage (Section 3.1) and the problem-refining stage (Section 3.2).The two stages are executed alternatively until the return condition is satisfied.</p>
<p>Return condition &amp; answer selection.There are two situations that terminate the iterative process.The first is when the last two answers are the same, indicating convergence of the answer.In this case, we can directly return the answer.The second situation is when the iteration number exceeds the maximum count T = 2. 3 In such case, we have multiple options for selecting the final answer, such as the answer to the original problem, the answer to the first generated problem, the answer to the last generated problem, or utilizing a majority voting approach to select the answer (Wang et al., 2022), which will be discussed in our ablation study in Section 5.1.Here we choose the answer to the last generated problem by default.As shown in Table 1, adding Progressively Refining to our method can bring further improvement across different promptconstruction approaches.</p>
<p>The overall framework is shown in Algorithm 1 in Appendix B.</p>
<p>Experiments</p>
<p>In this section, we conduct experiments to demonstrate the effectiveness and robustness of SP.</p>
<p>Experimental Setups.</p>
<p>Models.We employ three GPT-series models, namely text-davinci-002, text-davinci-003, and GPT-3.5-Turbo(Brown et al., 2020;Ouyang et al., 2022), as they are widely recognized and accessible to the public, ensuring reproducibility of our research.Our experiments are based on Ope-nAI's API.All methods use greedy decoding (i.e., temperature = 0) for stable responses. 3One iteration means one time of problem refinement.Note that a bigger T can yield a larger performance gain, as discussed in Section 5.1 Here we set T = 2 to achieve a balance in computational efficiency and performance.Datasets.We evaluate the performance of our method on five reasoning datasets, including GSM8K (Cobbe et al., 2021), AQuA (Ling et al., 2017), SVAMP (Patel et al., 2021), MultiArith (Roy and Roth, 2015) and MathQA (Amini et al., 2019).The datasets are evaluated by prior studies in the field of multi-hop reasoning (Wei et al., 2022b;Fu et al., 2022;Zhou et al., 2022a).We evaluate on the whole test set of AQuA and GSM8K.For other datasets, we adopt the split from Mishra et al. (2022) or randomly select 500 test instances, and perform 3 restarts for stable results.</p>
<p>Prompts.For the sake of generalizability, GSM8K, SVAMP and MultiArith share the same Self-Polish prompts constructed from GSM8K; AQuA and MathQA share the same Self-Polish prompts constructed from AQuA.See Appendix F for SP prompts.The prompts for the standard fewshot prompting method are from Wei et al. (2022b).</p>
<p>The prompts for Chain-of-thought, Least-to-Most, Auto-CoT and Complex-CoT are from previous work (Wei et al., 2022b;Zhou et al., 2022a;Zheng et al., 2023;Fu et al., 2022;Zhang et al., 2022)</p>
<p>Experimental Results</p>
<p>Standard few-shot setting.Figure 3 shows the results of evaluating the performance in the standard few-shot setting.We can find that : (1) Our method consistently improves reasoning performance by a large margin across multiple models and datasets, indicating its capability to enhance model understanding of problems.</p>
<p>(2) On relatively weaker models, automated prompting methods like Auto-CoT and Complex-CoT yield more gains compared to in-context SP.However, on stronger models, the differences in performance gain between the three approaches are not significant, revealing that the stronger models are less sensitive to prompts.</p>
<p>Combining Self-Polish with other prompting strategies.Table 2 demonstrates the evaluating results when combining our method with other state-of-the-art reasoning-side promoting strategies.</p>
<p>There are several critical and interesting observations: (1) Generally, SP yields substantial performance gains for all reasoning-side methods, revealing that when the model is able to better comprehend problems, both its step-by-step reasoning capabilities and problem decomposition abilities can be significantly enhanced.</p>
<p>(2) Whether for the reasoning side or the problem side, the Complexbased approach performs the best.This indicates that LLMs have the ability to generalize from com-plex tasks to simple ones, both in terms of reasoning and problem refinement.(3) As Fu et al. ( 2022) stated, the average number of words in problems, i.e., GSM8K (46.9),AQuA (51.9),SVAMP (32.1),MultiArith (31.2), and MathQA (60.1), can serve as a proxy for measuring the reasoning complexity of each task.We find that the more challenging the task, the higher the improvement achieved by SP, highlighting its suitability for intricate reasoning tasks.It is noteworthy that when combined with the CoT-series methods, our approach has limited improvement on MultiArith.This could be because the task itself can already be well solved by CoT and is relatively simple.Excessive refinement of simple problems carries the risk of information loss or semantic alterations, leading to a decline in performance, as depicted in Figure 9.</p>
<p>Robustness evaluation.GSM-IC (Shi et al., 2023) is an adversarial arithmetic reasoning dataset with distracting information in the problem to fool the model.So it is well-suited for evaluating the robustness of models.It has two splits: GSM-IC-2step which contains problems that require two reasoning steps to solve and GSM-IC-mstep which contains problems that require more than two reasoning steps to solve.As shown in Figure 4, our method enhances the robustness and reliability of various models across different prompting techniques, shielding them from the interference of low-quality problems.Figure 4: Evaluation results on GSMIC (Shi et al., 2023).Self-Polish (SP) enhances the robustness and reliability of various models when combined with different prompting techniques.</p>
<p>Discussion</p>
<p>Ablation Studies</p>
<p>As mentioned in Section 3.3, the maximum iteration times T and the strategy to select the final answer if the convergence is not achieved are two main components of Self-Polish.Here we perform ablation studies on them.</p>
<p>Max iterating times T .As shown in Figure 5(a) and Figure 5(c), for both the Standard and CoT methods, larger iteration counts lead to higher convergence accuracy ("Converge" in figures), which aligns with common knowledge and further demonstrates the effectiveness of our method: by gradually optimizing problems, we enable the model to handle them more easily.But when T is too big, the performance of SP may suffer a drop, indicating that excessive rewriting can lead to a decline in the quality of problems.We set T = 2 not only for the sake of efficiency, but also because it can achieve competitive performance especially when combined with CoT-series methods.</p>
<p>Final answer selection strategies.We can easily observe that with a smaller T , the "Last One" strat-egy tends to have an advantage, while as the iteration count increases, other strategies become more effective, even outperforming "Last One".This is intuitive as after multiple rewriting iterations, the semantic meaning of a problem may deviate significantly from the original one.T actual exhibits a long-tail distribution, with only a few samples exceeding the max times.This finding provides evidence that our method is highly efficient that consumes few additional computational resources.</p>
<p>Analysis of Actual Iterating Times</p>
<p>Further Improvement for Self-Consistency</p>
<p>Self-Consistency is a prompting method that samples multiple reasoning paths and generates a consistent answer by majority vote strategy (Wang  (Wang et al., 2022).Here, we combine the Self-Polish and Self-Consistency methods to investigate whether there will be further performance improvement.We conduct experiments on two difficult datasets (i.e., GSM8K and AQuA) with temperature = 0.7 for diversity following (Wang et al., 2022).</p>
<p>Results in Table 3 demonstrate that SP provides a substantial performance gain for SC in Auto-CoT and Complex-CoT manners.Moreover, an increase in the number of reasoning paths leads to a corresponding improvement in performance, showing the advantage of voting strategy.</p>
<p>Case Study</p>
<p>To further demonstrate the effectiveness of the problem-refining patterns we proposed and how our method embodies the proposed principles, we conducted a case study as shown in Figure 6.More cases can be found in the Appendix D (Figure 7 and Figure 8).</p>
<p>From Figure 6, we observe that removing irrelevant information (i.e., "Grover's neighbor made a salary of $10 last year.")can help the model avoid distractions and facilitate accurate reasoning.Next, rearranging the problem conditions and grouping pertinent conditions together can facilitate the model in generating more effective novel deductions during the process of reasoning (e.g., resulting in the streamlined computation of the total number of face masks in Refined Problem 2 ).</p>
<p>GSMIC</p>
<p>Figure 6: A case of Self-Polish on GSM-IC with Chainof-Thought.The case is with Text-davinci-003.The irrelevant information "Grover's neighbor made a salary of $10 last year." is removed.In the second iteration, the order of the condition "Each box has 20 face masks." is moved forward and the model can calculate the total number of masks more easily when performing reasoning.</p>
<p>Additionally, summarizing local conditions into new ones can effectively simplify complex problems, enabling the model to handle them with greater ease.This is demonstrated in the first iteration of Figure 7 and the second iteration of Figure 8. Furthermore, the second iteration in Figure 7 highlights how our approach can explicitly and precisely define the problem in a formal manner.Specifically, in the Refined Problem 2 of Figure 7, the model accurately identifies the two teams as "Team A" and "Team B" instead of referring to them as "one team" and "the other team", and then it is able to clearly specify the exact question to be asked.This significantly reduces the model's burden of understanding during the reasoning process, enhancing its overall performance.reasoning in large language models.We present a novel prompting method called Self-Polish which progressively refines the given reasoning problems to facilitate model comprehension and processing.It demonstrates impressive effectiveness, robustness, and reliability in various benchmarks across different models, and can seamlessly integrate with other state-of-the-art methods.We hope it could motivate future research in this field.</p>
<p>Limitations</p>
<p>Despite the significant enhancement in the reasoning performance achieved by our approach, this work still has limitations.Firstly, our criterion for convergence is based on obtaining two identical answers rather than assessing whether the problem itself has been sufficiently optimized.Future work could involve designing methods that enable the model to autonomously determine whether a problem has reached its optimal form.Secondly, we have explored two approaches to automatically construct problem-refining prompts (i.e., Auto-Sp and Complex-SP).However, in the future, it would be beneficial to incorporate more techniques for automatically generating instructions or selecting demonstrations.Thirdly, although our designed patterns for problem refining have proven highly effective, they do not encompass all possible scenarios in the real world.In the future, it is conceivable to incorporate additional patterns to further expand the scope of applicability.</p>
<p>In-context learning.It is demonstrated that a large language model can learn patterns from a few input-output examples in the context (input) to perform the task for an unseen inference-time example (Brown et al., 2020;Chowdhery et al., 2022), and such ability is referred to as in-context learning (ICL).Recent studies have further highlighted the impressive performance of ICL in reasoning tasks (Wei et al., 2022b;Fu et al., 2022;Zhou et al., 2022a).In our research, we capitalize on this capability to generate new formulations of problems by injecting rephrasing patterns to the demonstrations.</p>
<p>Instruction following.LLMs can learn to perform unseen tasks solely through the comprehension of task-specific natual language instructions (Sanh et al., 2022;Wei et al., 2022a;Chung et al., 2022;Ouyang et al., 2022).There is also work showing that combining instructions with incontext learning can provide further benefits and that few-shot demonstrations can be viewed as a special kind of instruction that arouses the implicit ability in LLMs (Chung et al., 2022;Zhou et al., 2022b;Qiao et al., 2022).</p>
<p>Compaison with LtM.The work that is most similar to ours may be Least-to-Most (LtM) which decomposes the original problem into a series of sub-problems that need to be solved sequentially (Zhou et al., 2022a).However, LtM is an variant of CoT, and there are differences in motivation and operation process between LtM and SP.Firstly, LtM is an answer/reasoning side approach that emphasizes the decomposition of a complex problem into sub-problems, while we emphasize refining the original problem to make it more understandable.Secondly, in LtM, sub-problems are solved sequentially, requiring the answer of the previous sub-problem to tackle the next one, which can lead to fragility in the reasoning chain.In contrast, our method allows for the combination of local related conditions to form new conditions parallelly.</p>
<p>B The Algorithm of Self-Polish</p>
<p>See Algorithm 1 for the overall framework of Self-Polish.</p>
<p>C Implementation Details</p>
<p>We set the maximum iterating count to T = 2.Note that the bigger maximum iteration count may lead to better performance, but here we set it to 2 to achieve a trade-off between computational efficiency and effectiveness.</p>
<p>When combining with other reasoning-side methods (i.e., CoT, LtM, Complex-CoT and Auto-CoT) on MultiArith and SVAMP, we set the answer selection strategy as "selecting the answer to the original problem" because this dataset is relatively easy for these prompting methods.Actually, in cases where it is not necessary, rewriting easy problems may result in the loss of critical information or altering the semantics of the original problem.In other settings, we set the answer selection strategy as "selecting the answer to the last problem".</p>
<p>D More Cases and Examples</p>
<p>Here we list more cases of Self-Polish in Figure 7 and Figure 8.We also list the failure case of excessive problem refining in Figure 9 [Refined Problem 2 ] : Two track teams are competing against each other in a 4 by 400 meter relay.Team A has four members that each run 400 meters in 55 seconds, while Team B has four members that run 400 meters in 60, 57, 54, and 51 seconds respectively.How many seconds faster will Team A finish the relay compared to Team B?</p>
<p>[Rationale 2 ] : Team A has four members that each run 400 meters in 55 seconds.Team B has four members that run 400 meters in 60, 57, 54, and 51 seconds respectively.So the total time for Team A is 4 * 55 = 220 seconds, and the total time for Team B is 60 + 57 + 54 + 51 = 222 seconds.Team A will finish the relay 2 seconds faster than Team B. The answer is 2.</p>
<p>[Answer 2 ] : 2.</p>
<p>[Refined Problem 1 ] : Two track teams are competing against each other in a 4 by 400 meter relay.One team has four members that each run 400 meters in 55 seconds, while the other team has four members that run 400 meters in 60, 57, 54, and 51 seconds respectively.How many seconds will the faster team win by?</p>
<p>[Rationale 1 ] : The faster team has four members that run 400 meters in 55, 60, 57, and 54 seconds respectively.The slower team has four members that run 400 meters in 51 seconds each.So the faster team will win by 55 -51 = 4 seconds.The answer is 4.</p>
<p>[Answer 1 ] : 4.</p>
<p>[Original Problem 0 ] : Two track teams are competing against each other in a 4 by 400 meter relay; a race where each competing team has four members that each run 400 meters, or one lap, around a standard track.One of the two teams is very well-rounded and each of their members will run their 400 meter leg in precisely 55 seconds.The other team is less well-rounded; their first runner will run their 400 meter leg in 60 seconds then each subsequent runner will be 3 seconds faster than the previous runner.Using this information, how many seconds will the faster team win by?In the first iteration, some irrelevant information is removed and the average time each member of the second team consumes is clarified.In the second iteration, the model accurately identifies the two teams as "Team A" and "Team B" instead of referring to them as "one team" and "the other team", and it explicitly states what the question to be asked is, reducing the burden of understanding on the model during the reasoning process.</p>
<p>E Sensitivity to Number and Order of Demonstrations</p>
<p>As widely recognized, in-context learning is highly sensitive to the number and order of demonstrations within the prompt (Min et al., 2022;Lu et al., 2022;Liu et al., 2022).In this regard, we investigate whether our problem-refining process is sensitive to these variables via experiments on GSM8K with Text-davinci-003.We randomly select 200 examples from the test set.For a specific shot number, we randomly select five sets of demonstrations.</p>
<p>For each set of demonstrations, we obtain performance results in five different orders.We observed that in the standard manner, increasing the number of demonstrations leads to improved performance.However, in the CoT manner, the performance con-</p>
<p>Gsm8k example 2</p>
<p>Figure 8: A case of Self-Polish on GSM8K with Chainof-Thought.In the first iteration, the model translates "every" to "1".In the second iteration, the model summarizes relevant conditions and specifies the total cost for Charlie.verges when the number of shots is equal to 5, demonstrating impressive sample efficiency.Additionally, in the standard manner, our method is not sensitive to the order of demonstrations while it is  In-context SP 24.5</p>
<p>Table 5: More results on MATH dataset, using Chainof-Thought as the answer-side method highly sensitive to the order of demonstrations in the CoT manner.</p>
<p>F Prompts of Self-Polish</p>
<p>The in-context Self-Polish prompt for AQuA and MathQA is in Table 6.The Auto-SP prompt for AQuA and MathQA is in Table 7 and Table 8.The Complex-SP prompt for AQuA and MathQA is in Table 9 and Table 10.The in-context Self-Polish prompt for GSM8K, SVAMP and MultiArith is in Table 11.The Auto-SP prompt for GSM8K, SVAMP and MultiArith is in Table 12 and Table 13.The Complex-SP prompt for GSM8K, SVAMP and MultiArith is in Table 14 and Table 15.</p>
<p>G More results on MATH dataset</p>
<p>As Table 5 shows, we also conducted Self-Polish methods on the MATH dataset (Hendrycks et al., 2021).Our approach demonstrated promising results.Specifically, we randomly selected 200 samples for testing, and use the Chain-of-Thought as the answer-side method.Original Question: Susy goes to a large school with 800 students, while Sarah goes to a smaller school with only 300 students.At the start of the school year, Susy had 100 social media followers.She gained 40 new followers in the first week of the school year, half that in the second week, and half of that in the third week.Sarah only had 50 social media followers at the start of the year, but she gained 90 new followers the first week, a third of that in the second week, and a third of that in the third week.After three weeks, how many social media followers did the girl with the most total followers have?New Question: At the start of the school year, Susy had 100 social media followers and Sarah had 50 social media followers.Susy gained 40 followers in the first week, 20 in the second week, and 10 in the third week.Sarah gained 90 followers in the first week, 30 in the second week, and 10 in the third week.After three weeks, how many social media followers did the girl with the most total followers have?</p>
<p>Original Question: Sam bought a dozen boxes, each with 30 highlighter pens inside, for $10 each box.He rearranged five of these boxes into packages of six highlighters each and sold them for $3 per package.He sold the rest of the highlighters separately at the rate of three pens for $2. How much profit did he make in total, in dollars?New Question: Sam bought 12 boxes for $10 each, and each contains 30 highlighter pens. 1 package contains 6 highlighters.He rearranged five of these boxes into packages and sold them for $3 per package.He sold the remaining highlighters separately at the price of $2 for every three one.How much profit did Sam make in total, in dollars?</p>
<p>Original Question: In a certain school, 2/3 of the male students like to play basketball, but only 1/5 of the female students like to play basketball.What percent of the population of the school do not like to play basketball if the ratio of the male to female students is 3:2 and there are 1000 students?New Question: In a certain school, there is a total of 1000 students, while 3 male students for every 2 female students.So there are 600 male students and 2/3 of the male students like to play basketball, and there are 400 female students but only 1/5 of the female students like to play basketball.What percent of the population of the school do not like to play basketball?</p>
<p>Figure 3 :
3
Figure3: Evaluating Self-Polish on various benchmarks with different models.Self-Polish consistently improves reasoning performance across multiple models and benchmarks.</p>
<p>Figure 5 :
5
Figure 5: Ablation studies and the distribution of actual iterating times.(a) and (c) illustrate the performance (vertical axis on the left) when using different final answer selection strategies and different max iterating times T .The "Converge" means the performance calculated by N conv /N all where N conv means the number of examples that are answered correctly with converged answers, while the N all means the number of all test examples.We also incorporate a line to represent the average actual iteration times at each value of T (vertical axis on the right).In (b) and (d), we show the distribution of actual iterating times when we set T = 5.</p>
<p>Figure 5
5
Figure5(a) and Figure5(c) show that the actual iterating times T actual does not grow significantly as the max iterating times T increases, revealing that SP can achieve a converged answer on most of the problems with few iterations.To verify this, we illustrate the distribution map of T actual with T = 5 in Figure5(b) and Figure5(d).T actual exhibits a long-tail distribution, with only a few samples exceeding the max times.This finding provides evidence that our method is highly efficient that consumes few additional computational resources.</p>
<p>[</p>
<p>Refined Problem 2 ] : Grover bought 3 boxes of face masks.Each box has 20 face masks.Grover bought the 3 boxes for $15.He plans to sell the face masks for $0.50 each.How much will be his total profit?[Rationale 2 ] : Grover bought 3 boxes of face masks.Each box has 20 face masks.So he bought 3 * 20 = 60 face masks in total.He bought the 3 boxes for 15 dollars.He plans to sell the face masks for 0.50 dollars each.So he will sell the 60 face masks for 60 * 0.50 = 30 dollars.His total profit will be 30 -15 = 15 dollars.Grover bought 3 boxes of face masks.He plans to sell them for $0.50 each.If each box has 20 face masks, and Grover bought the 3 boxes for $15, how much will be his total profit?[Rationale 1 ] : Grover bought 3 boxes of face masks.Each box has 20 face masks.He bought the 3 boxes for 15 dollars.He plans to sell them for 0.50 dollars each.So his total profit will be (3 Grover bought 3 boxes of face masks.Heplans to sell them for $0.50 each.Grover's neighbor made a salary of $10 last year.If each box has 20 face masks, and Grover bought the 3 boxes for $15, how much will be his total profit?</p>
<p>[Figure 7 :
7
Figure7: A case of Self-Polish on GSM8K with Chainof-Thought.In the first iteration, some irrelevant information is removed and the average time each member of the second team consumes is clarified.In the second iteration, the model accurately identifies the two teams as "Team A" and "Team B" instead of referring to them as "one team" and "the other team", and it explicitly states what the question to be asked is, reducing the burden of understanding on the model during the reasoning process.</p>
<p>Figure 9 :
9
Figure 9: A failure case of Self-Polish on MultiArith with Chain-of-Thought.The semantics of the second generated problem have deviated from the semantics of the original problem, and the model cannot answer correctly.</p>
<p>Original Question: The average weight of a, b and c is 45 kg.If the average weight of a and b be 40 kg and that of b and c be 45 kg , then the weight of b is? Answer Choices: (A) 31 kg (B) 32 kg (C) 33 kg (D) 35 kg (E) none of these New Question: The average weight of a, b and c is 45 kg, which means the total weight of a, b and c is 135 kg.If the average weight of a and b is 40 kg, which means the total weight of a and b is 80kg, so the weight of c is 45kg.The average weight of b and c is 45 kg which means the total weight of b and c is 90kg.What is the weight of b? Answer Choices: (A) 31 kg (B) 32 kg (C) 33 kg (D) 35 kg (E) none of these Original Question: The compound and the simple interests on a certain sum at the same rate of interest for two years are rs.11730 and rs.10200 respectively .The sum is? Answer Choices: (A) rs.17037 (B) rs.17000 (C) rs.17276 (D) rs.170287 (E) rs.171881New Question: A sum of money earns compound interest and simple interest at the same rate for two years.The compound interest is Rs.11730 and the simple interest is Rs.10200.What is the sum of money?Answer Choices: (A) rs.17037 (B) rs.17000 (C) rs.17276 (D) rs.170287 (E) rs.171881</p>
<p>Kylie makes 10 beaded necklaces on Monday and 2 beaded necklaces on Tuesday.Then Kylie makes 5 beaded bracelets on Wednesday.20 beads are needed to make one beaded necklace.10 beads are needed to make one beaded bracelet.Ada bought 2000 tomatoes from the grocery store.How many beads does Kylie use in total to make her jewelry?</p>
<p>[Original Problem 0 ] : [Answer 0 ] : 120.</p>
<p>Table 2 :
2
Evaluation results when combining Self-Polish with other answer/reasoning side prompting strategies.The results are with Text-davinci-003.The best performance for each answer side strategy of one task is underlined.The best performance for each task is in bold.</p>
<p>T</p>
<p>Algorithm 1: Self-Polish Prompting Input: language model G, problem set S, prompt P ref ine of the problem side refining method, prompt P answer of the answer/reasoning side method, max iteration number T , answer selection strategy Z.
1 for each problem s in S do 2 answer_list = [ ];3t = 0;4Procedure GENERATE ANSWER TO ORIGINAL PROBLEM5 6rationale t , ans t = G(P answer ⊕ s); answer_list.append(ans t );7t = t + 1;8Procedure ITERATE PROBLEM REFINEMENT AND ANSWER18end19 end
9 s = G(P ref ine ⊕ s); 10 rationale t , ans t = G(P answer ⊕ s); 11 if ans t == ans t−1 then 12 Return ans t .13 else if t &gt; T then 14 Return Z(answer_list).15 else 16 answer_list.append(anst ); 17 t = t + 1;</p>
<p>] : Charlie spent $20.00 in supplies to make 20 tapered candles. He sells each candle for $2.00. So he makes 20 * 2 = 40 dollars in sales. His net profit is 40 -20 = 20 dollars. The answer is 20. [Answer 2 ] : 20.
[Refined Problem 1 ] : Charlie wants to sell beeswax candles. He needs 2 pounds of beeswax and wicks to make 20 tapered candles, which cost him $20.00 in supplies. He sells each candle for $2.00. How much is his net profit if he makes and sells 20 candles? of beeswax and wicks to make 10 tapered candles, which cost him $10.00 in supplies. He sells each candle for $2.00. How much is his net profit if he makes and sells 20 candles? [Rationale 2 [Refined Problem 1 ] : Charlie wants to sell beeswax candles. He needs 1 pound [Rationale 1</p>
<p>] : Charlie needs 1 pound of beeswax and wicks to make 10 tapered candles, which cost him $10.00 in supplies. He sells each candle for $2.00. So he makes a profit of $2.00 per candle. If he makes and sells 20 candles, his net profit is 20 * 2 = 40 dollars. The answer is 40 dollars. [Answer 1 ] : 40. [Original Problem 0 ] :</p>
<p>Charlie wants to sell beeswax candles.For every pound of beeswax, he can make 10 tapered candles.One pound of beeswax and the wicks cost $10.00 in supplies.If he sells each candle for $2.00 each, what is his net profit if he makes and sells 20 candles?</p>
<p>[Rationale 0 ] : Charlie needs 1 pound of beeswax and wicks to make 10 candles, which costs $10. He sells each candle for $2.00. So for 20 candles, he will make a profit of 20 * 2 = 40 dollars. His net profit is 40 -10 = 30 dollars. The answer is 30. [Answer 0 ] : 30. Begin, Problem Refine A 1 != A 0 , Continue Refine A 2 != A 1 , Exceed Max Iterating Times, Return A 2</p>
<p>Rationale 2 ] : The teacher had 38 worksheets to grade originally. After grading 4 worksheets, 15 additional worksheets were turned in. So the teacher now has 38 + 15 = 53 worksheets to grade in total. The answer is 53. [Answer 2 ] : 53.
[Refined Problem 2 ] A teacher had 38 worksheets to grade. After grading 4worksheets, 15 additional worksheets were turned in. How many worksheets does theteacher now have to grade in total?[
[Refined Problem 1 ] : A teacher had 38 worksheets to grade.After grading 4 worksheets, 15 more were turned in.How many worksheets does the teacher now have to grade?[</p>
<p>Rationale 1 ] : The teacher had 38 worksheets to grade originally. After grading 4 worksheets, 15 more were turned in. So the teacher now has 38 + 15 = 53 worksheets to grade. The answer is 53. [Answer 1 ] : 53.</p>
<p>[Original Problem 0 ] : A teacher had 38 worksheets to grade.If she graded 4, but then another 15 were turned in, how many worksheets would she have to grade?</p>
<p>Table 4 :
4
Sensitivity to the number and order of problemrefining demonstrations.Mean represents the average performance for the current number of shots, while the order deviation represents the average standard deviation introduced by different demonstration orders.The results are with Text-davinci-003.In the problem side, we leverage the in-context SP.
METHODMATHNo Refinement21Zero-shot SP23.5</p>
<p>Table 10 :
10
Continuation of Complex-SP prompt for AQuA and MathQA.</p>
<p>Table 15 :
15
Continuation of Complex-SP prompt for GSM8K, SVAMP and MultiArith.</p>
<p>Note that a single example typically does not encompass all refining strategies. The example is constructed solely to illustrate our design patterns.
ConclusionThis paper focuses on a previously neglected aspect, namely the optimization of problem formulation, within the context of enhancing multi-step
AcknowledgementsThe authors wish to thank the anonymous reviewers for their helpful comments.This work was partially funded by National Natural Science Foundation of China (No.62206057,61976056,62076069), Shanghai Rising-Star Program (23QA1400200), Natural Science Foundation of Shanghai (23ZR1403500), Program of Shanghai Academic Research Leader under grant 22XD1401100.Appendix A Disscussion of More Related WorkRecent research has unveiled an unpredictable phenomenon known as emergent abilities, which manifest exclusively in larger language models while eluding their smaller counterparts(Schaeffer et al., 2023).In-context learning, instruction following, and multi-step reasoning are three emergent abilities that we focus on.We have discussed the multistep reasoning in Section 2 and we will discuss another two.We also compare our method with the LtM detailedly here.Please rewrite new versions of the original mathematical question (including the context and the final question) to be more understandable and easy to answer.Don't omit any useful information, especially the numbers.Original Question: Krishan and Nandan jointly started a business.Krishan invested six times as Nandan did and invested his money for double time as compared to Nandan.Nandan earned Rs. 6000.If the gain is proportional to the money invested and the time for which the money is invested then the total gain was?Answer Choices: (A) Rs.78000 (B) Rs.48000 (C) Rs.6000 (D) Rs.82000 (E) Rs.32000 New Question: Krishan and Nandan teamed up to start a business together.Krishan invested 12 times more money than Nandan did.Nandan's earnings from the business were Rs.6000.If the gain is directly proportional to both the amount of money invested and the time period, what was the total gain for both of them?Answer Choices: (A) Rs.78000 (B) Rs.48000 (C) Rs.6000 (D) Rs.82000 (E) Rs.32000Original Question: In a graduate physics course, 70 percent of the students are male and 30 percent of the students are married.If two-sevenths of the male students are married, what fraction of the male students is single?Answer Choices: (A) 2/7 (B) 1/3 (C) 1/2 (D) 2/3 (E) 5/7 New Question: In a graduate physics course, 7/10 of the students are male and 3/10 of the students are married.If 2/7 of the male students are married, what fraction of the male students is single?Please rewrite new versions of the original mathematical question (including the context and the final question) to be more understandable and easy to answer.Don't omit any useful information, especially the numbers.Original Question: A and B can together finish a work in 40days.They worked together for 10days and then B left.After another 12days, A finished the remaining work.In how many days A alone can finish the job?Answer Choices: (A) 10 (B) 25 (C) 60 (D) 16 (E) 20 New Question: A and B can together finish a work in 40 days.They worked together for 10 days and then B left, and the remaining work is 3/4 of the original one.After another 12 days, A finished the remaining work alone.In how many days A alone can finish the whole job?Answer Choices: (A) 10 (B) 25 (C) 60 (D) 16 (E) 20Original Question: A man buys an article and sells it at a profit of 20%.If he had bought it at 20% less and sold it for Rs.75 less, he could have gained 25%. What is the cost price?Answer Choices: (A) 388 (B) 375 (C) 288 (D) 266 (E) 269 New Question: A man buys an article at the price of x and sold it at the price of 1.2x, if he had bought it at a 20% discount which is 0.8x and sold it for Rs.75 less than 1.2x, he would have gained 25% of 0.8x.What was the original price of the article before any discounts or markups?Answer Choices: Original Question: A and B invests Rs.6000 and Rs.8000 in a business.After 6 months, A withdraws half of his capital and B withdraws one-fourth of his capital.In what ratio should they share the profits at the end of the year?Answer Choices: (A) 13:15 (B) 9:13 (C) 9:11 (D) 13:14 (E) 9:14 New Question: A and B invested Rs.6000 and Rs.8000 respectively in a business.After 6 months,A withdraws half of his investment and B withdraws 1/4 of his investment.What is the ratio of their remaining investment?Answer Choices: (A) 13:15 (B) 9:13 (C) 9:11 (D) 13:14 (E) 9:14   Please rewrite new versions of the original mathematical question (including the context and the final question) to be more understandable and easy to answer.Don't omit any useful information, especially the numbers.Original Question: If x &lt; y &lt; z and y -x &gt; 5 , where x is an even integer and y and z are odd integers , what is the least possible value s of z -x ?Answer Choices: (A) 6 (B) 7 (C) 8 (D) 9 (E) 10 New Question: If x is an even integer, y and z are odd integers, and y is greater than x by more than 5, and z is greater than y.What is the smallest possible difference between z and x? Answer Choices: (A) 6 (B) 7 (C) 8 (D) 9 (E) 10Original Question: What is the difference between the c.i. on rs .6000 for 1 1/2 years at 4 % per annum compounded yearly and half-yearly?Answer Choices: (A) s.Please rewrite new versions of the original mathematical question (including the context and the final question) to be more understandable and easy to answer.Don't omit any useful information, especially the numbers.Original Question: Each bird eats 12 beetles per day, each snake eats 3 birds per day, and each jaguar eats 5 snakes per day.If there are 6 jaguars in a forest, how many beetles are eaten each day?New Question: In a forest, there are 6 jaguars that each eat 5 snakes per day.Each snake eats 3 birds per day, and each bird eats 12 beetles per day.How many beetles are eaten each day by the jaguars?Original Question: Albert is wondering how much pizza he can eat in one day.He buys 2 large pizzas and 2 small pizzas.A large pizza has 16 slices and a small pizza has 8 slices.If he eats it all, how many pieces does he eat that day?New Question: Albert has purchased 2 large pizzas and 2 small pizzas and is wondering how many slices he can eat in one day.Each large pizza has 16 slices and each small pizza has 8 slices.If Albert eats all of the pizza, how many slices will he have eaten in one day?Original Question: In a truck, there are 26 pink hard hats, 15 green hard hats, and 24 yellow hard hats.If Carl takes away 4 pink hard hats, and John takes away 6 pink hard hats and twice as many green hard hats as the number of pink hard hats that he removed, then calculate the total number of hard hats that remained in the truck.New Question: In a truck, there are 26 pink hard hats, 15 green hard hats, and 24 yellow hard hats.Carl takes away 4 pink hard hats and John takes away 6 pink hard hats and 12 green hard hats.How many hard hats remain in the truck?Original Question: Jasper will serve charcuterie at his dinner party.He buys 2 pounds of cheddar cheese for $10, a pound of cream cheese that cost half the price of the cheddar cheese, and a pack of cold cuts that cost twice the price of the cheddar cheese.How much does he spend on the ingredients?New Question: Jasper is hosting a dinner party and wants to serve charcuterie.He buys 2 pounds of cheddar cheese for $10, 1 pound of cream cheese for $5, and a pack of cold cuts for $20.How much does he spend on the ingredients for the charcuterie?Original Question: Tomas ate 1.5 pounds of chocolate fudge last week.Katya ate half a pound of peanut butter fudge, while Boris ate 2 pounds of fudge.How many ounces of fudge did the Tomas, Katya and Boris eat in total?New Question: Tomas ate 24 ounces of chocolate fudge last week.Katya ate 8 ounces of peanut butter fudge, while Boris ate 32 ounces of fudge.How many ounces of fudge did the Tomas, Katya and Boris eat in total?Original Question: Tomas ate 24 ounces of chocolate fudge last week.Katya ate 8 ounces of peanut butter fudge, while Boris ate 32 ounces of fudge.How many ounces of fudge did the Tomas, Katya and Boris eat in total?New Question: Tomas ate 24 ounces of fudge last week.Katya ate 8 ounces of fudge, while Boris ate 32 ounces of fudge.How many ounces of fudge did the Tomas, Katya and Boris eat in total?Please rewrite new versions of the original mathematical question (including the context and the final question) to be more understandable and easy to answer.Don't omit any useful information, especially the numbers.Original Question: Monica is a teacher.She has 6 classes per day.The first class has 20 students.The second and third classes have 25 students.Her fourth class has half as many as her first class.Her fifth and sixth classes have 28 students.How many students does Monica see each day?New Question: Monica is a teacher with 6 classes per day.Her first class has 20 students, her second and third classes have 25 students, and her fourth class has 10 students.Her fifth and sixth classes have 28 students.How many students does Monica see each day in all of her classes?Original Question: Emily went to the store and bought art supplies for $20 and 2 skirts that cost the same amount of money.She spent a total of $50.How much did Emily pay for each of the skirts?New Question: Emily went to the store and bought art supplies for $20 and 2 skirts for a total of $50.How much did Emily pay for each of the skirts?Original Question: John's neighbor tells him to walk his dog for 1 hour each day for a total of $10.He does this for April, save for the 4 Sundays in April.He later spent $50 on books and gave his sister Kaylee the same amount.How much money did John have left?New Question: John's neighbor tells him to walk his dog for April (30 days excluding 4 Sundays) for a total of $10 each day.He later spent $50 on books and gave his sister Kaylee the same amount.How much money did John have left after these expenses?Original Question: Three years ago, Bethany was twice the age of her younger sister.In 5 years, her younger sister will be 16.How old is Bethany now?New Question: Three years ago, Bethany was twice the age of her younger sister, who is currently 11 years old.How old is Bethany now?Original Question: At the bookstore, Sarah bought 6 paperback books and 4 hardback books.Her brother bought one-third as many paperback books as Sarah bought, and two times the number of hardback books that she bought.How many books did her brother buy in total?New Question: At the bookstore, Sarah bought 6 paperback books and 4 hardback books.Her brother bought 2 paperback books and 8 hardback books.How many books did her brother buy in total?Original Question: Sandra had 2 different bags of candy.Each of her bags had 6 pieces of candy left.Her brother, Roger, also had 2 bags of candy.One of his bags of candy had 11 pieces left and the other had 3 pieces left.How much more candy did Roger have?New Question: Sandra had 2 bags of candy, each with 6 pieces left.Her brother, Roger, had 2 bags of candy, one with 11 pieces left and the other with 3 pieces left.How many more pieces of candy did Roger have than Sandra?Original Question: Joan wants to visit her family who live 480 miles away.If she drives at a rate of 60 mph and takes a lunch break taking 30 minutes, and 2 bathroom breaks taking 15 minutes each, how many hours did it take her to get there?New Question: Joan wants to visit her family who live 480 miles away.If she drives at a rate of 60 mph and takes a lunch break of 30 minutes, and 2 bathroom breaks of 15 minutes each, how many hours(60 minutes = 1 hour) does it take her to get there?Original Question: James gets a fleet of gas transportation vans.He gets 6 vans. 2 of them are 8000 gallons. 1 of them is 30% less than that.The remaining trucks are 50% larger than the 2 trucks.How many gallons can he transport?New Question: James has acquired a fleet of gas transportation vans.He has 6 vans in total. 2 of the vans have a capacity of 8000 gallons, while the other van has a capacity of 5600 gallons (30% less than the first two vans).The remaining 3 vans have a capacity of 12000 gallons (50% larger than the first two vans).What is the total capacity of the fleet in gallons?Please rewrite new versions of the original mathematical question (including the context and the final question) to be more understandable and easy to answer.Don't omit any useful information, especially the numbers.Original Question: Angelo and Melanie want to plan how many hours over the next week they should study together for their test next week.They have 2 chapters of their textbook to study and 4 worksheets to memorize.They figure out that they should dedicate 3 hours to each chapter of their textbook and 1.5 hours for each worksheet.If they plan to study no more than 4 hours each day, how many days should they plan to study total over the next week if they take a 10-minute break every hour, include 3 10-minute snack breaks each day, and 30 minutes for lunch each day?New Question: Angelo and Melanie want to plan how many hours over the next week they should study together for their test next week.They have 2 chapters of their textbook to study and they decide to dedicate 3 hours to each chapter.They also have 4 worksheets to memorize, and they decide to dedicate 1.5 hours for each worksheet.Taking into account 10-minute breaks every hour, if they plan to study no more than 4 hours each day including 3 10-minute snack breaks each day, and 30 minutes for lunch each day, how many days should they plan to study total over the next week?Original Question: Mark's basketball team scores 25 2 pointers, 8 3 pointers and 10 free throws.Their opponents score double the 2 pointers but half the 3 pointers and free throws.What's the total number of points scored by both teams added together?New Question: Mark's basketball team scores 25 2 pointers, 8 3 pointers and 10 free throws.Their opponents score 50 2 pointers, 4 3 pointers and 5 free throws.Both teams score 75 2 pointers, 12 3 pointers and 15 free throws.What is the total number of points scored by both teams combined?Original Question: Bella has two times as many marbles as frisbees.She also has 20 more frisbees than deck cards.If she buys 2/5 times more of each item, what would be the total number of the items she will have if she currently has 60 marbles?New Question: Bella currently has 60 marbles, and she has twice as many marbles as frisbees and 20 more frisbees than deck cards.She buys 2/5 times more of each item.What would be the total number of the items she will have?Original Question: A group of 4 fruit baskets contains 9 apples, 15 oranges, and 14 bananas in the first three baskets and 2 less of each fruit in the fourth basket.How many fruits are there?New Question: There is a group of 4 fruit baskets.The first three baskets each contains 9 apples, 15 oranges, and 14 bananas, and 7 apples, 13 oranges, and 12 bananas in the fourth basket.How many fruits are there in total?Original Question: You can buy 4 apples or 1 watermelon for the same price.You bought 36 fruits evenly split between oranges, apples and watermelons, and the price of 1 orange is $0.50.How much does 1 apple cost if your total bill was $66? New Question: You bought 36 fruits, with an equal number of oranges, apples and watermelons.The price of 1 watermelon equals to 4 apples, and the price of 1 orange is $0.50.If your total bill was $66, how much does 1 apple cost?
Mathqa: Towards interpretable math word problem solving with operation-based formalisms. Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, Hannaneh Hajishirzi, 10.18653/v1/n19-1245Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USAAssociation for Computational Linguistics2019. June 2-7, 20191</p>
<p>. Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ B Altman, Simran Arora, Michael S Sydney Von Arx, Jeannette Bernstein, Antoine Bohg, Emma Bosselut, Erik Brunskill, Shyamal Brynjolfsson, Dallas Buch, Rodrigo Card, Niladri S Castellon, Annie S Chatterji, Kathleen Chen, Jared Quincy Creel, Dorottya Davis, Chris Demszky, Moussa Donahue, Esin Doumbouya, Stefano Durmus, John Ermon, Kawin Etchemendy, Li Ethayarajh, Chelsea Fei-Fei, Trevor Finn, Lauren Gale, Karan Gillespie, Noah D Goel, Shelby Goodman, Neel Grossman, Tatsunori Guha, Peter Hashimoto, John Henderson, Daniel E Hewitt, Jenny Ho, Kyle Hong, Jing Hsu, Thomas Huang, Saahil Icard, Dan Jain, Jurafsky, abs/2108.07258Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudiand et al. 2021. On the opportunities and risks of foundation models. CoRR</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish; NeurIPS2020. 2020. 2020. December 6-12, 2020Language models are few-shot learners</p>
<p>. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, David Sepassi, Shivani Dohan, Mark Agrawal, Omernick, 10.48550/arXiv.2204.02311M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas EckJeff Dean, Slav Petrovand Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311</p>
<p>. Zewei Chu, Mingda Chen, Jing Chen, Miaosen Wang, Kevin Gimpel, Manaal Faruqui, Xiance Si, 2020</p>
<p>How to ask better questions? A large-scale multidomain dataset for rewriting ill-formed questions. The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference. New York, NY, USAAAAI PressFebruary 7-12, 20202020The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</p>
<p>. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Sharan Chowdhery, Gaurav Narang, Adams Mishra, Vincent Y Yu, Yanping Zhao, Andrew M Huang, Hongkun Dai, Slav Yu, Ed H Petrov, Jeff Chi, Jacob Dean, Adam Devlin, Denny Roberts, Quoc V Zhou, Jason Le, Wei, 10.48550/arXiv.2210.114162022Scaling instruction-finetuned language models. CoRR, abs/2210.11416</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, CoRR, abs/2110.141682021</p>
<p>Selection-inference: Exploiting large language models for interpretable logical reasoning. Antonia Creswell, Murray Shanahan, Irina Higgins, 10.48550/arXiv.2205.09712CoRR, abs/2205.097122022</p>
<p>Identifying well-formed natural language questions. Manaal Faruqui, Dipanjan Das, 10.18653/v1/d18-1091Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018. October 31 -November 4. 2018</p>
<p>Complexity-based prompting for multi-step reasoning. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot, 10.48550/arXiv.2210.00720CoRR, abs/2210.007202022</p>
<p>An empirical investigation of catastrophic forgeting in gradientbased neural networks. Ian J Goodfellow, Mehdi Mirza, Xia Da, Aaron C Courville, Yoshua Bengio, ICLR 20142nd International Conference on Learning Representations. Banff, AB, Canada2014. April 14-16, 2014Conference Track Proceedings</p>
<p>Measuring mathematical problem solving with the MATH dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021. the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 20212021. December 2021</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, NeurIPS. 2022</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, V Vinay, Ambrose Ramasesh, Slone, Cem Anil, Imanol Schlag. Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra2022In NeurIPS</p>
<p>On the advance of making language models better reasoners. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, 10.48550/arXiv.2206.02336CoRR, abs/2206.023362022</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, 10.18653/v1/P17-1015Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics2017. 2017. July 30 -August 41</p>
<p>What makes good in-context examples for gpt-3?. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, 10.18653/v1/2022.deelio-1.10Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, DeeLIO@ACL 2022. Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, DeeLIO@ACL 2022Dublin, Ireland and OnlineAssociation for Computational Linguistics2022. May 27, 2022</p>
<p>Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp, 10.18653/v1/2022.acl-long.556Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2022. May 22-27, 20221ACL 2022</p>
<p>Rethinking the role of demonstrations: What makes in-context learning work?. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022. December 7-11, 20222022</p>
<p>LILA: A unified benchmark for mathematical reasoning. Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, Ashwin Kalyan, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab Emirates2022. December 7-11, 20222022Association for Computational Linguistics</p>
<p>A survey of the usages of deep learning for natural language processing. Daniel W Otter, Julian R Medina, Jugal K Kalita, 10.1109/TNNLS.2020.2979670IEEE Trans. Neural Networks Learn. Syst. 3222021</p>
<p>. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, </p>
<p>Training language models to follow instructions with human feedback. Sandhini Zhang, Katarina Agarwal, Alex Slama, John Ray, Jacob Schulman, Fraser Hilton, Luke Kelton, Maddie Miller, Amanda Simens, Peter Askell, Paul F Welinder, Christiano, Jan Leike, and Ryan Lowe. 2022In NeurIPS</p>
<p>Continual lifelong learning with neural networks: A review. Ignacio German, Ronald Parisi, Jose L Kemker, Christopher Part, Stefan Kanan, Wermter, 10.1016/j.neunet.2019.01.012Neural Networks. 1132019</p>
<p>Are NLP models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, 10.18653/v1/2021.naacl-main.168Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021Association for Computational Linguistics2021. June 6-11, 2021</p>
<p>Reasoning with language model prompting: A survey. Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen, 10.48550/arXiv.2212.09597CoRR, abs/2212.095972022</p>
<p>. Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George Van Den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat Mcaleese, Amy Wu, Erich Elsen, M Siddhant, Elena Jayakumar, David Buchatskaya, Esme Budden, Karen Sutherland, Michela Simonyan, Laurent Paganini, Lena Sifre, Xiang Martens, Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew J. Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem AyoubKoray KavukcuogluJeff Stanway, Lorrayne Bennett, Demis HassabisCyprien de Masson d'Autumeand Geoffrey Irving. 2021. Scaling language models: Methods, analysis &amp; insights from training gopher. CoRR, abs/2112.11446</p>
<p>Solving general arithmetic word problems. Subhro Roy, Dan Roth, 10.18653/v1/d15-1202Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalThe Association for Computational Linguistics2015. 2015. September 17-21, 2015</p>
<p>Multitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, Canwen Saiful Bari, Urmish Xu, Shanya Thakker, Eliza Sharma Sharma, Taewoon Szczechla, Gunjan Kim, Chhablani, V Nihal, Debajyoti Nayak, Jonathan Datta, Mike Chang, Tian-Jian, Han Jiang, Matteo Wang, Sheng Manica, Zheng Xin Shen, Harshit Yong, Rachel Pandey, Thomas Bawden, Wang, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Févry, Jason , Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, Alexander M Rush, 2022. April 25-29, 2022OpenReview.net</p>
<p>Are emergent abilities of large language models a mirage?. Rylan Schaeffer, Brando Miranda, Sanmi Koyejo, 10.48550/arXiv.2304.15004CoRR, abs/2304.150042023</p>
<p>Large language models can be easily distracted by irrelevant context. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, Denny Zhou, 10.48550/arXiv.2302.00093CoRR, abs/2302.000932023</p>
<p>Effects of question formats on causal judgments and model evaluation. Yiyun Shou, Michael Smithson, 10.3389/fpsyg.2015.00467Frontiers in Psychology. 62015</p>
<p>Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet Rahane, Anantharaman S Iyer, Anders Andreassen, Andrea Santilli, Andreas Stuhlmüller, Andrew M Dai, Andrew La, Andrew K Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, 10.48550/arXiv.2206.04615Ayla KarakasArun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdemand et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. CoRR, abs/2206.04615</p>
<p>Selfconsistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Denny Zhou, 10.48550/arXiv.2203.11171CoRR, abs/2203.111712022</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Dai, V Quoc, Le, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. 2022a. April 25-29, 2022OpenReview.net</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, 2022bIn NeurIPS</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, 10.48550/arXiv.2309.07864CoRR, abs/2309.07864Yongyan Zheng, Xipeng Qiu, Xuanjing Huan, and Tao Gui. 2023. The rise and potential of large language model based agents: A survey. </p>
<p>The unreliability of explanations in few-shot in-context learning. Xi Ye, Greg Durrett, 10.48550/arXiv.2205.03401CoRR, abs/2205.034012022</p>
<p>Star: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah D Goodman, 2022In NeurIPS</p>
<p>SWAG: A large-scale adversarial dataset for grounded commonsense inference. Rowan Zellers, Yonatan Bisk, Roy Schwartz, Yejin Choi, 10.18653/v1/d18-1009Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018. October 31 -November 4. 2018</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, 10.48550/arXiv.2210.03493CoRR, abs/2210.034932022</p>
<p>Progressive-hint prompting improves reasoning in large language models. Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, Yu Li, 10.48550/arXiv.2304.09797CoRR, abs/2304.097972023</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, Ed H Chi, 10.48550/arXiv.2205.10625CoRR, abs/2205.106252022a</p>
<p>Teaching algorithmic reasoning via incontext learning. Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron C Courville, Behnam Neyshabur, Hanie Sedghi, 10.48550/arXiv.2211.09066CoRR, abs/2211.090662022b</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7859 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7859</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7859</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-277313389</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.19092v2.pdf" target="_blank">Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) are increasingly integral to information retrieval (IR), powering ranking, evaluation, and AI-assisted content creation. This widespread adoption necessitates a critical examination of potential biases arising from the interplay between these LLM-based components. This paper synthesizes existing research and presents novel experiment designs that explore how LLM-based rankers and assistants influence LLM-based judges. We provide the first empirical evidence of LLM judges exhibiting significant bias towards LLM-based rankers. Furthermore, we observe limitations in LLM judges'ability to discern subtle system performance differences. Contrary to some previous findings, our preliminary study does not find evidence of bias against AI-generated content. These results highlight the need for a more holistic view of the LLM-driven information ecosystem. To this end, we offer initial guidelines and a research agenda to ensure the reliable use of LLMs in IR evaluation.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7859.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7859.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-v1-Nano_judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini v1 Nano (LLM judge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Small Gemini-family LLM used as an automatic relevance judge in the paper's experiments; demonstrated near-zero or negative agreement with human TREC assessors and was judged unsuitable for relevance judging.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Ad-hoc passage retrieval relevance assessment / reranking</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC Deep Learning 2019 and 2020 (MS MARCO passages)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Gemini v1 Nano</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Commercial Gemini family model (v1 generation, Nano size); judgments performed with UMBRELA-derived prompt on a 4-point scale; sampling: top-p=1, temperature=0.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>TREC assessors (expert manual relevance judgments)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cohen's kappa (graded, average over DL19 & DL20)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>-0.0065</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>near-zero agreement with humans; unsuitability for judging relevance; poor discriminative power; inconsistent with human labels</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>v1 Nano produced Cohen's κ values close to zero or negative (graded: DL19=-0.002, DL20=-0.011) indicating no meaningful agreement with human assessors; Kendall's τ for system ordering was poor and unstable (All-systems τ: DL19=-0.253, DL20=0.011; Oracles-only τ: DL19=-0.067, DL20=0.067). Authors exclude v1 Nano from further analysis due to unsuitability.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Not reported for this (too-small) model; paper notes general LLM-judge benefits elsewhere (speed, scalability).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>UMBRELA-derived best prompt (4-point graded scale), judgments on BM25 top-100 filtered to human-judged pairs, NDCG@10 reporting; Cohen's κ (graded & binary) and Kendall's τ for system ordering computed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7859.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7859.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-v1-Pro_judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini v1 Pro (LLM judge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mid-capability Gemini model used as an automatic judge; showed modest positive agreement with human TREC assessors and moderate ability to reproduce system orderings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Ad-hoc passage retrieval relevance assessment / reranking</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC Deep Learning 2019 and 2020 (MS MARCO passages)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Gemini v1 Pro</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Commercial Gemini family model (v1 generation, Pro size); judgments performed with UMBRELA-derived prompt on a 4-point scale; top-p=1, temperature=0.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>TREC assessors (expert manual relevance judgments)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cohen's kappa (graded, average over DL19 & DL20)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.1415</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>leniency relative to humans; limited sensitivity to subtle system differences; vulnerability to evaluation setup choices (prompt/model choice/order effects)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Graded Cohen's κ: DL19=0.139, DL20=0.144 (avg ≈0.1415). Binary κ is higher (DL19=0.337, DL20=0.273). Kendall's τ (All-systems) modest (DL19=0.077, DL20=0.121); Oracles-only ordering much stronger (DL19=0.600, DL20=0.867). Authors observe that while v1 Pro is usable, it is more lenient than humans and sometimes fails to detect small but statistically significant differences.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Faster/scalable automatic judgments compared to human labeling (general advantage noted by paper); can produce reasonable agreement with humans for many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>UMBRELA-derived prompt (4-point scale), top-p=1 temperature=0; judgments compared to TREC labels using Cohen's κ (graded & binary) and Kendall's τ; evaluations report NDCG@10 on BM25 top-100 filtered to human-judged pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7859.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7859.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-v1.5-Flash_judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini v1.5 Flash (LLM judge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Higher-capability Gemini-generation judge that exhibited the best graded agreement among models tested and relatively strong system-order agreement on oracle runs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Ad-hoc passage retrieval relevance assessment / reranking</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC Deep Learning 2019 and 2020 (MS MARCO passages)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Gemini v1.5 Flash</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Commercial Gemini family model (v1.5 generation, Flash size); judgments performed with UMBRELA-derived prompt on a 4-point scale; top-p=1, temperature=0.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>TREC assessors (expert manual relevance judgments)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cohen's kappa (graded, average over DL19 & DL20)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.249</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>leniency (more likely to label partially relevant), limited discriminative power for subtle differences, possible sensitivity to prompt/ordering choices</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Graded Cohen's κ: DL19=0.268, DL20=0.230 (avg ≈0.249). Binary κ higher (DL19=0.461, DL20=0.370). Kendall's τ (All-systems) low (DL19=0.033, DL20=0.143) whereas Oracles-only τ high (DL19=0.600, DL20=0.867). Authors note v1.5 Flash performs clearly better than v1 models for individual judgments but that larger size within v1.5 family does not always mean better.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Higher alignment with human judgments than smaller models; scalable automatic judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>UMBRELA-derived prompt (4-point scale), top-p=1 temperature=0; comparisons with human TREC labels via Cohen's κ (graded/binary) and Kendall's τ; NDCG@10 reported for ranker evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7859.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7859.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-v1.5-Pro_judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini v1.5 Pro (LLM judge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Largest Gemini judge tested; showed competitive agreement with humans and demonstrated both failures to detect subtle true differences and false positives compared to human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Ad-hoc passage retrieval relevance assessment / reranking</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC Deep Learning 2019 and 2020 (MS MARCO passages)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Gemini v1.5 Pro</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Commercial Gemini family model (v1.5 generation, Pro size); judgments performed with UMBRELA-derived prompt on a 4-point scale; top-p=1, temperature=0.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>TREC assessors (expert manual relevance judgments)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cohen's kappa (graded, average over DL19 & DL20)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.198</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>leniency (labeling some non-relevant as partially relevant); limited sensitivity to subtle but statistically significant differences; bias toward outputs of LLM-based rankers; inconsistent detection of statistical significance vs. human labels</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Graded Cohen's κ: DL19=0.204, DL20=0.192 (avg ≈0.198). Binary κ: DL19=0.462, DL20=0.359. Kendall's τ (All-systems) modest (DL19=0.077, DL20=0.143); Oracles-only τ strong (DL19=0.600, DL20=0.867). Authors show examples where v1.5 Pro fails to detect significant differences (human p<0.001) and also flags differences humans did not.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Best single-model performance among those tested on agreement metrics; scalable, reproducible automatic judgments when configured consistently.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>UMBRELA-derived prompt (4-point scale), top-p=1 temperature=0; judgments compared to TREC human labels using Cohen's κ (graded & binary) and Kendall's τ for system ordering; NDCG@10 reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7859.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7859.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Discriminative_ability_oracle_experiment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Oracle-based discriminative ability comparison (LLM judges vs humans)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Controlled experiment comparing LLM judges and human assessors on a set of oracle rankers (perfect ranking and controlled degradations) to test sensitivity of LLM judges to subtle performance differences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Ad-hoc passage retrieval relevance assessment sensitivity testing</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC Deep Learning 2019 and 2020 (MS MARCO passages)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Gemini family (v1 & v1.5 variants used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>See individual Gemini variants (v1 Nano, v1 Pro, v1.5 Flash, v1.5 Pro). Oracle rankers generated by permuting ground-truth ranking to create graded degradations.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>TREC assessors (ground truth)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's tau (system-order agreement; oracles-only reported per judge and year)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>limited discriminative power; inability to reliably order oracle rankings (particularly on DL19) despite all pairwise differences being significant under human judgments; correlation metrics can be misleading when many diverse systems are included</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Authors used oracle rankers with controlled degradations; although pairwise differences between oracle levels were statistically significant according to humans (paired t-test p<0.05), LLM judges often failed to observe those differences or even reversed ordering. The paper highlights that Kendall's τ over all systems can mask failures to detect subtle top-of-leaderboard differences.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Provides a quick, automated signal for discriminative checks but not a reliable substitute for human sensitivity to small effects.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Oracle rankers constructed from TREC labels (Perfect, Swap variants); LLM judges evaluated same BM25 top-100 pools; pairwise statistical significance computed using human labels; meta-evaluation with Kendall's τ reported for 'All systems' and 'Oracles-only'.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7859.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7859.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bias_toward_LLM_rankers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bias of LLM-as-judge toward outputs from LLM-based rankers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical finding that LLM judges systematically favor the outputs of LLM-based rankers over non-LLM methods (including oracle runs), sometimes reversing true human-derived rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Ad-hoc passage retrieval evaluation comparing ranker families</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC Deep Learning 2019 and 2020 (MS MARCO passages)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Gemini family (v1 & v1.5 variants used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>LLM judges (Gemini variants) evaluating reranked lists produced by both LLM-based rankers (RankT5, RG, PRP, Gemini-based reranker) and oracle/non-LLM runs; NDCG@10 reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>TREC assessors (ground truth)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>N/A (ranking reversals / qualitative rank-order reversal reported)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>systematic favoritism of LLM-based rankers by LLM judges; severe underestimation of true performance of non-LLM systems; potential circularity when rankers and judges share modeling family</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Authors provide direct empirical evidence: LLM judges ranked all tested LLM-based rankers above selected oracle runs (human judgments placed oracles above those LLM rankers), in some cases completely reversing the human ranking. The magnitude of the bias was sufficient to change conclusions about which systems were best.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Not an advantage here — this is a documented failure mode that undermines trustworthiness of LLM-only evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Comparisons used BM25 top-100 pools, NDCG@10 reported for each ranker under both human and LLM judges; judgement prompt and settings as per UMBRELA-derived prompt; analysis includes visualizations and oracle controls to separate effects.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7859.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7859.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bias_toward_LLM_generated_text</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bias (or lack thereof) of LLM judges toward LLM-generated (AI-rewritten) text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experiment comparing LLM judge assessments on original human-written passages vs. LLM-rewritten (AI-assisted) versions to test whether judges inflate relevance for AI-generated content; this paper reports no evidence of systematic inflation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Ad-hoc passage relevance assessment comparing original vs LLM-rewritten documents</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Balanced sample (4,000 query-document pairs) from TREC DL19 & DL20 (500 per relevance level per year)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Gemini v1.5 Pro (main judge used for this experiment); Rewriter: Gemini v1.5 Flash</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Rewrites produced with Gemini v1.5 Flash (zero-shot simple rewrite prompt); judgments by Gemini v1.5 Pro using UMBRELA-derived prompt; 4-point graded scale; top-p=1, temp=0.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>TREC assessors (original labels) and prior human verification that rewriting did not change relevance (cited Dai et al. [10])</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>NDCG@10 difference (Perfect Oracle evaluated on Rewritten vs Original); distributional label shift examined</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>No observed systematic inflation for LLM-generated text in this setup; slight increase in lower relevance labels for rewritten texts in one analysis; no statistically significant change in Perfect Oracle NDCG@10 when replacing Original with Rewritten (DL19: 0.868 vs 0.883; DL20: 0.825 vs 0.818).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Distributional analysis showed LLM judge was lenient overall but did not shift towards higher relevance for LLM-rewritten text; in ranking context, Perfect Oracle NDCG@10 differences between Rewritten and Original were small and not statistically significant. Authors emphasize this is a preliminary result specific to the combination of rewriter and judge used.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Not an advantage per se; finding alleviates one hypothesized bias (preference for LLM-generated text) in this experimental configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Balanced sample of 4,000 pairs; rewrites created zero-shot with Gemini v1.5 Flash; judged by Gemini v1.5 Pro; comparisons include label distribution plots and NDCG@10 for Perfect Oracle on Original vs Rewritten; statistical significance tests reported (no significant differences).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Perspectives on Large Language Models for Relevance Judgment <em>(Rating: 2)</em></li>
                <li>A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look <em>(Rating: 2)</em></li>
                <li>LLM-based relevance assessment still can't replace human relevance assessment <em>(Rating: 2)</em></li>
                <li>Neural Retrievers are Biased Towards LLM-Generated Content <em>(Rating: 2)</em></li>
                <li>LLM Evaluators Recognize and Favor Their Own Generations <em>(Rating: 2)</em></li>
                <li>LLMs can be Fooled into Labelling a Document as Relevant <em>(Rating: 2)</em></li>
                <li>UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7859",
    "paper_id": "paper-277313389",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "Gemini-v1-Nano_judge",
            "name_full": "Gemini v1 Nano (LLM judge)",
            "brief_description": "Small Gemini-family LLM used as an automatic relevance judge in the paper's experiments; demonstrated near-zero or negative agreement with human TREC assessors and was judged unsuitable for relevance judging.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation",
            "evaluation_task": "Ad-hoc passage retrieval relevance assessment / reranking",
            "dataset_name": "TREC Deep Learning 2019 and 2020 (MS MARCO passages)",
            "judge_model_name": "Gemini v1 Nano",
            "judge_model_details": "Commercial Gemini family model (v1 generation, Nano size); judgments performed with UMBRELA-derived prompt on a 4-point scale; sampling: top-p=1, temperature=0.",
            "human_evaluator_type": "TREC assessors (expert manual relevance judgments)",
            "agreement_metric": "Cohen's kappa (graded, average over DL19 & DL20)",
            "agreement_score": -0.0065,
            "reported_loss_aspects": "near-zero agreement with humans; unsuitability for judging relevance; poor discriminative power; inconsistent with human labels",
            "qualitative_findings": "v1 Nano produced Cohen's κ values close to zero or negative (graded: DL19=-0.002, DL20=-0.011) indicating no meaningful agreement with human assessors; Kendall's τ for system ordering was poor and unstable (All-systems τ: DL19=-0.253, DL20=0.011; Oracles-only τ: DL19=-0.067, DL20=0.067). Authors exclude v1 Nano from further analysis due to unsuitability.",
            "advantages_of_llm_judge": "Not reported for this (too-small) model; paper notes general LLM-judge benefits elsewhere (speed, scalability).",
            "experimental_setting": "UMBRELA-derived best prompt (4-point graded scale), judgments on BM25 top-100 filtered to human-judged pairs, NDCG@10 reporting; Cohen's κ (graded & binary) and Kendall's τ for system ordering computed.",
            "uuid": "e7859.0",
            "source_info": {
                "paper_title": "Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Gemini-v1-Pro_judge",
            "name_full": "Gemini v1 Pro (LLM judge)",
            "brief_description": "Mid-capability Gemini model used as an automatic judge; showed modest positive agreement with human TREC assessors and moderate ability to reproduce system orderings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation",
            "evaluation_task": "Ad-hoc passage retrieval relevance assessment / reranking",
            "dataset_name": "TREC Deep Learning 2019 and 2020 (MS MARCO passages)",
            "judge_model_name": "Gemini v1 Pro",
            "judge_model_details": "Commercial Gemini family model (v1 generation, Pro size); judgments performed with UMBRELA-derived prompt on a 4-point scale; top-p=1, temperature=0.",
            "human_evaluator_type": "TREC assessors (expert manual relevance judgments)",
            "agreement_metric": "Cohen's kappa (graded, average over DL19 & DL20)",
            "agreement_score": 0.1415,
            "reported_loss_aspects": "leniency relative to humans; limited sensitivity to subtle system differences; vulnerability to evaluation setup choices (prompt/model choice/order effects)",
            "qualitative_findings": "Graded Cohen's κ: DL19=0.139, DL20=0.144 (avg ≈0.1415). Binary κ is higher (DL19=0.337, DL20=0.273). Kendall's τ (All-systems) modest (DL19=0.077, DL20=0.121); Oracles-only ordering much stronger (DL19=0.600, DL20=0.867). Authors observe that while v1 Pro is usable, it is more lenient than humans and sometimes fails to detect small but statistically significant differences.",
            "advantages_of_llm_judge": "Faster/scalable automatic judgments compared to human labeling (general advantage noted by paper); can produce reasonable agreement with humans for many cases.",
            "experimental_setting": "UMBRELA-derived prompt (4-point scale), top-p=1 temperature=0; judgments compared to TREC labels using Cohen's κ (graded & binary) and Kendall's τ; evaluations report NDCG@10 on BM25 top-100 filtered to human-judged pairs.",
            "uuid": "e7859.1",
            "source_info": {
                "paper_title": "Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Gemini-v1.5-Flash_judge",
            "name_full": "Gemini v1.5 Flash (LLM judge)",
            "brief_description": "Higher-capability Gemini-generation judge that exhibited the best graded agreement among models tested and relatively strong system-order agreement on oracle runs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation",
            "evaluation_task": "Ad-hoc passage retrieval relevance assessment / reranking",
            "dataset_name": "TREC Deep Learning 2019 and 2020 (MS MARCO passages)",
            "judge_model_name": "Gemini v1.5 Flash",
            "judge_model_details": "Commercial Gemini family model (v1.5 generation, Flash size); judgments performed with UMBRELA-derived prompt on a 4-point scale; top-p=1, temperature=0.",
            "human_evaluator_type": "TREC assessors (expert manual relevance judgments)",
            "agreement_metric": "Cohen's kappa (graded, average over DL19 & DL20)",
            "agreement_score": 0.249,
            "reported_loss_aspects": "leniency (more likely to label partially relevant), limited discriminative power for subtle differences, possible sensitivity to prompt/ordering choices",
            "qualitative_findings": "Graded Cohen's κ: DL19=0.268, DL20=0.230 (avg ≈0.249). Binary κ higher (DL19=0.461, DL20=0.370). Kendall's τ (All-systems) low (DL19=0.033, DL20=0.143) whereas Oracles-only τ high (DL19=0.600, DL20=0.867). Authors note v1.5 Flash performs clearly better than v1 models for individual judgments but that larger size within v1.5 family does not always mean better.",
            "advantages_of_llm_judge": "Higher alignment with human judgments than smaller models; scalable automatic judgments.",
            "experimental_setting": "UMBRELA-derived prompt (4-point scale), top-p=1 temperature=0; comparisons with human TREC labels via Cohen's κ (graded/binary) and Kendall's τ; NDCG@10 reported for ranker evaluations.",
            "uuid": "e7859.2",
            "source_info": {
                "paper_title": "Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Gemini-v1.5-Pro_judge",
            "name_full": "Gemini v1.5 Pro (LLM judge)",
            "brief_description": "Largest Gemini judge tested; showed competitive agreement with humans and demonstrated both failures to detect subtle true differences and false positives compared to human judgments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation",
            "evaluation_task": "Ad-hoc passage retrieval relevance assessment / reranking",
            "dataset_name": "TREC Deep Learning 2019 and 2020 (MS MARCO passages)",
            "judge_model_name": "Gemini v1.5 Pro",
            "judge_model_details": "Commercial Gemini family model (v1.5 generation, Pro size); judgments performed with UMBRELA-derived prompt on a 4-point scale; top-p=1, temperature=0.",
            "human_evaluator_type": "TREC assessors (expert manual relevance judgments)",
            "agreement_metric": "Cohen's kappa (graded, average over DL19 & DL20)",
            "agreement_score": 0.198,
            "reported_loss_aspects": "leniency (labeling some non-relevant as partially relevant); limited sensitivity to subtle but statistically significant differences; bias toward outputs of LLM-based rankers; inconsistent detection of statistical significance vs. human labels",
            "qualitative_findings": "Graded Cohen's κ: DL19=0.204, DL20=0.192 (avg ≈0.198). Binary κ: DL19=0.462, DL20=0.359. Kendall's τ (All-systems) modest (DL19=0.077, DL20=0.143); Oracles-only τ strong (DL19=0.600, DL20=0.867). Authors show examples where v1.5 Pro fails to detect significant differences (human p&lt;0.001) and also flags differences humans did not.",
            "advantages_of_llm_judge": "Best single-model performance among those tested on agreement metrics; scalable, reproducible automatic judgments when configured consistently.",
            "experimental_setting": "UMBRELA-derived prompt (4-point scale), top-p=1 temperature=0; judgments compared to TREC human labels using Cohen's κ (graded & binary) and Kendall's τ for system ordering; NDCG@10 reported.",
            "uuid": "e7859.3",
            "source_info": {
                "paper_title": "Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Discriminative_ability_oracle_experiment",
            "name_full": "Oracle-based discriminative ability comparison (LLM judges vs humans)",
            "brief_description": "Controlled experiment comparing LLM judges and human assessors on a set of oracle rankers (perfect ranking and controlled degradations) to test sensitivity of LLM judges to subtle performance differences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation",
            "evaluation_task": "Ad-hoc passage retrieval relevance assessment sensitivity testing",
            "dataset_name": "TREC Deep Learning 2019 and 2020 (MS MARCO passages)",
            "judge_model_name": "Gemini family (v1 & v1.5 variants used in experiments)",
            "judge_model_details": "See individual Gemini variants (v1 Nano, v1 Pro, v1.5 Flash, v1.5 Pro). Oracle rankers generated by permuting ground-truth ranking to create graded degradations.",
            "human_evaluator_type": "TREC assessors (ground truth)",
            "agreement_metric": "Kendall's tau (system-order agreement; oracles-only reported per judge and year)",
            "agreement_score": null,
            "reported_loss_aspects": "limited discriminative power; inability to reliably order oracle rankings (particularly on DL19) despite all pairwise differences being significant under human judgments; correlation metrics can be misleading when many diverse systems are included",
            "qualitative_findings": "Authors used oracle rankers with controlled degradations; although pairwise differences between oracle levels were statistically significant according to humans (paired t-test p&lt;0.05), LLM judges often failed to observe those differences or even reversed ordering. The paper highlights that Kendall's τ over all systems can mask failures to detect subtle top-of-leaderboard differences.",
            "advantages_of_llm_judge": "Provides a quick, automated signal for discriminative checks but not a reliable substitute for human sensitivity to small effects.",
            "experimental_setting": "Oracle rankers constructed from TREC labels (Perfect, Swap variants); LLM judges evaluated same BM25 top-100 pools; pairwise statistical significance computed using human labels; meta-evaluation with Kendall's τ reported for 'All systems' and 'Oracles-only'.",
            "uuid": "e7859.4",
            "source_info": {
                "paper_title": "Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Bias_toward_LLM_rankers",
            "name_full": "Bias of LLM-as-judge toward outputs from LLM-based rankers",
            "brief_description": "Empirical finding that LLM judges systematically favor the outputs of LLM-based rankers over non-LLM methods (including oracle runs), sometimes reversing true human-derived rankings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation",
            "evaluation_task": "Ad-hoc passage retrieval evaluation comparing ranker families",
            "dataset_name": "TREC Deep Learning 2019 and 2020 (MS MARCO passages)",
            "judge_model_name": "Gemini family (v1 & v1.5 variants used in experiments)",
            "judge_model_details": "LLM judges (Gemini variants) evaluating reranked lists produced by both LLM-based rankers (RankT5, RG, PRP, Gemini-based reranker) and oracle/non-LLM runs; NDCG@10 reported.",
            "human_evaluator_type": "TREC assessors (ground truth)",
            "agreement_metric": "N/A (ranking reversals / qualitative rank-order reversal reported)",
            "agreement_score": null,
            "reported_loss_aspects": "systematic favoritism of LLM-based rankers by LLM judges; severe underestimation of true performance of non-LLM systems; potential circularity when rankers and judges share modeling family",
            "qualitative_findings": "Authors provide direct empirical evidence: LLM judges ranked all tested LLM-based rankers above selected oracle runs (human judgments placed oracles above those LLM rankers), in some cases completely reversing the human ranking. The magnitude of the bias was sufficient to change conclusions about which systems were best.",
            "advantages_of_llm_judge": "Not an advantage here — this is a documented failure mode that undermines trustworthiness of LLM-only evaluation.",
            "experimental_setting": "Comparisons used BM25 top-100 pools, NDCG@10 reported for each ranker under both human and LLM judges; judgement prompt and settings as per UMBRELA-derived prompt; analysis includes visualizations and oracle controls to separate effects.",
            "uuid": "e7859.5",
            "source_info": {
                "paper_title": "Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Bias_toward_LLM_generated_text",
            "name_full": "Bias (or lack thereof) of LLM judges toward LLM-generated (AI-rewritten) text",
            "brief_description": "Experiment comparing LLM judge assessments on original human-written passages vs. LLM-rewritten (AI-assisted) versions to test whether judges inflate relevance for AI-generated content; this paper reports no evidence of systematic inflation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation",
            "evaluation_task": "Ad-hoc passage relevance assessment comparing original vs LLM-rewritten documents",
            "dataset_name": "Balanced sample (4,000 query-document pairs) from TREC DL19 & DL20 (500 per relevance level per year)",
            "judge_model_name": "Gemini v1.5 Pro (main judge used for this experiment); Rewriter: Gemini v1.5 Flash",
            "judge_model_details": "Rewrites produced with Gemini v1.5 Flash (zero-shot simple rewrite prompt); judgments by Gemini v1.5 Pro using UMBRELA-derived prompt; 4-point graded scale; top-p=1, temp=0.",
            "human_evaluator_type": "TREC assessors (original labels) and prior human verification that rewriting did not change relevance (cited Dai et al. [10])",
            "agreement_metric": "NDCG@10 difference (Perfect Oracle evaluated on Rewritten vs Original); distributional label shift examined",
            "agreement_score": null,
            "reported_loss_aspects": "No observed systematic inflation for LLM-generated text in this setup; slight increase in lower relevance labels for rewritten texts in one analysis; no statistically significant change in Perfect Oracle NDCG@10 when replacing Original with Rewritten (DL19: 0.868 vs 0.883; DL20: 0.825 vs 0.818).",
            "qualitative_findings": "Distributional analysis showed LLM judge was lenient overall but did not shift towards higher relevance for LLM-rewritten text; in ranking context, Perfect Oracle NDCG@10 differences between Rewritten and Original were small and not statistically significant. Authors emphasize this is a preliminary result specific to the combination of rewriter and judge used.",
            "advantages_of_llm_judge": "Not an advantage per se; finding alleviates one hypothesized bias (preference for LLM-generated text) in this experimental configuration.",
            "experimental_setting": "Balanced sample of 4,000 pairs; rewrites created zero-shot with Gemini v1.5 Flash; judged by Gemini v1.5 Pro; comparisons include label distribution plots and NDCG@10 for Perfect Oracle on Original vs Rewritten; statistical significance tests reported (no significant differences).",
            "uuid": "e7859.6",
            "source_info": {
                "paper_title": "Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Perspectives on Large Language Models for Relevance Judgment",
            "rating": 2,
            "sanitized_title": "perspectives_on_large_language_models_for_relevance_judgment"
        },
        {
            "paper_title": "A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look",
            "rating": 2,
            "sanitized_title": "a_largescale_study_of_relevance_assessments_with_large_language_models_an_initial_look"
        },
        {
            "paper_title": "LLM-based relevance assessment still can't replace human relevance assessment",
            "rating": 2,
            "sanitized_title": "llmbased_relevance_assessment_still_cant_replace_human_relevance_assessment"
        },
        {
            "paper_title": "Neural Retrievers are Biased Towards LLM-Generated Content",
            "rating": 2,
            "sanitized_title": "neural_retrievers_are_biased_towards_llmgenerated_content"
        },
        {
            "paper_title": "LLM Evaluators Recognize and Favor Their Own Generations",
            "rating": 2,
            "sanitized_title": "llm_evaluators_recognize_and_favor_their_own_generations"
        },
        {
            "paper_title": "LLMs can be Fooled into Labelling a Document as Relevant",
            "rating": 2,
            "sanitized_title": "llms_can_be_fooled_into_labelling_a_document_as_relevant"
        },
        {
            "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
            "rating": 2,
            "sanitized_title": "umbrela_umbrela_is_the_opensource_reproduction_of_the_bing_relevance_assessor"
        }
    ],
    "cost": 0.018057249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation
9 Jul 2025</p>
<p>Krisztian Balog krisztianb@google.com 
Google DeepMind Stavanger
Norway</p>
<p>Donald Metzler metzler@google.com 
Google DeepMind Mountain View
USA</p>
<p>Zhen Qin zhenqin@google.com 
Google DeepMind Mountain View
USA</p>
<p>Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation
9 Jul 2025BF7D567ADF9B70DFAA87C6D7DA60FC8310.1145/3726302.3730348arXiv:2503.19092v2[cs.IR]Large language modelsrankingevaluation
Large language models (LLMs) are increasingly integral to information retrieval (IR), powering ranking, evaluation, and AI-assisted content creation.This widespread adoption necessitates a critical examination of potential biases arising from the interplay between these LLM-based components.This paper synthesizes existing research and presents novel experiment designs that explore how LLM-based rankers and assistants influence LLM-based judges.We provide the first empirical evidence of LLM judges exhibiting significant bias towards LLM-based rankers.Furthermore, we observe limitations in LLM judges' ability to discern subtle system performance differences.Contrary to some previous findings, our preliminary study does not find evidence of bias against AI-generated content.These results highlight the need for a more holistic view of the LLM-driven information ecosystem.To this end, we offer initial guidelines and a research agenda to ensure the reliable use of LLMs in IR evaluation.CCS Concepts• Information systems → Information retrieval.</p>
<p>Introduction</p>
<p>Due to their remarkable capabilities, large language models (LLMs) are fundamentally reshaping the field of information retrieval (IR), becoming integral to core ranking algorithms and the automation of evaluation processes.Beyond their role in core IR processes, LLMs are also powering AI assistants that are rapidly changing how users generate content, from writing emails and articles to creating code and translating content between languages.As the reliance on LLMs is expected to deepen given their potential, it is increasingly crucial to maintain a balanced perspective by assessing and acknowledging the potential risks alongside the undeniable benefits.Could this heavy reliance on LLMs across content creation, retrieval, ranking, evaluation, etc., inadvertently introduce or amplify biases within these systems?</p>
<p>Recent research has begun to explore some of these emerging issues.For example, studies have shown that LLMs can exhibit biases in their output, favoring LLM-generated content over human-generated ones [10], and perpetuating biases present in their training data [15,21,33].Furthermore, LLM-based rating systems have been found to be susceptible to manipulation [2], may not accurately reflect human preferences [28], and demonstrate self-inconsistency [50].Additionally, the phenomenon of "model collapse" has also been observed, where LLMs trained on synthetic data generated by other LLMs can lead to a degradation of quality and diversity in generated content [47].</p>
<p>Within the IR research community, the use of LLMs for assessment is a subject of ongoing debate [14], with opinions ranging from complete rejection of LLMs for relevance assessment [48] to the assertion that they can fully replace human judgments [55].Investigations have thus far focused on the agreement of LLMgenerated ratings with human assessments [14,54,55] and the potential for LLMs to introduce biases in search results [10].However, a comprehensive analysis of the implications of LLMs across the entire information ecosystem, from content creation with AI assistance to LLM-based reranking and LLM-based judges for evaluation, remains a critical gap in the current literature.</p>
<p>This paper aims to advance our understanding of these issues by synthesizing prior research and, crucially, providing novel empirical evidence.We specifically focus on the novel challenge of understanding the effect LLM-based rankers and AI-powered content creation have on an LLM-based judge's ability to accurately assess relevance.Prior work has separately noted the potential interaction between LLM-based rankers and judges [14,30,42,54] (an interaction that has yet to be empirically investigated), while other initial work has explored the relationship between AI-powered content creation and rankers [10].However, we argue that the complex interplay between each of these roles must be considered holistically to fully understand the potential implications of widespread adoption of LLM-based judges.We present initial results demonstrating the importance of this interconnected perspective, showcasing how the use of LLMs across the information lifecycle can influence the accuracy and potential biases of LLM judges.</p>
<p>We start by considering the case of LLMs being used as both rankers and judges and present the first empirical demonstration of a significant bias of LLM judges towards LLM-based rankers.Novel to our approach is the examination of LLM judge performance via the use of oracle rankers, allowing for a controlled assessment of LLM judge behavior and discriminative ability.Using the TREC 2019 and 2020 Deep Learning track datasets, we conduct experiments that also compare different-sized LLM judges within the same model family.Our results reveal several key findings: (1) LLM judges are more lenient in their relevance assessments than human judges, confirming previous observations [55]; (2) LLM judges exhibit a significant bias towards LLM-based rankers, a phenomenon previously only hypothesized; and (3) LLM judges demonstrate limited ability to discern subtle, yet statistically significant, performance differences between systems.Additionally, we conduct a preliminary study into whether LLM judges demonstrate biases when they encounter AI-generated content.Contrary to some previously published findings [25,27,37], our experiments do not provide evidence of this bias, suggesting that deeper, more rigorous empirical evaluations are required to better understand this phenomenon.What emerges from these targeted studies is a better picture of how different interactions between LLM-based components give rise to different behaviors within LLM-based judges.Taken together, our findings yield one of the most holistic views of this problem space, provide unique insights into best practices for leveraging LLMs as judges, and motivate a rich set of future research questions that will need to be answered to understand the complexities of these interactions even better.</p>
<p>In summary, the primary contributions of this paper are: (1) a review of how LLMs are currently used in IR, bringing attention to the interconnected roles they play, and synthesizing the current understanding of their interactions; (2) experiments that highlight how interactions between LLMs might result in inaccurate or biased assessments of retrieval effectiveness; (3) a preliminary set of guidelines for using LLMs in IR evaluation; and (4) a research agenda aimed at sparking further discussion and research along this emerging direction.</p>
<p>Background</p>
<p>This section overviews the main uses of LLMs in information access, illustrated in Fig. 1, providing context for subsequent analysis of the interplay between some of these uses.</p>
<p>LLMs as Rankers.</p>
<p>In modern large-scale IR systems, a multistage retrieve-then-rerank pipeline has become a prominent approach, wherein an initial retrieval stage, often based on lexical matching or embedding-based methods, is followed by one or multiple reranking stages, utilizing more sophisticated models to refine the results.This reranking stage frequently employs LLMs, either fine-tuned for the task of ranking [34,35,39,64] or via prompting in a pointwise [12,20,44], pairwise [40], or listwise [29,51] fashion.Dai et al. [10] present results suggesting an inherent bias in neural retrieval models toward LLM-generated texts.This source bias may stem from shared Transformer-based architectures and pretraining approaches, and can lead to "semantic shortcuts" during matching.Neural IR models are also shown to be vulnerable to adversarial attacks, such as keyword stuffing and content injection [38,52].</p>
<p>LLMs as Judges.</p>
<p>Early LLMs, such as BERT, have been utilized for measuring the distributional similarity of texts [60,61] and for evaluating specific tasks via fine-tuning, including machine translation [63], text summarization [26], and question answering [31].The arrival of generative LLMs, such as ChatGPT, have enabled various data labeling and annotation tasks [17].The use of LLMs as surrogates for humans for evaluation, often referred to as "LLM-as-a-Judge" [62], now extends across virtually all natural language processing tasks, including text summarization and dialog response generation [18].However, recent research increasingly demonstrates their limitations, such as favoring longer responses (length bias) [13,58] or content generated by similar models (self bias) [27,59].Our interest is specifically in the use of LLMs for relevance assessments in IR.MacAvaney and Soldaini [30] is among the first to employ LLMs for automatic relevance labeling.They specifically focus on a setting where a single known relevant document per query is available for evaluation and explore several one-shot approaches.Faggioli et al. [14] present a spectrum of human-machine collaboration for producing relevance assessments, from AI assistance to fully automated judgments.They conduct a preliminary assessment of LLMs' capabilities of relevance judgments on two TREC collections and report a fair agreement between human assessors and LLMs.Thomas et al. [54] experiment with various prompt templates to improve quality and observe better agreement with the official TREC labels than Faggioli et al. [14].These improvements are attributed to both prompt design and the use of a more capable LLM.Thomas et al. [54] further share experiences on using LLMs for relevance assessment at Microsoft Bing, where LLMs have reportedly been used, in conjunction with expert human labelers, since late 2022.Upadhyay et al. [56] reproduce results from Thomas et al. [54], verifying their claims, and create an open-source implementation (UMBRELA).Most recently, LLMs are leveraged in the TREC 2024 Retrieval Augmented Generation (RAG) track for automatic relevance assessment [55].Relative system rankings are found to correlate with those obtained using human judgments, even if human assessors apply stricter relevance criteria than LLMs [55].The authors also experiment with various LLM-assisted labeling processes, such as using UMBRELA to pre-filter the pools or to suggest relevance labels that human judges can then post-edit, but find that those solutions "do not appear to have obvious tangible benefits over fully automatic processes" [55].Clarke and Dietz [6] raise concerns about the claims made by Upadhyay et al. [55] and highlight how LLM-based judgments fail to demonstrate strong alignment with manual judgments for top-performing systems.They further present evidence that when evaluation is performed through a publicly known automatic process, such as UMBRELA, it can be subject to manipulation.Chen et al. [5] show that when performing relevance assessments in batches, the relevance levels of earlier documents in a batch influences the relevance judgments of subsequent documents, and that some LLMs are more affected by this so-called threshold priming effect than others.Alaofi et al. [2] compare various open-source and proprietary LLMs in labeling passages for relevance.They demonstrate that most LLMs exhibit some degree of susceptibility to judging non-relevant documents as relevant if query words are inserted at random positions, simulating a keyword stuffing SEO strategy.Rahmani et al. [43] present a largescale synthetic passage ranking collection, SnyDL, by extending the TREC 2019-2023 Deep Learning collections via LLM-generated labels, and observe a high agreement on system ordering.</p>
<p>LLMs as Assistants.There is a wide array of AI tools available to aid people with content creation.Focusing only on textual content, the spectrum ranges from basic grammar and spell checkers to advanced tools that generate full articles.Studies indicate that by late 2024, LLM assistance is detectable in a significant portion of various text domains, with estimates reaching up to 18% of financial consumer complaints and 24% in corporate press releases [22].The use of powerful LLMs can lead to situations where it is unclear whether the content is primarily human-created with AI assistance or the other way around.</p>
<p>LLMs for Data Augmentation.While not considered for this role in the current paper, LLMs are also used for data augmentation.For example, Dai et al. [11] use few-shot prompting to generate synthetic queries, while Bonifacio et al. [3] consider query generation in a full unsupervised setting.Soudani et al. [49] present a survey on synthetic dialogue data generation in open-domain, task-oriented, and information seeking dialogue systems.The use of LLM-generated data brings forth new challenges in bias and unfairness, potentially affecting the reliability of IR systems [9].</p>
<p>Critical Issues with LLMs as Judges</p>
<p>While LLMs offer promising capabilities for automated evaluation in IR, a growing body of research highlights potential limitations and raises critical concerns about their widespread adoption as judges.This section synthesizes findings from prior work, identifying key challenges that warrant further investigation.We categorize these challenges into two broad areas: the quality of LLM judgments (Section 3.1) and the vulnerability of LLM judges to bias and manipulation (Section 3.2).Within these areas, we discuss specific issues related to validity, discriminative power, reliability, reproducibility, susceptibility to manipulation, and systemic biases.These issues, if unaddressed, could undermine the integrity of IR evaluation and potentially lead to misleading conclusions about system performance.This section discusses these critical issues, while Section 4 presents initial experiments designed to provide empirically-driven insight into each of the issues and Section 5 touches upon the fundamental issue of whether, and how, LLM judges should be used in practice.</p>
<p>Quality of Judgments</p>
<p>The fundamental question underlying the use of LLMs as judges is whether their judgments accurately reflect "true" relevance and effectively differentiate between systems of varying quality.We break down this question of quality along two sub-dimensions: validity and discriminative power and reliability and reproducibility.</p>
<p>Validity and Discriminative</p>
<p>Power.For LLMs to serve as effective judges, their assessments must align with human judgments of relevance.Existing research measures this in two ways: (1) agreement on individual document-query relevance labels and (2) agreement on the relative ranking of a set of systems.</p>
<p>• Agreement on Individual Relevance Judgments: Several studies demonstrated that it is possible to use LLMs for relevance assessment and obtain performance comparable to TREC judges [14,55] and notably better than crowd judges [54].At the same time, it has also been observed that LLMs are more lenient when labeling a document relevant [2,55], which leads to inflated evaluation scores.This leniency can lead to inflated evaluation scores, potentially masking subtle differences between systems.• Agreement on System Rankings: A common approach to metaevaluating LLM judges is to compare the relative ranking of retrieval systems based on LLM assessments with the ranking based on human-generated relevance judgments.This typically involves calculating the correlation between the two rankings, often using systems submitted to TREC tracks [14,30,55].While high correlation is often interpreted as evidence of LLM judge validity, this approach has significant limitations.</p>
<p>Issue #1: Discriminative Ability and the Limits of Correlation</p>
<p>Even though several studies report on strong leaderboard correlation between human and LLM judgments, Clarke and Dietz [6] argue that Kendall's  is "less informative for assessing progress at the top of the leaderboard" and demonstrate that LLM-based assessments fail to reliably identify the best-performing systems.Further, Alaofi et al. [2] show that correlation-based meta-evaluation hides interesting failure patterns.A crucial, often overlooked, aspect is the disconnect between typical TREC evaluation setups and the needs of many practical IR scenarios.TREC evaluations often involve dozens of systems with widely varying approaches and performance levels.In contrast, practitioners often need to compare a small number of high-performing (state-of-the-art) systems or distinguish between subtle variations of a single system (e.g., in ablation studies).It remains an open question whether LLM judges possess the necessary sensitivity to reliably detect small but meaningful performance differences in such scenarios.Indeed, achieving high correlation is inherently easier with a larger and more diverse set of systems; simply including more systems with varying performance levels can artificially inflate correlation, even if the LLM judge struggles to differentiate between the top contenders.</p>
<p>It thus remains an open question:</p>
<p>Can LLM judges reliably distinguish between high-performing systems with small, but meaningful, performance differences?</p>
<p>3.1.2Reliability and Reproducibility.Beyond validity, a critical concern for LLM-based evaluation is the reliability and reproducibility of the judgments.Even if an LLM demonstrates a reasonable level of agreement with human judgments on average, its utility as a judge is undermined if its assessments are highly sensitive to seemingly minor variations in setup or input.Indeed, existing research demonstrates that LLM judgments can be significantly influenced by factors such as the choice of LLM [2,5,14], the specific wording and structure of the prompt [2,54], and even the order in which documents are judged [5].This variability raises concerns about the reliability of results obtained with a single LLM.</p>
<p>Issue #2: The Impact of Model Choice A recurring theme in the literature is that more powerful LLMs (typically larger models with more parameters and trained on larger datasets) tend to exhibit better performance and consistency as judges [2].This raises a crucial, but largely unexplored, question: To what extent would the conclusions of a study change if a more (or less) powerful LLM were used as the judge?This sensitivity to model choice has not been systematically investigated, particularly in the context of comparing high-performing systems where subtle differences matter.</p>
<p>Vulnerability to Bias and Manipulation</p>
<p>Beyond the inherent quality of judgments, a separate set of concerns revolves around the potential for LLMs to be biased or manipulated, thereby impacting evaluation outcomes.</p>
<p>Vulnerability to Manipulation.</p>
<p>A significant concern with the adoption of LLMs as judges is their potential vulnerability to adversarial manipulation.Initial research suggests that LLM judges might be vulnerable to keyword stuffing and other SEO strategies [2].More broadly, knowledge of the (characteristics of the) LLM judge opens up ways to manipulate benchmarking results.This could lead to situations where a system achieves much higher scores under automatic evaluation with the LLM judge than under manual assessment [6].This "eval hacking" undermines the purpose of evaluation, which is to accurately assess the true utility of a system for users.</p>
<p>Issue #3: Understanding and Mitigating Vulnerabilities of LLM Judges While initial studies demonstrate the possibility of manipulating LLM judges, the extent of this vulnerability across different LLMs, attack strategies, and IR tasks remains largely unknown.What specific vulnerabilities do LLM judges exhibit, and how do these vulnerabilities vary across different models and evaluation settings?Furthermore, How can we design evaluation protocols that are robust to manipulation, ensuring that LLM-based evaluation remains a reliable and trustworthy measure of system performance?This is a crucial area for future research.</p>
<p>Systematic Biases.</p>
<p>A core challenge in using LLMs for both ranking and evaluation lies in the fundamental similarity of the two tasks: both involve estimating the relevance of a document to a given query.Several studies note the potential for significant systemic biases when LLMs are employed in both roles [14,30,42,54].</p>
<p>In their summary of the LLM4IR workshop, Rahmani et al. [42] state "if we were to use an LLM both as an assessor and as a ranker, we could expect such a model to be favoured over other evaluated models."Faggioli et al. [14] similarly caution that "if the model is used to judge relevance both for annotation and for retrieval, its evaluation would be overinflated, possibly with perfect performance." If both ranking and automatic evaluation are predisposed towards certain types of results, it becomes difficult to identify truly relevant results.This can lead to the suppression of diverse perspectives and the promotion of homogenous content.Novel ranking approaches that deviate from the LLM's inherent understanding of relevance might be unfairly penalized during the assessment phase.This phenomenon shares similarities with "reward hacking" observed in reinforcement learning, where agents exploit loopholes in the reward function to achieve high scores without genuinely solving the underlying task [4].A particularly concerning form of this bias is circularity, where retrieval models are trained on LLM-generated labels [6,14,42].This creates a self-reinforcing loop, where the ranker learns to produce outputs that the LLM judge deems relevant, further amplifying any existing biases.</p>
<p>Issue #4: Interrelated Systemic Biases in LLM-Based Evaluation While the potential for systemic biases in LLM-based IR evaluation is acknowledged, the specific interactions and magnitudes of these biases remain largely unquantified.We identify three interrelated potential biases:</p>
<p>• Bias Towards LLM-Based Rankers: LLM judges might favor the output of systems that also employ LLMs for ranking.While intuitively plausible, this bias needs to be systematically investigated and quantified, independent of the content being retrieved.• Bias Towards LLM-Generated Text: LLM judges might exhibit an inherent preference for text generated by LLMs, regardless of the ranking system that retrieved it.This could be due to factors like stylistic similarities, reduced noise, or other characteristics of LLM-generated text.Indeed, studies have observed that LLMs exhibit bias favoring texts generated by the same underlying model [27,37].However, there is a significant lack of studies systematically quantifying the extent to which LLM judges favor LLM-generated text in the specific context of IR evaluation.• Combined Bias (LLM Ranker + LLM-Generated Text): The most complex scenario involves the potential interaction of the two biases above.Dai et al. [10] show that neural retrievers prefer LLM-generated content, but their analysis relies on human judgments, not LLM judges.Does an LLM judge exhibit an even stronger preference for LLM-generated text when it is retrieved by an LLM-based ranker?This synergistic effect, if present, could significantly distort evaluation outcomes.</p>
<p>It thus remains a set of open questions:</p>
<p>To what extent do LLM judges exhibit biases towards (1) LLM-based rankers, (2) LLM-generated text, and (3) the combination of the two?How do these biases interact, and what is their combined impact on IR evaluation?</p>
<p>Experiments</p>
<p>To empirically demonstrate some of the challenges identified in Section 3, we present a series of targeted experiments aimed at investigating the discriminative ability of LLM judgments (Issue #1), the impact of model choice (Issue #2), and systematic biases (Issue #4).Note that, our goal is to provide illustrative evidence of these issues, rather than a comprehensive or exhaustive analysis.</p>
<p>Experiment Design</p>
<p>We study the classic ad hoc retrieval task where a ranked list of documents are returned in response to a user query.We follow a standard retrieve-then-rerank paradigm, where an initial set of potentially relevant documents is identified by a fast and efficient first stage retriever, which are then subsequently reranked by a computationally more intensive but more accurate model.Our focus lies specifically in this reranking stage, noting that LLMs may also be used for retrieval [53].</p>
<p>We employ a set of rankers built upon progressively more capable LLMs.This allows us to observe how their performance changes as the underlying LLM technology advances and whether LLM judges indeed exhibit bias toward LLM-based rankers.In a novel methodological approach, we also incorporate "oracle" rankings as reference points of comparison.These oracle rankings leverage ground truth human relevance labels to represent a hypothetical perfect ranking system as well as controlled degradations from this ideal.By intentionally degrading the perfect rankings, we create a spectrum of performance levels against which we can compare our LLM-based rankers as well as test the sensitivity of LLM judges.</p>
<p>For the judging side, we explore the sensitivity of evaluation by employing specific variations of LLM judges within a single model family-a relatively unexplored dimension in prior work.By using these specific variations of LLM judges, we aim to assess the consistency and reliability of LLM-based evaluation and to understand how the choice of LLM judge might influence the predicted effectiveness of different rankers.Crucially, we compare the judgments provided by these LLM judges against human assessments, which we consider as the ground truth for relevance.</p>
<p>To further explore the implications of LLM integration across the information lifecycle, we also examine the impact of LLM-assisted content creation on retrieval and evaluation.Specifically, we investigate how AI assistance in document authoring may influence relevance scores assigned by LLM-based rankers and judges.</p>
<p>Experimental Setup</p>
<p>We utilize the TREC Deep Learning (DL) 2019 and 2020 datasets [7,8], chosen due to their extensive use in prior research in this area.Both use the MS MARCO v1 passage corpus, which contains 8.8 million passages.We adopt the convention of referring to passages as "documents, " even if the unit of retrieval are passages in our experiments.The two datasets contain 43 and 54 queries, respectively, with human relevance annotations by TREC assessors.</p>
<p>Following [40,51], all comparisons are based on the reranking of the top 100 passages retrieved by BM25 [23].To ensure a fair comparison between human and LLM judges, we filter results that have not been judged by TREC assessors (instead of treating them as non-relevant).For simplicity, we report only on NDCG@10, which is the official evaluation metric of the DL track.</p>
<p>LLM Judges.</p>
<p>For automatic assessment, we use two model generations of a powerful commercial LLM, Gemini, in two sizes within each generation: v1 Nano, v1 Pro, v1.5 Flash, and v1.5 Pro.We use the best applicable prompt 1 in [54] based on the open source implementation UMBRELA [56], with judgments performed on a 4-point scale.We set top-p =1 and the temperature to 0.</p>
<p>LLM Rankers.</p>
<p>We consider both supervised and unsupervised LLM-based rankers, in addition to a BM25 baseline: 1 We use the prompt that considers multiple aspects (A), but not role (R) nor multiple judges (M); narrative (N) and description (D) are unavailable for TREC DL.
… Perfect Swap[3] … Swap[2] … Swap[1] … Swap[2,3] … Swap[1,2] … Rank 1 2 3 n-2 n-1 n
Figure 2: Illustration of oracle rankers, ordered by their expected performance, assuming that the top three results are highly relevant and the bottom three are non-relevant.</p>
<p>• RankT5 [64] is a reranker that uses T5 [41] and listwise ranking loss during supervised fine-tuning.It is considered a state-of-theart supervised LLM-based ranker.</p>
<p>• RG [20] is a pointwise prompting method based on Relevance Generation, where the prompt asks "Does the passage answer the query?" and the logit of "Yes" is used as the ranking score.We test RG with FLAN-T5-XXL and FLAN-UL2.Note that RG requires internal logits of output tokens and thus cannot be used with black-box LLMs such as Gemini.</p>
<p>• PRP [40] is a pairwise prompting approach that is effective and robust across LLMs with different sizes.Given a query and two passages, the prompt asks "Which of the two passages is more relevant to the query?"The winning rate is used as the ranking score for each passage.</p>
<p>Oracle Rankers.</p>
<p>We generate oracle rankings using the ground truth TREC relevance assessments.To ensure a fair comparison with LLM rankers, we consider the same initial set of BM25-retrieved documents for reranking.Specifically, we consider the following oracle rankers, which are visualized in Fig. 2:</p>
<p>• Perfect: Reranks results according to the ground truth relevance labels.While not perfect overall, this represents the ideal ranking within the initially retrieved set. ) we measure agreement with TREC judges (on all human-judged query-document pairs) in terms of Cohen's  using both graded and binary relevance labels.Following Faggioli et al. [14], we create binary relevance labels by merging levels 0 and 1 (non-relevant) and levels 2 and 3 (relevant).Additionally, we report on relative system ordering in terms of Kendall's .</p>
<p>Results</p>
<p>Table 1 presents the results of the various reranking methods evaluated using both human and LLM judges.Selected methods are  Figure 3: Visualization of the performance of selected rankers from Table 1.shown in Fig. 3 for easier visual inspection.Additionally, Table 2 reports on agreement between human and LLM judges.</p>
<p>Choice of LLM How well do LLM-based judgments align with human assessments when using different variations of LLM judges from the same model family?Looking at the evaluation scores of various rankers, we observe generally good agreement among the three largest models.In terms of agreement with human judges on individual relevance judgments (see Cohen's  Table 2) the results are comparable to those reported in prior work for these datasets [56], with the newer v1.5 models performing clearly better than the v1 models.Interestingly, within this newer model generation, a larger model is not necessarily more capable, at least not according to this measure; v1.5 Flash shows much better agreement with humans when a graded relevance scale is used than the v1.5 Pro.On the other hand, the smallest LLM (v1 Nano) is unable to provide useful judgments, as evidenced by the Cohen's  values being close to 0. While this model may be capable in other tasks [16], our results clearly indicate its unsuitability for judging relevance in this specific context.Therefore, we exclude the v1 Nano judge from subsequent analyses and discussions referring to "LLM judges." Another way to validate LLM judges is by measuring how well they agree on the relative ordering of systems with human judges; see Kendall's  Table 2.In this regard, the newest and largest model (v1.5 Pro) is the most capable overall, but there is in fact little difference in performance among the three largest models.Thus, while newer model generations clearly perform better (v1 vs. v1.5),larger models with the same generation do not necessarily make more capable judges (v1.5 Flash vs. v1.5 Pro).We also note that differentiating between the entire pool of systems ("All systems") proves to be especially challenging; we will elaborate on this next.</p>
<p>Discriminative Ability Can LLM judges reliably distinguish between high-performing systems with small, but meaningful, performance differences?The Oracle rankers, with their controlled performance degradations, enable us to assess the discriminative power of LLMs in a setting free from potential biases introduced by LLMbased rankers.While the absolute score differences between some pairs of Oracle rankings may be small, all pairwise differences are statistically significant according to human judgments (paired t-test,  &lt; 0.05).Therefore, a failure to observe a statistically significant difference, or, more critically, a reversal of the correct ordering, indicates that the LLM judge is not sufficiently sensitive.Table 2 (Oracle-only setting) reveals that accurately ordering the Oracle rankings is challenging for LLM judges, particularly on the DL19 dataset.This suggests limitations in their ability to discern subtle, yet statistically significant, performance differences.This limited discriminative ability is not confined to the Oracle setting; it also manifests when evaluating actual retrieval systems.For instance, the v1.5 Pro model, which performed best among the LLMs on the Oracle rankings, fails to identify statistically significant differences between certain pairs of systems (e.g., RankT5 vs. RG-FLAN-T5-XXL on DL19,  &lt; 0.001 according to human evaluation).Conversely, it can also identify differences as statistically significant (e.g., PRP-FLAN-UL2 vs. PRP-Gemini-v1.5-Flash, &lt; 0.05 for both years) when human judgments show no significant difference.</p>
<p>Furthermore, the substantial difference in correlation between the "All systems" and "Oracles-only" results in Table 2 provides direct evidence of the concerns raised in Issue #1 (Section 3.1.1),namely, how easily correlation-based metrics can be manipulated by the choice of systems included in the evaluation.</p>
<p>Bias Towards LLM-based Rankers Do LLM judges exhibit biases towards LLM-based rankers?The results presented in Fig. 3 demonstrate a clear and substantial bias in favor of LLM-based rankers when evaluated by LLM judges.While prior work has hinted at the potential for such a bias, this study provides direct empirical evidence of its existence and magnitude.Human judgments consistently place the selected Oracle rankers shown in Fig. 3  this order, ranking all LLM-based rankers as superior to these Oracle runs.This is not a subtle effect; the magnitude of the bias is sufficient to completely reverse the relative ranking of these two fundamentally different types of systems.The fact that the true performance of non-LLM-based systems is severly underestimated, highlighting a critical limitation of relying solely on LLM judges for evaluation, particularly when assessing fundamentally new or unconventional approaches.</p>
<p>Bias Towards LLM-generated Text Do LLM judges exhibit biases towards LLM-generated text (independent of the ranking mechanism used to retrieve that text)?We investigate this by comparing LLM judge assessments of original human-written documents and their AI-assisted counterparts.Using the MS MARCO dataset, which predates the widespread adoption of modern AI writing tools, we can reasonably assume that the original documents represent content created without significant AI assistance.To avoid bias potentially introduced by initial retrieval, we take a balanced sample: for each year (DL19 and DL20) we randomly sample 500 query-document pairs for each of the four relevance levels, resulting in a total of 4000 query-document pairs.We refer to this set as Original.We then employ our second most capable LLM (Gemini v1.5 Flash) to create an AI-rewritten version of each document in the Original set, following the methodology of Dai et al. [10]. 2 This rewritten set is referred to as Rewritten.We shall assume that this rewriting process does not substantially alter the relevance of the documents to their corresponding queries, as verified by human assessors in [10].Figure 4 presents the results using our most capable LLM (Gemini v1.5 Pro) as the judge.We can observe on the Original data that the LLM judge is lenient in its assessment of relevance, and specifically in labeling non-relevant documents as partially relevant.However, the judge does not appear to systematically inflate scores for the highest relevance level.Crucially, when comparing these results to the judgments on the Rewritten (LLM-generated) text, we do not observe a distributional shift towards higher relevance levels.</p>
<p>In fact, the Rewritten documents show a slight increase in lower relevance labels.While these findings are specific to this particular combination of LLM rewriter and judge, they provide evidence against a general bias towards LLM-generated content, even when both models are from the same family.It is important to note that the preceding analysis examines the distributional impact of LLM-generated text on relevance judgments.To further investigate potential biases in a ranking context, we conduct a second experiment.We take the rankings produced by the perfect Oracle method and re-evaluate them using our LLM judge (Gemini v1.5 Pro).However, instead of using the Original document content, we substitute the Rewritten versions.If the LLM judge exhibited a strong preference for LLM-generated text, we would expect to see a significant increase in the scores assigned to these rankings.However, according to our results, that is not the case.We find that the performance of the Perfect Oracle method, as assessed by the LLM judge, does not change significantly when using the Rewritten text instead of the Original text: we get an NDCG@10 of 0.868 vs. 0.883 on DL19 and 0.825 vs. 0.818 on DL20 for Rewritten vs. Original; none of these differences is statistically significant.This further reinforces the conclusion that, at least in this experimental setup, the LLM judge does not exhibit a strong bias towards LLM-generated content.</p>
<p>Combined Bias (LLM Ranker + LLM-Generated Text) Do LLM judges exhibit biases towards LLM-generated text when using LLMbased rankers?To address this question, we conduct an experiment combining an LLM-based ranker with LLM-generated text and an LLM judge.We utilize the same balanced sample of 4,000 querydocument pairs (500 per relevance level for each of DL19 and DL20) used in the previous experiment, comprising both the Original and Rewritten sets.For ranking, we employ a pointwise approach using RankT5 with a Flan-T5-XXL model.We compare the scenarios where the RankT5 model scores (1) the Original query-document pairs and (2) the Rewritten query-document pairs.Both scorings are then evaluated using the same LLM judge (Gemini v1.5 Pro).We observe minimal differences in the LLM-assigned evaluation scores between the Original and Rewritten scenarios.Closer inspection of the RankT5 scores reveals that the rewriting process had a negligible impact on retrieval scores for the vast majority of query-document pairs.The few observed changes were symmetrically distributed, with increases and decreases in scores mirroring each other; see Fig. 5.This aligns with the previous experiment's findings, suggesting that neither the LLM judge nor the LLM ranker (in this specific configuration) exhibits a strong preference for the LLM-rewritten content.Consequently, the LLM judge produces very similar evaluation results in both cases.While these results do not demonstrate a combined bias in this specific experimental setup, the potential for synergistic effects between LLM rankers, LLM-generated text, and LLM judges remains an open question requiring further, more comprehensive investigation.</p>
<p>The Role and Challenges of LLM Judges in IR</p>
<p>The feasibility of LLMs as automatic relevance assessors has been established and they have been rapidly adopted both in academia and in industry.The question, therefore, is not whether they can be used as judges, but rather how they should be used in a principled and effective manner.This requires a careful consideration of both the intended purpose of LLM judges and their inherent limitations.</p>
<p>Clarifying the Purpose Most studies, albeit often implicitly, employ LLMs as judges with the aim of replacing human assessors.However, as we discuss below, fundamental limitations preclude this possibility.We argue (in line with [14,30]) that a more appropriate and productive goal should be to enable more effective use of limited human assessor time and resources.This shift in perspective-from replacement to reducing human effort-is crucial for guiding the development and deployment of LLM judges.</p>
<p>Acknowledging Fundamental Limitations It is essential to recognize that both ranking and relevance assessment address the same problem: predicting the relevance of a document to a given query.This inherent overlap introduces fundamental limitations when using LLMs for both tasks.Clarke and Dietz [6] argue that "A true gold standard must originate from human assessments, as only humans can determine the relevance of information in a way that reflects real-world utility."We must also recognize that relevance itself carries an intrinsic uncertainty; it depends on the entire cognitive state of the person, which changes as they use the system [46].Because of these inherent limitations, "LLM assessments may themselves represent a strong ranking method, rather than a valid evaluation metric" [6].Recent work, such as [36], has begun to provide uncertainty measures with LLM relevance predictions.</p>
<p>What, then, is the Role of LLM Judges?Given the limitations outlined above, and recognizing that information access systems are ultimately built to serve human needs and provide utility to users, the highest-fidelity evaluation of these systems can only be achieved through online evaluation with real users.Offline evaluation, while valuable, remains an abstraction of the real-world task because it removes the user from the evaluation process.There is a genuine risk that findings from offline experiments, particularly those relying solely on LLM judges, may not translate to operational settings.It is crucial to recognize that the signal provided by LLM judges is inherently a noisy and potentially biased one, and therefore cannot be fully trusted as a direct proxy for utility.Nevertheless, this noisy signal can be a useful indicator, helping to identify which methods or system variants are promising enough to warrant the more resource-intensive process of human evaluation.</p>
<p>Guidelines for Employing LLMs as Judges</p>
<p>We outline several key considerations for employing LLMs in evaluation, aiming to foster a community-wide set of best practices and ensure methodological soundness.These are not exhaustive, but represent an important starting point.</p>
<p>• Consistent Evaluation Across Systems.All systems being compared within a single evaluation should be assessed using the same LLM judge configuration (model, prompt, settings).This ensures a fair and unbiased comparison, avoiding situations where some systems are evaluated with a more lenient or biased judge than others.Specifically, LLM judges should not be used selectively to fill "relevance holes" in existing human judgments [1].</p>
<p>Open Questions and Future Directions</p>
<p>The adoption of LLMs as judges in IR presents several open questions and necessitates further research to address the limitations and biases identified in previous sections.</p>
<p>• Assessing and Improving LLM Judge Quality.Our findings highlight the critical importance of LLM judge quality, revealing shortcomings in their discriminative ability and biases toward LLM-powered rankers.Developing robust methods for assessing and improving the quality of LLM judges is a crucial research direction for the IR community, potentially drawing motivations from horizontal autorater efforts [57].• Robustness Against Adversarial Attacks.LLM judges, similar to LLM-based rankers, are susceptible to adversarial attacks, including keyword stuffing and content injection [2,52].Understanding these vulnerabilities and developing effective mechanism to enhance the robustness of LLM judges against them are critical areas for ensuring the practical applicability of LLM judges in real-world scenarios.• Human-in-the-Loop LLM Judges.The potential for using LLMs to augment human assessors, e.g., for quality-control, has been suggested [14,48].While Upadhyay et al. [55] found that humanin-the-loop processes did not bring obvious tangible benefits, their study represents a preliminary investigation.Further research is needed to explore the potential of this approach more comprehensively.• From Passages to Longer Documents.Most of the existing work focuses on paragraphs as the unit of retrieval, using either the TREC DL [2,5,14,30] or RAG [55] benchmarks.There is much less work on ad hoc retrieval, with exceptions including TREC-8 [14] and Robust [54].It is known that LLMs handle long context differently [24] and its implication in judging long documents need further investigation.</p>
<p>• Alternative Judging Approaches.All existing studies apply LLMs in a pointwise manner, but a pairwise or listwise setup would also be possible.LLM judge research can borrow ideas from LLM ranking research where pairwise and listwise approaches are extensively explored.• Domain-specific Solutions.While most existing research focuses on general-purpose search, specific domains might require purposebuilt solutions.Recent work, for example, has explored the use of LLM judges for e-commerce search [32,45].Extending this concept of domain specialization, the applicability and value of LLM judges in specialized domains requiring expert knowledge (e.g., medical or legal search) remain less clear.In such domains, LLM judges might offer potential cost savings and more in-depth domain-specific knowledge compared to non-expert human assessors, but they also introduce new challenges, including the need for high accuracy, the potential for serious consequences from errors, and the complexities of expert judgment.Future research should explore the use of LLM judges in these contexts, carefully considering the trade-offs between cost, efficiency, and the risks of inaccurate assessments.• Smaller, Purpose-Built Models.The use of LLMs for judging at large scale raises concerns about computational cost and latency.</p>
<p>A promising research direction is to explore the development of smaller models, designed specifically for the judging task.These purpose-built models could potentially offer significant advantages in terms of efficiency and speed, while maintaining performance comparable to massive LLMs in terms of accuracy and reliability.• Internationalization. Most related research focuses on Englishlanguage corpora.The issues discussed in this paper may be amplified in other languages, especially low resource ones, due to limitations in LLMs' multilingual capabilities.Further research is needed to evaluate the performance of LLM judges across a wider range of languages.</p>
<p>• Training Models on LLM-generated Labels.Training retrieval models on data labeled by LLMs introduces a significant risk of circularity and bias amplification.If done recursively, this might lead to model collapse [47].Thomas et al. [54] acknowledge that parts of the Bing search engine are retrained using LLM-generated labels.Understanding the long-term effects of such training is an important research direction.</p>
<p>Conclusion</p>
<p>This paper has investigated the emerging and critical challenge of understanding the effect LLM-based rankers and AI-powered content creation may have on LLM-based judges' ability to accurately assess relevance.Through a synthesis of existing literature, we identified key concerns regarding the quality, validity, reliability, and potential biases of LLM judgments.Our experiments provided empirical evidence demonstrating how interactions between the various roles LLMs play can lead to inaccurate or biased assessments of retrieval effectiveness, particularly in scenarios involving LLM-based rankers.Finally, we presented guidelines for the use of LLMs as judges in IR and outlined a research agenda to address crucial open questions in this rapidly evolving field.</p>
<p>Figure 1 :
1
Figure 1: LLM usage in modern information access systems.</p>
<p>Figure 4 :
4
Figure4: Relevance levels estimated by an LLM judge (Gemini v1.5 Pro) for Original vs. AI-assisted content (Rewrites, using Gemini v1.5 Flash).According to human assessors, the labels should be uniformly distributed across the four relevance classes, as indicated by the dashed horizontal line.</p>
<p>FrequencyFigure 5 :
5
Figure 5: Distribution of score differences of a RankT5 ranker on LLM-rewritten vs. original text on a sample of 4000 querydocument pairs.</p>
<p>•</p>
<p>Swap[i]: Introduces controlled errors by swapping the top- ranked result with the bottom- result.Decreasing  (from 3 to 2 to 1) increases the deviation from the perfect ranking.•Swap[i,i+1]: Swaps the th and ( + 1)th highest-ranked results with the th and ( + 1)th lowest-ranked results.This represents further degradation from the Swap[i] methods.</p>
<p>4.2.4Measuring Alignment.Following prior work (cf.Section 3.1.1</p>
<p>Table 1 :
1
Results (NDCG@10) on the TREC DL 2019 and 2020 collections using both human and LLM judges.The best LLM and Oracle reranking approaches per judge are boldfaced.
TREC DL19TREC DL20MethodLLMHumanLLM judgesHumanLLM judgesjudgesv1 Nanov1 Prov1.5 Flashv1.5 Projudgesv1 Nanov1 Prov1.5 Flashv1.5 ProInitial retrievalBM25-0.5060.6070.7720.6890.7120.4830.6160.7860.6890.719LLM rerankingRankT5T5 (3B)0.7310.6330.9070.9110.9160.6960.6210.9240.8880.899RGFLAN-T5-XXL (11B)0.6730.6060.8950.8740.8810.6390.6190.9200.8770.878FLAN-UL2 (20B)0.6890.5950.8960.8840.8870.6670.6110.9220.8800.885PRPFLAN-T5-XL (3B)0.7160.6100.9240.9210.9090.6910.6180.9240.8980.901FLAN-T5-XXL (11B)0.7120.6200.9180.9220.9260.7120.6150.9380.9050.912FLAN-UL2 (20B)0.7340.6140.9230.9140.9280.7180.6220.9320.9090.917Gemini v1.5 Flash0.7470.6230.9370.9610.9470.6990.6190.9520.9370.933Oracle rerankingPerfect-0.8920.5820.8960.8760.8640.8710.6170.8720.8280.824Swap[3]-0.8240.5890.8680.8350.8270.7950.6110.8420.7950.796Swap[2]-0.8140.5890.8590.8250.8140.7900.6110.8530.7970.797Swap[1]-0.8030.5780.8700.8360.8360.7640.6210.8320.7780.776Swap[2,3] -0.7390.5960.8290.7790.7710.7060.6020.8210.7600.765Swap[1,2] -0.7130.5850.8310.7820.7830.6720.6150.8100.7430.746HUMANv1 Nanov1 Prov1.5 Flashv1.5 Pro</p>
<p>Table 2 :
2
Agreement between LLM and human judges (1) on individual relevance judgments (Cohen's ) using both graded and binary labels and (2) on relative ordering of systems (Kendall's ) considering all systems in Table1and Oracle rankers only.
Cohen's 𝜅Kendall's 𝜏LLM judgeGradedBinaryAll systemsOracles-onlyDL19DL20DL19 DL20DL19 DL20DL19 DL20v1 Nano-0.002 -0.0110.007 -0.003-0.253 0.011-0.067 0.067v1 Pro0.1390.1440.3370.2730.077 0.1210.600 0.867v1.5 Flash0.268 0.2300.461 0.3700.033 0.1430.600 0.867v1.5 Pro0.2040.1920.462 0.3590.077 0.1430.600 0.867</p>
<p>•</p>
<p>Transparency and Reproducibility.To enable reproducibility and facilitate comparisons across studies, researchers should clearly report the specific LLM used (model version), the exact prompt(s) employed, and any relevant settings or parameters.• Employing Multiple LLMs as Judges.Using a combination of different LLM judges can help mitigate biases stemming from LLMs favoring responses from their own model family and improve robustness (see, e.g., [19]).Reporting the distribution of scores across different judges, as suggested by Rahmani et al. [42], can further enhance robustness.• Alignment with Human Preferences.Ensuring alignment between human and LLM raters is a substantial effort that needs to be continuously monitored and refined [54].Ideally, results reported on LLM judges should also include human validation of the results on a representative sample.Researchers should also exercise care when making research claims based on results from LLM judges.</p>
<p>They use the straightforward prompt "Please rewrite the following text: humanwritten text" in a zero-shot setting.</p>
<p>Can We Use Large Language Models to Fill Relevance Judgment Holes?. Zahra Abbasiantaeb, Chuan Meng, Leif Azzopardi, Mohammad Aliannejadi, Joint Proceedings of the 1st Workshop on Evaluation Methodologies, Testbeds and Community for Information Access Research (EMTCIR 2024) and the 1st Workshop on User Modelling in Conversational Information Retrieval (UM-CIR 2024) co-located with the 2nd International ACM SIGIR Conference on Information Retrieval in the Asia Pacific (SIGIR-AP 2024). CEUR Workshop Proceedings. Tokyo, Japan2024. December 12. 20243854</p>
<p>LLMs can be Fooled into Labelling a Document as Relevant. Marwah Alaofi, Paul Thomas, Falk Scholer, Mark Sanderson, Proceedings of the 2024 Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region (SIGIR-AP '24. the 2024 Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region (SIGIR-AP '242024</p>
<p>InPars: Unsupervised Dataset Generation for Information Retrieval. Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Rodrigo Nogueira, Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '22). the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '22)2022</p>
<p>ODIN: Disentangled Reward Mitigates Hacking in RLHF. Lichang Chen, Chen Zhu, Jiuhai Chen, Davit Soselia, Tianyi Zhou, Tom Goldstein, Heng Huang, Mohammad Shoeybi, Bryan Catanzaro, Forty-first International Conference on Machine Learning (ICML '24). 2024</p>
<p>AI Can Be Cognitively Biased: An Exploratory Study on Threshold Priming in LLM-Based Batch Relevance Assessment. Nuo Chen, Jiqun Liu, Xiaoyu Dong, Qijiong Liu, Tetsuya Sakai, Xiao-Ming Wu, Proceedings of the 2024 Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region (SIGIR-AP '24). the 2024 Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region (SIGIR-AP '24)2024</p>
<p>L A Charles, Laura Clarke, Dietz, arXiv:2412.17156[cs.IR]LLM-based relevance assessment still can't replace human relevance assessment. 2024</p>
<p>Overview of the TREC 2020 Deep Learning Track. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Proceedings of the Twenty-Ninth Text REtrieval Conference (TREC '20). the Twenty-Ninth Text REtrieval Conference (TREC '20)2020</p>
<p>Overview of the TREC 2019 Deep Learning track. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Ellen M Voorhees, Proceedings of the Twenty-Eighth Text REtrieval Conference (TREC '19). the Twenty-Eighth Text REtrieval Conference (TREC '19)2019</p>
<p>Bias and Unfairness in Information Retrieval Systems: New Challenges in the LLM Era. Sunhao Dai, Chen Xu, Shicheng Xu, Liang Pang, Zhenhua Dong, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data MiningJun Xu. 2024</p>
<p>Neural Retrievers are Biased Towards LLM-Generated Content. Sunhao Dai, Yuqi Zhou, Liang Pang, Weihao Liu, Xiaolin Hu, Yong Liu, Xiao Zhang, Gang Wang, Jun Xu, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>Promptagator: Few-shot Dense Retrieval From 8 Examples. Zhuyun Dai, Y Vincent, Ji Zhao, Yi Ma, Jianmo Luan, Jing Ni, Anton Lu, Kelvin Bakalov, Keith Guu, Ming-Wei Hall, Chang, The Eleventh International Conference on Learning Representations. 2023ICLR '23</p>
<p>PaRaDe: Passage Ranking using Demonstrations with LLMs. Andrew Drozdov, Honglei Zhuang, Zhuyun Dai, Zhen Qin, Razieh Rahimi, Xuanhui Wang, Dana Alon, Mohit Iyyer, Andrew Mccallum, Donald Metzler, Kai Hui, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Length-Controlled AlpacaEval: A Simple Debiasing of Automatic Evaluators. Yann Dubois, Percy Liang, Tatsunori Hashimoto, First Conference on Language Modeling (COLM '24). 2024</p>
<p>Perspectives on Large Language Models for Relevance Judgment. Guglielmo Faggioli, Laura Dietz, L A Charles, Gianluca Clarke, Matthias Demartini, Claudia Hagen, Noriko Hauff, Evangelos Kando, Martin Kanoulas, Benno Potthast, Henning Stein, Wachsmuth, Proceedings of the 2023 ACM SIGIR International Conference on Theory of Information Retrieval. the 2023 ACM SIGIR International Conference on Theory of Information Retrieval2023ICTIR '23</p>
<p>Bias and Fairness in Large Language Models: A Survey. Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, Nesreen K Ahmed, Computational Linguistics. 502024. Sept. 2024</p>
<p>Gemini: A family of highly capable multimodal models. Gemini Team, Google , arXiv:2312.11805[cs.CL]2023</p>
<p>ChatGPT outperforms crowd workers for text-annotation tasks. Fabrizio Gilardi, Meysam Alizadeh, Maël Kubli, Proceedings of the National Academy of Sciences. 120e23050161202023. 2023</p>
<p>Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, Saizhuo Wang, Kun Zhang, Yuanzhuo Wang, Wen Gao, arXiv:2411.15594[cs.CL]Lionel Ni, and Jian Guo. 2024. A Survey on LLM-asa-Judge. </p>
<p>Alon Jacovi, Andrew Wang, Chris Alberti, Connie Tao, Jon Lipovetz, Kate Olszewska, Lukas Haas, Michelle Liu, Nate Keating, Adam Bloniarz, Carl Saroufim, Corey Fry, Dror Marcus, Doron Kukliansky, Gaurav Singh Tomar, James Swirhun, Jinwei Xing, Lily Wang, Madhu Gurumurthy, Michael Aaron, Moran Ambar, Rachana Fellinger, Rui Wang, Zizhao Zhang, arXiv:2501.03200[cs.CL]Sasha Goldshtein, and Dipanjan Das. 2025. The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground Responses to Long-Form Input. </p>
<p>Holistic Evaluation of Language Models. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Alexander Cosgrove, Christopher D Manning, Christopher Re, Diana Acosta-Navas, Drew Arad Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Wang Jue, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Andrew Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, Yuta Koreeda, Transactions on Machine Learning Research. 2023. 2023</p>
<p>Towards understanding and mitigating social biases in language models. Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, Ruslan Salakhutdinov, International Conference on Machine Learning (ICML '21. 2021</p>
<p>Weixin Liang, Yaohui Zhang, Mihai Codreanu, Jiayu Wang, Hancheng Cao, James Zou, arXiv:2502.09747[cs.CL]The Widespread Adoption of Large Language Model-Assisted Writing Across Society. 2025</p>
<p>Pyserini: A Python Toolkit for Reproducible Information Retrieval Research with Sparse and Dense Representations. Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, Rodrigo Nogueira, Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '21). the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '21)2021</p>
<p>Lost in the middle: How language models use long contexts. Kevin Nelson F Liu, John Lin, Ashwin Hewitt, Michele Paranjape, Fabio Bevilacqua, Percy Petroni, Liang, Transactions of the Association for Computational Linguistics. 122024. 2024</p>
<p>G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Text Summarization with Pretrained Encoders. Yang Liu, Mirella Lapata, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores. Yiqi Liu, Nafise Moosavi, Chenghua Lin, Findings of the Association for Computational Linguistics: ACL 2024. 2024</p>
<p>Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators. Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulić, Anna Korhonen, Nigel Collier, First Conference on Language Modeling (COLM '24). 2024</p>
<p>Zero-Shot Listwise Document Reranking with a Large Language Model. Xueguang Ma, Xinyu Zhang, Ronak Pradeep, Jimmy Lin, arXiv:2305.02156[cs.IR]2023</p>
<p>One-Shot Labeling for Automatic Relevance Estimation. Sean Macavaney, Luca Soldaini, Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '23. the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '232023</p>
<p>J S Mccarley, Rishav Chakravarti, Avirup Sil, arXiv:1910.06360[cs.CL]Structured Pruning of a BERT-based Question Answering Model. 2019</p>
<p>Navid Mehrdad, Hrushikesh Mohapatra, Mossaab Bagdouri, Prijith Chandran, Alessandro Magnani, Xunfan Cai, Ajit Puthenputhussery, Sachin Yadav, Tony Lee, Chengxiang Zhai, Ciya Liao, arXiv:2406.00247[cs.IR]Large Language Models for Relevance Judgment in Product Search. 2024</p>
<p>Biases in large language models: origins, inventory, and discussion. Roberto Navigli, Simone Conia, Björn Ross, ACM Journal of Data and Information Quality. 152023. 2023</p>
<p>Rodrigo Nogueira, Kyunghyun Cho, arXiv:1901.04085[cs.IR]Passage Re-ranking with BERT. 2019</p>
<p>Document ranking with a pretrained sequence-to-sequence model. Rodrigo Nogueira, Zhiying Jiang, Jimmy Lin, Findings of the Association for Computational Linguistics: EMNLP 2020. 2020</p>
<p>Reliable confidence intervals for information retrieval evaluation using generative ai. Harrie Oosterhuis, Rolf Jagerman, Zhen Qin, Xuanhui Wang, Michael Bendersky, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>LLM Evaluators Recognize and Favor Their Own Generations. Arjun Panickssery, R Samuel, Shi Bowman, Feng, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024NeurIPS '24</p>
<p>Analyzing Adversarial Attacks on Sequence-to-Sequence Relevance Models. Andrew Parry, Maik Fröbe, Sean Macavaney, Martin Potthast, Matthias Hagen, Proceedings of the 46th European Conference on Information Retrieval (ECIR '24). the 46th European Conference on Information Retrieval (ECIR '24)2024</p>
<p>The Expando-Mono-Duo Design Pattern for Text Ranking with Pretrained Sequence-to-Sequence Models. Ronak Pradeep, Rodrigo Nogueira, Jimmy Lin, arXiv:2101.05667[cs.IR]2021</p>
<p>Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting. Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Le Yan, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, Michael Bendersky, Findings of the Association for Computational Linguistics: NAACL 2024. 2024</p>
<p>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 212020. 2020</p>
<p>A Hossein, Clemencia Rahmani, Mohammad Siro, Nick Aliannejadi, Craswell, L A Charles, Guglielmo Clarke, Bhaskar Faggioli, Paul Mitra, Emine Thomas, Yilmaz, arXiv:2408.05388[cs.IR]Report on the 1st Workshop on Large Language Model for Evaluation in Information Retrieval. 2024. LLM4Eval 2024SIGIR 2024</p>
<p>A Hossein, Xi Rahmani, Emine Wang, Nick Yilmaz, Craswell, Mitra Bhaskar, Paul Thomas, arXiv:2408.16312[cs.IR]SynDL: A Large-Scale Synthetic Test Collection for Passage Retrieval. 2025</p>
<p>Improving Passage Retrieval with Zero-Shot Question Generation. Devendra Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-Tau Yih, Joelle Pineau, Luke Zettlemoyer, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Automated Query-Product Relevance Labeling using Large Language Models for E-commerce Search. Jayant Sachdev, Sean D Rosario, Abhijeet Phatak, He Wen, Swati Kirti, Chittaranjan Tripathy, arXiv:2502.15990[cs.IR]2025</p>
<p>Relevance reconsidered. Tefko Saracevic, Proceedings of the second conference on conceptions of library and information science. the second conference on conceptions of library and information science1996</p>
<p>Anderson, and Yarin Gal. 2024. AI models collapse when trained on recursively generated data. Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, J Ross, Nature. 631July 2024</p>
<p>Don't Use LLMs to Make Relevance Judgments. Ian Soboroff, Information Retrieval Research. 12025. Mar. 2025</p>
<p>Heydar Soudani, Roxana Petcu, arXiv:2405.13003[cs.CL]Evangelos Kanoulas, and Faegheh Hasibi. 2024. A Survey on Recent Advances in Conversational Data Generation. </p>
<p>Large Language Models are Inconsistent and Biased Evaluators. Rickard Stureborg, Dimitris Alikaniotis, Yoshi Suhara, arXiv:2405.01724[cs.CL]2024</p>
<p>Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents. Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, Zhaochun Ren, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Manveer Singh, Tamber , Jimmy Lin, arXiv:2501.18536[cs.IR]Illusions of Relevance: Using Content Injection Attacks to Deceive Retrievers, Rerankers, and LLM Judges. 2025</p>
<p>Transformer memory as a differentiable search index. Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, Proceedings of the 36th International Conference on Neural Information Processing Systems. the 36th International Conference on Neural Information Processing Systems2022NeurIPS '22</p>
<p>Large Language Models can Accurately Predict Searcher Preferences. Paul Thomas, Seth Spielman, Nick Craswell, Bhaskar Mitra, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '24). the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '24)2024</p>
<p>Shivani Upadhyay, Ronak Pradeep, Nandan Thakur, Daniel Campos, Nick Craswell, Ian Soboroff, Hoa , Trang Dang, Jimmy Lin, arXiv:2411.08275[cs.IR]A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look. 2024</p>
<p>UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor. Shivani Upadhyay, Ronak Pradeep, Nandan Thakur, Nick Craswell, Jimmy Lin, arXiv:2406.06519[cs.IR]2024</p>
<p>Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation. Tu Vu, Kalpesh Krishna, Salaheddin Alzubi, Chris Tar, Manaal Faruqui, Yun-Hsuan Sung, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024</p>
<p>How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey Macmillan, Noah A Smith, Iz Beltagy, Hannaneh Hajishirzi, Proceedings of the 37th International Conference on Neural Information Processing Systems. the 37th International Conference on Neural Information Processing Systems2023NeurIPS '23</p>
<p>Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement. Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, William Wang, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>BERTScore: Evaluating Text Generation with BERT. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, 8th International Conference on Learning Representations (ICLR '20). 2020</p>
<p>MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, Steffen Eger, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019. 19</p>
<p>Judging LLM-as-a-judge with MTbench and Chatbot Arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, Proceedings of the 37th International Conference on Neural Information Processing Systems. the 37th International Conference on Neural Information Processing Systems202323</p>
<p>Incorporating BERT into Neural Machine Translation. Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang Zhou, Houqiang Li, Tieyan Liu, International Conference on Learning Representations (ICLR '20). 2020</p>
<p>RankT5: Fine-Tuning T5 for Text Ranking with Ranking Losses. Honglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui, Ji Ma, Jing Lu, Jianmo Ni, Xuanhui Wang, Michael Bendersky, Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval2023SIGIR '23</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8322 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8322</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8322</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-278788644</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.16135v1.pdf" target="_blank">Sudoku-Bench: Evaluating creative reasoning with Sudoku variants</a></p>
                <p><strong>Paper Abstract:</strong> Existing reasoning benchmarks for large language models (LLMs) frequently fail to capture authentic creativity, often rewarding memorization of previously observed patterns. We address this shortcoming with Sudoku-Bench, a curated benchmark of challenging and unconventional Sudoku variants specifically selected to evaluate creative, multi-step logical reasoning. Sudoku variants form an unusually effective domain for reasoning research: each puzzle introduces unique or subtly interacting constraints, making memorization infeasible and requiring solvers to identify novel logical breakthroughs (``break-ins''). Despite their diversity, Sudoku variants maintain a common and compact structure, enabling clear and consistent evaluation. Sudoku-Bench includes a carefully chosen puzzle set, a standardized text-based puzzle representation, and flexible tools compatible with thousands of publicly available puzzles -- making it easy to extend into a general research environment. Baseline experiments show that state-of-the-art LLMs solve fewer than 15\% of puzzles unaided, highlighting significant opportunities to advance long-horizon, strategic reasoning capabilities.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8322.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8322.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini 2.5 Pro Preview</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini 2.5 Pro Preview</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A frontier commercial large language model evaluated on the Sudoku-Bench suite; capable of parsing complex natural-language puzzle constraints but frequently resorts to brute-force search and guessing rather than human-like break-in reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini 2.5 Pro Preview</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in the paper as a state-of-the-art LLM that can parse puzzle constraints and produce stepwise reasoning traces; no architecture or training data specifics are given in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku variants (Sudoku-Bench), examples: Ascension (example failure) and Sumthings (example success)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based logic puzzles (4×4, 6×6, 9×9 Sudoku variants) requiring spatial relations across grid cells (e.g., arrow sums, knight's-move constraints, region-based sum segments).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Evaluated in two modes: (1) Multi-step interactive setting where the model must return at least one committed digit per response and the user updates the board; the session keeps the most recent 5 model responses plus the initial puzzle spec. Interaction terminates on any incorrect placement; metrics include solve rate and average correct digits. (2) Single-shot setting where the model is prompted to return a complete solution in one response. All puzzles are provided as precise textual representations (coordinates and rule descriptions) rather than images.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Parsed natural-language rules into internal constraints, reduced candidate cases for particular line-sum variables (e.g., S2 and S3), then applied search/guessing until contradiction; used constraint-reduction to produce a small search space and then brute-force search; produced verbal reasoning traces but often relied on trial assignments (guess-and-check) rather than explicit geometric or coloring heuristics used by humans.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Part of the reported baseline set where 'state-of-the-art LLMs' solved fewer than 15% of the 100-puzzle benchmark without tools. Gemini 2.5 Pro Preview solved at least one 6×6 puzzle example (Sumthings) by reducing the search space and searching; however, it failed to make progress on the 9×9 Ascension example (could parse constraints but resorted to ineffective guess/search). Per-size trends: models (including Gemini) show moderate success on small 4×4 puzzles (aggregate rates across models 40–73%), sharply reduced performance on 6×6, and near-zero on 9×9 puzzles. (The paper reports averages across models and does not provide a clean per-model-per-size table in text.)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Qualitative analysis shows Gemini can identify and represent spatial constraints (e.g., knight-move exclusions, arrow sum relationships, line-segment sums S2/S3) and reduce candidate assignments, indicating some capacity to reason about spatially located constraints. However, failures on puzzles that require discovering geometric 'break-ins' (Ascension) indicate limited use of higher-level geometric/spatial strategies (symmetry, coloring, long-range monotonic chains) that expert humans use. No ablation studies or probing are reported to quantify internal spatial representations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to human expert performance (example: a human solver found Ascension's break-in in ≈4.5 minutes and finished in ≈35 minutes), Gemini fell short—able to parse constraints but unable to find the human-like insight and resorting to brute-force. Compared to other evaluated LLMs (e.g., OpenAI o3 family), Gemini is grouped with leading models that have overall solve rates below 15%; Gemini solved some moderately complex 6×6 puzzles whereas all models collapsed on 9×9 variants. Single-shot vs multi-step modes produced minimal differences in overall capability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Frequently resorts to brute-force search and guesswork, misses potential placements during search, and can reach incorrect global solutions; failed to find key break-ins based on geometric or parity insights (e.g., failed on Ascension); performance degrades rapidly with grid size and with puzzles that start with few or no givens; struggles where visual/geometric reasoning would help (paper uses text-only inputs to isolate reasoning), and multi-step interaction with limited context (most recent 5 responses) did not compensate. Failure modes categorized in analysis include Incorrect Solution, Surrender (giving up), Missing Information (claims rules/information are incomplete), Claimed Contradiction (incorrectly identifies contradictions), and No Reasoning Trace (insufficient traceability).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sudoku-Bench: Evaluating creative reasoning with Sudoku variants', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8322.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8322.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o3 mini high (OpenAI o3 family)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI o3 (o3 mini high variant as reported)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercially available OpenAI model variant evaluated as part of the Sudoku-Bench baselines; one of several frontier models that achieved low solve rates on Sudoku variants without tool assistance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o3 mini high (OpenAI o3 family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced as a current-generation LLM (OpenAI o3 family). The paper does not provide architecture, pretraining, or parameter-count details for this model—only its empirical behaviour on the benchmark is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku variants (Sudoku-Bench; 4×4, 6×6, 9×9)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based logic puzzles requiring spatially localized reasoning (row/column/box uniqueness, arrow sums, cage sums, knight's-move restrictions, region-segment sums).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Same dual-mode evaluation: (a) Multi-step committed-placement interaction (model places ≥1 digit per turn; user updates board; session retains last 5 model responses and initial puzzle spec), and (b) Single-shot full-solve prompt. Puzzles are supplied as precise textual encodings of visuals and rules. Models were terminated upon any incorrect committed placement in multi-step mode.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Observed behaviour: parsing of textual constraints followed by attempts to reduce candidate assignments for particular variables; when unable to find human-like break-ins, the model typically resorts to search/guessing and testing hypotheses to contradiction. The model did not demonstrably employ explicit algorithmic constraint solvers or external tools in the reported no-tool baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported as one of the leading models but with solve rates below 15% on the full 100-puzzle benchmark without tools. Generally solved a nontrivial fraction of 4×4 puzzles (aggregate across models 40–73%), performance collapsed on 6×6 and 9×9 puzzles approaching near-zero solves for 9×9. The paper's Table 1 shows per-size stratifications but the text emphasizes the key aggregate: <15% overall solve rate for leading models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Model could correctly interpret and restate spatial constraints in text form (coordinates, relations), but qualitative failure analyses show limited ability to exploit geometric/spatial insights (symmetry, coloring, long-range monotonic chains) needed for difficult break-ins. No quantitative probing of internal spatial representations is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared with Gemini 2.5 Pro Preview and other frontier LLMs: all achieve low overall solve rates; both single-shot and multi-step configurations give similar low performance for larger puzzles. Compared to humans and expert solvers from Cracking the Cryptic, the model's reasoning is less effective at producing insight-driven break-ins and more reliant on brute force.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Same failure taxonomy: Incorrect Solution, Missing Information claims, Claimed Contradiction, Surrender, and No Reasoning Trace. Also documented difficulty in converting visual puzzles into exact coordinate/text specs (hence benchmark uses text) and susceptibility to large search spaces when puzzles have few givens. Multi-step setting truncates history to the most recent five model responses which may hinder very long-horizon solves.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sudoku-Bench: Evaluating creative reasoning with Sudoku variants', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8322.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8322.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>State-of-the-art LLMs (aggregate baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>State-of-the-art large language models evaluated on Sudoku-Bench (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregate characterization of multiple contemporary LLMs evaluated on the Sudoku-Bench benchmark; collectively they show limited creative spatial reasoning on challenging Sudoku variants without external tools.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Aggregate: leading LLMs (examples: OpenAI o3 family, Gemini 2.5 Pro Preview, others reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A collection of top-tier LLMs evaluated in the paper; the benchmark provides standardized text prompts and optional agentic interactions via SudokuPad (though baseline experiments used text-only inputs). Individual model internals and sizes are not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku variants (Sudoku-Bench: 100 curated puzzles; 15 of 4×4, 15 of 6×6, 70 of 9×9)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based logic puzzle variants requiring spatial and relational reasoning across cells and regions (examples: Killer Sudoku, arrow constraints, anti-knight, Kropki relationships, line segment sums, parity constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Textual puzzle encodings presented in two modes: (1) single-shot full-solve prompt, and (2) multi-step interaction where the model must make committed digit placements and the user updates the board; multi-step sessions keep the last 5 model responses plus initial specification. Evaluations were run once per model per puzzle; multi-step terminated on a single incorrect placement.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Across models the common observed approaches were: (a) parsing natural-language constraints into text-based internal constraints, (b) reducing candidate spaces for certain variables (e.g., deducing small sets for line-sums S2/S3), (c) performing search/guess-and-check when break-ins could not be discovered, and (d) occasionally applying limited deductive chains to place digits. Models rarely demonstrated high-level, human-like spatial heuristics (e.g., coloring, exploitation of symmetry) in a reliable way.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Aggregate headline: state-of-the-art LLMs solve fewer than 15% of the 100-puzzle Sudoku-Bench set without external tools. Size-stratified behaviour reported: models typically solved 4×4 puzzles at moderate rates (aggregate range reported across models 40–73%), performance dropped substantially for 6×6 puzzles, and was nearly zero for 9×9 puzzles. Multi-step mode produced slightly better outcomes on small puzzles but did not meaningfully improve performance on larger variants.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Qualitative evidence: models can represent spatial constraints (coordinate-based descriptions, anti-knight, arrows) and sometimes reduce spatially-defined candidate sets; but the inability to find geometric break-ins and rapid performance degradation with puzzle size indicate a limited ability to perform the type of long-horizon spatial/structural reasoning humans use. No targeted ablation or probing studies were reported to measure internal spatial reasoning abilities quantitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Benchmarked against human expert solves (Cracking the Cryptic transcripts) showing humans more effectively find break-ins and use spatial heuristics; compared across LLMs with similar low aggregate performance. The paper also contrasts no-tool vs tool-use regimes: puzzles become trivial when translated to formal constraint solvers, but the benchmark intentionally focuses on no-tool evaluation to measure intrinsic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Major limitations include inability to discover initial 'break-ins' when puzzles start with few or no givens, overreliance on brute-force search that scales poorly, errors in search (missing alternative placements), difficulty parsing dense visual puzzles (hence use of text), truncated context in multi-step interactions limiting very long-horizon reasoning, and common failure categories (Incorrect Solution, Missing Information, Claimed Contradiction, Surrender, No Reasoning Trace). The paper also notes that many puzzle types would be easier given tool use, but that would evaluate different capabilities (translation and tool orchestration rather than intrinsic creative reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sudoku-Bench: Evaluating creative reasoning with Sudoku variants', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Step-by-step reasoning to solve grid puzzles: Where do LLMs falter? <em>(Rating: 2)</em></li>
                <li>VGRP-bench: Visual grid reasoning puzzle benchmark for large vision-language models <em>(Rating: 2)</em></li>
                <li>PuzzlePlex: A benchmark to evaluate the reasoning and planning of large language models on puzzles <em>(Rating: 2)</em></li>
                <li>EnigmaEval: A benchmark of long multimodal reasoning challenges <em>(Rating: 2)</em></li>
                <li>Causal language modeling can elicit search and reasoning capabilities on logic puzzles <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8322",
    "paper_id": "paper-278788644",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "Gemini 2.5 Pro Preview",
            "name_full": "Gemini 2.5 Pro Preview",
            "brief_description": "A frontier commercial large language model evaluated on the Sudoku-Bench suite; capable of parsing complex natural-language puzzle constraints but frequently resorts to brute-force search and guessing rather than human-like break-in reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini 2.5 Pro Preview",
            "model_description": "Described in the paper as a state-of-the-art LLM that can parse puzzle constraints and produce stepwise reasoning traces; no architecture or training data specifics are given in the paper.",
            "model_size": null,
            "puzzle_name": "Sudoku variants (Sudoku-Bench), examples: Ascension (example failure) and Sumthings (example success)",
            "puzzle_type": "Grid-based logic puzzles (4×4, 6×6, 9×9 Sudoku variants) requiring spatial relations across grid cells (e.g., arrow sums, knight's-move constraints, region-based sum segments).",
            "task_setup": "Evaluated in two modes: (1) Multi-step interactive setting where the model must return at least one committed digit per response and the user updates the board; the session keeps the most recent 5 model responses plus the initial puzzle spec. Interaction terminates on any incorrect placement; metrics include solve rate and average correct digits. (2) Single-shot setting where the model is prompted to return a complete solution in one response. All puzzles are provided as precise textual representations (coordinates and rule descriptions) rather than images.",
            "mechanisms_or_strategies": "Parsed natural-language rules into internal constraints, reduced candidate cases for particular line-sum variables (e.g., S2 and S3), then applied search/guessing until contradiction; used constraint-reduction to produce a small search space and then brute-force search; produced verbal reasoning traces but often relied on trial assignments (guess-and-check) rather than explicit geometric or coloring heuristics used by humans.",
            "performance_metrics": "Part of the reported baseline set where 'state-of-the-art LLMs' solved fewer than 15% of the 100-puzzle benchmark without tools. Gemini 2.5 Pro Preview solved at least one 6×6 puzzle example (Sumthings) by reducing the search space and searching; however, it failed to make progress on the 9×9 Ascension example (could parse constraints but resorted to ineffective guess/search). Per-size trends: models (including Gemini) show moderate success on small 4×4 puzzles (aggregate rates across models 40–73%), sharply reduced performance on 6×6, and near-zero on 9×9 puzzles. (The paper reports averages across models and does not provide a clean per-model-per-size table in text.)",
            "evidence_of_spatial_reasoning": "Qualitative analysis shows Gemini can identify and represent spatial constraints (e.g., knight-move exclusions, arrow sum relationships, line-segment sums S2/S3) and reduce candidate assignments, indicating some capacity to reason about spatially located constraints. However, failures on puzzles that require discovering geometric 'break-ins' (Ascension) indicate limited use of higher-level geometric/spatial strategies (symmetry, coloring, long-range monotonic chains) that expert humans use. No ablation studies or probing are reported to quantify internal spatial representations.",
            "comparisons": "Compared to human expert performance (example: a human solver found Ascension's break-in in ≈4.5 minutes and finished in ≈35 minutes), Gemini fell short—able to parse constraints but unable to find the human-like insight and resorting to brute-force. Compared to other evaluated LLMs (e.g., OpenAI o3 family), Gemini is grouped with leading models that have overall solve rates below 15%; Gemini solved some moderately complex 6×6 puzzles whereas all models collapsed on 9×9 variants. Single-shot vs multi-step modes produced minimal differences in overall capability.",
            "limitations_or_failure_cases": "Frequently resorts to brute-force search and guesswork, misses potential placements during search, and can reach incorrect global solutions; failed to find key break-ins based on geometric or parity insights (e.g., failed on Ascension); performance degrades rapidly with grid size and with puzzles that start with few or no givens; struggles where visual/geometric reasoning would help (paper uses text-only inputs to isolate reasoning), and multi-step interaction with limited context (most recent 5 responses) did not compensate. Failure modes categorized in analysis include Incorrect Solution, Surrender (giving up), Missing Information (claims rules/information are incomplete), Claimed Contradiction (incorrectly identifies contradictions), and No Reasoning Trace (insufficient traceability).",
            "uuid": "e8322.0",
            "source_info": {
                "paper_title": "Sudoku-Bench: Evaluating creative reasoning with Sudoku variants",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "o3 mini high (OpenAI o3 family)",
            "name_full": "OpenAI o3 (o3 mini high variant as reported)",
            "brief_description": "A commercially available OpenAI model variant evaluated as part of the Sudoku-Bench baselines; one of several frontier models that achieved low solve rates on Sudoku variants without tool assistance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "o3 mini high (OpenAI o3 family)",
            "model_description": "Referenced as a current-generation LLM (OpenAI o3 family). The paper does not provide architecture, pretraining, or parameter-count details for this model—only its empirical behaviour on the benchmark is reported.",
            "model_size": null,
            "puzzle_name": "Sudoku variants (Sudoku-Bench; 4×4, 6×6, 9×9)",
            "puzzle_type": "Grid-based logic puzzles requiring spatially localized reasoning (row/column/box uniqueness, arrow sums, cage sums, knight's-move restrictions, region-segment sums).",
            "task_setup": "Same dual-mode evaluation: (a) Multi-step committed-placement interaction (model places ≥1 digit per turn; user updates board; session retains last 5 model responses and initial puzzle spec), and (b) Single-shot full-solve prompt. Puzzles are supplied as precise textual encodings of visuals and rules. Models were terminated upon any incorrect committed placement in multi-step mode.",
            "mechanisms_or_strategies": "Observed behaviour: parsing of textual constraints followed by attempts to reduce candidate assignments for particular variables; when unable to find human-like break-ins, the model typically resorts to search/guessing and testing hypotheses to contradiction. The model did not demonstrably employ explicit algorithmic constraint solvers or external tools in the reported no-tool baselines.",
            "performance_metrics": "Reported as one of the leading models but with solve rates below 15% on the full 100-puzzle benchmark without tools. Generally solved a nontrivial fraction of 4×4 puzzles (aggregate across models 40–73%), performance collapsed on 6×6 and 9×9 puzzles approaching near-zero solves for 9×9. The paper's Table 1 shows per-size stratifications but the text emphasizes the key aggregate: &lt;15% overall solve rate for leading models.",
            "evidence_of_spatial_reasoning": "Model could correctly interpret and restate spatial constraints in text form (coordinates, relations), but qualitative failure analyses show limited ability to exploit geometric/spatial insights (symmetry, coloring, long-range monotonic chains) needed for difficult break-ins. No quantitative probing of internal spatial representations is provided.",
            "comparisons": "Compared with Gemini 2.5 Pro Preview and other frontier LLMs: all achieve low overall solve rates; both single-shot and multi-step configurations give similar low performance for larger puzzles. Compared to humans and expert solvers from Cracking the Cryptic, the model's reasoning is less effective at producing insight-driven break-ins and more reliant on brute force.",
            "limitations_or_failure_cases": "Same failure taxonomy: Incorrect Solution, Missing Information claims, Claimed Contradiction, Surrender, and No Reasoning Trace. Also documented difficulty in converting visual puzzles into exact coordinate/text specs (hence benchmark uses text) and susceptibility to large search spaces when puzzles have few givens. Multi-step setting truncates history to the most recent five model responses which may hinder very long-horizon solves.",
            "uuid": "e8322.1",
            "source_info": {
                "paper_title": "Sudoku-Bench: Evaluating creative reasoning with Sudoku variants",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "State-of-the-art LLMs (aggregate baseline)",
            "name_full": "State-of-the-art large language models evaluated on Sudoku-Bench (aggregate)",
            "brief_description": "Aggregate characterization of multiple contemporary LLMs evaluated on the Sudoku-Bench benchmark; collectively they show limited creative spatial reasoning on challenging Sudoku variants without external tools.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Aggregate: leading LLMs (examples: OpenAI o3 family, Gemini 2.5 Pro Preview, others reported)",
            "model_description": "A collection of top-tier LLMs evaluated in the paper; the benchmark provides standardized text prompts and optional agentic interactions via SudokuPad (though baseline experiments used text-only inputs). Individual model internals and sizes are not specified in the paper.",
            "model_size": null,
            "puzzle_name": "Sudoku variants (Sudoku-Bench: 100 curated puzzles; 15 of 4×4, 15 of 6×6, 70 of 9×9)",
            "puzzle_type": "Grid-based logic puzzle variants requiring spatial and relational reasoning across cells and regions (examples: Killer Sudoku, arrow constraints, anti-knight, Kropki relationships, line segment sums, parity constraints).",
            "task_setup": "Textual puzzle encodings presented in two modes: (1) single-shot full-solve prompt, and (2) multi-step interaction where the model must make committed digit placements and the user updates the board; multi-step sessions keep the last 5 model responses plus initial specification. Evaluations were run once per model per puzzle; multi-step terminated on a single incorrect placement.",
            "mechanisms_or_strategies": "Across models the common observed approaches were: (a) parsing natural-language constraints into text-based internal constraints, (b) reducing candidate spaces for certain variables (e.g., deducing small sets for line-sums S2/S3), (c) performing search/guess-and-check when break-ins could not be discovered, and (d) occasionally applying limited deductive chains to place digits. Models rarely demonstrated high-level, human-like spatial heuristics (e.g., coloring, exploitation of symmetry) in a reliable way.",
            "performance_metrics": "Aggregate headline: state-of-the-art LLMs solve fewer than 15% of the 100-puzzle Sudoku-Bench set without external tools. Size-stratified behaviour reported: models typically solved 4×4 puzzles at moderate rates (aggregate range reported across models 40–73%), performance dropped substantially for 6×6 puzzles, and was nearly zero for 9×9 puzzles. Multi-step mode produced slightly better outcomes on small puzzles but did not meaningfully improve performance on larger variants.",
            "evidence_of_spatial_reasoning": "Qualitative evidence: models can represent spatial constraints (coordinate-based descriptions, anti-knight, arrows) and sometimes reduce spatially-defined candidate sets; but the inability to find geometric break-ins and rapid performance degradation with puzzle size indicate a limited ability to perform the type of long-horizon spatial/structural reasoning humans use. No targeted ablation or probing studies were reported to measure internal spatial reasoning abilities quantitatively.",
            "comparisons": "Benchmarked against human expert solves (Cracking the Cryptic transcripts) showing humans more effectively find break-ins and use spatial heuristics; compared across LLMs with similar low aggregate performance. The paper also contrasts no-tool vs tool-use regimes: puzzles become trivial when translated to formal constraint solvers, but the benchmark intentionally focuses on no-tool evaluation to measure intrinsic reasoning.",
            "limitations_or_failure_cases": "Major limitations include inability to discover initial 'break-ins' when puzzles start with few or no givens, overreliance on brute-force search that scales poorly, errors in search (missing alternative placements), difficulty parsing dense visual puzzles (hence use of text), truncated context in multi-step interactions limiting very long-horizon reasoning, and common failure categories (Incorrect Solution, Missing Information, Claimed Contradiction, Surrender, No Reasoning Trace). The paper also notes that many puzzle types would be easier given tool use, but that would evaluate different capabilities (translation and tool orchestration rather than intrinsic creative reasoning).",
            "uuid": "e8322.2",
            "source_info": {
                "paper_title": "Sudoku-Bench: Evaluating creative reasoning with Sudoku variants",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Step-by-step reasoning to solve grid puzzles: Where do LLMs falter?",
            "rating": 2,
            "sanitized_title": "stepbystep_reasoning_to_solve_grid_puzzles_where_do_llms_falter"
        },
        {
            "paper_title": "VGRP-bench: Visual grid reasoning puzzle benchmark for large vision-language models",
            "rating": 2,
            "sanitized_title": "vgrpbench_visual_grid_reasoning_puzzle_benchmark_for_large_visionlanguage_models"
        },
        {
            "paper_title": "PuzzlePlex: A benchmark to evaluate the reasoning and planning of large language models on puzzles",
            "rating": 2,
            "sanitized_title": "puzzleplex_a_benchmark_to_evaluate_the_reasoning_and_planning_of_large_language_models_on_puzzles"
        },
        {
            "paper_title": "EnigmaEval: A benchmark of long multimodal reasoning challenges",
            "rating": 2,
            "sanitized_title": "enigmaeval_a_benchmark_of_long_multimodal_reasoning_challenges"
        },
        {
            "paper_title": "Causal language modeling can elicit search and reasoning capabilities on logic puzzles",
            "rating": 1,
            "sanitized_title": "causal_language_modeling_can_elicit_search_and_reasoning_capabilities_on_logic_puzzles"
        }
    ],
    "cost": 0.013225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Sudoku-Bench: Evaluating creative reasoning with Sudoku variants
22 May 2025</p>
<p>Jeffrey Seely jeffrey@sakana.ai 
SakanaAI</p>
<p>SakanaAI</p>
<p>Yuki Imajuku 
SakanaAI</p>
<p>Tianyu Zhao 
SakanaAI</p>
<p>Edoardo Cetin 
SakanaAI</p>
<p>Llion Jones 
SakanaAI</p>
<p>Sudoku-Bench: Evaluating creative reasoning with Sudoku variants
22 May 20251F7CF5D7AE9429008E24A39C37EB8EDBarXiv:2505.16135v1[cs.AI]If C(43)=9[...] * [...] {r3c4r2c4} (R4){r3c3r4c3} (R2){r3c2r4c2r3c1r2c1} (R1). * Constraint: Sums are equal. Let this sum be S2. * <code>S2 = r3c4 + r2c4</code> * <code>S2 = r3c3 + r4c3</code> * <code>S2 = r3c2 + r4c2 + r3c1 + r2c1</code> * The minimum sum for 4 distinct digits is 1+2+3+4=10. The maximum sum for 2 distinct digits is 5+6=11. ThereforeS2 must be 10 or 11. * [...] * The minimum sum for 4 distinct digits is 10. The maximum sum for 2 distinct digits is 11. ThereforeS3 must be 10 or 11. 3. Determine the Line Sums (S2 and S3) * [...] * Let A = digits in {r2c4r3c4}. Let B = digits in {r4c4r5c4r6c4r6c5}. A and B are disjoint sets of digits. * If S2=10A={46}. If S2=11A={56}. * If S3=10B={1234}. If S3=11B={1235}. * Test combinations: * S2=10S3=10: A={46}B={1234}. Union = {12346}. Missing 5. Impossible. * S2=10S3=11: A={46}B={1235}. Union = {123456}. Possible. * S2=11S3=10: A={56}B={1234}. Union = {123456}. Possible. * S2=11S3=11: A={56}B={1235}. Union = {12356}. Missing 4. Impossible. * Soeither (S2=10S3=11) or (S2=11S3=10). 4. Use the r1c4 = r6c5 Relationship [...] 5. Test the Two Possible Scenarios for S2 and S3 * Scenario 1: S2=10S3=11 [...] * Scenario 2: S2=11S3=10 [...] [...]
Existing reasoning benchmarks for large language models (LLMs) frequently fail to capture authentic creativity, often rewarding memorization of previously observed patterns.We address this shortcoming with S u d oku -Be nc h, a curated benchmark of challenging and unconventional Sudoku variants specifically selected to evaluate creative, multi-step logical reasoning.Sudoku variants form an unusually effective domain for reasoning research: each puzzle introduces unique or subtly interacting constraints, making memorization infeasible and requiring solvers to identify novel logical breakthroughs ("break-ins").Despite their diversity, Sudoku variants maintain a common and compact structure, enabling clear and consistent evaluation.Sudoku-Bench includes a carefully chosen puzzle set, a standardized text-based puzzle representation, and flexible tools compatible with thousands of publicly available puzzles-making it easy to extend into a general research environment.Baseline experiments show that state-of-the-art LLMs solve fewer than 15% of puzzles unaided, highlighting significant opportunities to advance long-horizon, strategic reasoning capabilities.Alongside this report, we release the Sudoku-Bench repository.</p>
<p>Matty Didn't Call You Back Because He Had The Wrogn Number by Lake</p>
<p>Normal sudoku rules apply.</p>
<p>Every clue in the puzzle is wrogn.A clue is wrogn if it is not completely correct.</p>
<p>Parity Paradox by Marty Sears</p>
<p>Normal sudoku rules apply.Each line within a 3x3 box has the same total, which is displayed in yellow in that box.(2-digit yellow totals read from left to right.)The digit in the purple square indicates how many of the yellow totals have the same parity (oddness/evenness) as itself (ie the purple square digit).</p>
<p>Reticule by Skeptical Mario</p>
<p>Figure 1 | Each Sudoku variant has a unique set of constraints explicitly described in the puzzle rules.Puzzles may feature whimsical rules such as in Rat Run, or meta-level constraints, such as requiring all standard Sudoku rules to be intentionally violated.</p>
<p>Introduction</p>
<p>Large-scale language models excel at short-form deduction (Long, 2023;Wei et al., 2022), yet genuinely creative reasoning remains elusive.Many standard benchmarks, where current models already rival or surpass human performance (Glazer et al., 2024;Hendrycks et al., 2021;Phan et al., 2025), often reward the memorization of solution templates (Bubeck et al., 2023).Once these templates are implicitly memorized, incremental accuracy gains offer limited insight into a model's capacity for novel reasoning.Benchmarks such as ARC (Chollet, 2019) effectively resist memorization; however, their solutions, while novel to models, remain straightforward for humans, insufficiently capturing the depth of human creative reasoning.</p>
<p>We propose Sudoku variants (Fig. 1) as a unique domain addressing this gap.A Sudoku variant is a logical puzzle defined by a partially filled  ×  grid, accompanied by visual constraints and even a problem-specific set of rules that can only be described in natural language.Yet, each puzzle still admits a unique solution-an  ×  grid fulfilling its constraints.Puzzle creators introduce original rules or combine common constraints in novel ways.Hundreds of user-submitted Sudoku variants are published daily on platforms like Logic Masters Germany (Log), deliberately designed to require creative insights and subtle logical breakthroughs.Such puzzles precisely target the type of novel, multi-step reasoning that memorization-focused and even popular reasoning benchmarks fail to consistently measure (Zhang et al., 2024).This paper's contribution is twofold.First, we introduce open-source tools interfacing directly with the popular puzzle application SudokuPad (Neumann, 2021), facilitating both agentic tool-use interaction and standardized textual puzzle representations.The agentic interaction provides an API to fetch images of the current board state and access to all the annotation tools available in SudokuPad that human solvers usually rely on.Our textual format isolates logical reasoning from visual processing, enabling effective evaluation with current language models.Second, we present Sudoku-Bench, a carefully curated benchmark of 100 Sudoku variants, selected in collaboration with hosts from the Cracking the Cryptic YouTube channel.These puzzles span a wide range of difficulties and reasoning styles, deliberately chosen to test model performance across diverse logical pathways and puzzle-specific "break-ins."</p>
<p>Our experiments showcase S u d oku -Be nc h poses a striking challenge for current state-of-the-art models.Without tool assistance, even the strongest publicly available LLM evaluated solves fewer than 15% of the benchmark.Notably, most of the successful completions come from the simplest subset of 4 × 4 puzzles, with performance rapidly collapsing with larger and less conventional grids.This is observed in both the one-shot configuration (prompt a model to solve a puzzle in one response) and a multi-step configuration (multi-turn interaction between the model providing at least one digit and the user providing the updated board state).</p>
<p>Beyond benchmarking, Sudoku variants offer a fertile laboratory for reasoning research.An extensive, ever-growing supply of human-generated puzzles allows scalable difficulty progression, from simpler 4 × 4 puzzles suitable for small models to highly intricate 9 × 9 puzzles, the hardest of which can stump all but the best expert human solvers.Rich auxiliary data, including detailed expert solution transcripts and interaction traces, facilitate imitation learning.We include, as part of S u d oku -Bench thousands of hours of reasoning transcripts and actions taken when solving from Cracking the Cryptic, a popular YouTube channel dedicated to detailed demonstrations of solving Sudoku variants with over 250M views.This data is entirely available for researchers who wish to explore supervised approaches to learn and fine-tune models from human reasoning -qualitatively far beyond the depth and diversity of synthetic reasoning datasets with current state-of-the-art language models (Li et al., 2025;Muennighoff et al., 2025).</p>
<p>The remainder of this paper proceeds as follows: Section 2 surveys Sudoku variants and their reasoning demands.Section 3 details the Sudoku-Bench dataset, text interface, and evaluation framework.Section 4 presents baseline results and analyses of model failure modes.We review related work in Section 5, and conclude with open research directions in Section 6.</p>
<p>Background: Sudoku Variants</p>
<p>Traditional Sudoku involves completing a 9 × 9 grid such that each digit from 1 to 9 appears exactly once in every row, column, and 3 × 3 subgrid.This structure provides a foundation for numerous variants that introduce additional constraints.For instance, Killer Sudoku combines elements of Sudoku and Kakuro, requiring digits within outlined cages to sum to specified totals without repeats.Thermometers are paths of adjacent cells where digits must increase monotonically.Digits along arrows must sum to the digit in the circled cell at the base.Kropki dots between cells indicate specific relationships, such as consecutive numbers or a 1:2 ratio.</p>
<p>The availability of web-based puzzle-making tools allowed puzzle authors to invent their own variants.In early 2020, the puzzle-hosting site Logic Masters saw a surge in the number of puzzles posted.As of May 2025, more than 27,000 user-submitted variants are published on the site (Log).</p>
<p>Puzzle creators frequently combine multiple constraints in unique ways.Often, these combined constraints result in puzzles starting with minimal or no digits, necessitating extensive logical reasoning to determine the initial placement, termed a "break-in."Such puzzles require solvers to meticulously explore the interaction of constraints, significantly diverging from the eager guessing often observed in reasoning LLMs (Section 4).</p>
<p>Beyond these standard constraint types, puzzle setters often employ meta-constraints, which involve deducing puzzle-specific parameters (e.g., "digits in a cage sum to an unknown value to be determined by solving," or "the line must be identified as either a palindrome or a renban sequence").These meta-constraints add another layer of complexity and creative reasoning.</p>
<p>Analysis</p>
<p>Confirmation of the puzzle rule of arrow and circle</p>
<p>Analysis</p>
<p>Confirmation of the puzzle rule of knight's move (Strange) Analysis</p>
<p>The model tries to exclude the possibility of some cells being 9, but the intuition is unclear, and this step of analysis does not effectively narrow the candidate space.</p>
<p>Search</p>
<p>With the temporary information of "circles&gt;=3", the model starts "guessing" until contradiction.</p>
<p>Firstly it tries circle cell r4c3=3, r4c6=5, which finally leads to a contradiction.While the search strategy is brutal, the model also makes mistakes of missing potential placements (e.g. when r4c3=3, it can also have r4c1=2, r4c2=1).</p>
<p>Then it tries circle cell r4c3=4, r4c6=5, etc.This search attempt finally leads to a contradiction again.</p>
<p>More Search</p>
<p>The above attempts of search provide a bit more information but the model continues to rely on search to solve the puzzle and reached a wrong solution.</p>
<p>Gemini 2.5 Pro Preview's attempt to solve the puzzle Ascension.In contrast to the break-in by a human solver, the model failed to effectively narrow its search space and had to rely on a more brute-force search strategy, which did not lead to the correct solution.Puzzle authors are ultimately limited only by imagination, often developing whimsical and novel rulesets (e.g., puzzles themed around rats in mazes (Fig. 1)).Crucially, all Sudoku variants maintain a structured format: an  ×  grid, natural-language puzzle rules, visual elements easily encoded as text, and a single unique solution.This structured yet flexible framework makes Sudoku variants exceptionally suitable for systematically investigating creative reasoning capabilities, meaning that the puzzles are very diverse and challenging but grounded and easy to verify if correct.</p>
<p>Puzzle example: Ascension</p>
<p>We illustrate some of these features with an example.Figure 2a highlights the novel interaction between a knight's move restriction and arrow constraints.</p>
<p>To find the puzzle's break-in, the solver must make three observations.First, whatever the digit highlighted in green (r4c6, box 5), it must occur somewhere in box 2, but not in column 6 (by standard Sudoku rules), or along its arrow tip, or a knight's move away, thus can only occur in one of the two half-shaded cells r1c4 or r1c5.This same pattern applies to the other cell groups highlighted by the other colors shown in the middle panel.The second observation is that since digits on the arrow must be smaller than the corresponding circled base, this creates a long-range chain dependency across the highlighted cells, namely, the circled cells shaded yellow, purple, green, blue, then red, must be monotonically increasing.This is a key insight but not enough to determine an exact digit yet.</p>
<p>The third observation is that the purple cell must be the sum of three Sudoku digits, the two in its arrow tip r4c1 and r4c2, but one of which is equal to the yellow cell of r7c3, which itself is the sum of two Sudoku digits by arrow rules.The only digit that can be the sum of three Sudoku digits and leave enough room for the monotonic chain along green, blue and red, is six.Therefore r4c6 must be six and the subsequent digits in the monotonic chain are forced (right panel).</p>
<p>In a video demonstrating this puzzle solve, an expert solver discovered this break-in in about 4.5 minutes, and a full puzzle solve taking about 35 minutes. 1 In all LLMs we tested, no model was able to make progress.For example, we show the reasoning summary of Gemini 2.5 Pro Preview (Fig. 2b), which was able to successfully parse and identify the puzzle constraints, but quickly resorts to guesswork and search.This highlights that there is still a gap between how LLMs reason and how humans prefer to reason; LLMs can rely on brute-force but humans will prefer to save time and energy by using precise logic to find shortcuts to correct digits.We hope to see this benchmark encouraging work on creating LLMs that reason in a more "human-like" manner.</p>
<p>The Ascension example highlights two facets of Sudoku variants.First, although both knight-move and arrow constraints are commonplace, this specific interaction is unique to this particular puzzle.Therefore, the memorization-resistance of Sudoku variants is not exclusively due to the inclusion of novel rulesets; familiar constraints can induce a solving tactic never seen before.Indeed, some of the most difficult puzzles adopt deceptively simple rulesets.The second point is that for puzzles with few or no given digits (as is common in variants), the search space is too large for initial guesswork to be effective.This also often necessitates a kind of meta-reasoning where one must decide at the outset what reasoning techniques should be applied, e.g. the use of coloring, set theory or looking at digit parity.</p>
<p>This pattern of needing to spend time at the beginning to understand how the constraints interact in a new manner is normal when humans tackle these puzzles.This also means that some of these initial deductions remain pertinent throughout the solve, meaning that in order to robustly solve some of these puzzles over 100s of steps will either require a form of memory, like a scratchpad, or a very long context window.</p>
<p>S u d oku -Be nc h: Dataset and Benchmark Design</p>
<p>We sought to select 100 puzzles that are representative of the breadth of Sudoku variants.To establish a graded evaluation curve, we selected 15 4 × 4 puzzles, 15 6 × 6 puzzles, and 70 9 × 9 puzzles.The 15 4 × 4 puzzles are included, in part, to measure progress in even modestly sized language models.Fifty of the 9 × 9 puzzles were curated by the hosts of Cracking the Cryptic exclusively for this benchmark.The selected puzzles evenly span difficulty ratings from novice-friendly "1-star" puzzles to expert-level "5-star" challenges that may require hours of careful analysis before any digits can be confidently placed.Twenty of the puzzles are difficult vanilla Sudokus, which were supplied by the puzzle company Nikoli, which popularized Sudoku in the 1980s.We aimed to create a smooth ramp in complexity such that an initial attempt at tackling the benchmark can yield some early success, but fully solving it will be very challenging, and we hope that this benchmark will resist being solved for a significant time span.</p>
<p>Text descriptions Each puzzle is given a pure text representation.For instance, Fig 3 shows a simple 4 × 4 puzzle whose line paths are represented as a sequence of rxcy (row x column y) coordinates, and the location of the dot is described as the two cells it lies between.The rules, visual elements, grid size, and initial board state (if any digits are given) are sufficient to unambiguously specify the puzzle and converted into a prompt.</p>
<p>While some of the most recent reasoning models have shifted toward multimodal inputs, we found that most, including OpenAI o3 (OpenAI, 2025), struggle in converting 9 × 9 puzzles into accurate coordinates.Puzzle benchmarks such as Enigma (Wang et al., 2025) and VGRP (Ren et al., 2025) emphasize the visual aspect of puzzles and require multimodal models.Given that current frontier models still struggle in exact specification of the visual elements of Sudoku puzzles, we opted to specify all elements precisely in text to isolate the creative reasoning process itself from visual understanding.Each puzzle's text representation has been precomputed for puzzles on Sudoku-Bench.We provide the code for extracting text descriptions from a puzzle specified in SudokuPad, allowing researchers to utilize this harness in other puzzles.</p>
<p>Note that many of the puzzles would benefit from visual reasoning, some even potentially requiring it, since many of the break-ins are geometric and use symmetry, or have some rules that reference the shapes in the puzzle.Some puzzles can be very visually dense (See Bottom-Right in Fig 1) and current vision model we tested are not powerful enough to extract all the features, like the small numbers.We suspect that solving this benchmark using vision would represent a significant improvement over current multimodal LLMs.</p>
<p>Expert reasoning traces</p>
<p>A core question is whether advancing reasoning capabilities in LLMs can benefit from adopting more "human-like" thinking.In reinforcement learning models, pretraining on human supervision is common, while other work has shown that RL from scratch yields better performance in contained environments (Hester et al., 2018;Lowe et al., 2017;Ouyang et al., 2022;Silver et al., 2016).Vanilla Sudoku is an interesting domain in that the strategies that humans use differ so significantly from search-based solvers (Pelánek, 2011), and this effect is especially pronounced in Sudoku variants.</p>
<p>The YouTube channel Cracking the Cryptic offers a particularly unique opportunity to explore the benefits of imitation learning.The channel contains over 3,000 published videos demonstrating the solving process of Sudoku variants.Notably, the hosts must verbally describe their thinking process, explaining to the viewer each logical deduction.A typical puzzle takes the hosts around 60 minutes</p>
<p>Rules:</p>
<p>Normal 4x4 sudoku rules apply.Digits separated by a black dot are in a 1:2 ratio.The difference between two adjacent digits on a coloured line indicates exactly how many pairs of adjacent digits along that line have that exact difference.</p>
<p>Initial grid:</p>
<p>. . . . . . . . . . . . . . . .</p>
<p>Differences Count -part 1 by Sujoyku and Marty Sears</p>
<p>Visual elements:</p>
<p>-line, color: pale green, coords: r1c1 r2c2 r4c3 -line, color: plum, coords: r2c2 r2c3 r3c4 r4c4 -circle, color: black, location: between r4c1 and r4c2</p>
<p>Figure 3 | A text representation of a puzzle.The rules, initial grid, and a text description of visual elements are sufficient to unambiguously specify the puzzle.</p>
<p>to solve, while some of the more difficult puzzles featured on the channel are over 3 hours in length.We developed a dataset consisting of the audio transcripts of each solve, together with a sequence of SudokuPad actions extracted from the video.The actions were extracted using a machine learning model trained on ground truth actions simulated on SudokuPad and then applied to video frames.This dataset is hosted on HuggingFace2 under an MIT license in agreement with the hosts of the channel.</p>
<p>Dataset format</p>
<p>The Sudoku-Bench puzzle dataset3 contains three subsets, challenge_100, nikoli_100, and ctc.The challenge_100 is described above and represents the core benchmark.Additional puzzle data include nikoli_100, a collection of hand-made vanilla Sudokus supplied by Nikoli for this benchmark (20 of which are featured in challenge_100).The nikoli_100 are designed to highlight creative or human-like reasoning in their solution paths, and may be applicable to many of the research approaches that use vanilla Sudoku as a testbed (Section 5).The ctc includes 2,565 Sudoku variants that have been solved on Cracking the Cryptic.Due to the breadth and variety of Sudoku variants, the text representation of each puzzle in ctc has not undergone manual checking, and an unambiguous representation of the board would require a screenshot in some cases.</p>
<p>SudokuPad environment</p>
<p>We also provide tools for interacting with SudokuPad in an agentic environment.SudokuPad enables common note-taking strategies used by human solvers, including color-coding cells (as in Fig. 2a) or providing candidate digits or pencil marks to cells.Our simple harness allows models to directly interface with the application to make use of these tools.Using SudokuPad in-the-loop may fit well with related benchmarks that evaluate reasoning models (including vision language models) in simple game environments (Paglieri et al., 2024;Ren et al., 2025).Our evaluation in this paper (Section 4) uses text interaction (relying only on SudokuPad for the initial puzzle data extraction).We make all   ).In the multi-step setting, a model is prompted to provide any number of digits in its response, with the user providing an updated board state at each turn.Interaction is terminated if the model makes an incorrect placement.The average number of correct placements are presented in the first column set.In the single-shot setting the model is prompted to solve the entire puzzle in a single response."-" indicates that fewer than the required number of responses were available due to cost limitations, so an aggregate could not be computed.</p>
<p>of these SudokuPad tools available for researchers on our repository https://github.com/SakanaAI/Sudoku-Bench.</p>
<p>Evaluation Framework</p>
<p>Multi-step and single-shot We evaluate models in both multi-round and single-shot configurations.</p>
<p>In a multi-round setup, we prompt the model to analyze the board and give at least one valid digit placement per response.We clarify that this is a committed digit(s) that cannot be undone (in the model's reasoning trace, any amount of internal backtracking is possible in order to deduce the digit).</p>
<p>Once the digit is placed, the user displays the updated board state.We continue until the puzzle is solved or the LLM misplaces any digit.In the multi-round setting, we track both the solve rate and correct digit placements per puzzle.To keep the context window manageable, we keep the most recent 5 responses from the LLM in context, while always keeping the first user message with the puzzle specification and instructions.We report the averages as average solve rate and average correct digits.In our evaluation, we run a single evaluation per model and per puzzle, so the average is across the 100 puzzles in the set.</p>
<p>In the single-shot configuration, we prompt the model to provide a solution in a single response.A single-shot configuration is appropriate for evaluating models with sufficiently large context, or for a more straightforward evaluation of the smaller 4 × 4 puzzles.In the single-shot setting, we report only the average solve rate.</p>
<p>Baseline Performance and Analysis</p>
<p>We evaluated the current generation of state-of-the-art large language models on Sudoku-Bench, revealing substantial difficulty posed by these Sudoku variants.Table 1 summarizes model performance across puzzle sizes and interaction modes on benchmark.Even leading models such as o3 mini high and Gemini 2.5 pro preview demonstrated solve rates below 15% for the complete set.Notably, performance varied significantly by puzzle size: models generally solved smaller 4 × 4 puzzles at rates between 40% to 73%, but performance sharply declined for 6 × 6 grids and dropped nearly to zero on 9 × 9 puzzles, underscoring the rapid escalation in complexity.</p>
<p>Comparing single-shot to multi-step evaluation modes, allowing iterative feedback slightly improved outcomes for smaller puzzles but did not meaningfully impact results for larger puzzles.The minimal difference between modes suggests that the fundamental difficulty for these models lies not merely in incremental reasoning but in effectively identifying initial logical breakthroughs.</p>
<p>Categorizing model failures</p>
<p>Analysis</p>
<p>The model effectively reduces the possible cases of two blue lines' segment sum (S2 and S3) to two candidates (10 or 11).</p>
<p>Analysis</p>
<p>From the 2 x 2 = 4 combinations of S2 and S3, it further reduces it to 2 possible cases, which forms a small constrained space for latter search.</p>
<p>Search</p>
<p>The model starts to search by starting from (S2=10, S3=11) or (S2=11, S3=10) and guessing other numbers on the lines until a contradiction.</p>
<p>Due to the small search space, the model can find the only correct placements of the lines and reach a correct global solution in the end.</p>
<p>Gemini 2.5 Pro's solution to the puzzle Sumthings.A successful solve While models often struggle with complex break-ins, they can sometimes succeed on moderately complex puzzles by effectively narrowing the search space.For instance, Figure 5 illustrates a 6×6 puzzle, Sumthings, which Gemini 2.5 Pro Preview solved.The model adopted a strategy of reducing the search space to a manageable size, then employing search to find the correct solution.This approach, however, proves less effective as puzzle complexity increases, where identifying specific "break-in" insights becomes crucial, as demonstrated by the Ascension example (Figure 2).</p>
<p>Related Work</p>
<p>S u d oku -Be nc h complements existing benchmarks designed to evaluate advanced reasoning in artificial intelligence, with a particular focus on Sudoku variants as a structured domain for assessing creative and logical deduction.</p>
<p>Benchmarks targeting creative deductive insight</p>
<p>Benchmarks such as the Abstraction and Reasoning Corpus (A R C; Chollet, 2019) present diverse tasks to test reasoning and generalization beyond pattern memorization.S u d oku -Be nc h similarly introduces novel constraints for each puzzle, resisting memorization through a continuous influx of unique puzzles.Unlike ARC, which emphasizes tasks simple for humans but challenging for AI, Sudoku variants span a broader difficulty spectrum, including puzzles challenging even for expert human solvers.Nonetheless, Sudoku puzzles offer recognizable logical breakthroughs readily appreciated by human novices, making Sudoku-Bench a valuable resource for precise evaluation of creative reasoning.</p>
<p>Puzzle-centric reasoning datasets</p>
<p>Several benchmarks focus on puzzle-solving for evaluating reasoning skills (Giadikiaroglou et al., 2024).For instance, PU Z Z L E S (Estermann et al., 2024) compiles canonical logic puzzles; Tyagi et al. (2024) systematically analyze grid puzzle-solving by LLMs; and E n igma E val (Wang et al., 2025) evaluates a large suite of problems from puzzle competitions.Recent additions include VG R P-Be nc h (Ren et al., 2025) for visual-grid reasoning, L ogicGame (Gui et al., 2024) for rule-based reasoning, and PuzzlePlex (Long et al., 2024) for evaluating conversational agents' reasoning.BALROG (Paglieri et al., 2024) evaluates LLM and VLM reasoning in complex game environments and could be extended using tools from Sudoku-Benchto include SudokuPad as an environment.</p>
<p>Sudoku as a reasoning testbed</p>
<p>The standard Sudoku puzzle has been extensively utilized in machine learning research.Models include Recurrent Relational Networks (Palm et al., 2018) employing message-passing, differentiable SATNet consistency layers (Wang et al., 2019), maskeddenoising and diffusion methods (Kim et al., 2025;Ye et al., 2024), and Kuramoto-inspired oscillator dynamics (Miyato et al., 2024).Further, large language models have achieved human-level accuracy through structured prompting and reasoning decomposition (Long, 2023).Shah et al. (2024) showed a high solve rate on vanilla Sudokus by training on a sequence of steps from a solver.S u d oku -Bench extends this research tradition by incorporating diverse and novel puzzle constraints, enabling evaluations that specifically target multi-step, strategic, and creative reasoning.</p>
<p>Discussion</p>
<p>The role of tool use Evaluating model reasoning can be distinguished by whether external tools, such as constraint solvers or code execution environments, are available.Without tool use, the evaluation specifically assesses the model's intrinsic reasoning capabilities, including logical deduction, maintaining global consistency, and internally generating creative insights, akin to solving puzzles by hand.This approach emphasizes pure cognitive reasoning skills and has been the primary evaluation mode presented in our baselines (Section 4).</p>
<p>Conversely, allowing tool use tests the model's ability to translate a given puzzle into a formal representation suitable for external solvers, effectively interact with these tools, and interpret solver results correctly.Standard Sudoku puzzles become straightforward when a solver is employed.Variants that only employ standard constraints such as arrows, cages, etc, are also easily solved by code execution.A third category of puzzles require natural language understanding and are not straightforward to interpret as a constraint satisfaction problem.This third category is itself a meaningful test for reasoning models with tool-use enabled.However, our current intention is to assess the reasoning required to find a puzzle's "break-in," and many puzzles such as Ascension from Fig. 2a are easily solved by tool-use, but the solution path would be substantially different than that intended by the puzzle setter.Therefore we selected the 100 puzzles of S u d oku -Be nc h for evaluating models without tool-use.Future work could consider a separate tool-use track, potentially with a different collection of puzzles.</p>
<p>Conclusion</p>
<p>We introduced S u d oku -Be nc h, a unified benchmark built around modern Sudoku variants that systematically stress long-horizon deduction, rule-interpretation, and strategic planning.In addition, the benchmark is uniquely suited for evaluating creative reasoning via the rich and varied collection of break-ins featured in most puzzles.The benchmark includes a curated puzzle corpora with textual representations, providing a controlled substrate for measuring how well language models cope with novel, tightly coupled constraints.Baseline experiments show that frontier LLMs solve fewer than 15% of instances without external tools, and performance falls sharply on 9 × 9 variants-evidence that substantial headroom remains for improvements.</p>
<p>7.</p>
<p>Digit 1 Analysis (Attempt C(4,3)=4): Led to C(4,1)=1, C(4,2)=3, C(5,4)=1, C(6,5)=3.Then C(4,6)=5, C(3,5)=2, C(2,4)=3.Then {C(5,6), C(6,6)}={1,4} or {2,3}.{1,4} means C(5,6)=1, C(6,6)=4.Fails (row 5 conflict: C(5,4)=1, C(5,6)=1).{2,3} requires C(5,6)=3, C(6,6)=2 (C(5,6)=2 impossible by AK C(3,5)=2, C(6,6)=2 impossible by AK C(3,5)=2).Grid: C(4,1)=1, C(4,2)=3, C(4,3)=4, C(5,4)=1, C(6,5)=3.C(2,4)=3, C(3,5)=2.C(4,6)=5.C(5,6)=3, C(6,6)=2.Check AK 3: C(4,2)=3, C(6,5)=3, C(2,4)=3, C(5,6)=3.C(4,2) vs C(2,4): OK.C(4,2) vs C(5,6): OK.C(6,5) vs C(2,4): OK.C(6,5) vs C(5,6): OK.C(2,4) vs C(5,6): OK.Looks OK.Let's write partial grid:</p>
<p>Figure 2 |
2
Figure 2 | Ascension example.</p>
<p>Figure 4 |
4
Figure 4 | Response categorization for the single-shot setting.</p>
<p>Analyzing model failures indicated several recurring patterns which we categorize in Fig.4.The most common failure mode was presenting with confidence an Incorrect Solution.Other failure modes included Surrender (model explicitly gives up), Missing Information (model incorrectly claims puzzle information or given constraints are incomplete), and Claimed Contradiction (model mistakenly identifies contradictions in the puzzle rules).Of note is Missing Information.Since variants are not as densely represented in the training set of foundation models compared to vanilla Sudoku, it appears the new rules and variants throw them off, most notably due to the fact that variants typically have fewer starting digits (often none) compared to the minimum of 17 in a vanilla 9 × 9 Sudoku.In addition, a part of model responses contain No Reasoning Trace so we cannot make a fine-grained categorization of its error type, otherwise we use Claude-3.5-Haikuto classify a wrong solution response into one of the other four error types.Analysis Understanding the rules.</p>
<p>Figure 5 |
5
Figure 5 | Sumthings example.</p>
<p>Model Multi-step correct placements Multi-step solve rate (%) Single-shot solve rate (%)
4×4 6×6 9×9All4×4 6×6 9×9All4×4 6×6 9×9AllO3 Mini High9.70.7--60.0 0.0--73.3 6.72.914.0Gemini 2.5 Pro11.6 0.61.83.173.3 0.0</p>
<p>Table 1 |
1
Sudoku-Bench leaderboard.Performance comparison of various LLMs on Sudoku-Bench.Percentage of puzzles completely solved for each evaluation mode (multi-step vs. single-shot), stratified by grid size.The right-most All columns aggregate across grid sizes (15 puzzles for 4×4 and 6×6, 70 for 9×9</p>
<p>https://www.youtube.com/watch?v=-7OR_IK4Th8
huggingface.co/datasets/SakanaAI/Sudoku-CTC-Reasoning
huggingface.co/datasets/SakanaAI/Sudoku-Bench
AcknowledgmentsWe thank Neumann, author of SudokuPad, for help with the development of tooling used in Sudoku-Bench, and for permission in using SudokuPad as part of this project.Sudoku-Bench is developed in partnership with Cracking the Cryptic, with an agreement to provide content from the channel for use in the AI research community.The puzzles provided in the benchmark are featured on the channel.We acknowledge all puzzle creators and provide a list of setter acknowledgments in our repository.We thank the hosts of Cracking the Cryptic, Simon Anthony and Mark Goodliffe, for various help in the development of this benchmark, including the curated selection of puzzles in S u d oku -Be nc h, and for providing SudokuPad replay files for the reasoning traces described in 3.1.The handmade vanilla Sudoku puzzles were provided by Nikoli.We thank Nikoli for graciously agreeing to provide their puzzles for this benchmark.<em> C(4,6) = C(5,6)+C(6,6) = C(3,5)+C(2,4).C(4,6) != C(5,6), C(6,6).C(4,6) != C(3,5), C(2,4).</em> C(1,6) = C(1,5)+C(1,4) = C(2,7)+C(3,8).C(1,6) != C(1,5), C(1,4).C(1,6) != C(2,7), C(3,8).4. Anti-Knight + Arrow Interactions: * C(4,3) is knight move from C(2,4) &amp; C(3,5).Since C(4,6) = C(2,4)+C(3,5), C(4,6) &gt; C(2,4) and C(4,6) &gt; C(3,5).This means C(4,3) cannot be C(2,4) or C(3,5).Thus, C(4,3) != C(4,6).* [...] 5. Digit 9 Analysis: * Digits on arrows cannot be 9. * C(1,9) cannot be 9: If C(1,9)=9, then C(2,7)!=9, C(3,8)!=9 (knight moves).C(1,6)=C(2,7)+C(3,8). C(1,6)!=9 (same box).C(2,9)+C(3,9)=9.This doesn't lead to the previous contradiction.Let's re-evaluate.C(1,9)=9 -&gt; C(2,7)!=9, C(3,8)!=9.C(1,6) = C(2,7)+C(3,8).C(1,6) != C(1,9)=9 because they are in the same box.This means C(2,7)+C(3,8) != 9.This is possible.Where did the earlier contradiction come from?Ah, I misinterpreted C(1,6)!=9.Sumthings by Kennet's DadPlace the digits 1-6, once each, in every row, column, and region.Region borders divide the blue line into segments with the same sum.Description of the puzzle Sumthings.
S Bubeck, V Chandrasekaran, R Eldan, J A Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y.-F Li, S M Lundberg, H Nori, H Palangi, M T Ribeiro, Y Zhang, ArXiv, abs/2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023</p>
<p>On the measure of intelligence. F Chollet, 2019</p>
<p>Puzzles: A benchmark for neural algorithmic reasoning. B Estermann, L A Lanzendörfer, Y Niedermayr, R Wattenhofer, Advances in Neural Information Processing Systems. A Globerson, L Mackey, D Belgrave, A Fan, U Paquet, J Tomczak, C Zhang, Curran Associates, Inc202437</p>
<p>Puzzle solving using reasoning of large language models: A survey. P Giadikiaroglou, M Lymperaiou, G Filandrianos, G Stamou, arXiv:2402.112912024arXiv preprint</p>
<p>Frontiermath: A benchmark for evaluating advanced mathematical reasoning in ai. E Glazer, E Erdil, T Besiroglu, D Chicharro, E Chen, A Gunning, C F Olsson, J.-S Denain, A Ho, E De Oliveira Santos, O Jă¤rviniemi, M Barnett, R Sandler, M Vrzala, J Sevilla, Q Ren, E Pratt, L Levine, G Barkley, N Stewart, B Grechuk, T Grechuk, S V Enugandla, M Wildon, 2024</p>
<p>J Gui, Y Liu, J Cheng, X Gu, X Liu, H Wang, Y Dong, J Tang, M Huang, arXiv:2408.15778Logicgame: Benchmarking rule-based reasoning abilities of large language models. 2024arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, 2021NeurIPS</p>
<p>Deep q-learning from demonstrations. T Hester, M Vecerik, O Pietquin, M Lanctot, T Schaul, B Piot, D Horgan, J Quan, A Sendonaris, I Osband, G Dulac-Arnold, J Agapiou, J Z Leibo, A Gruslys, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence, AAAI'18/IAAI'18/EAAI'18. the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence, AAAI'18/IAAI'18/EAAI'18AAAI Press2018</p>
<p>Train for the worst, plan for the best: Understanding token ordering in masked diffusions. J Kim, K Shah, V Kontonis, S Kakade, S Chen, arXiv:2502.067682025arXiv preprint</p>
<p>Llms can easily learn to reason from demonstrations structure. D Li, S Cao, T Griggs, S Liu, X Mo, E Tang, S Hegde, K Hakhamaneshi, S G Patil, M Zaharia, arXiv:2502.073742025not content, is what matters! arXiv preprint</p>
<p>Large language model guided tree-of-thought. J Long, arXiv:2305.082912023arXiv preprint</p>
<p>PuzzlePlex: A benchmark to evaluate the reasoning and planning of large language models on puzzles. Y Long, T Jiang, Y Zhao, A Cohan, D Shasha, 2024</p>
<p>Multi-agent actor-critic for mixed cooperative-competitive environments. R Lowe, Y Wu, A Tamar, J Harb, P Abbeel, I Mordatch, Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17, page 6382â€"6393. the 31st International Conference on Neural Information Processing Systems, NIPS'17, page 6382â€"6393Red Hook, NY, USACurran Associates Inc2017ISBN 9781510860964</p>
<p>T Miyato, S Löwe, A Geiger, M Welling, arXiv:2410.13821Artificial kuramoto oscillatory neurons. 2024arXiv preprint</p>
<p>N Muennighoff, Z Yang, W Shi, X L Li, L Fei-Fei, H Hajishirzi, L Zettlemoyer, P Liang, E Candès, T Hashimoto, arXiv:2501.19393s1: Simple test-time scaling. 2025arXiv preprint</p>
<p>. S Neumann, Sudokupad, 2021</p>
<p>Openai, OpenAI o3 and o4-mini System Card. 2025</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P Christiano, J Leike, R Lowe, Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS '22. the 36th International Conference on Neural Information Processing Systems, NIPS '22Red Hook, NY, USACurran Associates Inc2022ISBN 9781713871088</p>
<p>D Paglieri, B Cupiał, S Coward, U Piterbarg, M Wolczyk, A Khan, E Pignatelli, Ł Kuciński, L Pinto, R Fergus, arXiv:2411.13543Benchmarking agentic llm and vlm reasoning on games. 2024arXiv preprint</p>
<p>Recurrent relational networks. R Palm, U Paquet, O Winther, Advances in neural information processing systems. 201831</p>
<p>Difficulty rating of sudoku puzzles by a computational model. R Pelánek, The Florida AI Research Society. 2011</p>
<p>Humanity's last exam. L Phan, A Gatti, Z Han, N Li, J Hu, H Zhang, C B C Zhang, M Shaaban, J Ling, S Shi, 2025</p>
<p>Y Ren, K Tertikas, S Maiti, J Han, T Zhang, S Süsstrunk, F Kokkinos, arXiv:2503.23064Vgrp-bench: Visual grid reasoning puzzle benchmark for large vision-language models. 2025arXiv preprint</p>
<p>Causal language modeling can elicit search and reasoning capabilities on logic puzzles. K Shah, N Dikkala, X Wang, R Panigrahy, arXiv:2409.105022024arXiv preprint</p>
<p>Mastering the game of Go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, S Dieleman, D Grewe, J Nham, N Kalchbrenner, I Sutskever, T Lillicrap, M Leach, K Kavukcuoglu, T Graepel, D Hassabis, 10.1038/nature16961Nature. 5297587Jan. 2016</p>
<p>Step-by-step reasoning to solve grid puzzles: Where do LLMs falter?. N Tyagi, M Parmar, M Kulkarni, A Rrv, N Patel, M Nakamura, A Mitra, C Baral, 10.18653/v1/2024.emnlp-main.1111Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Y Al-Onaizan, M Bansal, Y.-N Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics19898-19915. Nov. 2024</p>
<p>Enigmaeval: A benchmark of long multimodal reasoning challenges. C J Wang, D Lee, C Menghini, J Mols, J Doughty, A Khoja, J Lynch, S Hendryx, S Yue, D Hendrycks, 2025</p>
<p>Satnet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver. P.-W Wang, P Donti, B Wilder, Z Kolter, 2019</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, NeurIPS. 2022</p>
<p>Beyond autoregression: Discrete diffusion for complex reasoning and planning. J Ye, J Gao, S Gong, L Zheng, X Jiang, Z Li, L Kong, arXiv:2410.141572024arXiv preprint</p>
<p>A careful examination of large language model performance on grade school arithmetic. H Zhang, J Da, D Lee, V Robinson, C Wu, W Song, T Zhao, P Raja, C Zhuang, D Slack, Advances in Neural Information Processing Systems. 202437</p>            </div>
        </div>

    </div>
</body>
</html>
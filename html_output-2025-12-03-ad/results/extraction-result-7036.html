<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7036 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7036</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7036</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-643e596ba6c95e9dfc3bc952f4dc874fc0e9e9fb</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/643e596ba6c95e9dfc3bc952f4dc874fc0e9e9fb" target="_blank">CodeHalu: Investigating Code Hallucinations in LLMs via Execution-based Verification</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> The concept of code hallucinations is introduced and a classification method for code hallucination based on execution verification based on execution verification is proposed, which categorize code hallucinations into four main types: mapping, naming, resource, and logic hallucinations.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have made significant progress in code generation, offering developers groundbreaking automated programming support. However, LLMs often generate code that is syntactically correct and even semantically plausible, but may not execute as expected or fulfill specified requirements. This phenomenon of hallucinations in the code domain has not been systematically explored. To advance the community's understanding and research on this issue, we introduce the concept of code hallucinations and propose a classification method for code hallucination based on execution verification. We categorize code hallucinations into four main types: mapping, naming, resource, and logic hallucinations, with each category further divided into different subcategories to understand and address the unique challenges faced by LLMs in code generation with finer granularity. Additionally, we present a dynamic detection algorithm called CodeHalu designed to detect and quantify code hallucinations. We also introduce the CodeHaluEval benchmark, which includes 8,883 samples from 699 tasks, to systematically and quantitatively evaluate code hallucinations. By evaluating 17 popular LLMs using this benchmark, we reveal significant differences in their accuracy and reliability in code generation, offering detailed insights for further improving the code generation capabilities of LLMs.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7036.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7036.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeHalu</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeHalu dynamic detection algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dynamic detection algorithm introduced in this paper that detects and quantifies code hallucinations by using execution-based verification of generated code to classify failures into fine-grained hallucination types.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Execution-based verification: dynamic execution of generated code to validate behavior and identify hallucinations</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>CodeHaluEval</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Benchmark introduced alongside CodeHalu containing 8,883 samples from 699 tasks to systematically evaluate code hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Detection and classification of code hallucinations (including mapping, naming, resource, and logic hallucinations) via execution checks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CodeHalu provides a dynamic, execution-based method to detect and quantify code hallucinations and was used to evaluate 17 popular LLMs, revealing significant differences in their code generation accuracy and reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Details of algorithmic implementation, runtime environment, and quantitative performance of CodeHalu are not provided in the available text; exact failure modes and false‑positive/false‑negative rates are not reported in the provided excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeHalu: Investigating Code Hallucinations in LLMs via Execution-based Verification', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7036.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7036.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeHaluEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeHaluEval benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark introduced in this paper composed of 8,883 samples across 699 tasks, designed for systematic, execution-based evaluation and fine-grained categorization of code hallucinations in LLM-generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Execution-based evaluation (benchmark targets verification of generated code by executing/tests to reveal hallucinations)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>CodeHaluEval</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>8,883 samples from 699 code generation tasks covering four main hallucination categories (mapping, naming, resource, logic) to enable quantitative evaluation of hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Code generation correctness evaluation and hallucination detection, including logical correctness checks of generated programs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Used to evaluate 17 popular LLMs and to reveal substantial differences in model accuracy and reliability for code generation; enables finer-grained analysis by hallucination subtype.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>The provided excerpt does not include per-model benchmark scores, exact task definitions beyond broad categories, or annotation procedures; detailed benchmark statistics and limitations are not available in the text provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeHalu: Investigating Code Hallucinations in LLMs via Execution-based Verification', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7036.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7036.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logic hallucination</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic hallucination (category of code hallucination)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>One of the four main categories of code hallucinations defined in the paper, referring to generated code that is logically incorrect or fails to implement the required logic despite appearing plausible or syntactically correct.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>CodeHaluEval</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Part of the CodeHaluEval's taxonomy; includes tasks/samples where the primary failure mode is logical incorrectness in generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Logical correctness of program code / detection of logic-level errors in code generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Logic hallucinations are identified as a distinct and important failure mode in LLM code generation and are explicitly included in the benchmark taxonomy for targeted evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>No quantitative breakdown of prevalence or model-specific performance on logic hallucinations is provided in the available text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeHalu: Investigating Code Hallucinations in LLMs via Execution-based Verification', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7036.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7036.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>17 LLMs (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Seventeen popular large language models (unnamed in provided excerpt)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of 17 widely used LLMs that the authors evaluated with CodeHalu and CodeHaluEval to measure code generation accuracy, reliability, and types of hallucinations; specific model identities and configurations are not listed in the provided text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>CodeHaluEval</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Models were evaluated on the CodeHaluEval benchmark to measure different hallucination types and code execution correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Code generation tasks across 699 tasks in the benchmark, including detection of mapping, naming, resource, and logic hallucinations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Evaluation across the 17 models revealed significant differences in accuracy and reliability for code generation and hallucination propensity, highlighting variability among LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>The provided excerpt does not name the models, provide model sizes, architectures, reasoning augmentations, or report per-model performance metrics; therefore model-level conclusions cannot be reconstructed from the available text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeHalu: Investigating Code Hallucinations in LLMs via Execution-based Verification', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7036",
    "paper_id": "paper-643e596ba6c95e9dfc3bc952f4dc874fc0e9e9fb",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "CodeHalu",
            "name_full": "CodeHalu dynamic detection algorithm",
            "brief_description": "A dynamic detection algorithm introduced in this paper that detects and quantifies code hallucinations by using execution-based verification of generated code to classify failures into fine-grained hallucination types.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "architecture_type": null,
            "training_data": null,
            "reasoning_method": "Execution-based verification: dynamic execution of generated code to validate behavior and identify hallucinations",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "CodeHaluEval",
            "benchmark_description": "Benchmark introduced alongside CodeHalu containing 8,883 samples from 699 tasks to systematically evaluate code hallucinations.",
            "task_type": "Detection and classification of code hallucinations (including mapping, naming, resource, and logic hallucinations) via execution checks",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": null,
            "key_findings": "CodeHalu provides a dynamic, execution-based method to detect and quantify code hallucinations and was used to evaluate 17 popular LLMs, revealing significant differences in their code generation accuracy and reliability.",
            "limitations": "Details of algorithmic implementation, runtime environment, and quantitative performance of CodeHalu are not provided in the available text; exact failure modes and false‑positive/false‑negative rates are not reported in the provided excerpt.",
            "uuid": "e7036.0",
            "source_info": {
                "paper_title": "CodeHalu: Investigating Code Hallucinations in LLMs via Execution-based Verification",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "CodeHaluEval",
            "name_full": "CodeHaluEval benchmark",
            "brief_description": "A benchmark introduced in this paper composed of 8,883 samples across 699 tasks, designed for systematic, execution-based evaluation and fine-grained categorization of code hallucinations in LLM-generated code.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "architecture_type": null,
            "training_data": null,
            "reasoning_method": "Execution-based evaluation (benchmark targets verification of generated code by executing/tests to reveal hallucinations)",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "CodeHaluEval",
            "benchmark_description": "8,883 samples from 699 code generation tasks covering four main hallucination categories (mapping, naming, resource, logic) to enable quantitative evaluation of hallucinations.",
            "task_type": "Code generation correctness evaluation and hallucination detection, including logical correctness checks of generated programs",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": null,
            "key_findings": "Used to evaluate 17 popular LLMs and to reveal substantial differences in model accuracy and reliability for code generation; enables finer-grained analysis by hallucination subtype.",
            "limitations": "The provided excerpt does not include per-model benchmark scores, exact task definitions beyond broad categories, or annotation procedures; detailed benchmark statistics and limitations are not available in the text provided.",
            "uuid": "e7036.1",
            "source_info": {
                "paper_title": "CodeHalu: Investigating Code Hallucinations in LLMs via Execution-based Verification",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Logic hallucination",
            "name_full": "Logic hallucination (category of code hallucination)",
            "brief_description": "One of the four main categories of code hallucinations defined in the paper, referring to generated code that is logically incorrect or fails to implement the required logic despite appearing plausible or syntactically correct.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "architecture_type": null,
            "training_data": null,
            "reasoning_method": null,
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "CodeHaluEval",
            "benchmark_description": "Part of the CodeHaluEval's taxonomy; includes tasks/samples where the primary failure mode is logical incorrectness in generated code.",
            "task_type": "Logical correctness of program code / detection of logic-level errors in code generation",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": null,
            "key_findings": "Logic hallucinations are identified as a distinct and important failure mode in LLM code generation and are explicitly included in the benchmark taxonomy for targeted evaluation.",
            "limitations": "No quantitative breakdown of prevalence or model-specific performance on logic hallucinations is provided in the available text.",
            "uuid": "e7036.2",
            "source_info": {
                "paper_title": "CodeHalu: Investigating Code Hallucinations in LLMs via Execution-based Verification",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "17 LLMs (evaluated)",
            "name_full": "Seventeen popular large language models (unnamed in provided excerpt)",
            "brief_description": "A set of 17 widely used LLMs that the authors evaluated with CodeHalu and CodeHaluEval to measure code generation accuracy, reliability, and types of hallucinations; specific model identities and configurations are not listed in the provided text.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "architecture_type": null,
            "training_data": null,
            "reasoning_method": null,
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "CodeHaluEval",
            "benchmark_description": "Models were evaluated on the CodeHaluEval benchmark to measure different hallucination types and code execution correctness.",
            "task_type": "Code generation tasks across 699 tasks in the benchmark, including detection of mapping, naming, resource, and logic hallucinations",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": null,
            "key_findings": "Evaluation across the 17 models revealed significant differences in accuracy and reliability for code generation and hallucination propensity, highlighting variability among LLMs.",
            "limitations": "The provided excerpt does not name the models, provide model sizes, architectures, reasoning augmentations, or report per-model performance metrics; therefore model-level conclusions cannot be reconstructed from the available text.",
            "uuid": "e7036.3",
            "source_info": {
                "paper_title": "CodeHalu: Investigating Code Hallucinations in LLMs via Execution-based Verification",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.00929025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><!DOCTYPE html>
<html lang="en-US" xml:lang="en-US">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>
        CodeHalu: Investigating Code Hallucinations in LLMs via Execution-based Verification
                            | Proceedings of the AAAI Conference on Artificial Intelligence
            </title>


<meta name="generator" content="Open Journal Systems 3.2.1.1">
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/" />
<meta name="DC.Creator.PersonalName" content="Yuchen Tian"/>
<meta name="DC.Creator.PersonalName" content="Weixiang Yan"/>
<meta name="DC.Creator.PersonalName" content="Qian Yang"/>
<meta name="DC.Creator.PersonalName" content="Xuandong Zhao"/>
<meta name="DC.Creator.PersonalName" content="Qian Chen"/>
<meta name="DC.Creator.PersonalName" content="Wen Wang"/>
<meta name="DC.Creator.PersonalName" content="Ziyang Luo"/>
<meta name="DC.Creator.PersonalName" content="Lei Ma"/>
<meta name="DC.Creator.PersonalName" content="Dawn Song"/>
<meta name="DC.Date.created" scheme="ISO8601" content="2025-04-11"/>
<meta name="DC.Date.dateSubmitted" scheme="ISO8601" content="2020-09-10"/>
<meta name="DC.Date.issued" scheme="ISO8601" content="2025-04-11"/>
<meta name="DC.Date.modified" scheme="ISO8601" content="2025-04-11"/>
<meta name="DC.Description" xml:lang="en" content="Large Language Models (LLMs) have made significant progress in code generation, offering developers groundbreaking automated programming support. However, LLMs often generate code that is syntactically correct and even semantically plausible, but may not execute as expected or fulfill specified requirements. This phenomenon of hallucinations in the code domain has not been systematically explored. To advance the community's understanding and research on this issue, we introduce the concept of code hallucinations and propose a classification method for code hallucination based on execution verification. We categorize code hallucinations into four main types: mapping, naming, resource, and logic hallucinations, with each category further divided into different subcategories to understand and address the unique challenges faced by LLMs in code generation with finer granularity. Additionally, we present a dynamic detection algorithm called CodeHalu designed to detect and quantify code hallucinations. We also introduce the CodeHaluEval benchmark, which includes 8,883 samples from 699 tasks, to systematically and quantitatively evaluate code hallucinations. By evaluating 17 popular LLMs using this benchmark, we reveal significant differences in their accuracy and reliability in code generation, offering detailed insights for further improving the code generation capabilities of LLMs."/>
<meta name="DC.Format" scheme="IMT" content="application/pdf"/>
<meta name="DC.Identifier" content="34717"/>
<meta name="DC.Identifier.pageNumber" content="25300-25308"/>
<meta name="DC.Identifier.DOI" content="10.1609/aaai.v39i24.34717"/>
<meta name="DC.Identifier.URI" content="https://ojs.aaai.org/index.php/AAAI/article/view/34717"/>
<meta name="DC.Language" scheme="ISO639-1" content="en"/>
<meta name="DC.Rights" content="Copyright (c) 2025 Association for the Advancement of Artificial Intelligence"/>
<meta name="DC.Rights" content=""/>
<meta name="DC.Source" content="Proceedings of the AAAI Conference on Artificial Intelligence"/>
<meta name="DC.Source.ISSN" content="2374-3468"/>
<meta name="DC.Source.Issue" content="24"/>
<meta name="DC.Source.Volume" content="39"/>
<meta name="DC.Source.URI" content="https://ojs.aaai.org/index.php/AAAI"/>
<meta name="DC.Title" content="CodeHalu: Investigating Code Hallucinations in LLMs via Execution-based Verification"/>
<meta name="DC.Type" content="Text.Serial.Journal"/>
<meta name="DC.Type.articleType" content="AAAI Technical Track on Natural Language Processing III"/>
<meta name="gs_meta_revision" content="1.1"/>
<meta name="citation_journal_title" content="Proceedings of the AAAI Conference on Artificial Intelligence"/>
<meta name="citation_journal_abbrev" content="AAAI"/>
<meta name="citation_issn" content="2374-3468"/> 
<meta name="citation_author" content="Yuchen Tian"/>
<meta name="citation_author_institution" content="Hong Kong Baptist University"/>
<meta name="citation_author" content="Weixiang Yan"/>
<meta name="citation_author_institution" content="University of California, Santa Barbara"/>
<meta name="citation_author" content="Qian Yang"/>
<meta name="citation_author_institution" content="Montreal Institute for Learning Algorithms
Mila - Québec AI Institute"/>
<meta name="citation_author" content="Xuandong Zhao"/>
<meta name="citation_author_institution" content="University of California, Berkeley"/>
<meta name="citation_author" content="Qian Chen"/>
<meta name="citation_author_institution" content="Alibaba Group"/>
<meta name="citation_author" content="Wen Wang"/>
<meta name="citation_author_institution" content="Alibaba Group"/>
<meta name="citation_author" content="Ziyang Luo"/>
<meta name="citation_author_institution" content="Hong Kong Baptist University"/>
<meta name="citation_author" content="Lei Ma"/>
<meta name="citation_author_institution" content="The University of Tokyo
University of Alberta"/>
<meta name="citation_author" content="Dawn Song"/>
<meta name="citation_author_institution" content="University of California Berkeley"/>
<meta name="citation_title" content="CodeHalu: Investigating Code Hallucinations in LLMs via Execution-based Verification"/>
<meta name="citation_language" content="en"/>
<meta name="citation_date" content="2025/04/11"/>
<meta name="citation_volume" content="39"/>
<meta name="citation_issue" content="24"/>
<meta name="citation_firstpage" content="25300"/>
<meta name="citation_lastpage" content="25308"/>
<meta name="citation_doi" content="10.1609/aaai.v39i24.34717"/>
<meta name="citation_abstract_html_url" content="https://ojs.aaai.org/index.php/AAAI/article/view/34717"/>
<meta name="citation_pdf_url" content="https://ojs.aaai.org/index.php/AAAI/article/download/34717/36872"/>
<link rel="alternate" type="application/atom+xml" href="https://ojs.aaai.org/index.php/AAAI/gateway/plugin/WebFeedGatewayPlugin/atom">
<link rel="alternate" type="application/rdf+xml" href="https://ojs.aaai.org/index.php/AAAI/gateway/plugin/WebFeedGatewayPlugin/rss">
<link rel="alternate" type="application/rss+xml" href="https://ojs.aaai.org/index.php/AAAI/gateway/plugin/WebFeedGatewayPlugin/rss2">
    <link rel="stylesheet" href="https://ojs.aaai.org/index.php/AAAI/$$$call$$$/page/page/css?name=stylesheet" type="text/css" /><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Noto+Sans:400,400italic,700,700italic" type="text/css" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.css?v=3.2.1.1" type="text/css" /><link rel="stylesheet" href="https://ojs.aaai.org/public/journals/2/styleSheet.css?v=3.2.1.1" type="text/css" />
</head>
<body class="pkp_page_article pkp_op_view" dir="ltr">

    <div class="pkp_structure_page">

                <header class="pkp_structure_head" id="headerNavigationContainer" role="banner">
                         <nav class="cmp_skip_to_content" aria-label="Jump to content links">
    <a href="#pkp_content_main">Skip to main content</a>
    <a href="#siteNav">Skip to main navigation menu</a>
        <a href="#pkp_content_footer">Skip to site footer</a>
</nav>

            <div class="pkp_head_wrapper">

                <div class="pkp_site_name_wrapper">
                    <button class="pkp_site_nav_toggle">
                        <span>Open Menu</span>
                    </button>
                                        <div class="pkp_site_name">
                                                                <a href="                       https://ojs.aaai.org/index.php/AAAI/index
                    " class="is_text">Proceedings of the AAAI Conference on Artificial Intelligence</a>
                                        </div>
                </div>


                <nav class="pkp_site_nav_menu" aria-label="Site Navigation">
                    <a id="siteNav"></a>
                    <div class="pkp_navigation_primary_row">
                        <div class="pkp_navigation_primary_wrapper">
                                                                                <ul id="navigationPrimary" class="pkp_navigation_primary pkp_nav_list">
                                <li class="">
                <a href="https://ojs.aaai.org/index.php/AAAI/issue/current">
                    Current
                </a>
                            </li>
                                <li class="">
                <a href="https://ojs.aaai.org/index.php/AAAI/issue/archive">
                    Archives
                </a>
                            </li>
                                                            <li class="">
                <a href="https://ojs.aaai.org/index.php/AAAI/about">
                    About
                </a>
                                    <ul>
                                                                                    <li class="">
                                    <a href="https://ojs.aaai.org/index.php/AAAI/about">
                                        About the Journal
                                    </a>
                                </li>
                                                                                                                <li class="">
                                    <a href="https://ojs.aaai.org/index.php/AAAI/about/submissions">
                                        Submissions
                                    </a>
                                </li>
                                                                                                                                                                    <li class="">
                                    <a href="https://ojs.aaai.org/index.php/AAAI/about/privacy">
                                        Privacy Statement
                                    </a>
                                </li>
                                                                                                                <li class="">
                                    <a href="https://ojs.aaai.org/index.php/AAAI/about/contact">
                                        Contact
                                    </a>
                                </li>
                                                                        </ul>
                            </li>
            </ul>




    <form class="pkp_search pkp_search_desktop" action="https://ojs.aaai.org/index.php/AAAI/search/search" method="get" role="search" aria-label="Article Search">
        <input type="hidden" name="csrfToken" value="7784118d1ef46d7590a17d5d7fcc5058">

            <input name="query" value="" type="text" aria-label="Search Query">


        <button type="submit">
            Search
        </button>
        <div class="search_controls" aria-hidden="true">
            <a href="https://ojs.aaai.org/index.php/AAAI/search/search" class="headerSearchPrompt search_prompt" aria-hidden="true">
                Search
            </a>
            <a href="#" class="search_cancel headerSearchCancel" aria-hidden="true"></a>
            <span class="search_loading" aria-hidden="true"></span>
        </div>
    </form>
                                                    </div>
                    </div>
                    <div class="pkp_navigation_user_wrapper" id="navigationUserWrapper">
                            <ul id="navigationUser" class="pkp_navigation_user pkp_nav_list">
                                                            <li class="profile">
                <a href="https://ojs.aaai.org/index.php/AAAI/login">
                    Login
                </a>
                            </li>
                                        </ul>

                    </div>

    <form class="pkp_search pkp_search_mobile" action="https://ojs.aaai.org/index.php/AAAI/search/search" method="get" role="search" aria-label="Article Search">
        <input type="hidden" name="csrfToken" value="7784118d1ef46d7590a17d5d7fcc5058">

            <input name="query" value="" type="text" aria-label="Search Query">


        <button type="submit">
            Search
        </button>
        <div class="search_controls" aria-hidden="true">
            <a href="https://ojs.aaai.org/index.php/AAAI/search/search" class="headerSearchPrompt search_prompt" aria-hidden="true">
                Search
            </a>
            <a href="#" class="search_cancel headerSearchCancel" aria-hidden="true"></a>
            <span class="search_loading" aria-hidden="true"></span>
        </div>
    </form>
                                    </nav>
            </div><!-- .pkp_head_wrapper -->
        </header><!-- .pkp_structure_head -->

                        <div class="pkp_structure_content has_sidebar">
            <div class="pkp_structure_main" role="main">
                <a id="pkp_content_main"></a>

<div class="page page_article">
            <nav class="cmp_breadcrumbs" role="navigation" aria-label="You are here:">
    <ol>
        <li>
            <a href="https://ojs.aaai.org/index.php/AAAI/index">
                Home
            </a>
            <span class="separator">/</span>
        </li>
        <li>
            <a href="https://ojs.aaai.org/index.php/AAAI/issue/archive">
                Archives
            </a>
            <span class="separator">/</span>
        </li>
                    <li>
                <a href="https://ojs.aaai.org/index.php/AAAI/issue/view/647">
                    Vol. 39 No. 24: AAAI-25 Technical Tracks 24
                </a>
                <span class="separator">/</span>
            </li>
                <li class="current" aria-current="page">
            <span aria-current="page">
                                    AAAI Technical Track on Natural Language Processing III
                            </span>
        </li>
    </ol>
</nav>

            <article class="obj_article_details">


    <h1 class="page_title">
        CodeHalu: Investigating Code Hallucinations in LLMs via Execution-based Verification
    </h1>


    <div class="row">
        <div class="main_entry">

                            <section class="item authors">
                    <h2 class="pkp_screen_reader">Authors</h2>
                    <ul class="authors">
                                            <li>
                            <span class="name">
                                Yuchen Tian
                            </span>
                                                            <span class="affiliation">
                                    Hong Kong Baptist University
                                </span>
                                                                                </li>
                                            <li>
                            <span class="name">
                                Weixiang Yan
                            </span>
                                                            <span class="affiliation">
                                    University of California, Santa Barbara
                                </span>
                                                                                </li>
                                            <li>
                            <span class="name">
                                Qian Yang
                            </span>
                                                            <span class="affiliation">
                                    Montreal Institute for Learning Algorithms
Mila - Québec AI Institute
                                </span>
                                                                                </li>
                                            <li>
                            <span class="name">
                                Xuandong Zhao
                            </span>
                                                            <span class="affiliation">
                                    University of California, Berkeley
                                </span>
                                                                                </li>
                                            <li>
                            <span class="name">
                                Qian Chen
                            </span>
                                                            <span class="affiliation">
                                    Alibaba Group
                                </span>
                                                                                </li>
                                            <li>
                            <span class="name">
                                Wen Wang
                            </span>
                                                            <span class="affiliation">
                                    Alibaba Group
                                </span>
                                                                                </li>
                                            <li>
                            <span class="name">
                                Ziyang Luo
                            </span>
                                                            <span class="affiliation">
                                    Hong Kong Baptist University
                                </span>
                                                                                </li>
                                            <li>
                            <span class="name">
                                Lei Ma
                            </span>
                                                            <span class="affiliation">
                                    The University of Tokyo
University of Alberta
                                </span>
                                                                                </li>
                                            <li>
                            <span class="name">
                                Dawn Song
                            </span>
                                                            <span class="affiliation">
                                    University of California Berkeley
                                </span>
                                                                                </li>
                                        </ul>
                </section>

                                                                                                                <section class="item doi">
                        <h2 class="label">
                                                        DOI:
                        </h2>
                        <span class="value">
                            <a href="https://doi.org/10.1609/aaai.v39i24.34717">
                                https://doi.org/10.1609/aaai.v39i24.34717
                            </a>
                        </span>
                    </section>


                                        <section class="item abstract">
                    <h2 class="label">Abstract</h2>
                    Large Language Models (LLMs) have made significant progress in code generation, offering developers groundbreaking automated programming support. However, LLMs often generate code that is syntactically correct and even semantically plausible, but may not execute as expected or fulfill specified requirements. This phenomenon of hallucinations in the code domain has not been systematically explored. To advance the community's understanding and research on this issue, we introduce the concept of code hallucinations and propose a classification method for code hallucination based on execution verification. We categorize code hallucinations into four main types: mapping, naming, resource, and logic hallucinations, with each category further divided into different subcategories to understand and address the unique challenges faced by LLMs in code generation with finer granularity. Additionally, we present a dynamic detection algorithm called CodeHalu designed to detect and quantify code hallucinations. We also introduce the CodeHaluEval benchmark, which includes 8,883 samples from 699 tasks, to systematically and quantitatively evaluate code hallucinations. By evaluating 17 popular LLMs using this benchmark, we reveal significant differences in their accuracy and reliability in code generation, offering detailed insights for further improving the code generation capabilities of LLMs.
                </section>





        </div><!-- .main_entry -->

        <div class="entry_details">

                                        <div class="item cover_image">
                    <div class="sub_item">
                                                    <a href="https://ojs.aaai.org/index.php/AAAI/issue/view/647">
                                <img src="https://ojs.aaai.org/public/journals/2/AAAI25Proceedings-Cover.jpg" alt="AAAI-25 / IAAI-25 / EAAI-25 Proceedings Cover">
                            </a>
                                            </div>
                </div>

                                        <div class="item galleys">
                    <h2 class="pkp_screen_reader">
                        Downloads
                    </h2>
                    <ul class="value galleys_links">
                                                    <li>




<a class="obj_galley_link pdf" href="https://ojs.aaai.org/index.php/AAAI/article/view/34717/36872">


    PDF

    </a>
                            </li>
                                            </ul>
                </div>

                        <div class="item published">
                <section class="sub_item">
                    <h2 class="label">
                        Published
                    </h2>
                    <div class="value">
                                                                            <span>2025-04-11</span>
                                                                    </div>
                </section>
                            </div>

                                        <div class="item citation">
                    <section class="sub_item citation_display">
                        <h2 class="label">
                            How to Cite
                        </h2>
                        <div class="value">
                            <div id="citationOutput" role="region" aria-live="polite">
                                <div class="csl-bib-body">
  <div class="csl-entry">Tian, Y., Yan, W., Yang, Q., Zhao, X., Chen, Q., Wang, W., Luo, Z., Ma, L., &#38; Song, D. (2025). CodeHalu: Investigating Code Hallucinations in LLMs via Execution-based Verification. <i>Proceedings of the AAAI Conference on Artificial Intelligence</i>, <i>39</i>(24), 25300-25308. https://doi.org/10.1609/aaai.v39i24.34717</div>
</div>
                            </div>
                            <div class="citation_formats">
                                <button class="cmp_button citation_formats_button" aria-controls="cslCitationFormats" aria-expanded="false" data-csl-dropdown="true">
                                    More Citation Formats
                                </button>
                                <div id="cslCitationFormats" class="citation_formats_list" aria-hidden="true">
                                    <ul class="citation_formats_styles">
                                                                                    <li>
                                                <a
                                                    aria-controls="citationOutput"
                                                    href="https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/acm-sig-proceedings?submissionId=34717&amp;publicationId=32990"
                                                    data-load-citation
                                                    data-json-href="https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/acm-sig-proceedings?submissionId=34717&amp;publicationId=32990&amp;return=json"
                                                >
                                                    ACM
                                                </a>
                                            </li>
                                                                                    <li>
                                                <a
                                                    aria-controls="citationOutput"
                                                    href="https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/acs-nano?submissionId=34717&amp;publicationId=32990"
                                                    data-load-citation
                                                    data-json-href="https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/acs-nano?submissionId=34717&amp;publicationId=32990&amp;return=json"
                                                >
                                                    ACS
                                                </a>
                                            </li>
                                                                                    <li>
                                                <a
                                                    aria-controls="citationOutput"
                                                    href="https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/apa?submissionId=34717&amp;publicationId=32990"
                                                    data-load-citation
                                                    data-json-href="https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/apa?submissionId=34717&amp;publicationId=32990&amp;return=json"
                                                >
                                                    APA
                                                </a>
                                            </li>
                                                                                    <li>
                                                <a
                                                    aria-controls="citationOutput"
                                                    href="https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/associacao-brasileira-de-normas-tecnicas?submissionId=34717&amp;publicationId=32990"
                                                    data-load-citation
                                                    data-json-href="https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/associacao-brasileira-de-normas-tecnicas?submissionId=34717&amp;publicationId=32990&amp;return=json"
                                                >
                                                    ABNT
                                                </a>
                                            </li>
                                                                                    <li>
                                                <a
                                                    aria-controls="citationOutput"
                                                    href="https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/chicago-author-date?submissionId=34717&amp;publicationId=32990"
                                                    data-load-citation
                                                    data-json-href="https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/chicago-author-date?submissionId=34717&amp;publicationId=32990&amp;return=json"
                                                >
                                                    Chicago
                                                </a>
                                            </li>
                                                                                    <li>
                                                <a
                                                    aria-controls="citationOutput"
                                                    href="https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/harvard-cite-them-right?submissionId=34717&amp;publicationId=32990"
                                                    data-load-citation
                                                    data-json-href="https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/harvard-cite-them-right?submissionId=34717&amp;publicationId=32990&amp;return=json"
                                                >
                                                    Harvard
                                                </a>
                                            </li>
                                                                                    <li>
                                                <a
                                                    aria-controls="citationOutput"
                                                    href="https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/ieee?submissionId=34717&amp;publicationId=32990"
                                                    data-load-citation
                                                    data-json-href="https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/ieee?submissionId=34717&amp;publicationId=32990&amp;return=json"
                                                >
                                                    IEEE
                                                </a>
                                            </li>
                                                                                    <li>
                                                <a
                                                    aria-controls="citationOutput"
                                                    href="https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/modern-language-association?submissionId=34717&amp;publicationId=32990"
                                                    data-load-citation
                                                    data-json-href="https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/modern-language-association?submissionId=34717&amp;publicationId=32990&amp;return=json"
                                                >
                                                    MLA
                                                </a>
                                            </li>
                                                                                    <li>
                                                <a
                                                    aria-controls="citationOutput"
                                                    href="https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/turabian-fullnote-bibliography?submissionId=34717&amp;publicationId=32990"
                                                    data-load-citation
                                                    data-json-href="https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/turabian-fullnote-bibliography?submissionId=34717&amp;publicationId=32990&amp;return=json"
                                                >
                                                    Turabian
                                                </a>
                                            </li>
                                                                                    <li>
                                                <a
                                                    aria-controls="citationOutput"
                                                    href="https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/vancouver?submissionId=34717&amp;publicationId=32990"
                                                    data-load-citation
                                                    data-json-href="https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/get/vancouver?submissionId=34717&amp;publicationId=32990&amp;return=json"
                                                >
                                                    Vancouver
                                                </a>
                                            </li>
                                                                            </ul>
                                                                            <div class="label">
                                            Download Citation
                                        </div>
                                        <ul class="citation_formats_styles">
                                                                                            <li>
                                                    <a href="https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/download/ris?submissionId=34717&amp;publicationId=32990">
                                                        <span class="fa fa-download"></span>
                                                        Endnote/Zotero/Mendeley (RIS)
                                                    </a>
                                                </li>
                                                                                            <li>
                                                    <a href="https://ojs.aaai.org/index.php/AAAI/citationstylelanguage/download/bibtex?submissionId=34717&amp;publicationId=32990">
                                                        <span class="fa fa-download"></span>
                                                        BibTeX
                                                    </a>
                                                </li>
                                                                                    </ul>
                                                                    </div>
                            </div>
                        </div>
                    </section>
                </div>

                                        <div class="item issue">

                                            <section class="sub_item">
                            <h2 class="label">
                                Issue
                            </h2>
                            <div class="value">
                                <a class="title" href="https://ojs.aaai.org/index.php/AAAI/issue/view/647">
                                    Vol. 39 No. 24: AAAI-25 Technical Tracks 24
                                </a>
                            </div>
                        </section>

                                            <section class="sub_item">
                            <h2 class="label">
                                Section
                            </h2>
                            <div class="value">
                                AAAI Technical Track on Natural Language Processing III
                            </div>
                        </section>
                                    </div>





        </div><!-- .entry_details -->
    </div><!-- .row -->

</article>



</div><!-- .page -->

    </div><!-- pkp_structure_main -->

                                    <div class="pkp_structure_sidebar left" role="complementary" aria-label="Sidebar">
                <div class="pkp_block block_information">
    <h2 class="title">Information</h2>
    <div class="content">
        <ul>
                            <li>
                    <a href="https://ojs.aaai.org/index.php/AAAI/information/readers">
                        For Readers
                    </a>
                </li>
                                        <li>
                    <a href="https://ojs.aaai.org/index.php/AAAI/information/authors">
                        For Authors
                    </a>
                </li>
                                        <li>
                    <a href="https://ojs.aaai.org/index.php/AAAI/information/librarians">
                        For Librarians
                    </a>
                </li>
                    </ul>
    </div>
</div>
<div class="pkp_block block_hosted_by">
    <div class="content">
        <ul><li>
            <a href="https://pkpservices.sfu.ca/" id="hostedByLogo" target="_blank"><img src="https://ojs.aaai.org/plugins/blocks/hostedBy/icons/pkpps.svg" style="border: 0; margin-bottom: 8px; width: 80%"/></a>
            <p>
                Part of the<br />
                <a href="https://pkpservices.sfu.ca/" id="hostedBy" target="_blank">PKP Publishing Services Network</a>
            </p>
        </li></ul>
</div>
</div>

            </div><!-- pkp_sidebar.left -->
            </div><!-- pkp_structure_content -->

<div class="pkp_structure_footer_wrapper" role="contentinfo">
    <a id="pkp_content_footer"></a>

    <div class="pkp_structure_footer">

                    <div class="pkp_footer_content">
                <p>Copyright © 2024, Association for the Advancement of Artificial Intelligence</p>
            </div>

        <div class="pkp_brand_footer" role="complementary">
            <a href="https://ojs.aaai.org/index.php/AAAI/about/aboutThisPublishingSystem">
                <img alt="More information about the publishing system, Platform and Workflow by OJS/PKP." src="https://ojs.aaai.org/templates/images/ojs_brand.png">
            </a>
        </div>
    </div>
</div><!-- pkp_structure_footer_wrapper -->

</div><!-- pkp_structure_page -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js?v=3.2.1.1" type="text/javascript"></script><script src="//ajax.googleapis.com/ajax/libs/jqueryui/1.12.0/jquery-ui.min.js?v=3.2.1.1" type="text/javascript"></script><script src="https://ojs.aaai.org/lib/pkp/js/lib/jquery/plugins/jquery.tag-it.js?v=3.2.1.1" type="text/javascript"></script><script src="https://ojs.aaai.org/plugins/themes/default/js/lib/popper/popper.js?v=3.2.1.1" type="text/javascript"></script><script src="https://ojs.aaai.org/plugins/themes/default/js/lib/bootstrap/util.js?v=3.2.1.1" type="text/javascript"></script><script src="https://ojs.aaai.org/plugins/themes/default/js/lib/bootstrap/dropdown.js?v=3.2.1.1" type="text/javascript"></script><script src="https://ojs.aaai.org/plugins/themes/default/js/main.js?v=3.2.1.1" type="text/javascript"></script><script src="https://ojs.aaai.org/plugins/generic/citationStyleLanguage/js/articleCitation.js?v=3.2.1.1" type="text/javascript"></script>


</body>
</html>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9234 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9234</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9234</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-276761482</p>
                <p><strong>Paper Title:</strong> Automatic recognition of cross-language classic entities based on large language models</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) hold immense potential for the intelligent processing of classical texts. They offer new approaches for digital research on classical literature resources, cross-linguistic understanding, text knowledge mining, and the promotion and preservation of cultural heritage. To explore the performance of named entity recognition (NER) tasks supported by LLMs, this study first fine-tuned four LLMs—Xunzi-Baichuan, Baichuan2-7B-Base, Xunzi-GLM, and ChatGLM3-6B—using supervised fine-tuning methods based on open-source models. Zero-shot, one-shot, and few-shot prompting methods were then employed to validate the performance of these models in the NER tasks. Finally, the applicability of fine-tuning LLMs in specific domains for NER tasks was examined using BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L, precision, recall, and F1 scores as evaluation metrics for model performance and applicability. The experimental results indicated that fine-tuned LLMs achieved high scores across multiple metrics, demonstrating strong performance in text generation. In entity extraction, the Xunzi-Baichuan model performed optimally across several metrics and also exhibited generalization capabilities. In addition, we have open-sourced our models for community research. https://github.com/Xunzi-LLM-of-Chinese-classics/XunziALLM.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9234.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9234.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Xunzi-Baichuan</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Xunzi-Baichuan (fine-tuned Baichuan2-7B for ancient texts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter large language model derived from Baichuan2-7B-Base and further pretrained/fine-tuned by the authors for intelligent processing of ancient Chinese texts; evaluated on NER and text-generation metrics under different prompt formats (zero-/one-/few-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Xunzi-Baichuan</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Named Entity Recognition (NER) and generation-evaluation on classical/modern Chinese corpora</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Extract entity types (PER, BOOK, OFI) from classical and modern Chinese texts (Twenty-Four Histories data) and evaluate generated labeled outputs with BLEU-4 and ROUGE metrics plus NER precision/recall/F1/accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Prompting after supervised fine-tuning (SFT+LoRA). Evaluated with zero-shot prompts (no examples), one-shot prompts (single example), few-shot prompts (three-shot and five-shot). Prompt format instructs extraction with labels using the pattern {entity|LABEL} (Table 1); dataset input formatted as instruction + input (instruction concatenated with input).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared zero-shot vs one-shot vs few-shot (three-shot and five-shot). Also compared fine-tuned Xunzi-Baichuan against other models (Baichuan2-7B-Base, Xunzi-GLM, ChatGLM3-6B) under same prompting formats.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Generation: BLEU-4: zero-shot 93.34, one-shot 93.43, three-shot 93.38, five-shot 93.33; ROUGE-1 ~98.02–98.04, ROUGE-2 ~93.49–93.57, ROUGE-L ~96.36–96.42. NER (ALL, cross-linguistic combined): zero-shot F1 = 81.87 (accuracy 82.32, recall 81.41); one-shot F1 = 82.19; three-shot and five-shot F1 around 81.98–81.83 (see Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Zero-shot BLEU-4: Xunzi-Baichuan 93.34 vs ChatGLM3-6B 91.54 (difference +1.80 BLEU points). Zero-shot NER F1: Xunzi-Baichuan 81.87 vs ChatGLM3-6B 78.11 (difference +3.76 F1 points). Across shot settings Xunzi-Baichuan consistently top-ranked among the four evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+1.8 BLEU-4 (Xunzi-Baichuan vs ChatGLM3-6B, zero-shot); +3.76 F1 (NER, Xunzi-Baichuan vs ChatGLM3-6B, zero-shot). Changes across shot counts for Xunzi-Baichuan itself were small (BLEU-4 varied by ≈0.09 between zero and one shot; NER F1 varied ≲1 point across shot settings).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors attribute strong performance to domain-adaptive supervised fine-tuning (SFT with LoRA) which improves instruction adherence and domain knowledge for ancient texts; few-shot prompts supply contextual examples that can help guide generation for domain-specific tasks, but the paper notes shot-to-shot gains are modest for already fine-tuned models. They also note parameter size and base-model training influence performance (7B model often outperforming 6B).</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Fine-tuning: Supervised Fine-Tuning (SFT) with LoRA (rank 8, dropout 0.1), learning rate 5e-5, cutoff_len 1024, per-device train batch size 4. Dataset: Twenty-Four Histories proofread data (classical: 55,804 train / 620 test; modern: 26,702 train / 296 test). Prompt templates per Table 1 (zero/one/three/five-shot examples); instruction+input concatenation. Evaluation metrics: BLEU-4, ROUGE-1/2/L, Precision, Recall, F1, Accuracy. Hardware: multi-GPU (Quadro RTX 8000), PyTorch/DeepSpeed used for training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic recognition of cross-language classic entities based on large language models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9234.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9234.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baichuan2-7B-Base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Baichuan2-7B-Base</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter open-source base LLM provided by Baichuan, pretrained on large multilingual corpora; used in this study both as a baseline (un-fine-tuned) and as the base for further fine-tuning (Xunzi-Baichuan).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Baichuan2-7B-Base</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Named Entity Recognition (NER) and generation-evaluation on classical/modern Chinese corpora</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same NER extraction and generation-evaluation tasks as above: extract PER, BOOK, OFI from classical/modern Chinese; evaluate generation similarity (BLEU/ROUGE) and NER metrics (precision/recall/F1/accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Evaluated with zero-shot, one-shot, three-shot, and five-shot prompting. Some experiments used Baichuan2-7B as an un-fine-tuned baseline; prompts and label format per Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared across zero/one/three/five-shot prompts and versus fine-tuned variants (Xunzi-Baichuan and Xunzi-GLM) and ChatGLM3-6B.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Generation: BLEU-4: zero-shot 92.80, one-shot 92.81, three-shot 92.75, five-shot 92.38; ROUGE-1 ~97.75–97.79, ROUGE-2 ~92.75–92.88, ROUGE-L ~95.97–96.05. NER (ALL combined): zero-shot ALL F1 ≈ 80.79 (accuracy ~80.79, recall ~80.16, precision ~81.42) per Table 7; one-shot and few-shot show modest variation around these values.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>In some settings (notably one-shot and some few-shot experiments) the un-fine-tuned Baichuan2-7B-Base outperformed some fine-tuned models on Modern Chinese NER tasks (paper explicitly notes Baichuan2-7B-Base showed better accuracy/recall/F1 than fine-tuned models in one-shot for certain entity types). In three-shot, Baichuan2-7B-Base outperformed ChatGLM3-6B by accuracy/recall/F1 differences of 2.22, 2.46, and 2.34 points respectively (paper text).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>BLEU-4 differences across shot counts were small (≈0.4 points between best and worst shot settings). Three-shot advantage over ChatGLM3-6B: accuracy +2.22, recall +2.46, F1 +2.34 (explicitly reported).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors suggest that larger parameter size (7B vs 6B) and stronger base pretraining can yield better few-shot performance; also that being un-fine-tuned on the specific SFT data can sometimes preserve broader base-model capabilities useful for certain modern Chinese tasks, causing un-fine-tuned baseline to outperform domain-finetuned variants in specific scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluated with the same prompts and datasets as other models. Prompt examples per Table 1; shots = 0/1/3/5. Fine-tuning for other models used LoRA; Baichuan2-7B-Base is reported both as baseline and as base for Xunzi-Baichuan. Dataset splits: classical and modern as in Table 2. Metrics: BLEU-4, ROUGE-1/2/L, Precision/Recall/F1/Accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic recognition of cross-language classic entities based on large language models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9234.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9234.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Xunzi-GLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Xunzi-GLM (fine-tuned ChatGLM3-6B for ancient texts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-tuned variant of ChatGLM3-6B (6B parameters) further pretrained and fine-tuned by the authors for ancient-text processing; evaluated on the same NER and generation metrics under multiple prompting formats.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Xunzi-GLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Named Entity Recognition (NER) and generation-evaluation on classical/modern Chinese corpora</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Extract PER/BOOK/OFI from classical and modern Chinese corpora and evaluate labeled-output similarity (BLEU/ROUGE) and NER metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Evaluated after supervised fine-tuning (SFT+LoRA) and prompted in zero-shot, one-shot, three-shot, and five-shot formats using the same instruction/labeling format ({entity|LABEL}) as other models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared zero/one/three/five-shot and compared against Baichuan2-7B-Base, Xunzi-Baichuan, and ChatGLM3-6B.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Generation: BLEU-4: zero-shot 92.42, one-shot 92.45, three-shot 92.59, five-shot 92.57; ROUGE-1 ~97.71–97.75, ROUGE-2 ~92.56–92.77, ROUGE-L ~95.83–95.94. NER (ALL combined): zero-shot ALL F1 ≈ 79.56 (accuracy ~79.56, recall ~79.04, precision ~80.09) per Table 10; shot-to-shot NER F1 typically in high-70s to low-80s depending on entity and modern/ancient split.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Xunzi-GLM tended to underperform Xunzi-Baichuan (7B) consistently and sometimes underperform the un-fine-tuned Baichuan2-7B-Base on Modern Chinese NER (paper notes Xunzi-GLM's Modern Chinese F1 was 1.94 points lower than Baichuan2-7B-Base in one-shot). BLEU-4 differences vs Xunzi-Baichuan around 0.38–0.98 points (dependent on shot).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>BLEU-4 increases from zero to three shots were modest (~0.17 points). NER differences between shot settings often <2 points for Xunzi-GLM; performance gap vs 7B models ~0.3–1.9 BLEU points and ~1–3 F1 points in various settings (reported in text).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors attribute some underperformance to smaller parameter size (6B) and the characteristics of base model pretraining. They also note that fine-tuning tailored to the ancient-text domain tends to help cross-linguistic ancient text recognition but that parameter count and base-model strengths affect few-shot promptability.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Fine-tuned with LoRA (rank 8, dropout 0.1), learning rate 5e-5, cutoff_len 1024, per-device batch size 4. Evaluation used zero/one/three/five-shot prompts with the same label-format instructions. Dataset splits and metrics as for other models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic recognition of cross-language classic entities based on large language models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9234.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9234.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGLM3-6B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGLM3-6B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 6B-parameter general-purpose conversational LLM (GLM architecture) from Tsinghua University team, used here as a baseline and as a base for Xunzi-GLM; evaluated under different prompt presentation formats.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGLM3-6B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Named Entity Recognition (NER) and generation-evaluation on classical/modern Chinese corpora</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Extract PER/BOOK/OFI entities and evaluate outputs by BLEU/ROUGE and NER precision/recall/F1/accuracy on Twenty-Four Histories-derived datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Evaluated in zero-shot, one-shot, three-shot, and five-shot prompting formats (prompt templates per Table 1) both as a baseline and relative to fine-tuned variants. Prompts used explicit instruction and example-based few-shot formats.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared across zero/one/three/five-shot and against Baichuan2-7B-Base, Xunzi-Baichuan, and Xunzi-GLM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Generation: BLEU-4: zero-shot 91.54, one-shot 91.67, three-shot 91.79, five-shot 91.74; ROUGE-1 ~97.37–97.45, ROUGE-2 ~91.55–91.86, ROUGE-L ~95.31–95.44. NER (ALL combined): zero-shot ALL F1 ≈ 78.11 (accuracy ~78.11, recall ~77.52, precision ~78.71) per Table 8; one-shot/three/five-shot F1 values generally in the high-70s.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>ChatGLM3-6B was the lowest-performing model among the four in most shot settings; e.g., zero-shot BLEU-4 91.54 vs Xunzi-Baichuan 93.34 (+1.80). Zero-shot NER F1 78.11 vs Xunzi-Baichuan 81.87 (+3.76).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Across shot settings, ChatGLM3-6B showed small improvements from zero to three/five shots (BLEU-4 increased by ≈0.2–0.3 from zero to three-shot). Relative format effect sizes versus other models: differences of ~1.8 BLEU and ~3.7 F1 between ChatGLM3-6B and top-performing Xunzi-Baichuan in zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors point to base-model capacity (6B vs 7B) and domain-adaptive fine-tuning as contributing factors to lower performance; few-shot prompting improves performance modestly, but fine-tuning yields larger gains for domain-specific NER. They also caution that few-shot examples can introduce semantic drift if not well designed.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluated with prompts in Table 1 across 0/1/3/5 shots. Used the same dataset, evaluation metrics, and experimental hardware. ChatGLM3-6B used both as baseline and as a base model for Xunzi-GLM fine-tuning; fine-tuning details applied to Xunzi-GLM (LoRA, SFT) as above.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automatic recognition of cross-language classic entities based on large language models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Fine-tuned language models are zero-shot learners <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Continued pretraining for better zero-and few-shot promptability <em>(Rating: 2)</em></li>
                <li>State-of-the-art Parameter-Efficient Fine-Tuning (PEFT) methods <em>(Rating: 1)</em></li>
                <li>An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9234",
    "paper_id": "paper-276761482",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "Xunzi-Baichuan",
            "name_full": "Xunzi-Baichuan (fine-tuned Baichuan2-7B for ancient texts)",
            "brief_description": "A 7B-parameter large language model derived from Baichuan2-7B-Base and further pretrained/fine-tuned by the authors for intelligent processing of ancient Chinese texts; evaluated on NER and text-generation metrics under different prompt formats (zero-/one-/few-shot).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Xunzi-Baichuan",
            "model_size": "7B",
            "task_name": "Named Entity Recognition (NER) and generation-evaluation on classical/modern Chinese corpora",
            "task_description": "Extract entity types (PER, BOOK, OFI) from classical and modern Chinese texts (Twenty-Four Histories data) and evaluate generated labeled outputs with BLEU-4 and ROUGE metrics plus NER precision/recall/F1/accuracy.",
            "presentation_format": "Prompting after supervised fine-tuning (SFT+LoRA). Evaluated with zero-shot prompts (no examples), one-shot prompts (single example), few-shot prompts (three-shot and five-shot). Prompt format instructs extraction with labels using the pattern {entity|LABEL} (Table 1); dataset input formatted as instruction + input (instruction concatenated with input).",
            "comparison_format": "Compared zero-shot vs one-shot vs few-shot (three-shot and five-shot). Also compared fine-tuned Xunzi-Baichuan against other models (Baichuan2-7B-Base, Xunzi-GLM, ChatGLM3-6B) under same prompting formats.",
            "performance": "Generation: BLEU-4: zero-shot 93.34, one-shot 93.43, three-shot 93.38, five-shot 93.33; ROUGE-1 ~98.02–98.04, ROUGE-2 ~93.49–93.57, ROUGE-L ~96.36–96.42. NER (ALL, cross-linguistic combined): zero-shot F1 = 81.87 (accuracy 82.32, recall 81.41); one-shot F1 = 82.19; three-shot and five-shot F1 around 81.98–81.83 (see Table 9).",
            "performance_comparison": "Zero-shot BLEU-4: Xunzi-Baichuan 93.34 vs ChatGLM3-6B 91.54 (difference +1.80 BLEU points). Zero-shot NER F1: Xunzi-Baichuan 81.87 vs ChatGLM3-6B 78.11 (difference +3.76 F1 points). Across shot settings Xunzi-Baichuan consistently top-ranked among the four evaluated models.",
            "format_effect_size": "+1.8 BLEU-4 (Xunzi-Baichuan vs ChatGLM3-6B, zero-shot); +3.76 F1 (NER, Xunzi-Baichuan vs ChatGLM3-6B, zero-shot). Changes across shot counts for Xunzi-Baichuan itself were small (BLEU-4 varied by ≈0.09 between zero and one shot; NER F1 varied ≲1 point across shot settings).",
            "explanation_or_hypothesis": "Authors attribute strong performance to domain-adaptive supervised fine-tuning (SFT with LoRA) which improves instruction adherence and domain knowledge for ancient texts; few-shot prompts supply contextual examples that can help guide generation for domain-specific tasks, but the paper notes shot-to-shot gains are modest for already fine-tuned models. They also note parameter size and base-model training influence performance (7B model often outperforming 6B).",
            "null_or_negative_result": false,
            "experimental_details": "Fine-tuning: Supervised Fine-Tuning (SFT) with LoRA (rank 8, dropout 0.1), learning rate 5e-5, cutoff_len 1024, per-device train batch size 4. Dataset: Twenty-Four Histories proofread data (classical: 55,804 train / 620 test; modern: 26,702 train / 296 test). Prompt templates per Table 1 (zero/one/three/five-shot examples); instruction+input concatenation. Evaluation metrics: BLEU-4, ROUGE-1/2/L, Precision, Recall, F1, Accuracy. Hardware: multi-GPU (Quadro RTX 8000), PyTorch/DeepSpeed used for training.",
            "uuid": "e9234.0",
            "source_info": {
                "paper_title": "Automatic recognition of cross-language classic entities based on large language models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Baichuan2-7B-Base",
            "name_full": "Baichuan2-7B-Base",
            "brief_description": "A 7B-parameter open-source base LLM provided by Baichuan, pretrained on large multilingual corpora; used in this study both as a baseline (un-fine-tuned) and as the base for further fine-tuning (Xunzi-Baichuan).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Baichuan2-7B-Base",
            "model_size": "7B",
            "task_name": "Named Entity Recognition (NER) and generation-evaluation on classical/modern Chinese corpora",
            "task_description": "Same NER extraction and generation-evaluation tasks as above: extract PER, BOOK, OFI from classical/modern Chinese; evaluate generation similarity (BLEU/ROUGE) and NER metrics (precision/recall/F1/accuracy).",
            "presentation_format": "Evaluated with zero-shot, one-shot, three-shot, and five-shot prompting. Some experiments used Baichuan2-7B as an un-fine-tuned baseline; prompts and label format per Table 1.",
            "comparison_format": "Compared across zero/one/three/five-shot prompts and versus fine-tuned variants (Xunzi-Baichuan and Xunzi-GLM) and ChatGLM3-6B.",
            "performance": "Generation: BLEU-4: zero-shot 92.80, one-shot 92.81, three-shot 92.75, five-shot 92.38; ROUGE-1 ~97.75–97.79, ROUGE-2 ~92.75–92.88, ROUGE-L ~95.97–96.05. NER (ALL combined): zero-shot ALL F1 ≈ 80.79 (accuracy ~80.79, recall ~80.16, precision ~81.42) per Table 7; one-shot and few-shot show modest variation around these values.",
            "performance_comparison": "In some settings (notably one-shot and some few-shot experiments) the un-fine-tuned Baichuan2-7B-Base outperformed some fine-tuned models on Modern Chinese NER tasks (paper explicitly notes Baichuan2-7B-Base showed better accuracy/recall/F1 than fine-tuned models in one-shot for certain entity types). In three-shot, Baichuan2-7B-Base outperformed ChatGLM3-6B by accuracy/recall/F1 differences of 2.22, 2.46, and 2.34 points respectively (paper text).",
            "format_effect_size": "BLEU-4 differences across shot counts were small (≈0.4 points between best and worst shot settings). Three-shot advantage over ChatGLM3-6B: accuracy +2.22, recall +2.46, F1 +2.34 (explicitly reported).",
            "explanation_or_hypothesis": "Authors suggest that larger parameter size (7B vs 6B) and stronger base pretraining can yield better few-shot performance; also that being un-fine-tuned on the specific SFT data can sometimes preserve broader base-model capabilities useful for certain modern Chinese tasks, causing un-fine-tuned baseline to outperform domain-finetuned variants in specific scenarios.",
            "null_or_negative_result": false,
            "experimental_details": "Evaluated with the same prompts and datasets as other models. Prompt examples per Table 1; shots = 0/1/3/5. Fine-tuning for other models used LoRA; Baichuan2-7B-Base is reported both as baseline and as base for Xunzi-Baichuan. Dataset splits: classical and modern as in Table 2. Metrics: BLEU-4, ROUGE-1/2/L, Precision/Recall/F1/Accuracy.",
            "uuid": "e9234.1",
            "source_info": {
                "paper_title": "Automatic recognition of cross-language classic entities based on large language models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Xunzi-GLM",
            "name_full": "Xunzi-GLM (fine-tuned ChatGLM3-6B for ancient texts)",
            "brief_description": "A domain-tuned variant of ChatGLM3-6B (6B parameters) further pretrained and fine-tuned by the authors for ancient-text processing; evaluated on the same NER and generation metrics under multiple prompting formats.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Xunzi-GLM",
            "model_size": "6B",
            "task_name": "Named Entity Recognition (NER) and generation-evaluation on classical/modern Chinese corpora",
            "task_description": "Extract PER/BOOK/OFI from classical and modern Chinese corpora and evaluate labeled-output similarity (BLEU/ROUGE) and NER metrics.",
            "presentation_format": "Evaluated after supervised fine-tuning (SFT+LoRA) and prompted in zero-shot, one-shot, three-shot, and five-shot formats using the same instruction/labeling format ({entity|LABEL}) as other models.",
            "comparison_format": "Compared zero/one/three/five-shot and compared against Baichuan2-7B-Base, Xunzi-Baichuan, and ChatGLM3-6B.",
            "performance": "Generation: BLEU-4: zero-shot 92.42, one-shot 92.45, three-shot 92.59, five-shot 92.57; ROUGE-1 ~97.71–97.75, ROUGE-2 ~92.56–92.77, ROUGE-L ~95.83–95.94. NER (ALL combined): zero-shot ALL F1 ≈ 79.56 (accuracy ~79.56, recall ~79.04, precision ~80.09) per Table 10; shot-to-shot NER F1 typically in high-70s to low-80s depending on entity and modern/ancient split.",
            "performance_comparison": "Xunzi-GLM tended to underperform Xunzi-Baichuan (7B) consistently and sometimes underperform the un-fine-tuned Baichuan2-7B-Base on Modern Chinese NER (paper notes Xunzi-GLM's Modern Chinese F1 was 1.94 points lower than Baichuan2-7B-Base in one-shot). BLEU-4 differences vs Xunzi-Baichuan around 0.38–0.98 points (dependent on shot).",
            "format_effect_size": "BLEU-4 increases from zero to three shots were modest (~0.17 points). NER differences between shot settings often &lt;2 points for Xunzi-GLM; performance gap vs 7B models ~0.3–1.9 BLEU points and ~1–3 F1 points in various settings (reported in text).",
            "explanation_or_hypothesis": "Authors attribute some underperformance to smaller parameter size (6B) and the characteristics of base model pretraining. They also note that fine-tuning tailored to the ancient-text domain tends to help cross-linguistic ancient text recognition but that parameter count and base-model strengths affect few-shot promptability.",
            "null_or_negative_result": false,
            "experimental_details": "Fine-tuned with LoRA (rank 8, dropout 0.1), learning rate 5e-5, cutoff_len 1024, per-device batch size 4. Evaluation used zero/one/three/five-shot prompts with the same label-format instructions. Dataset splits and metrics as for other models.",
            "uuid": "e9234.2",
            "source_info": {
                "paper_title": "Automatic recognition of cross-language classic entities based on large language models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "ChatGLM3-6B",
            "name_full": "ChatGLM3-6B",
            "brief_description": "A 6B-parameter general-purpose conversational LLM (GLM architecture) from Tsinghua University team, used here as a baseline and as a base for Xunzi-GLM; evaluated under different prompt presentation formats.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGLM3-6B",
            "model_size": "6B",
            "task_name": "Named Entity Recognition (NER) and generation-evaluation on classical/modern Chinese corpora",
            "task_description": "Extract PER/BOOK/OFI entities and evaluate outputs by BLEU/ROUGE and NER precision/recall/F1/accuracy on Twenty-Four Histories-derived datasets.",
            "presentation_format": "Evaluated in zero-shot, one-shot, three-shot, and five-shot prompting formats (prompt templates per Table 1) both as a baseline and relative to fine-tuned variants. Prompts used explicit instruction and example-based few-shot formats.",
            "comparison_format": "Compared across zero/one/three/five-shot and against Baichuan2-7B-Base, Xunzi-Baichuan, and Xunzi-GLM.",
            "performance": "Generation: BLEU-4: zero-shot 91.54, one-shot 91.67, three-shot 91.79, five-shot 91.74; ROUGE-1 ~97.37–97.45, ROUGE-2 ~91.55–91.86, ROUGE-L ~95.31–95.44. NER (ALL combined): zero-shot ALL F1 ≈ 78.11 (accuracy ~78.11, recall ~77.52, precision ~78.71) per Table 8; one-shot/three/five-shot F1 values generally in the high-70s.",
            "performance_comparison": "ChatGLM3-6B was the lowest-performing model among the four in most shot settings; e.g., zero-shot BLEU-4 91.54 vs Xunzi-Baichuan 93.34 (+1.80). Zero-shot NER F1 78.11 vs Xunzi-Baichuan 81.87 (+3.76).",
            "format_effect_size": "Across shot settings, ChatGLM3-6B showed small improvements from zero to three/five shots (BLEU-4 increased by ≈0.2–0.3 from zero to three-shot). Relative format effect sizes versus other models: differences of ~1.8 BLEU and ~3.7 F1 between ChatGLM3-6B and top-performing Xunzi-Baichuan in zero-shot.",
            "explanation_or_hypothesis": "Authors point to base-model capacity (6B vs 7B) and domain-adaptive fine-tuning as contributing factors to lower performance; few-shot prompting improves performance modestly, but fine-tuning yields larger gains for domain-specific NER. They also caution that few-shot examples can introduce semantic drift if not well designed.",
            "null_or_negative_result": false,
            "experimental_details": "Evaluated with prompts in Table 1 across 0/1/3/5 shots. Used the same dataset, evaluation metrics, and experimental hardware. ChatGLM3-6B used both as baseline and as a base model for Xunzi-GLM fine-tuning; fine-tuning details applied to Xunzi-GLM (LoRA, SFT) as above.",
            "uuid": "e9234.3",
            "source_info": {
                "paper_title": "Automatic recognition of cross-language classic entities based on large language models",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Fine-tuned language models are zero-shot learners",
            "rating": 2,
            "sanitized_title": "finetuned_language_models_are_zeroshot_learners"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Continued pretraining for better zero-and few-shot promptability",
            "rating": 2,
            "sanitized_title": "continued_pretraining_for_better_zeroand_fewshot_promptability"
        },
        {
            "paper_title": "State-of-the-art Parameter-Efficient Fine-Tuning (PEFT) methods",
            "rating": 1,
            "sanitized_title": "stateoftheart_parameterefficient_finetuning_peft_methods"
        },
        {
            "paper_title": "An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models",
            "rating": 1,
            "sanitized_title": "an_adapter_family_for_parameterefficient_finetuning_of_large_language_models"
        }
    ],
    "cost": 0.0150175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Shuiqing Huang sqhuang@njau.edu.cn 
School of Information Management
Nanjing Agricultural University
NanjingChina</p>
<p>School of Economics &amp; Management
Nanjing University of Science and Technology
NanjingChina
FC3CAF6E0CB84724DC83AA4F19C1E21210.1038/s40494-025-01624-yReceived: 16 September 2024; Accepted: 25 November 2024;</p>
<p>https://doi.org/10.1038/s40494-025-01624-yAutomatic recognition of cross-language classic entities based on large language models Check for updates Qiankun Xu 1 , Yutong Liu 1 , Dongbo Wang 1,2 &amp; Shuiqing Huang 1,2 Large language models (LLMs) hold immense potential for the intelligent processing of classical texts.They offer new approaches for digital research on classical literature resources, cross-linguistic understanding, text knowledge mining, and the promotion and preservation of cultural heritage.To explore the performance of named entity recognition (NER) tasks supported by LLMs, this study first fine-tuned four LLMs-Xunzi-Baichuan, Baichuan2-7B-Base, Xunzi-GLM, and ChatGLM3-6Busing supervised fine-tuning methods based on open-source models.Zero-shot, one-shot, and fewshot prompting methods were then employed to validate the performance of these models in the NER tasks.Finally, the applicability of fine-tuning LLMs in specific domains for NER tasks was examined using BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L, precision, recall, and F1 scores as evaluation metrics for model performance and applicability.The experimental results indicated that fine-tuned LLMs achieved high scores across multiple metrics, demonstrating strong performance in text generation.In entity extraction, the Xunzi-Baichuan model performed optimally across several metrics and also exhibited generalization capabilities.In addition, we have open-sourced our models for community research.https://github.com/Xunzi-LLM-of-Chinese-classics/XunziALLM.</p>
<p>Chinese culture has a long history, and classical texts are a vital inheritance of historical memory and a repository of the long-standing civilization of the Chinese nation, playing an important role in its development and transmission.These texts contain rich knowledge and embody the long history and culture of the Chinese people.The development and utilization of classical texts can better promote the excellent traditional Chinese culture.Accelerating the transformation and utilization of antiquarian book resources, especially promoting the digitization of antiquarian books and popularizing and disseminating antiquarian books.Not only do classical texts need to be passed down from generation to generation, but their study also needs to keep up with the times and embrace innovation.To preserve classical texts and continue the long-standing traditional culture of China, an increasing number of scholars are engaged in classical text research, promoting the creative transformation and innovative development of Chinese civilization and bringing the written words in classical texts to life.</p>
<p>One important way of bringing the written words in classical texts to life is to improve the level of classical text protection and accelerate the transformation and utilization of classical text resources.Digitizing classical texts involves the use of technologies such as scanning and optical character recognition to preserve the texts in digital form.Classical text digitization is both an essential foundation for humanities research and an important means of value excavation and effective utilization.By utilizing digitization technologies to process and organize vast amounts of ancient book resources, not only can the construction of standardized classical knowledge bases be accelerated, but the in-depth integration and utilization of classical collection resources in China will be promoted 1 .With the transformation and upgradation of classical text digitization applications, increasing attention has been paid to research on the extraction and application of classical text information resources, including tasks 2 such as sentence punctuation 3 , word segmentation 4 , part-of-speech tagging 5 , named entity recognition 6 , and machine translation 7 .Digitization not only helps to protect and disseminate cultural heritage, preventing erosion due to time or natural disasters, but also enhances the dissemination efficiency of ancient texts, allowing more users to appreciate the cultural treasures of classical texts.</p>
<p>Named entity recognition (NER) in classical texts is a crucial part of information extraction and plays a vital role in text knowledge mining, knowledge organization, and knowledge graph construction.NER focuses on identifying entities within a text and aims to extract specified entities such as names, locations, titles, and book names from unstructured text.Large language models (LLMs) aim to understand human intentions and generate text similar to natural human language by analyzing input text through pretraining to produce target results.With the development of LLMs and big data technologies, their use in presenting the knowledge in classical texts in a structured form, assisting in text analysis and semantic understanding, and deeply mining the intrinsic value of classical texts will advance the digital, structured, and knowledgeable storage and presentation of classical texts.This is not only a valuable supplement to traditional cultural resources, but also a fundamental resource for cross-linguistic research.</p>
<p>This study aims to explore the performance of LLMs in downstream tasks within the field of natural language processing.First, it validates the feasibility of NER tasks using open-source LLMs and those that were further pretrained and fine-tuned by our research team.Second, it transforms the NER task into a text-generation task and verifies the impact of zero-shot, one-shot, and few-shot prompting on LLM performance in NER tasks after fine-tuning.Finally, it examines the ability of the model to recognize entities with a small amount of high-precision annotated data and determines the generalization capability of LLMs in entity extraction based on the results of fine-tuned models.The application of LLMs in NER tasks introduces new possibilities to traditional natural language processing research, particularly highlighting its advantages in understanding complex contextual content in cross-linguistic classical texts, handling multiple types of entities, and transfer learning, providing insights for future research on LLMs in entity recognition tasks.</p>
<p>• To explore the performance of NER tasks supported by LLMs.</p>
<p>• Zero-shot, one-shot, and few-shot prompting methods were used to validate the performance and applicability of LLMs in NER tasks.• The fine-tuned LLMs achieved high scores across multiple metrics and demonstrated the generalizability of the model.Additionally, we conducted research using open-source models.</p>
<p>Related work</p>
<p>Classical text entity annotation datasets.NER aims to identify entities such as names, locations, and titles from classical texts and present knowledge in these texts in a structured format to assist in text analysis and semantic understanding.This process helps mine the intrinsic value of ancient texts and promotes the dissemination and utilization of classical knowledge.However, classical text NER faces challenges, including the scarcity of publicly available datasets, high annotation costs, and the need for annotators with specific knowledge backgrounds.Current efforts to construct classical text datasets have mainly focused on extracting entities such as names and locations from classical texts.For example, Zhuo 8 used grid-based long short-term memory (LSTM) networks to annotate three types of entities-names, locations, and titles-from the Stratagems of the Warring States.Xu 9 employed four deep learning models -Bi-RNN, Bi-LSTM, Bi-LSTM-CRF, and BERT-to explore four types of entities in ancient texts on local chronicles: aliases of products, people, cited books, and production areas.Zhu 10 annotated the parts of speech in the Annals of the Ming Dynasty for the first time.Huang 11 used rule-based methods to identify names in the Records of the Three Kingdoms: Book of Shu by Chen Shou.Huang 12 automatically identified pre-Qin place names in the Spring and Autumn Annals of the Left and conducted a statistical analysis of the internal and external features of place names, building an automatic recognition model based on conditional random fields (CRF).Li 13 studied the agricultural text Local Chronicles of Products and used CRF to identify aliases, locations, and cited names.Xie 14 constructed a named entity corpus by analyzing entities and used pre-training + fine-tuning + word vectors to recognize entities in the Huangdi Neijing: Suwen of traditional Chinese medicine.Lin 15 used the SikuBERT pre-trained model to build a named entity model for animals in classical texts and identified animal entities in the Records of the Grand Historian.Cui 16 used pretrained models to recognize seven types of named entities in classical chrysanthemum poetry: time, location, season, flower names, flower colors, tasks, and festivals.Tang 17 constructed a Chinese historical information extraction corpus that includes four types of entities: names, locations, titles, and book names.Tang 18 annotated seven major types of relationships in the Twenty-Four Histories, including person-to-person, organization-to-organization, and location-to-organization, as well as 25 specific relationship types such as subordinate, sibling, position, location, attachment, and promotion.Qin 19 extracted entity types such as names, locations, titles, and time from five classical texts: Xunzi, Analects of Confucius, Mozi, Mencius, and Zhuangzi by adding features such as part-of-speech, word length, boundary words, and function words.Hu 20 extracted four types of entities-names, titles, organizations, and locations-from the Comprehensive Mirror for Aid in Government.Li 21 proposed aligning low-resource languages with crosslingual sample instructions to build instruction datasets containing more domain-specific knowledge.Instruction tuning based on instruction datasets allows large multilingual models to demonstrate better zero-shot multilingual performance and language generalization results by leveraging the generative capabilities of LLMs in high-resource languages.Tuning lowresource language samples provides insights into constructing instructional datasets.Therefore, the construction of classical text datasets can facilitate systematically organizing, classifying, and presenting classical text content, making it a knowledge resource that is easier to understand and utilize, thereby promoting knowledge sharing and cultural exchange.</p>
<p>Current research on NER in classical texts.Entity annotation not only helps in understanding the content of classical texts but also promotes the systematic and associative utilization of knowledge.The accurate identification and categorization of entities in classical texts can significantly enhance the readability, retrieval efficiency, and accuracy of information extraction, thereby providing a solid foundation for subsequent knowledge management and application.Liu 22 analyzed the characteristics of names in pre-Qin texts and conducted research on name disambiguation and statistical distribution, which provided insights for future name entity extraction studies.Wang 23 improved the model's understanding of classical texts using domain knowledge-based continued pre-training and fine-tuning methods, as well as context-based entity correction methods, thus enhancing the effectiveness of entity recognition and promoting the intelligent development and utilization of classical resources.Zhang 24 proposed a method for classical text NER based on information theory and discourse information.Li 25 developed a pretraining method based on classical text data that effectively extracts entity information from classical texts.Yan 26 proposed a general framework for the automated extraction of named entities from classical texts, covering aspects such as entity pre-annotation, iterative extraction, and annotation decisions in human-machine interactions.Gu 27 addressed the issue of limited research on the internal composition of entity words by proposing a method to explore patterns in entity spans in Chinese NER, that is, pattern-inspired recognition networks.Xiong 28 extracted titles and ranks from pre-Qin texts such as the Zuo Zhuan and Guo Yu, providing a reference for diachronic corpus work in classical text processing and exploring the evolution of titles in different eras and genres.Xie 29 investigated the performance of LSTM neural networks and conditional random field models in extracting person names, locations, titles, and organization names in the Records of the Grand Historian, followed by entity disambiguation algorithms to improve extraction accuracy.Kang 30 addressed the language characteristics of single-character representation and multiple usages in classical texts by using a pre-trained model with integrated dictionary information to extract names, locations, titles, organization names, and time entities from the Comprehensive Mirror for Aid in Government.Lü 31 replaced the CRF module with an attentionbased LAN model in the LatticeCRF model, improving the integration of entity label information and enabling faster entity recognition in classical texts.Jiang 32 demonstrated that extracting features from Chinese character shapes and stroke compositions can enhance NER performance.Chen 33 proposed a domain-adaptive NER algorithm based on attention mechanisms.By fine-tuning and adding an attention-based adaptive neural network and applying transfer learning strategies to ancient Chinese corpora, this approach builds cross-domain NER models.</p>
<p>Research progress on LLMs in NER.With the rapid development of artificial intelligence and information technology, the integration of digital humanities and computer technology has provided tools and technical advantages for traditional humanities research.Digital humanities is an interdisciplinary field that combines humanities research with computer technology to advance research and development in the humanities using emerging digital technologies and methods.In the context of the digital intelligence era, classical text organization and research should align with trends of artificial intelligence and big data to enhance the level of intelligent organization and research on classical texts.As tools in natural language processing, LLMs have played a significant role in digital humanities research.In particular, domain-specific LLMs exhibit higher accuracy and efficiency when handling text and tasks from specific domains.Cross-linguistic NER aims to train NER models for the target language using labeled source and unlabeled targetlanguage data.Wang 34 built entity recognition and knowledge questionanswering LLMs based on four LLMs-Baichuan2-13B-Chat, ChatGLM2-6B, Llama-2-13B-Chat, and ChatGPT-following the highquality corpus + pre-trained large model + fine-tuning approach.Ding 35 proposed a global-local denoising framework for cross-lingual NER, utilizing global and local information in the semantic space to correct incorrect pseudo-labels and improve model generalization.Qiao 36 achieved good results in NER tasks by compressing LLMs with advanced training objectives and data strategies.Zheng 37 proposed the Lexicon-Syntax Enhanced Multilingual BERT framework, which integrates syntactic knowledge and lexical information, achieving better results than baseline models in NER tasks.Jiang 38 introduced a generation-based NER framework that addresses the issues of limited open entity datasets and multiple entity types that disperse model attention.Riaz 39 proposed a fully modular neural network method that combines fine-tuned language models with language rules, using masked language models as unsupervised NER models, and applying part-of-speech tags to identify and eliminate unannotated entities, showing that semi-supervised NER models outperform GPT-3.5.Heng 40 enhanced the performance of LLMs in structured knowledge extraction tasks by prompting the models to self-reflect on specific domains, generating domain-related content and creating rich training data.Kim S 41 proposed an enhanced NER framework that leveraged the knowledge reasoning ability of LLMs.This framework uses the reasoning capability of LLMs to identify errors in existing NER methods based on knowledge and context information, thereby providing insights into research on low-resource data.Through model fine-tuning, technological optimization, and interdisciplinary collaboration, further development of LLMs and digital humanities research offers more opportunities for progress in the humanities, enhancing the efficiency of tasks like classical text NER and providing technical support for in-depth analysis of classical texts.</p>
<p>Research methods and approach</p>
<p>Prompt engineering.A prompt refers to a natural language text input provided to generative artificial intelligence models to execute a specific task or answer a question.This prompt serves as the initial input for the model to generate responses aimed at achieving the desired results.It can be in the form of a question, description, keywords, or any other type of text and is used to guide the model to produce specific content.For example, when using ChatGPT, users interact with an LLM by posing questions (prompts), which allows the model to perform tasks such as answering questions, generating content, and extracting information.Figure 1 illustrates the entire process of research on automatic identification of canonical entities.</p>
<p>Based on the task requirements, prompts can be categorized as hard, soft, online, offline, or vertical prompts.The purpose of these various types of prompts is to help users interact more effectively with LLMs and optimize the model's output to meet various application needs.Research indicates that LLMs are well-suited for tasks such as text generation, relation extraction, and entity recognition.Prompt Engineering focuses on guiding LLMs to generate target results without updating model parameters or weights.This involves the use and optimization of prompts to help users leverage LLMs in general scenarios.The main processes in prompt engineering include formatting, content, context, drafting, and optimization.Formatting refers to determining the structure of a prompt according to task requirements or desired outcomes, such as questions, keywords, or descriptions.Content involves the selection of appropriate words, phrases, and questions to ensure that the model understands the intent of the user's input.The context considers prior information or additional data to ensure that the model responses are relevant to previous questions or information.Drafting involves creating clear and specific natural language descriptions to accurately convey the user's needs.Optimization involves refining prompts based on the generated results to achieve the desired outcome.</p>
<p>In generative AI, prompt engineering bridges the gap between users and LLMs by understanding user inputs and generating the best possible results.It enhances user experience by enabling quick retrieval of desired outcomes and helps mitigate the impact of biases in the training data on model performance.</p>
<p>Basic Prompting.In LLMs, Basic Prompting involves leveraging the pre-trained knowledge of the model to understand user intentions while minimizing input and quickly adapting to and efficiently handling complex tasks.LLMs demonstrate exceptional zero-shot capabilities but often perform poorly on complex tasks.Few-shot prompting refers to the inclusion of contextual information in the prompts to better guide the model in executing tasks.LLMs possess strong few-shot learning abilities, allowing them to perform various natural language processing tasks with only a few examples provided in the prompt, thereby reducing the need for task-specific data.Wu 42 explored improving the zero-shot or few-shot performance of natural language prompts during continuous pretraining and achieved highly accurate results with minimal task-specific learning parameters.Brown 43 obtained good results in many natural language processing tasks by pre-training on a large corpus of text and then finetuning for specific tasks.</p>
<p>(1) Zero-Shot Prompting: Zero-shot prompting refers to performing downstream tasks directly through textual prompts without further pre-training or fine-tuning the LLM for specific domain tasks.By training general-purpose LLMs and learning the fundamental rules of natural language from training data, models can be guided to complete specific tasks through soft prompts without modifying their parameters or weights.Zero-shot prompting relies heavily on the pretraining process and training dataset of the LLM, which may introduce biases that affect the model's output.To address issues of accuracy or meet the expected outcomes, further pretraining or finetuning of the model for specific domains may be necessary.Wei 44 proposed a method to enhance the zero-shot learning capability of language models by fine-tuning an LLM on datasets, thereby improving the zero-shot performance.(2) Few-Shot Prompting: Few-shot prompting involves adding a small number of examples to the prompt during a dialog to help the LLM understand the user's intent and generate the desired outcome.Wei 45 proposed that by showing a few examples to an LLM and explaining the logic and reasoning process within these examples, the model can be guided to understand the intent of the problem and generate results through a reasoning process.This often results in more accurate outputs.</p>
<p>The application of zero-shot and few-shot prompting depends on the specific requirements of the task, including aspects such as task specificity, data requirements, accuracy, and scalability.Task Specificity: Zero-shot prompting is suitable for general tasks that do not require domain-specific knowledge, whereas few-shot prompting is appropriate for domain-specific tasks.Few-shot prompting can generate domain-specific knowledge as in finance, law, and medicine even in the absence of large datasets.Data Needs: Zero-shot prompting does not require additional examples when constructing prompts; it relies on the model's pre-training process.In contrast, few-shot prompting involves providing a small number of example inputs to guide the model toward generating content that meets the expected goals.Accuracy: Zero-shot prompting may produce less accurate content because of the lack of specific contextual information.Few-shot prompting improves the accuracy by providing specific examples that help the model understand the contextual information, thereby generating more accurate answers.Scalability: Zero-shot prompting depends on the model's pretraining process and does not require task-specific fine-tuning.In contrast, few-shot prompting involves further pre-training and fine-tuning of the LLM using domain-specific data.The examples provided might cause semantic drift, potentially reducing the accuracy and relevance of the generated results.The prompt content used in this study is listed in Table 1.</p>
<p>Supervised fine-tuning.Supervised fine-tuning of LLMs refers to the process of further pre-training and fine-tuning a pretrained language model using specific datasets from particular domains to adapt the model for specific tasks or fields.A pretrained model serves as a base model suitable for general tasks, including question answering, text generation, and code writing.In general, LLMs may not efficiently or accurately extract entities from cross-linguistic texts.Therefore, it is necessary to fine-tune a suitable base model with cross-linguistic text datasets to meet the demands of specific tasks.Chang 46 developed a platform based on human preference evaluations for LLMs.Mangrulkar 47 proposed a method to avoid the comprehensive fine-tuning of downstream tasks by fine-tuning only a portion of the model's parameters, achieving performance comparable to that of fully fine-tuned models while saving storage space.Hu 48 created the LLM-Adapters framework to explore efficient fine-tuning methods for LLMs' PEFT parameters to cater to different task requirements.Sun 49 proposed a two-stage method for constructing prompts to address the issue of decreased generalization ability when fine-tuning LLMs for specific domains.This method involves the generation of a variety of prompts covering a wide range of tasks and expressions to produce high-quality data.</p>
<p>The supervised fine-tuning used in this study effectively enhanced the performance of LLMs.This provides a general method for improving the performance of existing language models.Compared with full-parameter fine-tuning, supervised fine-tuning requires fewer instruction data and has lower fine-tuning costs, significantly boosting the model's performance in specific domains.Supervised fine-tuning helps language models follow natural language instructions to perform specific tasks or generate specific content, resulting in a model with a certain degree of generalization.</p>
<p>Model Introductions.The Baichuan2-7B-Base language model 50 is the second-generation base model of the Baichuan series, open-sourced by Baichuan Company.It is trained on 2.6 trillion tokens of high-quality, multilingual data, demonstrating significant improvements in mathematical abilities, coding, security, logical reasoning, and semantic understanding.The model has been tested on authoritative datasets across six domains: general, legal, medical, mathematical, coding, and multilingual translation.Testing utilized 5-shot evaluations on general domain datasets, including the C-Eval Chinese base model assessment dataset, MMLU with 57 tasks, CMMLU with 67 topics, Gaokao using Chinese college entrance exam questions, AGIEval for single-choice  questions, and BBH evaluation datasets.In the legal and medical fields, the JEC-QA dataset from China's judicial exams was used.For mathematics and coding, the OpenCompass evaluation framework was applied with 4-shot tests on the GSM8K and MATH datasets.The Flores-101 dataset, which covers news, travel guides, and books in 101 languages, was used for multilingual translation.The model achieved the highest results for multiple authoritative Chinese and English datasets with the same number of parameters.The ChatGLM3-6B language model 51 was developed by a team from Tsinghua University, and supports question-answering dialogs in both Chinese and English.Utilizing the General Language Model (GLM) architecture, it was trained on Chinese and English datasets and incorporated techniques such as supervised fine-tuning, reinforcement learning based on human feedback, and self-feedback to enhance the model's question-answering and conversational abilities.The model also employs techniques like rotary positional encoding and autoregressive blank-filling tasks to improve language comprehension.The base model, ChatGLM3-6B-Base, was evaluated on various datasets across semantics, coding, reasoning, and mathematics, achieving first place among 44 publicly available Chinese and English datasets.Additionally, the ChatGLM3-6B model uses a new prompt format that supports multi-turn dialog, tool invocation, code execution, and complex agent tasks.</p>
<p>The Xunzi-Baichuan and Xunzi-GLM language models were developed by Professor Wang Dongbo's team at the School of Information Management, Nanjing Agricultural University, specifically for the domain of ancient text processing.The Xunzi-Baichuan model is a continuation of pre-training and fine-tuning based on the Baichuan2-7B-Base model, designed for intelligent information processing of ancient texts.The Xunzi-GLM model was developed from the ChatGLM3-6B base model through further pre-training and fine-tuning.These ancient text-processing models have demonstrated strong performance in intelligent indexing, information extraction, poetry generation, translation, reading comprehension, lexical analysis, and automatic punctuation.</p>
<p>Experiment conclusion analysis</p>
<p>This study explores the applicability of LLMs in natural language processing downstream tasks by extracting named entities from ancient cross-linguistic texts using both open-source LLMs and those pre-trained and fine-tuned by our research team.</p>
<p>Experiment data.Among the vast array of Chinese ancient historical documents, the Twenty-Four Histories comprise 24 official historical texts written during various Chinese dynasties.Spanning the period from around 2550 BCE to the 17th year of the Chongzhen era ( 1644 2.</p>
<p>Data format.The corpus used in this study was formatted into three components: instruction, input, and output., representing the prompt instruction, input content, and output result, respectively.When constructing the dataset, the instruction prompt and input content are typically concatenated and the output result is predicted.The instructions used in this study are described in the Basic Prompting section, and Table 3 illustrates the data format for zero-shot prompting.</p>
<p>Experimental environment and parameters.The hardware environment configurations used in the experiments are listed in Table 4.The experiments were conducted on a Linux platform using the PyTorch framework for fine-tuning and testing open-source LLMs.</p>
<p>Fine-tuning open-source LLMs not only enhances the models' understanding and adherence to instructions but also improves their knowledge and capabilities in cross-linguistic ancient texts by incorporating domain-specific knowledge.The fine-tuning was performed using Low-Rank Adaptation (LoRA), which significantly reduces the number of parameters that need to be fine-tuned.The fine-tuning phase was designated Supervised Fine-Tuning (SFT), and DeepSpeed was used for distributed training.Proper parameter settings enabled efficient fine-tuning of LLMs in a multi-GPU environment.Specific parameters included maximum input sequence length, learning rate, rank of LoRA low-rank matrices, overfitting prevention measures, and model layers to apply LoRA.Some model parameters are listed in Table 5.</p>
<p>Evaluation metrics.BLEU 52 is a metric used to assess the machine translation quality.It evaluates translation quality by calculating the number of N-grams in the translated text that match those in the reference translations.The BLEU-4 score used in this study is a specific form of BLEU that considers the 1-gram, 2-gram, 3-gram, and 4-gram matches.The process involves calculating the matching degree of the N-grams, applying a weighted geometric mean, and using a penalty factor for brevity (Brevity Penalty, BP).The formula is as follows:
BLEU ¼ BP × exp X N n¼1 w n log p n !ð1ÞBP ¼ min 1; exp 1 À r cð2Þ
In BLEU scoring, BP is the bre vity penalty factor that reduces the BLEU score when the generated text is shorter than the reference text.p n represents the precision of N-gram matches, w n is the weight of the Ngram, r is the length of the reference text, and c is the length of the generated text.</p>
<p>ROUGE 53 is a widely used metric in natural language processing tasks to evaluate the quality of the generated text by comparing its similarity to the reference text.ROUGE has several variants, of which ROUGE-1, ROUGE-2, and ROUGE-L are the most commonly used.ROUGE-1 focuses on wordlevel recall.It evaluates similarity based on unigram overlap by calculating the number of overlapping words between the automatically generated text and the reference text.The formula is as follows:
ROUGE 1 ¼ P S2 Ref Summaries f g P unigram2S Count match unigram À Á P S2 Ref Summaries f g P unigram2S Count unigram À Áð3Þ
In ROUGE-1, the numerator is the number of unigrams shared between the generated text and the reference text, while the denominator is the total number of unigrams in the reference text.</p>
<p>ROUGE-2 evaluates similarity based on bigram overlap, focusing on the overlap of consecutive word pairs and considering word order matching.It is used to assess the accuracy of generated text at the phrase level.The formula is as follows:
ROUGE 2 ¼ P S2 Ref Summaries f g P bigram2S Count match bigram ð Þ P S2 Ref Summaries f g P bigram2S Count bigram À Áð4Þ
In ROUGE-2, the numerator is the number of bigrams shared between the generated text and the reference text, and the denominator is the total number of bigrams in the reference text.</p>
<p>ROUGE-L measures similarity based on the longest common subsequence (LCS) between the generated text and the reference text, with the evaluation metric calculated according to the length of this sequence.The formula is as follows:
ROUGE L ¼ LCS Ref Summary; Auto Summary À Á Length of Ref Summaryð5Þ
In ROUGE-L, LCS represents the longest common subsequence between the reference text and the generated text.</p>
<p>To assess the performance of fine-tuning LLMs in this study, three metrics were used: Precision 54 , Recall, and the F1 score 55 .Precision measures the number of identified entities that are correct, ignoring the entities that the model fails to predict.Recall measures the number of actual entities that are correctly identified, ignoring errors in identification.The F1 score balances Precision and Recall, providing a more comprehensive view of model performance.
Precision ¼ TP TP þ FPð6ÞRecall ¼ TP TP þ FNð7ÞF1 ¼ 2 Ã Precision Ã Recall Precision þ Recallð8Þ
In these metrics: True Positives (TPs) are the number of entities correctly predicted by the model as belonging to a known type.False Positives  (FPs) are the number of entities incorrectly predicted as belonging to a known type or as entities when they are not.False Negatives (FN) are the number of entities that belong to a known type but are incorrectly predicted as not belonging to that type.</p>
<p>Analysis of BLEU and ROUGE results.To evaluate the performance of fine-tuned LLMs in NER tasks, four metrics-BLEU-4, ROUGE-1, ROUGE-2, and ROUGE-L-were used to assess the ChatGLM3-6B, Baichuan2-7B-Base, Xunzi-Baichuan, and Xunzi-GLM language models.The results are summarized in Table 6.</p>
<p>From the analysis of the experimental results, it is evident that in zeroshot prompting fine-tuning, the Xunzi-Baichuan model achieved the highest BLEU-4 score of 93.34, surpassing the ChatGLM3-6B model by 1.8 points.Furthermore, Xunzi-Baichuan had the highest scores across all ROUGE metrics (ROUGE-1, ROUGE-2, and ROUGE-L) among the four models.The fine-tuned Xunzi-GLM model had a BLEU-4 score 0.38 lower than that of the Baichuan2-7B-Base model, indicating that the number of parameters in LLMs does impact performance.In one-shot prompting fine-tuning, the Xunzi-Baichuan model again showed the best performance across all four evaluation metrics.However, the difference between the Xunzi-GLM and Baichuan2-7B-Base models was relatively small, with the BLEU-4, ROUGE-1, ROUGE-2, and ROUGE-L scores differing by 0.36, 0.09, 0.3, and 0.21, respectively.Overall, Xunzi-GLM outperformed the Baichuan2-7B-Base model.In few-shot prompting fine-tuning, the Xunzi-Baichuan model remained the top performer across all four metrics, followed by the Xunzi-GLM, Baichuan2-7B-Base, and ChatGLM3-6B models, in that order.These results suggest that the LLMs used in this study demonstrated excellent performance in text generation and evaluation.The BLEU-4 scores ranged from a high of 93.34 to a low of 91.54, indicating the models' effectiveness in generating text with similar structures.The ROUGE-1, ROUGE-2, and ROUGE-L results further confirmed the high accuracy of the models in terms of text semantics and grammatical structures.Overall, the models exhibited robust performance and a degree of generalization capability.Figure 2 illustrates the differences in results between the models.</p>
<p>Analysis of experimental results.To evaluate the applicability of LLMs in NER tasks, experiments were conducted using the ChatGLM3-6B, Baichuan2-7B-Base, Xunzi-Baichuan, and Xunzi-GLM models on the constructed datasets.Tables 7, 8, 9, and 10 display the experimental results.The accuracy, recall, and F1 scores of these models were analyzed.</p>
<p>In zero-shot prompting, the Xunzi-Baichuan model achieved the highest F1 scores for NER in both Classical and Modern Chinese.It had the highest accuracy of 82.32, surpassing the ChatGLM3-6B model by 3.61 points.The recall was also highest for Xunzi-Baichuan, at 81.41, exceeding the ChatGLM3-6B model by 3.89 points.The highest F1 score was 81.87, again for Xunzi-Baichuan, which was 3.76 points higher than that of the ChatGLM3-6B model.The results indicate that among all types of named entities, the best-performing models are Xunzi-Baichuan, followed by Baichuan2-7B-Base, Xunzi-GLM, and ChatGLM3-6B.Specifically, Xunzi-Baichuan performed best in extracting the Person names (PER), Book titles (BOOK), and Official titles (OFI) entity types.For book title recognition in Modern Chinese, the ChatGLM3-6B model had a higher accuracy, 54.39, than Xunzi-GLM's 52.92.However, their F1 scores differed by only 0.02, likely due to variability in text generation.In Ancient Chinese BOOK entity recognition, Xunzi-Baichuan outperformed ChatGLM3-6B by 0.49 in accuracy and 0.65 in F1 score, possibly due to the smaller number of such entities in the training data affecting performance.</p>
<p>In one-shot prompting, the Xunzi-Baichuan model again showed the highest accuracy across cross-linguistic datasets, with a score that was 3.38 points higher than the lowest-performing ChatGLM3-6B model (79.28).For recall, Xunzi-Baichuan achieved the highest score of 81.73, exceeding ChatGLM3-6B by 3.84 points.The highest F1 score was 82.19, outperforming ChatGLM3-6B by 3.61 points.However, the un-fine-tuned Baichuan2-7B-Base model showed better performance in accuracy, recall, and F1 score across all entity types than did the fine-tuned models.In Modern Chinese entity recognition, Xunzi-GLM's F1 score was 1.94 points lower than that of Baichuan2-7B-Base, possibly because Xunzi-GLM is finetuned from ChatGLM3-6B, whereas Baichuan2-7B-Base has a larger parameter size.Xunzi-Baichuan's recall for the BOOK entity type in Modern Chinese was 1.15 points lower than that of ChatGLM3-6B, but its F1 score was higher by 1.89 points.Overall, the fine-tuned models generally performed better in one-shot prompting.</p>
<p>In few-shot prompting, specifically in the three-shot experiments, Baichuan2-7B-Base significantly outperformed ChatGLM3-6B across all entity types, with accuracy, recall, and F1 scores differing by 2.22, 2.46, and 2.34 points, respectively.This suggests that in NER tasks, a model with 7 B parameters outperforms one with 6 B parameters.The Xunzi-Baichuan model achieved the highest F1 scores across all entity types, with a score 3.28 points higher than the lowest-performing ChatGLM3-6B.In Ancient Chinese entity recognition, Xunzi-Baichuan's F1 score was 82.50, 3.73 points higher than ChatGLM3-6B.For Modern Chinese entity types, Xunzi-Baichuan had an F1 score 2.64 points higher than Xunzi-GLM.In experiments focusing on single-entity types, fine-tuned models generally performed better than non-fine-tuned models.However, fine-tuned models did not always outperform non-fine-tuned ones in Modern Chinese.For instance, Xunzi-GLM's recognition of PER entities was 1.87 points lower than that of Baichuan2-7B-Base, and the F1 score difference between Baichuan2-7B-Base and ChatGLM3-6B was only 0.25 points.Five-shot experiments revealed that Baichuan2-7B-Base outperformed Xunzi-GLM with a 0.27 higher F1 score.For PER entities in Ancient Chinese, Baichuan2-7B-Base also outperformed Xunzi-GLM, with a 0.02 higher F1 score.This indicates that the parameter size of LLMs affects their performance to some extent.For Modern Chinese entity recognition, Xunzi-GLM's results were close to those of ChatGLM3-6B, but the fine-tuned models tailored to specific domains generally outperformed the general models.Figure 3 presents a more intuitive comparison of the model results.</p>
<p>During the testing of the fine-tuned LLMs, it was observed that the models exhibited a certain level of generalization capability.In the instruction prompts and input text, the focus was solely on extracting three entity types: PER, BOOK, and OFI.However, during the evaluation, the LLMs demonstrated generalization abilities, as shown in Table 11.Although the test corpus did not include location entity types or corresponding entity labels, the generated results included the "LOC" entity label and the location entity "Shile River."</p>
<p>Conclusion and future work</p>
<p>With the development and application of technologies, such as artificial intelligence and big data, LLMs can be leveraged to extract entities with</p>
<p>Fig. 1 |
1
Fig. 1 | General research framework for automatic recognition of canonical entities.</p>
<p>CE) of the Ming Dynasty, these texts total 3249 volumes and approximately 40 million characters.The collection includes the following: Records of the Grand Historian (Shiji), Book of Han (Hanshu), Book of the Later Han (Houhanshu), Records of the Three Kingdoms (Sanguozhi), Book of Jin (Jinshu), Book of Song (Songshu), Book of the Southern Qi (Nanqishu), Book of Liang (Liangshu), Book of Chen (Chenshu), Book of Wei (Weishu), Book of the Northern Qi (Beiqishu), Book of Zhou (Zhoushu), Book of Sui (Suishu), Book of Southern History (Nanshi), Book of Northern History (Beishi), Old Book of Tang (Jiutangshu), New Book of Tang (Xintangshu), Old History of the Five Dynasties (Jiu Wudai Shi), New History of the Five Dynasties (Xin Wudai Shi), History of Song (Songshi), History of Liao (Liaoshi), History of Jin (Jinshi), History of Yuan (Yuanshi), and History of Ming (Mingshi).This study utilizes proofread training data from the Twenty-Four Histories, consisting of 62,006 classical Chinese texts and 29,669 modern Chinese texts.Both datasets were divided into training and testing sets at a ratio of 9:1.The basic statistical information of this dataset is in Table</p>
<p>Fig. 2 |
2
Fig. 2 | Comparison results of BLEU-4, ROUGE-1, ROUGE-2, and ROUGE-L metrics.</p>
<p>specific significance from classical texts, such as person names, place names, official titles, and book titles.By exploring and validating the performance of LLMs in the subtask of NER within natural language processing, new research ideas and methods for the intelligent processing of classical texts have emerged.This study used supervised fine-tuning methods to adjust and test four LLMs-Xunzi-Baichuan, Baichuan2-7B-Base, Xunzi-GLM, and ChatGLM3-6B-to verify the feasibility of LLMs in NER research.Implementing named entity extraction in classical texts with LLMs not only preserves the integrity and logical structure of the content but also advances digitalization and knowledge-based approaches for classical texts.LLMs show immense potential in the intelligent processing of classical information, and ongoing research and technological updates can achieve a more accurate and in-depth cross-linguistic understanding and application of classical texts.Future research could focus on enhancing the model's crosslinguistic generalization ability, improving the understanding of the cultural context of classical texts, and increasing model interpretability.</p>
<p>Table 1 |
1
Examples of Basic Prompts
Basic PromptsPrompt FormatPrompt ExamplesZero-shotInstruction请在给定的原文文中抽出实体, 其中PER代表人名、OFI代表地名、用BOOK代表书名, 使用{实体|PER}、{实体|OFI}、{实体|BOOK}格式标注, 不要输出其他内容。One-shotInstructionZero-shot prompt examplesText從還京師, 再遷兵部尚書, 封建平縣子。Labeled Results從還京師, 再遷{兵部尚書|OFI}, 封{建平縣子|OFI}。Three-shotInstructionOne-shot prompt examplesText元和十三年, 李愬為襄陽節度使, 注往依之, 愬得其藥力, 因厚遇之, 署為節度衙推。Labeled Results元和十三年, {李愬|PER}為襄陽{節度使|OFI}, {注|PER}往依之, {愬|PER}得其藥力, 因厚遇之, 署為{節度衙推|OFI}。Text梁故都官郎琅邪王澄美之, 次其行事為孝義傳。Labeled Results梁故{都官郎|OFI}琅邪{王澄|PER}美之, 次其行事為{孝義傳|BOOK}。Five-shotInstructionThree-shot prompt examplesText吳興太守王曇生、義興太守劉延熙、晉陵太守袁標一時響應。Labeled Results吳興{太守|OFI}{王曇生|PER}、義興{太守|OFI}{劉延熙|PER}、晉陵{太守|OFI}{袁標|PER}一時響應。Text案舊唐書: 乾寕元年十二月, 以李匡威故將劉仁恭為幽州兵馬留後。Labeled Results案{舊唐書|BOOK}: 乾寕元年十二月, 以{李匡威|PER}故將{劉仁恭|PER}為{幽州兵馬留後|OFI}。</p>
<p>Table 2 |
2
Basic statistics of the dataset
Text TypeTraining SetTest SetAverage Sentence LengthTotal Character CountClassical55,8046202221,379,989Modern26,7022967351,048,225</p>
<p>Table 4 |
4
Environment setup
SetupSpecification ParametersOperating systemUbuntu 20.04.6 LTSCPUIntel(R) Xeon(R) Gold 5218 CPU @ 2.30 GHzGPUQuadro RTX 8000Memory503GPythonPython 3.10PyTorch2.1.2+cu121</p>
<p>Table 5 |
5
Model parameter settings
ParametersParameter DescriptionParameterContentstageSpecify Training StageSFTfinetuning_typeSpecify Fine-Tuning TypeLoRAtemplateSpecify Template Name fordefaultData Processingcutoff_lenSpecify Maximum Length1024Truncation for InputSequencelearning_rateLearning Rate5e-05per_device_train_batch_sizeBatch Size per GPU4lora_rankRank of LoRA8lora_dropoutDropout rate of LoRA0.1lora_targetTarget layers of LoRAW_pack</p>
<p>Table 3 |
3
Data format examples</p>
<p>Table 6 |
6
BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L evaluation metrics results
ModelChatGLM3-6BBaichuan2-7B-BaseShotZeroOneThreeFiveZeroOneThreeFiveBLEU-491.5491.6791.7991.7492.8092.8192.7592.38ROUGE-197.3797.3897.4597.4197.7597.7897.7397.62ROUGE-291.5591.6991.8691.7992.7592.8892.8192.47ROUGE-L95.3195.3895.4395.3995.9796.0495.9995.80ModelXunzi-BaichuanXunzi-GLMShotZeroOneThreeFiveZeroOneThreeFiveBLEU-493.3493.4393.3893.3392.4292.4592.5992.57ROUGE-198.0298.0398.0498.0297.7197.6997.7497.75ROUGE-293.4993.5793.5593.5292.5692.5892.7692.73ROUGE-L96.3696.4196.4096.3995.8395.8395.9395.93</p>
<p>Table 7 |
7
Experiment results of the Baichuan2-7B-Base Language Model
Baichuan2-7B-BaseShotALLFANJIANZeroallPEROFIBOOKallPEROFIBOOKallPEROFIBOOKF180.7984.5077.4351.2481.0084.1178.5852.8580.4185.2075.4248.05R80.1684.1276.7654.4780.3583.7377.8050.2079.8484.8274.9344.83P81.4284.8878.1248.3781.6784.4979.3855.8080.9985.5875.9151.77OneF181.2684.4678.5153.9181.5984.2879.5756.2680.6884.7976.6549.29R80.7084.2677.7251.1180.9784.0378.6653.5480.2384.6676.0646.36P81.8384.6779.3157.0482.2284.5380.4959.2681.1384.9177.2452.61ThreeF181.0484.6777.5754.5781.3484.2878.8956.7080.5285.3775.2650.31R80.4684.3676.8752.0280.7283.9378.0954.5380.0085.1174.7247.13P81.6384.9978.2957.3981.9684.6379.7159.0681.0485.6275.8153.95FiveF180.1984.0476.4852.3680.3483.7677.3353.5879.9284.5474.9849.90R79.5783.6375.8549.6779.7183.3376.6051.5779.3384.1574.5345.98P80.8184.4677.1255.3680.9884.2078.0855.7480.5284.9375.4354.55</p>
<p>Table 10 |
10
Experiment results of the Xunzi-GLM Language Model
Xunzi-GLMShotALLFANJIANZeroallPEROFIBOOKallPEROFIBOOKallPEROFIBOOKF179.5683.5475.5253.1580.2183.6676.0754.4378.4383.3472.9050.70R79.0483.4074.7650.4679.5183.3678.0051.3878.2383.4772.4548.66P80.0983.6976.3056.1580.9283.9677.0257.8778.6483.2173.3552.92OneF179.7383.6575.7254.3380.2883.6277.1955.8978.7483.6873.1351.23R79.1783.4074.9851.7679.7583.4376.3853.7478.1483.3472.5247.89P80.2983.8976.4757.1880.8283.8178.0158.2179.3584.0373.7655.07ThreeF179.8283.7675.8653.0480.6183.9177.6255.3078.4283.5072.7848.56R79.1783.4875.0249.9479.9283.6976.5552.3677.8583.1072.3345.21P80.4784.0476.7156.5581.3184.1278.7258.5979.0083.9173.2352.44FiveF179.9283.6476.0956.0180.5683.7477.6157.0578.7983.4773.4153.96R79.3683.4575.3053.0680.0083.6476.6754.1378.2583.1272.8850.96P80.4983.8376.8959.3081.1483.8578.5860.3179.3483.8173.9557.33</p>
<p>Table 9 |
9
Experiment results of the Xunzi-Baichuan Language Model
Xunzi-BaichuanShotALLFANJIANZeroallPEROFIBOOKallPEROFIBOOKallPEROFIBOOKF181.8785.6478.1155.9182.3985.6879.4257.2680.9485.5875.8253.31R81.4185.5677.4253.1981.9085.6078.6554.3380.5585.4975.2750.96P82.3285.7378.8258.9382.8985.7680.2060.5381.3385.6776.3955.88OneF182.1985.5779.0656.2482.6385.3580.4258.6381.4385.9476.6851.53R81.7385.3978.5453.0682.0585.1279.6555.5181.1885.8476.5848.28P82.6685.7579.5959.8283.2285.5981.2162.1181.6786.0376.7755.26ThreeF181.9885.2179.0157.3082.5085.2580.3158.7081.0685.1476.7454.47R81.5385.0378.3655.1482.0985.1579.6057.0980.5384.8476.1851.34P82.4485.3879.6859.6382.9285.3581.0360.4281.6085.4477.3058.01FiveF181.8385.2178.8154.9082.2285.0580.0356.2981.1585.4776.6452.21R81.4385.0178.3652.4181.8084.8679.5653.7480.7685.2776.2549.81P82.2485.4079.2657.6582.6485.2480.5159.0981.5485.6877.0554.85</p>
<p>Table 8 |
8
Experiment results of the ChatGLM3-6B Language Model
ChatGLM3-6BShotALLFANJIANZeroallPEROFIBOOKallPEROFIBOOKallPEROFIBOOKF178.1181.8974.1754.6278.2581.2975.3856.6177.8682.9472.0350.72R77.5281.5173.5858.1577.6280.9174.6653.5477.3582.5771.6647.51P78.7182.2774.7751.5078.9081.6776.1160.0478.3883.3272.4154.39OneF178.5882.2974.8753.5778.4581.7975.3853.6578.8283.1773.9653.42R77.8981.8874.0650.7277.7781.4374.4551.3878.1282.6773.3749.43P79.2882.7175.6956.7779.1482.1576.3356.1379.5483.6974.5658.11ThreeF178.7082.1875.2854.3278.7781.5876.4654.9478.5683.2573.2053.09R78.0081.7574.4751.5078.0981.1875.5852.5677.8482.7472.5249.43P79.4182.6276.1057.4779.4781.9877.3657.5479.3083.7573.8957.33FiveF178.2681.9874.3156.0377.9881.0275.1056.3878.7683.6772.9255.35R77.6381.5973.5953.4577.3780.7174.3053.9478.1083.1472.3352.49P78.9082.3875.0558.8878.5981.3475.9259.0579.4384.2073.5258.55
npj Heritage Science | (2025) 13:59
AcknowledgementsThis work was supported by the National Social Science Foundation of China (Grants Nos.22 and ZD262) and the National Social Science Foundation of China (Grants Nos.21 and ZD331).Data availabilityData and code will be available soon at https://github.com/Xunzi-LLM-of-Chinese-classics/XunziALLM.AbbreviationsCompeting interestsThe authors declare no competing interests.
Review and trend of researches on ancient Chinese character information processing. S Q Huang, D B Wang, Libr. Inf. Serv. 612017in Chinese</p>
<p>Key technologies for digitization of ancient Chinese books. Q Su, R F Hu, Y C Zhu, C X Yan, J Wang, Digit. Humanit. Res. 12021in Chinese</p>
<p>Review of automatic processing of ancient Chinese character and prospects for its development trends in the new era. S H Deng, H T Hu, H Wang, D B Wang, Sci. Inf. Res. 32021in Chinese</p>
<p>Knowledge representation and sentence segmentation of ancient Chinese based on deep language model. R F Hu, S Li, Y C Zhu, J. Chin. Inf. Process. 352021in Chinese</p>
<p>Construction of computer-aided collation repository of ancient editions. J Y Liu, X W Zhou, 2017Libr. Theory Practin Chinese</p>
<p>Ancient-modern Chinese translation with a new large training dataset. D H Liu, K X Yang, Q Qu, ACM Trans. Asian Low.-Resour. Lang. Inf. Process. 192019TALLIP 19</p>
<p>The construction and application of ancient Chinese corpus with word sense annotation. L Shu, Y L Guo, H P Wang, X T Zhang, R F Hu, J. Chin. Inf. Process. 362022in Chinese</p>
<p>Ancient Chinese named entity recognition based on deeping learning. M C Zhuo, J D Z Sang, R J Cai, Comput. Sci. Appl. 102020</p>
<p>Automatic recognition of produce entities from local chronicles with deep learning. C F Xu, H Y Ye, P Bao, Data Anal. Knowl. Discov. 42020in Chinese</p>
<p>Named entity recognition and part-of-speech tagging for classical Chinese chronicle texts. X Zhu, 2012Fudan Universityin Chinese</p>
<p>Rule-based Chinese person names identification in ancient Chinese literature of annals-biography (Jizhuan) style. P J Huang, L Y Wang, Libr. Inf. Serv. 572013in Chinese</p>
<p>Research on constructing automatic recognition model for ancient Chinese place names based on pre-qin corpus. S Q Huang, D B Wang, I He, Libr. Inf. Serv. 592015in Chinese</p>
<p>Construction of automatic recognition model of multi-type named entities for local gazetteers. N Li, Libr. Trib. 412021in Chinese</p>
<p>Study on named entity recognition of traditional Chinese medicine classics: taking SikuBERT pre-training model enhanced by the flat-lattice transformer for example. J Xie, J F Liu, D B Wang, Libr. Trib. 422022in Chinese</p>
<p>Animal named entity recognition in ancient Chinese Classics from the perspective of digital humanities: based on SikuBERT pre-training model. L T Lin, D B Wang, J F Llu, B Li, M X Feng, Libr. Trib. 422022in Chinese</p>
<p>Named entity recognition of chrysanthemum poetry based on deep learning models. J F Cui, D J Zheng, D B Wang, T T Li, Inf. Stud.: Theory Appl. 432020in Chinese</p>
<p>CHisIEC: an information extraction corpus for ancient Chinese history. X M Tang, Q Su, J Wang, LREC-COLING 2024. 2024</p>
<p>Classifying ancient Chinese text relations with entity information. X M Tang, Q Su, J Wang, Data Anal. Knowl. Discov. 82024in Chinese</p>
<p>Research on the classification of various classics of sinology citation series by integrating entity feature knowledge. H R Qin, 2019Nanjing Agricultural Universityin Chinese</p>
<p>Joint extraction of ancient Chinese entity relations by combining global correspondence matrix and relative position information. Y Y Hu, J L Zuo, X Q Ceng, Proc. 22nd Chinese National Conference on Computational Linguistics. 22nd Chinese National Conference on Computational Linguistics2023</p>
<p>X-Instruction: Aligning Language Model in Low-resource Languages with Self-curated Cross-lingual Instructions. C Li, W Yang, J J Zhang, ACL (Findings). 2024</p>
<p>Perspective of digital humanities on person names in Chinese pre-qin classic. L Liu, C F Liu, W Q Li, J. Comput. Cult. Herit. 172024</p>
<p>Named entity recognition for ancient books based on continual pre-training method and context augmentation strategy. S Q Wang, L L Shi, L W Pu, Chinese Information Processing Society of China. 2023</p>
<p>System report for CCL23-Eval Task 1: information theory constraint and paragraph-based classical named entity recognition. X H Zhang, T Y Liu, W Y Zhang, Chinese Information Processing Society of China. 2023</p>
<p>GuNER based on incremental pretraining and adversarial learning. J L Li, Y R Yu, X Y Liu, Chinese Information Processing Society of China. 2023</p>
<p>HanNER: a general framework for the automatic extraction of named entities in ancient Chinese Corpora. C X Yan, X M Tang, H Yang, Q Su, J Wang, J. China Soc. Sci. Tech. Inf. 422023</p>
<p>Delving deep into regularity: a simple but effective method for Chinese NER. Y J Gu, X Y Qu, Z F Wang, </p>
<p>Recognition and extraction of titles in Chinese Diachronic Corpora. D Xiong, J Xu, Q Lu, Proc. Chinese Lexical Semantic Workshop. Chinese Lexical Semantic Workshop20146</p>
<p>The Research and Implementation of Named Entity Recognition based on Ancient Literature (Beijing University of Posts and Telecommunications. T Xie, 2018in Chinese</p>
<p>A Study on the Recognition of Named Entities of Ancient Books Using Lexical Information. W J Kang, J L Zuo, A Q Jie, Chinese Information Processing Society of China. 2023</p>
<p>Research and implementation of named entity recognition in the field of ancient Chinese. Z Y Lv, 2020Dalian University of Technologyin Chinese</p>
<p>Named entity recognition method incorporating stroke features. L Y Jiang, Y D Wu, S H Wang, W H Zhang, Y Li, Sci. Technol. Eng. 232023in Chinese</p>
<p>An adaptive entity recognition method with attention mechanism. Q L Chen, G H Huang, Y Z Wang, K Zhang, Z Y Du, J. Chin. Inf. Process. 352021+73(in Chinese</p>
<p>Agricultural technology knowledge intelligent question-answering system based on large language model. T Wang, N Wang, Y P Cui, J Liu, Smart Agric. 52023</p>
<p>Improving Pseudo Labels with Global-Local Denoising Framework for Cross-lingual Named Entity Recognition. Z J Ding, W Wei, X Y Qu, IJCAI. 625262602024</p>
<p>OpenBA-V2: Reaching 77. D Qiao, Y Su, P Z Wang, 3% High Compression Ratio with Fast Multi-Stage Pruning. CoRR abs/ 2405. 20245957</p>
<p>Incorporating Lexical and Syntactic Knowledge for Unsupervised Cross-Lingual Transfer. J Y Zheng, F F Fan, J Q Li, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation. the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation2024</p>
<p>G C Jiang, Z Q Luo, Y C Shi, Toner, Type-oriented Named Entity Recognition with Generative Language Model. LREC/COLING. 2024</p>
<p>ELLEN: extremely lightly supervised learning for efficient named entity recognition. H Riaz, R G Dumitru, M Surdeanu, LREC-COLING 2024. 2024</p>
<p>Y Z Heng, C Y Deng, Y T Li, Proggen, Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models. ACL (Findings). 2024</p>
<p>VerifiNER: Verificationaugmented NER via Knowledge-grounded Reasoning with Large Language Models. S Kim, K Seo, H Chae, J Yeo, D Lee, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Continued pretraining for better zero-and few-shot promptability. Z F Wu, Iv R L Logan, P Walsh, A Bhagia, D Groeneveld, S Singh, I Beltagy, Proc. Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language essing2022</p>
<p>Language models are few-shot learners. T Brown, Advances in Neural Information Processing Systems. 33NeurIPS 2020. 2020</p>
<p>Fine-tuned language models are zero-shot learners. J Wei, M Bosma, V Y Zhao, K Guu, Y A Wei, ICLR 2022 Oral. 2022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X W Wang, D Schuurmans, M Bosma, Neural Information Processing Systems. NeurIPS 2022. 2022</p>
<p>W L Chang, L M Zheng, Y Sheng, Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference. Forty-first International Conference on Machine Learning. 2024</p>
<p>State-of-the-art Parameter-Efficient Fine-Tuning (PEFT) methods. S Mangrulkar, </p>
<p>An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models. Z Q Hu, Y H Lan, L Wang, Llm-Adapters, EMNLP. 525452762023</p>
<p>Dial-insight: Fine-tuning Large Language Models with High-Quality Domain-Specific Data Preventing Capability Collapse. J W Sun, C L Mei, L L Wei, CoRR abs/2403.091672024</p>
<p>Baichuan 2: Open Large-scale Language Models. A Y Yang, CoRR abs/2309.103052023</p>
<p>GLM: general language model pretraining with autoregressive blank infilling. Z X Du, Y J Qian, X Liu, Proc. 60th Annual Meeting of the Association for Computational Linguistics 320-335. 60th Annual Meeting of the Association for Computational Linguistics 320-335Association for Computational Linguistics2022</p>
<p>BLEU: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W Zhu, Proc. 40th Annual Meeting of the Association for Computational Linguistics (ACL). 40th Annual Meeting of the Association for Computational Linguistics (ACL)Association for Computational Linguistics2002</p>
<p>ROUGE: a package for automatic evaluation of summaries. C.-Y Lin, Proc. Workshop on Text Summarization Branches Out. Workshop on Text Summarization Branches Out2004</p>
<p>MUC-4 evaluation metrics. N Chinchor, Proc. Conference on Message Understanding. Conference on Message UnderstandingAssociation for Computational Linguistics1992</p>
<p>A probabilistic interpretation of precision, recall and F-Score, with implication for evaluation. C Goutte, E Gaussier, 10.1007/978-3-540-31865-1_25Advances in Information Retrieval. ECIR 2005. Lecture Notes in Computer Science. D E Losada, J M &amp;fernández-Luna, Springer20053408</p>            </div>
        </div>

    </div>
</body>
</html>
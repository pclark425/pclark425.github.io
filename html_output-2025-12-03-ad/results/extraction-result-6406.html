<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6406 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6406</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6406</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-273403613</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.13699v2.pdf" target="_blank">Unconstrained Model Merging for Enhanced LLM Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Recent advancements in building domain-specific large language models (LLMs) have shown remarkable success, especially in tasks requiring reasoning abilities like logical inference over complex relationships and multi-step problem solving. However, creating a powerful all-in-one LLM remains challenging due to the need for proprietary data and vast computational resources. As a resource-friendly alternative, we explore the potential of merging multiple expert models into a single LLM. Existing studies on model merging mainly focus on generalist LLMs instead of domain experts, or the LLMs under the same architecture and size. In this work, we propose an unconstrained model merging framework that accommodates both homogeneous and heterogeneous model architectures with a focus on reasoning tasks. A fine-grained layer-wise weight merging strategy is designed for homogeneous models merging, while heterogeneous model merging is built upon the probabilistic distribution knowledge derived from instruction-response fine-tuning data. Across 7 benchmarks and 9 reasoning-optimized LLMs, we reveal key findings that combinatorial reasoning emerges from merging which surpasses simple additive effects. We propose that unconstrained model merging could serve as a foundation for decentralized LLMs, marking a notable progression from the existing centralized LLM framework. This evolution could enhance wider participation and stimulate additional advancement in the field of artificial intelligence, effectively addressing the constraints posed by centralized models.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6406.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6406.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-Math-7B-RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-Math-7B-RL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-class DeepSeek variant fine-tuned / RL-optimized for mathematical problem solving and multi-step reasoning; used as a source expert model in merging and fusion experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-Math-7B-RL</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Instruction- / RL-fine-tuned on math-focused datasets (e.g., MetaMath-style datasets, math problem-solving corpora referenced in DeepSeek work); explicitly math-optimized fine-tuning is emphasized.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH, MMLU (also used as a source in fusion/merged evals)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word problems, algebraic reasoning, other math problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems; also evaluated when generating Python code to solve math problems (GSM8K-Coding, MATH-Coding)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K (grade-school to intermediate arithmetic); MATH (harder, multi-step mathematical problems)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Chain-of-Thought (COT) evaluation used for GSM8K-COT and MATH-COT; standard instruction / conversation templates for SFT and for use as pivot in fusion</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (problem-solution correctness) and code-execution correctness when generating programs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Used as a high-performing math expert source; in case studies it often answers correctly on MATH and GSM8K where coder models fail; reported qualitatively to improve fused/merged outcomes when used as pivot</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Acts as the primary provider of mathematical reasoning in fusion/merging; no internal mechanistic probing reported beyond behavioural comparisons (e.g., true/false answer distributions, examples showing preserved math steps).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Occasional incorrect numeric/logic outputs in examples (some errors shown in case studies), but generally stronger on math than coder models; specific failure types are not deeply probed for this model alone in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Paper reports math-optimized models (like DeepSeek-Math or MetaMath-70B) serve better as pivots in fusion than coding-optimized models, suggesting fine-tuning for complex math is important for transfer in fusion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unconstrained Model Merging for Enhanced LLM Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6406.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6406.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-Coder-Ins-v1.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-Coder-Instruct-v1.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A DeepSeek-derived instruction-tuned model specialized for code generation and programming tasks; used as a source in merging and fusion experiments and as a baseline for coding benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-Coder-Instruct-v1.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (as reported in source list)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Instruction-tuned on code and programming datasets; includes code-focused instruction-response fine-tuning (DeepSeek code corpora referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>HumanEval, InfiBench, GSM8K-Coding, MATH-Coding (when asked to write Python to solve math problems)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>program synthesis / code generation for algorithmic tasks and for solving math problems via code</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language prompts requesting code; evaluation via execution & keyword matching (InfiBench includes keyword-based scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>programming tasks of varying difficulty (HumanEval-style), and math problems posed as code tasks (GSM8K/MATH coding variants)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Instruction / conversation templates, evaluated under SFT setup; Chain-of-Thought style reasoning used for GSM8K-COT/MATH-COT comparisons where applicable</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>code-execution correctness, keyword-matching metrics (InfiBench), and general accuracy on coding QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Strong on coding benchmarks relative to math-specialist models; in case studies coder model often fails math-specific reasoning steps that math model handles correctly; merged models sometimes surpass the coder baseline on coding+math integrated tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioural observations show coder model sometimes provides irrelevant reasoning or incorrect arithmetic steps; merging with math model can correct coder mistakes (qualitative examples provided). No neuron-level mechanistic probes reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Irrelevant or incorrect intermediate reasoning in math word problems, arithmetic miscalculations when not backed by math reasoning; produces incorrect chain-of-thoughts in some GSM8K examples.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Fine-tuning (instruction-tuning) influence is strong; coder models provide coding skill transfer but are weaker at math reasoning unless merged/fused with math experts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unconstrained Model Merging for Enhanced LLM Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6406.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6406.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetaMath-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MetaMath-70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large (70B) model optimized for mathematical problem solving (MetaMath family); used as a source in fusion experiments and shown to contribute math reasoning when fused with coding models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MetaMath-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained / instruction-tuned on MetaMath-style math datasets (MetaMathQA and other math instruction corpora), designed for advanced math reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH, GSM8K-COT, MATH-COT (used in fusion evals and case studies)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step math word problems, algebra, number theory style tasks</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems; examples also used to transfer reasoning to coding tasks via fusion</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>MATH-level harder problems; GSM8K-level problems as well</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>SFT conversation templates when used as pivot or source; Chain-of-Thought style evaluation applied to datasets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy on math benchmarks, qualitative correctness in examples</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Serves as a strong math source in fusion; case studies show MetaMath answers some problems that coder models miss and fusion models often inherit those correct solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No internal mechanistic probes; contribution characterized behaviourally via examples and improvements in fused models' correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Occasional incorrect outputs in examples; when both sources wrong, fusion usually also fails but some exceptions exist.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Large math-specialized models can transfer math capability to fused models, supporting the paper's point that using math-optimized models as pivots yields superior fusion results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unconstrained Model Merging for Enhanced LLM Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6406.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6406.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EvoMerged</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evolutionary layer-wise merged model (Task Arithmetic + TIES merged with CMA-ES)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A merged LLM created via homogeneous parameter-level merging using Task Arithmetic and TIES-Merging recipes, with layer-wise merging coefficients optimized by CMA-ES on small target datasets to improve reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Merged model (e.g., DeepSeek-Math-7B-RL + DeepSeek-Coder-Instruct-v1.5 merged)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (merged weights from homogeneous architectures)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B-class (result of merging two 7B-class models; reported as same size as sources)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>No raw-data retraining; merging operates on pretrained/fine-tuned weights. CMA-ES optimization used supervised target sets (samples from MMLU, GSM8K, MBPP, MBPP/MBPP-like coding samples) to choose layer coefficients.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH, MMLU, HumanEval, InfiBench, GSM8K-COT, MATH-COT (evaluated comprehensively)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word problems, algebra, and code generation for math problems (GSM8K-Coding, MATH-Coding)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems, and prompts requesting Python code (execution checks used for coding tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>varied: GSM8K (grade-level multi-step), MATH (harder multi-step math)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Evaluation used Chain-of-Thought prompting (GSM8K-COT, MATH-COT) and standard instruction templates; Task Arithmetic merging uses task vectors derived from fine-tuned weights.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy on benchmarks and code-execution correctness; improvements reported qualitatively and flagged as >5% (↑↑) or 1-5% (↑) relative to sources in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Merged models often outperform individual source models on integrated math+coding tasks; paper reports cases where merged model exceeds a source by >=5% (marked ↑↑) or 1-5% (↑); also instances where merged model underperforms due to interference.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Several analyses reported: (1) Table of True/False answer distributions showing merged model tends to be correct when both sources are correct and often correct when one math source is correct; (2) TIES-Merging analysis: high per-layer density of retained weights and sign conflicts can cause interference; (3) CMA-ES optimizes per-layer merging coefficients (every 5 or 10 layers share weights/density) using small target sets; (4) dataset-target mismatch (e.g., MBPP vs HumanEval) impacts outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Interference due to sign conflicts in TIES (parameter sign disagreement can degrade domain-specific performance); overfitting to small search datasets leading to poor generalization on other benchmarks; merged models can still fail when both sources fail; coding benchmarks (HumanEval/InfiBench) sometimes remain worse than source due to inadequate coverage in the target dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Paper reports combinatorial emergent capabilities (merged model acquires integrated reasoning beyond additive effects). Performance depends strongly on target dataset used for CMA-ES search and choice of pivot; using more math-optimized sources or larger/more diverse search data improves results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unconstrained Model Merging for Enhanced LLM Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6406.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6406.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DistFusion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distribution-based fusion for heterogeneous LLMs (MinCE fusion + token alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A heterogeneous-model fusion pipeline that aligns instruction-response output distributions across models via token-alignment statistics and fuses them using a minimum cross-entropy (MinCE) fusion objective, followed by SFT with a combined loss.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Fusion models (pairwise heterogeneous fusions such as OpenMath-Mistral-7B + DeepSeek-Coder-Ins-v1.5, Qwen2.5-Math-7B-Ins + CodeLlama-70B-Ins, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>heterogeneous decoder-only transformers (various sizes & tokenizers)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>varied across pairs (7B up to 70B reported for sources)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Fusion uses instruction-response fine-tuning datasets: a curated subset (~60k samples: ~15k MetaMathQA + 15k DeepSeek coder samples plus other in-domain instruction data) to compute per-token output distributions and train fused targets via a combined SFT + fusion loss.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH, MMLU, HumanEval, InfiBench (evaluated on same benchmarks as homogeneous experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic/word problems and program synthesis for solving math tasks (GSM8K-Coding, MATH-Coding), plus coding QA</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems and instruction prompts for code generation; fusion uses the pivot model's conversation template for processing inputs</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>varied; experiments include grade-school to advanced math (GSM8K, MATH subsets) and code evaluation suites (HumanEval/InfiBench)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Pivot-based conversation templates; SFT training with combined loss L = λ L_SFT + (1-λ) L_Fusion where L_Fusion encourages output distribution alignment to fused MinCE distributions; evaluation uses Chain-of-Thought where relevant.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy on math benchmarks and code-execution correctness; comparisons vs pivot/source models shown in tables and figures</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Many fusion pairs outperform their pivot models on benchmarks; qualitative and comparative gains shown in figures/tables. Some fusions do not outperform pivot (noted cases), indicating dependence on pivot, conversation template, and fine-tuning balance.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Key methodological analyses: (1) token alignment strategy using sequence-level dynamic programming to map tokens between tokenizers (handles 1:1, 1:n, n:1 mappings via global statistics); (2) MinCE fusion function to fuse aligned distributions; (3) observed that conversation template choice (pivot) and finetuning loss balance strongly affect fusion effectiveness; (4) behavioural observations showing fusion can transfer math reasoning into coding outputs (case studies).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Conversation-template mismatch (pivot template non-universal) can prevent gains; math skills are harder to transfer than coding; fusion can fail to outperform pivot in some pairs; when both source models fail, fusion usually fails though some exceptions occur.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Using models optimized for more complex tasks (math) as pivots yields better fused performance; fine-tuning of the target model often dominates benefit from fusion; transferability depends on task difficulty and pivot choice.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unconstrained Model Merging for Enhanced LLM Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6406.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6406.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (COT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting / evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting/evaluation method where models produce intermediate reasoning steps (COT) and are evaluated on multi-step arithmetic/math benchmarks (GSM8K-COT, MATH-COT) to reveal reasoning behavior and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain-of-Thought prompting (method applied to evaluated models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>prompting strategy (not a model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K-COT, MATH-COT (benchmarks where COT responses are elicited and scored)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic and math word problems requiring stepwise reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems eliciting step-by-step chain-of-thought</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>multi-step problems where explicit intermediate steps help (GSM8K, MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>explicit Chain-of-Thought elicitation during evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy of final answer (and qualitative correctness of intermediate steps); code-execution for coding variants when COT leads to code output</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Used to compare source, merged, and fused models; merged/fused models often show preserved or improved COT performance on math tasks relative to coder-only sources (qualitative improvements illustrated in examples and plotted in figures).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper uses COT outputs to qualitatively analyze whether merged/fused models inherit correct intermediate reasoning steps (case study examples show merged model often corrects coder's irrelevant steps). No mechanistic/neuron-level analysis of COT processing presented.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Coder models often produce irrelevant or incorrect chains; merged/fused models can still produce incorrect chains especially when both sources fail; COT performance sensitive to training/fine-tuning coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Merged/fused models can preserve or enhance COT performance, especially when one source specializes in math; effectiveness depends on source specialties and merging/fusion recipe.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unconstrained Model Merging for Enhanced LLM Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deepseek-coder: When the large language model meets programming - the rise of code intelligence <em>(Rating: 2)</em></li>
                <li>Deepseekmath: Pushing the limits of mathematical reasoning in open language models <em>(Rating: 2)</em></li>
                <li>MetaMath: Bootstrap your own mathematical questions for large language models <em>(Rating: 2)</em></li>
                <li>Ties-merging: Resolving interference when merging models <em>(Rating: 2)</em></li>
                <li>Editing models with task arithmetic <em>(Rating: 2)</em></li>
                <li>Knowledge fusion of large language models <em>(Rating: 2)</em></li>
                <li>Fusechat: Knowledge fusion of chat models <em>(Rating: 2)</em></li>
                <li>Model Soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time <em>(Rating: 1)</em></li>
                <li>The cma evolution strategy: a comparing review <em>(Rating: 1)</em></li>
                <li>Infibench: Evaluating the question-answering capabilities of code llms <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6406",
    "paper_id": "paper-273403613",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "DeepSeek-Math-7B-RL",
            "name_full": "DeepSeek-Math-7B-RL",
            "brief_description": "A 7B-class DeepSeek variant fine-tuned / RL-optimized for mathematical problem solving and multi-step reasoning; used as a source expert model in merging and fusion experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek-Math-7B-RL",
            "model_family": "decoder-only transformer",
            "model_size": "7B",
            "training_data_description": "Instruction- / RL-fine-tuned on math-focused datasets (e.g., MetaMath-style datasets, math problem-solving corpora referenced in DeepSeek work); explicitly math-optimized fine-tuning is emphasized.",
            "benchmark_name": "GSM8K, MATH, MMLU (also used as a source in fusion/merged evals)",
            "task_type": "multi-step arithmetic word problems, algebraic reasoning, other math problem solving",
            "problem_format": "natural-language word problems; also evaluated when generating Python code to solve math problems (GSM8K-Coding, MATH-Coding)",
            "difficulty_level": "GSM8K (grade-school to intermediate arithmetic); MATH (harder, multi-step mathematical problems)",
            "prompting_method": "Chain-of-Thought (COT) evaluation used for GSM8K-COT and MATH-COT; standard instruction / conversation templates for SFT and for use as pivot in fusion",
            "performance_metric": "accuracy (problem-solution correctness) and code-execution correctness when generating programs",
            "performance_value": "Used as a high-performing math expert source; in case studies it often answers correctly on MATH and GSM8K where coder models fail; reported qualitatively to improve fused/merged outcomes when used as pivot",
            "internal_analysis": "Acts as the primary provider of mathematical reasoning in fusion/merging; no internal mechanistic probing reported beyond behavioural comparisons (e.g., true/false answer distributions, examples showing preserved math steps).",
            "failure_modes": "Occasional incorrect numeric/logic outputs in examples (some errors shown in case studies), but generally stronger on math than coder models; specific failure types are not deeply probed for this model alone in the paper.",
            "scaling_trend": "Paper reports math-optimized models (like DeepSeek-Math or MetaMath-70B) serve better as pivots in fusion than coding-optimized models, suggesting fine-tuning for complex math is important for transfer in fusion.",
            "uuid": "e6406.0",
            "source_info": {
                "paper_title": "Unconstrained Model Merging for Enhanced LLM Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "DeepSeek-Coder-Ins-v1.5",
            "name_full": "DeepSeek-Coder-Instruct-v1.5",
            "brief_description": "A DeepSeek-derived instruction-tuned model specialized for code generation and programming tasks; used as a source in merging and fusion experiments and as a baseline for coding benchmarks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek-Coder-Instruct-v1.5",
            "model_family": "decoder-only transformer",
            "model_size": "7B (as reported in source list)",
            "training_data_description": "Instruction-tuned on code and programming datasets; includes code-focused instruction-response fine-tuning (DeepSeek code corpora referenced).",
            "benchmark_name": "HumanEval, InfiBench, GSM8K-Coding, MATH-Coding (when asked to write Python to solve math problems)",
            "task_type": "program synthesis / code generation for algorithmic tasks and for solving math problems via code",
            "problem_format": "natural-language prompts requesting code; evaluation via execution & keyword matching (InfiBench includes keyword-based scoring)",
            "difficulty_level": "programming tasks of varying difficulty (HumanEval-style), and math problems posed as code tasks (GSM8K/MATH coding variants)",
            "prompting_method": "Instruction / conversation templates, evaluated under SFT setup; Chain-of-Thought style reasoning used for GSM8K-COT/MATH-COT comparisons where applicable",
            "performance_metric": "code-execution correctness, keyword-matching metrics (InfiBench), and general accuracy on coding QA",
            "performance_value": "Strong on coding benchmarks relative to math-specialist models; in case studies coder model often fails math-specific reasoning steps that math model handles correctly; merged models sometimes surpass the coder baseline on coding+math integrated tasks.",
            "internal_analysis": "Behavioural observations show coder model sometimes provides irrelevant reasoning or incorrect arithmetic steps; merging with math model can correct coder mistakes (qualitative examples provided). No neuron-level mechanistic probes reported.",
            "failure_modes": "Irrelevant or incorrect intermediate reasoning in math word problems, arithmetic miscalculations when not backed by math reasoning; produces incorrect chain-of-thoughts in some GSM8K examples.",
            "scaling_trend": "Fine-tuning (instruction-tuning) influence is strong; coder models provide coding skill transfer but are weaker at math reasoning unless merged/fused with math experts.",
            "uuid": "e6406.1",
            "source_info": {
                "paper_title": "Unconstrained Model Merging for Enhanced LLM Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "MetaMath-70B",
            "name_full": "MetaMath-70B",
            "brief_description": "A large (70B) model optimized for mathematical problem solving (MetaMath family); used as a source in fusion experiments and shown to contribute math reasoning when fused with coding models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MetaMath-70B",
            "model_family": "decoder-only transformer",
            "model_size": "70B",
            "training_data_description": "Trained / instruction-tuned on MetaMath-style math datasets (MetaMathQA and other math instruction corpora), designed for advanced math reasoning.",
            "benchmark_name": "GSM8K, MATH, GSM8K-COT, MATH-COT (used in fusion evals and case studies)",
            "task_type": "multi-step math word problems, algebra, number theory style tasks",
            "problem_format": "natural-language word problems; examples also used to transfer reasoning to coding tasks via fusion",
            "difficulty_level": "MATH-level harder problems; GSM8K-level problems as well",
            "prompting_method": "SFT conversation templates when used as pivot or source; Chain-of-Thought style evaluation applied to datasets",
            "performance_metric": "accuracy on math benchmarks, qualitative correctness in examples",
            "performance_value": "Serves as a strong math source in fusion; case studies show MetaMath answers some problems that coder models miss and fusion models often inherit those correct solutions.",
            "internal_analysis": "No internal mechanistic probes; contribution characterized behaviourally via examples and improvements in fused models' correctness.",
            "failure_modes": "Occasional incorrect outputs in examples; when both sources wrong, fusion usually also fails but some exceptions exist.",
            "scaling_trend": "Large math-specialized models can transfer math capability to fused models, supporting the paper's point that using math-optimized models as pivots yields superior fusion results.",
            "uuid": "e6406.2",
            "source_info": {
                "paper_title": "Unconstrained Model Merging for Enhanced LLM Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "EvoMerged",
            "name_full": "Evolutionary layer-wise merged model (Task Arithmetic + TIES merged with CMA-ES)",
            "brief_description": "A merged LLM created via homogeneous parameter-level merging using Task Arithmetic and TIES-Merging recipes, with layer-wise merging coefficients optimized by CMA-ES on small target datasets to improve reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Merged model (e.g., DeepSeek-Math-7B-RL + DeepSeek-Coder-Instruct-v1.5 merged)",
            "model_family": "decoder-only transformer (merged weights from homogeneous architectures)",
            "model_size": "7B-class (result of merging two 7B-class models; reported as same size as sources)",
            "training_data_description": "No raw-data retraining; merging operates on pretrained/fine-tuned weights. CMA-ES optimization used supervised target sets (samples from MMLU, GSM8K, MBPP, MBPP/MBPP-like coding samples) to choose layer coefficients.",
            "benchmark_name": "GSM8K, MATH, MMLU, HumanEval, InfiBench, GSM8K-COT, MATH-COT (evaluated comprehensively)",
            "task_type": "multi-step arithmetic word problems, algebra, and code generation for math problems (GSM8K-Coding, MATH-Coding)",
            "problem_format": "natural-language word problems, and prompts requesting Python code (execution checks used for coding tasks)",
            "difficulty_level": "varied: GSM8K (grade-level multi-step), MATH (harder multi-step math)",
            "prompting_method": "Evaluation used Chain-of-Thought prompting (GSM8K-COT, MATH-COT) and standard instruction templates; Task Arithmetic merging uses task vectors derived from fine-tuned weights.",
            "performance_metric": "accuracy on benchmarks and code-execution correctness; improvements reported qualitatively and flagged as &gt;5% (↑↑) or 1-5% (↑) relative to sources in tables.",
            "performance_value": "Merged models often outperform individual source models on integrated math+coding tasks; paper reports cases where merged model exceeds a source by &gt;=5% (marked ↑↑) or 1-5% (↑); also instances where merged model underperforms due to interference.",
            "internal_analysis": "Several analyses reported: (1) Table of True/False answer distributions showing merged model tends to be correct when both sources are correct and often correct when one math source is correct; (2) TIES-Merging analysis: high per-layer density of retained weights and sign conflicts can cause interference; (3) CMA-ES optimizes per-layer merging coefficients (every 5 or 10 layers share weights/density) using small target sets; (4) dataset-target mismatch (e.g., MBPP vs HumanEval) impacts outcomes.",
            "failure_modes": "Interference due to sign conflicts in TIES (parameter sign disagreement can degrade domain-specific performance); overfitting to small search datasets leading to poor generalization on other benchmarks; merged models can still fail when both sources fail; coding benchmarks (HumanEval/InfiBench) sometimes remain worse than source due to inadequate coverage in the target dataset.",
            "scaling_trend": "Paper reports combinatorial emergent capabilities (merged model acquires integrated reasoning beyond additive effects). Performance depends strongly on target dataset used for CMA-ES search and choice of pivot; using more math-optimized sources or larger/more diverse search data improves results.",
            "uuid": "e6406.3",
            "source_info": {
                "paper_title": "Unconstrained Model Merging for Enhanced LLM Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "DistFusion",
            "name_full": "Distribution-based fusion for heterogeneous LLMs (MinCE fusion + token alignment)",
            "brief_description": "A heterogeneous-model fusion pipeline that aligns instruction-response output distributions across models via token-alignment statistics and fuses them using a minimum cross-entropy (MinCE) fusion objective, followed by SFT with a combined loss.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Fusion models (pairwise heterogeneous fusions such as OpenMath-Mistral-7B + DeepSeek-Coder-Ins-v1.5, Qwen2.5-Math-7B-Ins + CodeLlama-70B-Ins, etc.)",
            "model_family": "heterogeneous decoder-only transformers (various sizes & tokenizers)",
            "model_size": "varied across pairs (7B up to 70B reported for sources)",
            "training_data_description": "Fusion uses instruction-response fine-tuning datasets: a curated subset (~60k samples: ~15k MetaMathQA + 15k DeepSeek coder samples plus other in-domain instruction data) to compute per-token output distributions and train fused targets via a combined SFT + fusion loss.",
            "benchmark_name": "GSM8K, MATH, MMLU, HumanEval, InfiBench (evaluated on same benchmarks as homogeneous experiments)",
            "task_type": "multi-step arithmetic/word problems and program synthesis for solving math tasks (GSM8K-Coding, MATH-Coding), plus coding QA",
            "problem_format": "natural-language word problems and instruction prompts for code generation; fusion uses the pivot model's conversation template for processing inputs",
            "difficulty_level": "varied; experiments include grade-school to advanced math (GSM8K, MATH subsets) and code evaluation suites (HumanEval/InfiBench)",
            "prompting_method": "Pivot-based conversation templates; SFT training with combined loss L = λ L_SFT + (1-λ) L_Fusion where L_Fusion encourages output distribution alignment to fused MinCE distributions; evaluation uses Chain-of-Thought where relevant.",
            "performance_metric": "accuracy on math benchmarks and code-execution correctness; comparisons vs pivot/source models shown in tables and figures",
            "performance_value": "Many fusion pairs outperform their pivot models on benchmarks; qualitative and comparative gains shown in figures/tables. Some fusions do not outperform pivot (noted cases), indicating dependence on pivot, conversation template, and fine-tuning balance.",
            "internal_analysis": "Key methodological analyses: (1) token alignment strategy using sequence-level dynamic programming to map tokens between tokenizers (handles 1:1, 1:n, n:1 mappings via global statistics); (2) MinCE fusion function to fuse aligned distributions; (3) observed that conversation template choice (pivot) and finetuning loss balance strongly affect fusion effectiveness; (4) behavioural observations showing fusion can transfer math reasoning into coding outputs (case studies).",
            "failure_modes": "Conversation-template mismatch (pivot template non-universal) can prevent gains; math skills are harder to transfer than coding; fusion can fail to outperform pivot in some pairs; when both source models fail, fusion usually fails though some exceptions occur.",
            "scaling_trend": "Using models optimized for more complex tasks (math) as pivots yields better fused performance; fine-tuning of the target model often dominates benefit from fusion; transferability depends on task difficulty and pivot choice.",
            "uuid": "e6406.4",
            "source_info": {
                "paper_title": "Unconstrained Model Merging for Enhanced LLM Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Chain-of-Thought (COT)",
            "name_full": "Chain-of-Thought prompting / evaluation",
            "brief_description": "A prompting/evaluation method where models produce intermediate reasoning steps (COT) and are evaluated on multi-step arithmetic/math benchmarks (GSM8K-COT, MATH-COT) to reveal reasoning behavior and accuracy.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Chain-of-Thought prompting (method applied to evaluated models)",
            "model_family": "prompting strategy (not a model)",
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "GSM8K-COT, MATH-COT (benchmarks where COT responses are elicited and scored)",
            "task_type": "multi-step arithmetic and math word problems requiring stepwise reasoning",
            "problem_format": "natural-language word problems eliciting step-by-step chain-of-thought",
            "difficulty_level": "multi-step problems where explicit intermediate steps help (GSM8K, MATH)",
            "prompting_method": "explicit Chain-of-Thought elicitation during evaluation",
            "performance_metric": "accuracy of final answer (and qualitative correctness of intermediate steps); code-execution for coding variants when COT leads to code output",
            "performance_value": "Used to compare source, merged, and fused models; merged/fused models often show preserved or improved COT performance on math tasks relative to coder-only sources (qualitative improvements illustrated in examples and plotted in figures).",
            "internal_analysis": "Paper uses COT outputs to qualitatively analyze whether merged/fused models inherit correct intermediate reasoning steps (case study examples show merged model often corrects coder's irrelevant steps). No mechanistic/neuron-level analysis of COT processing presented.",
            "failure_modes": "Coder models often produce irrelevant or incorrect chains; merged/fused models can still produce incorrect chains especially when both sources fail; COT performance sensitive to training/fine-tuning coverage.",
            "scaling_trend": "Merged/fused models can preserve or enhance COT performance, especially when one source specializes in math; effectiveness depends on source specialties and merging/fusion recipe.",
            "uuid": "e6406.5",
            "source_info": {
                "paper_title": "Unconstrained Model Merging for Enhanced LLM Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deepseek-coder: When the large language model meets programming - the rise of code intelligence",
            "rating": 2,
            "sanitized_title": "deepseekcoder_when_the_large_language_model_meets_programming_the_rise_of_code_intelligence"
        },
        {
            "paper_title": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models",
            "rating": 2,
            "sanitized_title": "deepseekmath_pushing_the_limits_of_mathematical_reasoning_in_open_language_models"
        },
        {
            "paper_title": "MetaMath: Bootstrap your own mathematical questions for large language models",
            "rating": 2,
            "sanitized_title": "metamath_bootstrap_your_own_mathematical_questions_for_large_language_models"
        },
        {
            "paper_title": "Ties-merging: Resolving interference when merging models",
            "rating": 2,
            "sanitized_title": "tiesmerging_resolving_interference_when_merging_models"
        },
        {
            "paper_title": "Editing models with task arithmetic",
            "rating": 2,
            "sanitized_title": "editing_models_with_task_arithmetic"
        },
        {
            "paper_title": "Knowledge fusion of large language models",
            "rating": 2,
            "sanitized_title": "knowledge_fusion_of_large_language_models"
        },
        {
            "paper_title": "Fusechat: Knowledge fusion of chat models",
            "rating": 2,
            "sanitized_title": "fusechat_knowledge_fusion_of_chat_models"
        },
        {
            "paper_title": "Model Soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
            "rating": 1,
            "sanitized_title": "model_soups_averaging_weights_of_multiple_finetuned_models_improves_accuracy_without_increasing_inference_time"
        },
        {
            "paper_title": "The cma evolution strategy: a comparing review",
            "rating": 1,
            "sanitized_title": "the_cma_evolution_strategy_a_comparing_review"
        },
        {
            "paper_title": "Infibench: Evaluating the question-answering capabilities of code llms",
            "rating": 2,
            "sanitized_title": "infibench_evaluating_the_questionanswering_capabilities_of_code_llms"
        }
    ],
    "cost": 0.016867,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Unconstrained Model Merging for Enhanced LLM Reasoning
21 Oct 2024</p>
<p>Yiming Zhang 
The Hong Kong Polytechnic University</p>
<p>Baoyi He 
Zhejiang University</p>
<p>Shengyu Zhang 
Zhejiang University</p>
<p>Yuhao Fu 
Qi Zhou 
Harbin Institute of Technology
Shenzhen</p>
<p>Zhijie Sang 
Reallm Labs</p>
<p>Zijin Hong 
The Hong Kong Polytechnic University</p>
<p>Kejing Yang 
Reallm Labs</p>
<p>Wenjun Wang 
South China University of Technology</p>
<p>Jianbo Yuan 
Guanghan Ning 
Linyi Li 
Simon Fraser University
7 Independent</p>
<p>Chunlin Ji 
Fei Wu 
Zhejiang University</p>
<p>Hongxia Yang hongxia.yang@polyu.edu.hk 
The Hong Kong Polytechnic University</p>
<p>Reallm Labs</p>
<p>Jacob Austin 
Augustus Odena 
Maxwell Nye 
Maarten Bosma 
Henryk Michalewski 
David Dohan 
Ellen Jiang 
Carrie Cai 
Michael Terry 
Xiao Bi 
Deli Chen 
Guanting Chen 
Shanhuang Chen 
Damai Dai 
Chengqi Deng 
Honghui Ding 
Kai Dong 
Mark Chen 
Jerry Tworek 
Heewoo Jun 
Qiming Yuan 
Henrique Ponde 
Oliveira Pinto 
Jared Ka- Plan 
Harri Edwards 
Yuri Burda 
Nicholas Joseph 
Greg Brockman 
Alex Ray 
Raul Puri 
Gretchen Krueger 
Michael Petrov 
Heidy Khlaaf 
Girish Sas- Try 
Pamela Mishkin 
Brooke Chan 
Scott Gray 
Nick Ryder 
Mikhail Pavlov 
Alethea Power 
Lukasz Kaiser 
Mohammad Bavarian 
Clemens Winter 
Philippe Tillet 
Felipe Petroski Such 
Dave Cum- Mings 
Matthias Plappert 
Fotios Chantzis 
Eliza- Beth Barnes 
Ariel Herbert-Voss 
William Hebgen Guss 
Alex Nichol 
Alex Paino 
Nikolas Tezak 
Jie Tang 
Igor Babuschkin 
Suchir Balaji 
Shantanu Jain 
William Saunders 
Christopher Hesse 
Andrew N Carr 
Jan Leike 
Josh Achiam 
Vedant Misra 
Evan Morikawa 
Alec Radford 
Matthew Knight 
Miles Brundage 
Mira Murati 
Katie Mayer 
Peter Welinder 
Bob Mcgrew 
Dario Amodei 
Sam Mccandlish 
Ilya Sutskever 
Wojciech 2021 Zaremba 
Evaluat 
Karl Cobbe 
Vineet Kosaraju 
Jacob Hilton 
Reiichiro Nakano 
Unconstrained Model Merging for Enhanced LLM Reasoning
21 Oct 202438819E0C73DC415ED79CDB087C4E035DarXiv:2410.13699v2[cs.CL]
Recent advancements in building domainspecific large language models (LLMs) have shown remarkable success, especially in tasks requiring reasoning abilities like logical inference over complex relationships and multi-step problem solving.However, creating a powerful all-in-one LLM remains challenging due to the need for proprietary data and vast computational resources.As a resource-friendly alternative, we explore the potential of merging multiple expert models into a single LLM.Existing studies on model merging mainly focus on generalist LLMs instead of domain experts, or the LLMs under the same architecture and size.In this work, we propose an unconstrained model merging framework that accommodates both homogeneous and heterogeneous model architectures with a focus on reasoning tasks.A fine-grained layer-wise weight merging strategy is designed for homogeneous models merging, while heterogeneous model merging is built upon the probabilistic distribution knowledge derived from instruction-response fine-tuning data.Across 7 benchmarks and 9 reasoning-optimized LLMs, we reveal key findings that combinatorial reasoning emerges from merging which surpasses simple additive effects.We propose that unconstrained model merging could serve as a foundation for decentralized LLMs, marking a notable progression from the existing centralized LLM framework.This evolution could enhance wider participation and stimulate additional advancement in the field of artificial intelligence, effectively addressing the constraints posed by centralized models.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have demonstrated remarkable capabilities, attributed to emergent abilities that arise with scale and diverse training data.Different organizations are developing their own LLMs, each excelling in specific reasoning tasks.In the context of LLMs, reasoning refers to the capability to perform logical inference, understand complex relationships, and solve problems that require multi-step thought processes, e.g., natural language reasoning, code programming, mathematical problem-solving with or without tools.However, applications in many other domains might require a combination of these abilities.For instance, in educational platforms or automated theorem proving, an LLM needs to comprehend complex mathematical concepts, reason through problems, and generate correct and efficient code solutions.An intuitive solution is to aggregate the datasets used to train these specialized LLMs and develop a more comprehensive, generalpurpose model.However, this approach could be more practical due to the enormous computational resources and human labor required.Moreover, the proprietary or sensitive nature of the data used to train individual models frequently restricts access, limiting the feasibility of data-centric methods.Therefore, there is a pressing need for efficiently building large models over off-the-shelf models (MoM), combining abilities without retraining or access to the original training data.Model merging has emerged as a promising avenue to address this challenge.Early approaches, such as weight averaging (Utans, 1996;Smith and Gashler, 2017), laid the foundation for techniques like Linear Mode Connectivity (LMC) (Garipov et al., 2018), which facilitates the merging of models trained from a common base.Methods like Model Soups (Wortsman et al., 2022) and Task Arithmetic (Ilharco et al., 2022) further exploit these principles by averaging weights or performing arithmetic operations on task-specific vectors.More advanced strategies such as TIES (Yadav et al., 2023) and Git-Rebasin (Ainsworth et al., 2022) address challenges related to permutation symmetries, enabling the alignment of models with Figure 1: The framework on unconstrained model merging.We first establish a robust evaluator and select the top-ranking domain-specific small models (DSSMs) with the strongest math or coding abilities.For unconstrained model merging, we apply the CMA-ES algorithm to search for optimal parameters for homogeneous models, and leverage instruction-response distribution based fusion for heterogeneous LLMs.</p>
<p>differing initializations.In parallel, model fusion techniques, such as CALM (Bansal et al., 2024) and FUSELLM (Wan et al., 2024a), explore crossattention mechanisms and probabilistic distribution alignment to integrate heterogeneous architectures.Despite their progress, the specific challenges of unconstrained model merging for enhancing LLM reasoning-particularly across text, mathematics, and code reasoning tasks and under varying architectures and model sizes-have not been thoroughly investigated, leaving a gap in understanding LLM reasoning enhancement under model merging without incurring significant resource costs.In this paper, we propose a comprehensive framework for unconstrained model merging (UMM) that accommodates both homogeneous and heterogeneous architectures, with a particular focus on reasoning tasks such as text, math, and code reasoning.For homogeneous LLMs, we perform parameter level merging of LLMs through two approaches, TIES-Merging and Task Arithmetic (Yadav et al., 2024;Ilharco et al., 2022).To merge heterogeneous LLMs with different architectures and sizes, we integrate their diverse knowledge through probabilistic distribution matrices derived from instruction-response pairs in the fine-tuning dataset.Through extensive evaluations across 7 benchmarks and 9 state-of-the-art reasoning-optimized LLMs, we have the following interesting findings:</p>
<p>• Merging mathematical and coding models enhances both mathematical and coding abilities beyond those of the original coding model.This potentially suggests that combinatorial capabil-ities-coding with mathematical thinking-are acquired through LLM merging, not merely the addition of individual skills.</p>
<p>• LLMs optimized for more complex tasks, such as mathematics over coding, as pivot models yields superior results.Additionally, if the pivot model is a pretrained model and the models to be merged with it are chat models, the fine-tuning process following the initial pretraining becomes crucial for optimal integration.</p>
<p>To summarize, this work makes the following key contributions:</p>
<p>• We propose to investigate LLM merging in unconstrained contexts, i.e., heterogeneous reasoning capabilities, model architectures, and sizes, for resouce-limited scenarios.</p>
<p>• We introduce the unconstrained model merging framework that integrates techniques such as evolutionary merging and probabilistic distribution knowledge-based fusion.</p>
<p>• We conduct extensive experiments, revealing the merits and limitations of unconstrained model merging across 7 benchmarks and 9 SOTA reasoning LLMs.</p>
<p>2 Related Work</p>
<p>Model Merging</p>
<p>Model merging integrates two or more pretrained models with similar architectures into a unified model (Ainsworth et al., 2022), leveraging the strengths of each original.This process is built on weight averaging techniques (Utans, 1996;Smith and Gashler, 2017) and utilizes mode connectivity in the loss landscape (Garipov et al., 2018).For example, Linear Mode Connectivity (LMC) allows for the merging of models that have been fine-tuned from a shared base model (Nagarajan and Kolter, 2019;Entezari et al., 2021;Neyshabur et al., 2021).Additionally, methods that use permutation symmetries and weight transformations align models within common regions of the loss landscape to enhance compatibility and performance (Ainsworth et al., 2022;Stoica et al., 2023;Verma and Elbayad, 2024).Simple weight averaging, exemplified by Model Soups (Wortsman et al., 2022), uses LMC to combine models with identical architectures and initializations.Task Arithmetic (Ilharco et al., 2022) introduces task vectors to manage differences between fine-tuned models and a common base, enabling precise model adjustments.Advanced approaches like TIES-Merging (Yadav et al., 2023), Model Breadcrumbs (Davari andBelilovsky, 2023), and DARE (Yu et al., 2023a) facilitate the sparsification and combination of task vectors, supporting the merging of a larger number of models with minimal capability loss.Git-Rebasin (Ainsworth et al., 2022) and Optimal Transport Fusion (Singh and Jaggi, 2020) apply permutation symmetries to align and merge models with different starting points.These methods have been extended to support Transformer-based architectures (Imfeld et al., 2023;Verma and Elbayad, 2024) and models trained on distinct tasks (Stoica et al., 2023), showcasing their flexibility and broad applicability in contemporary AI development.</p>
<p>Model Fusion</p>
<p>Model Fusion emphasizes integrating language models with diverse architectures to boost their collective capabilities.The Composition to Augment Language Models (CALM) (Bansal et al., 2024), utilizes cross-attention mechanisms to blend representations from various models, harmonizing different neural network structures to capitalize on their combined strengths.Similarly, FUSELLM (Wan et al., 2024a) focuses on aligning and merging the probabilistic distributions of source large language models to enrich the knowledge of the fused model.Extending to chat-based models, FUSECHAT (Wan et al., 2024b) introduces a twostage fuse-and-merge framework: initially, it conducts pairwise knowledge fusion of source chat models to create several target models with uniform structures and sizes; subsequently, it combines these models in the parameter space.Despite these advancements, there is a notable gap in detailed analysis of unconstrained model merging, particularly in reasoning tasks for LLMs, with few efforts addressing the fusion of models with heterogeneous architectures.</p>
<p>3 Modeling</p>
<p>Overview</p>
<p>We propose a comprehensive framework for unconstrained model merging that accommodates both homogeneous and heterogeneous architectures, with a particular focus on reasoning tasks such as text, math, and code reasoning.</p>
<p>Evolutionary Merging for Homogeneous LLMs</p>
<p>For homogeneous LLMs, we perform parameterlevel merging of LLMs through a fine-grained, layer-wise weight evolutionary merging approach that combines the merging recipes, like TIES-Merging and Task Arithmetic with an evolutionary algorithm (Yadav et al., 2024;Ilharco et al., 2022;Akiba et al., 2024;Goddard et al., 2024).Task Arithmetic improves models' performance by modifying and combining task vectors through arithmetic operations.Let θ pre be the weights of the base model, and θ t ft represent the weights after the base model fine tuned on the task t.The task vectors are defined as τ t = θ t ft − θ pre , indicating the direction of improvements from a base model to a fine-tuned model on the given task.The final weights of the merged model are computed using the weights of the base model plus a linear combination of task vectors θ Merge = θ pre + (λ * n i=1 τ i ).TIES-Merging constructs a merged model by resolving disagreements among task-specific models.Given multiple task vectors {τ t } n t=1 , where each τ t ∈ R d represents the parameter updates for task t, the method involves three steps: (1) Trim: Redundant parameters are trimmed by keeping the top k% values based on magnitude, creating τt .(2) Elect: An aggregate sign vector γ is formed by choosing the sign with the highest total magnitude across models for each parameter, computed as γ = sgn ( n t=1 τt ).</p>
<p>(3) Disjoint Merge: A disjoint mean for each parameter is computed by averaging over models that share the same sign as the aggregate sign, resulting in the creation of the merged task vector τ .We optimize the merging with the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) algorithm (Hansen, 2006), an evolutionary algorithm adept at handling high-dimensional, non-convex optimization problems common in neural network parameter spaces, guided by several task-specific metrics.CMA-ES optimizes the merging coefficients without relying on gradient information, making it suitable for complex, non-separable optimization problems.We define a set of merging coefficients {α l } L l=1 for each layer l of the LLM, where L is the total number of layers.These coefficients determine the weighted combination of the corresponding parameters from different models being merged.CMA-ES iteratively optimizes these coefficients by sampling candidate solutions from a multivariate normal distribution N m (g) , σ 2 (g) C (g) at each generation g, where m (g) is the mean vector of the coefficients, σ (g) is the global step size, and C (g) is the covariance matrix capturing the dependencies between coefficients.For each sampled set of coefficients {α l } L l=1 , we perform layer-wise merging of the models and evaluate the merged model's performance on selected tasks, guiding the evolutionary process.CMA-ES updates the mean m (g) , step size σ (g) , and covariance matrix C (g) based on the evaluated fitness, navigating the search space toward optimal merging coefficients.This process continues until convergence criteria are met and produces a merged model with the strengths of individual models while minimizing parameter conflicts.</p>
<p>Distribution Based Fusion for Heterogeneous LLMs</p>
<p>To merge heterogeneous LLMs with different architectures and sizes, inspired by previous works (Wan et al., 2024b,a),we integrate their diverse knowledge through probabilistic distribution matrices derived from instruction-response pairs (I i , R i ) in the fine-tuning dataset D. For each θ j , we compute a distribution matrix P θ j ,i ∈ R N ×V j , where N is the response length and V j is the vocabulary size of LLM θ j .We employ an enhanced token alignment strategy for large language models that utilizes mapping statistics derived from sequencelevel alignments via dynamic programming to accommodate varying tokenization schemes.This strategy captures 1 to 1, 1 to n, and n to 1 token mappings by constructing a global statistical matrix recording the frequency of mappings between pivot and source tokens.In the distribution dimension, we align tokens by selecting mappings with maximum frequency for 1 to 1 and 1 to n cases, and by computing weighted averages of source distributions for n to 1 cases, thereby preserving significant distributional information while minimizing misalignments.The aligned distributions from the pivot LLM θ v and each source LLM θ j are then fused using the minimum cross-entropy (MinCE) fusion function:
P i,j = Fusion P θv,i , P θ j ,i ,(1)
resulting in fused matrices {P i,j } that encapsulate the collective knowledge of the models.We fine-tune each target LLM M j,t by minimizing a combined loss function:
L = λL SFT + (1 − λ)L Fusion ,(2)
where L SFT is the standard supervised fine-tuning loss, and
L Fusion = −E (I i ,R i )∼D H P i,j ∥Q i,ϕ j(3)
encourages the target LLM's output distribution Q i,ϕ j to align with the fused distribution P i,j .</p>
<p>4 Experiments  (Hendrycks et al., 2021a).Source LLMs In the experiments of evolutionary merging, we employ a collection of source LLMs to create a model with improved reasoning capabilities.The source models include DeepSeek-Math-7B-RL (Shao et al., 2024) and DeepSeek-Coder-Instruct-v1.5 (Guo et al., 2024), which are derived from DeepSeek LLMs (Bi et al., 2024).</p>
<p>Evolutionary Search Evolutionary model merging is performed using the CMA-ES algorithm (Hansen, 2006) provided by Mergekit (Goddard et al., 2024;Akiba et al., 2024).This method provides an efficient way to find the optimal parameters of merge recipes given target datasets.Specifically, we utilize the CMA-ES algorithm on two merging approaches, Task Arithmetic and TIES-Merging (Yadav et al., 2024;Ilharco et al., 2022).</p>
<p>In Task Arithmetic, the parameters to be optimized are the weights of the task vectors and every 5 layers share the same weights.The target dataset includes 285 samples from MMLU, 300 samples from GSM8K and 374 samples from MBPP.For TIES-Merging, there is an additional parameter, namely density, which determines the fraction of parameters to retain from the source models.In this experiment, the target dataset includes 500 samples from GSM8K and 500 samples from MBPP.Every 10 layers share the same density and weights.</p>
<p>For both experiments, the optimal parameters is searched for 30 iterations based on their performance on the target dataset.</p>
<p>Evaluation We evaluate our models using the following benchmarks: MMLU, GSM8K and MATH are used to assess the models' mathematical reasoning abilities, while HumanEval (Chen et al., 2021), and InfiBench (InfiCoderTeam, 2024) are utilized to evaluate models' coding proficiency.Additionally, we instruct the models to generate code for solving mathematical problems from GSM8K and MATH to evaluate their integrated capabilities in both coding and mathematics.</p>
<p>Heterogeneous LLMs</p>
<p>Dataset In the fusion experiments, we primarily utilize a subset of 60k in-domain data, with half coming from math problem-solving and the other half from code generation tasks.Specifically, we randomly select 15K samples from MetaMathQA (Yu et al., 2023b) et al., 2023) and DeepSeek-Coder-Ins-v1.5 (Guo et al., 2024) are selected as source models, while in the math domain, Qwen2.5-Math-7B-Ins(Yang et al., 2024a;Team, 2024), WizardMath-7B-V1.1 (Luo et al., 2023), OpenMath-Mistral 7B (Toshniwal et al., 2024), MetaMath-7B and MetaMath-70B (Yu et al., 2023b) are selected.The sizes and structures of the source models are listed in Table 2.</p>
<p>Training We select 9 pairs for fusion, with each pair consisting of one math model and one coding model.In each pair, one model is designated as the pivot model, and its conversation template is used to process the input data.The batch size is set to 32 for obtaining model representations and reduced to 16 for aligning the representations.During fine-tuning, we use a learning rate of 5e-6 and apply the AdamW optimizer (Loshchilov and Hutter, 2019).To improve efficiency, we incorporate FlashAttention (Dao et al., 2022) for optimizing attention computation, reducing memory usage and enhancing computational speed.</p>
<p>Evaluation The same benchmarks as the aformentioned experiment in Homogeneous LLMs (Section 4.1.1)are used for evaluation.</p>
<p>Results</p>
<p>Homogeneous LLM</p>
<p>We use seven benchmarks to assess the models' capabilities in general reasoning, mathematics, coding, and the integration of math and coding.Notably, in the GSM8K-Coding and MATH-Coding benchmarks, the models are required to write Python code to solve math problems, and we assume this can demonstrate their integrated proficiency in both domains.In the TIES-Merging method, the direction with the largest magnitude is selected for each parameter to determine the merged sign, and only parameters matching the merged sign are averaged.Both mathematical and coding capabilities involve reasoning   (Yang et al., 2024b;Yu et al., 2023b;Toshniwal et al., 2024;Guo et al., 2024;Shao et al., 2024;Luo et al., 2023;Roziere et al., 2023).GSM8k-coding and MATH-coding are evaluated using the evaluation framework provided by DeepSeek-Coder (Guo et al., 2024).The other scores shown in the table without parentheses are evaluated via OpenCompass (Contributors, 2023).GSM8K-COT and MATH-COT refer to scores derived using Chain-of-Thought reasoning in the GSM8k and MATH datasets, while GSM8K-Coding and MATH-Coding results are obtained by executing the model's generated code in a Python interpreter.InfiBench (InfiCoderTeam, 2024) is a code benchmark for evaluating question-answering (QA) abilities 10 .Models that exceed the performance of one of their corresponding source models by five or more percents are indicated with (↑↑).Those that outperform by one to less than 5% are denoted with (↑).Those that underperform both source models by more than 1% are marked with (↓).Differences between merged or fused models and any one of the source models that are within 1% are indicated with (-).skills and share some overlap.However, during the TIES-Merging process, the two models may exhibit divergent merged signs for certain parameters, requiring a selection between them.This choice could degrade the performance in one domains, which may explain why the merged model underperforms compared to DeepSeek-Coder-Instruct-v1.5 and DeepSeek-Math-7B-RL in their respective domains.Table 3 further supports this, showing that the density of the two source models in each layer is very high.This suggests that the majority of the source models' weights are retained, likely leading to interference from sign conflicts .</p>
<p>In addition, the dataset used to search optimal merging parameters with the CMA-ES algorithm could also impact the performance of the merged model.There is a gap between the target dataset, HumanEval and InfiBench in terms of the topic content and the format.Due to the absence of a training set in the HumanEval dataset, we use the MBPP dataset as an alternative for adapting coding abilities of the merged model.Furthermore, the target dataset lacks diverse coding-related questions compared with InfiBench.This could explain why the performance on Humaneval and InfiBench still falls short of the source models.Employing more diverse data in the target dataset could potentially improve the model's overall capabilities.Our Task Arithmetic experiment has an additional MMLU dataset in the target dataset compared to the TIES-Merging experiment, but the MMLU score has hardly improved.We infer that this is because we only sampled 285 samples from MMLU, which may be insufficient.MMLU contains a wide range of knowledge, and more samples are needed to ensure that these knowledge are covered.</p>
<p>Heterogeneous LLMs</p>
<p>We evaluate the abilities by benchmarks: MMLU, GSM8K, MATH, HumanEval and InfiBench, and the evaluation results of the source models and the fused models are in Table 2.It is obvious that the overall performances of most fusion models are better than the pivot models which can be demonstrated by the higher scores of the benchmarks as shown in Figure 6 We have observed that using LLMs optimized for more complex tasks, such as mathematics over coding, as pivot models yields superior results.For example, the scores of the fusion model OpenMath-Mistral 7B + DeepSeek-Coder-Ins-v1.5 are far better than those of the pivot model OpenMath-Mistral 7B, while the score difference between the fusion model CodeLlama-7B-Ins + Qwen2.5-Math-7B-Ins and its pivot model CodeLlama-7B-Ins is not that large.We assume this is because the math capability is more difficult than the coding capability to transfer from the source models to the fusion models, which may be an important guidance for fusion.</p>
<p>The fusion model Qwen2.5-Math-7B-Ins+ CodeLlama-70B-Ins does not outperform the pivot model Qwen2.5-Math-7B-Ins,potentially due to the conversation template's lack of universality.Future work will focus on designing a more universally applicable conversation template to enhance fusion effectiveness.Similarly, when OpenMath-Mistral 7B is the pivot model, the performance differences between the fusion models OpenMath-Mistral 7B + CodeLlama-70B-Ins and OpenMath-Mistral 7B + Deepseeker-Coder-Ins-v1.5 are minimal.This trend is also observed when CodeLlama-7B-Ins is the pivot model, suggesting that finetuning significantly influences model performance, often overshadowing the benefits of fusion with different source models.Balancing fine-tuning loss and fusion loss will be a key area of our research going forward.</p>
<p>Conclusion and Future Work</p>
<p>In this work, we propose to explore unconstrained model merging particularly for LLM reasoning enhancement.Our approach leverages evolutionary merging for homogeneous models and probabilistic distribution knowledge-based fusion for heterogeneous models.Through comprehensive experimentation, we demonstrated unconstrained model merging's ability to not only improve individual reasoning capabilities from the source models but also enable novel combinatorial reasoning capabilities, such as applying mathematical thinking to improve coding, yielding performance gains across all source models.This research represents one of the initial explorations in the "Model over Model" (MoM) paradigm, where minimal constraints are imposed on constructing new, integrated models without incurring prohibitive training costs.We believe the insights from this work could serve as a foundation for future innovations in this area.</p>
<p>Moving forward, several promising research directions emerge, including a deeper investigation into unconstrained model merging's effects on finegrained reasoning behaviors, such as COT and instruction-following behaviors.Furthermore, we aim to explore unconstrained model merging's applicability to broader LLM capabilities, such as planning and decision-making.We posit that unconstrained model merging will lay the groundwork for decentralized LLMs, marking a substantial progression from the existing centralized LLM paradigm.This evolution is anticipated to enhance participation and spur further advancements in artificial intelligence, overcoming the restrictions associated with centralized models.</p>
<p>Limitations</p>
<p>Due to the constraints of computational resources and time, our experiments were primarily conducted within standard benchmarks.Although the results are promising, the full extent of the method's generalizability and robustness in realworld scenarios remains to be further explored.Importantly, our research adheres to ethical standards and does not present any societal or security concerns, as it focuses solely on the technical dimensions of a conventional information extraction task without involving any sensitive data or applications.This limitation opens up avenues for future studies to validate and extend the method's applicability under diverse and more complex conditions.Table 4: Evolution: True/False distribution of models' answers.The table reveals that when both source models answer a question correctly, the merged model is also likely to provide an accurate solution.For questions where one source model answers correctly and the other does not, the merged model has a higher probability of producing a correct answer in math tasks; however, this pattern does not hold for HumanEval.On the other hand, if both source models fail to answer a question correctly, their merged version is also unlikely to produce the correct answer, though a few exceptions exist.CodeLlama-7B-Ins [Wrong, Wrong] ID 3: James decides to run 3 sprints 3 times a week.He runs 60 meters each sprint.How many total meters does he run a week?</p>
<p>A Case Study</p>
<p>James runs 3 sprints per week, and each sprint is 60 meters long.So, he runs a total of 3 x 60 = 180 meters per week.The answer is 180 meters.</p>
<p>ID 1192: Jen works for 7.5 hours a day 6 days a week.Her hourly rate is $1.5.Jen also receives an additional $10 if she has complete attendance.Suppose Jen did not incur any absences for April, and there are exactly 4 weeks in April, how much will she receive?</p>
<p>Jen works for 7.5 hours a day, 6 days a week, for a total of 7.5 hours per week.She works for 4 weeks in April, which is 4 x 7.5 = 30 hours in April.Her total hours worked in April is 30 hours.Her total earnings in April is 30 hours x $1.50/hour = $45.00.Since she has complete attendance in April, she will receive an additional $10, so her total earnings in April is $45.00 + $10 = $55.00.The answer is $55.00 .</p>
<p>MetaMath-70B [Correct, Wrong] James runs 3 sprints 3 times a week, so he runs 3<em>3=9 sprints a week.Each sprint is 60 meters, so he runs 9</em>60=540 meters a week.The answer is 540 .</p>
<p>Jen works for 7.5 hours a day 6 days a week, so she works a total of 7.5<em>6=45 hours in a week.Her hourly rate is $1.5, so she earns 45</em>$1.5=$67.5 in a week.Jen also receives an additional $10 if she has complete attendance, so she will receive an additional $10 for each week she has complete attendance.Since there are exactly 4 weeks in April, Jen will receive an additional $10*4=$40 for complete attendance.In total, Jen will receive $67.5+$40=$107.5 for the month of April.The answer is 107.5</p>
<p>Fusion Model [Correct, Correct] James runs 3 sprints 3 times a week, so he runs a total of 3<em>3=9 sprints per week.Each sprint is 60 meters long, so James runs a total of 9</em>60=540 meters per week.The answer is 540 .</p>
<p>Jen works for 7.5 hours a day, so she works for 7.5<em>6 = 45 hours in a week.There are 4 weeks in April, so Jen works for 45</em>4 = 180 hours in April.Jen's hourly rate is $1.5, so she earns $1.5/hour.Jen works for 180 hours in April, so she earns $1.5/hour*180 = $270 in April.Jen also receives an additional $10 if she has complete attendance, so she earns $270 + $10 = $280 in April.The answer is $280 .The smallest possible value of 3 √ ax is the smallest possible value of a, which is 1.This is because any positive integer multiplied by 720 will be a perfect cube, and the cube root of 1 is 1.So, the smallest possible value of 3 √ ax is 1 .Final Answer: The final answer is 1.I hope it is correct.</p>
<p>DeepSeek-Math-RL [Correct]</p>
<p>If First, we need to find the prime factorization of 720.The prime factorization of 720 is 2 4 • 3 2 • 5 1 .Now, in order for ax to be a perfect cube, the exponents of all prime factors in the prime factorization of ax must be multiples of 3. Therefore, we need to multiply the prime factorization of 720 by another factor of 2 2 • 5 2 to make the exponents of all prime factors multiples of 3.This gives us
2 4 • 3 2 • 5 1 • 2 2 • 5 2 = 2 6 • 3 2 • 5 3
. Now, we can find ax by multiplying the prime factorization together: ax = 2 6 • 3 2 • 5 3 .Finally, we can find 3 √ ax by taking the cube root of ax: Charlie can make 10 tapered candles from one pound of beeswax.If he makes 20 candles, he needs 20/10 = 2 pounds of beeswax.The cost of the beeswax and wicks for 2 pounds is $10.00.Charlie sells each candle for $2.00.So, the total revenue from selling 20 candles is 20 * $2.00 = $40.00.His net profit is the total revenue minus the cost of the beeswax and wicks.So, his net profit is $40.00 -$10.00 = $30.00.
3 √ ax = 3 √ 2 6 • 3 2 • 5 3 = 2 2 • 3 • 5 = 60.
The answer is $30.00 .</p>
<p>ID 45: Alice is given the numbers 2, 3, 5, 7, and 9.She must choose a pair of these digits, with the digits being different, to form a two-digit number.How many of the numbers she can construct will be divisible by 3?</p>
<p>A number is divisible by 3 if the sum of its digits is divisible by 3. The numbers Alice can construct are 23,25,27,29,35,37,39,57,59,79.</p>
<p>The sum of the digits of these numbers are 5,7,10,11,8,10,12,12,14,16.Only the numbers 23, 35, 37, 57, 79 have a sum divisible by 3, so Alice can form 5 numbers divisible by 3. Final Answer:</p>
<p>The final answer is 5.I hope it is correct.</p>
<p>DeepSeek-Math-RL [Correct]</p>
<p>Charlie can make 10 candles from one pound of beeswax.If he makes 20 candles, he will need 20/10 = 2 pounds of beeswax.The cost of the beeswax and wicks is $10.00 per pound, and he needs 2 pounds, so the total cost is 2 * $10 = $20.If he sells each candle for $2.00, and he sells 20 candles, his total revenue is 20 * $2 = $40.To find his net profit, we subtract the cost from the revenue: $40 -$20 = $20.His net profit is $20.The answer is 20 .</p>
<p>A number is divisible by 3 if and only if the sum of its digits is divisible by 3. The sum of the digits 2, 3, 5, 7, and 9 is 26, which is not divisible by 3.However, if we choose a pair of digits from these five, the sum of the digits of the resulting two-digit number will be the sum of the digits.To find the number of two-digit numbers that are divisible by 3, we need to count the number of pairs of digits whose sum is divisible by 3. The pairs of digits with sum divisible by 3 are: (2, 9), (3, 6), (4, 5), (5, 4), (6, 3), and (9, 2).There are 6 pairs of digits whose sum is divisible by 3. Therefore, Alice can construct 6 of the numbers she can construct will be divisible by 3. The answer is: 6</p>
<p>Merged Model [Correct] Charlie can make 10 candles from one pound of beeswax.If he makes 20 candles, he will need 20/10 = 2 pounds of beeswax.The cost of the beeswax and wicks is $10.00 for each pound, so for 2 pounds, it will cost him $10 * 2 = $20.00.He sells each candle for $2.00, so for 20 candles, he will make $2 * 20 = $40.00.To find his net profit, we need to subtract the cost of supplies from the total sales.So his net profit is 40−20 = $20.00 .</p>
<p>A number is divisible by 3 if and only if the sum of its digits is divisible by 3.So we need to find all pairs of digits whose sum is divisible by 3. The pairs whose sum is divisible by 3 are: (2, 7), (3, 6), (5, 8), (7, 2), (6, 3), (8, 5).So there are 6 pairs that can form a number divisible by 3. Therefore, Alice can construct 6 numbers that are divisible by 3. The answer is 6 .The answer is 6 .</p>
<p>Let's start by using the information that there are 3 times as many girls as boys.So, if there are x boys, there are 3x girls.We also know that there are 1/10 as many nongendered children as boys.So, if there are x boys, there are 10x nongendered children.We are given that there are 30 boys in the classroom.The answer is 123 .</p>
<p>Figure 2 :Figure 3 :
23
Figure 2: The accuracy of the source models and fusion models in solving problems of different math levels.</p>
<p>Figure 4 :
4
Figure 4: The performance of the fusion model on the MATH across different types and difficulty levels.</p>
<p>Figure 5 :
5
Figure 5: The Performance of COT reasoning on MATH topics and difficulty levels.</p>
<p>Figure 6 :
6
Figure 6: Comparison of scores between fusion models and source models in GSM8K-COT and GSM8K-Coding.</p>
<p>Figure 7 :
7
Figure 7: Comparison of scores between fusion models and source models in MATH-COT and MATH-Coding.</p>
<p>Figure 8 :
8
Figure 8: Comparison of scores between fusion models and source models in HumanEval.</p>
<p>Figure 9 :
9
Figure 9: Comparison of scores between fusion models and source models in InfiBench.</p>
<p>So, we can set up the equation: 3x + 10x = 30 Simplifying the equation: 4x = 30 Dividing both sides by 4: x = 7.5 So, there are 7.5 boys and 7.5 x 10 = 75 nongendered children in the classroom.Now, we can find the number of girls by multiplying 3x by 3: 3x = 21 So, there are 21 girls in the classroom.The total number of children in the classroom is the sum of the number of boys, girls, and nongendered children: 30 + 21 + 75 = 126 Therefore, the classroom has 126 children.WizardMath-7B-V1.1 [Wrong, Correct]When James walked halfway to the store, he covered a distance of 4/2 = 2 miles He then walked back 2 miles to get his forgotten item, so he walked 2+2 = 4 miles total.James walks at a rate of 4 miles per hour, so it took him 4/4 = 1 hour to walk that distance.The answer is 1 .The classroom has 3 times as many girls as boys, meaning there are 3<em>30=90 girls in the classroom.The number of nongendered children is 1/10 the number of boys, meaning there are 1/10</em>30 = 3 nongendered children.In total, the classroom has 90 girls + 30 boys + 3 nongendered children = 123 children The answer is 123 .Fusion Model[Correct, Correct]   James walked 4 miles to reach the store, so he walked 4 miles / 4 miles per hour = 1 hour to reach the store.He then realized he forgot something at home and had to walk back, so he walked back 4 miles / 4 miles per hour = 1 hour to walk back.In total, it took James 1 hour to reach the store and 1 hour to walk back, so it took him 2 hours to complete the trip.The answer is 2 hours.Let's break down the information given: Number of boys: 30 Number of girls: 3 times as many as boys = 30 * 3 = 90 Number of nongendered children: 1/10 as many as boys = 30 * 1/10 = 3 Total number of children: Number of boys + Number of girls + Number of nongendered children = 30 + 90 + 3 = 123</p>
<p>Table 2
2presents the bench-</p>
<p>Table 1 :
1
The details of dataset selection in Fusion, includes the original sizes of varied datasets and the sample sizes we selected.
TypesGeneral DataMath DataCode DataDatasetOrca-Best 5 Capybara 6 No-Robots 7 ShareGPT-GPT4 8 Oasst Top1 9 MetaMathQA MMIQC OSS-Instruct Evol-Alpaca Python-CodeOriginal Size329K16K9.5K103K12.9K395K2.29M75K111K23KSample Size10K10K9.3K6.5K4.7K30K30K10K10K10K</p>
<p>Table 2 :
2
Benchmarks of source LLMs and merged LLMs with different merging strategies.Scores in parentheses are from the original papers</p>
<p>Table 3 :
3
Parameters in the TIES-Merging.Every 10 layers share the same density and weights.In the bracket, the first value is density, representing the fraction of weights of source models that are remained, and the second value is weights of source models contributing to the merged model.The sum of the weights are not 1, but they are normalized when merging.
Model#Layer 0-10 #Layer 10-20 #Layer 20-30DeepSeek-Coder-Ins-v1.5 [0.90, 0.45][0.73, 0.083][1.0, 0.52]DeepSeek-Math-7B-RL[1.0, 0.58][1.0, 0.78][1.0, 0.78]</p>
<p>Table 5 :
5
Case study: Fusion results (CodeLlama-7B-Ins + MetaMath-70B) in GSM8K-COT In question 3, the CodeLlama-7B-Ins model failed to correctly understand the problem and produced an incorrect answer, while MetaMath-70B model answered it correctly.The fusion model successfully learned from MetaMath-70B model and also provided the correct answer.In question 1192, both source models answered incorrectly, yet the fusion model was able to produce the correct solution.
DeepSeek-Coder-Ins-v1.5 DeepSeek-Math-RL Merged Model GSM8K-COT GSM8K-Coding MATH-COT MATH-Coding HumanEvalTrueTrueTrue692812335100150TrueTrueFalse308318917418TrueFalseTrue526249118TrueFalseFalse16466619030FalseTrueTrue36314410715493FalseTrueFalse78598283604FalseFalseTrue25333062033FalseFalseFalse1101162181243238Total1319131950005000164ModelGSM8k-COTGSM8K-COTQuestionAnswerQuestionAnswer</p>
<p>Table 6 :
6
Case study: Evolution results in GSM8K-COT and Math-COT.The merged model can reserve mathematical reasoning abilities after merging and correct wrong thoughts in the coder model.For instance, in question 24 (GSM8K), the coder model fails to address the problem, providing reasoning steps that are entirely irrelevant.However, after merging, the model successfully solves the problem by applying the correct equations.The similar finding also holds for question 44 (GSM8K), where the coder model makes a mistake in calculating costs for two pounds of beeswas, but the merged model correct this issue.This phenomenon becomes more obvious in MATH benchmark, since MATH task can more easily reveals models' math capability.In the case of question 2, the coder model clearly lacks an understanding of the concept of the perfect cube, wherease the merged model inherited from the math model can point out that the exponents of all prime factors in the prime factorization of ax must be multiples of 3, which is a critical point for solving this problem.The same happens to question 45.Though the coder model knows a number is divisible by if and only if the sum of its digits is divisible by 3, but it calculates wrongly the sum of 23, 35, 37 and 79.Compared with the answer from the math model, which is relatively redundant, the merge model seems to combine advantages of the source models, and output a precise and accurate solution.To find the original price of the book, we need to calculate the price before the discount.This can be done by dividing the discounted price by the discount rate.The discount rate is 25% or 0.25 in decimal form.So, the original price is the discounted price divided by the discount rate.The original price of the book is $19.50 / 0.25 = $78.So, the original price of the book was $78 .
ModelGSM8k-COTMath-COTQuestionAnswerQuestionAnswerDeepSeek-Coder-Ins-v1.5 [Wrong] ID 24: Kyle bought last year'sID 2: If x = 720 and ax is abest-selling book for $19.50. This isperfect cube where a is awith a 25% discount from the original price. What was the original price ofpositive integer, what is the smallest possible value of 3 √ ax?the book?</p>
<p>the discounted price is $19.50 and this is 75% of the original price (100% -25% discount), we can set up the equation: 0.75 * original price = $19.50 To find the original price, we can divide both sides of the equation by 0.75: original price = $19.50 / 0.75 = $26 So the original price of the book was $26.The answer is 26 First, we need to factorize 720.The prime factorization of 720 is 2 4 • 3 2 • 5. To make ax a perfect cube, we need to multiply 720 by a number that makes all the exponents in the prime factorization a multiple of 3. The smallest such number is 2 • 3 • 5 = 30.So, let a = 30.Then, ax = 30 • 720 = 21600.The cube root of 21600 is 3
√ 21600 = 3 √ 2 4 • 3 3 • 5 3 = 60. Therefore, the smallest possible value of 3 √ ax is 60.The answer is: 60Merged Model [Correct]Let's say the original price of the book is x. The bookhas a 25% discount, which means Kyle paid 75% ofthe original price. So, 0.75x = $19.50. To find theoriginal price, we need to solve for x. To do this, wedivide both sides of the equation by 0.75: x = $19.50/ 0.75 = $26. So, the original price of the book was$26. The answer is $26 .
We avoid using the test sets of the benchmarks during evolutionary search to prevent overfitting.
https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K
https://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1
https://huggingface.co/datasets/ajibawa-2023/Python-Code-23k-ShareGPT
https://huggingface.co/datasets/shahules786/orca-best
https://huggingface.co/datasets/LDJnr/Capybara
https://huggingface.co/datasets/HuggingFaceH4/no_robots
https://huggingface.co/datasets/shibing624/sharegpt_gpt4
https://huggingface.co/datasets/OpenAssistant/oasst_top1_2023-08-25
InfiBench's evaluation criteria include keyword matching, meaning that models with weak coding capabilities can still score points if their generated code contains some of the keywords found in the ground-truth labels.
</llm-code> <llm-code-output> 77.5 </llm-code-output> If Allen will be 10 years older in 10 years, his age will be 77.5 + 10 = 87.5So Allen will be 88 years old in 10 years.Jill gets paid $20 per hour to teach and $30 to be a cheerleading coach.If she works 50 weeks a year, 35 hours a week as a teacher and 15 hours a week as a coach, what's her annual salary?Jill works 50 weeks a year, 35 hours a week as a teacher, and 15 hours a week as a coach.She also gets paid $20 per hour to teach and $30 to be a cheerleading coach.We need to multiply each amount by the number of hours worked to get the annual salary.</llm-code> <llm-code-output> 14600 </llm-code-output> Therefore, Jill's annual salary is 14600 .Correct]First, we need to find out how oldPython code: """ Given a positive integer n, return the count of the numbers of n-digit positive integers that start or end with 1. """ Python code: def starts_one_ends(n):""" Given a positive integer n, return the count of the numbers of n-digit positive integers that start or end with 1. """ # The total number of n-digit numbers is 10^n -10^(n-1) total_numbers = 10<strong>n -10</strong>(n-1) # The number of n-digit numbers that start with 1 is 10^(n-1) starts_with_one = 10<strong>(n-1) # The number of n-digit numbers that end with 1 is 10^(n-1) ends_with_one = 10</strong>(n-1) # The number of n-digit numbers that start and end with 1 is 10^(n-2) starts_and_ends_with_one = 10**(n-2) # The total number of n-digit numbers that start or end with 1 is the sum of the above total_starts_or_ends_with_one = starts_with_one + ends_with_one -starts_and_ends_with_one return total_starts_or_ends_with_onePython prompt:def exchange(lst1, lst2): """ In this problem, you will implement a function that takes two lists of numbers, and determines whether it is possible to perform an exchange of elements between them to make lst1 a list of only even numbers.There is no limit on the number of exchanged elements between lst1 and lst2.If it is possible to exchange elements between the lst1 and lst2 to make all the elements of lst1 to be even, return "YES".Otherwise, return "NO".For example: exchange([1, 2, 3, 4],[1,2,3,4]) =&gt; "YES" exchange([1, 2, 3, 4],[1,5,3,4]) =&gt; "NO"It is assumed that the input lists will be non-empty.""" Given an array of integers nums, find the minimum sum of any non-empty sub-array of nums.Example minSubArraySum([2, 3, 4, 1, 2, 4]) == 1 minSubArraySum([-1, -2, -3]) == -6 """ Python code:
Jonathan Samuel K Ainsworth, Siddhartha Hayase, Srinivasa, Git re-basin: Merging models. 2022</p>
<p>Flashattention: Fast and memory-efficient exact attention with io-awareness. Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, Christopher Ré, arXiv:2205.14135arXiv:2312.06795MohammadReza Davari and Eugene Belilovsky. 2023. Model breadcrumbs: Scaling multi-task model merging with sparse masks. 2022arXiv preprint</p>
<p>The role of permutation invariance in linear mode connectivity of neural networks. Rahim Entezari, Hanie Sedghi, Olga Saukh, Behnam Neyshabur, arXiv:2110.062962021arXiv preprint</p>
<p>Loss surfaces, mode connectivity, and fast ensembling of dnns. Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, Andrew G Wilson, Advances in neural information processing systems. 201831</p>
<p>Arcee's mergekit: A toolkit for merging large language models. Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vlad Karpukhin, Brian Benedict, Mark Mcquade, Jacob Solawetz, arXiv:2403.132572024arXiv preprint</p>
<p>. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y K Li, Y Wu, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. 2024. Deepseek-coder: When the large language model meets programming -the rise of code intelligence</p>
<p>Towards a new evolutionary computation: Advances in the estimation of distribution algorithms. Nikolaus Hansen, 2006The cma evolution strategy: a comparing review</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.033002021aPreprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021bNeurIPS</p>
<p>Next-generation database interfaces: A survey of llmbased text-to-sql. Zijin Hong, Zheng Yuan, Qinggang Zhang, Hao Chen, Junnan Dong, Feiran Huang, Xiao Huang, arXiv:2406.084262024arXiv preprint</p>
<p>Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, Ali Farhadi, arXiv:2212.04089Editing models with task arithmetic. 2022arXiv preprint</p>
<p>Sotiris Anagnostidis, and Sidak Pal Singh. Jacopo Moritz Imfeld, Marco Graldi, Thomas Giordano, Hofmann, arXiv:2310.05719Transformer fusion with optimal transport. 2023arXiv preprint</p>
<p>Infibench: Evaluating the question-answering capabilities of code llms. Inficoderteam, 2024</p>
<p>Haoxiong Liu, Yifan Zhang, Yifan Luo, Andrew Chi-Chih Yao, arXiv:2401.09003Augmenting math word problems via iterative question Preprint. 2024</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, arXiv:1711.051012019Preprint</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Dongmei Zhang, arXiv:2308.09583Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. 2023arXiv preprint</p>
<p>Uniform convergence may be unable to explain generalization in deep learning. Vaishnavh Nagarajan, Kolter Zico, Advances in Neural Information Processing Systems. 201932</p>
<p>Hanie Behnam Neyshabur, Chiyuan Sedghi, Zhang, arXiv:2008.11687What is being transferred in transfer learning?. 2021arXiv preprint</p>
<p>Jonas Baptiste Roziere, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Romain Liu, Tal Sauvestre, Remez, arXiv:2308.12950Code llama: Open foundation models for code. 2023arXiv preprint</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y K Li, Y Wu, Daya Guo, Deepseekmath: Pushing the limits of mathematical reasoning in open language models. 2024</p>
<p>Model fusion via optimal transport. Pal Sidak, Martin Singh, Jaggi, Advances in Neural Information Processing Systems. 202033</p>
<p>An investigation of how neural networks learn from the experiences of peers through periodic weight averaging. Joshua Smith, Michael Gashler, 2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA). IEEE2017</p>
<p>Zipit! merging models from different tasks without training. George Stoica, Daniel Bolya, Jakob Bjorner, Taylor Hearn, Judy Hoffman, arXiv:2305.030532023arXiv preprint</p>
<p>Qwen Team. 2024. Qwen2.5: A party of foundation models. </p>
<p>Openmathinstruct-1: A 1.8 million math instruction tuning dataset. Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, Igor Gitman, arXiv: Arxiv- 2402.101762024arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov ; Zheng Yan, Iliyan Zarov, Yuchen Zhang, arXiv:2307.09288Pushkar Mishra, Igor Molybog. Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurelien RodriguezPreprintand Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models</p>
<p>Weight averaging for neural networks and local resampling schemes. Joachim Utans, Proc. AAAI-96 Workshop on Integrating Multiple Learned Models. AAAI-96 Workshop on Integrating Multiple Learned ModelsCiteseerAAAI Press1996</p>
<p>Merging text transformer models from different initializations. Neha Verma, Maha Elbayad, arXiv:2403.009862024arXiv preprint</p>
<p>Knowledge fusion of large language models. Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, Shuming Shi, arXiv:2401.104912024aarXiv preprint</p>
<p>Fanqi Wan, Ziyi Yang, Longguang Zhong, Xiaojun Quan, Xinting Huang, Wei Bi, arXiv:2402.16107Fusechat: Knowledge fusion of chat models. 2024barXiv preprint</p>
<p>Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. Mitchell Wortsman, Gabriel Ilharco, Ya Samir, Rebecca Gadre, Raphael Roelofs, Ari S Gontijo-Lopes, Hongseok Morcos, Ali Namkoong, Yair Farhadi, Simon Carmon, Kornblith, International Conference on Machine Learning. PMLR2022</p>
<p>Prateek Yadav, Derek Tam, arXiv:2306.01708Leshem Choshen, Colin Raffel, and Mohit Bansal. 2023. Resolving interference when merging models. arXiv preprint</p>
<p>Ties-merging: Resolving interference when merging models. Advances in Neural Information Processing Systems. Prateek Yadav, Derek Tam, Leshem Choshen, Colin A Raffel, Mohit Bansal, ; Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, arXiv:2407.10671Qwen2 technical report. 2024. 2024a36arXiv preprint</p>
<p>An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Lin, arXiv:2409.12122Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. 2024barXiv preprint</p>
<p>Language models are super mario: Absorbing abilities from homologous models as a free lunch. Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, Yongbin Li, arXiv:2311.030992023aarXiv preprint</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, arXiv:2309.12284Metamath: Bootstrap your own mathematical questions for large language models. 2023barXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
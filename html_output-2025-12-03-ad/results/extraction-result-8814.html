<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8814 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8814</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8814</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-ce07df583f7a0e975175c4efc8b93a436376fef8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ce07df583f7a0e975175c4efc8b93a436376fef8" target="_blank">Enhancing AMR-to-Text Generation with Dual Graph Representations</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> A novel graph-to-sequence model that encodes different but complementary perspectives of the structural information contained in the AMR graph, learning parallel top-down and bottom-up representations of nodes capturing contrasting views of the graph.</p>
                <p><strong>Paper Abstract:</strong> Generating text from graph-based data, such as Abstract Meaning Representation (AMR), is a challenging task due to the inherent difficulty in how to properly encode the structure of a graph with labeled edges. To address this difficulty, we propose a novel graph-to-sequence model that encodes different but complementary perspectives of the structural information contained in the AMR graph. The model learns parallel top-down and bottom-up representations of nodes capturing contrasting views of the graph. We also investigate the use of different node message passing strategies, employing different state-of-the-art graph encoders to compute node representations based on incoming and outgoing perspectives. In our experiments, we demonstrate that the dual graph representation leads to improvements in AMR-to-text generation, achieving state-of-the-art results on two AMR datasets</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8814.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8814.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dual TD/BU representation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dual Top-Down and Bottom-Up Graph Representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representation that encodes each AMR graph as two complementary directed views — a top-down (TD) view and a bottom-up (BU) view — and learns separate node embeddings for each view which are concatenated to form final node representations used for sequence decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Dual TD/BU Graph Representation (Dual Graph Encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>The input AMR graph is converted into two directed views G^t (top-down) and G^b (bottom-up, obtained by reversing the edges of G^t). Each node in the transformed graph is embedded and encoded separately by two graph encoders (GE^t and GE^b) that compute h_i^t and h_i^b respectively; these are concatenated with the node embedding e_i to produce the final node vector r_i = [h_i^t || h_i^b || e_i]. The node representations are then ordered (depth-first traversal) and fed into a bidirectional LSTM before decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Abstract Meaning Representation (AMR) rooted directed graphs (semantic graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Create two directed graph views: G^t (via Levi transformation to an unlabeled bipartite graph if used) and G^b (reverse edges of G^t); encode neighborhoods separately using GNNs for incoming neighbors in each view; concatenate per-node TD and BU encodings; generate node sequence by depth-first traversal for the sequence decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (graph-to-sequence text generation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Using this dual representation with GGNN encoder (G2S-GGNN): LDC2015E86 test BLEU = 24.32 ± 0.16, METEOR = 30.53 ± 0.30; LDC2017T10 test BLEU = 27.87 ± 0.15, METEOR = 33.21 ± 0.15. With additional Gigaword pretraining (200K) G2S-GGNN BLEU = 32.23 on LDC2015E86. Ablation: biLSTM baseline BLEU = 22.50; GE_t+biLSTM BLEU = 26.33; GE_b+biLSTM BLEU = 26.12; GE_t+GE_b+biLSTM BLEU = 27.37 (development set, LDC2017T10).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Outperforms a sequential linearized baseline (S2S) and several recent graph-based models on AMR-to-text benchmarks. Improves BLEU and METEOR relative to S2S and competes with/outscores Damonte & Cohen (2019), Cao & Clark (2019), and other GNN-based approaches; ablation shows the dual encoding outperforms using only TD or only BU encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Captures complementary global (top-down) and local/semantic-head (bottom-up) structural signals separately, easing encoding burden compared to single aggregated neighborhood vectors; removes need for additional positional encodings used when a single undirected graph must carry both TD and BU signals; empirically improves BLEU/METEOR and semantic entailment/adequacy.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Introduces extra parameters (slightly larger model size; e.g., complete model 61.7M vs 57.6M baseline) and computational cost due to two graph encoders; requires producing and processing two graph views and then concatenating representations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Performance degrades on larger/more complex graphs: models suffer as graph diameter increases and for long target sentences. Some GNN variants within the dual framework also struggle with very high node out-degree (see G2S-GAT and G2S-GGNN negative deltas for max out-degree 9-18) and performance is worse for graphs with diameter >13 (noting a 17.9% METEOR drop for G2S-GGNN between small and large-diameter graphs).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing AMR-to-Text Generation with Dual Graph Representations', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8814.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8814.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Levi transformation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Levi Graph Transformation (edge-to-node bipartite conversion)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph transformation that turns labeled edges into nodes, yielding an unlabeled bipartite graph where original nodes and former-edges (now nodes) alternate; used here to represent AMR labeled relations as explicit nodes so label information can be encoded by node embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph-to-sequence learning using gated graph neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Levi Transformation / Edge-to-node Bipartite Conversion</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each labeled edge (v_i, r, v_j) is replaced by two unlabeled edges (v_i, r) and (r, v_j) and a new node representing the relation r; resulting graph G^t has |V^t| = n + m and |E^t| = 2m (n nodes, m edges originally). This converts edge labels into nodes so relation labels can be embedded and participate in message passing like normal nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Abstract Meaning Representation (AMR) graphs with labeled directed edges</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>For every labeled directed edge (v_i, r, v_j) in AMR, create a new node for r and connect v_i -> r and r -> v_j (or reverse for the BU view). Use this transformed bipartite graph as input to GNN encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (graph-to-sequence); also used generally to enable GNN encoders to model labeled-edge information.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Applied as part of the Dual Graph Encoder pipeline; see dual-representation model metrics: G2S-GGNN BLEU (LDC2017T10) = 27.87 ± 0.15, METEOR = 33.21 ± 0.15; ablation indicates graph encoders (which operate on Levi-transformed graphs) substantially improve BLEU over sequential baselines (e.g., GE_t+GE_b+biLSTM BLEU = 27.37 vs biLSTM 22.50 on dev set).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Levi transformation allows representing labeled AMR relations without special edge-label-aware aggregation (compared to methods that keep labeled edges or aggregate neighborhood info into a single vector). It is used here instead of making edges undirected + positional encodings (a technique used elsewhere).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Makes relation labels explicit and learnable as node embeddings; enables GNN message passing to directly include relation label information; simplifies edge-label handling in standard GNNs.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Increases graph size (#nodes = n + m, #edges = 2m), which raises computational and memory costs for GNN layers; converting edges to nodes can increase path lengths between original concept nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Larger transformed graphs may exacerbate the issues of encoding very large graphs (performance drops with larger diameter and very high-degree nodes); no specific failure case beyond general scaling issues reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing AMR-to-Text Generation with Dual Graph Representations', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8814.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8814.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linearization (DFS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Depth-First Traversal Linearization (graph serialization via DFS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A common method that serializes a graph into a sequence by traversing the graph (typically depth-first) and emitting node labels in traversal order, enabling sequence-to-sequence models to treat graphs as linear inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural amr: Sequence-to-sequence models for parsing and generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Linearization / Depth-First Traversal Serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>The AMR graph is linearized by a depth-first traversal (DFS) producing a sequence of node tokens which is fed to a sequence encoder (biLSTM) in a seq2seq model; used both as a baseline (S2S) and as the ordering for node vectors generated by the graph encoders (node sequence for the bidirectional LSTM).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Abstract Meaning Representation (AMR) graphs / general semantic graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Perform DFS on the graph to yield an ordered list/sequence of node labels; feed this token sequence to a sequence encoder (attention-based LSTM decoder used to generate text).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (used in sequence-to-sequence baselines and as node ordering input to decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>S2S baseline (linearized input + copy & coverage) reported: LDC2015E86 BLEU = 22.55 ± 0.17, METEOR = 29.90 ± 0.31; LDC2017T10 BLEU = 22.73 ± 0.18, METEOR = 30.15 ± 0.14. Dual graph methods (G2S) outperform the linearized S2S baseline by several BLEU/METEOR points.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Linearization (S2S) is simpler but performs worse than graph-encoder-based approaches; prior works (Konstas et al., Flanigan et al.) used linearization but required additional anonymization heuristics to handle rare entities.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple to implement; allows reuse of standard seq2seq architectures and pretrained sequence modules; low graph-specific engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Linearization can obscure graph structure (edge labels, reentrancies/coreference) and forces structural signals into sequential order leading to loss of explicit non-sequential relations; tends to produce worse generation quality compared to graph encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Fails to represent graph phenomena such as reentrancies/coreference and multi-parent nodes naturally; yields worse BLEU/METEOR relative to graph-encoder approaches, especially as graph structural complexity grows.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing AMR-to-Text Generation with Dual Graph Representations', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8814.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8814.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GGNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gated Graph Neural Network (GGNN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A gated message-passing GNN that uses gated recurrent units (GRUs) to iteratively update node hidden states based on aggregated neighbor messages for a fixed number of steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gated graph sequence neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GGNN-based neighborhood aggregation (gated graph encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>GGNN layers compute node updates as GRU(h_i^{(l-1)}, Sum_{j in N(i)} W_1 h_j^{(l-1)}), using a fixed number of recurrent steps and backpropagation-through-time to learn parameters; used as GE^t and GE^b to produce TD and BU node encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (transformed via Levi when used here), directed graphs suitable for gated message passing</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No serialization; use GGNN message passing directly on the (Levi-transformed) graph to compute per-node representations, then concatenate TD/BU encodings and feed to sequence decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (graph-to-sequence)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>G2S-GGNN (best-performing variant here): LDC2015E86 BLEU = 24.32 ± 0.16, METEOR = 30.53 ± 0.30; LDC2017T10 BLEU = 27.87 ± 0.15, METEOR = 33.21 ± 0.15. With 200K Gigaword pretraining BLEU = 32.23 on LDC2015E86.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>G2S-GGNN outperforms G2S-GIN and G2S-GAT in these AMR-to-text experiments; authors hypothesize GGNN gating better captures long-distance dependencies in graphs compared to GIN/GAT.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Gating (GRU) can model longer-range dependencies and control information flow effectively; empirically gave the best BLEU/METEOR among tested GNNs in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires BPTT across message-passing steps (computational cost); larger number of layers/steps increases compute and memory.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>G2S-GGNN performance degrades for graphs with very high node out-degree (max out-degree 9-18), where S2S sometimes outperforms G2S-GGNN; performance also drops for graphs with large diameter and long target sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing AMR-to-Text Generation with Dual Graph Representations', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8814.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8814.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GAT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Attention Network (GAT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph encoder that uses self-attention over each node's neighborhood to compute attention-weighted sums of neighbor representations, potentially with multi-head attention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph Attention Networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GAT-based neighborhood aggregation (attention graph encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>At each layer, a node's new state is computed as a weighted combination of its own transformed state and its neighbors' transformed states, where attention coefficients alpha_{i,j} are computed via a learned scoring function and softmax normalization. Used separately for TD and BU graph views.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (Levi-transformed) / general directed graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Apply attention-based message passing on the (Levi-transformed) graph to compute per-node representations; concatenate TD/BU encodings and feed to sequence decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>G2S-GAT reported: LDC2015E86 BLEU = 23.42 ± 0.16, METEOR = 29.87 ± 0.14; LDC2017T10 BLEU = 26.72 ± 0.20, METEOR = 32.52 ± 0.02.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>G2S-GAT performs comparably to G2S-GIN but below G2S-GGNN on the AMR-to-text benchmarks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Attention mechanism focuses on most relevant neighbor information; multi-head attention can capture different aspects of neighborhood influence.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>May be less effective than gated message-passing (GGNN) in capturing long-range dependencies for AMR-to-text in these experiments; computational overhead for attention computation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Shows reduced performance on graphs with very high node out-degree (negative relative deltas in METEOR for out-degree 9-18) and on large-diameter graphs compared to small ones.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing AMR-to-Text Generation with Dual Graph Representations', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8814.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8814.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GIN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Isomorphic Network (GIN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph neural network architecture proven as powerful as the Weisfeiler-Lehman isomorphism test for distinguishing graph structures; aggregates node state plus neighbor states and applies an MLP.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How powerful are graph neural networks?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GIN-based neighborhood aggregation (isomorphic graph encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each GIN layer computes h_i^{(l)} = MLP(h_i^{(l-1)} + Sum_{j in N(i)} h_j^{(l-1)}), i.e., a simple sum aggregation of node and neighbor states followed by an MLP, used separately for TD and BU graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (Levi-transformed) / general graphs where structural discriminability matters</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Apply GIN message passing on the transformed graph to compute per-node representations for both TD and BU views, then concatenate for decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>G2S-GIN reported: LDC2015E86 BLEU = 22.93 ± 0.20, METEOR = 29.72 ± 0.09; LDC2017T10 BLEU = 26.90 ± 0.19, METEOR = 32.62 ± 0.04. Ablation and adequacy metrics indicate G2S-GIN is closest to human references in 'added' vs 'miss' token statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>G2S-GIN performs similarly to G2S-GAT and below G2S-GGNN on BLEU/METEOR, but shows strengths for graphs with very high node out-degree (>9) where it handles long-tailed degree distributions better.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Powerful discriminative capacity for graph structures (theoretically as powerful as WL test); better at handling graphs with very high node out-degree in these experiments; closer to GOLD regarding added/missing token coverage in generated sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Overall BLEU/METEOR lower than GGNN in these AMR-to-text experiments; simpler sum-aggregation may miss nuanced attention/gating benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Lower overall generation scores vs GGNN for most graph sizes; still suffers from decreased performance on very large graph diameters and long target sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing AMR-to-Text Generation with Dual Graph Representations', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8814.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8814.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Anonymization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Entity Anonymization / Delexicalization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preprocessing approach that replaces named entities and other sparse lexical items with placeholder tokens to reduce data sparsity, requiring a post-processing re-lexicalization step.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural amr: Sequence-to-sequence models for parsing and generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Anonymization / Delexicalization of rare entities</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Replace named entities and rare lexical items with anonymized placeholders during training and decoding; at generation time a post-processing step replaces placeholders with original or matched surface forms.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs / text associated with AMR</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Preprocess input graphs and references to substitute rare names/values with placeholders; sequence models are trained on these anonymized sequences and outputs are post-processed to reinsert actual entity strings.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (reducing sparsity for seq2seq models)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Prior works using anonymization (e.g., Konstas et al.) reported competitive BLEU scores but required ad hoc matching procedures; this paper reports that their non-anonymized copy-based approach reaches competitive or better scores (e.g., G2S-GGNN > some anonymization-based baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>The paper contrasts anonymization-based pipelines with their copy+coverage strategy: anonymization can help seq2seq but requires dataset-specific heuristics and can lose semantic signals; the authors avoid anonymization and use copy mechanism instead.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Reduces sparsity for rare entities facilitating learning for small data; simpler generation vocabularies.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires ad hoc, corpus-specific matching and re-lexicalization rules; can produce incorrect or incomplete delexicalizations/reinsertion; may lose semantic signals if anonymized tokens drop information.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Anonymization can fail when matching rare input items to output strings (e.g., complex multi-token entity names), causing incorrect or incomplete re-lexicalization; authors report avoiding anonymization yields better semantic coverage in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing AMR-to-Text Generation with Dual Graph Representations', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8814.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8814.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Copy+Coverage</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pointer-Copy and Coverage Mechanisms (copy and coverage)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mechanism that allows the decoder to copy tokens from the input graph representation (pointer-generator style) and uses a coverage vector to discourage repetition and track attention history.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Get to the point: Summarization with pointer-generator networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Copy (pointer) and Coverage mechanisms</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>At each decoding step, a copy distribution over input positions is combined with the vocabulary distribution to allow copying input tokens; a coverage vector accumulates past attention to reduce repetition and encourage coverage of input content.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (as serialized node sequences or node embeddings attended by decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Decoder attends to graph-derived node representations; copy scores allow emitting tokens present in the input graph; coverage vector is accumulated from attention distributions to penalize repeated attention.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation; handling OOVs and reducing repetition</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used in all reported seq2seq and G2S models as part of training; baseline S2S uses copy+coverage and gets BLEU 22.55 (LDC2015E86) while graph-encoded models using the same copy+coverage outperform it (e.g., G2S-GGNN BLEU 24.32).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Presented as an alternative to entity anonymization; authors claim copy+coverage is generic and dataset-independent and allows omission of anonymization preprocessing while maintaining or improving performance.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Handles rare and OOV tokens without dataset-specific anonymization heuristics; coverage helps avoid repetition in generated text; simple to integrate into encoder-decoder framework.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Copying depends on input representations and attention quality; copy may not always correctly map complex entity forms without additional alignment heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No specific failure cases reported beyond general limits: copying is only as good as attention—if the input-node representations or attention are poor, copy mechanism may not recover correct surface forms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing AMR-to-Text Generation with Dual Graph Representations', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural amr: Sequence-to-sequence models for parsing and generation <em>(Rating: 2)</em></li>
                <li>Graph-to-sequence learning using gated graph neural networks <em>(Rating: 2)</em></li>
                <li>Deep graph convolutional encoders for structured data to text generation <em>(Rating: 2)</em></li>
                <li>Structural neural encoders for AMR-to-text generation <em>(Rating: 2)</em></li>
                <li>Densely connected graph convolutional networks for graph-to-sequence learning <em>(Rating: 2)</em></li>
                <li>Text Generation from Knowledge Graphs with Graph Transformers <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8814",
    "paper_id": "paper-ce07df583f7a0e975175c4efc8b93a436376fef8",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "Dual TD/BU representation",
            "name_full": "Dual Top-Down and Bottom-Up Graph Representation",
            "brief_description": "A representation that encodes each AMR graph as two complementary directed views — a top-down (TD) view and a bottom-up (BU) view — and learns separate node embeddings for each view which are concatenated to form final node representations used for sequence decoding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Dual TD/BU Graph Representation (Dual Graph Encoder)",
            "representation_description": "The input AMR graph is converted into two directed views G^t (top-down) and G^b (bottom-up, obtained by reversing the edges of G^t). Each node in the transformed graph is embedded and encoded separately by two graph encoders (GE^t and GE^b) that compute h_i^t and h_i^b respectively; these are concatenated with the node embedding e_i to produce the final node vector r_i = [h_i^t || h_i^b || e_i]. The node representations are then ordered (depth-first traversal) and fed into a bidirectional LSTM before decoding.",
            "graph_type": "Abstract Meaning Representation (AMR) rooted directed graphs (semantic graphs)",
            "conversion_method": "Create two directed graph views: G^t (via Levi transformation to an unlabeled bipartite graph if used) and G^b (reverse edges of G^t); encode neighborhoods separately using GNNs for incoming neighbors in each view; concatenate per-node TD and BU encodings; generate node sequence by depth-first traversal for the sequence decoder.",
            "downstream_task": "AMR-to-text generation (graph-to-sequence text generation)",
            "performance_metrics": "Using this dual representation with GGNN encoder (G2S-GGNN): LDC2015E86 test BLEU = 24.32 ± 0.16, METEOR = 30.53 ± 0.30; LDC2017T10 test BLEU = 27.87 ± 0.15, METEOR = 33.21 ± 0.15. With additional Gigaword pretraining (200K) G2S-GGNN BLEU = 32.23 on LDC2015E86. Ablation: biLSTM baseline BLEU = 22.50; GE_t+biLSTM BLEU = 26.33; GE_b+biLSTM BLEU = 26.12; GE_t+GE_b+biLSTM BLEU = 27.37 (development set, LDC2017T10).",
            "comparison_to_others": "Outperforms a sequential linearized baseline (S2S) and several recent graph-based models on AMR-to-text benchmarks. Improves BLEU and METEOR relative to S2S and competes with/outscores Damonte & Cohen (2019), Cao & Clark (2019), and other GNN-based approaches; ablation shows the dual encoding outperforms using only TD or only BU encoders.",
            "advantages": "Captures complementary global (top-down) and local/semantic-head (bottom-up) structural signals separately, easing encoding burden compared to single aggregated neighborhood vectors; removes need for additional positional encodings used when a single undirected graph must carry both TD and BU signals; empirically improves BLEU/METEOR and semantic entailment/adequacy.",
            "disadvantages": "Introduces extra parameters (slightly larger model size; e.g., complete model 61.7M vs 57.6M baseline) and computational cost due to two graph encoders; requires producing and processing two graph views and then concatenating representations.",
            "failure_cases": "Performance degrades on larger/more complex graphs: models suffer as graph diameter increases and for long target sentences. Some GNN variants within the dual framework also struggle with very high node out-degree (see G2S-GAT and G2S-GGNN negative deltas for max out-degree 9-18) and performance is worse for graphs with diameter &gt;13 (noting a 17.9% METEOR drop for G2S-GGNN between small and large-diameter graphs).",
            "uuid": "e8814.0",
            "source_info": {
                "paper_title": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "Levi transformation",
            "name_full": "Levi Graph Transformation (edge-to-node bipartite conversion)",
            "brief_description": "A graph transformation that turns labeled edges into nodes, yielding an unlabeled bipartite graph where original nodes and former-edges (now nodes) alternate; used here to represent AMR labeled relations as explicit nodes so label information can be encoded by node embeddings.",
            "citation_title": "Graph-to-sequence learning using gated graph neural networks",
            "mention_or_use": "use",
            "representation_name": "Levi Transformation / Edge-to-node Bipartite Conversion",
            "representation_description": "Each labeled edge (v_i, r, v_j) is replaced by two unlabeled edges (v_i, r) and (r, v_j) and a new node representing the relation r; resulting graph G^t has |V^t| = n + m and |E^t| = 2m (n nodes, m edges originally). This converts edge labels into nodes so relation labels can be embedded and participate in message passing like normal nodes.",
            "graph_type": "Abstract Meaning Representation (AMR) graphs with labeled directed edges",
            "conversion_method": "For every labeled directed edge (v_i, r, v_j) in AMR, create a new node for r and connect v_i -&gt; r and r -&gt; v_j (or reverse for the BU view). Use this transformed bipartite graph as input to GNN encoders.",
            "downstream_task": "AMR-to-text generation (graph-to-sequence); also used generally to enable GNN encoders to model labeled-edge information.",
            "performance_metrics": "Applied as part of the Dual Graph Encoder pipeline; see dual-representation model metrics: G2S-GGNN BLEU (LDC2017T10) = 27.87 ± 0.15, METEOR = 33.21 ± 0.15; ablation indicates graph encoders (which operate on Levi-transformed graphs) substantially improve BLEU over sequential baselines (e.g., GE_t+GE_b+biLSTM BLEU = 27.37 vs biLSTM 22.50 on dev set).",
            "comparison_to_others": "Levi transformation allows representing labeled AMR relations without special edge-label-aware aggregation (compared to methods that keep labeled edges or aggregate neighborhood info into a single vector). It is used here instead of making edges undirected + positional encodings (a technique used elsewhere).",
            "advantages": "Makes relation labels explicit and learnable as node embeddings; enables GNN message passing to directly include relation label information; simplifies edge-label handling in standard GNNs.",
            "disadvantages": "Increases graph size (#nodes = n + m, #edges = 2m), which raises computational and memory costs for GNN layers; converting edges to nodes can increase path lengths between original concept nodes.",
            "failure_cases": "Larger transformed graphs may exacerbate the issues of encoding very large graphs (performance drops with larger diameter and very high-degree nodes); no specific failure case beyond general scaling issues reported.",
            "uuid": "e8814.1",
            "source_info": {
                "paper_title": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "Linearization (DFS)",
            "name_full": "Depth-First Traversal Linearization (graph serialization via DFS)",
            "brief_description": "A common method that serializes a graph into a sequence by traversing the graph (typically depth-first) and emitting node labels in traversal order, enabling sequence-to-sequence models to treat graphs as linear inputs.",
            "citation_title": "Neural amr: Sequence-to-sequence models for parsing and generation",
            "mention_or_use": "use",
            "representation_name": "Linearization / Depth-First Traversal Serialization",
            "representation_description": "The AMR graph is linearized by a depth-first traversal (DFS) producing a sequence of node tokens which is fed to a sequence encoder (biLSTM) in a seq2seq model; used both as a baseline (S2S) and as the ordering for node vectors generated by the graph encoders (node sequence for the bidirectional LSTM).",
            "graph_type": "Abstract Meaning Representation (AMR) graphs / general semantic graphs",
            "conversion_method": "Perform DFS on the graph to yield an ordered list/sequence of node labels; feed this token sequence to a sequence encoder (attention-based LSTM decoder used to generate text).",
            "downstream_task": "AMR-to-text generation (used in sequence-to-sequence baselines and as node ordering input to decoder)",
            "performance_metrics": "S2S baseline (linearized input + copy & coverage) reported: LDC2015E86 BLEU = 22.55 ± 0.17, METEOR = 29.90 ± 0.31; LDC2017T10 BLEU = 22.73 ± 0.18, METEOR = 30.15 ± 0.14. Dual graph methods (G2S) outperform the linearized S2S baseline by several BLEU/METEOR points.",
            "comparison_to_others": "Linearization (S2S) is simpler but performs worse than graph-encoder-based approaches; prior works (Konstas et al., Flanigan et al.) used linearization but required additional anonymization heuristics to handle rare entities.",
            "advantages": "Simple to implement; allows reuse of standard seq2seq architectures and pretrained sequence modules; low graph-specific engineering.",
            "disadvantages": "Linearization can obscure graph structure (edge labels, reentrancies/coreference) and forces structural signals into sequential order leading to loss of explicit non-sequential relations; tends to produce worse generation quality compared to graph encoders.",
            "failure_cases": "Fails to represent graph phenomena such as reentrancies/coreference and multi-parent nodes naturally; yields worse BLEU/METEOR relative to graph-encoder approaches, especially as graph structural complexity grows.",
            "uuid": "e8814.2",
            "source_info": {
                "paper_title": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "GGNN",
            "name_full": "Gated Graph Neural Network (GGNN)",
            "brief_description": "A gated message-passing GNN that uses gated recurrent units (GRUs) to iteratively update node hidden states based on aggregated neighbor messages for a fixed number of steps.",
            "citation_title": "Gated graph sequence neural networks",
            "mention_or_use": "use",
            "representation_name": "GGNN-based neighborhood aggregation (gated graph encoder)",
            "representation_description": "GGNN layers compute node updates as GRU(h_i^{(l-1)}, Sum_{j in N(i)} W_1 h_j^{(l-1)}), using a fixed number of recurrent steps and backpropagation-through-time to learn parameters; used as GE^t and GE^b to produce TD and BU node encodings.",
            "graph_type": "AMR graphs (transformed via Levi when used here), directed graphs suitable for gated message passing",
            "conversion_method": "No serialization; use GGNN message passing directly on the (Levi-transformed) graph to compute per-node representations, then concatenate TD/BU encodings and feed to sequence decoder.",
            "downstream_task": "AMR-to-text generation (graph-to-sequence)",
            "performance_metrics": "G2S-GGNN (best-performing variant here): LDC2015E86 BLEU = 24.32 ± 0.16, METEOR = 30.53 ± 0.30; LDC2017T10 BLEU = 27.87 ± 0.15, METEOR = 33.21 ± 0.15. With 200K Gigaword pretraining BLEU = 32.23 on LDC2015E86.",
            "comparison_to_others": "G2S-GGNN outperforms G2S-GIN and G2S-GAT in these AMR-to-text experiments; authors hypothesize GGNN gating better captures long-distance dependencies in graphs compared to GIN/GAT.",
            "advantages": "Gating (GRU) can model longer-range dependencies and control information flow effectively; empirically gave the best BLEU/METEOR among tested GNNs in this paper.",
            "disadvantages": "Requires BPTT across message-passing steps (computational cost); larger number of layers/steps increases compute and memory.",
            "failure_cases": "G2S-GGNN performance degrades for graphs with very high node out-degree (max out-degree 9-18), where S2S sometimes outperforms G2S-GGNN; performance also drops for graphs with large diameter and long target sentences.",
            "uuid": "e8814.3",
            "source_info": {
                "paper_title": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "GAT",
            "name_full": "Graph Attention Network (GAT)",
            "brief_description": "A graph encoder that uses self-attention over each node's neighborhood to compute attention-weighted sums of neighbor representations, potentially with multi-head attention.",
            "citation_title": "Graph Attention Networks",
            "mention_or_use": "use",
            "representation_name": "GAT-based neighborhood aggregation (attention graph encoder)",
            "representation_description": "At each layer, a node's new state is computed as a weighted combination of its own transformed state and its neighbors' transformed states, where attention coefficients alpha_{i,j} are computed via a learned scoring function and softmax normalization. Used separately for TD and BU graph views.",
            "graph_type": "AMR graphs (Levi-transformed) / general directed graphs",
            "conversion_method": "Apply attention-based message passing on the (Levi-transformed) graph to compute per-node representations; concatenate TD/BU encodings and feed to sequence decoder.",
            "downstream_task": "AMR-to-text generation",
            "performance_metrics": "G2S-GAT reported: LDC2015E86 BLEU = 23.42 ± 0.16, METEOR = 29.87 ± 0.14; LDC2017T10 BLEU = 26.72 ± 0.20, METEOR = 32.52 ± 0.02.",
            "comparison_to_others": "G2S-GAT performs comparably to G2S-GIN but below G2S-GGNN on the AMR-to-text benchmarks in this paper.",
            "advantages": "Attention mechanism focuses on most relevant neighbor information; multi-head attention can capture different aspects of neighborhood influence.",
            "disadvantages": "May be less effective than gated message-passing (GGNN) in capturing long-range dependencies for AMR-to-text in these experiments; computational overhead for attention computation.",
            "failure_cases": "Shows reduced performance on graphs with very high node out-degree (negative relative deltas in METEOR for out-degree 9-18) and on large-diameter graphs compared to small ones.",
            "uuid": "e8814.4",
            "source_info": {
                "paper_title": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "GIN",
            "name_full": "Graph Isomorphic Network (GIN)",
            "brief_description": "A graph neural network architecture proven as powerful as the Weisfeiler-Lehman isomorphism test for distinguishing graph structures; aggregates node state plus neighbor states and applies an MLP.",
            "citation_title": "How powerful are graph neural networks?",
            "mention_or_use": "use",
            "representation_name": "GIN-based neighborhood aggregation (isomorphic graph encoder)",
            "representation_description": "Each GIN layer computes h_i^{(l)} = MLP(h_i^{(l-1)} + Sum_{j in N(i)} h_j^{(l-1)}), i.e., a simple sum aggregation of node and neighbor states followed by an MLP, used separately for TD and BU graphs.",
            "graph_type": "AMR graphs (Levi-transformed) / general graphs where structural discriminability matters",
            "conversion_method": "Apply GIN message passing on the transformed graph to compute per-node representations for both TD and BU views, then concatenate for decoding.",
            "downstream_task": "AMR-to-text generation",
            "performance_metrics": "G2S-GIN reported: LDC2015E86 BLEU = 22.93 ± 0.20, METEOR = 29.72 ± 0.09; LDC2017T10 BLEU = 26.90 ± 0.19, METEOR = 32.62 ± 0.04. Ablation and adequacy metrics indicate G2S-GIN is closest to human references in 'added' vs 'miss' token statistics.",
            "comparison_to_others": "G2S-GIN performs similarly to G2S-GAT and below G2S-GGNN on BLEU/METEOR, but shows strengths for graphs with very high node out-degree (&gt;9) where it handles long-tailed degree distributions better.",
            "advantages": "Powerful discriminative capacity for graph structures (theoretically as powerful as WL test); better at handling graphs with very high node out-degree in these experiments; closer to GOLD regarding added/missing token coverage in generated sentences.",
            "disadvantages": "Overall BLEU/METEOR lower than GGNN in these AMR-to-text experiments; simpler sum-aggregation may miss nuanced attention/gating benefits.",
            "failure_cases": "Lower overall generation scores vs GGNN for most graph sizes; still suffers from decreased performance on very large graph diameters and long target sentences.",
            "uuid": "e8814.5",
            "source_info": {
                "paper_title": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "Anonymization",
            "name_full": "Entity Anonymization / Delexicalization",
            "brief_description": "A preprocessing approach that replaces named entities and other sparse lexical items with placeholder tokens to reduce data sparsity, requiring a post-processing re-lexicalization step.",
            "citation_title": "Neural amr: Sequence-to-sequence models for parsing and generation",
            "mention_or_use": "mention",
            "representation_name": "Anonymization / Delexicalization of rare entities",
            "representation_description": "Replace named entities and rare lexical items with anonymized placeholders during training and decoding; at generation time a post-processing step replaces placeholders with original or matched surface forms.",
            "graph_type": "AMR graphs / text associated with AMR",
            "conversion_method": "Preprocess input graphs and references to substitute rare names/values with placeholders; sequence models are trained on these anonymized sequences and outputs are post-processed to reinsert actual entity strings.",
            "downstream_task": "AMR-to-text generation (reducing sparsity for seq2seq models)",
            "performance_metrics": "Prior works using anonymization (e.g., Konstas et al.) reported competitive BLEU scores but required ad hoc matching procedures; this paper reports that their non-anonymized copy-based approach reaches competitive or better scores (e.g., G2S-GGNN &gt; some anonymization-based baselines).",
            "comparison_to_others": "The paper contrasts anonymization-based pipelines with their copy+coverage strategy: anonymization can help seq2seq but requires dataset-specific heuristics and can lose semantic signals; the authors avoid anonymization and use copy mechanism instead.",
            "advantages": "Reduces sparsity for rare entities facilitating learning for small data; simpler generation vocabularies.",
            "disadvantages": "Requires ad hoc, corpus-specific matching and re-lexicalization rules; can produce incorrect or incomplete delexicalizations/reinsertion; may lose semantic signals if anonymized tokens drop information.",
            "failure_cases": "Anonymization can fail when matching rare input items to output strings (e.g., complex multi-token entity names), causing incorrect or incomplete re-lexicalization; authors report avoiding anonymization yields better semantic coverage in their experiments.",
            "uuid": "e8814.6",
            "source_info": {
                "paper_title": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "Copy+Coverage",
            "name_full": "Pointer-Copy and Coverage Mechanisms (copy and coverage)",
            "brief_description": "A mechanism that allows the decoder to copy tokens from the input graph representation (pointer-generator style) and uses a coverage vector to discourage repetition and track attention history.",
            "citation_title": "Get to the point: Summarization with pointer-generator networks",
            "mention_or_use": "use",
            "representation_name": "Copy (pointer) and Coverage mechanisms",
            "representation_description": "At each decoding step, a copy distribution over input positions is combined with the vocabulary distribution to allow copying input tokens; a coverage vector accumulates past attention to reduce repetition and encourage coverage of input content.",
            "graph_type": "AMR graphs (as serialized node sequences or node embeddings attended by decoder)",
            "conversion_method": "Decoder attends to graph-derived node representations; copy scores allow emitting tokens present in the input graph; coverage vector is accumulated from attention distributions to penalize repeated attention.",
            "downstream_task": "AMR-to-text generation; handling OOVs and reducing repetition",
            "performance_metrics": "Used in all reported seq2seq and G2S models as part of training; baseline S2S uses copy+coverage and gets BLEU 22.55 (LDC2015E86) while graph-encoded models using the same copy+coverage outperform it (e.g., G2S-GGNN BLEU 24.32).",
            "comparison_to_others": "Presented as an alternative to entity anonymization; authors claim copy+coverage is generic and dataset-independent and allows omission of anonymization preprocessing while maintaining or improving performance.",
            "advantages": "Handles rare and OOV tokens without dataset-specific anonymization heuristics; coverage helps avoid repetition in generated text; simple to integrate into encoder-decoder framework.",
            "disadvantages": "Copying depends on input representations and attention quality; copy may not always correctly map complex entity forms without additional alignment heuristics.",
            "failure_cases": "No specific failure cases reported beyond general limits: copying is only as good as attention—if the input-node representations or attention are poor, copy mechanism may not recover correct surface forms.",
            "uuid": "e8814.7",
            "source_info": {
                "paper_title": "Enhancing AMR-to-Text Generation with Dual Graph Representations",
                "publication_date_yy_mm": "2019-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural amr: Sequence-to-sequence models for parsing and generation",
            "rating": 2,
            "sanitized_title": "neural_amr_sequencetosequence_models_for_parsing_and_generation"
        },
        {
            "paper_title": "Graph-to-sequence learning using gated graph neural networks",
            "rating": 2,
            "sanitized_title": "graphtosequence_learning_using_gated_graph_neural_networks"
        },
        {
            "paper_title": "Deep graph convolutional encoders for structured data to text generation",
            "rating": 2,
            "sanitized_title": "deep_graph_convolutional_encoders_for_structured_data_to_text_generation"
        },
        {
            "paper_title": "Structural neural encoders for AMR-to-text generation",
            "rating": 2,
            "sanitized_title": "structural_neural_encoders_for_amrtotext_generation"
        },
        {
            "paper_title": "Densely connected graph convolutional networks for graph-to-sequence learning",
            "rating": 2,
            "sanitized_title": "densely_connected_graph_convolutional_networks_for_graphtosequence_learning"
        },
        {
            "paper_title": "Text Generation from Knowledge Graphs with Graph Transformers",
            "rating": 2,
            "sanitized_title": "text_generation_from_knowledge_graphs_with_graph_transformers"
        }
    ],
    "cost": 0.0189715,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Enhancing AMR-to-Text Generation with Dual Graph Representations</h1>
<p>Leonardo F. R. Ribeiro ${ }^{\dagger}$, Claire Gardent ${ }^{\ddagger}$ and Iryna Gurevych ${ }^{\dagger}$<br>${ }^{\dagger}$ Research Training Group AIPHES and UKP Lab, Technische Universität Darmstadt<br>www.ukp.tu-darmstadt.de<br>${ }^{\ddagger}$ CNRS/LORIA, Nancy, France<br>claire.gardent@loria.fr</p>
<h4>Abstract</h4>
<p>Generating text from graph-based data, such as Abstract Meaning Representation (AMR), is a challenging task due to the inherent difficulty in how to properly encode the structure of a graph with labeled edges. To address this difficulty, we propose a novel graph-to-sequence model that encodes different but complementary perspectives of the structural information contained in the AMR graph. The model learns parallel top-down and bottom-up representations of nodes capturing contrasting views of the graph. We also investigate the use of different node message passing strategies, employing different state-of-the-art graph encoders to compute node representations based on incoming and outgoing perspectives. In our experiments, we demonstrate that the dual graph representation leads to improvements in AMR-to-text generation, achieving state-of-the-art results on two AMR datasets ${ }^{\ddagger}$.</p>
<h2>1 Introduction</h2>
<p>Abstract Meaning Representation (AMR; Banarescu et al. (2013)) is a linguistically-grounded semantic formalism that represents the meaning of a sentence as a rooted directed graph, where nodes are concepts and edges are semantic relations. As AMR abstracts away from surface word strings and syntactic structure producing a language neutral representation of meaning, its usage is beneficial in many semantic related NLP tasks, including text summarization (Liao et al., 2018) and machine translation (Song et al., 2019).</p>
<p>The purpose of AMR-to-text generation is to produce a text which verbalises the meaning encoded by an input AMR graph. This is a challenging task as capturing the complex structural information stored in graph-based data is not trivial, as these are non-Euclidean structures, which</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>implies that properties such as global parametrization, vector space structure, or shift-invariance do not hold (Bronstein et al., 2017). Recently, Graph Neural Networks (GNNs) have emerged as a powerful class of methods for learning effective graph latent representations (Xu et al., 2019) and graph-to-sequence models have been applied to the task of AMR-to-text generation (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Guo et al., 2019).</p>
<p>In this paper, we propose a novel graph-tosequence approach to AMR-to-text generation, which is inspired by pre-neural generation algorithms. These approaches explored alternative (top-down, bottom-up and mixed) traversals of the input graph and showed that a hybrid traversal combining both top-down (TD) and bottom-up (BU) information was best as this permits integrating both global constraints top-down from the input and local constraints bottom-up from the semantic heads (Shieber et al., 1990; Narayan and Gardent, 2012).</p>
<p>Similarly, we present an approach where the input graph is represented by two separate structures, each representing a different view of the graph. The nodes of these two structures are encoded using separate graph encoders so that each concept and relation in the input graph is assigned both a TD and a BU representation.</p>
<p>Our approach markedly differs from existing graph-to-sequence models for MR-to-Text generation (Marcheggiani and Perez Beltrachini, 2018; Beck et al., 2018; Damonte and Cohen, 2019) in that these approaches aggregate all the immediate neighborhood information of a node in a single representation. By exploiting parallel and complementary vector representations of the AMR graph, our approach eases the burden on the neural model in encoding nodes (concepts) and edges (relations) in a single vector representation. It also elimi-</p>
<p>nates the need for additional positional information (Beck et al., 2018) which is required when the same graph is used to encode both TD and BU information, thereby making the edges undirected.</p>
<p>Our main contributions are the following:</p>
<ul>
<li>We present a novel architecture for AMR-to-text generation which explicitly encodes two separate TD and BU views of the input graph.</li>
<li>We show that our approach outperforms recent AMR-to-text generation models on two datasets, including a model that leverages additional syntactic information (Cao and Clark, 2019).</li>
<li>We compare the performance of three graph encoders, which have not been studied so far for AMR-to-text generation.</li>
</ul>
<h2>2 Related Work</h2>
<p>Early works on AMR-to-text generation employ statistical methods (Flanigan et al., 2016b; Pourdamghani et al., 2016; Castro Ferreira et al., 2017) and apply linearization of the graph by means of a depth-first traversal.</p>
<p>Recent neural approaches have exhibited success by linearising the input graph and using a sequence-to-sequence architecture. Konstas et al. (2017) achieve promising results on this task. However, they strongly rely on named entities anonymisation. Anonymisation requires an ad hoc procedure for each new corpus. The matching procedure needs to match a rare input item correctly (e.g., "United States of America") with the corresponding part in the output text (e.g., "USA") which may be challenging and may result in incorrect or incomplete delexicalisations. In contrast, our approach omits anonymisation. Instead, we use a copy mechanism (See et al., 2017), a generic technique which is easy to integrate in the encoder-decoder framework and can be used independently of the particular domain and application. Our approach further differs from Konstas et al. (2017) in that we build a dual TD/BU graph representation and use graph encoders to represent nodes.</p>
<p>Cao and Clark (2019) factor the generation process leveraging syntactic information to improve the performance. However, they linearize both AMR and constituency graphs, which implies that
important parts of the graphs cannot well be represented (e.g., coreference).</p>
<p>Several graph-to-sequence models have been proposed. Marcheggiani and Perez Beltrachini (2018) show that explicitly encoding the structure of the graph is beneficial with respect to sequential encoding. They evaluate their model on two tasks, WebNLG (Gardent et al., 2017) and SR11Deep (Belz et al., 2011), but do not apply it to AMR benchmarks. Song et al. (2018) and Beck et al. (2018) apply recurrent neural networks to directly encode AMR graphs. Song et al. (2018) use a graph LSTM as the graph encoder, whereas Beck et al. (2018) develop a model based on GRUs. We go a step further in that direction by developing parallel encodings of graphs which are able to highlight different graph properties.</p>
<p>In a related task, Koncel-Kedziorski et al. (2019) propose an attention-based graph model that generates sentences from knowledge graphs. Schlichtkrull et al. (2018) use Graph Convolutional Networks (GCNs) to tackle the tasks of link prediction and entity classification on knowledge graphs.</p>
<p>Damonte and Cohen (2019) show that off-theshelf GCNs cannot achieve good performance for AMR-to-text generation. To tackle this issue, Guo et al. (2019) introduce dense connectivity to GNNs in order to integrate both local and global features, achieving good results on the task. Our work is related to Damonte and Cohen (2019), that use stacking of GCN and LSTM layers to improve the model capacity and employ anonymization. However, our model is substantially different: (i) we learn dual representations capturing top-down and bottom-up adjuvant views of the graph, (ii) we employ more effective graph encoders (with different neighborhood aggregations) than GCNs and (iii) we employ copy and coverage mechanisms and do not resort to entity anonymization.</p>
<h2>3 Graph-to-Sequence Model</h2>
<p>In this section, we describe (i) the representations of the graph adopted as inputs, (ii) the model architecture, including the Dual Graph Encoder and (iii) the GNNs employed as graph encoders.</p>
<h3>3.1 Graph Preparation</h3>
<p>Let $G=(V, E, R)$ denote a rooted and directed AMR graph with nodes $v_{i} \in V$ and labeled edges $\left(v_{i}, r, v_{j}\right) \in E$, where $r \in R$ is a relation type.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a) an example sentence, (b) its original AMR graph (G) and different graph perspectives: (c) top-down (G<sup>t</sup>) and (d) bottom-up (G<sup>b</sup>).</p>
<p>Let n = |V| and m = |E| denote the numbers of nodes and edges, respectively.</p>
<p>We convert each AMR graph into an unlabeled and connected bipartite graph G<sup>t</sup> = (V<sup>t</sup>, E<sup>t</sup>), transforming each labeled edge (v<sup>i</sup>, r, v<sup>j</sup>) ∈ E into two unlabeled edges (v<sup>i</sup>, r), (r, v<sup>j</sup>) ∈ E<sup>t</sup>, with |V<sup>t</sup>| = n + m and |E<sup>t</sup>| = 2m. This process, called Levi Transformation (Beck et al., 2018), turns original edges into nodes creating an unlabeled graph. For instance, the edge between semester and that with label :mod in Figure 1(b) is replaced by two edges and one node in 1(c): an edge between semester, and the new node :mod and another one between :mod and that. The new graph allows us to directly represent the relationships between nodes using embeddings. This enables us to encode label edge information using distinct message passing schemes employing different GNNs.</p>
<p>G<sup>t</sup> captures a TD view of the graph. We also create a BU view of the graph G<sup>b</sup> = (V<sup>t</sup>, E<sup>b</sup>), where each directed edge e<sup>k</sup> = (v<sup>i</sup>, v<sup>j</sup>) ∈ E<sup>t</sup> becomes e<sup>k</sup> = (v<sup>j</sup>, v<sup>i</sup>) ∈ E<sup>b</sup>, that is, we reverse the direction of original edges. An example of a sentence, its AMR graph and the two new graphs G<sup>t</sup> and G<sup>b</sup> is shown in Figure 1.</p>
<h3>3.2 Dual Graph Encoder</h3>
<p>We represent each node v<sup>i</sup> ∈ V<sup>t</sup> with a node embedding e<sup>i</sup> ∈ ℝ<sup>d</sup>, generated from the node label. In order to explicitly encode structural information, our encoder starts with two graph encoders, denoted by GE<sup>t</sup> and GE<sup>b</sup>, that compute representations for nodes in G<sup>t</sup> and G<sup>b</sup>, respectively.</p>
<p>Each GE learns node representations based on the specific view of its particular graph, G<sup>t</sup> or G<sup>b</sup>. Since G<sup>t</sup> and G<sup>b</sup> capture distinct perspectives of the graph structure, the information flow is propagated throughout TD and BU directions, respectively. In particular, for each node v<sup>i</sup>, the GE receives the node embeddings of v<sup>i</sup> and its neighbors, and computes its node representation:</p>
<p>$$
\begin{aligned}
\mathbf{h}<em i="i">{i}^{t} &amp;= GE_i({\mathbf{e}_i, \mathbf{e}_j : j \in \mathcal{N}_t(i)}), \
\mathbf{h}</em>_b(i)}),
\end{aligned}
$$}^{b} &amp;= GE_b({\mathbf{e}_i, \mathbf{e}_j : j \in \mathcal{N</p>
<p>where N<sub>t</sub>(i) and N<sub>b</sub>(i) are the immediate incoming neighborhoods of v<sup>i</sup> in G<sup>t</sup> and G<sup>b</sup>, respectively.</p>
<p>Each node v<sup>i</sup> is represented by two different hidden states, h<sup>t</sup><sub>i</sub> and h<sup>b</sup><sub>i</sub>. Note that we learn two representations per relation and node of the original AMR graph. The hidden states h<sup>t</sup><sub>i</sub> and h<sup>b</sup><sub>i</sub>, and embedding e<sup>i</sup> contain different information regarding v<sup>i</sup>. We concatenate them building a final node representation:</p>
<p>$$
\mathbf{r}<em i="i">i = \left[ \mathbf{h}</em>
$$}^{t} \left| \mathbf{h}_{i}^{b} \right| \mathbf{e}_i \right]. \tag{1</p>
<p>This approach is similar to bidirectional RNNs (Schuster and Paliwal, 1997). Bidirectional RNNs benefit from left-to-right and right-to-left propagation. They learn the hidden representations separately and concatenate them at the end. We perform a similar encoding: first we learn TD and BU representations independently, and lastly, we concatenate them.</p>
<p>The final representation r<sup>i</sup> is employed in a sequence input of a bidirectional LSTM. For each AMR graph, we generate a node sequence by depth-first traversal order. In particular, given a representation sequence from r<sup>1</sup> to r<sup>n</sup>, the hidden forward and backward states of r<sup>i</sup> are defined as:</p>
<p>$$
\begin{array}{l}
\overrightarrow{\mathbf{h}}<em i-1="i-1">{i} = LSTM_f(\mathbf{r}_i, \overrightarrow{\mathbf{h}}</em>), \
\overrightarrow{\mathbf{h}}<em i-1="i-1">{i} = LSTM_b(\mathbf{r}_i, \overrightarrow{\mathbf{h}}</em>),
\end{array}
$$</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Dual Graph Encoder. The encoder receives the two graph views and generates structural node representations that are used by the decoder. Representations in blue, yellow and orange are $\mathbf{e}<em i="i">{i}, \mathbf{h}</em>}^{t}$ and $\mathbf{h<em f="f">{i}^{b}$, respectively.
where $L S T M</em>}$ is a forward LSTM and $L S T M_{b}$ is a backward LSTM. Note that, for the backward LSTM, we feed the reversed input as the order from $\mathbf{r<em 1="1">{n}$ to $\mathbf{r}</em>$. Lastly, we obtain the final hidden state by concatenating them as:</p>
<p>$$
\mathbf{h}<em i="i">{i}=\left[\stackrel{\leftrightarrow}{\mathbf{h}}</em>\right]
$$} | \stackrel{\leftarrow}{\mathbf{h}}_{i</p>
<p>The resulting hidden state $\mathbf{h}_{i}$ encodes the information of both preceding and following nodes.</p>
<p>Stacking layers was demonstrated to be effective in graph-to-sequence approaches (Marcheggiani and Perez Beltrachini, 2018; KoncelKedziorski et al., 2019; Damonte and Cohen, 2019) and allows us to test for their contributions to the system performance more easily. We employ different GNNs for both graph encoders (Section 3.3). Figure 2 shows the proposed encoder architecture.</p>
<h3>3.3 Graph Neural Networks</h3>
<p>The $G E$ s incorporate, in each node representation, structural information based on both views of the graph. We explore distinct strategies for neighborhood aggregation, adopting three GNNs: Gated Graph Neural Networks (GGNN, Li et al. (2016)), Graph Attention Networks (GAT, Veličković et al. (2018)) and Graph Isomorphic Networks (GIN, Xu et al. (2019)). Each GNN employs a specific message passing scheme which allows capturing different nuances of structural information.</p>
<p>Gated Graph Neural Networks GGNNs employ gated recurrent units to encode node representations, reducing the recurrence to a fixed number of steps. In particular, the $l$-th layer of a GGNN is calculated as:</p>
<p>$$
\mathbf{h}<em i="i">{i}^{(l)}=G R U\left(\mathbf{h}</em>}^{(l-1)}, \sum_{j \in \mathcal{N}(i)} \mathbf{W<em j="j">{\mathbf{1}} \mathbf{h}</em>\right)
$$}^{(l-1)</p>
<p>where $\mathcal{N}(i)$ is the immediate neighborhood of $v_{i}$, $\mathbf{W}_{\mathbf{1}}$ is a parameter and $G R U$ is a gated recurrent unit (Cho et al., 2014). Different from other GNNs, GGNNs use back-propagation through time (BPTT) to learn the parameters. GGNNs also do not require to constrain parameters to ensure convergence.</p>
<p>Graph Attention Networks GATs apply attentive mechanisms to improve the exploitation of non-trivial graph structure. They encode node representations by attending over their neighbors, following a self-attention strategy:</p>
<p>$$
\mathbf{h}<em i="i" i_="i,">{i}^{(l)}=\alpha</em>} \mathbf{W<em i="i">{\mathbf{2}} \mathbf{h}</em>}^{(l-1)}+\sum_{j \in \mathcal{N}(i)} \alpha_{i, j} \mathbf{W<em j="j">{\mathbf{2}} \mathbf{h}</em>
$$}^{(l-1)</p>
<p>where attention coefficients $\alpha_{i, j}$ are computed as:</p>
<p>$$
\alpha_{i, j}=\text { softmax }\left(\sigma\left(\mathbf{a}^{\top}\left[\mathbf{W}<em i="i">{\mathbf{2}} \mathbf{h}</em>}^{(l-1)} | \mathbf{W<em j="j">{\mathbf{2}} \mathbf{h}</em>\right]\right)\right)
$$}^{(l-1)</p>
<p>where $\sigma$ is the activation function and $|$ denotes concatenation. $\mathbf{W}_{\mathbf{2}}$ and $\mathbf{a}$ are model parameters. The virtue of the attention mechanism is its ability to focus on the most important parts of the node neighborhood. In order to learn attention weights in different perspectives, GATs can employ multihead attentions.</p>
<p>Graph Isomorphic Networks GIN is a GNN as powerful as the Weisfeiler-Lehman (WL) graph isomorphism test (Weisfeiler and Lehman, 1968) in representing isomorphic and non-isomorphic graphs with discrete attributes. Its $l$-th layer is defined as:</p>
<p>$$
\mathbf{h}<em _mathbf_W="\mathbf{W">{i}^{(l)}=h</em>}}\left(\mathbf{h<em _in="\in" _mathcal_N="\mathcal{N" j="j">{i}^{(l-1)}+\sum</em>\right)
$$}(i)} \mathbf{h}_{j}^{(l-1)</p>
<p>where $h_{\mathbf{W}}$ is a multi-layer perceptron (MLP). In contrast to other GNNs, which combine node feature with its aggregated neighborhood feature, GINs do not apply the combination step and simply aggregate the node along with its neighbors.</p>
<p>Each of these GNNs applies different approaches to learn structural features from graph data and has achieved impressive results on many graph-based tasks (Li et al., 2016; Veličković et al., 2018; Xu et al., 2019).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">LDC2015E86</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LDC2017T10</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">training, dev and test instances</td>
<td style="text-align: center;">16,833</td>
<td style="text-align: center;">1,368</td>
<td style="text-align: center;">1,371</td>
<td style="text-align: center;">36,521</td>
<td style="text-align: center;">1,368</td>
<td style="text-align: center;">1,371</td>
</tr>
<tr>
<td style="text-align: left;">min, average and max graph diameter</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">6.9</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: left;">min, average and max node degree</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: left;">min, average and max number of nodes</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">151</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">151</td>
</tr>
<tr>
<td style="text-align: left;">min, average and max number of edges</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">172</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">172</td>
</tr>
<tr>
<td style="text-align: left;">number of DAG and non-DAG graphs</td>
<td style="text-align: center;">18,679</td>
<td style="text-align: center;">893</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">37,284</td>
<td style="text-align: center;">1,976</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">min, average and max length sentences</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">225</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">225</td>
</tr>
</tbody>
</table>
<p>Table 1: Data statistics of LDC2015E86 and LDC2017T10 datasets. The values are calculated for all splits (train, development and test sets). DAG stands for directed acyclic graph.</p>
<h3>3.4 Decoder</h3>
<p>An attention-based unidirectional LSTM decoder is used to generate sentences, attending to the hidden representations of edges and nodes. In each step $t$, the decoder receives the word embedding of the previous word (during training, this is the previous word of the reference sentence; at test time it is the previously generated word), and has the decoder state $\mathbf{s}_{t}$. The attention distribution $\mathbf{a}^{t}$ is calculated as in See et al. (2017):</p>
<p>$$
\begin{aligned}
\mathbf{e}<em h="h">{i}^{t} &amp; =\mathbf{v} \cdot \tanh \left(\mathbf{W}</em>} \mathbf{h<em s="s">{i}+\mathbf{W}</em>} \mathbf{s<em c="c">{t}+\mathbf{w}</em>\right) \
\mathbf{a}^{t} &amp; =\operatorname{softmax}\left(\mathbf{e}^{t}\right)
\end{aligned}
$$} \mathbf{s}_{c}+\mathbf{b</p>
<p>where $\mathbf{s}<em h="h">{c}$ is the coverage vector and $\mathbf{v}, \mathbf{W}</em>}, \mathbf{W<em c="c">{s}$, $\mathbf{w}</em>$ are learnable parameters. The coverage vector is the accumulation of all attention distributions so far.}$ and $\mathbf{b</p>
<p>Copy and Coverage Mechanisms Previous works (Damonte and Cohen, 2019; Cao and Clark, 2019) use anonymization to handle names and rare words, alleviating the data sparsity. In contrast, we employ copy and coverage mechanisms to address out-of-vocabulary issues for rare target words and to avoid repetition (See et al., 2017).</p>
<p>The model is trained to optimize the negative log-likelihood:</p>
<p>$$
\mathcal{L}=-\sum_{t=1}^{|Y|} \log p\left(y_{t} \mid y_{1: t-1}, X ; \theta\right)
$$</p>
<p>where $Y=y_{1}, \ldots, y_{|Y|}$ is the sentence, $X$ is the AMR graph and $\theta$ represents the model parameters.</p>
<h2>4 Data</h2>
<p>We use two AMR corpora, LDC2015E86 and LDC2017T10 ${ }^{2}$. In these datasets, each instance</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Distribution of the AMR graph diameter (left) and node degree (right) in the training set for LDC2015E86 (red) and LDC2017T10 (blue) datasets.
contains an AMR graph and a sentence. Table 1 shows the statistics for both datasets. Figure 3 shows the distribution of the AMR graph diameters and node degrees for both datasets. The AMR graph structures are similar for most examples. Note that $90 \%$ of AMR graphs in both datasets have the diameter less than or equal to 11 and $90 \%$ of nodes have the degree of 4 or less. Very structurally similar graphs pose difficulty for the graph encoder by making it harder to learn the differences between their similar structures. Therefore, the word embeddings used as additional input play an important role in helping the model to deal with language information. That is one of the reasons why we concatenate this information in the node representation $\mathbf{r}_{i}$.</p>
<h2>5 Experiments and Discussion</h2>
<p>Implementation Details We extract vocabularies (size of 20,000) from the training sets and initialize the node embeddings from GloVe word embeddings (Pennington et al., 2014) on Common Crawl. Hyperparameters are tuned on the development set of the LDC2015E86 dataset. For GIN, GAT, and GGNN graph encoders, we set the number of layers to 2,5 and 5 , respectively. To regu-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">METEOR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LDC2015E86</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Konstas et al. (2017)</td>
<td style="text-align: center;">22.00</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Song et al. (2018)</td>
<td style="text-align: center;">23.28</td>
<td style="text-align: center;">30.10</td>
</tr>
<tr>
<td style="text-align: center;">Cao et al. (2019)</td>
<td style="text-align: center;">23.50</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Damonte et al.(2019)</td>
<td style="text-align: center;">24.40</td>
<td style="text-align: center;">23.60</td>
</tr>
<tr>
<td style="text-align: center;">Guo et al. (2019)</td>
<td style="text-align: center;">25.70</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">S2S</td>
<td style="text-align: center;">$22.55 \pm 0.17$</td>
<td style="text-align: center;">$29.90 \pm 0.31$</td>
</tr>
<tr>
<td style="text-align: center;">G2S-GIN</td>
<td style="text-align: center;">$22.93 \pm 0.20$</td>
<td style="text-align: center;">$29.72 \pm 0.09$</td>
</tr>
<tr>
<td style="text-align: center;">G2S-GAT</td>
<td style="text-align: center;">$23.42 \pm 0.16$</td>
<td style="text-align: center;">$29.87 \pm 0.14$</td>
</tr>
<tr>
<td style="text-align: center;">G2S-GGNN</td>
<td style="text-align: center;">$24.32 \pm 0.16$</td>
<td style="text-align: center;">$\mathbf{3 0 . 5 3} \pm 0.30$</td>
</tr>
<tr>
<td style="text-align: center;">LDC2017T10</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Back et al. (2018)</td>
<td style="text-align: center;">23.30</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Song et al. (2018)</td>
<td style="text-align: center;">24.86</td>
<td style="text-align: center;">31.56</td>
</tr>
<tr>
<td style="text-align: center;">Damonte et al.(2019)</td>
<td style="text-align: center;">24.54</td>
<td style="text-align: center;">24.07</td>
</tr>
<tr>
<td style="text-align: center;">Cao et al. (2019)</td>
<td style="text-align: center;">26.80</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Guo et al. (2019)</td>
<td style="text-align: center;">27.60</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">S2S</td>
<td style="text-align: center;">$22.73 \pm 0.18$</td>
<td style="text-align: center;">$30.15 \pm 0.14$</td>
</tr>
<tr>
<td style="text-align: center;">G2S-GIN</td>
<td style="text-align: center;">$26.90 \pm 0.19$</td>
<td style="text-align: center;">$32.62 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">G2S-GAT</td>
<td style="text-align: center;">$26.72 \pm 0.20$</td>
<td style="text-align: center;">$32.52 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: center;">G2S-GGNN</td>
<td style="text-align: center;">$\mathbf{2 7 . 8 7} \pm 0.15$</td>
<td style="text-align: center;">$\mathbf{3 3 . 2 1} \pm 0.15$</td>
</tr>
</tbody>
</table>
<p>Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.
larize the model, during training we apply dropout (Srivastava et al., 2014) to the graph layers with a rate of 0.3 . The graph encoder hidden vector sizes are set to 300 and hidden vector sizes for LSTMs are set to 900 .</p>
<p>The models are trained for 30 epochs with early stopping based on the development BLEU score. For our models and the baseline, we used a twolayer LSTM decoder. We use Adam optimization (Kingma and Ba, 2015) as the optimizer with an initial learning rate of 0.001 and 20 as the batch size. Beam search with the beam size of 5 is used for decoding.</p>
<p>Results We call the models G2S-GIN (isomorphic encoder), G2S-GAT (graph-attention encoder), and G2S-GGNN (gated-graph encoder), according to the graph encoder utilized. As a baseline (S2S), we train an attention-based encoderdecoder model with copy and coverage mechanisms, and use a linearized version of the graph generated by depth-first traversal order as input. We compare our models against several state-of-the-art results reported on the two datasets (Konstas et al., 2017; Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Cao and Clark, 2019; Guo et al., 2019).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">External</th>
<th style="text-align: left;">BLEU</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Konstas et al. (2017)</td>
<td style="text-align: left;">200 K</td>
<td style="text-align: left;">27.40</td>
</tr>
<tr>
<td style="text-align: left;">Song et al. (2018)</td>
<td style="text-align: left;">200 K</td>
<td style="text-align: left;">28.20</td>
</tr>
<tr>
<td style="text-align: left;">Guo et al. (2019)</td>
<td style="text-align: left;">200 K</td>
<td style="text-align: left;">31.60</td>
</tr>
<tr>
<td style="text-align: left;">G2S-GGNN</td>
<td style="text-align: left;">200 K</td>
<td style="text-align: left;">$\mathbf{3 2 . 2 3}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Results on LDC2015E86 test set when models are trained with additional Gigaword data.</p>
<p>We use both BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) as evaluation metrics ${ }^{3}$. In order to mitigate the effects of random seeds, we report the averages for 4 training runs of each model along with their standard deviation. Table 2 shows the comparison between the proposed models, the baseline and other neural models on the test set of the two datasets.</p>
<p>For both datasets, our approach substantially outperforms the baselines. In LDC2015E86, G2S-GGNN achieves a BLEU score of 24.32, $4.46 \%$ higher than Song et al. (2018), who also use the copy mechanism. This indicates that our architecture can learn to generate better signals for text generation. On the same dataset, we have competitive results to Damonte and Cohen (2019). However, we do not rely on preprocessing anonymisation not to lose semantic signals. In LDC2017T10, G2S-GGNN achieves a BLEU score of 27.87, which is 3.33 points higher than Damonte and Cohen (2019), a state-of-the-art model that does not employ external information. We also have competitive results to Guo et al. (2019), a very recent state-of-the-art model.</p>
<p>We also outperform Cao and Clark (2019) improving BLEU scores by $3.48 \%$ and $4.00 \%$, in LDC2015E86 and LDC2017T10, respectively. In contrast to their work, we do not rely on (i) leveraging supplementary syntactic information and (ii) we do not require an anonymization preprocessing step. G2S-GIN and G2S-GAT have comparable performance on both datasets. Interestingly, G2S-GGNN has better performance among our models. This suggests that graph encoders based on gating mechanisms are very effective in text generation models. We hypothesize that the gating mechanism can better capture longdistance dependencies between nodes far apart in the graph.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">BLEU</th>
<th style="text-align: left;">METEOR</th>
<th style="text-align: left;">Size</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">biLSTM</td>
<td style="text-align: left;">22.50</td>
<td style="text-align: left;">30.42</td>
<td style="text-align: left;">57.6 M</td>
</tr>
<tr>
<td style="text-align: left;">$G E_{t}+$ biLSTM</td>
<td style="text-align: left;">26.33</td>
<td style="text-align: left;">32.62</td>
<td style="text-align: left;">59.6 M</td>
</tr>
<tr>
<td style="text-align: left;">$G E_{b}+$ biLSTM</td>
<td style="text-align: left;">26.12</td>
<td style="text-align: left;">32.49</td>
<td style="text-align: left;">59.6 M</td>
</tr>
<tr>
<td style="text-align: left;">$G E_{t}+G E_{b}+$ biLSTM</td>
<td style="text-align: left;">27.37</td>
<td style="text-align: left;">33.30</td>
<td style="text-align: left;">61.7 M</td>
</tr>
</tbody>
</table>
<p>Table 4: Results of the ablation study on the LDC2017T10 development set.</p>
<p>Additional Training Data Following previous works (Konstas et al., 2017; Song et al., 2018; Guo et al., 2019), we also evaluate our models employing additional data from English Gigaword corpus (Napoles et al., 2012). We sample 200K Gigaword sentences and use JAMR ${ }^{4}$ (Flanigan et al., 2016a) to parse them. We follow the method of Konstas et al. (2017), which is fine-tuning the model on the LDC2015E86 training set after every epoch of pretraining on the Gigaword data. G2S-GGNN outperforms others with the same amount of Gigaword sentences (200K), achieving a 32.23 BLEU score, as shown in Table 3. The results demonstrate that pretraining on automatically generated AMR graphs enhances the performance of our model.</p>
<p>Ablation Study In Table 4, we report the results of an ablation study on the impact of each component of our model on the development set of LDC2017T10 dataset by removing the graph encoders. We also report the number of parameters (including embeddings) used in each model. The first thing we notice is the huge increase in metric scores ( $17 \%$ in BLEU) when applying the graph encoder layer, as the neural model receives signals regarding the graph structure of the input. The dual representation helps the model with a different view of the graph, increasing BLEU and METEOR scores by 1.04 and 0.68 points, respectively. The complete model has slightly more parameters than the model without graph encoders ( 57.6 M vs 61.7 M ).</p>
<h2>Impact of Graph Size, Arity and Sentence</h2>
<p>Length The good overall performance on the datasets shows the superiority of using graph encoders and dual representations over the sequential encoder. However, we are also interested in estimating the performance of the models concerning different data properties. In order to evaluate how the models handle graph and sentence features, we</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>| Model | Graph Diameter |  |  |  |
| :--: | :--: | :--: | :--: | :--: |
|  | 0-7 | $\Delta$ | 7-13 | 14-20 |
| S2S | 33.2 |  | 29.7 | 28.8 |
| G2S-GIN | $35.2+6.0 \%$ |  | $31.8+7.4 \%$ | $31.5+9.2 \%$ |
| G2S-GAT | $35.1+5.9 \%$ |  | $32.0+7.8 \%$ | $31.5+9.51 \%$ |
| G2S-GGNN | $36.2+9.0 \%$ |  | $33.0+11.4 \%$ | $30.7+6.7 \%$ |
| Sentence Length |  |  |  |  |
|  | 0-20 | $\Delta$ | 20-50 | 50-240 |
| S2S | 34.9 |  | 29.9 | 25.1 |
| G2S-GIN | $36.7+5.2 \%$ |  | $32.2+7.8 \%$ | $26.5+5.8 \%$ |
| G2S-GAT | $36.9+5.7 \%$ |  | $32.3+7.9 \%$ | $26.6+6.1 \%$ |
| G2S-GGNN | $37.9+8.5 \%$ |  | $33.3+11.2 \%$ | $26.9+6.8 \%$ |
| Max Node Out-degree |  |  |  |  |
|  | 0-3 | $\Delta$ | 4-8 | 9-18 |
| S2S | 31.7 |  | 30.0 | 23.9 |
| G2S-GIN | $33.9+6.9 \%$ |  | $32.1+6.9 \%$ | $25.4+6.2 \%$ |
| G2S-GAT | $34.3+8.0 \%$ |  | $32.0+6.7 \%$ | $22.5-6.0 \%$ |
| G2S-GGNN | $35.0+10.3 \%$ |  | $33.1+10.4 \%$ | $22.2-7.3 \%$ |</p>
<p>Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.
perform an inspection based on different sizes of graph diameter, sentence length, and max node out-degree. Table 5 shows METEOR ${ }^{5}$ scores for the LDC2017T10 dataset.</p>
<p>The performances of all models decrease as the diameters of the graphs increase. G2S-GGNN has a $17.9 \%$ higher METEOR score in graphs with a diameter of at most 7 compared to graphs with diameters higher than 13 . This is expected as encoding a bigger graph (containing more information) is harder than encoding smaller graphs. Moreover, $71 \%$ of the graphs in the training set have a diameter less than or equal to 7 and only $2 \%$ have a diameter bigger than 13 (see Figure 3). Since the models have fewer examples of bigger graphs to learn from, this also leads to worse performance when handling graphs with higher diameters. We also investigate the performance with respect to the sentence length. The models have better results when handling sentences with 20 or fewer tokens. Longer sentences pose additional challenges to the models.</p>
<p>G2S-GIN has a better performance in handling graphs with node out-degrees higher than 9. This indicates that GINs can be employed in tasks where the distribution of node degrees has a long</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>|  | REF $\Rightarrow$ GEN |  |  |
| Model | ENT | CON | NEU |
| --- | --- | --- | --- |
| S2S | 38.45 | 11.17 | 50.38 |
| G2S-GIN | 49.78 | 9.80 | 40.42 |
| G2S-GAT | 49.48 | 8.09 | 42.43 |
| G2S-GGNN | 51.32 | 8.82 | 39.86 |
|  |   |   |   |
|  Model | ENT | CON | NEU |
| S2S | 73.79 | 12.75 | 13.46 |
| G2S-GIN | 76.27 | 10.65 | 13.08 |
| G2S-GAT | 77.54 | 8.54 | 13.92 |
| G2S-GGNN | 77.64 | 9.64 | 12.72 |</p>
<p>Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.
tail. Surprisingly, S2S has a better performance than G2S-GGNN and G2S-GAT when handling graphs that contain high degree nodes.</p>
<p>Semantic Equivalence We perform an entailment experiment using BERT (Devlin et al., 2019) fine-tuned on the MultiNLI dataset (Williams et al., 2018) as a NLI model. We are interested in exploring whether a generated sentence (hypothesis) is semantically entailed by the reference sentence (premise). In a related text generation task, Falke et al. (2019) employ NLI models to rerank alternative predicted abstractive summaries.</p>
<p>Nevertheless, uniquely verifying whether the reference (REF) entails the generated sentence (GEN) or vice-versa (GEN entails REF) is not sufficient. For example, suppose that "Today Jon walks" is the REF and "Jon walks" is the GEN. Even though REF entails GEN, GEN does not entail REF, that is, GEN is too general (missing information). Furthermore, suppose that "Jon walks" is the REF and "Today Jon walks" is the GEN, GEN entails REF but REF does not entail GEN, that is, GEN is too specific (added information). Therefore, in addition to verify whether the reference entails the generated sentence, we also verify whether the generated sentence entails the reference.</p>
<p>Table 6 shows the average probabilities for entailment, contradiction and neutral classes on the LDC2017T10 test set. All G2S models have
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Human evaluation of the sentences generated by S2S and G2S-GGNN models. Results are statistically significant with $p&lt;0.05$, using Wilcoxon ranksum test.
higher entailment compared to S2S. G2S-GGNN has $33.5 \%$ and $5.2 \%$ better entailment performances than S2S, when REF entails GEN and GEN entails REF, respectively. G2S models also generate sentences that contradict the reference sentences less. This suggests that our models are capable of capturing better semantic information from the graph generating outputs semantically related to the reference sentences.</p>
<p>Human Evaluation To further assess the quality of the generated sentences, we conduct a human evaluation. We employ the Direct Assessment (DA) method (Graham et al., 2017) via Amazon Mechanical Turk. Using the DA method inspired by Mille et al. (2018), we assess two quality criteria: (i) meaning similarity: how close in meaning the generated text is to the gold sentence; and (ii) readability: how well the generated sentence reads (Is it good fluent English?).</p>
<p>We randomly select 100 sentences generated by S2S and G2S-GGNN and randomly assign them to HITs (following Mechanical Turk terminology). Human workers rate the sentences according to meaning similarity and readability on a 0-100 rating scale. The tasks are executed separately and workers were first given brief instructions. For each sentence, we collect scores from 5 workers and average them. Models are ranked according to the mean of sentence-level scores. We apply a quality control step filtering workers who do not score some faked and known sentences properly.</p>
<p>Figure 4 shows the results. In both metrics, G2S-GGNN has better human scores for meaning similarity and readability, suggesting a higher</p>
<table>
<thead>
<tr>
<th style="text-align: center;">(a / agree :ARG0 (a2 / and :op1 (c / country :wiki China :name (n / name :op1 <br> China)) :op2 (c2 / country :wiki Kyrgyzstan :name (n2 / name :op1 Kyrgyzstan))) :ARG1 (t / threaten-01 :ARG0 (a3 / and :op1 (t2 / terrorism) :op2 (s / separatism) :op3 (e / extremism)) :ARG2 (a4 / and :op1 (s3 / security :mod (r / region)) :op2 (s4 / stability :mod r)) :time (s2 / still) :ARG1-of (m / major-02)) :medium (c3 / communique :mod (j / joint)))</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GOLD</td>
<td style="text-align: center;">China and Kyrgyzstan agreed in a joint communique that terrorism, separatism and extremism still pose major threats to regional security and stability.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">S2S</td>
<td style="text-align: center;">In the joint communique, China and Kyrgyzstan still agreed to threaten terrorism, separatism, extremism and regional stability.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Song et. al (2018)</td>
<td style="text-align: center;">In a joint communique, China and Kyrgyzstan have agreed to still be a major threat to regional security, and regional stability.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">G2S-GGNN</td>
<td style="text-align: center;">At a joint communique, China and Kyrgyzstan agreed that terrorism, separatism and extremism are still a major threat to region security and stability.</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 7: An example of an AMR graph and generated sentences. GOLD refers to the reference sentence.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">ADDED</th>
<th style="text-align: center;">MISS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">S2S</td>
<td style="text-align: center;">47.34</td>
<td style="text-align: center;">37.14</td>
</tr>
<tr>
<td style="text-align: left;">G2S-GIN</td>
<td style="text-align: center;">48.67</td>
<td style="text-align: center;">33.64</td>
</tr>
<tr>
<td style="text-align: left;">G2S-GAT</td>
<td style="text-align: center;">48.24</td>
<td style="text-align: center;">33.73</td>
</tr>
<tr>
<td style="text-align: left;">G2S-GGNN</td>
<td style="text-align: center;">48.66</td>
<td style="text-align: center;">34.06</td>
</tr>
<tr>
<td style="text-align: left;">GOLD</td>
<td style="text-align: center;">50.77</td>
<td style="text-align: center;">28.35</td>
</tr>
</tbody>
</table>
<p>Table 8: Fraction of elements in the output that are not present in the input (ADDED) and the fraction of elements in the input graph that are missing in the generated sentence (MISS), for the test set of LDC2017T10. The token lemmas are used in the comparison. GOLD refers to the reference sentences.
quality of the generated sentences regarding S2S. The Pearson correlations between meaning similarity and readability scores, and METEOR ${ }^{6}$ scores are 0.50 and 0.22 , respectively.</p>
<p>Semantic Adequacy We also evaluate the semantic adequacy of our model (how well does the generated output match the input?) by comparing the number of added and missing tokens that occur in the generated versus reference sentences (GOLD). An added token is one that appears in the generated sentence but not in the input graph. Conversely, a missing token is one that occurs in the input but not in the output. In GOLD, added tokens are mostly function words while missing tokens are typically input concepts that differ from the output lemma. For instance, in Figure 1, there and of are added tokens while person is a missing token. As shown in Table 8, G2S approaches outperform the S2S baseline. G2S-GIN is closest to GOLD with respect to both metrics suggesting that this model is better able to generate novel words to construct the sentence and captures a larger range of concepts from the input AMR graph, covering</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>more information.
Manual Inspection Table 7 shows sentences generated by S2S, Song et al. (2018), G2S-GAT, and the reference sentence. The example shows that our approach correctly verbalises the subject of the embedded clause "China and ... agreed that terrorism, separatism and extremism ${ }_{S U B J} \ldots$ pose major threats to ...", while S2S and Song et al. (2018) are fooled by the fact that agree frequently takes an infinitival argument which shares its subject ("China ...SUBJ agreed to threaten / have agreed to be a major threat"). While this is a single example, it suggests that dual encoding enhances the model ability to take into account the dependencies and the graph structure information, rather than the frequency of n-grams.</p>
<h2>6 Conclusion</h2>
<p>We have studied the problem of generating text from AMR graphs. We introduced a novel architecture that explicitly encodes two parallel and adjuvant representations of the graph (top-down and bottom-up). We showed that our approach outperforms state-of-the-art results in AMR-to-text generation. We provided an extensive evaluation of our models and demonstrated that they are able to achieve the best performance. In the future, we will consider integrating deep generative graph models to express probabilistic dependencies among AMR nodes and edges.</p>
<h2>Acknowledgments</h2>
<p>This work has been supported by the German Research Foundation as part of the Research Training Group Adaptive Preparation of Information from Heterogeneous Sources (AIPHES) under grant No. GRK 1994/1.</p>
<h2>References</h2>
<p>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 178-186, Sofia, Bulgaria. Association for Computational Linguistics.</p>
<p>Daniel Beck, Gholamreza Haffari, and Trevor Cohn. 2018. Graph-to-sequence learning using gated graph neural networks. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 273-283, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Anja Belz, Mike White, Dominic Espinosa, Eric Kow, Deirdre Hogan, and Amanda Stent. 2011. The first surface realisation shared task: Overview and evaluation results. In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation, pages 217-226, Nancy, France. Association for Computational Linguistics.</p>
<p>Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. 2017. Geometric deep learning: Going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):1842.</p>
<p>Kris Cao and Stephen Clark. 2019. Factorising AMR generation through syntax. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2157-2163, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Thiago Castro Ferreira, Iacer Calixto, Sander Wubben, and Emiel Krahmer. 2017. Linguistic realisation as machine translation: Comparing different MT models for AMR-to-text generation. In Proceedings of the 10th International Conference on Natural Language Generation, pages 1-10, Santiago de Compostela, Spain. Association for Computational Linguistics.</p>
<p>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 17241734, Doha, Qatar. Association for Computational Linguistics.</p>
<p>Marco Damonte and Shay B. Cohen. 2019. Structural neural encoders for AMR-to-text generation. In Proceedings of the 2019 Conference of the North</p>
<p>American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3649-3658, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Michael Denkowski and Alon Lavie. 2014. Meteor universal: Language specific translation evaluation for any target language. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 376-380, Baltimore, Maryland, USA. Association for Computational Linguistics.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Tobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych. 2019. Ranking generated summaries by correctness: An interesting but challenging application for natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2214-2220, Florence, Italy. Association for Computational Linguistics.</p>
<p>Jeffrey Flanigan, Chris Dyer, Noah A. Smith, and Jaime Carbonell. 2016a. CMU at SemEval-2016 task 8: Graph-based AMR parsing with infinite ramp loss. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 1202-1206, San Diego, California. Association for Computational Linguistics.</p>
<p>Jeffrey Flanigan, Chris Dyer, Noah A. Smith, and Jaime Carbonell. 2016b. Generation from abstract meaning representation using tree transducers. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 731-739, San Diego, California. Association for Computational Linguistics.</p>
<p>Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. The WebNLG challenge: Generating text from RDF data. In Proceedings of the 10th International Conference on Natural Language Generation, pages 124-133, Santiago de Compostela, Spain. Association for Computational Linguistics.</p>
<p>Yvette Graham, Timohy Baldwin, Alistair Moffat, and Justin Zobel. 2017. Can machine translation systems be evaluated by the crowd alone. Natural Language Engineering, 23(1):3-30.</p>
<p>Zhijiang Guo, Yan Zhang, Zhiyang Teng, and Wei Lu. 2019. Densely connected graph convolutional networks for graph-to-sequence learning. Transactions</p>
<p>of the Association for Computational Linguistics, 7:297-312.</p>
<p>Diederick P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), San Diego, CA, USA.</p>
<p>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL '07, pages 177-180, Stroudsburg, PA, USA. Association for Computational Linguistics.</p>
<p>Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, and Hannaneh Hajishirzi. 2019. Text Generation from Knowledge Graphs with Graph Transformers. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2284-2293, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, and Luke Zettlemoyer. 2017. Neural amr: Sequence-to-sequence models for parsing and generation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 146-157, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Yujia Li, Richard Zemel, Marc Brockschmidt, and Daniel Tarlow. 2016. Gated graph sequence neural networks. In Proceedings of the International Conference on Learning Representations (ICLR), San Juan, Puerto Rico.</p>
<p>Kexin Liao, Logan Lebanoff, and Fei Liu. 2018. Abstract meaning representation for multi-document summarization. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1178-1190, Santa Fe, New Mexico, USA. Association for Computational Linguistics.</p>
<p>Diego Marcheggiani and Laura Perez Beltrachini. 2018. Deep graph convolutional encoders for structured data to text generation. In Proceedings of the 11th International Conference on Natural Language Generation, pages 1-9, Tilburg University, The Netherlands. Association for Computational Linguistics.</p>
<p>Simon Mille, Anja Belz, Bernd Bohnet, Yvette Graham, Emily Pitler, and Leo Wanner. 2018. The first multilingual surface realisation shared task (SR'18): Overview and evaluation results. In Proceedings of the First Workshop on Multilingual Surface Realisation, pages 1-12, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Courtney Napoles, Matthew Gormley, and Benjamin Van Durme. 2012. Annotated gigaword. In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, AKBC-WEKEX '12, pages 95-100, Stroudsburg, PA, USA. Association for Computational Linguistics.</p>
<p>Shashi Narayan and Claire Gardent. 2012. Structuredriven lexicalist generation. In Proceedings of COLING 2012, pages 2027-2042, Mumbai, India. The COLING 2012 Organizing Committee.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL '02, pages 311-318, Stroudsburg, PA, USA. Association for Computational Linguistics.</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532-1543, Doha, Qatar. Association for Computational Linguistics.</p>
<p>Nima Pourdamghani, Kevin Knight, and Ulf Hermjakob. 2016. Generating English from abstract meaning representations. In Proceedings of the 9th International Natural Language Generation conference, pages 21-25, Edinburgh, UK. Association for Computational Linguistics.</p>
<p>Michael Sejr Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. 2018. Modeling relational data with graph convolutional networks. In The Semantic Web - 15th International Conference, ESWC 2018, Heraklion, Crete, Greece, June 3-7, 2018, Proceedings, pages 593-607.</p>
<p>Mike Schuster and Kuldip K. Paliwal. 1997. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, 45(11):2673-2681.</p>
<p>Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointergenerator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10731083, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Stuart M. Shieber, Gertjan van Noord, Fernando C. N. Pereira, and Robert C. Moore. 1990. Semantic-head-driven generation. Computational Linguistics, 16(1):30-42.</p>
<p>Linfeng Song, Daniel Gildea, Yue Zhang, Zhiguo Wang, and Jinsong Su. 2019. Semantic neural machine translation using AMR. Transactions of the Association for Computational Linguistics, 7:19-31.</p>
<p>Linfeng Song, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2018. A graph-to-sequence model for AMR-to-text generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1616-1626, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929-1958.</p>
<p>Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph Attention Networks. In International Conference on Learning Representations, Vancouver, Canada.</p>
<p>Boris Weisfeiler and A.A. Lehman. 1968. A reduction of a graph to a canonical form and an algebra arising during this reduction. Nauchno-Technicheskaya Informatsia, pages 12-16.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How powerful are graph neural networks? In International Conference on Learning Representations, New Orleans, LA, USA.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ METEOR score is used as it is a sentence-level metric.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{5}$ METEOR score is used as it is a sentence-level metric.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
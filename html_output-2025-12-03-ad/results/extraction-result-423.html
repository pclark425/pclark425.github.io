<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-423 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-423</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-423</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-271903728</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.09176v1.pdf" target="_blank">Cognitive LLMs: Towards Integrating Cognitive Architectures and Large Language Models for Manufacturing Decision-making</a></p>
                <p><strong>Paper Abstract:</strong> Resolving the dichotomy between the human-like yet constrained reasoning processes of Cognitive Architectures and the broad but often noisy inference behavior of Large Language Models (LLMs) remains a challenging but exciting pursuit, for enabling reliable machine reasoning capabilities in production systems. Because Cognitive Architectures are famously developed for the purpose of modeling the internal mechanisms of human cognitive decision-making at a computational level, new investigations consider the goal of informing LLMs with the knowledge necessary for replicating such processes, e.g., guided perception, memory, goal-setting, and action. Previous approaches that use LLMs for grounded decision-making struggle with complex reasoning tasks that require slower, deliberate cognition over fast and intuitive inference -- reporting issues related to the lack of sufficient grounding, as in hallucination. To resolve these challenges, we introduce LLM-ACTR, a novel neuro-symbolic architecture that provides human-aligned and versatile decision-making by integrating the ACT-R Cognitive Architecture with LLMs. Our framework extracts and embeds knowledge of ACT-R's internal decision-making process as latent neural representations, injects this information into trainable LLM adapter layers, and fine-tunes the LLMs for downstream prediction. Our experiments on novel Design for Manufacturing tasks show both improved task performance as well as improved grounded decision-making capability of our approach, compared to LLM-only baselines that leverage chain-of-thought reasoning strategies.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e423.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e423.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-ACTR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-ACTR (LLM augmented with ACT-R)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neurosymbolic hybrid that injects latent neural representations distilled from an ACT-R cognitive architecture into an LLM (LlaMa-2 13B) via adapter/LoRA fine-tuning to produce human-aligned, explainable manufacturing decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-ACTR</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LLM-ACTR integrates a rule-based ACT-R cognitive architecture (VSM-ACTR 2.0) with a transformer LLM (LlaMa-2 13B). ACT-R is executed in stochastic simulation to produce timestamped decision traces; those traces are semantically embedded (sentence-transformer), dimension-reduced (SREE + PCA) to a compact latent representation, and then used to augment the LLM. Augmentation is performed by using the LLM's final-layer contextual embeddings together with targets/adapter inputs derived from ACT-R traces and fine-tuning via Low-Rank Adaptation (LoRA) and a softmax classification head (cross-entropy loss, Adam optimizer). The purpose is to ground LLM decisions in human-aligned cognitive process representations to improve grounded decision-making and explainability in Design-for-Manufacturing tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>ACT-R cognitive architecture (VSM-ACTR 2.0): symbolic declarative chunks (decision, decision merits, goal), production-rule system (18 procedural rules across ~20 states), explicit goal/imaginal/procedural buffers, and TD-style utility updating for productions. Representations are symbolic production firings and structured chunks (slot-value).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Transformer neural network (LlaMa-2 13B) providing final-layer contextual embeddings; gradient-based fine-tuning using Adam and LoRA adapters; classification head (softmax) trained with cross-entropy; logistic/multinomial regressions used in auxiliary feature-extraction experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Neural-symbolic assimilation: ACT-R traces -> sentence-transformer embeddings -> SREE to select principal components -> PCA to reduce dimensionality; distilled ACT-R embeddings are used as targets and/or concatenated with LLM final-layer embeddings. Primary integration for deployed model uses adapter-based injection (LoRA) into the LLM and fine-tunes the LLM for classification; an alternative/auxiliary integration concatenates reduced ACT-R embeddings with LLM last-layer embeddings and trains a prediction head (logistic/multinomial regression or neural classification). Ragged traces are padded/imputed when concatenated; LoRA used for computationally-efficient fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Improved human alignment and grounded decision-making (reduced noisy/hallucinatory outputs), better holdout predictive performance vs LLM-only baselines, partial capture of sequential learning trajectories (novice→expert) from ACT-R, and improved explainability via access to ACT-R traces and production utilities. The hybrid shows ability to map LLM contextual representations onto symbolic process components (procedural, imaginal, goal, utility updating) as revealed by PCA/semantic mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Design-for-Manufacturing decision tasks: binary decision (which sector to reduce time: pre-assembly vs assembly) and multi-faceted decision (same binary decision + strategy persona: novice/intermediate/expert). Dataset derived from VSM-ACTR simulations (2,012 trials across 32 problem sets) and prompts formatted for LlaMa.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Hold-out (reported in Table 2): Negative Log-Likelihood (NLL) = 0.6534; Accuracy = 0.6576. Additional breakdown: single-facet target (NLL=0.63, Accuracy=0.64); multi-facet target (NLL=1.18, Accuracy=0.42).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td>LlaMa (pretrained, no fine-tuning) baseline: NLL = 1.1330; Accuracy = 0.3564. Chance-level baseline: NLL = 0.6931; Accuracy = 0.4826.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>The hybrid generalizes better than the pretrained LlaMa baseline on in-distribution holdout tasks (higher accuracy and lower NLL). However, generalization is limited for multi-faceted, higher-complexity decision targets (accuracy drops substantially). The paper reports limited evaluation of true out-of-distribution (OOD) or compositional generalization; limitations due to small ACT-R embedding scale and dataset size reduce robustness to more complex/novel combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High interpretability on the declarative side: ACT-R provides concurrent protocol traces, production-rule firings, and utility updates that can be inspected to explain decisions. The hybrid's latent embeddings preserve semantic mapping to ACT-R components (procedural, imaginal, goal, utility), enabling some explainability of the LLM outputs through mapping back to symbolic traces. However, the injected knowledge becomes latent in LLM weights/embeddings, so interpretability of the neural portion is partial and requires post-hoc alignment to ACT-R traces.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Fails or weakly captures complex multi-faceted decision rationales (strategies) and compositional behaviors; concatenation experiments showed no significant accuracy improvement in some settings, possibly due to scale imbalance (ACT-R reduced embeddings ≪ LlaMa embeddings), ragged tensor handling (padding/imputation may dilute symbolic signals), limited dataset size (preliminary full-trace experiments used only 240 traces), and imperfect human alignment of the ACT-R model; additional engineering required to preserve full semantic space and scale features appropriately.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Complementary strengths / division-of-labor neuro-symbolic principle: use Cognitive Architectures (ACT-R) to provide explicit, human-aligned symbolic procedural/declarative process models (grounding, explainability, reinforcement-driven utilities) and use large neural LLMs for flexible language grounding, in-context representation, and generalization; the hybrid functions by mapping symbolic process traces into the LLM latent space and using adapter-based fine-tuning to bias LLM behavior toward human-like, explainable decision policies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cognitive LLMs: Towards Integrating Cognitive Architectures and Large Language Models for Manufacturing Decision-making', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e423.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e423.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VSM-ACTR 2.0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VSM-ACTR 2.0 (Value Stream Map ACT-R cognitive model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ACT-R based cognitive architecture model specialized for manufacturing decision-making that encodes declarative chunks and production rules and simulates learning across novice→expert personas using TD-style utility updating.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>VSM-ACTR 2.0</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A domain-specific ACT-R cognitive model implementing Value Stream Mapping decision logic for manufacturing. It defines three chunk types (decision, decision merits, goal), ~18 production rules across ~20 states, and meta-cognitive processes for evaluating strategies including headcount-cost reward signals. Learning uses a temporal-difference-like update to production utilities and softmax selection with noise to simulate progression from novice to expert.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Purely symbolic ACT-R representation: declarative chunks (slot-based structures for decisions, merits, goals) and production-rule procedural knowledge (explicit IF-THEN rules), with symbolic buffers (goal, imaginal, procedural) and explicit utility values for productions.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Within-ACT-R algorithmic learning: temporal-difference-style utility updates (Eqn. 1) and softmax production selection with noise (Eqn. 2). While still symbolic, these are iterative algorithmic procedures simulating reinforcement learning dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Used as the declarative knowledge source for the hybrid: stochastic ACT-R simulations produce timestamped traces that are semantically embedded and reduced to form the symbolic-to-neural representation injected into the LLM (either as targets for fine-tuning or concatenated embeddings).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Generates realistic human-like learning trajectories (novice→intermediate→expert), produces concurrent-protocol traces amenable to semantic embedding, and provides temporally-structured explanations (production firings and utility changes) that can be used to ground neural models.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Simulated manufacturing decision-making (Value Stream Map based DFM tasks) used to generate training/validation data and measure learning progression (2012 trials across 32 problem sets).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td>Internal evaluation across 2012 trials: significant positive effect of trial exposure on progression coefficient = 0.086 (p < 0.05); ordered logistic regression threshold novice→intermediate = 0.88 (p < 0.05), intermediate→expert threshold = 0.1 (p = 0.021).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>VSM-ACTR produces consistent simulated learning curves and individual-difference variance is small; however, the CA alone cannot process natural language or generalize across tasks outside its rule set without manual rule creation. Its symbolic generalization is limited by the hand-authored chunk/rule representations.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High interpretability: ACT-R concurrent traces, chunk contents, and production utilities provide direct mechanistic explanations of decisions and learning dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Cannot natively accept natural language inputs or generalize beyond rule-specified domains; requires many pre-defined rules and domain tuning; its distilled embeddings are 'ragged' and variable-length, causing engineering challenges when integrating into neural pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Standard ACT-R cognitive architecture theory: production-system-based cognition with declarative/procedural separation and reinforcement-driven utility learning (temporal difference).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cognitive LLMs: Towards Integrating Cognitive Architectures and Large Language Models for Manufacturing Decision-making', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e423.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e423.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Embedding Concatenation Pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ACT-R-to-LLM Embedding Concatenation and Classifier Pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preliminary hybrid integration variant that concatenates reduced ACT-R semantic embeddings with an LLM's final-layer contextual embeddings and trains a prediction head (logistic/multinomial regression or neural classifier) for behavior prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ACT-R to LLM Embedding Concatenation Pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The pipeline reduces full ACT-R traces into fixed-size semantic vectors (sentence-transformer embeddings followed by SREE and PCA), pads/imputes to handle variable-length ('ragged') runs, transposes to (1, X) tensors, concatenates these with LlaMa final-layer contextual embeddings for the same prompt, and trains a classifier (regularized logistic or multinomial regression, or a neural classification head) to predict decisions/strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Distilled ACT-R trace embeddings representing symbolic process components (procedural matches, imaginal buffer accesses, utility updates, decision actions) reduced via PCA.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>LLM final-layer contextual embeddings (LlaMa-2 13B) plus gradient-free/linear classifiers (regularized logistic regression, multinomial regression with L2) or simple neural prediction head trained with cross-entropy for concatenated embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Feature-level fusion (vector concatenation) of symbolic-derived reduced ACT-R embeddings and LLM contextual embeddings; handling of variable-length traces via padding/imputation; classifiers trained on concatenated features. This is distinct from adapter-based fine-tuning (LoRA) and represents a modular, non end-to-end fusion.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Intended to combine explicit symbolic process semantics with rich contextual LLM features to improve behavior prediction and provide interpretable inputs; empirically produced slight improvements in NLL in some experiments but not consistent accuracy gains, indicating limited emergent synergy in preliminary runs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Same Design-for-Manufacturing decision tasks (single-facet and multi-facet targets) derived from VSM-ACTR simulations; preliminary experiments used 240 full traces (and other experiments used larger synthetic sets).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Preliminary metrics (Appendix/Future Work): concatenated embeddings vs LlaMa-only: prediction accuracy = 0.71 for both datasets (no accuracy gain); concatenated NLL = 0.9535 vs LlaMa-only NLL = 0.9553 (slight NLL improvement). Earlier concatenation experiment on 240 traces showed no significant improvement in accuracy over LLM-only.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td>LlaMa-only embeddings in the same experiment: prediction accuracy = 0.71; NLL = 0.9553.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Preliminary results show no consistent accuracy advantage; slight NLL improvements indicate limited calibration benefits but not robust generalization gains. Problems include scale imbalance (ACT-R embeddings small relative to LlaMa embeddings, roughly 1:10), ragged-tensor padding dilution, and small dataset size — all limiting generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Because ACT-R embeddings are explicit reductions of symbolic traces, the concatenated feature vector preserves some symbolic semantics that can be inspected; however, padding/imputation and PCA reduction obscure raw symbolic structure, and classifier decisions remain partly opaque.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No reliable accuracy improvement observed; sensitive to embedding scale mismatch (ACT-R << LlaMa), ragged tensor handling (padding/imputation can dilute signal), and small sample of full traces (240) in preliminary tests. Additional engineering (longer traces, better preservation of full embeddings, scaling/normalization) required.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Modular feature fusion neuro-symbolic approach: explicit feature-level union of symbolic process semantics and neural contextual representations with a downstream classifier, relying on the hypothesis that symbolic semantics augment neural features for improved prediction/calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cognitive LLMs: Towards Integrating Cognitive Architectures and Large Language Models for Manufacturing Decision-making', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Enabling High-Level Machine Reasoning with Cognitive Neuro-Symbolic Systems <em>(Rating: 2)</em></li>
                <li>Generalizable neuro-symbolic systems for commonsense question answering <em>(Rating: 2)</em></li>
                <li>LoRA: Low-Rank Adaptation of Large Language Models <em>(Rating: 2)</em></li>
                <li>Cognitive architectures for language agents <em>(Rating: 2)</em></li>
                <li>Turning Large Language Models into Cognitive Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-423",
    "paper_id": "paper-271903728",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "LLM-ACTR",
            "name_full": "LLM-ACTR (LLM augmented with ACT-R)",
            "brief_description": "A neurosymbolic hybrid that injects latent neural representations distilled from an ACT-R cognitive architecture into an LLM (LlaMa-2 13B) via adapter/LoRA fine-tuning to produce human-aligned, explainable manufacturing decision-making.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLM-ACTR",
            "system_description": "LLM-ACTR integrates a rule-based ACT-R cognitive architecture (VSM-ACTR 2.0) with a transformer LLM (LlaMa-2 13B). ACT-R is executed in stochastic simulation to produce timestamped decision traces; those traces are semantically embedded (sentence-transformer), dimension-reduced (SREE + PCA) to a compact latent representation, and then used to augment the LLM. Augmentation is performed by using the LLM's final-layer contextual embeddings together with targets/adapter inputs derived from ACT-R traces and fine-tuning via Low-Rank Adaptation (LoRA) and a softmax classification head (cross-entropy loss, Adam optimizer). The purpose is to ground LLM decisions in human-aligned cognitive process representations to improve grounded decision-making and explainability in Design-for-Manufacturing tasks.",
            "declarative_component": "ACT-R cognitive architecture (VSM-ACTR 2.0): symbolic declarative chunks (decision, decision merits, goal), production-rule system (18 procedural rules across ~20 states), explicit goal/imaginal/procedural buffers, and TD-style utility updating for productions. Representations are symbolic production firings and structured chunks (slot-value).",
            "imperative_component": "Transformer neural network (LlaMa-2 13B) providing final-layer contextual embeddings; gradient-based fine-tuning using Adam and LoRA adapters; classification head (softmax) trained with cross-entropy; logistic/multinomial regressions used in auxiliary feature-extraction experiments.",
            "integration_method": "Neural-symbolic assimilation: ACT-R traces -&gt; sentence-transformer embeddings -&gt; SREE to select principal components -&gt; PCA to reduce dimensionality; distilled ACT-R embeddings are used as targets and/or concatenated with LLM final-layer embeddings. Primary integration for deployed model uses adapter-based injection (LoRA) into the LLM and fine-tunes the LLM for classification; an alternative/auxiliary integration concatenates reduced ACT-R embeddings with LLM last-layer embeddings and trains a prediction head (logistic/multinomial regression or neural classification). Ragged traces are padded/imputed when concatenated; LoRA used for computationally-efficient fine-tuning.",
            "emergent_properties": "Improved human alignment and grounded decision-making (reduced noisy/hallucinatory outputs), better holdout predictive performance vs LLM-only baselines, partial capture of sequential learning trajectories (novice→expert) from ACT-R, and improved explainability via access to ACT-R traces and production utilities. The hybrid shows ability to map LLM contextual representations onto symbolic process components (procedural, imaginal, goal, utility updating) as revealed by PCA/semantic mapping.",
            "task_or_benchmark": "Design-for-Manufacturing decision tasks: binary decision (which sector to reduce time: pre-assembly vs assembly) and multi-faceted decision (same binary decision + strategy persona: novice/intermediate/expert). Dataset derived from VSM-ACTR simulations (2,012 trials across 32 problem sets) and prompts formatted for LlaMa.",
            "hybrid_performance": "Hold-out (reported in Table 2): Negative Log-Likelihood (NLL) = 0.6534; Accuracy = 0.6576. Additional breakdown: single-facet target (NLL=0.63, Accuracy=0.64); multi-facet target (NLL=1.18, Accuracy=0.42).",
            "declarative_only_performance": null,
            "imperative_only_performance": "LlaMa (pretrained, no fine-tuning) baseline: NLL = 1.1330; Accuracy = 0.3564. Chance-level baseline: NLL = 0.6931; Accuracy = 0.4826.",
            "has_comparative_results": true,
            "generalization_properties": "The hybrid generalizes better than the pretrained LlaMa baseline on in-distribution holdout tasks (higher accuracy and lower NLL). However, generalization is limited for multi-faceted, higher-complexity decision targets (accuracy drops substantially). The paper reports limited evaluation of true out-of-distribution (OOD) or compositional generalization; limitations due to small ACT-R embedding scale and dataset size reduce robustness to more complex/novel combinations.",
            "interpretability_properties": "High interpretability on the declarative side: ACT-R provides concurrent protocol traces, production-rule firings, and utility updates that can be inspected to explain decisions. The hybrid's latent embeddings preserve semantic mapping to ACT-R components (procedural, imaginal, goal, utility), enabling some explainability of the LLM outputs through mapping back to symbolic traces. However, the injected knowledge becomes latent in LLM weights/embeddings, so interpretability of the neural portion is partial and requires post-hoc alignment to ACT-R traces.",
            "limitations_or_failures": "Fails or weakly captures complex multi-faceted decision rationales (strategies) and compositional behaviors; concatenation experiments showed no significant accuracy improvement in some settings, possibly due to scale imbalance (ACT-R reduced embeddings ≪ LlaMa embeddings), ragged tensor handling (padding/imputation may dilute symbolic signals), limited dataset size (preliminary full-trace experiments used only 240 traces), and imperfect human alignment of the ACT-R model; additional engineering required to preserve full semantic space and scale features appropriately.",
            "theoretical_framework": "Complementary strengths / division-of-labor neuro-symbolic principle: use Cognitive Architectures (ACT-R) to provide explicit, human-aligned symbolic procedural/declarative process models (grounding, explainability, reinforcement-driven utilities) and use large neural LLMs for flexible language grounding, in-context representation, and generalization; the hybrid functions by mapping symbolic process traces into the LLM latent space and using adapter-based fine-tuning to bias LLM behavior toward human-like, explainable decision policies.",
            "uuid": "e423.0",
            "source_info": {
                "paper_title": "Cognitive LLMs: Towards Integrating Cognitive Architectures and Large Language Models for Manufacturing Decision-making",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "VSM-ACTR 2.0",
            "name_full": "VSM-ACTR 2.0 (Value Stream Map ACT-R cognitive model)",
            "brief_description": "An ACT-R based cognitive architecture model specialized for manufacturing decision-making that encodes declarative chunks and production rules and simulates learning across novice→expert personas using TD-style utility updating.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "VSM-ACTR 2.0",
            "system_description": "A domain-specific ACT-R cognitive model implementing Value Stream Mapping decision logic for manufacturing. It defines three chunk types (decision, decision merits, goal), ~18 production rules across ~20 states, and meta-cognitive processes for evaluating strategies including headcount-cost reward signals. Learning uses a temporal-difference-like update to production utilities and softmax selection with noise to simulate progression from novice to expert.",
            "declarative_component": "Purely symbolic ACT-R representation: declarative chunks (slot-based structures for decisions, merits, goals) and production-rule procedural knowledge (explicit IF-THEN rules), with symbolic buffers (goal, imaginal, procedural) and explicit utility values for productions.",
            "imperative_component": "Within-ACT-R algorithmic learning: temporal-difference-style utility updates (Eqn. 1) and softmax production selection with noise (Eqn. 2). While still symbolic, these are iterative algorithmic procedures simulating reinforcement learning dynamics.",
            "integration_method": "Used as the declarative knowledge source for the hybrid: stochastic ACT-R simulations produce timestamped traces that are semantically embedded and reduced to form the symbolic-to-neural representation injected into the LLM (either as targets for fine-tuning or concatenated embeddings).",
            "emergent_properties": "Generates realistic human-like learning trajectories (novice→intermediate→expert), produces concurrent-protocol traces amenable to semantic embedding, and provides temporally-structured explanations (production firings and utility changes) that can be used to ground neural models.",
            "task_or_benchmark": "Simulated manufacturing decision-making (Value Stream Map based DFM tasks) used to generate training/validation data and measure learning progression (2012 trials across 32 problem sets).",
            "hybrid_performance": null,
            "declarative_only_performance": "Internal evaluation across 2012 trials: significant positive effect of trial exposure on progression coefficient = 0.086 (p &lt; 0.05); ordered logistic regression threshold novice→intermediate = 0.88 (p &lt; 0.05), intermediate→expert threshold = 0.1 (p = 0.021).",
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "VSM-ACTR produces consistent simulated learning curves and individual-difference variance is small; however, the CA alone cannot process natural language or generalize across tasks outside its rule set without manual rule creation. Its symbolic generalization is limited by the hand-authored chunk/rule representations.",
            "interpretability_properties": "High interpretability: ACT-R concurrent traces, chunk contents, and production utilities provide direct mechanistic explanations of decisions and learning dynamics.",
            "limitations_or_failures": "Cannot natively accept natural language inputs or generalize beyond rule-specified domains; requires many pre-defined rules and domain tuning; its distilled embeddings are 'ragged' and variable-length, causing engineering challenges when integrating into neural pipelines.",
            "theoretical_framework": "Standard ACT-R cognitive architecture theory: production-system-based cognition with declarative/procedural separation and reinforcement-driven utility learning (temporal difference).",
            "uuid": "e423.1",
            "source_info": {
                "paper_title": "Cognitive LLMs: Towards Integrating Cognitive Architectures and Large Language Models for Manufacturing Decision-making",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Embedding Concatenation Pipeline",
            "name_full": "ACT-R-to-LLM Embedding Concatenation and Classifier Pipeline",
            "brief_description": "A preliminary hybrid integration variant that concatenates reduced ACT-R semantic embeddings with an LLM's final-layer contextual embeddings and trains a prediction head (logistic/multinomial regression or neural classifier) for behavior prediction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ACT-R to LLM Embedding Concatenation Pipeline",
            "system_description": "The pipeline reduces full ACT-R traces into fixed-size semantic vectors (sentence-transformer embeddings followed by SREE and PCA), pads/imputes to handle variable-length ('ragged') runs, transposes to (1, X) tensors, concatenates these with LlaMa final-layer contextual embeddings for the same prompt, and trains a classifier (regularized logistic or multinomial regression, or a neural classification head) to predict decisions/strategies.",
            "declarative_component": "Distilled ACT-R trace embeddings representing symbolic process components (procedural matches, imaginal buffer accesses, utility updates, decision actions) reduced via PCA.",
            "imperative_component": "LLM final-layer contextual embeddings (LlaMa-2 13B) plus gradient-free/linear classifiers (regularized logistic regression, multinomial regression with L2) or simple neural prediction head trained with cross-entropy for concatenated embeddings.",
            "integration_method": "Feature-level fusion (vector concatenation) of symbolic-derived reduced ACT-R embeddings and LLM contextual embeddings; handling of variable-length traces via padding/imputation; classifiers trained on concatenated features. This is distinct from adapter-based fine-tuning (LoRA) and represents a modular, non end-to-end fusion.",
            "emergent_properties": "Intended to combine explicit symbolic process semantics with rich contextual LLM features to improve behavior prediction and provide interpretable inputs; empirically produced slight improvements in NLL in some experiments but not consistent accuracy gains, indicating limited emergent synergy in preliminary runs.",
            "task_or_benchmark": "Same Design-for-Manufacturing decision tasks (single-facet and multi-facet targets) derived from VSM-ACTR simulations; preliminary experiments used 240 full traces (and other experiments used larger synthetic sets).",
            "hybrid_performance": "Preliminary metrics (Appendix/Future Work): concatenated embeddings vs LlaMa-only: prediction accuracy = 0.71 for both datasets (no accuracy gain); concatenated NLL = 0.9535 vs LlaMa-only NLL = 0.9553 (slight NLL improvement). Earlier concatenation experiment on 240 traces showed no significant improvement in accuracy over LLM-only.",
            "declarative_only_performance": null,
            "imperative_only_performance": "LlaMa-only embeddings in the same experiment: prediction accuracy = 0.71; NLL = 0.9553.",
            "has_comparative_results": true,
            "generalization_properties": "Preliminary results show no consistent accuracy advantage; slight NLL improvements indicate limited calibration benefits but not robust generalization gains. Problems include scale imbalance (ACT-R embeddings small relative to LlaMa embeddings, roughly 1:10), ragged-tensor padding dilution, and small dataset size — all limiting generalization.",
            "interpretability_properties": "Because ACT-R embeddings are explicit reductions of symbolic traces, the concatenated feature vector preserves some symbolic semantics that can be inspected; however, padding/imputation and PCA reduction obscure raw symbolic structure, and classifier decisions remain partly opaque.",
            "limitations_or_failures": "No reliable accuracy improvement observed; sensitive to embedding scale mismatch (ACT-R &lt;&lt; LlaMa), ragged tensor handling (padding/imputation can dilute signal), and small sample of full traces (240) in preliminary tests. Additional engineering (longer traces, better preservation of full embeddings, scaling/normalization) required.",
            "theoretical_framework": "Modular feature fusion neuro-symbolic approach: explicit feature-level union of symbolic process semantics and neural contextual representations with a downstream classifier, relying on the hypothesis that symbolic semantics augment neural features for improved prediction/calibration.",
            "uuid": "e423.2",
            "source_info": {
                "paper_title": "Cognitive LLMs: Towards Integrating Cognitive Architectures and Large Language Models for Manufacturing Decision-making",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Enabling High-Level Machine Reasoning with Cognitive Neuro-Symbolic Systems",
            "rating": 2,
            "sanitized_title": "enabling_highlevel_machine_reasoning_with_cognitive_neurosymbolic_systems"
        },
        {
            "paper_title": "Generalizable neuro-symbolic systems for commonsense question answering",
            "rating": 2,
            "sanitized_title": "generalizable_neurosymbolic_systems_for_commonsense_question_answering"
        },
        {
            "paper_title": "LoRA: Low-Rank Adaptation of Large Language Models",
            "rating": 2,
            "sanitized_title": "lora_lowrank_adaptation_of_large_language_models"
        },
        {
            "paper_title": "Cognitive architectures for language agents",
            "rating": 2,
            "sanitized_title": "cognitive_architectures_for_language_agents"
        },
        {
            "paper_title": "Turning Large Language Models into Cognitive Models",
            "rating": 1,
            "sanitized_title": "turning_large_language_models_into_cognitive_models"
        }
    ],
    "cost": 0.016593249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Cognitive LLMs: Towards Integrating Cognitive Architectures and Large Language Models for Manufacturing Decision-making
17 Aug 2024</p>
<p>Siyu Wu 
College of Information Sciences and Technology
The Pennsylvania State University
University ParkUSA</p>
<p>Alessandro Oltramari alessandro.oltramari@us.bosch.com 
Bosch Center for Artificial Intelligence
PittsburghUSA</p>
<p>Carnegie Mellon University
PittsburghUSA</p>
<p>Jonathan Francis jon.francis@us.bosch.com 
Bosch Center for Artificial Intelligence
PittsburghUSA</p>
<p>Carnegie Mellon University
PittsburghUSA</p>
<p>C Lee Giles 
College of Information Sciences and Technology
The Pennsylvania State University
University ParkUSA</p>
<p>Frank E Ritter frank.ritter@psu.edu 
College of Information Sciences and Technology
The Pennsylvania State University
University ParkUSA</p>
<p>Cognitive LLMs: Towards Integrating Cognitive Architectures and Large Language Models for Manufacturing Decision-making
17 Aug 2024299B0FF1E121A027F4DA1AB665D934FBarXiv:2408.09176v1[cs.AI]
Resolving the dichotomy between the human-like yet constrained reasoning processes of Cognitive Architectures and the broad but often noisy inference behavior of Large Language Models (LLMs) remains a challenging but exciting pursuit, for enabling reliable machine reasoning capabilities in production systems.Because Cognitive Architectures are famously developed for the purpose of modeling the internal mechanisms of human cognitive decision-making at a computational level, new investigations consider the goal of informing LLMs with the knowledge necessary for replicating such processes, e.g., guided perception, memory, goal-setting, and action.Previous approaches that use LLMs for grounded decision-making struggle with complex reasoning tasks that require slower, deliberate cognition over fast and intuitive inference-reporting issues related to the lack of sufficient grounding, as in hallucination.To resolve these challenges, we introduce LLM-ACTR, a novel neurosymbolic architecture that provides human-aligned and versatile decision-making by integrating the ACT-R Cognitive Architecture with LLMs.Our framework extracts and embeds knowledge of ACT-R's internal decision-making process as latent neural representations, injects this information into trainable LLM adapter layers, and fine-tunes the LLMs for downstream prediction.Our experiments on novel Design for Manufacturing tasks show both improved task performance as well as improved grounded decision-making capability of our approach, compared to LLM-only baselines that leverage chain-of-thought reasoning strategies.</p>
<p>Introduction</p>
<p>Large-capacity neural foundation models, such as Large Language Models (LLMs), have gained considerable popularity for a wide range of prediction and decision-making tasks, spanning applications, such as robotics and control, neural question-answering, scene understanding, code generation, mathematical reasoning, etc. LLMs are trained on massive datasets, can be used both as discriminative scoring functions as well as generative models, and their capacity allows them to accumulate and retain vast amounts of knowledge (Brown et al. 2020;Andreas 2022;Dong et al. 2022;Francis et al. 2022;Hu et al. 2023).On the surface, typical usage of LLMs mirrors 'system-1 reasoning processes' (Sloman 1996;Hagendorff, Fabi, and Kosinski 2023), from the dual-process theory of human cogni-tion (Wason and Evans 1974; Kahneman 2011), which provide fast, intuitive, and automatic reasoning-underpinning tasks like navigating daily environments and making quick decisions.Advancements in multi-agent LLM frameworks as well as emergent capabilities such as in-context learning (Coda-Forno et al. 2024;Dong et al. 2022) have enabled LLMs to employ more sophisticated reasoning strategies, such as 'chain-of-thought' reasoning (CoT) (Bhattamishra et al. 2023).These capabilities facilitate LLMs' pursuit of 'system-2 processes' (Tversky and Kahneman 1974), which involve slower, deliberate cognition and critical thinking for complex tasks (Brown et al. 2020;Webb, Holyoak, and Lu 2022)-essential for decision-making in realistic settings.While LLMs have shown promise in this area, key concerns remain, e.g., over discrepancies between LLM inference behavior and human reasoning (Binz and Schulz 2023;Liu et al. 2024), in analyses showing that LLMs prioritize fast and intuitive "system-1" thinking over slower and deliberate analysis (Hagendorff, Fabi, and Kosinski 2023), and over issues of insufficient grounding such as hallucination (Chakraborty, Ornik, and Driggs-Campbell 2024).These issues raise potential concerns about deployment settings where LLMs are left to perform inference, without having been first grounded on reliable knowledge sources or decision processes (Yao et al. 2023).</p>
<p>To alleviate these issues, we propose LLM-ACTR, which shows improved decision-making capabilities over LLMs by integrating intermediate representations extracted from a well-establish neuro-symbolic system: the ACT-R cognitive architecture (Anderson 2009;Ritter, Tehranchi, and Oury 2023).ACT-R offers an integrated theory of the mind -encompassing perception, memory, goal-setting, and action -and has been pivotal in developing synthetic agents for learning and training (Anderson et al. 2019).The representation extracted from ACT-R cognitive models serves as domain knowledge, infusing LLMs with decision-making augmentation.LLM-ACTR uses ACT-R models to represent human repeated decision-making with learning.We infuse ACT-R model's intermediate representations with the last hidden layers of open source LLM, and add a top classification layer for fine-tuning.The architecture is then deployed in unseen decision-making tasks.The LlaMa model family (Touvron et al. 2023) was selected for this research, due to its full accessibility to network architecture, including its pre-trained weights, and its proven efficacy in prior applications involving the extraction of the last hidden layer for predicting behavior discrepancies (Binz and Schulz 2024).This approach integrates the ACT-R model's representation of human-like decision-making patterns into the LLM, enhancing its ability to make decisions that are both humanaligned and explainable.The fine-tuned LLM transcends mere prediction of human decisions for unseen problems.Significantly, it outlines a road-map for enabling high-level machine reasoning through cognitive neuro-symbolic systems (Oltramari 2023).LLM-ACTR leverages the strengths of both LLMs and CAs by using LLMs' natural language processing and generative capabilities, complemented by the human-aligned reasoning and explainability offered by CAs.</p>
<p>This paper presents a deployment case of LLM-ACTR in manufacturing decision-making, demonstrating how this approach addresses the typically noisy inference behavior associated with off-the-shelf LLMs in real-world settings.The task is associated with the key aspect of DFM: enhancing product development and optimizing production system performance by improving time efficiency and reducing headcount costs (Ulrich et al. 1993).In this work, we introduce Cognitive LLMs, describe our proposed framework LLM-ACTR, and provide experimental comparisons on a novel task with strong baselines.</p>
<p>Related Work</p>
<p>Cognitive Decision Making The exploration of human cognition and decision-making processes has been a longstanding pursuit in cognitive science and artificial intelligence research (Gluck and Pew 2006).Two prominent frameworks for cognitive modeling are ACT-R (Anderson 2009;Bothell 2017) and Soar (Laird 2012): these frameworks serve as robust tools for simulating human behavior across various cognitive tasks.They are referred to as Cognitive architectures (CAs) (Laird 2012;Anderson 1998), reflecting a set of intertwined mechanisms to model human behavior and aiming for a unified representation of mind (Newell 1994).CAs use task-specific knowledge to generate behavior.They represent various types of knowledge, including declarative (factual), procedural (how-to), and even recent advancements in perception and motor skills.This knowledge allows CAs to not only simulate behavior but also to explain it, both through direct examination and by tracing the reasoning steps involved in real-time (concurrent protocol).ACT-R encompasses perception, memory, goal-setting, and action, and it has been widely applied in decision-making tasks across psychology and computer science e.g., (Best and Lebiere 2003;Wu et al. 2023;Gonzalez, Lerch, and Lebiere 2003).ACT-R was chosen for this study to provide the intermediate representations of real time reasoning steps.However, the ACT-R model does not accept natural language as input and cannot generalize across different tasks, even within the same domain, which limits its flexibility for decision-making.In contrast, LLM-ACTR combines the strengths of both LLMs and ACT-R models by leveraging the natural language processing and generative capabilities of LLMs, and making decisions that are grounded by those of ACT-R models.</p>
<p>Integration of Cognitive Architectures and LLMs</p>
<p>CAs face limitations due to domain restrictions, which have hampered their broader application.They are incapable of processing natural language, they are limited to areas that can be described by logical rules, and they require a significant number of pre-defined rules to function.Intriguingly, LLMs (Brown et al. 2020) offer potential solutions to mitigating these weaknesses.LLMs can process a variety of text inputs and are more flexible than rule-based systems.Additionally, they seem to learn rules implicitly, through pre-training, eliminating the need for manual rule creation.Hence, the notion of integrating CAs and LLMs is attractive, for leveraging the strengths of both approaches and thereby creating a more robust unified theory of computational models.This integration can take several forms, however, e.g., leveraging the implicit world knowledge of LLMs to replace the CAs' declarative knowledge mechanisms or to enhance their traditional symbolic mechanisms for procedural knowledge (Wray, Kirk, and Laird 2024;Kirk et al. 2024).Additional research explores how principles from cognitive architectures can guide the design of LLM-based agent frameworks (Sumers et al. 2023), demonstrating a comprehensive integration effort that spans from knowledge representation to interaction with the environment.However, to our knowledge, unlike these previous efforts that incorporate LLMs into CAs, there is currently no research focusing on assimilating the advantages of CAs into LLMs.In this paper, we leverage a cognitive architecture to ground the reasoning process and outputs of LLMs; by assimilating a neural representation of ACT-R model within LLMs, we aim to enhance LLMs' human alignment and explainability.</p>
<p>Problem Definition: Design for Manufacturing</p>
<p>We define the terminology that constitutes our problem.The problem setting is a prototypical manufacturing productionline workflow, from supplier to customer, for which there exists a Value Stream Map (VSM; see Figure 2), which allows for tracking the efficiency at different sectors of the process and abstracts the overall problem for mathematical modeling and optimization.Key sectors include: Body Production, Pre-Assembly, Assembly, Honing, Washing, Testing, and Packaging.Early sectors pose potential efficiency problems in the workflow and may warrant optimization (triangles), while later stages are governed by First-In-First-Out (FIFO) processes.The metrics at each stage include Cycle Time (CT), Overall Equipment Effectiveness (OEE), and Mean Absolute Error (MAE); the flow progresses through each stage, aiming for efficient operation, performance monitoring, and error minimization to ensure high-quality production output and timely customer delivery.</p>
<p>Focused on maintaining stable output for manufacturing plants, we consider plant managers' feedback alongside the VSM structure to define two decision-making problems that aim to reduce Total Assembly Time (TAT) while minimizing Total Defect Rate (TDR).An agent G is a predictive model that takes a natural language question Q as a prompt, along with N snapshots of the sector-wise production flow data {CT, OEE, MAE}.In a single-facet decision-making problem, G outputs a binary decision (0 or 1) on which of two sectors, pre-assembly or assembly, requires a time reduction.In a more-challenging multi-faceted decision-making problem, G should output the same binary decision as before, about which sector should be the optimization target, along with an optimization strategy S. Here, S is a strategy defined by one of several decision-making personas that govern manufacturing process management, which we refer to in the manuscript as 'novice', 'intermediate', and 'expert'.</p>
<p>Cognitive LLMs: Hybrid Architectures for Human-Aligned Decision Making</p>
<p>We start by providing a brief background on the central components of the ACT-R Cognitive model, before providing details about our proposed Cognitive LLM framework, LLM-ACTR.Our approach demonstrates essential character-istics, derived from ACT-R cognitive model, which are crucial for augmenting decision-making using foundation models with cognitive reasoning.</p>
<p>Cognitive Architectures based on ACT-R</p>
<p>LLM-ACTR relies on an ACT-R cognitive model capable of (1) executing tasks from problem definition using decisionmaking behaviors observed in humans and retrieving knowledge representations similarly, (2) integrating personas ranging from novice to intermediate and expert levels, and (3) simulating the reinforcement learning processes of decisionmakers as they transition from novice to expert.</p>
<p>Human-aligned Cognitive Models We released VSM-ACTR 1.0 (see Background: the VSM-ACTR 1.0 Model; Authors (2024)), which is a rule-based ACT-R cognitive decision-making model for manufacturing decisionmaking that implements multiple problem-solving strategies, through a combination of production rules.We now provide a new version of this model, VSM-ACTR 2.0, which has incorporated the meta-cognitive processes that reflect on and evaluate the progress of chosen strategies-with an emphasis on headcount cost evaluation, through a reward structure that enables a process akin to reinforcement learning.</p>
<p>This system allows the model to dynamically assess the impact of headcount costs on decision-making outcomes, computing a reward or penalty for each decision cycle.These rewards or penalties then propagate back to the initial production rule that initiated the decision cycle, thereby dynamically adjusting the utility of each decision-making strategy.VSM-ACTR 2.0 integrates the prototypical decision process with insights into how cognitive models represent different levels of expertise (Blessing and Anderson 1996;Martin, Gonzalez, and Lebiere 2004), categorizing users into three levels of expertise: novices, intermediates, and experts.Novices engage in decision-making using intuitive deliberative chunks.Intermediates can manage key metrics such as CT and OEE but struggle with the systematic analysis of intertwined variables.Experts, on the other hand, make judgments systematically.The cognitive model employs three types of knowledge chunks: decisions, decision merits, and goals.The 'decision chunk' encodes eight slots including reduction time (goal), decision-making state (novice, intermediate, expert), and related variables.The 'decision merits chunk' holds information on sector weights, defect increases by sector, and comparative defect rate increases.The 'goal chunk' captures the initial production conditions and the ultimate goal of achieving the optimal decision.In addition, the model uses 18 procedural rules driven by goal-focused objectives across 20 states, covering actions such as choosing strategies, actions, working memory management, decisions, and evaluations.</p>
<p>Reinforcement Mechanisms in Production Systems</p>
<p>Dopaminergic signals are believed to transmit reinforcement information to the corpus striatum (Schönberg et al. 2007), traditionally signaling reward-related activities.However, these signals are now understood to represent the error signal in the temporal difference (TD) algorithm from reinforcement learning (Sutton and Barto 1999), which is applied in ACT-R's learning mechanism.As expressed in Eqn. 1, Each production rule in the ACT-R model has a utility-a value or strength-associated with it, which is updated using the TD algorithm:
Eqn. 1 : U i (n) = U i (n − 1) + α [R i (n) + U i (n − 1)] ,
where U i (n) represents the value or utility of some item i (e.g., a production) after its n-th occurrence, and R i (n) represents the reward received on the n-th occurrence.The parameter α (0 &lt; α &lt; 1) controls the learning rate.</p>
<p>A key strength of the TD algorithm is its ability to propagate rewards back to earlier critical productions, through a chain of productions, influencing their utilities.This mechanism is tied to the widely-used 'softmax' function, which is also integral to ACT-R's production selection, as expressed in Equation 2. After propagation, if multiple productions compete with expected utility values U j , the probability of of selecting production i is given by:
Eqn. 2 : Probability(i) = e Ui/ √ 2s j e Uj / √ 2s
, where the summation over j is over all the productions that currently have their conditions satisfied; and s is the noise.</p>
<p>To understand the dynamics of the learning mechanism, consider a scenario involving penalties within a decisionmaking process.The reward function R(s, f (x)) calculates the reward at the end of one decision-making round.This function takes two parameters: S, representing the strategy used, and f (x), the outcome of the cost analysis, resulting in either a reward or a penalty.In one decision round, a penalty of -2 is computed due to the use of a novice strategy coupled with an inefficient cost.Factoring in the memory retention effect after a 0.5 time step, the subsequent penalty calculation modifies the impact of the decision: R(S, x) − 0.5 time-steps = −2.5
U (7) = U (6)+α [R(S, x) − 0.5 time step + U (6)] = −1.02 U (7)
represents the utility of novice strategy production at the seventh occurrence of firing.While U (6) represents the utility at sixth occurrences; α is set at 0.2, based on the learning rule from Wagner and Rescorla (1972).This framework allows penalties to retroactively influence previous decisions, thus shaping the model's strategic choices in subsequent rounds.</p>
<p>VSM-ACTR model evaluation</p>
<p>We ran the VSM-ACTR model across 2012 decision-making trials and 32 problem sets to analyze its behavior (Ritter et al. 2011).Each model run comprised 15-16 trials until reach a more stable expert behavior.We encoded decision types as 0, 1, and 2 for novice, intermediate, and expert strategies, respectively.</p>
<p>To assess learning, individual differences, and progression, we initially used descriptive statistics to chart the average progression of decision types over 16 trials.We then employed a mixed linear model to evaluate the influence of trial numbers on decision types, incorporating repeated measures and random effects to account for individual variance.</p>
<p>Additionally, an ordered logistic regression analyzed the relationship between the number of trials and the learning progression from novice to expert.</p>
<p>The results of the descriptive statistics demonstrate a significant positive impact of trial exposure on decision-making progression, evidenced by a coefficient of 0.086 (P &lt; 0.05).A mixed linear model regression confirms the effect of trials on decision-making and further reveals a variance of 0.007 in the random group effects.This indicates that while there are differences between groups, these differences are relatively small, suggesting that the trials themselves predominantly explain the variability in decision type.</p>
<p>Threshold analysis using ordered logistic regression reveals significant transition thresholds.The transition from novice to intermediate has a significant threshold of 0.88 (P &lt; 0.05), indicating a challenging progression to higher decision-making skills.In contrast, the transition from intermediate to expert shows a significantly lower threshold of 0.1 (P = 0.021), suggesting it is easier to progress from intermediate to expert than from novice to intermediate.</p>
<p>Comparing these results to the earlier VSM-ACTR 1.0 version, it's find that the transition from intermediate to expert has become significantly more pronounced.This change is attributed to enhancements in headcount cost evaluation factors that have reinforced the progression from intermediate to expert levels.</p>
<p>The LLM-ACTR Framework</p>
<p>Figure 1 illustrates the approach to creating LLM-ACTR, which begins with the collection of task data and documentation.The task procedures are then modeled using ACT-R, employing stochastic simulations to analyze these tasks on a large scale.After the simulation phase, the generated synthetic data is semi-automatically distilled and combined with prompt requests.This data is subsequently used to infuse into an open source LLM through fine-tuning, resulting in a type of cognitive LLM, named LLM-ACTR.</p>
<p>Selecting Salient Decision Information</p>
<p>The decisionmaking process demonstrates cognitive reasoning through VSM-ACTR's traces, which capture the reasoning steps in real-time using a concurrent protocol (see Example ACT-R Decision Trace in the Appendix).These traces log the cognitive operations executed by various modules at each decision point, including the activation of the goal module to drive decisions, the use of the imaginal buffer for accessing working memory, procedural memory matching and firing, and utility updating driven by reinforcement learning, along with the decision actions.</p>
<p>Reserving information from ACT-R model's decisionmaking traces poses challenges.A single decision-making round can generate a vast number of lines of traces, each timestamped as frequently as every 5 milliseconds.Deciding which lines to select-or whether to preserve all lines-requires a balance between minimizing information loss and reducing computational costs.The rationale for choosing outputs from specific modules as reliable sources within the decision representation lies in their clear correspondence to deterministic cognitive processes.The ratio- nale for preserving all traces involves processes of semantic embedding extraction and dimensional reduction.</p>
<p>The information used to augment decision-making in this study focuses on distilling macro-level cognitive processes related to executive function (Gilbert and Burgess 2008), capturing the evolution of decision-making results across trials and how decisions adapt through learning and experience.Furthermore, the decision actions are categorized into strategy levels (novice, intermediate, expert), reflecting the learning phases.Neurologically, as cognitive strategies evolve from novice to expert, there is a corresponding increase in the efficiency and effectiveness of neural circuits in the prefrontal cortex and basal ganglia in humans (see paragraphs: Implementing a reinforcement-learning mechanism in a production system framework).</p>
<p>Learning an Embedding Space of Decision Traces</p>
<p>The next step involves converting the traces into tensors that the LLM can process.This study explores two approaches: one uses selected traces, and another uses full traces.</p>
<p>The selected traces are components distilled from macrolevel cognitive processes related to executive function.This process requires human involvement to log decision results and strategy traces, which are then numerically encoded.For instance, '0' represents a decision for reduced time in preassembly section, and '1' for assembly.These data are subsequently fed into the neural network as single vectors.</p>
<p>In contrast, the holistic traces approach (see Figure 3a) retains both macro-and micro-level cognitive processes, with the latter including metacognition (Nelson and Narens 1994).Metacognition involves an awareness and understanding of one's own cognitive processes, as exhibited through model traces that demonstrate the use of the imaginal buffer for accessing working memory, procedural memory matching and firing, headcount cost analysis, and the assessment of strategy effectiveness.</p>
<p>The investigation begins with the transformation of full traces from VSM-ACTR, representing both cognitive and metacognitive processes, into a format that balances information retention with computational efficiency.Cognitive reasoning traces for each task are processed through a sentence transformer to obtain semantic embeddings for each timestamp.A Sum of Ranked Explanatory Effects (SREE) analysis is then applied to determine the number (N) of principal components that account for at least 70% of the variance.Finally, these embeddings are reduced to N dimensions using Principal Components Analysis (PCA) (Abdi and Williams 2010).</p>
<p>Injecting Decision Information into LLMs With the VSM-ACTR model, which represents human-like cognitive reasoning in repeated decision-making tasks, this section outlines the experimental settings for fine-tuning of the LLM-ACTR framework.Fine-tuning, sometimes referred to as transfer learning, involves optimizing all model weights for the given task.The process includes parsing consistent template prompts that reflect the decision making task into an open-source LLM, aligning the task for the cognitive model Using the LLM as the base model to access the last hidden layer and obtain masked embeddings, constructing a classification layer with softmax activation on top of the base model, using targets containing the salient decision representations of the cognitive model and features from the masked embeddings of the base LLM, and fine-tuning the LLM for classification using the LORA method (see Additional Figures and Illustrations).The key points are: (1) The targets decode the salient decision information from the cognitive model.(2) Use the final layer of contextualized embeddings in transformer-based LLMs, generated through the attention block mechanism.The attention block, a key feature of transformers, distinguishes them from other architectures like recurrent neural networks (Graves 2012).It creates embeddings that capture the in-context meaning of tokens by recombining them with other tokens' embeddings.Successive attention blocks further refine these embeddings, producing multiple layers of abstraction.The final layer, a blend of these refined embeddings, is used in this pipeline because it offers the richest semantic information while balancing minimal information loss and reduced computational costs for fine-tuning.(3) Use Low-Rank Adaptation (LoRa) for its efficiency in fine-tuning, reducing the computational resources and time required while maintaining high model performance (Hu et al. 2022).</p>
<p>Experiments Problem Setting</p>
<p>As an instantiation of the problem definition, above, our manufacturing line has two sections with potential defect sources: pre-assembly and assembly.Pre-assembly takes 40 seconds with an OEE rate of 88%, while assembly takes 44 seconds with an OEE rate of 80.1%.To reduce total assembly time by 4, we must identify which section can be shortened with minimal defect increase.We note that reducing cycle time will also lead to an increase in headcount costs.</p>
<p>Implementation Details</p>
<p>The LlaMa-2 13B model was chosen as the foundation for this research because of its demonstrated effectiveness and efficiency in NLP tasks (Huang et al., 2024).As a state-ofthe-art large language model, LlaMa has been trained on trillions of tokens from publicly available datasets.Unlike other transformer-based models such as the GPT family, which can only be accessed at the user's end, LlaMa's architecture, including its pre-trained weights, is fully accessible.Furthermore, its proven capability to extract the last hidden layer for predicting behavioral discrepancies has been provided (Binz and Schulz, 2024).These attributes collectively establish LlaMa-2 13B as an optimal choice for this study.</p>
<p>To determine the dataset size that can effectively perform the task while balancing efficacy and resource limitations, we referred to Kumar, Sharma, and Bedi (2024), who showed evidence that LlaMa-2 13B achieves F1 scores above 0.9 in resource-limited text classification tasks, with datasets as 1,000 rows per class.Based on this, we developed the dataset size for fine-tuning as N (number of classes) * 1,000.The ACT-R dataset for binary decision-making classification contains 2,012 decision-making trials, Obtained by running the developed ACT-R model across 32 problem sets, each ACT-R persona was run for 15-16 trials until more stable expert behavior was achieved (Ritter et al. 2011).</p>
<p>Baseline Models</p>
<p>This study compared the goodness-of-fit and prediction accuracy of the resulting models using holdout data against two baselines: a random guess model and LlaMa without fine-tuning, obtained by reading out log-probabilities of the pre-trained LlaMa.</p>
<p>A random guess model serves as the most basic form of chance level baseline and represents the simplest hypothesis for model comparison.In psychological interdisciplinary experiments, control conditions often employ random responses to distinguish the effects of treatment from chance (Gaab et al. 2019).This approach allows assessing the extent to which decisions are influenced by knowledge versus being purely stochastic.</p>
<p>On the other hand, using LlaMa without fine-tuning as a baseline provides a reference point to measure the impact of fine-tuning on the model's performance.This comparison reveals how much the model 'learns' from the fine-tuning process compared to its generic, pre-trained state.</p>
<p>Research Questions</p>
<p>Based on our framework's components, we identify a set of research questions that we answer through experiments.</p>
<p>RQ1: What are the properties of a useful neural network representation of the decision-making process in Cognitive Architectures?</p>
<p>Answering this question sets the groundwork for developing a context-aware domain knowledge base for augmenting decision-making in LLMs.</p>
<p>RQ2: What level of complexity in behavior representation can LLMs effectively capture?</p>
<p>Previous research has used LLM conceptual embeddings to predict human behavior based on past behavioral studies (Binz and Schulz 2024), confirming LLMs' ability to replicate known human patterns.However, high costs and extensive data collection efforts limit this method.By incorporating cognitive model simulations, the study seeks to address these limitations and broaden the investigation to determine the extent to which LLMs can reproduce decision-making knowledge.This will, in turn, help define the depth of decision-making domain knowledge that can be effectively integrated with the innate learning capabilities of LLMs.</p>
<p>RQ3: Can we inform the LLM with knowledge about the reasoning process of the cognitive architecture?</p>
<p>Inspired by previous works on knowledge-injection (Oltramari et al. 2021;Ma et al. 2019), answering this question offers insights into knowledge transfer from domain-specific bases to LLMs and evaluates its impact on performance in holdout tasks.The method for addressing RQ1 was introduced in the first two sections of our approach framework.</p>
<p>Feature Extraction for Behavior Prediction</p>
<p>To answer RQ2: What level of complexity in behavior representation can LLMs effectively capture?Building on previous research that used conceptual embeddings from LLMs to predict human behavior with historical behavioral data (Binz and Schulz 2024), we adopted the same method of LLM feature extraction for behavior prediction (Hussain et al. 2023).We created datasets consisting of last contextual embeddings as features and the corresponding different levels of VSM-ACTR decision actions representations as targets.We obtained embeddings by passing prompts that included all the information that VSM-ACTR had access to on a given trial through LlaMa and then extracting the hidden activations of the final layer, as shown in Figure 3b.</p>
<p>The first dataset used features extracted from prompts (see LLM System Prompt Templates) identical to the VSM-ACTR task, with targets being the VSM-ACTR decision-making results, where '0' indicates reduced time in preassembly and '1' indicates assembly.The second dataset's prompt template added an explanation of the strategy adopted by VSM-ACTR and used compound targets comprising both the decision-making results and the strategies reflecting the learning trajectory (novice, intermediate, and expert).The targets were encoded as follows: 0, 1, and 2 for preassembly choices using novice, intermediate, and expert strategies, respectively, and 3, 4, and 5 for assembly choices following the same pattern.With these two datasets, we fitted a regularized logistic regression model using 10-fold crossvalidation for dataset 1 and multinomial regression using 10-fold cross-validation with L2 regularization for dataset 2. Model performance was assessed by measuring the goodness of fit through negative log-likelihood (NLL) and the predictive accuracy of hold-out data.</p>
<p>Fine Tuning for Knowledge Transfer</p>
<p>To answer RQ3: whether LLMs can be informed with knowledge about the reasoning processes of cognitive architecture-we use the fine-tuning approach of LLM-ACTR Framework.The fine-tuning process employs Cross-Entropy as the loss function and uses Adam optimization.Training involves a train test split of 0.2 and uses a batch size of 5 for both training and validation phases.The learning rate is set to 1e-5, with the training spanning across 10 epochs.To ensure regularization and prevent overfitting, a weight decay of 0.01, and a dropout of 0.5 are applied, and gradient accumulation is set to 2.Last but not least, gradient clipping is employed to maintain a maximum gradient norm of 1.0 for gradient explosion control.We evaluate the model fitting and generalization quality using training loss and validation loss across epochs, then compare the goodness of fit and prediction accuracy of the hold-out data against the baseline models.</p>
<p>Results</p>
<p>Finding Useful Decision Process Embeddings</p>
<p>The approach of distilling macro-level cognitive processes related to executive function captures the evolution of decision-making results across trials and how decisions adapt through learning and experience, all represented as a sequential single vector.This format facilitates ease of use for downstream tasks involving knowledge transfer.However, this method retains only partial cognitive decisionmaking knowledge.</p>
<p>In contrast, the holistic semantic preservation approach encompasses both macro and micro-level cognition processes.However, the embeddings produced vary in shape due to the individual differences in traces originating from stochastic simulations.They cannot be directly fed into neural networks for downstream tasks.Nevertheless, the first two principal components of the reduced embeddings, which correspond to the semantic mapping of ACT-R's components-including procedural, imaginal, goal knowledge, utility updating, and decision-making-are detailed in Figure 6.</p>
<p>The MANOVA analysis was conducted to assess the overall effect of the independent variables, which include label categories or ACT-R components, on the combined dependent variables-components of reduced embeddings.This analysis reveals a significant relationship with the semantic mapping of ACT-R's components.For instance, the ex-tremely low Wilks' lambda value (0.0004) suggests that the label or ACT-R component categories explain nearly all the variance in the dependent variables, indicative of a strong group effect.The statistical tests applied-Wilks' lambda, Pillai's trace, Hotelling-Lawley trace, and Roy's greatest root-all demonstrate strong significance, as evidenced by the extremely low p-values across all tests.These findings highlight that the principal components retained in the PCA successfully capture the essential variance related to these cognitive processes.</p>
<p>This result validates that our semantic abstraction method has the potential to retain the maximum semantics of neural symbolic representations at a minimal computational cost.However, further work is required to address the issue of ragged tensors for downstream tasks.</p>
<p>In a preliminary experiment, we addressed the issue of ragged tensors by employing padding with value imputation.We then integrated the 240 full cognitive reasoning traces from the VSM-ACTR model with LLM using embedding concatenation and conducted feature extraction for behavior prediction.Specifically, we transposed the reduced embeddings from each cognitive model run into a (1, X) dimension tensor and subsequently concatenated this with the LLM's last contextual embedding from the same prompt.These concatenated embeddings served as resources for predicting decision-making within the VSM-ACTR model.The prediction targets were multifaceted, including both the decisionmaking results and the strategies used.The results showed no significant improvement in prediction accuracy with concatenated embeddings compared to using LLM embeddings alone.Further details, illustrations, discussions on potential reasons, and suggestions for improvements, are provided in Additional Experiments, in the Appendix.</p>
<p>Assessing Behavior Complexity Captured by LLMs</p>
<p>Table 1 shows that LLM-ACTR captures a single facet of decision-making, achieving an average accuracy of 0.64 across 10 validation folds in the holdout task.When decision-making targets involve multi-ple facets-encompassing both choices and strategies that shape the learning trajectory-the accuracy decreases to 0.42.While this reduction suggests that capturing complex decision-making processes is less accurate, the results still show promise in handling these complexities.However, the Negative Log-Likelihood (NLL) reveals greater predictive uncertainty for multifaceted decision-making processes, as evidenced by a significantly higher NLL of 1.18 compared to 0.65 in single-facet scenarios.We then report the comparison of the LLM-ACTR with the baseline models on goodness of fit using negative log likelihood (NLL) and accuracy score for hold-out data.The LLM-ACTR model demonstrates significantly better performance across all metrics compared to the LlaMa-only model, highlighting its effectiveness in decision-making tasks involving sequential cognitive reasoning.Additionally, the LlaMa-only model performs worse than the chance-level model.This underscores the necessity of fine-tuning pretrained language models like LlaMa to adapt them to specific human-aligned repeated decision-making tasks.</p>
<p>Discussion and Conclusion</p>
<p>Main Insights/Takeaways Resolving the dichotomy between the human-like yet constrained reasoning processes of CAs and the broad, often noisy inference behavior of LLMs remains a challenging but exciting pursuit.This is crucial for enabling reliable machine reasoning capabilities in production systems.This study introduces LLM-ACTR, a novel neuro-symbolic architecture designed to enhance humanaligned and versatile decision-making by integrating the ACT-R model's cognitive process with LLMs.Our framework extracts and embeds ACT-R model's internal decisionmaking processes as latent neural representations based on using traces of its performance, then injects this information into trainable LLM adapter layers, and finally fine-tunes the LLMs for downstream prediction tasks.LLM-ACTR addresses the data scarcity issue often encountered in research aimed at aligning LLMs with human reasoning.Our approach demonstrates improved grounded decision-making capabilities compared to LLM-only baselines that leverage chain-of-thought reasoning strategies.</p>
<p>We explore distilling latent representations.The findings show that distilling macro-level cognitive processes preserves high-level neural symbolic knowledge, aiding downstream tasks but only partially capturing decision-making knowledge.A holistic semantic preservation approach, covering both cognitive and metacognitive processes, better retains full neural symbolic semantics with low computational costs.However, challenges with ragged tensors in downstream tasks require further research.We then use a VSM-ACTR cognitive model, developed for a manufacturing design task, to distill its macro-level cognitive processes as domain knowledge.This knowledge was then employed in both a feature extraction for behavior prediction method and a fine-tuning pipeline to investigate the LLM's capabilities in (1) capturing the complexity of behavioral representations and (2) determining whether and how the LLM can be informed by the reasoning processes inherent in the cognitive architecture.The results show that (1) prompt embeddings generated through feature extraction capture repeated human-aligned decision-making and the overall structure of learning.However, they struggle to capture complex decision-making rationales, such as the strategies employed.</p>
<p>(2) Fine-tuning effectively informs the LLM with knowledge of decision-making tasks involving sequential cognitive reasoning.</p>
<p>Limitations The limitation lies in achieving close human alignment with the VSM-ACTR model.While the model is currently tuned to reflect general human learning behavior patterns in decision-making, it still requires more granular human data for cognitive fine-tuning.The closer the VSM-ACTR model aligns with human behavior, the more accurately it can represent human decision-making processes.</p>
<p>Future Work To address this limitation, the project aims to enhance the human alignment of the cognitive model by fine-tuning it with granular human data.This will be followed by integrating the full semantic space of ACT-R into the LLM's decision-making processes.The strategy of using semantic abstraction from cognitive model trace lines demonstrates the potential.However, the preliminary experimental results highlight challenges in integrating such knowledge, which are worth further exploration.Grounding LLMs in a robust cognitive framework while minimizing computational costs could pave the way for effectively scaling the infusion of superior domain knowledge into LLMs.We employed a train-test split of 0.4 and conducted multinomial regression with L2 regularization, using two-fold crossvalidation to compare the prediction accuracy and goodness of fit, as measured by negative log-likelihood, between concatenated embeddings and LlaMa embeddings alone.The results indicated that the prediction accuracy for both datasets was 0.71, with the concatenated embeddings showing a slightly better negative log-likelihood of 0.9535 compared to 0.9553 for LlaMa-only embeddings.</p>
<p>The current findings suggest no significant improvement in behavior prediction when combining masked embeddings versus using LlaMa embeddings alone.One possible explanation is the relative scale of the VSM-ACTR reduced embeddings compared to those of LlaMa, which is disproportionately small (1:10).Consequently, the LlaMa embeddings may dominated the decision-making process within the model due to their larger scale.A potential solution could be to generate longer VSM-ACTR model traces, including tenfold more decision-making trials, thereby enhancing the scale and variability of its features.</p>
<p>Also, the method we use to handle ragged tensors-padding followed by value imputation-could potentially dilute the VSM-ACTR embeddings and reduce their accuracy.Finding an alternative method to preserve the full embeddings from VSM-ACTR may potentially improve the results.</p>
<p>Lastly, the limited dataset size could be influencing the results.The preliminary test used only 240 complete traces.Expanding the dataset may provide more insights into the performance of the proposed approach.</p>
<p>Description of the Code and Data</p>
<p>In LLM-ACTR-main.zip</p>
<p>Background: the VSM-ACTR 1.0 Model</p>
<p>Here is a brief description of VSM-ACTR (Authors 2024) -to support the discussion in the main content of this paper.It is a concurrent work that the present manuscript extends.This referenced manuscript has been peer-reviewed and has been accepted for publication in an upcoming conference venue.</p>
<p>This section starts with capturing intuition and domain knowledge from decision makers, followed by the model structure and learning mechanism, and concludes by examining a model output snippet from one run of our VSM model.</p>
<p>Model Design</p>
<p>The model, built upon the prototypical decision process distilled by Bosch plant managers, incorporates how cognitive models are designed for different levels of expertise.For novices, the model utilizes intuitive deliberative chunks to make decisions.For intermediates, it understands key metrics such as cycle time (CT) and Overall Equipment Effectiveness (OEE).However, intermediates often lack the ability to systematically analyze how these metrics interrelate and cumulatively impact efficiency and quality.Experts, on the other hand, make well-informed judgments based on a comprehensive view of all relevant metrics, obtained through Value Stream Mapping (VSM).</p>
<p>Declarative Chunks We created chunks representing knowledge from intuitions to professional expertise.These representations are divided into three chunk types: decisions, decision merits, and goals.Decision chunk encodes six slots: reduction time, decision-making state (e.g., novice, intermediate, expert), OEE, and CT.The decision merits chunk holds knowledge on weights for sectors, defect increase for sectors, and the difference in defect rate increase between the two.The goal chunk encodes the initial production conditions and the ultimate goal of making the optimal decision.We use the expert production rule set as an example, as shown in Figure 2. Once the decision-choice center decides to activate this set of expert decision productions, it starts by perceiving the problem and retrieving related decision-making metrics from chunks.The imaginal buffer then acts as a temporary workspace, holding and manipulating relevant information during decision-making.It allows the model to build new mental representations or modify existing ones based on incoming data or problem-solving needs.This involves using the imaginal buffer to assess the relationships between the decision target and decision metrics, particularly considering the impact of each sector's weight on the defect rate change, and determining the final defect rate increase for each sector.These results are stored in the imaginal buffer and later retrieved for comparison.This then allows the model to select the sector with the lowest defect increase.</p>
<p>Level of Expertise Mechanism</p>
<p>The model can learn while performing tasks through two mechanisms leading to varying levels of expertise, as shown in Figure 3.</p>
<p>The model mimics human decision-making behavior through differentiating knowledge representations.Declarative Memories: These memories store knowledge that aligns with human intuition and expertise gained from the VSM.For example, the green triangles in the figure represents a portion of the intuition used by novice decision-makers.Production Rules: These rules capture the rational decision-making processes observed in human subjects.The green (lighter) lines illustrate how the imaginal Finally, the goal buffer utilizes the "goal focus" command to manipulate the different phases of the task.</p>
<p>Beyond mimicking human behavior, the model also simulates the learning progress achieved by the Decision-Choice Control, which manages errors, learning, and memory through utility learning and reinforcement rewards.Novice decision-making starts with a utility base and includes a noise setting.The intermediate and expert production rules receive rewards when the corresponding decision-making results are achieved.The utility of these production rules updates is based on the rewards received and the retention of memory, which depends on the time passed since the rule last fired.</p>
<p>Example ACT-R Decision Trace</p>
<p>We provide a reference ACT-R trace, which we explain is the basis for extracting the decision-making knowledge representation.</p>
<p>The model begins by setting up the goal (line 1), followed by starting with a novice strategy (line 3).For the production rules associated with each strategy, each production rule's utility is updated based on the reward received and the time since the last selection.For example, the utility of the NAIVE-CHOICE rule decreased from 3 to 1.96 (lines 14-16) due to a penalty of -2.25 for the time passed since the last selection.As the utility of naive strategies decreases, the likelihood of EXPERT-Strategy (lines 87-89) being triggered increases.</p>
<p>Figure 1 :
1
Figure 1: Decision augmentation using a neural-symbolic cognitive architecture approach.(1) Tasks are modeled with cognitive architecture.(2) Cognitive model used to run stochastic simulation of task at scale.(3) Synthetic data are distilled from simulation and combined with prompt requests.(4) A fine tuning pipeline is used to calibrate open source LLM to perform decision augmentation for task in exercise.</p>
<p>Figure 2 :
2
Figure 2: A Value Stream Map of manufacturing process.</p>
<p>Figure 3: (a) Obtaining decision representations from VSM-ACT-R.(b) LLM feature extraction for behavior prediction.</p>
<p>Figure 4 :
4
Figure 4: ACTR embedding mapping</p>
<p>Figure 6 :
6
Figure 6: Full knowledge-extraction and feature-injection pipeline.</p>
<p>Figure 7 :
7
Figure 7: Production rules control structure for expert decision making and their use of the ACT-R Goal and Imaginal buffers</p>
<p>Figure 8 :
8
Figure 8: Level of expertise mechanism in VSM-ACT-R</p>
<p>PROCEDURAL PRODUCTION-FIRED DECIDE-BRUTE 4 0.150 PROCEDURAL PRODUCTION-FIRED BRUTE-DECISION 5 assembly is always a good place to reduce time!6 0.200 PROCEDURAL PRODUCTION-FIRED REHEADCOUNT</p>
<p>Table 1 :
1
Evaluation for Single and Multi Facets Targets
Target TypeNLL AccuracySingle Facet Target 0.630.64Multi Facets Target 1.180.42Table 2: Comparison of VSM-ACTR with baselinesModelNLLAccuracyChance-level0.69310.4826LlaMa1.13300.3564LLM-ACTR (ours) 0.65340.6576Injecting LLMs with CA Decision ProcessWe first report training and validation losses, across 10epochs, to reveal the fine-tuned model's learning and gen-eralization behavior. Initially, the training loss begins at ap-proximately 0.73, with a slight fluctuation observed in sub-sequent epochs, peaking around epoch 2 and showing a no-table dip at epoch 7. In contrast, the validation loss starts ataround 0.64 and remains remarkably stable throughout theepochs. This consistency in validation loss, coupled witha generally downward trend in training loss after its initialvariations, suggests that the model is learning effectively.The overall trend indicates an improvement in model perfor-mance over time, reflecting its capability to generalize wellon unseen data.
Appendix Additional Figures and IllustrationsAdditional ExperimentsExtended feature-injection pipeline Here, we show the preliminary results of combining masked embeddings, extracted from open-source LLMs (e.g., LlaMa), with embeddings of ACT-R's internal decision-making process for training a prediction head on top -as illustrated in Figure6.We integrated the 240 full cognitive reasoning traces from the VSM-ACTR model with LlaMa using embedding concatenation and conducted feature extraction for behavior prediction.Specifically, we transposed the reduced embeddings from each cognitive model run into a (1, X) dimension tensor and subsequently concatenated this with the LlaMa's last contextual embedding from the same prompt.These concatenated embeddings served as resources for predicting decision-making within the VSM-ACTR model.The prediction targets were multifaceted, including both the decision-making results and the strategies reflecting the learning trajectory (novice, intermediate, and expert).LLM System Prompt TemplatesPrompt template for fine-tuning and single-facet target behavior prediction:Our manufacturing line has two sections with potential defect sources: pre-assembly (0) and assembly (1).Pre-assembly takes CT1 seconds with an Overall Equipment Effectiveness(OEE) rate of {OEE1}%, while assembly takes {CT2} seconds with an OEE rate of {OEE2}%.To reduce total assembly time by 4 seconds, we need to identify which section can be shortened with minimal defect increase.It's important to note that reducing cycle time will also lead to an increase in line headcount costs.There are two options: reduce pre-assembly time (0) or reduce assembly time (1).Question: Which section do you choose to optimize?Answer:Prompt template for target behavior prediction:Our manufacturing line features two sections prone to defects: pre-assembly and assembly.Pre-assembly requires CT1 seconds to complete with an Overall Equipment Effectiveness (OEE) rate of OEE1%.Assembly takes CT2 seconds and has an OEE rate of OEE2%.To cut total assembly time by 4 seconds, we must decide which section's duration can be reduced with the least increase in defects.Reducing cycle times will also result in higher line headcount costs.We have three strategy levels for decision-making:Novice strategy (targets encoded as 0 for pre-assembly, 3 for assembly): Intuitive choice.Intermediate strategy (targets encoded as 1 for pre-assembly, 4 for assembly): Make decision using key metrics.Expert strategy (targets encoded as 2 for pre-assembly, 5 for assembly): make well-informed judgments based on a comprehensive understanding of all relevant metric.Question: Given the different strategy levels, which options would you choose?Answer:LLM Conversation ExamplesBoth GPT-4 and Google Gemini deliver contradictory recommendations across prompts, despite the chain-of-thought reasoning in each case being similar and seemingly plausible.Shared System Prompt Prompt used for generating LLM responsesOur manufacturing line has two sections with potential defect sources: pre-assembly and assembly.Preassembly takes 40 seconds with an Overall Equipment Effective (OEE) rate of 88%, while assembly takes 44 seconds with an OEE rate of 80.1%.To reduce total assembly time by 4 seconds, we need to identify which section can be shortened with minimal defect increase.It's important to note that reducing cycle time will also lead to an increase in headcount costs.There are two options: reduce pre-assembly time or reduce assembly time.Which section will you choose to reduce time?Google Gemini Response 1 Reducing pre-assembly time is likely the better option in this scenario.Here's why:• Higher OEE: Pre-assembly has a higher Overall Equipment Effectiveness (OEE) rate of 88% compared to assembly's 80.1%.This indicates that pre-assembly has a smaller inherent defect rate, and speeding it up is less to introduce additional defects.• Smaller time reduction required: Reducing pre-assembly time by 4 seconds (40 seconds -4 seconds = 36 seconds) achieves the overall target compared to needing a 4.4 second reduction in assembly time (44 seconds -4 seconds = 39.6 seconds).A smaller time reduction generally translates to less disruption and less likelihood of introducing defects.Therefore, focusing on optimizing the pre-assembly process to achieve a 4-second reduction has a better chance of success with minimal defect increase.Here are some additional factors to consider:• Complexity of the processes: If pre-assembly is a more complex process with more steps, it might be harder to optimize without impacting quality.• Automation potential: Analyze if either process can be automated further.Automation can improve consistency and potentially reduce cycle time.It's always recommended to pilot any changes in a controlled environment before full-scale implementation.This allows you to measure the actual impact on cycle time, defect rates, and headcount requirements.Google Gemini Response 2Analyzing the available information, shortening assembly time would likely be the better option to achieve a 4-second reduction in total assembly time while minimizing defect impact.Here's the reasoning:• Higher OEE in Pre-Assembly:The OEE rate for pre-assembly (88%) is higher than assembly (80.1%).OEE reflects how efficiently a process utilizes available production time, factoring in quality and speed.A higher OEE suggests pre-assembly is already closer to optimal performance, and squeezing out 4 seconds of improvement might be more difficult without impacting quality.• Potential for Improvement in Assembly: The lower OEE in assembly indicates more room for improvement without necessarily causing more defects.Optimizing this section might yield the needed 4-second reduction more readily.Here's a breakdown of the thought process:1. Available Time for Improvement: OEE helps calculate the available time for improvement within a process.A higher OEE signifies less available time for improvement without sacrificing quality.2. Prioritizing Efficiency with Quality: Since minimizing defect rates is crucial, focusing on the section with a higher OEE (pre-assembly) for improvement might be riskier.Additional Considerations:• Analyze Defect Rates: While OEE is a good indicator, analyzing actual defect rates per section would provide a clearer picture.Even with a lower OEE, if assembly's defect rate is already low, improvement might be achievable.• Cost-Effectiveness: Consider the cost of implementing changes in each section.Optimizing assembly might involve minor adjustments, while pre-assembly changes could require more effort.It's recommended to conduct a more detailed analysis of defect rates and potential improvement methods for both sections before making a final decision.However, based on the OEE data provided, prioritizing assembly time reduction seems like a promising initial approach to achieve the 4-second target while minimizing defect risks.OpenAI GPT 4.0 Response 1To decide which section to focus on for reducing time while considering minimal defect increase and cost implications, let's analyze the options:1. Pre-assembly Section: Implications:• Time Saved: 4 seconds per unit at assembly.Considerations:• Defect Rate Increase: Typically, when cycle times are reduced, there is a risk of increased defects due to rushed processes or errors.• Cost Implications: Reducing cycle time often requires additional investment, such as hiring more workers to maintain output or investing in more efficient equipment.Decision: Given the goal is to reduce total assembly time by 4 seconds with minimal defect increase and considering cost implications:• Reducing Assembly Time: This option directly meets the required reduction goal (4 seconds) without needing additional reduction efforts in the pre-assembly section.• Defect Risk: While both sections pose a risk of increased defects with faster cycle times, assembly might be more critical as it directly affects the final product quality and functionality.Conclusion: I would choose to focus on reducing the cycle time in the Assembly Section.This choice aligns with the goal of reducing assembly time by 4 seconds and may be more straightforward in achieving the required reduction compared to trying to split the reduction between pre-assembly and assembly.However, careful monitoring of defects and quality control measures should accompany any cycle time reduction to mitigate risks effectively.OpenAI GPT 4.0 Response 2To determine which section to reduce time in order to minimize defect increase, let's analyze the impact of reducing cycle time in each section based on the given OEE (Overall Equipment Effectiveness) rates and defect considerations.Definitions and Given Data:• Pre-assembly:-Cycle Time: 40 seconds -OEE: 88%• Assembly:-Cycle Time: 44 seconds -OEE: 80.1%OEE Formula: OEE is given by: OEE = Availability × Performance × QualityThe quality rate can be interpreted as the proportion of good products out of total products produced.Therefore, the defect rate (D) for each section is:D Pre-assembly = 1 − OEE Pre-assembly = 1 − 0.88 = 0.12 D Assembly = 1 − OEE Assembly = 1 − 0.801 = 0.199 Impact of Time Reduction:1. Reducing Pre-assembly Time:• Original Pre-assembly Time: 40 seconds • New Pre-assembly Time: 40 -x seconds (where x is the reduction amount) • Defect Rate Impact: The defect rate due to time reduction in pre-assembly is minimal because the OEE is relatively high.Also, reducing pre-assembly time might not significantly affect the defect rate if the process is already running efficiently.2. Reducing Assembly Time:• Original Assembly Time: 44 seconds • New Assembly Time: 44 -y seconds (where y is the reduction amount)• Defect Rate Impact: Assembly has a lower OEE, which translates to a higher defect rate.Reducing time in a section with a lower OEE often leads to an increase in defects because the process might be less stable and more sensitive to changes.Recommendation: Given that reducing time in the section with a higher defect rate (Assembly) will likely lead to a more significant increase in defects compared to reducing time in the section with a higher OEE (Pre-assembly), the preferable option is: Reduce Pre-assembly Time.Reasoning:• Lower Defect Rate Increase: Pre-assembly has a higher OEE and thus a lower defect rate.Reducing its time is less likely to significantly increase the defect rate compared to reducing time in the Assembly section.• Operational Impact: While reducing time in either section will increase headcount costs, reducing Preassembly time is less likely to destabilize the process compared to Assembly, which has a lower OEE and higher defect rate.In summary, to reduce total assembly time by 4 seconds with minimal defect increase, it is more advantageous to reduce Pre-assembly time rather than Assembly time.
Principal Component Analysis. H Abdi, L J Williams, Wiley Interdisciplinary Reviews: Computational Statistics. 242010</p>
<p>The atomic components of thought. J R Anderson, 1998ErlbaumMahwah, NJ</p>
<p>How can the human mind occur in the physical universe?. J R Anderson, 2009Oxford University Press</p>
<p>Learning rapid and precise skills. J R Anderson, S Betts, D Bothell, R Hope, C Lebiere, Psychological Review. 1262019</p>
<p>Language models as agent models. J Andreas, Findings of the Association for Computational Linguistics: EMNLP 2022. 2022</p>
<p>VSM-ACTR: Toward Using Cognitive Architecture for Manufacturing Solutions. Accepted to the 17th International Conference on Social Computing, Behavioral-Cultural Modeling &amp; Prediction and Behavior Representation in Modeling and Simulation. SBP-BRIMs2024</p>
<p>Teamwork, Communication, and Planning in ACT-R Agents Engaging in Urban Combat in Virtual Environments. B J Best, C Lebiere, Proceedings of the 2003 IJCAI Workshop on Cognitive Modeling of Agents and Multi-Agent Interactions. the 2003 IJCAI Workshop on Cognitive Modeling of Agents and Multi-Agent Interactions2003</p>
<p>Understanding in-context learning in transformers and llms by learning to learn discrete functions. S Bhattamishra, A Patel, P Blunsom, V Kanade, arXiv:2310.030162023arXiv preprint</p>
<p>Using cognitive psychology to understand GPT-3. M Binz, E Schulz, 2023Proceedings of the National Academy of Sciences120e2218523120</p>
<p>Turning Large Language Models into Cognitive Models. M Binz, E Schulz, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2024In press</p>
<p>How People Learn to Skip Steps. S B Blessing, J R Anderson, Journal of Experimental Psychology: Learning, Memory, and Cognition. 2235761996. 2017ACT-R 7 Reference Manual</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, Advances in Neural Information Processing Systems (NeurIPS). 202033</p>
<p>Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art. N Chakraborty, M Ornik, K Driggs-Campbell, arXiv, 2403.16527202431</p>
<p>Meta-in-context learning in large language models. J Coda-Forno, M Binz, Z Akata, M Botvinick, J Wang, E Schulz, Advances in Neural Information Processing Systems. 202436</p>
<p>Core challenges in embodied vision-language planning. Q Dong, L Li, D Dai, C Zheng, Z Wu, B Chang, X Sun, J Xu, Z Sui, J Francis, N Kitamura, F Labelle, X Lu, I Navarro, J Oh, arXiv:2301.00234A survey on in-context learning. 2022. 202274arXiv preprint</p>
<p>Effects and Components of Placebos with a Psychological Treatment Rationale-Three Randomized-Controlled Studies. J Gaab, J Kossowsky, U Ehlert, C Locher, Scientific Reports. 9114212019</p>
<p>Executive Function. S J Gilbert, P W Burgess, Current Biology. 1832008</p>
<p>Modeling human behavior with integrated cognitive architectures: Comparison, evaluation, and validation. K A Gluck, R W ; Pew, J F Lerch, C Lebiere, Cognitive Science. 272006. 2003Psychology PressInstancebased learning in dynamic decision making</p>
<p>Humanlike intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT. A Graves, T Hagendorff, S Fabi, M Kosinski, arXiv:1211.3711Nature Computational Science. 3102012. 2023arXiv preprintSequence transduction with recurrent neural networks</p>
<p>LoRA: Low-Rank Adaptation of Large Language Models. E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, International Conference on Learning Representations. 2022</p>
<p>Toward general-purpose robots via foundation models: A survey and meta-analysis. Y Hu, Q Xie, V Jain, J Francis, J Patrikar, N Keetha, S Kim, Y Xie, T Zhang, Z Zhao, arXiv:2312.087822023. 2023arXiv preprintA tutorial on open-source large language models for behavioral science. PsyArXiv preprint</p>
<p>Improving Knowledge Extraction from LLMs for Task Learning through Agent Analysis. D ; Kahneman, Macmillan, J R Kirk, R E Wray, P Lindes, J E Laird, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2011. 202438Thinking, fast and slow</p>
<p>Towards Optimal NLP Solutions: Analyzing GPT and LLaMA-2 Models Across Model Scale, Dataset Size, and Task Diversity. Engineering. A Kumar, R Sharma, P Bedi, Technology &amp; Applied Science Research. 1432024</p>
<p>Large Language Models Assume People are More Rational Than We Really are. arXiv, cs. J E Laird, R Liu, J Geng, J C Peterson, I Sucholutsky, T L ; Griffiths, Cl, K Ma, J Francis, Q Lu, E Nyberg, A Oltramari, M K Martin, C Gonzalez, C Lebiere, arXiv:1910.14087Proceedings of the Sixth International Conference on Cognitive Modeling. the Sixth International Conference on Cognitive ModelingPittsburgh, PAPage Numbers Here2012. 2024. 2019. 2004Carnegie Mellon University/University of Pittsburgh. NelsonarXiv preprintLearning to Make Decisions in Dynamic Environments: ACT-R Plays the Beer Game. and Narens, L. 1994. Why Investigate Metacognition? Journal Name Here, Volume Here(Issue Number Here</p>
<p>Unified Theories of Cognition. A Newell, 1994Harvard University Press</p>
<p>Enabling High-Level Machine Reasoning with Cognitive Neuro-Symbolic Systems. A Oltramari, Proceedings of the AAAI Symposium Series. the AAAI Symposium Series20232</p>
<p>Generalizable neuro-symbolic systems for commonsense question answering. A Oltramari, J Francis, F Ilievski, K Ma, R Mirzaee, Neuro-Symbolic Artificial Intelligence: The State of the Art. IOS Press2021</p>
<p>. F E Ritter, M J Schoelles, K S Quigley, L C Klein, </p>
<p>Determining the number of model runs: Treating cognitive models as theories by not sampling their behavior. Springer-VerlagLondon</p>
<p>ACT-R: A cognitive architecture for modeling cognition. F E Ritter, F Tehranchi, J D Oury, Cognitive Science. 102023Wiley Interdisciplinary Reviews</p>
<p>Reinforcement learning signals in the human striatum distinguish learners from nonlearners during reward-based decision making. T Schönberg, N D Daw, D Joel, J P Doherty, Journal of Neuroscience. 27472007</p>
<p>The empirical case for two systems of reasoning. S A Sloman, Psychological Bulletin. 11911996</p>
<p>T R Sumers, S Yao, K Narasimhan, T L Griffiths, arXiv:2309.02427Cognitive architectures for language agents. 2023arXiv preprint</p>
<p>Reinforcement Learning. R S Sutton, A G Barto, Journal of Cognitive Neuroscience. 1111999</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, arXiv:2302.13971Tversky, A.; and Kahneman, D. 1974. Judgment under Uncertainty: Heuristics and Biases. 2023185LLaMA: Open and Efficient Foundation Language Models</p>
<p>Including the Value of Time in Design-for-Manufacturing Decision Making. K Ulrich, D Sartorius, S Pearson, M Jakiela, Management Science. 3941993</p>
<p>Inhibition in Pavlovian conditioning: Application of a theory. A R Wagner, R A Rescorla, Inhibition and learning. 1972</p>
<p>Dual processes in reasoning?. P C Wason, J S B Evans, Cognition. 321974</p>
<p>T Webb, K J Holyoak, H Lu, arXiv:2212.09196Emergent Analogical Reasoning in Large Language Models. 2022arXiv preprint</p>
<p>Comparing LLMs for Prompt-Enhanced ACT-R and Soar Model Development: A Case Study in Cognitive Simulation. R E Wray, J R Kirk, J E Laird, S Wu, R Ferreira, F E Ritter, L Walter, arXiv:2405.12147Proceedings of the 38th Annual Association for the Advancement of Artificial Intelligence (AAAI) Conference on Artificial Intelligence, Fall Symposium Series on Integrating Cognitive Architecture and Generative Models. the 38th Annual Association for the Advancement of Artificial Intelligence (AAAI) Conference on Artificial Intelligence, Fall Symposium Series on Integrating Cognitive Architecture and Generative ModelsArlington, Virginia, USA2024. 2023arXiv preprintEliciting Problem Specifications via Large Language Models</p>
<p>S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, REACT: Synergizing Reasoning and Acting in Language Models. International Conference on Learning Representations (ICLR). 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1016 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1016</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1016</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-231942563</p>
                <p><strong>Paper Title:</strong> Automated curriculum learning for embodied agents a neuroevolutionary approach</p>
                <p><strong>Paper Abstract:</strong> We demonstrate how the evolutionary training of embodied agents can be extended with a curriculum learning algorithm that automatically selects the environmental conditions in which the evolving agents are evaluated. The environmental conditions are selected to adjust the level of difficulty to the ability level of the current evolving agents, and to challenge the weaknesses of the evolving agents. The method does not require domain knowledge and does not introduce additional hyperparameters. The results collected on two benchmark problems, that require to solve a task in significantly varying environmental conditions, demonstrate that the method proposed outperforms conventional learning methods and generates solutions which are robust to variations and able to cope with different environmental conditions.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1016.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1016.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Long Double-Pole Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Long double-pole balancing agent (LSTM controller)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated cart-pole embodied agent controlled by a 3-layer LSTM (3 sensory, 10 internal, 1 motor) whose connection weights are evolved with neuroevolutionary strategies; trained to keep two poles balanced under wide-ranging initial-state variations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automated curriculum learning for embodied agents a neuroevolutionary approach</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Long double-pole balancing agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Neuroevolutionary training of a recurrent neural controller (LSTM, 3-layer, 3 sensory neurons, 10 internal units, 1 motor) using OpenAI-ES and a Steady-State EA; fitness is accumulated reward per timestep for keeping poles balanced.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Long double-pole balancing environment (cart with two poles)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Cart (1 kg) with two poles (long pole length 1.0 m mass 0.5 kg; short pole length 0.5 m mass 0.25 kg) attached by passive hinges; initial state variables that vary are cart position x, cart velocity ẋ, pole angles θ1, θ2 and their angular velocities; episodes end after 1000 steps or when cart/angles exceed safety bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>State dimensionality (6 initial-state variables varied: x, ẋ, θ1, θ1̇, θ2, θ2̇); physical parameters (pole length/mass ratio increases difficulty relative to classic problem); episode length up to 1000 steps; number of discrete environment instances used during training 5^6 = 15,625 (as reported), post-evaluation grid 3^6 = 729 conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (described in paper as significantly more complex than the classic double-pole problem due to larger variation ranges and second pole size)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number of distinct initial-state instances (discretized into 5 values per variable → 5^6 = 15,625 possible environments as used during evolution; post-evaluation considered 3^6 = 729 combinations); ranges given for each variable (explicit numeric ranges in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (wide ranges of initial positions and velocities; combinatorially many possible initial conditions)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Average episodic fitness = reward of 1 point per timestep alive (max 1000) aggregated across evaluation episodes; statistical comparisons (Kruskal–Wallis, Mann–Whitney U) used for significance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported as average fitness of best agents (numeric fitness values not printed in body text). Key quantitative training/test details: training run for 1 × 10^9 evaluation steps; post-evaluation: best agents evaluated over 1000 episodes with uniformly-sampled environmental conditions. Curriculum-learning variants (power functions, esp. cubic) significantly outperform standard random sampling (p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper explicitly discusses that environment complexity scales with the number and ranges of variables and that the number of conditions (variations) grows combinatorially, creating scalability challenges. It shows that preferentially sampling more difficult environmental conditions (via a power-based difficulty function) improves evolved performance and that exposing agents to environmental conditions with similar difficulty levels reduces stochasticity in fitness estimates. The paper also notes an exponential increase in time required to estimate difficulty as the number of varied environmental variables increases.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Neuroevolution with curriculum learning (environment selection); comparisons against standard random environment sampling and curriculum with linear vs power difficulty functions; algorithms used: OpenAI-ES and Steady-State EA.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Yes — best agents were post-evaluated across a dense grid of environmental conditions (3^6 = 729) and the curriculum-learning (cubic power) condition equaled or outperformed the standard method in the large majority of those environmental conditions; authors report curriculum-trained agents are more robust to variations.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training reported: 1 × 10^9 evaluation steps for OpenAI-ES experiments (10 replications); authors note the curriculum-learning (cubic) starts to outperform the standard algorithm relatively early during training (figure-based claim), but exact sample-to-performance curves' numeric thresholds are not reported in text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Applying curriculum learning that estimates difficulty per-environment (based on recent agents' fitness) and preferentially samples harder conditions (power function, cubic performed best) produces agents that achieve higher average fitness and are more robust across the combinatorially large set of initial-state variations; also reduces fitness stochasticity by equalizing difficulty exposure across individuals. The approach faces scalability limits because difficulty estimation cost grows exponentially with number of varied variables.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1016.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1016.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bipedal Hardcore Walker</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bipedal walker hardcore agent (feedforward controller)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated bipedal walker (OpenAI Gym 'BipedalWalkerHardcore') controlled by a 3-layer feed-forward network (24 sensory, 64 internal, 4 motor) whose weights are evolved via neuroevolution; tested across procedurally varied obstacle configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automated curriculum learning for embodied agents a neuroevolutionary approach</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Bipedal walker hardcore agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Neuroevolutionary training of a feed-forward neural controller (3-layer, 24 inputs, 64 hidden units, 4 outputs) using OpenAI-ES and Steady-State EA; rewarded for forward distance traveled (capped) and penalized for falling.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>BipedalWalkerHardcore environment (OpenAI Gym)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A 2D bipedal agent (skull + two two-segment legs) must walk forward through an environment containing sequences of five obstacles; obstacles chosen from five types (small stumps, large stumps, uphill stairs, downhill stairs, holes). A random downward force is applied to hull at start of episodes; falling yields a −100 penalty, distance reward up to 300.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Obstacle sequence combinatorics (each of 5 obstacle slots independently chosen among 5 types); sensor and action dimensionality (24 sensors, 4 motors); episode scoring and falling penalty define task difficulty; number of possible environments reported as 5^5 = 3,625 (paper's reported combinatorial count).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (challenging locomotion with obstacles, random perturbations, and high-dimensional sensorimotor input)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number of distinct environment instances = 5^5 = 3,625 (as reported in paper); randomness in initial downward force; obstacle permutations provide structured variation across episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (multiple obstacle types and permutations leading to thousands of possible environment instances)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Average episodic fitness (distance-based reward up to 300, with −100 fall penalty); authors use average fitness of best agents over post-evaluation episodes and report statistical tests (Kruskal–Wallis, Mann–Whitney U).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported as average fitness of best agents (exact numeric fitness values not printed in main text). Training details: evolutionary runs for 3 × 10^8 evaluation steps; post-evaluation: best agents tested for 500 episodes with random obstacle categories. Curriculum-learning (power difficulty functions) significantly outperform standard sampling (p < 0.01).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper frames environmental variation (obstacle permutations and random perturbations) as a primary source of task difficulty distinct from initial-state variation in the pole task; it shows that preferentially sampling more difficult environment categories (via power-based difficulty functions) improves evolved performance and robustness. The authors again note scalability concerns: estimating per-environment difficulty across many variation dimensions is costly and grows combinatorially.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Neuroevolution with curriculum learning (environment selection by difficulty subsets); compared to standard uniform random environment sampling and curriculum with linear vs power difficulty functions; used OpenAI-ES and Steady-State EA.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Yes — post-evaluation across all 5^5 (reported 3,625) obstacle combinations showed the curriculum-learning (cubic/power variants) equals or outperforms the standard method in the majority of environment instances; curriculum-trained agents are reported to be more robust across obstacle permutations.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training reported: 3 × 10^8 evaluation steps for the OpenAI-ES experiments (10 replications); curriculum-learning with cubic function begins to show advantage early in training (figure-based claim), but specific episode counts to reach thresholds are not provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Curriculum learning that ranks environments by difficulty (estimated from recent agents' fitness) and biases selection toward harder categories (power-based interval sizing) yields higher final performance and more robust walkers across many obstacle permutations compared to uniform sampling; improvements hold across both OpenAI-ES and Steady-State EA. The method reduces fitness stochasticity and increases exposure to challenging conditions but faces scalability limitations as variation dimensionality grows combinatorially.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Paired open-ended trailblazer (poet): endlessly generating increasingly complex and diverse learning environments and their solutions <em>(Rating: 2)</em></li>
                <li>Evolution strategies as a scalable alternative to reinforcement learning <em>(Rating: 2)</em></li>
                <li>Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments <em>(Rating: 2)</em></li>
                <li>Incremental evolution of complex general behavior <em>(Rating: 1)</em></li>
                <li>New methods for competitive coevolution <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1016",
    "paper_id": "paper-231942563",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "Long Double-Pole Agent",
            "name_full": "Long double-pole balancing agent (LSTM controller)",
            "brief_description": "A simulated cart-pole embodied agent controlled by a 3-layer LSTM (3 sensory, 10 internal, 1 motor) whose connection weights are evolved with neuroevolutionary strategies; trained to keep two poles balanced under wide-ranging initial-state variations.",
            "citation_title": "Automated curriculum learning for embodied agents a neuroevolutionary approach",
            "mention_or_use": "use",
            "agent_name": "Long double-pole balancing agent",
            "agent_description": "Neuroevolutionary training of a recurrent neural controller (LSTM, 3-layer, 3 sensory neurons, 10 internal units, 1 motor) using OpenAI-ES and a Steady-State EA; fitness is accumulated reward per timestep for keeping poles balanced.",
            "agent_type": "simulated agent",
            "environment_name": "Long double-pole balancing environment (cart with two poles)",
            "environment_description": "Cart (1 kg) with two poles (long pole length 1.0 m mass 0.5 kg; short pole length 0.5 m mass 0.25 kg) attached by passive hinges; initial state variables that vary are cart position x, cart velocity ẋ, pole angles θ1, θ2 and their angular velocities; episodes end after 1000 steps or when cart/angles exceed safety bounds.",
            "complexity_measure": "State dimensionality (6 initial-state variables varied: x, ẋ, θ1, θ1̇, θ2, θ2̇); physical parameters (pole length/mass ratio increases difficulty relative to classic problem); episode length up to 1000 steps; number of discrete environment instances used during training 5^6 = 15,625 (as reported), post-evaluation grid 3^6 = 729 conditions.",
            "complexity_level": "high (described in paper as significantly more complex than the classic double-pole problem due to larger variation ranges and second pole size)",
            "variation_measure": "Number of distinct initial-state instances (discretized into 5 values per variable → 5^6 = 15,625 possible environments as used during evolution; post-evaluation considered 3^6 = 729 combinations); ranges given for each variable (explicit numeric ranges in paper).",
            "variation_level": "high (wide ranges of initial positions and velocities; combinatorially many possible initial conditions)",
            "performance_metric": "Average episodic fitness = reward of 1 point per timestep alive (max 1000) aggregated across evaluation episodes; statistical comparisons (Kruskal–Wallis, Mann–Whitney U) used for significance.",
            "performance_value": "Reported as average fitness of best agents (numeric fitness values not printed in body text). Key quantitative training/test details: training run for 1 × 10^9 evaluation steps; post-evaluation: best agents evaluated over 1000 episodes with uniformly-sampled environmental conditions. Curriculum-learning variants (power functions, esp. cubic) significantly outperform standard random sampling (p &lt; 0.001).",
            "complexity_variation_relationship": "Yes — the paper explicitly discusses that environment complexity scales with the number and ranges of variables and that the number of conditions (variations) grows combinatorially, creating scalability challenges. It shows that preferentially sampling more difficult environmental conditions (via a power-based difficulty function) improves evolved performance and that exposing agents to environmental conditions with similar difficulty levels reduces stochasticity in fitness estimates. The paper also notes an exponential increase in time required to estimate difficulty as the number of varied environmental variables increases.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Neuroevolution with curriculum learning (environment selection); comparisons against standard random environment sampling and curriculum with linear vs power difficulty functions; algorithms used: OpenAI-ES and Steady-State EA.",
            "generalization_tested": true,
            "generalization_results": "Yes — best agents were post-evaluated across a dense grid of environmental conditions (3^6 = 729) and the curriculum-learning (cubic power) condition equaled or outperformed the standard method in the large majority of those environmental conditions; authors report curriculum-trained agents are more robust to variations.",
            "sample_efficiency": "Training reported: 1 × 10^9 evaluation steps for OpenAI-ES experiments (10 replications); authors note the curriculum-learning (cubic) starts to outperform the standard algorithm relatively early during training (figure-based claim), but exact sample-to-performance curves' numeric thresholds are not reported in text.",
            "key_findings": "Applying curriculum learning that estimates difficulty per-environment (based on recent agents' fitness) and preferentially samples harder conditions (power function, cubic performed best) produces agents that achieve higher average fitness and are more robust across the combinatorially large set of initial-state variations; also reduces fitness stochasticity by equalizing difficulty exposure across individuals. The approach faces scalability limits because difficulty estimation cost grows exponentially with number of varied variables.",
            "uuid": "e1016.0"
        },
        {
            "name_short": "Bipedal Hardcore Walker",
            "name_full": "Bipedal walker hardcore agent (feedforward controller)",
            "brief_description": "A simulated bipedal walker (OpenAI Gym 'BipedalWalkerHardcore') controlled by a 3-layer feed-forward network (24 sensory, 64 internal, 4 motor) whose weights are evolved via neuroevolution; tested across procedurally varied obstacle configurations.",
            "citation_title": "Automated curriculum learning for embodied agents a neuroevolutionary approach",
            "mention_or_use": "use",
            "agent_name": "Bipedal walker hardcore agent",
            "agent_description": "Neuroevolutionary training of a feed-forward neural controller (3-layer, 24 inputs, 64 hidden units, 4 outputs) using OpenAI-ES and Steady-State EA; rewarded for forward distance traveled (capped) and penalized for falling.",
            "agent_type": "simulated agent",
            "environment_name": "BipedalWalkerHardcore environment (OpenAI Gym)",
            "environment_description": "A 2D bipedal agent (skull + two two-segment legs) must walk forward through an environment containing sequences of five obstacles; obstacles chosen from five types (small stumps, large stumps, uphill stairs, downhill stairs, holes). A random downward force is applied to hull at start of episodes; falling yields a −100 penalty, distance reward up to 300.",
            "complexity_measure": "Obstacle sequence combinatorics (each of 5 obstacle slots independently chosen among 5 types); sensor and action dimensionality (24 sensors, 4 motors); episode scoring and falling penalty define task difficulty; number of possible environments reported as 5^5 = 3,625 (paper's reported combinatorial count).",
            "complexity_level": "high (challenging locomotion with obstacles, random perturbations, and high-dimensional sensorimotor input)",
            "variation_measure": "Number of distinct environment instances = 5^5 = 3,625 (as reported in paper); randomness in initial downward force; obstacle permutations provide structured variation across episodes.",
            "variation_level": "high (multiple obstacle types and permutations leading to thousands of possible environment instances)",
            "performance_metric": "Average episodic fitness (distance-based reward up to 300, with −100 fall penalty); authors use average fitness of best agents over post-evaluation episodes and report statistical tests (Kruskal–Wallis, Mann–Whitney U).",
            "performance_value": "Reported as average fitness of best agents (exact numeric fitness values not printed in main text). Training details: evolutionary runs for 3 × 10^8 evaluation steps; post-evaluation: best agents tested for 500 episodes with random obstacle categories. Curriculum-learning (power difficulty functions) significantly outperform standard sampling (p &lt; 0.01).",
            "complexity_variation_relationship": "Yes — the paper frames environmental variation (obstacle permutations and random perturbations) as a primary source of task difficulty distinct from initial-state variation in the pole task; it shows that preferentially sampling more difficult environment categories (via power-based difficulty functions) improves evolved performance and robustness. The authors again note scalability concerns: estimating per-environment difficulty across many variation dimensions is costly and grows combinatorially.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Neuroevolution with curriculum learning (environment selection by difficulty subsets); compared to standard uniform random environment sampling and curriculum with linear vs power difficulty functions; used OpenAI-ES and Steady-State EA.",
            "generalization_tested": true,
            "generalization_results": "Yes — post-evaluation across all 5^5 (reported 3,625) obstacle combinations showed the curriculum-learning (cubic/power variants) equals or outperforms the standard method in the majority of environment instances; curriculum-trained agents are reported to be more robust across obstacle permutations.",
            "sample_efficiency": "Training reported: 3 × 10^8 evaluation steps for the OpenAI-ES experiments (10 replications); curriculum-learning with cubic function begins to show advantage early in training (figure-based claim), but specific episode counts to reach thresholds are not provided in text.",
            "key_findings": "Curriculum learning that ranks environments by difficulty (estimated from recent agents' fitness) and biases selection toward harder categories (power-based interval sizing) yields higher final performance and more robust walkers across many obstacle permutations compared to uniform sampling; improvements hold across both OpenAI-ES and Steady-State EA. The method reduces fitness stochasticity and increases exposure to challenging conditions but faces scalability limitations as variation dimensionality grows combinatorially.",
            "uuid": "e1016.1"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Paired open-ended trailblazer (poet): endlessly generating increasingly complex and diverse learning environments and their solutions",
            "rating": 2,
            "sanitized_title": "paired_openended_trailblazer_poet_endlessly_generating_increasingly_complex_and_diverse_learning_environments_and_their_solutions"
        },
        {
            "paper_title": "Evolution strategies as a scalable alternative to reinforcement learning",
            "rating": 2,
            "sanitized_title": "evolution_strategies_as_a_scalable_alternative_to_reinforcement_learning"
        },
        {
            "paper_title": "Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments",
            "rating": 2,
            "sanitized_title": "teacher_algorithms_for_curriculum_learning_of_deep_rl_in_continuously_parameterized_environments"
        },
        {
            "paper_title": "Incremental evolution of complex general behavior",
            "rating": 1,
            "sanitized_title": "incremental_evolution_of_complex_general_behavior"
        },
        {
            "paper_title": "New methods for competitive coevolution",
            "rating": 1,
            "sanitized_title": "new_methods_for_competitive_coevolution"
        }
    ],
    "cost": 0.010811749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Automated curriculum learning for embodied agents a neuroevolutionary approach
0123456789</p>
<p>Nicola Milano *email:nicola.milano@istc.cnr.it 
Institute of Cognitive Science and Technologies
National Research Council
RomeItaly</p>
<p>Stefano Nolfi 
Institute of Cognitive Science and Technologies
National Research Council
RomeItaly</p>
<p>Automated curriculum learning for embodied agents a neuroevolutionary approach
012345678910.1038/s41598-021-88464-5
We demonstrate how the evolutionary training of embodied agents can be extended with a curriculum learning algorithm that automatically selects the environmental conditions in which the evolving agents are evaluated. The environmental conditions are selected to adjust the level of difficulty to the ability level of the current evolving agents, and to challenge the weaknesses of the evolving agents. The method does not require domain knowledge and does not introduce additional hyperparameters. The results collected on two benchmark problems, that require to solve a task in significantly varying environmental conditions, demonstrate that the method proposed outperforms conventional learning methods and generates solutions which are robust to variations and able to cope with different environmental conditions.</p>
<p>Neuroevolution 1 is a method widely used to evolve embodied and situated agents. Clearly, the conditions in which the agents are evaluated affect the course of the evolutionary process. Ideally, the environmental conditions should match the skill level of the evolving agents, i.e. should be sufficiently difficult to exhort an adequate selective pressure and sufficiently simple to ensure that random variations can occasionally produce progresses. This can be obtained by varying the evaluation conditions during the evolutionary process, i.e. by increasing the complexity of the environmental conditions across generations and by selecting conditions that are challenging for the current evolving agents.</p>
<p>A possible method that can be used to achieve this objective is incremental evolution, i.e. the utilization of an evolutionary process divided into successive phases of increasing complexity. For example the evolution of an ability to visually track target objects can be realized by exposing the evolving agents first to large immobile targets, then to small immobile targets, and finally to small moving targets 2 . Similarly, the evolution of agents evolved for the ability to capture escaping prey can be organized in a series of subsequent phases in which the speed of the prey and the pursuit delay is progressively increased 3 . However, these approaches presuppose that the tasks can be ordered by difficulty, when in reality they might vary along multiple axes of difficulty. Alternatively, the incremental process can be realized by dividing the problem in a set of sub-problems and by using a multi-objective optimization algorithm that select the agents that excel in at least one sub-problem 4 . In general, incremental approaches can be effective but introduce hard to tune hyperparameters and require the utilization of domain dependent knowledge. In the context of reinforcement learning, incremental methods of this kind are referred with the term task-level curriculum learning 5 .</p>
<p>A second possible method consists in competitive co-evolution, i.e. the evolution of agents that compete with other evolving agents. For example, the co-evolution of two populations of predators and prey robots selected for the ability to capture prey and escape predators, respectively [6][7][8][9][10][11][12] . Indeed, competitive co-evolution can produce automatically a progressive complexification of the environmental conditions for both populations. Moreover, competitive co-evolution permits to expose the evolving agents to conditions that challenge their weaknesses since their exploitation have an adaptive value for the opponent population. Unfortunately, competitive coevolution does not necessarily lead to a progressive complexification of the adaptive problem (for a discussion see 10,12 ). In addition, it can only be applied to problems that can be formulated in a competitive manner. In the context of reinforcement learning, competitive methods of this kind are referred with the term self-play 5 .</p>
<p>A third possible method, that we investigate in this article, consists in extending the evolutionary methods with a curriculum learning algorithm that manipulates the environmental conditions in which the evolving agents are evaluated by selecting those that have the proper level of difficulty and that challenge the weaknesses of the current evolving agents. We refer to this method with the term curriculum learning. Following the www.nature.com/scientificreports/ terminology commonly used in the evolutionary computation community, we do not extend the meaning of the term to the incremental and the competitive methods described above. The utility of selecting the learning samples or the learning conditions is widely investigated in the context of supervised and reinforcement learning.</p>
<p>In the case of supervised learning 13 , demonstrated how the preferential selection of samples which are neither too hard or too easy can improve the learning speed and generalization. In addition 14 , demonstrated how gradually increasing the complexity of the samples permits to solve problems that cannot be learned with conventional methods, providing that easy samples continue to be presented, although less frequently, to avoid forgetting.</p>
<p>In the context of reinforcement learning, the selection of the learning samples can be realized by storing the previous learning experiences in a replay buffer and by selecting the samples from the buffer according to some measure of difficulty or usefulness. The priority can be given to the samples which generate the highest learning progress (measured through the temporal difference (TD) error 15 ), or the samples with the highest complexity (measured by the relationship between the TD error and the difficulty of the current curriculum for sample efficiency 16 ), or the less common samples which maximize sample diversity 17 .</p>
<p>In the context of evolutionary algorithm, the possible advantage of manipulating the learning experiences, i.e. the condition in which the evolving agents are evaluated, has not been explored yet. In a recent work 18 proposed an algorithm that evolve a population of increasingly complex agent-environmental couples. Each couple is formed by an agent evolved for the ability to operate effectively in certain environmental conditions and by a description of that environmental conditions, i.e. the environmental condition in which the agent is evaluated. The progressive complexification is obtained by: (1) optimizing agents in their associated environments, (2) generating new environments by creating copies with variation of the existing environments, and (3) attempting to transfer copies of the current agents into another agent-environmental couple. As shown by the authors, this method permits to produce agents capable to operate effectively in remarkably complex conditions. On the other hand, this method produces solutions tailored to specific environmental conditions that do not necessarily generalize to other conditions.</p>
<p>In this work we propose to extend evolutionary methods with a curriculum learning algorithm that automatically selects the environmental conditions in which the evolving agents are evaluated. The environmental conditions are selected so to adjust the level of difficulty to the ability level of the current evolving agents and so to challenge the weaknesses of the evolving agents. The method can be used in combination with any evolutionary algorithm and does not introduce additional hyperparameters. The results collected on two benchmark problems, that requires to solve a task in significantly varying environmental conditions, demonstrate that the method proposed produces significantly better performance than a conventional method and generates solutions that are robust to variations.</p>
<p>The method</p>
<p>The rationale of the method consists in estimating the difficulty level of the environmental conditions encountered by the evolving agents of recent generations and in using this information to expose the agents of successive generations to environmental conditions that have the appropriate level of difficulty. The difficulty level of the environmental conditions can be estimated on the basis of the inverse of the relative fitness obtained by the agents evaluated in those conditions. Notice that this also permits to select environmental conditions that threaten the specific weaknesses of the current evolving agents. Indeed, the level of difficulty calculated in this way rate the level of difficulty for the current evolving agents.</p>
<p>We describe our method and the problems used to verify its efficacy in the following sub-sections.</p>
<p>The adaptive problems. To verify the efficacy of the method we selected two problems, commonly used as benchmark in continuous control optimization, that are sufficiently difficult to appreciate the relative efficacy of different algorithms and that require to solve a problem in significantly varying conditions. The first problem is the Long double-pole balancing problem 19 (https:// github. com/ snolfi/ longd pole), a harder version than the classic non-markovian double-pole balancing problem.</p>
<p>The problem consists in controlling a cart with two poles, attached with passive hinge joints on the top side of the cart, for the ability to keep the poles balanced (Fig. 1). The cart has a mass of 1 kg. The long pole and the short pole have a mass of 0.5 and 0.25 kg and a length of 1.0 and 0.5 m, respectively. The agent has three sensors, that encode the current position of the cart on the track (x) and the current angle of the two poles (θ 1 and θ 2 ) and one motor. The activation state of the motor is normalized in the range [− 10.0, 10.0] N and is used to set the force applied to the cart.</p>
<p>The neural network controller of the agent is constituted by a LSTM network 20 with 3 layers, 3 sensory neurons, 10 internal units, and 1 motor neuron. We used a LSTM network since the problem requires to determine the actions also on the basis of the previous observation/internal/action state.</p>
<p>The environmental conditions that are subjected to variation, in the case of this problem, are the initial position of the cart and the initial angular positions and velocity of the poles. These properties vary in the following intervals: [− 1.944 &lt; x &lt; 1.944, − 1.215 &lt; ẋ &lt; 1.215, − 0.0472 &lt; θ 1 &lt; 0.0472, − 0.135088 &lt; θ 1 &lt; 0.135088, − 0.10472 &lt; θ 2 &lt; 0.10472, − 0.135088 &lt; θ 2 &lt; 0.135088]. Evaluation episodes terminate after 1000 steps or prematurely when the angular position of one of the two poles exceeded the range [− π 5 , π 5 ] rad or the position of the cart exceed the range [− 2.4, 2.4] m. During the evolutionary process, the environmental conditions are determined by using 5 values distributed in uniform manner within the intervals described above. Consequently, the number to different environmental conditions that can be experienced by the evolving agents are 5 6  www.nature.com/scientificreports/ The cart is rewarded with 1 point every step until the termination of the episode, i.e. it is rewarded for the ability to keep the poles balanced as long as possible. The state of the sensors, the activation of the neural network, the force applied to the cart, and the position and velocity of the cart and of the poles are updated every 0.02 s.</p>
<p>The long double pole differs from the classic non-Markovian double-pole balancing problem for the length of the second pole and the range of variation of initial state of the agent. More specifically, the length and the mass of the second pole corresponds to 1 2 of the length of the first pole (instead of 1 10 ), and the range of variation of the environmental conditions is much larger than in the classic double-pole balancing problem. These variations increase significantly the complexity of the problem (for more details see 19,20 ).</p>
<p>The second problem is the Bipedal walker hardcore available in the Open AI Gym library 22 . The problem consists in controlling a bipedal agent formed by a skull and two two-segments legs for the ability to walk forward in an environment containing obstacles, holes, and stairs. The agent is rewarded for the distance travelled forward, up to a maximum distance of 300, and punished with − 100 for falling down.</p>
<p>The agent has 24 sensory neurons that encode the orientation of the skull, the angular, horizontal and vertical velocity of the skull, the position and the velocity of the two hinge joints of each leg, the state of the contact sensors located in the terminal segments the legs, and the state of the ten LIDAR rangefinders that provide information on the nearby frontal portion of the environment. Moreover, the agent has 4 motor neurons that control the torque applied on the joints of the legs.</p>
<p>The neural network of the agent is constituted by a 3-layers feed-forward neural network with 24 sensory neurons, 64 internal neurons, and 4 motor neurons.</p>
<p>The pitfalls present in the environment are subjected to variations. Each environment include a sequence of five obstacles, see an example in Fig. 2. Since each obstacle is selected among five different alternatives (small  www.nature.com/scientificreports/ stumps, large stumps, uphill stairs, downhill stairs, and holes), the number of possible different environments is 5 5 = 3625. A random downward force is applied to the walker's hull at the beginning of each episode. Notice that in the case of the long double-pole balancing problem the environmental variations affect the initial state of the agent. In the case of the bipedal walker hardcore, instead, the environmental variations affect primarily the characteristics of the external environment.</p>
<p>The connection weights of the neural network controllers are encoded in a vector of floating point values and evolved through the evolutionary algorithm described in the following section.</p>
<p>The evolutionary algorithm extended with curriculum learning. To evolve the connection weights of the neural network controller we used the Open-AI evolutionary strategy (OpenAI-ES) proposed by 23 extended with our curriculum learning algorithm.</p>
<p>We selected the OpenAI-ES method since it is one of the most effective evolutionary methods for continuous control problems 6,21,23,24 . The algorithm operates on a population centered on a single parent (θ), uses a form of finite difference method to estimate the gradient of the expected fitness, and update the center of the population distribution with the Adam stochastic optimizer 25 .</p>
<p>The pseudo-code of the algorithm is reported below. At each generation, the algorithm generates the gaussian vectors ε that are used to make the offspring, i.e. the perturbed versions of the parent (line 4), which are then evaluated (lines 5-6). The usage of couples of mirrored samples, that receive opposite perturbations, improves the accuracy of the estimation of the gradient 26  In the standard version of the algorithm each offspring is evaluated for η episodes in which the environmental conditions are chosen randomly within a uniform distribution. In other words, the function φ used to select the environmental conditions correspond to a simple random function. This means that in the case of the long double-pole problem, the position of the cart and the angle and the velocity of the poles are selected randomly with a uniform distribution at the beginning of each evaluation episodes within the variations ranges described in "The adaptive problems" section. In the case of the bipedal walker hardcore problem, the type of each of the five obstacles present in the environment is chosen randomly from among one of the five possible types.</p>
<p>In the curriculum learning version of the algorithm, instead, the η environmental conditions experienced during the η evaluation episodes are chosen randomly from among all possible environmental conditions during the first 1 10 of the evolutionary process only. Then, they are chosen randomly from among η subsets characterized by different levels of difficulty. The subsets are obtained by: (1)  www.nature.com/scientificreports/ intervals on the basis of a difficulty function (see below), and (3) selecting η environmental conditions randomly from η subsets, i.e. selecting η environmental conditions characterized by η different levels of difficulty. See the pseudocode included below.</p>
<p>In case a subset results empty, the environmental conditions are chosen from one of the two nearest nonempty subsets, chosen randomly. The initial phase in which the environmental conditions are chosen randomly from among all possible conditions is necessary in order to start estimating the performance obtained in different conditions. This technique ensures that each agent experiences conditions with different level of difficulties, ranging from the easiest to the hardest conditions. Furthermore, it ensures that all agents experience conditions with a similar overall level of difficulty. It thus reduces the level of stochasticity of the fitness measure caused by the fact that the fitness of lucky and un-lucky individuals, i.e. of individuals that encountered easier and harder environmental conditions, tend to be over-estimated and under-estimated, respectively. The fact that the selection of the environmental conditions is stochastic reduces the risk of overfitting and ensures that the estimated difficulty of the environmental conditions continue to be updated during the course of the evolutionary process. Indeed, all environmental conditions have a chance to be selected, although the probability with which they are selected varies. For an analysis of the importance of using stochasticity to avoid overfitting in a related model see Ref. 27 .</p>
<p>The need to avoid overfitting is also the reason why we preferred our method, that operates by selecting the environmental conditions within the set of all possible conditions, to possible alternative methods that operate by generating suitable environmental conditions. An approach of this type can be effective to generate conditions characterized by few variables, e.g. to generate the goal of an agent trained through reinforcement learning 28 . However, it might not be suitable to generate sufficiently varied conditions characterized by several dimension of variability.</p>
<p>The overall level of difficulty can be varied by varying the function used to determine the intervals of the categories. For this reason, we will refer to this function with the term difficulty function.</p>
<p>The utilization of linear function (Fig. 3, blue line) does not alter the overall level of the difficulty of the η environmental conditions experienced by the agents with respect to a standard evolutionary algorithm in which the environmental conditions are selected randomly within all possible conditions. Instead, the utilization of a power function permits to select preferentially difficult environmental conditions (Fig. 3, red line). Indeed, the intervals calculated with a power function are larger for easy conditions, i.e. conditions in which the agents obtained a high fitness, and progressively smaller for difficult conditions. i.e. conditions in which the agents achieved a low fitness (Fig. 3, red line). This implies that the numerosity of the categories is large for the easiest conditions and progressively smaller for the more difficult conditions and consequently that the conditions belonging to the easier subsets are chosen less frequently than the conditions belonging to the harder subsets. Notice that, as mentioned above, the performance of the environmental conditions is calculated on the basis of the fitness obtained by recent evolving agents. Consequently, the performance values do not indicate an absolute level of difficulty but rather the level of difficulty relative to the skills of the current evolving agents.</p>
<p>The intensity of the preferential selection of difficult condition can be varied by varying the exponential of the power function. The higher the exponential of the function is, the higher the overall difficulty of the selected environmental conditions becomes.</p>
<p>To verify the generality of the curriculum learning algorithm we tested it also in combination with a classic evolutionary algorithm, i.e. the steady state algorithm described in 19 . This algorithm operates by replacing, each generation, the worst half individuals with a perturbed version of the fittest half individuals.</p>
<p>The pseudo-code of the algorithm is reported below. The procedure starts by creating a population of randomly different vectors that encode the parameters of a corresponding population (line 1). The population θ thus  Figure 4 shows the results obtained on the long double-pole problem by using the standard Open-AI algorithm and the Open-AI algorithm with curriculum learning. For the latter case, we report the results obtained with the linear difficulty function and with power difficulty functions with exponentials 2, 3 and 4. The evolutionary process has been continued for 1 × 10 9 evaluation steps. Performance refer to the average fitness obtained by the best agents of each replication post-evaluated for 1000 evaluation episodes during which the environmental conditions were chosen randomly with a uniform distribution within the range described in "The adaptive problems" section. www.nature.com/scientificreports/ The group comparison performed with the Kruskal-Wallis test indicates that at least one condition dominates other conditions (data number = 10, p value &lt; 0.001). The pairwise comparison carried out with the Mann-Whitney U test with Bonferroni correction shows that the curriculum learning with the power functions outperform the curriculum learning condition with the linear function (data number = 10, p value &lt; 0.001 with Bonferroni corrections α = 5) and the standard method (data number = 10, p value &lt; 0.001 with Bonferroni corrections α = 5). The curriculum learning with the linear function outperform the standard condition (data number = 10, p value &lt; 0.01 with Bonferroni corrections α = 5). The curriculum learning condition with the cubic power function outperform the curriculum learning conditions with the x 2 and x 4 power functions (data number = 10, p value &lt; 0.01 with Bonferroni corrections α = 5). See also Table 1 in supplementary material for a more exhaustive analysis. Videos displaying the typical behaviors obtained with and without curriculum learning are available from: https:// www. youtu be. com/ playl ist? list= PLcNe-IgRAL 8Umwy ythmM lJKy5 ZUZip rCI.</p>
<p>Results</p>
<p>The fact that the curriculum learning algorithm with the linear difficulty function outperforms the standard algorithm demonstrates that a first advantage of the curriculum learning algorithm is due to its ability to expose all agents to environmental conditions with similar levels of difficulty.</p>
<p>The fact that the curriculum learning algorithm with power difficulty functions outperform the curriculum learning algorithm with the linear difficulty function demonstrates that selecting preferentially difficult environmental conditions promotes the evolution of better solutions.</p>
<p>The analysis of the performance during the evolutionary process indicates that the curriculum learning algorithm with the cubic difficulty function starts to outperform the standard algorithm rather early (Fig. 5). Figure 6 shows an analysis of the performance across generations for different environmental conditions. For this analysis we considered all the possible environmental conditions that can be generated by combining 3 values, distributed in a uniform manner within the intervals described above, for the 6 environmental variables subjected to variation, i.e. the initial position and velocity of the cart and the initial angular positions and velocity of the poles. Consequently, the number to different environmental conditions on which the agents are  www.nature.com/scientificreports/ post-evaluated are 3 6 = 729 . The comparison of performance obtained with the standard algorithm (Fig. 6, top) and with the curriculum learning algorithm with the cubic difficulty function shows that the latter algorithm equals or outperforms the former algorithm in the large majority of the environmental conditions (Fig. 6, bottom). Figure 7 shows the performance obtained in the case of the bipedal walker hardcore problem. The evolutionary process has been continued for 3 × 10 8 evaluation steps. Performance refer to the average fitness obtained by www.nature.com/scientificreports/ the best agents of each replication post-evaluated for 500 evaluation episodes during which the category of each of the five obstacles has been selected randomly. Also in the case of this problem the Kruskal-Wallis test indicates that at least one condition dominates other conditions (data number = 10, p value &gt; 0.05). The Mann-Whitney U pairwise comparison shows that the curriculum learning with the power functions outperform the curriculum learning condition with the linear function (data number = 10, p value &lt; 0.01 with Bonferroni corrections α = 5) and the standard method (data number = 10, p value &lt; 0.01 with Bonferroni corrections α = 5). The curriculum learning conditions with the x 2 , x 3 and x 4 power functions do not differ among themselves (data number = 10, p value &gt; 0.05 with Bonferroni corrections α = 5). See also Table 2 in supplementary material for a more exhaustive analysis. Videos displaying the typical behaviors obtained with and without curriculum learning are available from: https:// www. youtu be. com/ playl ist? list= PLcNe-IgRAL 8Umwy ythmM lJKy5 ZUZip rCI.</p>
<p>The analysis of the course of the evolutionary process indicates that the curriculum learning algorithm with the cubic difficulty function starts to outperform the standard algorithm rather early (Fig. 8). Figure 9 shows an analysis of the performance across generations for different environmental conditions. For this analysis we considered all the possible environmental conditions that can be generated by combining all the 5 obstacles. Consequently, the number of different environmental conditions on which the agents are postevaluated are 5 5 = 3625. Also in this case, the comparison of performance obtained with the standard algorithm ( Fig. 9, top) and with the curriculum learning algorithm with the cubic function (Fig. 9, center) shows that the latter algorithm equals or outperform the former algorithm in the large majority of the environmental conditions (Fig. 9, bottom).</p>
<p>The results of the experiments performed with the Steady State algorithm demonstrate that the usage of curriculum learning permit to achieve significantly better results also with this algorithm both in the case of the long double-pole balancing problem (Figs. 10 and 11) and in the case of the bipedal hardcore problem (Figs. 12  and 13). More specifically, the pairwise comparison carried out with the Mann-Whitney U test shows that the  www.nature.com/scientificreports/ curriculum learning condition outperform the standard condition both the long douple-pole and in the bipedal hardcore problems (data number = 10, p value &lt; 0.001 in both cases).</p>
<p>Discussion</p>
<p>We proposed a method that enables evolutionary algorithms to select the environmental conditions that facilitate the evolution of effective solutions. This is realized by adding a curriculum learning component that estimates the difficulty level of the environmental conditions from the perspective of the evolving agents and selects   www.nature.com/scientificreports/ conditions with different level of difficulties in which the frequency of difficult cases is greater than the frequency of easier cases. The estimation of the difficulty level of the environmental conditions is performed on the basis of the fitness obtained in those conditions by agents evaluated recently. The selection of suitable environmental conditions is realized by selecting η environmental conditions from η corresponding subsets characterized by different levels of difficulty, where η is the number of evaluation episodes. Finally, the preferential selection of difficult conditions is realized by increasing the probability to select difficult environmental conditions, i.e. by determining the intervals of the subsets with a power function. The utilization of this method also reduces the stochasticity of the fitness measure since it ensures that agents are exposed to environmental conditions that have similar levels of difficulty.</p>
<p>The curriculum learning component proposed is general and can be combined with any evolutionary algorithm. In this paper, we verified its efficacy in combination with the Open-AI neuro-evolutionary strategy 23 and with a Steady State evolutionary strategy 19 . We evaluated the efficacy of the method on the long double-pole and on the bipedal hardcore walker problems, that are commonly used to benchmark evolutionary and reinforcement learning problems and that require to handle significantly varying environmental conditions.</p>
<p>The obtained results indicate that the curriculum learning method produces significantly better results in the two problems considered in all conditions. The fact that the curriculum learning condition with the cubic difficulty function produce significantly better performance than the curriculum learning condition with the linear function confirms that the preferential selection of difficult condition enhances the efficacy of the evolutionary process. The fact that the curriculum learning condition with the linear difficulty function outperforms the standard algorithm demonstrates that exposing the agents to environmental conditions with similar level of difficulty also enhances the efficacy of the evolutionary process. Consequently, the advantage gained by the curriculum learning with the cubic difficulty function can be ascribed both to the ability of the method to reduce the stochasticity of the fitness measure and to the ability to increase the frequency of difficult environmental conditions.</p>
<p>The method proposed presents several advantages with respect to related techniques such as incremental evolution and competitive coevolution: it can be applied to any problem, it does not require to set additional hyperparameters, and it does not require the usage of domain specific knowledge. A limit of the method concerns its scalability with respect to the number of environmental variables that are subjected to variations. This since the time required to estimate the difficulty level of the environmental conditions increases exponentially with the number of variables subjected to variation. This is a general challenge that also affect incremental evolution and competitive evolution. A possible solution to this problem, that deserves to be investigated in future studies, consists in estimating the difficulty level of the environmental conditions through a neural network that receives as input the environmental conditions and produces as output the estimated difficulty levels. This network could be trained on the basis of the same historical data that we used in our method. The potential advantage of this approach is that it can generalize, i.e. it can estimate correctly the difficulty level also of environmental conditions that were not experienced in previous evaluations.</p>
<p>Another aspect that can be considered in future studies is the criterion used to select the environmental conditions. In this work we proposed an approach that relies on the difficulty level of the environmental conditions. An alternative criterion, that has been explored in the context of supervised learning [29][30][31] , and reinforcement learning 24 is learning progress, namely the propensity of examples or of environmental conditions to induce learning progress.  </p>
<p>Figure 1 .
1The long double-pole balancing problem.</p>
<p>Figure 2 .
2The bipedal worker hardcore problem. The agents is shown on the left of the figure. In this example the environment includes, from left to right, a large stump, a small stump, an uphill stair, a downhill stair, and a hole. The type of each of the five obstacles vary in evaluation episodes.Scientific Reports | (2021) 11:8985 | https://doi.org/10.1038/s41598-021-88464-5</p>
<p>. Offspring are evaluated for η episodes in variable environmental conditions (lines 5-6). The average fitness values obtained during evaluation episodes are then ranked and normalized in the range [− 0.5, 0.5] (line 7). This normalization makes the algorithm invariant to the distribution of fitness values and reduce the effect of outliers. The estimated gradient g corresponds to the average of the dot product of the samples ε and of the normalized fitness values (line 8). Finally, the gradient is used to update the parameters of the parent through the Adam 25 stochastic optimizer (line 9).</p>
<p>normalizing the performance ρ achieved on each environmental condition during the last 5 evaluations in the range [0.0, 1.0], (2) calculating η performance Scientific Reports | (2021) 11:8985 | https://doi.org/10.1038/s41598-021-88464-5</p>
<p>a matrix including λ vectors which encode the parameters of λ corresponding individuals. Then, for a certain number of generations, the algorithm evaluates the fitness of the individuals forming the population (line 3), ranks the individual of the population on the basis of their fitness (line 5), and replace the parameters of the worse 2 individuals with a copy with variations of the 2 fittest individuals (line 8). Variations are introduced by adding to copied parameters a vector of Gaussian numbers with average 0 and variance σ.</p>
<p>Figure 3 .
3Difficulty intervals obtained by using a linear and a quadratic power difficulty function. Data shown in blue and red, respectively. The horizontal axes represent performance normalized in the range [0.0, 1.0], i.e. the fitness obtained by the last 5 agents normalized in the range [0.0, 1.0]. The vertical axes represents the intervals of the subsets, with η = 10. The intervals of the subsets in the case of the linear function are [</p>
<p>Figure 4 .
4Performance of the agents evolved with the OpenAI-ES algorithm on the double pole balancing problem with the standard and with the curriculum learning version of the algorithm. For the latter case the figure reports the results obtained by using a linear difficulty function (linear) and a power function with exponential 2, 3 and 4 (x 2 , x 3 , and x 4 ). Each boxplot shows the results obtained in 10 replications. Boxes represent the inter-quartile range of the data and horizontal lines inside the boxes mark the median values. The whiskers extend to the most extreme data points within 1.5 times the inter-quartile range from the box.</p>
<p>Figure 5 .
5Performance during the evolutionary process of the agents evolved with the standard and with curriculum learning OpenAI-ES algorithm. The latter data refers to the experiments performed with the cubic difficulty function. Each curve shows the average results of 10 replications. The shadow indicate the 90% bootstrapped confidence intervals. Scientific Reports | (2021) 11:8985 | https://doi.org/10.1038/s41598-021-88464-5</p>
<p>Figure 6 .
6Heatmap of the performance for different environmental conditions during the evolutionary process. The vertical and horizontal axes indicate the environmental conditions and the generations, respectively. The top and central figures show the performance obtained in the experiment performed with the standard algorithm and with the curriculum learning algorithm with the cubic difficulty function. The bottom figure shows the difference between the performance obtained in the two conditions. Average results of 10 replications for each experimental condition. The figure is generated using python 3.6.7 https:// www. python. org/ downl oads/. Scientific Reports | (2021) 11:8985 | https://doi.org/10.1038/s41598-021-88464-5</p>
<p>Figure 7 .
7Performance of the agents evolved with the OpenAI-ES algorithm on the bipedal hardcore problem with the standard and the curriculum learning version of the algorithm. For the latter the figure reports the results obtained with the linear difficulty function (linear) and a power function with exponential 2, 3 and 4 (x 2 , x 3 , and x 4 ). Each boxplot shows the results obtained in 10 replications. Boxes represent the inter-quartile range of the data and horizontal lines inside the boxes mark the median values. The whiskers extend to the most extreme data points within 1.5 times the inter-quartile range from the box.</p>
<p>Figure 8 .
8Performance during the evolutionary process of the agents evolved with the standard and with the curriculum learning OpenAI-ES algorithm. The latter refers to the experiments performed with the cubic function. The curves show the average results of 10 replications. The shadow indicate the 90% bootstrapped confidence intervals. Scientific Reports | (2021) 11:8985 | https://doi.org/10.1038/s41598-021-88464-5</p>
<p>Figure 9 .Figure 10 .
910Heatmap of the performance for different environmental conditions during the evolutionary process. The vertical and horizontal axes indicate the environmental conditions and the generation. The top and central figures show the performance obtained in the experiment performed with the standard algorithm and with the curriculum learning algorithm with the cubic difficulty function. The bottom figure shows the difference between the performance obtained in the two conditions. Average results of 10 replications for each experimental condition. The figure is generated using python 3.6.7 https:// www. python. org/ downl oads/. Performance of the agents evolved with the Steady State algorithm on the double pole balancing problem with the standard and with the curriculum learning version of the algorithm. The latter data refers to the experiments performed with the cubic difficulty function. Boxes represent the inter-quartile range of the data and horizontal lines inside the boxes mark the median values. The whiskers extend to the most extreme data points within 1.5 times the inter-quartile range from the box.</p>
<p>Figure 11 .
11Performance during the evolutionary process of the agents evolved with the standard and with curriculum learning Steady State algorithms on the double pole balancing problem. The latter data refers to the experiments performed with the cubic difficulty function. Each curve shows the average results of 10 replications. The shadow indicate the 90% bootstrapped confidence intervals.</p>
<p>Figure 12 .
12Performance of the agents evolved with the Steady State algorithm on the bipedal hardcore problem with the standard and with the curriculum learning version of the algorithm. The latter data refers to the experiments performed with the cubic difficulty function. Boxes represent the inter-quartile range of the data and horizontal lines inside the boxes mark the median values. The whiskers extend to the most extreme data points within 1.5 times the inter-quartile range from the box. Scientific Reports | (2021) 11:8985 | https://doi.org/10.1038/s41598-021-88464-5</p>
<p>Received: 22 January 2021; Accepted: 12 April 2021</p>
<p>Figure 13 .
13Performance during the evolutionary process of the agents evolved with the standard and with curriculum learning Steady State algorithms on the bipedal hardcore problem. The latter data refers to the experiments performed with the cubic difficulty function. Each curve shows the average results of 10 replications. The shadow indicate the 90% bootstrapped confidence intervals.</p>
<p>= 15,625.   Scientific Reports | 
(2021) 11:8985 | 
https://doi.org/10.1038/s41598-021-88464-5 </p>
<p>© The Author(s) 2021
Scientific Reports |Author contributionsN.M. and S.N. contributed equally to the work.Competing interestsThe authors declare no competing interests.Additional informationSupplementary InformationThe online version contains supplementary material available at https:// doi. org/ 10. 1038/ s41598-021-88464-5.Correspondence and requests for materials should be addressed to N.M.Reprints and permissions information is available at www.nature.com/reprints.Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
. J Lehman, R Miikkulainen, Neuroevolution. Schorpedia. 8630977Lehman, J. &amp; Miikkulainen, R. Neuroevolution. Schorpedia 8(6), 30977 (2013).</p>
<p>Seeing the light: artificial evolution, real vision. I Harvey, P Husbands, D Cliff, From Anim. Anim. 3Harvey, I., Husbands, P. &amp; Cliff, D. Seeing the light: artificial evolution, real vision. From Anim. Anim. 3, 392-401 (1994).</p>
<p>Incremental evolution of complex general behavior. F Gomez, R Miikkulainen, Adapt. Behav. 53-4Gomez, F. &amp; Miikkulainen, R. Incremental evolution of complex general behavior. Adapt. Behav. 5(3-4), 317-342 (1997).</p>
<p>Incremental evolution of animats' behaviors as a multi-objective optimization. J B Mouret, S Doncieux, International Conference on Simulation of Adaptive Behavior. Berlin, HeidelbergSpringerMouret, J. B. &amp; Doncieux, S. Incremental evolution of animats' behaviors as a multi-objective optimization. In International Con- ference on Simulation of Adaptive Behavior. 210-219. (Springer, Berlin, Heidelberg, 2008).</p>
<p>Curriculum learning for reinforcement learning domains: a framework and survey. S Narvekar, J. Mach. Learn. Res. 21181Narvekar, S. et al. Curriculum learning for reinforcement learning domains: a framework and survey. J. Mach. Learn. Res. 21(181), 1-50 (2020).</p>
<p>New methods for competitive coevolution. C D Rosin, R K Belew, Evol. Comput. 5Rosin, C. D. &amp; Belew, R. K. New methods for competitive coevolution. Evol. Comput. 5, 1-29 (1997).</p>
<p>Co-evolving predator and prey robots: Do 'arms-races' arise in artificial evolution?. S Nolfi, D Floreano, Artif. Life. 4Nolfi, S. &amp; Floreano, D. Co-evolving predator and prey robots: Do 'arms-races' arise in artificial evolution?. Artif. Life 4, 1-26 (1998).</p>
<p>The MaxSolve algorithm for coevolution. De Jong, E , Genetic and Evolutionary Computation (GECCO 2005). De Jong, E. The MaxSolve algorithm for coevolution. In: Genetic and Evolutionary Computation (GECCO 2005), Lecture Notes in Computer Science. 483-489 (2005).</p>
<p>Relationship between generalization and diversity in coevolutionary learning. S Y Chong, P Tiňo, X Yao, IEEE Trans. Comput. Intell. AI Games. 1Chong, S. Y., Tiňo, P. &amp; Yao, X. Relationship between generalization and diversity in coevolutionary learning. IEEE Trans. Comput. Intell. AI Games 1, 214-232 (2009).</p>
<p>Why coevolution doesn't "work": superiority and progress in coevolution. T Miconi, Proceedings of the 12th European Conference on Genetic Programming. the 12th European Conference on Genetic ProgrammingBerlinSpringerMiconi, T. Why coevolution doesn't "work": superiority and progress in coevolution. In: Proceedings of the 12th European Confer- ence on Genetic Programming, Lecture Notes in Computer Science. 49-60 (Springer, Berlin, 2009).</p>
<p>Coevolving game-playing agents: Measuring performance and intransitivities. S Samothrakis, S Lucas, T P Runarsson, D Robles, IEEE Trans. Evol. Comput. 17Samothrakis, S., Lucas, S., Runarsson, T. P. &amp; Robles, D. Coevolving game-playing agents: Measuring performance and intransitivi- ties. IEEE Trans. Evol. Comput. 17, 213-226 (2013).</p>
<p>Long-term progress and behavior complexification in competitive co-evolution. L Simione, S Nolfi, Artificial Life. preprint https:// arxiv. org/ abs/ 1909. 08303 (in pressSimione, L. &amp; Nolfi, S. Long-term progress and behavior complexification in competitive co-evolution. Artificial Life. preprint https:// arxiv. org/ abs/ 1909. 08303 (in press).</p>
<p>Curriculum learning. Y Bengio, J Lauradour, R Collobert, J Weston, ICML'09. 41-48.Proceedings of the 26th Annual International Conference on Machine Learning. the 26th Annual International Conference on Machine LearningNew YorkACM PressBengio, Y., Lauradour, J., Collobert, R. &amp; Weston, J. Curriculum learning. In Proceedings of the 26th Annual International Confer- ence on Machine Learning, ICML'09. 41-48. (ACM Press, New York, 2009).</p>
<p>Learning to execute. I Sutskever, W Zaremba, Sutskever, I. &amp; Zaremba, W. Learning to execute. arXiv preprint https:// arxiv. org/ abs/ 1410. 4615 (2014).</p>
<p>Prioritized experience replay. T Schaul, J Quan, I Antonoglou, D Silver, Schaul, T., Quan, J., Antonoglou, I. &amp; Silver, D. Prioritized experience replay. arXiv preprint https:// arxiv. org/ abs/ 1511. 05952 (2015).</p>
<p>Self-paced prioritized curriculum learning with coverage penalty in deep reinforcement learning. Z Ren, D Dong, H Li, C Chen, IEEE Trans. Neural Netw. Learn. Syst. 296Ren, Z., Dong, D., Li, H. &amp; Chen, C. Self-paced prioritized curriculum learning with coverage penalty in deep reinforcement learning. IEEE Trans. Neural Netw. Learn. Syst. 29(6), 2216-2226 (2018).</p>
<p>Incorporating structural alignment biases into an attentional neural translation model. T Cohn, C D V Hoang, E Vymolova, K Yao, C Dyer, G Haffari, Cohn, T., Hoang, C. D. V., Vymolova, E., Yao, K., Dyer, C. &amp; Haffari, G. Incorporating structural alignment biases into an attentional neural translation model. arXiv preprint https:// arxiv. org/ abs/ 1601. 01085 (2016).</p>
<p>Paired open-ended trailblazer (poet): endlessly generating increasingly complex and diverse learning environments and their solutions. R Wang, J Lehman, J Clune, K O Stanley, arXiv preprintWang, R., Lehman, J., Clune, J. &amp; Stanley, K. O. Paired open-ended trailblazer (poet): endlessly generating increasingly complex and diverse learning environments and their solutions. arXiv preprint https:// arxiv. org/ abs/ 1901. 01753 (2019).</p>
<p>Maximizing adaptive power in neuroevolution. P Pagliuca, N Milano, S Nolfi, PLoS ONE. 137198788Pagliuca, P., Milano, N. &amp; Nolfi, S. Maximizing adaptive power in neuroevolution. PLoS ONE 13(7), e0198788 (2018).</p>
<p>LSTM recurrent networks learn simple context free and context sensitive languages. F A Gers, J Schmidhuber, IEEE Trans. Neural Netw. 126Gers, F. A. &amp; Schmidhuber, J. LSTM recurrent networks learn simple context free and context sensitive languages. IEEE Trans. Neural Netw. 12(6), 1333-1340 (2001).</p>
<p>Efficacy of modern neuro-evolutionary strategies for continuous control optimization. P Pagliuca, N Milano, S Nolfi, Front. Robot. AI. 798Pagliuca, P., Milano, N. &amp; Nolfi, S. Efficacy of modern neuro-evolutionary strategies for continuous control optimization. Front. Robot. AI 7, 98 (2020).</p>
<p>. G Brockman, V Cheung, L Pettersson, J Schneider, J Schulman, J Tang, W Openai Zaremba, Gym, Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J. &amp; Zaremba, W. OpenAI Gym. https:// arxiv. org/ abs/ 1606. 01540 (2016).</p>
<p>Evolution strategies as a scalable alternative to reinforcement learning. T Salimans, J Ho, X Chen, S Sidor, I Sutskever, Salimans, T., Ho, J., Chen, X., Sidor, S. &amp; Sutskever, I. Evolution strategies as a scalable alternative to reinforcement learning. https:// arxiv. org/ abs/ 1703. 03864 v2 (2017).</p>
<p>Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments. R Portelas, C Colas, K Hofmann, P Y Oudeyer, Conference on Robot Learning. PMLRPortelas, R., Colas, C., Hofmann, K. &amp; Oudeyer, P. Y. Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments. In Conference on Robot Learning. 835-853 (PMLR, 2020).</p>
<p>A method for stochastic optimization. D P Kingma, J Ba, Adam, Kingma, D. P. &amp; Ba, J. Adam: A method for stochastic optimization. arXiv preprint https:// arxiv. org/ abs/ 1412. 6980 (2014)..</p>
<p>Mirrored sampling and sequential selection for evolution strategies. D Brockhoff, A Auger, N Hansen, D V Arnold, T Hohm, International Conference on Parallel Problem Solving from Nature. BerlinSpringerBrockhoff, D., Auger, A., Hansen, N., Arnold, D. V. &amp; Hohm, T. Mirrored sampling and sequential selection for evolution strate- gies. In International Conference on Parallel Problem Solving from Nature (Springer, Berlin, 2010).</p>
<p>Learning symmetric and low-energy locomotion. W Yu, G Turk, C K Liu, ACM Trans. Graph. (TOG). 374Yu, W., Turk, G. &amp; Liu, C. K. Learning symmetric and low-energy locomotion. ACM Trans. Graph. (TOG) 37(4), 1-12 (2018).</p>
<p>Automatic goal generation for reinforcement learning agents. D Held, X Geng, C Florensa, P Abbeel, Held, D., Geng, X., Florensa, C. &amp; Abbeel, P. Automatic goal generation for reinforcement learning agents. arXiv preprint https:// arxiv. org/ abs/ 1705. 06366 (2017).</p>
<p>Hybrid computing using a neural network with dynamic external memory. A Graves, Nature. 5387626Graves, A. et al. Hybrid computing using a neural network with dynamic external memory. Nature 538(7626), 471-476 (2016).</p>
<p>Automated curriculum learning for neural networks. A Graves, M G Bellemare, J Menick, R Munos, K Kavukcuoglu, arXiv preprintGraves, A., Bellemare, M. G., Menick, J., Munos, R., &amp; Kavukcuoglu, K. Automated curriculum learning for neural networks. arXiv preprint https:// arxiv. org/ abs/ 1704. 03003 (2017).</p>
<p>Teacher-student curriculum learning. T Matiisen, A Oliver, T Cohen, J Schulman, IEEE Trans. Neural Netw. Learn. Syst. 319Matiisen, T., Oliver, A., Cohen, T. &amp; Schulman, J. Teacher-student curriculum learning. IEEE Trans. Neural Netw. Learn. Syst. 31(9), 3732-3740 (2019).</p>            </div>
        </div>

    </div>
</body>
</html>
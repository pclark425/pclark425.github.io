<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-718 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-718</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-718</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-237420666</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2109.02429v2.pdf" target="_blank">Learning Neural Causal Models with Active Interventions</a></p>
                <p><strong>Paper Abstract:</strong> Discovering causal structures from data is a challenging inference problem of fundamental importance in all areas of science. The appealing properties of neural networks have recently led to a surge of interest in differentiable neural network-based methods for learning causal structures from data. So far, differentiable causal discovery has focused on static datasets of observational or ﬁxed interventional origin. In this work, we introduce an active intervention targeting (AIT) method which enables a quick identiﬁcation of the underlying causal structure of the data-generating process. Our method signiﬁcantly reduces the required number of interactions compared with random intervention targeting and is applicable for both discrete and continuous optimization formulations of learning the underlying directed acyclic graph (DAG) from data. We examine the proposed method across multiple frameworks in a wide range of settings and demonstrate superior performance on multiple benchmarks from simulated to real-world data.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e718.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e718.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AIT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active Intervention Targeting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active intervention design technique that selects informative single- or multi-variable interventions for neural, differentiable causal discovery by scoring candidate targets according to how much post-interventional sample distributions disagree across sampled hypothesis graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Active Intervention Targeting (AIT)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>AIT samples a set of hypothesis DAGs from the learned structural belief (soft-adjacency), applies a candidate intervention to the functional model under each hypothesis, generates post-interventional samples via ancestral sampling, and computes a discrepancy score per candidate that measures between-graph variance (VBG) versus within-graph variance (VWG). Targets with high VBG/VWG are chosen as they produce post-interventional distributions that differ across plausible graphs and thus are maximally informative for discriminating graph structures. The method masks intervened variables (sets them to zero) when computing variances to avoid their direct contribution, and it can select single or multi-node interventions. AIT plugs into differentiable neural causal discovery frameworks (e.g., SDI, DCDI) and uses a two-stage DAG sampler to generate hypothesis graphs efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Neural causal discovery frameworks on synthetic and real SCM datasets (SDI / DCDI; synthetic structured graphs, Erdős–Rényi random graphs, Sachs, Asia)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Batch-intervention, interactive experimental setting where an agent sequentially chooses interventions and receives interventional samples; environments are simulated SCMs (discrete and continuous) and real-world flow cytometry / Asia networks. The setup allows active experimentation (selection of intervention targets) but is not an open-ended RL environment — interventions are planned in episodes and experiments use fixed sample budgets per intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Detects and avoids uninformative or potentially distracting interventions by (1) sampling hypothesis graphs and comparing post-interventional distributions to detect where graph hypotheses disagree (thus identifying informative variables), and (2) masking intervened variables and preferring targets with low within-graph variance to rule out targets whose high apparent disagreement is due to sampling noise; indirectly reduces interventions on sink/no-child variables that act as distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Measurement noise (random flips), uninformative/irrelevant variables (variables without children acting as distracting intervention targets), sampling variance leading to spurious apparent differences across graphs; implicitly addresses spurious edge assignments via targeted interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Samples hypothetical DAGs from the soft-adjacency, applies candidate interventions to functional parameters, generates post-interventional samples per hypothesis, computes per-target between-graph variance (VBG) and within-graph variance (VWG) of the non-intervened variables, and flags targets with high VBG (and low VWG) as informative — i.e., where spurious agreement/disagreement is unlikely and hypotheses can be discriminated.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>No explicit per-variable statistical downweighting is applied; instead, AIT reduces influence of spurious/uninformative signals by not selecting targets whose discrepancies are due to high within-graph variance (VWG). Masking intervened variables removes their direct variance contribution when scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Uses targeted interventions to refute candidate graphs: if post-interventional distributions differ across hypotheses for a chosen target, collecting real interventional data allows the learner to update structural belief and rule out graphs inconsistent with observed post-interventional outcomes. Recovery from incorrectly converged edges is explicitly improved by repeatedly selecting informative interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>F-test-inspired discrepancy maximization: compute D_k = VBG_k / VWG_k for each candidate target I_k using sampled hypothesis graphs and simulated post-interventional samples; choose target(s) with maximal D_k to maximize information gain about structure (prefer targets with high between-graph disagreement but low within-graph sample variance).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Across benchmarks AIT + base differentiable methods substantially lowers Structural Hamming Distance (SHD) vs random targeting, often achieving near-perfect identifiability on structured 15-node graphs (except the densest 'full' graph). AIT reduces sample complexity and converges faster; in noise-perturbed binary experiments AIT still converges for noise levels up to η=0.05 (flip prob. 5%) while maintaining low SHD.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Random intervention targeting (baseline) yields higher SHD, slower convergence, and fails to converge to ground truth for noise levels > η≈0.02 in tested ER-4 graphs; random targeting also performs poorly on larger/dense graphs and is prone to locking into unfavorable belief states that are hard to recover from.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AIT reliably identifies informative interventions by exploiting disagreement across sampled hypothesis graphs (VBG/VWG). This targeted strategy lowers SHD, reduces required interventional samples, speeds convergence (notably after an 'elbow' point corresponding to isolating the Markov equivalence class), reduces interventions on uninformative sink variables (thus avoiding distractors), and markedly improves robustness under measurement-noise perturbations compared with random targeting. AIT also helps recover edges that erroneously converged under random policies and scales better with graph size/density.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Neural Causal Models with Active Interventions', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e718.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e718.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Discrepancy Score (VBG/VWG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>F-test-inspired Variance-Ratio Discrepancy Score (Between-Graph / Within-Graph Variance)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A per-intervention-target scoring rule that computes the ratio of variance of post-interventional sample means across sampled hypothesis graphs (VBG) to the average within-graph sample variance (VWG) to prioritize interventions that best discriminate among graph hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Variance-ratio discrepancy score (D = VBG / VWG)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>For each candidate intervention target I_k, AIT: (1) samples a set of hypothesis DAGs from the structural belief; (2) applies the intervention to the functional parameters for each hypothesis; (3) generates S_i,k post-interventional samples for each hypothesis graph G_i; (4) masks intervened variables and computes per-graph sample means μ_i,k and the overall mean μ_k; (5) computes between-graph variance VBG_k = Σ_i ||μ_i,k - μ_k||^2 and within-graph variance VWG_k = Σ_i Σ_j ||S_i,k^j - μ_i,k||^2; (6) sets D_k = VBG_k / VWG_k and selects targets with maximal D_k. The score aims to prefer targets where hypothesis graphs predict different post-interventional behavior while discounting targets whose apparent disagreement is due to high within-graph variance.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same experimental settings as AIT (SDI/DCDI on synthetic SCMs and real datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Used inside an active experimental design loop in simulated SCMs or datasets where hypothetical post-interventional samples can be generated from functional parameters, enabling evaluation of candidate interventions before performing real experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Distinguishes informative disagreement from noise-induced disagreement via VWG: targets with high between-graph variance but also high within-graph variance (likely due to noisy/uninformative interventional distributions) are deprioritized, so distractor-style signals arising from sampling noise are downweighted.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Sampling variance that can masquerade as informative disagreement; measurement noise indirectly via increased VWG.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Directly detects disagreement across hypothesis graphs (high VBG) while checking VWG to avoid picking targets where disagreement is explained by high sample variance, thereby avoiding spurious signals.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Downweights candidate interventions whose D_k is low because VWG is large relative to VBG; this is an implicit downweighting of targets where apparent disagreement is due to noise.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>By selecting high-D targets and carrying out real interventions, hypotheses whose predicted post-interventional distribution contradicts observed data are refuted and their structural belief mass is reduced in subsequent updates.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Score-evaluate-sample: compute D_k for all targets using simulated post-interventional samples and choose top-scoring target(s).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Enables AIT to avoid uninformative/noisy targets and to prioritize interventions that reduce SHD quickly; empirically leads to faster convergence and robustness to noise as reported for AIT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>If only VBG is considered (ignoring VWG), the method may select targets whose apparent disagreement is caused by high sampling variance, leading to less informative interventions and worse downstream structure identification (observed empirically via baselines that do not use this discriminant).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The VBG/VWG ratio is effective at distinguishing genuinely informative interventions from those where apparent disagreement is driven by noise; using the ratio yields improved selection of intervention targets, better sample-efficiency, and improved robustness to measurement noise compared with naive between-graph disagreement alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Neural Causal Models with Active Interventions', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e718.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e718.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Two-Stage DAG Sampler</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Two-Stage DAG Sampling Procedure (topological-order sampling + Bernoulli edges)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scalable sampler that generates acyclic hypothesis DAGs from a soft-adjacency matrix by (1) iteratively sampling a topological node ordering using a refined root-score (temperature-scaled softmax over root probabilities) and (2) sampling edges by Bernoulli draws on the upper-triangular permuted soft-adjacency to ensure DAGness by construction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Two-Stage DAG Sampling (node-order sampling + Bernoulli edge draws)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Phase 1: From soft-adjacency A = σ(γ), iteratively compute p_child as per-row maxima and set p_root = 1 - p_child, temperature-scale via softmax and sample a root node; remove it and repeat to form a topological ordering. Phase 2: Permute A according to sampled ordering, zero out lower-triangular entries (or enforce upper triangular), and draw independent Bernoulli samples for remaining edges to obtain a DAG adjacency matrix; invert permutation to original node ordering. This produces diverse DAG samples consistent with the soft-adjacency while guaranteeing acyclicity without rejection sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Used inside AIT + differentiable causal discovery frameworks (SDI, DCDI) to generate hypothesis graph ensembles</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Deterministic sampling routine integrated into the active targeting pipeline to efficiently produce many plausible DAG hypotheses from a continuous soft-adjacency belief for downstream simulated interventions and scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Enables scalable generation of many plausible DAGs for AIT's hypothesis-testing; improves computational tractability and supports the discrepancy scoring pipeline, which indirectly contributes to robustness of intervention selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Without an efficient DAG sampler one would need rejection/Gibbs/MCMC sampling over acyclic graphs which is computationally expensive and would limit the number of hypotheses used for scoring, reducing discrimination power and potentially harming robustness of selected interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The two-stage sampler provides an efficient, scalable way to draw diverse acyclic graph hypotheses from a soft-adjacency belief, supporting AIT's need for many graph configurations; it avoids costly rejection or Gibbs sampling and allows control of sampling entropy via a temperature parameter.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Neural Causal Models with Active Interventions', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e718.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e718.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SDI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structure Discovery from Interventions (SDI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A differentiable framework for causal discovery from discrete/fused data that alternates between functional fitting (training CPD models under sampled structural masks) and structural fitting (evaluating hypothesized graphs on interventional data to update structural beliefs).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning neural causal models from unknown interventions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>SDI (two-stage alternating structural/functional fitting)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>SDI parametrizes structural belief as a soft-adjacency matrix and functional conditionals as neural networks (one per variable). It alternates between (a) functional fitting: sample graphs from the structural belief and train conditional models masked to each sampled parent set (dropout-like) so CPDs adapt to stochastic parent sets; and (b) structural fitting: freeze functional parameters and score hypothesized graphs against interventional data to update the structural parameters. SDI originally processes interventions in random order; in this paper AIT is integrated to replace random targeting.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Discrete-valued SCM datasets (synthetic structured graphs, ER random graphs, Sachs, Asia)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Batch/fused data setting with observational and interventional samples provided to the learner; SDI itself is not an active method but can be used with AIT to select interventions actively.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>When augmented with AIT, SDI achieves lower SHD, faster convergence, and improved robustness to noise compared to SDI with random intervention targeting; near-perfect identifiability on many structured 15-node graphs was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Vanilla SDI with random targeting shows slower convergence, higher SHD, and fails at higher noise levels where AIT succeeds; processes interventions in random and independent manner which scales poorly with larger graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SDI provides the structural/functional parameterization that AIT leverages; SDI alone (with random targeting) is sensitive to intervention ordering and sampling noise, whereas integrating AIT alleviates these issues by directing informative experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Neural Causal Models with Active Interventions', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e718.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e718.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DCDI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differentiable Causal Discovery from Interventional Data (DCDI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A continuous constrained optimization approach for causal discovery on continuous data that jointly optimizes adjacency parameters and neural conditional models over fused (observational + interventional) data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Differentiable Causal Discovery from Interventional Data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>DCDI (joint optimization over fused continuous data)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>DCDI models conditional distributions (Gaussians or normalizing flows) via neural nets and optimizes structural adjacency and functional parameters jointly under acyclicity constraints. The original DCDI uses random interventions; in this paper AIT is integrated by restricting/estimating a target set per episode and performing L gradient steps per episode to allow active selection of interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Continuous-valued SCM datasets (non-linear continuous random graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Fused-data joint-optimization setting for continuous variables. The authors adapt DCDI to an episodic active-intervention regime: AIT selects a subset of targets K per episode, DCDI runs L gradient steps on that restricted target set, and the target set is re-evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>DCDI augmented with AIT (DCDI+AIT) outperforms or matches vanilla DCDI in SHD with improved sample complexity across evaluated random non-linear graphs (N=10); AIT reduces interventions on sink nodes and shows similar topological target preferences as in discrete setting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Vanilla DCDI (random targeting) is less sample-efficient and slower to identify structure than DCDI+AIT; using full target space without active selection yields slower convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Integrating AIT into DCDI (by choosing a target-space K per episode) improves sample-efficiency and convergence while preserving or improving final SHD; AIT's targeting preferences (favoring high-outdegree/downstream nodes) are consistent in the continuous setting and reduce wasteful interventions on uninformative variables.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Neural Causal Models with Active Interventions', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning neural causal models from unknown interventions <em>(Rating: 2)</em></li>
                <li>Differentiable Causal Discovery from Interventional Data <em>(Rating: 2)</em></li>
                <li>Active learning of causal Bayes net structure <em>(Rating: 1)</em></li>
                <li>Two optimal strategies for active learning of causal models from interventional data <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-718",
    "paper_id": "paper-237420666",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "AIT",
            "name_full": "Active Intervention Targeting",
            "brief_description": "An active intervention design technique that selects informative single- or multi-variable interventions for neural, differentiable causal discovery by scoring candidate targets according to how much post-interventional sample distributions disagree across sampled hypothesis graphs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Active Intervention Targeting (AIT)",
            "method_description": "AIT samples a set of hypothesis DAGs from the learned structural belief (soft-adjacency), applies a candidate intervention to the functional model under each hypothesis, generates post-interventional samples via ancestral sampling, and computes a discrepancy score per candidate that measures between-graph variance (VBG) versus within-graph variance (VWG). Targets with high VBG/VWG are chosen as they produce post-interventional distributions that differ across plausible graphs and thus are maximally informative for discriminating graph structures. The method masks intervened variables (sets them to zero) when computing variances to avoid their direct contribution, and it can select single or multi-node interventions. AIT plugs into differentiable neural causal discovery frameworks (e.g., SDI, DCDI) and uses a two-stage DAG sampler to generate hypothesis graphs efficiently.",
            "environment_name": "Neural causal discovery frameworks on synthetic and real SCM datasets (SDI / DCDI; synthetic structured graphs, Erdős–Rényi random graphs, Sachs, Asia)",
            "environment_description": "Batch-intervention, interactive experimental setting where an agent sequentially chooses interventions and receives interventional samples; environments are simulated SCMs (discrete and continuous) and real-world flow cytometry / Asia networks. The setup allows active experimentation (selection of intervention targets) but is not an open-ended RL environment — interventions are planned in episodes and experiments use fixed sample budgets per intervention.",
            "handles_distractors": true,
            "distractor_handling_technique": "Detects and avoids uninformative or potentially distracting interventions by (1) sampling hypothesis graphs and comparing post-interventional distributions to detect where graph hypotheses disagree (thus identifying informative variables), and (2) masking intervened variables and preferring targets with low within-graph variance to rule out targets whose high apparent disagreement is due to sampling noise; indirectly reduces interventions on sink/no-child variables that act as distractors.",
            "spurious_signal_types": "Measurement noise (random flips), uninformative/irrelevant variables (variables without children acting as distracting intervention targets), sampling variance leading to spurious apparent differences across graphs; implicitly addresses spurious edge assignments via targeted interventions.",
            "detection_method": "Samples hypothetical DAGs from the soft-adjacency, applies candidate interventions to functional parameters, generates post-interventional samples per hypothesis, computes per-target between-graph variance (VBG) and within-graph variance (VWG) of the non-intervened variables, and flags targets with high VBG (and low VWG) as informative — i.e., where spurious agreement/disagreement is unlikely and hypotheses can be discriminated.",
            "downweighting_method": "No explicit per-variable statistical downweighting is applied; instead, AIT reduces influence of spurious/uninformative signals by not selecting targets whose discrepancies are due to high within-graph variance (VWG). Masking intervened variables removes their direct variance contribution when scoring.",
            "refutation_method": "Uses targeted interventions to refute candidate graphs: if post-interventional distributions differ across hypotheses for a chosen target, collecting real interventional data allows the learner to update structural belief and rule out graphs inconsistent with observed post-interventional outcomes. Recovery from incorrectly converged edges is explicitly improved by repeatedly selecting informative interventions.",
            "uses_active_learning": true,
            "inquiry_strategy": "F-test-inspired discrepancy maximization: compute D_k = VBG_k / VWG_k for each candidate target I_k using sampled hypothesis graphs and simulated post-interventional samples; choose target(s) with maximal D_k to maximize information gain about structure (prefer targets with high between-graph disagreement but low within-graph sample variance).",
            "performance_with_robustness": "Across benchmarks AIT + base differentiable methods substantially lowers Structural Hamming Distance (SHD) vs random targeting, often achieving near-perfect identifiability on structured 15-node graphs (except the densest 'full' graph). AIT reduces sample complexity and converges faster; in noise-perturbed binary experiments AIT still converges for noise levels up to η=0.05 (flip prob. 5%) while maintaining low SHD.",
            "performance_without_robustness": "Random intervention targeting (baseline) yields higher SHD, slower convergence, and fails to converge to ground truth for noise levels &gt; η≈0.02 in tested ER-4 graphs; random targeting also performs poorly on larger/dense graphs and is prone to locking into unfavorable belief states that are hard to recover from.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "AIT reliably identifies informative interventions by exploiting disagreement across sampled hypothesis graphs (VBG/VWG). This targeted strategy lowers SHD, reduces required interventional samples, speeds convergence (notably after an 'elbow' point corresponding to isolating the Markov equivalence class), reduces interventions on uninformative sink variables (thus avoiding distractors), and markedly improves robustness under measurement-noise perturbations compared with random targeting. AIT also helps recover edges that erroneously converged under random policies and scales better with graph size/density.",
            "uuid": "e718.0",
            "source_info": {
                "paper_title": "Learning Neural Causal Models with Active Interventions",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Discrepancy Score (VBG/VWG)",
            "name_full": "F-test-inspired Variance-Ratio Discrepancy Score (Between-Graph / Within-Graph Variance)",
            "brief_description": "A per-intervention-target scoring rule that computes the ratio of variance of post-interventional sample means across sampled hypothesis graphs (VBG) to the average within-graph sample variance (VWG) to prioritize interventions that best discriminate among graph hypotheses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Variance-ratio discrepancy score (D = VBG / VWG)",
            "method_description": "For each candidate intervention target I_k, AIT: (1) samples a set of hypothesis DAGs from the structural belief; (2) applies the intervention to the functional parameters for each hypothesis; (3) generates S_i,k post-interventional samples for each hypothesis graph G_i; (4) masks intervened variables and computes per-graph sample means μ_i,k and the overall mean μ_k; (5) computes between-graph variance VBG_k = Σ_i ||μ_i,k - μ_k||^2 and within-graph variance VWG_k = Σ_i Σ_j ||S_i,k^j - μ_i,k||^2; (6) sets D_k = VBG_k / VWG_k and selects targets with maximal D_k. The score aims to prefer targets where hypothesis graphs predict different post-interventional behavior while discounting targets whose apparent disagreement is due to high within-graph variance.",
            "environment_name": "Same experimental settings as AIT (SDI/DCDI on synthetic SCMs and real datasets)",
            "environment_description": "Used inside an active experimental design loop in simulated SCMs or datasets where hypothetical post-interventional samples can be generated from functional parameters, enabling evaluation of candidate interventions before performing real experiments.",
            "handles_distractors": true,
            "distractor_handling_technique": "Distinguishes informative disagreement from noise-induced disagreement via VWG: targets with high between-graph variance but also high within-graph variance (likely due to noisy/uninformative interventional distributions) are deprioritized, so distractor-style signals arising from sampling noise are downweighted.",
            "spurious_signal_types": "Sampling variance that can masquerade as informative disagreement; measurement noise indirectly via increased VWG.",
            "detection_method": "Directly detects disagreement across hypothesis graphs (high VBG) while checking VWG to avoid picking targets where disagreement is explained by high sample variance, thereby avoiding spurious signals.",
            "downweighting_method": "Downweights candidate interventions whose D_k is low because VWG is large relative to VBG; this is an implicit downweighting of targets where apparent disagreement is due to noise.",
            "refutation_method": "By selecting high-D targets and carrying out real interventions, hypotheses whose predicted post-interventional distribution contradicts observed data are refuted and their structural belief mass is reduced in subsequent updates.",
            "uses_active_learning": true,
            "inquiry_strategy": "Score-evaluate-sample: compute D_k for all targets using simulated post-interventional samples and choose top-scoring target(s).",
            "performance_with_robustness": "Enables AIT to avoid uninformative/noisy targets and to prioritize interventions that reduce SHD quickly; empirically leads to faster convergence and robustness to noise as reported for AIT.",
            "performance_without_robustness": "If only VBG is considered (ignoring VWG), the method may select targets whose apparent disagreement is caused by high sampling variance, leading to less informative interventions and worse downstream structure identification (observed empirically via baselines that do not use this discriminant).",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "The VBG/VWG ratio is effective at distinguishing genuinely informative interventions from those where apparent disagreement is driven by noise; using the ratio yields improved selection of intervention targets, better sample-efficiency, and improved robustness to measurement noise compared with naive between-graph disagreement alone.",
            "uuid": "e718.1",
            "source_info": {
                "paper_title": "Learning Neural Causal Models with Active Interventions",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Two-Stage DAG Sampler",
            "name_full": "Two-Stage DAG Sampling Procedure (topological-order sampling + Bernoulli edges)",
            "brief_description": "A scalable sampler that generates acyclic hypothesis DAGs from a soft-adjacency matrix by (1) iteratively sampling a topological node ordering using a refined root-score (temperature-scaled softmax over root probabilities) and (2) sampling edges by Bernoulli draws on the upper-triangular permuted soft-adjacency to ensure DAGness by construction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Two-Stage DAG Sampling (node-order sampling + Bernoulli edge draws)",
            "method_description": "Phase 1: From soft-adjacency A = σ(γ), iteratively compute p_child as per-row maxima and set p_root = 1 - p_child, temperature-scale via softmax and sample a root node; remove it and repeat to form a topological ordering. Phase 2: Permute A according to sampled ordering, zero out lower-triangular entries (or enforce upper triangular), and draw independent Bernoulli samples for remaining edges to obtain a DAG adjacency matrix; invert permutation to original node ordering. This produces diverse DAG samples consistent with the soft-adjacency while guaranteeing acyclicity without rejection sampling.",
            "environment_name": "Used inside AIT + differentiable causal discovery frameworks (SDI, DCDI) to generate hypothesis graph ensembles",
            "environment_description": "Deterministic sampling routine integrated into the active targeting pipeline to efficiently produce many plausible DAG hypotheses from a continuous soft-adjacency belief for downstream simulated interventions and scoring.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": "Enables scalable generation of many plausible DAGs for AIT's hypothesis-testing; improves computational tractability and supports the discrepancy scoring pipeline, which indirectly contributes to robustness of intervention selection.",
            "performance_without_robustness": "Without an efficient DAG sampler one would need rejection/Gibbs/MCMC sampling over acyclic graphs which is computationally expensive and would limit the number of hypotheses used for scoring, reducing discrimination power and potentially harming robustness of selected interventions.",
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "The two-stage sampler provides an efficient, scalable way to draw diverse acyclic graph hypotheses from a soft-adjacency belief, supporting AIT's need for many graph configurations; it avoids costly rejection or Gibbs sampling and allows control of sampling entropy via a temperature parameter.",
            "uuid": "e718.2",
            "source_info": {
                "paper_title": "Learning Neural Causal Models with Active Interventions",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "SDI",
            "name_full": "Structure Discovery from Interventions (SDI)",
            "brief_description": "A differentiable framework for causal discovery from discrete/fused data that alternates between functional fitting (training CPD models under sampled structural masks) and structural fitting (evaluating hypothesized graphs on interventional data to update structural beliefs).",
            "citation_title": "Learning neural causal models from unknown interventions",
            "mention_or_use": "use",
            "method_name": "SDI (two-stage alternating structural/functional fitting)",
            "method_description": "SDI parametrizes structural belief as a soft-adjacency matrix and functional conditionals as neural networks (one per variable). It alternates between (a) functional fitting: sample graphs from the structural belief and train conditional models masked to each sampled parent set (dropout-like) so CPDs adapt to stochastic parent sets; and (b) structural fitting: freeze functional parameters and score hypothesized graphs against interventional data to update the structural parameters. SDI originally processes interventions in random order; in this paper AIT is integrated to replace random targeting.",
            "environment_name": "Discrete-valued SCM datasets (synthetic structured graphs, ER random graphs, Sachs, Asia)",
            "environment_description": "Batch/fused data setting with observational and interventional samples provided to the learner; SDI itself is not an active method but can be used with AIT to select interventions actively.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "When augmented with AIT, SDI achieves lower SHD, faster convergence, and improved robustness to noise compared to SDI with random intervention targeting; near-perfect identifiability on many structured 15-node graphs was reported.",
            "performance_without_robustness": "Vanilla SDI with random targeting shows slower convergence, higher SHD, and fails at higher noise levels where AIT succeeds; processes interventions in random and independent manner which scales poorly with larger graphs.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "SDI provides the structural/functional parameterization that AIT leverages; SDI alone (with random targeting) is sensitive to intervention ordering and sampling noise, whereas integrating AIT alleviates these issues by directing informative experiments.",
            "uuid": "e718.3",
            "source_info": {
                "paper_title": "Learning Neural Causal Models with Active Interventions",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "DCDI",
            "name_full": "Differentiable Causal Discovery from Interventional Data (DCDI)",
            "brief_description": "A continuous constrained optimization approach for causal discovery on continuous data that jointly optimizes adjacency parameters and neural conditional models over fused (observational + interventional) data.",
            "citation_title": "Differentiable Causal Discovery from Interventional Data",
            "mention_or_use": "use",
            "method_name": "DCDI (joint optimization over fused continuous data)",
            "method_description": "DCDI models conditional distributions (Gaussians or normalizing flows) via neural nets and optimizes structural adjacency and functional parameters jointly under acyclicity constraints. The original DCDI uses random interventions; in this paper AIT is integrated by restricting/estimating a target set per episode and performing L gradient steps per episode to allow active selection of interventions.",
            "environment_name": "Continuous-valued SCM datasets (non-linear continuous random graphs)",
            "environment_description": "Fused-data joint-optimization setting for continuous variables. The authors adapt DCDI to an episodic active-intervention regime: AIT selects a subset of targets K per episode, DCDI runs L gradient steps on that restricted target set, and the target set is re-evaluated.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": "DCDI augmented with AIT (DCDI+AIT) outperforms or matches vanilla DCDI in SHD with improved sample complexity across evaluated random non-linear graphs (N=10); AIT reduces interventions on sink nodes and shows similar topological target preferences as in discrete setting.",
            "performance_without_robustness": "Vanilla DCDI (random targeting) is less sample-efficient and slower to identify structure than DCDI+AIT; using full target space without active selection yields slower convergence.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Integrating AIT into DCDI (by choosing a target-space K per episode) improves sample-efficiency and convergence while preserving or improving final SHD; AIT's targeting preferences (favoring high-outdegree/downstream nodes) are consistent in the continuous setting and reduce wasteful interventions on uninformative variables.",
            "uuid": "e718.4",
            "source_info": {
                "paper_title": "Learning Neural Causal Models with Active Interventions",
                "publication_date_yy_mm": "2021-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning neural causal models from unknown interventions",
            "rating": 2,
            "sanitized_title": "learning_neural_causal_models_from_unknown_interventions"
        },
        {
            "paper_title": "Differentiable Causal Discovery from Interventional Data",
            "rating": 2,
            "sanitized_title": "differentiable_causal_discovery_from_interventional_data"
        },
        {
            "paper_title": "Active learning of causal Bayes net structure",
            "rating": 1,
            "sanitized_title": "active_learning_of_causal_bayes_net_structure"
        },
        {
            "paper_title": "Two optimal strategies for active learning of causal models from interventional data",
            "rating": 1,
            "sanitized_title": "two_optimal_strategies_for_active_learning_of_causal_models_from_interventional_data"
        }
    ],
    "cost": 0.016158,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Learning Neural Causal Models with Active Interventions</p>
<p>Nino Scherrer 
Olexa Bilaniuk 
Yashas Annadani 
Anirudh Goyal 
Patrick Schwab 
Bernhard Schölkopf 
Michael C Mozer 
Yoshua Bengio 
Stefan Bauer 
Nan Rosemary Ke 
Learning Neural Causal Models with Active Interventions</p>
<p>Discovering causal structures from data is a challenging inference problem of fundamental importance in all areas of science. The appealing properties of neural networks have recently led to a surge of interest in differentiable neural networkbased methods for learning causal structures from data. So far, differentiable causal discovery has focused on static datasets of observational or fixed interventional origin. In this work, we introduce an active intervention targeting (AIT) method which enables a quick identification of the underlying causal structure of the data-generating process. Our method significantly reduces the required number of interactions compared with random intervention targeting and is applicable for both discrete and continuous optimization formulations of learning the underlying directed acyclic graph (DAG) from data. We examine the proposed method across multiple frameworks in a wide range of settings and demonstrate superior performance on multiple benchmarks from simulated to real-world data.</p>
<p>Introduction</p>
<p>Inferring causal structure from data is a challenging but important task that lies at the heart of scientific reasoning and accompanying progress (Lauritzen &amp; Spiegelhalter, 1988;Friedman et al., 2000;Robins et al., 2000;Sachs et al., 2005;Korb &amp; Nicholson, 2010;Hill et al., 2016;Vandenbroucke et al., 2016;de Castro et al., 2019). Recently, there has been a surge in interest in differentiable causal structure learning with neural networks, also known as neural causal discovery (Ke et al., 2019;Schölkopf et al., 2021;Xia et al., 2021). These methods propose to recast the discrete search over the combinatorial solution space by treating it as an Preprint. Obs. Data Int. Data AIT is an active intervention targeting technique which is applicable to all neural causal discovery frameworks of fused data. Based on the state of a learned neural causal model N up to a given timepoint, AIT selects the next informative intervention target for the causal discovery.</p>
<p>optimization problem with smoothly differentiable parameters. The set of neural parameters embodies a neural causal model N that represents parameters of both structural and functional nature. Structural parameters express the belief about the graph structure through a distribution over graphs, for example with a soft-adjacency matrix. On the other hand, functional parameters characterize the conditional probability distributions of the factorized joint distribution of a directed graphical model. Overall, such models offer promising abilities with respect to generalization and fast adaptation (Bengio et al., 2019).</p>
<p>Existing neural causal discovery methods focus on fixed datasets of either observational (Zheng et al., 2018;Yu et al., 2019;Zheng et al., 2020;Bengio et al., 2019;Lorch et al., 2021;Annadani et al., 2021;Cundy et al., 2021) or fused (observational and interventional) nature (Ke et al., 2019;Brouillard et al., 2020;Lippe et al., 2021). While having access to interventional data can significantly improve the identification of the underlying causal structure, the improvement critically depends on the nature of the experiments and the number of interventional samples available to the learner (Heckerman et al., 1995;Eberhardt et al., 2012). However, interventions tend to be costly and can be technically impossible or even unethical (Peters et al., 2011). Hence it is desirable for an agent to conduct active interventions to recover the underlying causal structure in an adaptive and efficient manner. While a large body of work has addressed this need based on non-differentiable frameworks (He &amp; Geng, 2008;Eberhardt, 2012;Hyttinen et al., 2013;Hauser &amp; Bühlmann, 2014;Shanmugam et al., 2015;Kocaoglu et al., 2017b;a;Lindgren et al., 2018;Ghas- In this work, we propose to augment neural causal discovery methods with the ability to actively intervene. Therefore, we introduce Active Intervention Targeting (AIT), an adaptive intervention design technique for the batch-wise acquisition of interventional samples. AIT can be easily incorporated into any neural causal discovery method which provides access to structural and functional parameters (see Figure 1). In AIT, we decide where to intervene by computing a score for all possible intervention targets (over a single or multiple variables). This score provides us with an estimate how informative an intervention at that target would be with respect to the current evidence. For a set of hypothesis graphs sampled from the structural belief and a fixed intervention target, we apply the intervention on all hypothesis graphs and generate hypothetical samples through an ancestral sampling process based on the functional parameters. This allows us to compare statistics of the post-interventional sample distributions across the hypothesis graphs (see Figure 2). We conjecture (and empirically show) that interventions that do not agree across different hypothesis graphs contain more information about the causal structure and hence enable more efficient learning.</p>
<p>Summary of Empirical Results. We propose an intervention design method (single and multi-target) which identifies the underlying graph efficiently and can be used for any differentiable causal discovery method. We examine the proposed intervention-targeting method across multiple differentiable causal discovery frameworks in a wide range of settings and demonstrate superior performance against established competitive baselines on multiple benchmarks from simulated to real-world data. We provide empirical insights on the distribution of selected intervention targets and its connection to the topological order of the variables in the underlying data-generating distribution.</p>
<p>Preliminaries</p>
<p>Structural Causal Model (SCM). An SCM (Peters et al., 2017) is defined over a set of random variables X 1 , . . . , X M or just X for short, associated with a directed acyclic graph
(DAG) G = (V, E) over variable nodes V = {1, . . . M }.
The random variables are connected by edges in E via functions f i and jointly independent noise variables U i through
X i = f i (X pa(i) , U i ) where X pa(i) are X i 's parents in G,
and directed edges in the graph represent direct causation. The conditionals P (X i |X pa(i) ) define the conditional distribution of X i given its parents. This characterization entails a factorization of the joint observational distribution:
P (X 1 , . . . , X N ) = N i=1 P (X i |X pa(i) )
Interventions. Interventions on X i change the conditional distribution of P (X i |X pa(i) ) to a different distribution, hence affecting the outcome of X i . Interventions can be perfect (hard) or imperfect (soft). Hard interventions entirely remove the dependencies of a variable X i on its parents X pa(i) , hence defining the conditional probability distribution of X i by someP (X i ) rather than P (X i |X pa(i) ). A more general form of intervention is the soft intervention, where the intervention changes the effect of the parents of X i on itself by modifying the conditional distribution from P i (X i |X pa(i) ) to an alternative, denotedP i (X i |X pa(i) ).</p>
<p>Neural Causal Discovery from Fused Data. Neural causal discovery from fused data aims at fitting fused data with a neural causal model N , an SCM with smoothly differentiable parameters of functional and structural nature, using a score-based objective. Structural parameters γ encode our belief in the underlying graph structure G, usually in form of a learned soft-adjacency matrix representing a distribution over graphs. Functional parameters θ encode the conditional probability distributions (CPDs) P (X i |X pa(i ) through neural networks that either learn parameters of a distributional family (e.g. Gaussians or normalizing flows (Rezende &amp; Mohamed, 2015)) or approximate the function itself. This is usually realized by a stack of MLPs, i.e. one MLP per variable, to represent its conditional distribution.</p>
<p>We evaluate our proposed intervention design method under two frameworks that can handle fused data. While we focus on Structure Discovery from Interventions (SDI) (Ke et al., 2019) in the main text, we provide futher context and demonstrate AIT's ability based on Differentiable Causal Discovery from Interventional Data (DCDI) (Brouillard et al., 2020) in §A.7.</p>
<p>Structure Discovery from Interventions (SDI). The SDI approach reformulates the problem of causal discovery from discrete data as a discrete optimization problem using neural networks. The framework proposes to learn the parameters of a neural causal model using a two-stage training procedure with alternating phases of optimization (see Figure 3). Under a fixed structural belief, the functional fitting stage fits the functional parameters θ (representing the observational CPDs) to observational data. In order to account for the stochastic nature of the structural belief, the method samples different hypothesized graphs in this stage and uses them in a dropout-like fashion to mask out all variables except the direct causal parents according to the graph while fitting the functional parameters. This enforces the CPDs to be trained on different sets of parents and will converge to the set of true parents as the structure converges. On the other hand, the structural fitting freezes the functional parameters and evaluates the fit to interventional data of different hypothesized graphs. The adaptation scores are then used to update the belief in the graph structure by propagating them to update the structural parameters. The method performs competitively to many other methods. However, it processes all interventions in a random and independent manner, a strategy that scales poorly to larger graphs.</p>
<p>Active Intervention Targeting (AIT)</p>
<p>We present a score-based, adaptive intervention design strategy, called AIT, which is applicable to any neural causal discovery method which provides access to structural and functional parameters. In addition, we present a scalable two-stage DAG sampling technique for the efficient generation of hypothesis DAGs based on a soft-adjacency matrix, which is a common parametrization of the structural belief. Finally, we show how our proposed method can be easily plugged into recent differentiable causal discovery frameworks for guided exploration using interventional data.</p>
<p>Assumptions. The proposed method does not have to assume causal sufficiency per se. However, it inherits the assumptions of the selected base framework, and this may include causal sufficiency depending on the base algorithm of choice. In case the underlying framework can handle unobserved variables and offers a generative method for interventional samples, then our method is also applicable 3.1. A score for intervention targeting Given a structural belief state γ with its corresponding functional parameters θ, and a possible set of intervention targets I (single and multi-node intervention targets), we wish to select the most informative intervention target(s) I k * ∈ I to identify as quickly as possible the underlying structure. In AIT, we decide where to intervene by computing a score for all possible intervention targets. This score provides us with an estimate how informative an intervention at that target would be with respect to the current evidence. We claim that such informative interventions would yield relatively high discrepancies between post-interventional samples drawn under different hypothesis graphs, making it possible to discriminate better among these candidate graphs and indicating larger uncertainty about the intervention target's relation to its parents and/or children.</p>
<p>We thus construct an F-test-inspired score to seek the target I k * exhibiting the highest discrepancies between postinterventional sample distributions generated by likely graph structures under fixed functional parameters θ. In order to compare sample distributions over different graphs, we distinguish between two sources of variation: variance between graphs (VBG) and variance within graphs (VWG). While VBG characterizes the variance of sample means over multiple graphs, VWG accounts for the sample variance when a specific graph is fixed. We mask the contribution of the intervened variables I k to VBG and VWG, and construct our discrepancy score D as a ratio D = VBG VWG . This discrepancy score attains high values for intervention targets of particular interest. While VBG itself indicates for which intervention targets the model is unsettled about, an extension to the proposed variance ratio enables more control over the region of interest. Given a fixed set of graphs G and a fixed interventional sample size across all graphs, let us assume a scenario where multiple intervention targets attain high VBG. Assessing VWG allows us to distinguish between two extreme cases: (a) targets with sample populations that exhibit large VWG (b) targets with sample populations that exhibit low VWG. While high VBG in (a) might be induced by an insufficient sample size due to high variance in the interventional distribution itself, (b) clearly indicates high discrepancy between graphs and should be preferentially studied.</p>
<p>Computational Details. We begin by sampling a set of graphs G = {G i }, i = 1, 2, 3, . . . from our structural parameters γ. This G will remain fixed for all considered interventions for the current experimental round. Then, we fix an intervention target I k and apply the corresponding intervention to θ, resulting in partially altered functional parameters θ k where some conditionals have been temporarily changed to be overriden by the intervention. Next, we draw interventional samples S i,k from θ k on the post-interventional graphs G i,k (i.e. intervention on target I k applied to graph G i ). In the variance calculation, we set the variables of the intervention targets I k to zero to mask off their contribution to the variance. Having collected all samples over the considered graphs for the specific intervention target I k , we compute VBG k and VWG k as follows:
VBG k = i &lt; µ i,k −μ k , µ i,k −μ k &gt; VWG k = i j &lt; S i,k j − µ i,k , S i,k j − µ i,k &gt;
whereμ k is a vector of the same dimension as any sample in S and denotes the overall sample-mean over all graphs in the interventional setting I k . Further, µ i,k denotes the mean of samples drawn from graph G i,k and S i,k j is the j-th sample of the i-th graph configuration under intervention I k . Finally, we construct the discrepancy score D k of I k as:
D k ← VBG k VWG k .
In contrast to the original definition of the F-Score, we can ignore the normalization constants due to equal group size and degree-of-freedoms. An outline of the method is provided in Algorithm 1.</p>
<p>Two-Phase DAG sampling</p>
<p>Embedding AIT into recent differentiable causal discovery frameworks requires a graph sampler that generates a set of likely graph configurations under the current graph belief state. However, drawing samples from unconstrained graphs</p>
<p>Algorithm 1 Active Intervention Targeting (AIT)</p>
<p>Input: Functional Parameters θ, Structural Parameters γ, Interventional Target Space I Output: Intervention Target I k * G ← Sample a set of hypothesis graphs from γ for each intervention target I k in I do
θ k ← Perform intervention I k on θ for each graph Gi in G do G i,k ← Apply intervention I k to G i,k S i,k ← Draw samples from G i,k using θ k S i,k ← Set variables in I k to 0 end for D k ← i &lt; µ i,k −μ k , µ i,k −μ k &gt; i j &lt; S i,k j −µ i,k , S i,k j −µ i,k &gt; end for Target Intervention I k * ← arg max k (D k )
(e.g. partially undirected graphs or cyclic directed graphs) is an expensive multi-pass process. Here, we thus constrain our graph sampling space to DAGs. Since most differentiable causal structure learning algorithms learn edge beliefs in the form of a soft-adjacency matrix, we present a scalable, two-stage DAG sampling procedure which exploits structural information of the soft-adjacency matrix beyond independent edge confidences (see Figure 4 for a visual illustration). More precisely, we start by sampling topological node orderings from an iterative refined score and construct DAGs in the constrained space by independent Bernoulli draws over possible edges. We can thus guarantee DAGness by construction and do not have to rely on expensive, non-scalable techniques such as rejection sampling or Gibbs sampling. The overall method is inspired by topological sorting algorithms of DAGs where we iteratively identify nodes with no incoming edges, remove them from the graph and repeat until all nodes are processed. Soft-Adjacency. Given a learnable graph structure γ ∈ R N ×N of a graph over N variables, the soft-adjacency matrix is given as σ(γ) ∈ [0, 1] N ×N such that σ(γ ij ) ∈ [0, 1] encodes the probabilistic belief in random variable X j being a direct cause of X i , where σ(x) = (1 + exp(−x)) −1 denotes the sigmoid function. For the ease of notation, we define A = σ(γ) and A l denotes the considered softadjacency σ(γ) at iteration l. Note that the shape of A l changes through the iterations. Sample node orderings. For the iterative root sampling procedure, we start at iteration l = 0 with an initial softadjacency A l = A and apply the following routine for N iterations. We take the maximum over rows of A l , resulting in a vector of independent probabilities p child l , where p child l (i) denotes the maximal probability of variable X i being a child of any other variable at the current belief state. After taking the complement p root l = 1−p child l , we arrive at p root l where p root l (i) denotes the approximated probability of variable X i being a root node in the current round. In order to arrive at a normalized distribution to sample a root . Two-Stage DAG Sampling: Based on a soft-adjacency σ(γ), we sample a topological node ordering from an iterative refined score which is repeatedly computed until we have processed all nodes of the graph. We proceed by permuting σ(γ) according to the drawn node ordering and constrain the upper triangular part to ensure DAGness. Finally, we take independent Bernoulli draws of the unconstrained edge beliefs and arrive at a sampled DAG. node, we apply a temperature-scaled softmax:
p l (i) = softmax(p root l /t) i = exp p root l (i)/t j exp p root l (j)/t
where t denotes the temperature. The introduction of temperature-scaling allows to control the distribution over nodes and account for the entropy of the structural belief. We proceed by sampling a (root) node as r l ∼ Categorical(p l ) and delete all corresponding rows and columns from A l and arrive at a shrinked soft-adjacency A l+1 ∈ [0, 1] (N −l−1)×(N −l−1) over the remaining variables. We repeat the procedure until we have processed all nodes and have a resulting topological node ordering ≺ of [r 0 , ..., r N −1 ].</p>
<p>Sample DAGs based on node orderings. Given a node ordering ≺, we permute the soft-adjacency A accordingly and constrain the upper triangular part by setting values to 0 to ensure DAGness by construction (as shown in Figure 4). Finally, we sample a DAG by independent Bernoulli draws of the edge beliefs, as proposed in Ke et al. (2019).</p>
<p>Applicability to SDI</p>
<p>Before integrating our method into the SDI framework, we must choose/design a graph sampler based on SDI's graph belief characterization and define a sampling routine to generate interventional samples under a given state of the structural and functional parameters. SDI offers a learnable graph structure over N variables with γ ∈ R N ×N such that σ(γ) ∈ [0, 1] N ×N encodes the soft-adjacency matrix. This formulation naturally suggests the application of the introduced two-phase DAG sampling to generate hypothetical DAGs under current beliefs. Under these acyclic graph configurations, one may then apply an intervention to SDI functional parameters γ and sample data using ancestral sampling. SDI's architectural choices allow a seamless integration of AIT into the structural fitting stage of SDI, where graphs are evaluated using interventional data to update the structural belief. See §2 for a compact description of the base framework.</p>
<p>Experiments</p>
<p>We evaluate AIT on single-target interventions under two different settings: SDI (Ke et al., 2019) and DCDI (Brouillard et al., 2020). We investigate the impact of AIT under both settings with respect to identifiability, sample complexity, and convergence behaviour compared to random targeting where the next intervention target is chosen independent of the current evidence. In a further line of experiments, we analyze the targeting dynamics with respect to convergence behaviour and the distribution of target node selections. This section will highlight our results on SDI while also pointing to key findings with respect to DCDI (structural discovery and identifiability). However, the results and analysis of DCDI results have been shifted to the appendix.</p>
<p>Evaluation Setup. A huge variety of SCMs and their induced DAGs exist, each of which can stress causal structure discovery algorithms in different ways. We perform a systematic evaluation over a selected set of synthetic and non-synthetic SCMs (and datasets). We distinguish between synthetic structured graphs and random graphs, the latter generated from the Erdős-Rényi (ER) model with varying edge densities (see §A.3 for a detailed description of the setup). For conciseness, we only report results on 15-node graphs in this section for the noise-free synthetic setting for AIT on SDI and on 10-node graphs for the noisy setting for AIT on SDI (discrete data). In addition, we point to key results on 10-node graphs for AIT on DCDI (continuous data) in the main text and provide further results and ablation studies in Appendix. We complete the setup with the Sachs flow cytometry dataset (Sachs et al., 2005) and the Asia network (Lauritzen &amp; Spiegelhalter, 1988) to evaluate the proposed method on well-known real-world datasets for causal structure discovery. . SDI with active intervention targeting (green) leads to superior performance over random intervention targeting (red) on random graphs of size 15. The performance gap becomes more significant with increasing edges density. The plot shows average performance in terms of SHD. Error bands were estimated using 10 random ER graphs per setting.</p>
<p>Key Findings. (a) We report strong results for activetargeted structure discovery on both discrete and continuousvalued datasets, outperforming random targeting in all experiments. (b) The proposed intervention targeting mechanism significantly reduces sample complexity with strong benefits for graphs of increasing size and density. (c) The distribution of target selections during graph exploration is strongly connected to the topology of the underlying graph.</p>
<p>(d) Our method is capable of identifying informative targets. (e) Undesirable interventions are drastically reduced.</p>
<p>(f) When monitoring structured Hamming distance (SHD) throughout the procedure, an "elbow" point appears approximately when the Markov equivalence class (MEC) has been isolated. (g) AIT introduces desirable properties such as improved recovery of erroneously converging edges. (h) AIT significantly improves robustness in noise-perturbed environments.</p>
<p>Structure discovery: Synthetic datasets. We evaluate accuracy in terms of Structural Hamming Distance (SHD) (Acid &amp; de Campos, 2003) on a diverse set of synthetic nonlinear datasets under both SDI and DCDI, adopting their respective evaluation setups. SDI with AIT outperforms all baselines and SDI with random intervention targeting over all presented datasets (see results in Table 1). It enables almost perfect identifiability on all structured graphs of size 15 except for the full15 graph, and significantly improves structure discovery of random graphs with varying densities. As the size or density of the underlying causal graphs increases, the benefit of the selection policy becomes more apparent (see Figure 5). We also examine the effectiveness of our proposed method for DCDI (Brouillard et al., 2020) on non-linear data from random graphs of size 10. Active Intervention Targeting improves the identification in terms of sample complexity and structural identifiability compared with random exploration (see §A.7 for results and analysis). We observe a clear impact of the targeting mechanisms on the order and frequency of selected interventional targets by the learner.</p>
<p>Structure discovery: flow cytometry and asia dataset. While the synthetic datasets systematically explore the strengths and weaknesses of causal structure discovery methods, we further evaluate their capabilities on the realworld flow cytometry dataset (also known as Sachs network) (Sachs et al., 2005) and the Asia network (Lauritzen &amp; Spiegelhalter, 1988) from the BnLearn Repository. SDI with active intervention targeting outperforms all measured baselines and achieves the same result as random targeting in terms of SHD, but with reduced sample complexity. Despite AIT deviating only by 6 undirected edges from the (concensus) ground truth structure of Sachs et al. (Sachs et al., 2005), there is some concern about the correctness of this graph and the different assumptions associated with the dataset (Mooij et al., 2020;Zemplenyi &amp; Miller, 2021). Therefore, perfect identification may not be achievable by any method in practice in the Sachs setting.</p>
<p>Effect of intervention targeting on sample complexity. Aside from the significantly improved identification of underlying causal structures, our method allows for a substantial reduction in interventional sample complexity. After reaching the "elbow" point in terms of structural Hamming distance, random intervention targeting requires a fairly long time to converge to a solution within the MEC. In contrast, our proposed technique continues to select informative intervention targets beyond the elbow point and more quickly converges to the correct graph within the MEC. The continued effectiveness of our method directly translates to increased sample-efficiency and convergence speed, and is apparent for all examined datasets (see Figure 5).</p>
<p>Distribution of intervention targets. The careful study of the behaviour of the proposed method under our chosen synthetic graphs enable us to reason about the method's underlying dynamics. Analysing the dynamics of intervention targeting reveals that the distribution of target node selections is linked to the topology of the underlying graph. More specifically, the number of selections of a given target node strongly correlates with its out-degree and number of descendants in the underlying ground-truth graph structure (see Figure 7). That our method prefers interventions on nodes with greater (downstream) impact on the overall system can be most clearly observed in the distribution of target 
(AIT) 0 0 0 0 0 7 0.0 (±0.0) 0.0 (±0.0) 0.0 (±0.0) 6 0
Figure 6. SDI: Dynamics and target distribution of AIT for a structured jungle graph of size 15. The graphs' nodes are sorted in topological order, root node first. The graph is binary-tree-like with 4 levels. For the dense jungle15, the multi-level structure characteristic of the tree-like graph is readily apparent even before the elbow point. Nodes without children are very rarely chosen.</p>
<p>selection on the example of the synthetic jungle graph in Figure 6.</p>
<p>Selection of informative targets. Apart our strong results in the discovery of the underlying causal graph, we demonstrate AIT's general ability of detecting informative intervention targets. In a careful designed empirical, we preinitalize the structural parameters to the ground truth graph but keep one or multiple edges within the skeleton undirected. Over all evaluated settings, AIT rapidly detects the informative intervention targets in order to direct the undirected edges. Detailed results are shown in §A.6.7.</p>
<p>Reduction of undesirable interventions. An intervention destroys the original causal influence of other variables on the intervened target variable I k , so its samples cannot be used to determine the causal parents of I k in the undisturbed system. Therefore, if a variable without children is detected, interventions upon it should be avoided since they effectively result in redundant observational samples of the remaining variables that are of no benefit for causal structure discovery. Active intervention targeting leads to the desirable property that interventions on such variables are drastically reduced (see Figure 6).</p>
<p>Identification of Markov equivalence class.</p>
<p>Investigating the evolution of the intervention target distribution over time reveals that the causal discovery seems to be divided into two phases of exploration: Phase 1 lasts until the elbow point in terms of SHD, and Phase 2 from the elbow point until convergence (see Figure 5). We observed over multiple experiments that phase 1 tends to quickly discover the underlying skeleton (removing superfluous connections while keeping some edges undirected), until a belief state γ elbow is reached representing a MEC, or a class of graphs very close to a MEC. Phase 2 is predominantly operating on the partially directed skeleton and directs the remaining edges.</p>
<p>Recovery of erroneously converging edges. Recovery of incorrectly-converging edges critically depends on adapting the order of interventions, which a random intervention policy does not. In sharp contrast, intervention targeting significantly promotes early recovery from incorrect assignment of an edge. In contrast, the observed edge dynamics and the corresponding graph belief states indicate that the random policy can lock itself into unfavorable belief states from which recovery is extremely difficult, while AIT provides an escape hatch throughout learning.</p>
<p>Improved robustness in noise-perturbed environments.</p>
<p>Considering that noise significantly impairs the performance of causal discovery, we examine the performance of active intervention targeting in noise-perturbed environments with respect to SHD and convergence speed and compare it with random intervention targeting. We conduct experiments under different noise levels in the setting of binary data generated from structured and random graphs of varying density. A noise level η denotes the probability of flipping a random variable and applying it to all measured variables of observational and interventional samples. Through all examined settings, we observe that active intervention targeting significantly improves identifiability in contrast to 0.14 0.14 -0.14 -0.14 -0.14 0.09 0.  random targeting (see §A.6.6 for detailed results). Active intervention targeting perfectly identifies all structured graphs, except for the collider and full graph, up to a noise level of η = 0.05, i.e. where every 20th variable is flipped. The observed performance boost is even more noticeable in the convergence speed, as shown in Fig. 8 for ER-4 graphs spanning over 10 variables. While the convergence-gap gets more significant with an increasing noise level, random targeting does not converge to the ground-truth graphs for a noise level higher than η = 0.02. In contrast, AIT still converges to the correct graph and shows even a convergence tendency for η = 0.05. These findings support our observation from different experiments that active intervention targeting leads to a more controlled and robust graph discovery. Further experimental results in noise-perturbed environments can be found in §A.6.6.</p>
<p>Conclusion</p>
<p>Promising results have driven the recent surge of interest in differentiable methods for causal structure learning from observational and interventional data. In this work, we augment existing neural causal discovery methods with the ability to actively intervene and propose an active learning method to choose interventions. We show in a systematic empirical study across multiple noise-free and noiseperturbed datasets that active intervention targeting not only improves sample efficiency but also the identification of the underlying causal structures compared to random intervention targeting. Our results indicate that the guided selection of intervention targets leads to a more controlled discovery with favourable properties with respect to the optimization. The increased performance boost for larger graphs is in line with our expectation as random intervention targeting scales poorly to graphs of larger size.</p>
<p>While our method shows significant improvements with respect to sample efficiency and graph recovery over existing methods across multiple noise-free and noise-perturbed datasets, the number of interventions is not yet optimal (Atkinson &amp; Fedorov, 1975;Eberhardt et al., 2012) and can potentially be reduced in future work. Further, in this work, the interventional samples were presented to the evaluated frameworks according to a fixed learning schema (e.g. fixed number of samples for evaluated interventions in graph scoring). It would be interesting to see if the information discovered by AIT could be used for a more adaptive learning procedure to further improve sample efficiency.</p>
<p>Agrawal, R., Squires, C., Yang, K., Shanmugam, K., and Uhler, C.   </p>
<p>A. Appendix</p>
<p>A.1. Related Work</p>
<p>Causal induction can use either observational and (or) interventional data. With purely observational data, the causal graph is only identifiable up to a Markov equivalence class (MEC) (Spirtes et al., 2000), interventions are needed in order to identify the underlying causal graph (Eberhardt &amp; Scheines, 2007). Our work focuses on causal induction from fused data (observational and interventional data).</p>
<p>Causal Structure Learning. There exists several approaches for causal induction from interventional data: score-based, constraint-based, conditional independence test based and continuous optimization. We refer to (Heinze-Deml et al., 2018;Vowels et al., 2021) for recent overviews. While most algorithms perform heuristic, guided searches through the discrete space of DAGs, Zheng et al. (2018) reformulates it as a continuous optimization problem constrained to the zero level set of the adjacency matrix exponential. This important result has driven recent work in the field and showed promising results (Kalainathan et al., 2018;Yu et al., 2019;Ng et al., 2019;Lachapelle et al., 2020;Zheng et al., 2020;Zhu et al., 2020). Due to the limitations of purely observational data, Ke et al. (2019) and Brouillard et al. (2020) extend the continuous optimization framework to make use of interventional data. Lippe et al. (2021) scales in a concurrent work with ours the work of (Ke et al., 2019) to higher dimensions by splitting structural edge parameters in separate orientation and likelihood parameters and leveraging it in an adapted gradient formulation with lower variance. In contrast to (Brouillard et al., 2020;Ke et al., 2019) and our work, they require interventional data on every variable.</p>
<p>Active Causal Structure Learning. Interventions are usually hard to perform and in some cases even impossible (Peters et al., 2017). Minimizing the number of interventions performed is desirable. Active causal structure learning addresses this problem, and a number of approaches have been proposed in the literature. These approaches can be divided into those that select intervention targets using graph-theoretic frameworks, and those using Bayesian methods and information gain.</p>
<p>Graph-theoretic frameworks usually proceed from a pre-specified MEC or CPDAG (completed partially directed acyclic graph) and either investigate special graph substructures (He &amp; Geng, 2008) such as cliques (Eberhardt, 2012;Squires et al., 2020), trees (Greenewald et al., 2019), or they prune and orient edges until a satisfactory solution is reached (Ghassami et al., 2018;Hyttinen et al., 2013), perhaps under a cost budget (Kocaoglu et al., 2017a;Lindgren et al., 2018). Their chief limitation is that an incorrect starting CPDAG can prevent reaching the correct graph structure even with an optimal choice of interventions.</p>
<p>The other popular set of techniques involve sampling graphs from the posterior distribution in a Bayesian framework using MCMC and then selecting the interventions which maximize the information gain on discrete (Murphy, 2001;Tong &amp; Koller, 2001) or Gaussian (Cho et al., 2016) variables. The drawbacks of these techniques is the difficulty of integrating them with non-Bayesian methods, except perhaps by bootstrapping (Agrawal et al., 2019).</p>
<p>In contrast to existing work, our base frameworks do not start from a pre-specified MEC or CPDAG and existing graphtheoretical approaches are hence not directly applicable unless we pre-initalize them with a known skeleton. However, in the case we offer access to a predefined structure in the form of a MEC or CPDAG, a previously directed edge is likely to be inverted during the ongoing process which contradicts with the underlying assumptions of existing approaches. Further, we build atop non-Bayesian frameworks and are therefore limited in applying methods based on information gain which require access to a posterior distribution over graph structures. While bootstrapping would allow us to approximate the posterior distribution over graph structures in our non-Bayesian setting, it is not guaranteed to achieve full support over all graphs since the support is limited to graphs estimated in the bootstrap procedure (Agrawal et al., 2019). Furthermore, the computational burden of bootstrap would limit us in scaling to graphs of larger size.</p>
<p>A.2. Two-Stage DAG Sampling</p>
<p>A.2.1. ALGORITHM OUTLINE</p>
<p>We present an outline of the proposed two-stage DAG sampling procedure which exploits structural information of the soft-adjacency beyond independent edge confidences. The routine is based on a graph belief state γ where σ(γ) denotes a soft-adjacency characterization. We start by sampling topological node orderings from an iterative refined score and construct DAGs in the constrained space by independent Bernoulli draws over possible edges. We can therefore guarantee DAGness by construction.</p>
<p>The temperature parameter t &gt; 0 of the temperature-scaled softmax can be used to account for the entropy of the graph belief state. However, in the general setting we suggest to initialize the parameter to t = 0.1. Note that initializing t → 0 results in always picking the maximizing argument and t → ∞ results in an uniform distribution.</p>
<p>Algorithm 2 Two-Stage DAG Sampling Input: Graph Belief State σ(γ) in the form of a soft-adjacency matrix Output: DAG Adjacency Matrix A Dag
Phase 1: Sample Node Ordering ≺ 1: A 0 ← σ(γ) 2: nodes ← [0, ..., N − 1] 3: for k = 0 to N − 1 do 4: p child k (i) ← max A k [i, :] 5: p root k (i) ← 1 − p child k (i) 6: p k (i) ← exp[p root k (i)/t] j exp[p root k (j)/t] 7: r k ← nodes[idx k ] where idx k ∼ Categorical(p k ) 8:
Remove r k from nodes 9:  (LUCE, 1959;PLACKETT, 1975) Our proposed node ordering sampling routine can be regarded as an extension of the Placket-Luce distribution over node permutations. In contrast, we refine scores in an iterative fashion rather than setting them apriori as we account for previously drawn nodes to estimate the probability of a node being the root node in the current iteration.
A k+1 ← A k [</p>
<p>A.3. Experimental Setup</p>
<p>A huge variety of SCMs and their induced DAGs exist, each of which can stress causal structure discovery algorithms in different ways. In this work, We perform a systematic evaluation over a selected set of synthetic and non-synthetic SCMs. We distinguish between discrete (based on DSDI (Ke et al., 2019)) or continuous (based on DCDI (Brouillard et al., 2020)) valued random variables. Through all experiment, we limit us to 1000 samples per intervention.</p>
<p>A.3.1. SYNTHETIC DATASETS Graph Structure. We adopt the structured graphs (see Fig. 9) proposed in the work of DSDI (Ke et al., 2019) as they adequately represent topological diversity of possible DAGs in a compact fashion. They can be split up in a set of graphs without cycles in the undirected skeletons, and one group with cycles. Extending the setup with random graphs with varying edge densities, generated from the Erdős-Rényi (ER) model, allows us to assess the generalized performance of the proposed method from sparse to dense DAGs.</p>
<p>Discrete Data Generation. We adopt the generative setup of DSDI (Ke et al., 2019) and model the SCMs using two-layer MLPs with Leaky ReLU activations between layers. For every variable X i , a seperate MLP models the conditional relationship P (X i |X pa(i) ). The MLP parameters are initialized orthogonally within the range of [−2.5, 2.5] and biases uniformly in the range of [−1.1, 1.1].</p>
<p>Continuous Data Generation.</p>
<p>For the evaluation of the adapted DCDI framework, we adopt their generative setup as described in (Brouillard et al., 2020) and use the existing non-linear datasets.</p>
<p>Graphs with acyclic skeletons:
(a) Chain (b) Collider (c) Tree
Graphs with cyclic skeletons: 
(d) Bidiag (e) Jungle (f) Full</p>
<p>. REAL-WORLD DATASETS</p>
<p>Besides the many synthetic graphs, we evaluate our method on real-world datasets provided by the BnLearn data repository. Namely on the Asia (Lauritzen &amp; Spiegelhalter, 1988) and the Sachs (Sachs et al., 2005) datasets (see Fig. 10 for a visualization of their underlying ground-truth structure). Sachs (Sachs et al., 2005) represents a systems biology dataset which exhibits non-linearity, confounding and complex structure.</p>
<p>(a) Asia (b) Sachs • BnLearn Data Repository: https://www.bnlearn.com/bnrepository/ Learning Neural Causal Models with Active Interventions</p>
<p>A.5. Hyper-Parameters</p>
<p>We used a similar set of hyperparameters for our AIT + DSDI and AIT + DCDI models as those used in the original paper (Ke et al., 2019;Brouillard et al., 2020). The specific hyperparamters we used are stated as follows.</p>
<p>DSDI.  In this section, we show further results and visualizations of experiments on discrete data and single-target interventions in various settings (such as graphs of varying size, noise-free vs. noise-perturbed, limited intervention targets). All experiments are based on the framework DSDI.</p>
<p>A.6.1. EVALUATION (SHD) ON GRAPHS OF VARYING SIZE AND DENSITY Table 4. SHD (lower is better) on various 5-variable synthetic datasets. Structured graphs are sorted in ascending order according to their edge density. ( * ) denotes average SHD over 10 random graphs., †ER-2 graphs on 5 results in the full5 graph and ER-4 graphs on 5 node graphs are non-existing  Table 5. SHD (lower is better) on various 10-variable synthetic datasets. Structured graphs are sorted in ascending order according to their edge density. ( * ) denotes average SHD over 10 random graphs.</p>
<p>Structured Graphs Random Graphs</p>
<p>Chain  Table 6. SHD (lower is better) on various 15-variable synthetic datasets. Structured graphs are sorted in ascending order according to their edge density. ( * ) denotes average SHD over 10 random graphs.</p>
<p>Structured Graphs Random Graphs</p>
<p>Chain </p>
<p>. EVALUATION OF CONVERGENCE SPEED ON GRAPHS OF VARYING SIZE AND DENSITY</p>
<p>While we have shown the effectiveness of AIT on random ER graphs of size 15 in §4, we observe similar effects on ER graphs of size 10 (see Figure 11). Overall, the results indicate a greater impact of our proposed targeting mechanisms on graphs of bigger size compared to random intervention targeting which poorly scales to graphs of larger size. . DSDI with AIT (green) leads to superior performance over random intervention targeting (red) on random graphs of size 10 of varying edge densities. Error bands were estimated using 10 random ER graphs per setting. . DSDI with AIT (green) leads to superior performance over random intervention targeting (red) on random graphs of size 15 of varying edge densities. Error bands were estimated using 10 random ER graphs per setting.</p>
<p>A.6.3. TARGET SELECTION ANALYSIS FOR GRAPHS OF VARYING SIZE AND DENSITY</p>
<p>We evaluate the distribution of target node selections over multiple DAGs of varying size to investigate the behaviour of our proposed method. Over all performed experiments, our method prefers interventions on nodes with greater (downstream) impact on the overall system, i.e. nodes of higher topological rank in the underlying DAG. 0.14 0.14 -0.14 -0.14 -0.14 0.09 0. (a) Active Intervention Targeting Figure 13. Correlation scores over graphs of varying size and density between the number of individual target selections and different topological properties of those targets. AIT shows strong correlations with the measured properties over all graphs, which indicates a controlled discovery of the underlying structure through preferential targeting of nodes with greater (downstream) impact on the overall system.</p>
<p>A.6.4. VISUALIZATION OF TARGET DISTRIBUTION ON STRUCTURED GRAPHS OF SIZE 5 (a) Random Targeting (b) Active Intervention Targeting Figure 14. Visualization of target selection on structured graphs of size 5 -bigger node size denotes more selection of the node. While Random Targeting acts as we expect and selects every node an uniform amount, AIT prefers targeting of nodes with greater (downstream) impact on the overall system, i.e. nodes of higher topological order.</p>
<p>Learning Neural Causal Models with Active Interventions</p>
<p>A.6.5. EXTENDED ANALYSIS OF EDGE DYNAMICS</p>
<p>We show all edge dynamics of all structured graphs over 15 variables and compare the dynamics of random targeting to active intervention targeting in a noise-free setting where we have access to all possible single-target interventions. A.6.6. IMPROVED ROBUSTNESS WITH DSDI+AIT IN NOISE PERTURBED ENVIRONMENTS While section §4 highlights our key findings in noise-perturbed systems, we examine the impact of AIT in noise perturbed environments more thoroughly in this section. Therefore, we systematically analyze experiments under different noise levels in the setting of binary data generated from random graphs of varying densities. A noise level η denotes the probability of flipping a random variable and apply it to all measured variables of observational and interventional samples.</p>
<p>Evaluating convergence on various ER graphs of varying densities over 10 variables under different noise levels reveals that the impact of AIT becomes of larger magnitude as the density of the graph and the noise level increases. Table 7. Performance evaluation (SHD) under different noise level η for structured and random graphs ( * ) denotes average SHD over 3 random graphs. (ii) ER-2:</p>
<p>(iii) ER-4: Figure 17. Convergence behaviour in terms of SHD for random ER graphs of various densities over 10 variables under different noise levels η. Overall, Active Intervention Targeting (orange) clearly outperforms Random Targeting (blue) over all densities under all noise levels. The performance gap becomes of larger magnitude as density of the graph and the noise level increases. Error bands were estimated using 3 random ER graphs per setting.
(a) η = 0 (b) η = 0.01 (c) η = 0.02 (d) η = 0.05</p>
<p>A.6.7. IDENTIFICATION OF INFORMATIVE INTERVENTION TARGETS</p>
<p>Our proposed method aims to select most informative intervention target(s) I k * ∈ I with respect to identifiability of the underlying structure. We conjecture that such targets yield relatively high discrepancy between samples drawn under different hypothesis graphs, indicating larger uncertainty about the target node's relation to its parents and/or children.</p>
<p>In order to evaluate our methods capability of detecting informative intervention targets, we perform multiple experiments on structured graph structures (chain5, tree5 and full5) where we preinitalize the structural belief to the ground-truth structure structure but keeping one edge between a pair of nodes (i,j) undirected, i.e. σ(γ i,j ) = σ(γ j,i ) = 0.5. Throughout the experiments, we vary the position of the undirected edge and analyze which nodes are targeted by our method.</p>
<p>Over all evaluated settings, we can observe how AIT preferentially targets the pair of nodes corresponding to the undirected edge, with small preferences towards the source nodes of the correct directed edge (see in Figure 18 and Figure 19). This observation is in line with our conjecture that AIT preferentially targets nodes with larger uncertainty about the target node's relation to its parents and/or children. Learning Neural Causal Models with Active Interventions Figure 19. AIT chooses informative intervention targets by preferentially identifying and targeting the pair of nodes corresponding to the undirected edge (nodes are marked red in the distribution of selected target nodes and edges is visualized red in the graph on the right). -Second set of experiments based on the structured graph full5.</p>
<p>A.6.8. LIMITED INTERVENTION TARGETS</p>
<p>While we allow access to all possible single-target interventions in all other experiments, real world settings are usually more restrictive. Specific interventions might be either technically impossible or even unethical, or the experiments might want to prevent interventions upon specific target nodes due to increased experiment costs. In order to test the capability of AIT, we limit the set of possible intervention targets in the following experiments and analyze the resulting behaviour based on DSDI. We examine speed of convergence and the effect on the target distribution under different scenarios on structured graphs using DSDI with AIT based on single-target interventions.</p>
<p>Scenario 1: We perform experiments on a Chain5 graph where we restrict us on intervening upon a different node in five experiment and once allow access to all targets as a comparison.</p>
<p>Throughout the experiments, we observe that blocking interventions on nodes of a higher topological level results in greater degradation of the convergence speed compared to blocked intervention on lower levels (see Figure 20). Furthermore, the distribution of selected targets indicates that our method preferentially chooses neighboring nodes of a blocked target node in the restricted setting. The impact of the restricted target node (red circled node) is clearly observable in the convergence speed (left) and distribution of target selections (middle). The speed of convergence indicates a dependence on the topological characteristic of the restricted intervention target.</p>
<p>Learning Neural Causal Models with Active Interventions Scenario 2: We perform multiple experiments on a Tree5 graph where we restrict access to different subsets of nodes (e.g. root node, set of all sink nodes) for single-target interventions.</p>
<p>Similar to the experiments on Chain5, we observe a clear impact of the available intervention targets on the convergence speed and identifiability of the underlying structure (see Figure 21). While preventing interventions on all sink nodes (node 2, 3 and 4) results in improved convergence towards the underlying structure, restricted access to the set of nodes which act as causes of other nodes (node 0 and 1) prevents us from identifying the correct underlying structure. </p>
<p>A.7. Continuous Setting: Technical Details and Results</p>
<p>While the original framework of DCDI (Brouillard et al., 2020) proposes a joint-optimization over the observational and interventional sample space by selecting samples at random, we adapt their framework to the setting of active causal discovery where we acquire interventional sample in an adaptive manner. We hypothesize that a controlled selection of informative intervention targets allows a more rapid and controlled discovery of the underlying causal structure.</p>
<p>A.7.1. DIFFERENTIABLE CAUSAL DISCOVERY FROM INTERVENTIONS (DCDI)</p>
<p>The work of DCDI (Brouillard et al., 2020) addresses causal discovery from continuous data as a continuous-constrained optimization problem using neural networks to model parameters of Gaussian distributions or normalizing flows (Rezende &amp; Mohamed, 2015) which represent conditional distributions. Unlike SDI's iterative training of the structural and functional parameters, DCDI optimizes the causal adjacency matrix and functional parameters jointly over the fused data space. But like SDI, DCDI uses random and independent interventions.</p>
<p>A.7.2. INTEGRATION OF AIT INTO DCDI</p>
<p>Instead of demanding the full interventional target space during the complete optimization as in the original approach, we split the optimization procedure into different episodes, where AIT is used to estimate a target space I of size K for each episode. This is done by computing the discrepancy scores over all possible intervention targets and selecting the K highest scoring targets. During an episode, we continue by performing L gradient steps using the fixed target space I and reevaluate it afterwards for the next episode. We visualize the adaption in the following high-level outline of the individual methodologies.</p>
<p>Algorithm 3 DCDI 1: I ← Full target space of size K = N 2: Run DCDI on I until convergence ⇒ Algorithm 4 DCDI + AIT 1: for episode e = 0 until convergence do 2: I ← Estimate target space of size K using AIT 3:</p>
<p>Run L gradient steps of DCDI on I 4: end for A.7.3. EVALUATION</p>
<p>We evaluate the effectiveness of AIT in the base framework of DCDI in the setting of non-linear, continuous data generated from random graphs over N = 10 variables and show the potential of our proposed method.</p>
<p>Structural Identification / Convergence: Despite their joint optimization formulation is not apriori designed for the setting of experimental design, an AIT guided version shows superior/competitive performance in terms of structural identification and sample complexity over the original formulation (see Figure 22). Distribution of Intervention Targets: As in DSDI, we observe strong correlation of the number of target selections with the measured topological properties of the specific nodes. This indicates a controlled discovery of the underlying causal structure through preferential targeting of nodes with greater (downstream) impact on the overall system. In addition, interventions on variables without children are drastically reduced (see also §4 for equivalent observations in DSDI).</p>
<p>Effect of Target Space Size K: While the original formulation assumes K = N for the complete optimization procedure (i.e. L = 1) and relies on random samples out of the full target space, our adapted AIT-guided version of DCDI constrains the target space to a subset of targets for each episode. An ablation study on the size of the target space shows that for all choices of K ∈ {2, 4, 6, 8}, our approach outperforms the original formulation in terms of sample complexity while achieving same or better performance in terms of SHD. Figure 22. While DCDI Vanilla assumes access to the full interventional target space through the complete optimization, the AIT guided DCDI approach reevaluates its interventional target space of size K = 6 every L = 1000 gradient steps. Among the above evaluated graphs (ground-truth on the left), DCDI+AIT demonstrates a more rapid identification of the underlying causal structure while achieving same or better performance in terms of SHD. The distribution of selected single-node intervention targets reveals again its connection to the topological properties of the corresponding nodes.</p>
<p>(a) K = 2 (b) K = 4 (d) K = 6 (d) K = 8 Figure 23. All evaluated target space sizes K ∈ {2, 4, 6, 8} show that DCDI+AIT (orange) outperforms DCDI (blue) in terms of sample complexity while achieving similar performance. Error bands were estimated using 10 random ER graphs per setting.</p>
<p>Figure 1 .
1Figure 1. AIT is an active intervention targeting technique which is applicable to all neural causal discovery frameworks of fused data. Based on the state of a learned neural causal model N up to a given timepoint, AIT selects the next informative intervention target for the causal discovery.</p>
<p>Figure 2 .
2AIT decides where to intervene by computing a score for all possible intervention targets. Given a neural causal model N , AIT starts by sampling a set of hypothesis DAGs G from the structural parameters. It proceeds by applying an intervention I k to all Gi ∈ G. Based on the topological orderings of the post-interventional DAGs G i,k and the functional parameters, it generates a set of post-interventional samples S i,k . AIT proceeds by comparing statistics of the post-interventional sample distributions across the hypothesis graphs to compute the discrepancy score D k .sami et al., 2018; Greenewald et al., 2019; Squires  et al., 2020;Murphy, 2001; Tong &amp; Koller, 2001;Masegosa &amp; Moral, 2013; Cho et al., 2016;Ness et al., 2017; Agrawal  et al., 2019; Zemplenyi &amp; Miller, 2021; Gamella &amp; Heinze- Deml, 2020), existing work in neural causal discovery has not yet focused on incorporating active interventions.</p>
<p>Figure 3 .
3SDI learns a neural causal model from fused data using alternating phases of functional and structural fitting.</p>
<p>Figure 4
4Figure 4. Two-Stage DAG Sampling: Based on a soft-adjacency σ(γ), we sample a topological node ordering from an iterative refined score which is repeatedly computed until we have processed all nodes of the graph. We proceed by permuting σ(γ) according to the drawn node ordering and constrain the upper triangular part to ensure DAGness. Finally, we take independent Bernoulli draws of the unconstrained edge beliefs and arrive at a sampled DAG.</p>
<p>Figure 5
5Figure 5. SDI with active intervention targeting (green) leads to superior performance over random intervention targeting (red) on random graphs of size 15. The performance gap becomes more significant with increasing edges density. The plot shows average performance in terms of SHD. Error bands were estimated using 10 random ER graphs per setting.</p>
<p>Figure 7 . 05 Figure 8 .
7058Correlation scores between the number of individual target selections and different topological properties of those targets. AIT shows strong correlations with the measured properties over all graphs, which indicates a controlled discovery of the underlying structure through preferential targeting of nodes with greater (downstream) impact on the overall system.(a) η = 0 (b) η = 0.01 (c) η = 0.02 (d) η = 0.Convergence Behaviour in terms of SHD for random ER-4 graphs over 10 variables under different noise levels η, where AIT (green) clearly outperforms Random Targeting (red) over all noise levels. The performance gap becomes of larger magnitude as the noise level increases. Error bands were estimated using 3 random ER graphs per setting.</p>
<p>Greenewald, K., Katz, D., Shanmugam, K., Magliacane, S., Kocaoglu, M., Adsera, E. B., and Bresler, G. Sample efficient active learning of causal trees. 2019. Hauser, A. and Bühlmann, P. Characterization and greedy learning of interventional markov equivalence classes of directed acyclic graphs. The Journal of Machine Learning Research, 13(1):2409-2464, 2012. Hauser, A. and Bühlmann, P. Two optimal strategies for active learning of causal models from interventional data. International Journal of Approximate Reasoning, 55(4): 926-939, 2014. He, Y.-B. and Geng, Z. Active learning of causal networks with intervention experiments and optimal designs. Journal of Machine Learning Research, 9(Nov):2523-2547, 2008. Heckerman, D., Geiger, D., and Chickering, D. M. Learning bayesian networks: The combination of knowledge and statistical data. Machine learning, 20(3):197-243, 1995. Heinze-Deml, C., Maathuis, M. H., and Meinshausen, N. Causal structure learning. Annual Review of Statistics and Its Application, 5:371-391, 2018. Hill, S. M., Heiser, L. M., Cokelaer, T., Unger, M., Nesser, N. K., Carlin, D. E., Zhang, Y., Sokolov, A., Paull, E. O., Wong, C. K., et al. Inferring causal molecular networks: empirical assessment through a community-based effort. Nature methods, 13(4):310-318, 2016.</p>
<p>Figure 9 .
9Visualization of Structured Graphs as proposed in Ke et al. (2019) -adapted illustration A.3.2</p>
<p>Figure 10 .
10Ground-truth structure of the evaluated real-world datasets provided by the BnLearn data repository -Illustration from: https://www.bnlearn.com/bnrepository/discrete-small.htmlA.4. Availability of Used (Existing) Assets Base Frameworks. • DSDI (Ke et al., 2019): https://github.com/nke001/causal_learning_unknown_interventions • DCDI (Brouillard et al., 2020): https://github.com/slachapelle/dcdi Baseline Methods. • GES (Chickering, 2002) and GIES (Hauser &amp; Bühlmann, 2012): www.github.com/FenTechSolutions/ CausalDiscoveryToolbox (Kalainathan &amp; Goudet, 2019) • ICP (Peters et al., 2016): https://github.com/juangamella/aicp • A-ICP (Gamella &amp; Heinze-Deml, 2020): https://github.com/juangamella/aicp • NOTEARS (Zheng et al., 2018): https://github.com/xunzheng/notears • DAG-GNN (Yu et al., 2019): https://github.com/fishmoon1234/DAG-GNN Datasets.</p>
<p>Figure 11
11Figure 11. DSDI with AIT (green) leads to superior performance over random intervention targeting (red) on random graphs of size 10 of varying edge densities. Error bands were estimated using 10 random ER graphs per setting.</p>
<p>Figure 12
12Figure 12. DSDI with AIT (green) leads to superior performance over random intervention targeting (red) on random graphs of size 15 of varying edge densities. Error bands were estimated using 10 random ER graphs per setting.</p>
<p>Figure 15 .Figure 16 .
1516Edge Dynamics of the examined structured graphs spanning over 15 variables -Part 1: The upper part shows the dynamics of random targeting and the lower of active intervention targeting. Edge Dynamics of the examined structured graphs spanning over 15 variables -Part 2: The upper part shows the dynamics of random targeting and the lower of active intervention targeting.</p>
<p>Figure 18 .
18AIT chooses informative intervention targets by preferentially identifying and targeting the pair of nodes corresponding to the undirected edge (nodes are marked red in the distribution of selected target nodes and edges is visualized red in the graph on the right). -First set of experiments based on the structured graphs chain5 and tree5.</p>
<p>Figure 20 .
20Limited intervention targets on Chain5:</p>
<p>Figure 21 .
21Limited intervention targets on Tree5: The impact of the restricted target nodes (red circled nodes) is clearly observable in the convergence speed (left) and distribution of target selections (middle).</p>
<p>Table 1 .
1SHD (lower is better) on various datasets of synthetic or real-world origin. Structured graphs are sorted in ascending order according to their edge density. Notation: ( * ) denotes average SHD over 10 different random graphs / O: Uses observational data / I: Uses interventional data / A: Active approach / ∂: Differentiable approachMethod 
SYNTHETIC (N=15) 
REAL </p>
<p>Classification 
Structured Graphs 
Random Graphs 
BnLearn </p>
<p>O 
I 
A ∂ Chain Collider Tree Bidiag Jungle Full 
ER-1 ( * ) 
ER-2 ( * ) 
ER-4 ( * ) Sachs Asia </p>
<p>GES (Chickering, 2002) 
13 
1 
12 
14 
14 
69 
8.3 (±1.9) 17.6 (±4.6) 39.4 (±6.7) 
19 
4 
NOTEARS (Zheng et al., 2018) 
22 
21 
26 
33 
35 
93 23.7 (±4.0) 35.8 (±5.2) 59.5 (±3.7) 
22 
14 
DAG-GNN (Yu et al., 2019) 
11 
14 
15 
27 
25 
97 16.0 (±3.7) 30.6 (±3.4) 59.7 (±4.1) 
19 
10 </p>
<p>GIES (Hauser &amp; Bühlmann, 2012) 
13 
6 
10 
17 
23 
60 10.9 (±4.2) 18.1 (±4.3) 39.3 (±5.6) 
16 
11 
ICP (Peters et al., 2016) 
14 
14 
14 
27 
26 105 16.2 (±3.6) 31.1 (±3.4) 60.1 (±3.9) 
17 
8 </p>
<p>A-ICP (Gamella &amp; Heinze-Deml, 2020) 
14 
14 
14 
27 
26 105 16.2 (±3.6) 31.1 (±3.4) 60.1 (±3.9) 
17 
8 </p>
<p>SDI (Random) (Ke et al., 2019) 
0 
0 
2 
3 
7 
24 
1.4 (±1.6) 
2.1 (±2.3) 
7.2 (±2.7) 
6 
0 
SDI </p>
<p>nodes, nodes] 10: end for 11: ≺= [r 0 , ..., r N −1 ] Phase 2: Sample DAG based on node ordering ≺ 12: A P erm ← Permute σ(γ) according to ≺ 13: A P erm ← Constrain upper diagonal part by setting values to 0 14: A Ber ← Bernoulli(A P erm ) 15: A Dag ← Apply inverse permutation of ≺ to A Ber A.2.2. CONNECTION TO PLACKETT-LUCE DISTRIBUTION</p>
<p>Table 2 .
2Hyperparametersfor DSDI including the corresponding 
AIT parameters </p>
<p>Number of iterations 
1000 
Batch size 
256 
Sparsity Regularizer 
0.1 
DAG Regularizer 
0.5 
Functional parameter training iterations 
10000 
Number of interventions per phase 2 
25 
Number of data batches for scoring 
10 
Number of graph configurations for scoring 
-Graph Size 5: 
10 
-Graph Size 10: 
20 
-Graph Size 15 
40 </p>
<p>AIT: 
-Number of graph configurations 
100 
-Number of interventional samples per graph &amp; target 
256 </p>
<p>DCDI. </p>
<p>Table 3 .
3Hyperparametersfor DCDI including the corresponding 
AIT parameters </p>
<p>µ 0 
10 −8 
γ 0 
0 
η 
2 
δ 
0.9 
Augmented Lagrangian Thresh 
10 −8 
Learning rate 
10 −3 
Nr. of hidden units 
16 
Nr. of hidden layers 
2 </p>
<p>AIT: 
-Number of graph configurations 
100 
-Number of interventional samples per graph &amp; target 
256 </p>
<p>ETH Zurich 2 Mila, Universite de Montréal 3 GlaxoSmithKline 4 Max Planck Institute for Intelligent Systems 5 Google Research, Brain Team 6 KTH Stockholm 7 DeepMind. Correspondence to: Nino Scherrer <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#110;&#105;&#110;&#111;&#46;&#115;&#99;&#104;&#101;&#114;&#114;&#101;&#114;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;">&#110;&#105;&#110;&#111;&#46;&#115;&#99;&#104;&#101;&#114;&#114;&#101;&#114;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;</a>.</p>
<p>Searching for bayesian network structures in the space of restricted acyclic partially directed graphs. S Acid, L M De Campos, Journal of Artificial Intelligence Research. 18Acid, S. and de Campos, L. M. Searching for bayesian network structures in the space of restricted acyclic par- tially directed graphs. Journal of Artificial Intelligence Research, 18:445-490, 2003.</p>
<p>D Kalainathan, O Goudet, I Guyon, D Lopez-Paz, M Sebag, Sam, arXiv:1803.04929Structural agnostic model, causal discovery and penalized adversarial learning. arXiv preprintKalainathan, D., Goudet, O., Guyon, I., Lopez-Paz, D., and Sebag, M. Sam: Structural agnostic model, causal dis- covery and penalized adversarial learning. arXiv preprint arXiv:1803.04929, 2018.</p>
<p>Learning neural causal models from unknown interventions. N R Ke, O Bilaniuk, A Goyal, S Bauer, H Larochelle, B Schölkopf, M C Mozer, C Pal, Y Bengio, arXiv:1910.01075arXiv preprintKe, N. R., Bilaniuk, O., Goyal, A., Bauer, S., Larochelle, H., Schölkopf, B., Mozer, M. C., Pal, C., and Bengio, Y. Learning neural causal models from unknown interven- tions. arXiv preprint arXiv:1910.01075, 2019.</p>
<p>Costoptimal learning of causal graphs. M Kocaoglu, A Dimakis, S Vishwanath, International Conference on Machine Learning. PMLRKocaoglu, M., Dimakis, A., and Vishwanath, S. Cost- optimal learning of causal graphs. In International Con- ference on Machine Learning, pp. 1875-1884. PMLR, 2017a.</p>
<p>Experimental design for learning causal graphs with latent variables. M Kocaoglu, K Shanmugam, E Bareinboim, Advances in Neural Information Processing Systems. Kocaoglu, M., Shanmugam, K., and Bareinboim, E. Ex- perimental design for learning causal graphs with latent variables. In Advances in Neural Information Processing Systems, pp. 7018-7028, 2017b.</p>
<p>Bayesian artificial intelligence. K B Korb, A E Nicholson, CRC pressKorb, K. B. and Nicholson, A. E. Bayesian artificial intelli- gence. CRC press, 2010.</p>
<p>Gradient-based neural dag learning. S Lachapelle, P Brouillard, T Deleu, S Lacoste-Julien, International Conference on Learning Representations. Lachapelle, S., Brouillard, P., Deleu, T., and Lacoste-Julien, S. Gradient-based neural dag learning. In International Conference on Learning Representations, 2020.</p>
<p>Local computations with probabilities on graphical structures and their application to expert systems. S L Lauritzen, D J Spiegelhalter, Journal of the Royal Statistical Society: Series B (Methodological). 502Lauritzen, S. L. and Spiegelhalter, D. J. Local computa- tions with probabilities on graphical structures and their application to expert systems. Journal of the Royal Statis- tical Society: Series B (Methodological), 50(2):157-194, 1988.</p>
<p>Experimental design for cost-aware learning of causal graphs. E M Lindgren, M Kocaoglu, A G Dimakis, S Vishwanath, arXiv:1810.11867arXiv preprintLindgren, E. M., Kocaoglu, M., Dimakis, A. G., and Vish- wanath, S. Experimental design for cost-aware learning of causal graphs. arXiv preprint arXiv:1810.11867, 2018.</p>
<p>P Lippe, T Cohen, E Gavves, arXiv:2107.10483Efficient neural causal discovery without acyclicity constraints. arXiv preprintLippe, P., Cohen, T., and Gavves, E. Efficient neural causal discovery without acyclicity constraints. arXiv preprint arXiv:2107.10483, 2021.</p>
<p>L Lorch, J Rothfuss, B Schölkopf, A Krause, Dibs, arXiv:2105.11839Differentiable bayesian structure learning. arXiv preprintLorch, L., Rothfuss, J., Schölkopf, B., and Krause, A. Dibs: Differentiable bayesian structure learning. arXiv preprint arXiv:2105.11839, 2021.</p>
<p>Individual Choice Behavior: A Theoretical analysis. R D Luce, WileyNew York, NY, USALuce, R. D. Individual Choice Behavior: A Theoretical analysis. Wiley, New York, NY, USA, 1959.</p>
<p>An interactive approach for bayesian network learning using domain/expert knowledge. A R Masegosa, S Moral, International Journal of Approximate Reasoning. 548Masegosa, A. R. and Moral, S. An interactive approach for bayesian network learning using domain/expert knowl- edge. International Journal of Approximate Reasoning, 54(8):1168-1181, 2013.</p>
<p>Joint causal inference from multiple contexts. J M Mooij, S Magliacane, T Claassen, Journal of Machine Learning Research. 2199Mooij, J. M., Magliacane, S., and Claassen, T. Joint causal inference from multiple contexts. Journal of Machine Learning Research, 21(99):1-108, 2020. URL http: //jmlr.org/papers/v21/17-123.html.</p>
<p>Active learning of causal bayes net structure. K P Murphy, Murphy, K. P. Active learning of causal bayes net structure. 2001.</p>
<p>A bayesian active learning experimental design for inferring signaling networks. R O Ness, K Sachs, P Mallick, O Vitek, International Conference on Research in Computational Molecular Biology. SpringerNess, R. O., Sachs, K., Mallick, P., and Vitek, O. A bayesian active learning experimental design for infer- ring signaling networks. In International Conference on Research in Computational Molecular Biology, pp. 134-156. Springer, 2017.</p>
<p>A graph autoencoder approach to causal structure learning. I Ng, S Zhu, Z Chen, Z Fang, arXiv:1911.07420arXiv preprintNg, I., Zhu, S., Chen, Z., and Fang, Z. A graph autoencoder approach to causal structure learning. arXiv preprint arXiv:1911.07420, 2019.</p>
<p>Identifiability of causal graphs using functional models. J Peters, J M Mooij, D Janzing, B Schölkopf, Proceedings of the 27th Annual Conference on Uncertainty in Artificial Intelligence (UAI). the 27th Annual Conference on Uncertainty in Artificial Intelligence (UAI)Peters, J., Mooij, J. M., Janzing, D., and Schölkopf, B. Iden- tifiability of causal graphs using functional models. In Proceedings of the 27th Annual Conference on Uncer- tainty in Artificial Intelligence (UAI), pp. 589-598, 2011.</p>
<p>Causal inference by using invariant prediction: identification and confidence intervals. J Peters, P Bühlmann, N Meinshausen, Journal of the Royal Statistical Society: Series B (Statistical Methodology). 785Peters, J., Bühlmann, P., and Meinshausen, N. Causal in- ference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Soci- ety: Series B (Statistical Methodology), 78(5):947-1012, 2016.</p>
<p>Elements of causal inference: foundations and learning algorithms. J Peters, D Janzing, B Schölkopf, The MIT PressPeters, J., Janzing, D., and Schölkopf, B. Elements of causal inference: foundations and learning algorithms. The MIT Press, 2017.</p>
<p>The analysis of permutations. R L Plackett, Journal of the Royal Statistical Society: Series C (Applied Statistics). 242Plackett, R. L. The analysis of permutations. Journal of the Royal Statistical Society: Series C (Applied Statistics), 24(2):193-202, 1975.</p>
<p>D J Rezende, S Mohamed, arXiv:1505.05770Variational inference with normalizing flows. arXiv preprintRezende, D. J. and Mohamed, S. Variational inference with normalizing flows. arXiv preprint arXiv:1505.05770, 2015.</p>
<p>Marginal structural models and causal inference in epidemiology. J M Robins, M A Hernan, B Brumback, Robins, J. M., Hernan, M. A., and Brumback, B. Marginal structural models and causal inference in epidemiology, 2000.</p>
<p>Causal protein-signaling networks derived from multiparameter single-cell data. K Sachs, O Perez, D Pe&apos;er, D A Lauffenburger, G P Nolan, Science. 3085721Sachs, K., Perez, O., Pe'er, D., Lauffenburger, D. A., and Nolan, G. P. Causal protein-signaling networks derived from multiparameter single-cell data. Science, 308(5721): 523-529, 2005.</p>
<p>Toward causal representation learning. B Schölkopf, F Locatello, S Bauer, N R Ke, N Kalchbrenner, A Goyal, Y Bengio, Proceedings of the IEEE. 1095Schölkopf, B., Locatello, F., Bauer, S., Ke, N. R., Kalch- brenner, N., Goyal, A., and Bengio, Y. Toward causal representation learning. Proceedings of the IEEE, 109(5): 612-634, 2021.</p>
<p>Learning causal graphs with small interventions. K Shanmugam, M Kocaoglu, A G Dimakis, S Vishwanath, NIPS. Shanmugam, K., Kocaoglu, M., Dimakis, A. G., and Vish- wanath, S. Learning causal graphs with small interven- tions. In NIPS, pp. 3195-3203, 2015.</p>
<p>Causation, prediction, and search. P Spirtes, C N Glymour, R Scheines, D Heckerman, C Meek, G Cooper, T Richardson, MIT pressSpirtes, P., Glymour, C. N., Scheines, R., Heckerman, D., Meek, C., Cooper, G., and Richardson, T. Causation, prediction, and search. MIT press, 2000.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8868 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8868</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8868</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-a7f801222c4d053c1ac49fb91ce3a4f750e8343b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a7f801222c4d053c1ac49fb91ce3a4f750e8343b" target="_blank">Which Modality should I use - Text, Motif, or Image? : Understanding Graphs with Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> NAACL-HLT</p>
                <p><strong>Paper TL;DR:</strong> Key findings indicate that the image modality, especially with vision-language models like GPT-4V, is superior to text in balancing token limits and preserving essential information and outperforms prior graph neural net (GNN) encoders.</p>
                <p><strong>Paper Abstract:</strong> Our research integrates graph data with Large Language Models (LLMs), which, despite their advancements in various fields using large text corpora, face limitations in encoding entire graphs due to context size constraints. This paper introduces a new approach to encoding a graph with diverse modalities, such as text, image, and motif, coupled with prompts to approximate a graph's global connectivity, thereby enhancing LLMs' efficiency in processing complex graph structures. The study also presents GraphTMI, a novel benchmark for evaluating LLMs in graph structure analysis, focusing on homophily, motif presence, and graph difficulty. Key findings indicate that the image modality, especially with vision-language models like GPT-4V, is superior to text in balancing token limits and preserving essential information and outperforms prior graph neural net (GNN) encoders. Furthermore, the research assesses how various factors affect the performance of each encoding modality and outlines the existing challenges and potential future developments for LLMs in graph understanding and reasoning tasks. All data will be publicly available upon acceptance.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8868.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8868.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text Encoder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text Modality Graph Encoder (Adjacency/Edge Serializations + Node-Label Mapping)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Encodes graph structure and node labels as textual serializations (node-to-label dictionary plus an edge encoding) and prompts an LLM (GPT-4) to perform node-level prediction (zero-shot). Used with different edge serialization formats to balance informativeness and verbosity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Text serialization / adjacency-list based linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>A node-to-label dictionary (e.g., {nodeID: label}) combined with an explicit serialized edge representation (adjacency list, edgelist, or plain-English edge descriptions) placed inside the prompt. The prompt encloses the mapping and edges in delimiting markers and asks the LLM to predict the label of a marked node.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Citation networks (real-world graphs: CORA, CITESEER, PUBMED); generally node-classification graphs / undirected graphs used in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Construct node-to-label mapping as a dictionary and serialize edges using one of several formats (Adjacency List: node: [neighbors]; Edgelist: list of (u,v) tuples; Edgetext: 'Node A is connected to Node B'; or GraphML/GML XML-like formats) and place this text in the prompt alongside the task instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node classification (Node label prediction for an unlabeled node in a subgraph/ego-graph)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Text modality (LLM GPT-4) reported mean test accuracies: CORA 0.81 ± 0.04 (denial rate D=0.07 ± 0.03), CITESEER 0.75 ± 0.05 (D=0.07 ± 0.01), PUBMED 0.83 ± 0.01 (D=0.08 ± 0.01). Metrics used: Accuracy (A), Mismatch rate (M), Denial rate (D), Token limit fraction (T).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Text modality achieved the highest accuracy in several settings and is comparable to GNN baselines, but uses more tokens and has higher denial and token-limit fractions than image modality. Within text encodings, the Adjacency List was identified as the best-performing edge representation (see separate entry).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Provides detailed local context (explicit neighbor lists and label counts); can reach or exceed GNN baseline accuracy in experiments; straightforward to construct and human-readable.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Verbose for larger subgraphs and real-world graphs, often approaches/exceeds LLM token/context limits; higher token-limit fraction and higher denial rate in some datasets; sensitive to choice of edge representation and sampling strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Fails or returns -1 when prompts become too verbose or confusing due to large subgraph sizes (high token usage); certain graph structures (high heterophily, fragmented components like CITESEER with inappropriate sampling) produce high denial or misclassification; requires careful sampling (ego vs forest-fire) and edge-format selection to avoid poor results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which Modality should I use - Text, Motif, or Image? : Understanding Graphs with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8868.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8868.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adjacency List</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adjacency List Text Encoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specific text serialization used as the edge-encoding in the text modality where each node maps to an explicit list of its neighbors (node: [neighbors]); reported as the best edge representation among alternatives in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Adjacency list linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Edges serialized as lines of 'nodeID: [neighborID1, neighborID2, ...]' alongside a node-to-label mapping dictionary; gives a granular view of local neighborhoods without excessive verbosity compared to some alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Citation networks (CORA, CITESEER, PUBMED) and subgraph samples (ego graphs up to 3 hops)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Compute subgraph (typically ego graph up to 3 hops), produce node-to-label mapping dictionary, then emit an adjacency list string and include it in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node classification (same as Text Encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Adjacency list reported as highest-performing edge representation in Figure 16: produces the highest Accuracy and lower Mismatch, Denial and Token limit fraction compared to Edgelist, Edgetext, GML, GraphML in their experiments (quantitative per-modality metrics aggregated in paper figures/tables).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Outperformed Edgelist, Edgetext, GML and GraphML encodings in experiments (Fig.16). It achieved the best balance of informativeness (accuracy) and verbosity (token fraction).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Granular neighborhood information with relatively compact textual form; best empirical tradeoff between accuracy and token usage among tested text encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Still can be verbose for large neighborhoods (large ego graphs / many hops); subject to token-limit concerns for big subgraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Large ego-graphs in PUBMED produced higher token-limit fractions; when sampling strategy yields very large neighborhoods, adjacency-list prompts can become unwieldy and lead to denials or mispredictions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which Modality should I use - Text, Motif, or Image? : Understanding Graphs with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8868.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8868.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Edgelist / Edgetext / GML / GraphML</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alternative Text Edge Encodings (Edgelist, Edgetext, GML, GraphML)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Several text-based edge serialization formats evaluated as alternatives to adjacency lists, including explicit lists of edge tuples (edgelist), plain-English edge descriptions (edgetext), and structured graph markup (GML/GraphML XML).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Edgelist, Edgetext, GML/GraphML textual encodings</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Edgelist: a sequence of (u,v) pairs; Edgetext: human-readable sentences like 'Node A is connected to Node B'; GML/GraphML: structured markup embedding node ids and labels with edge source/target tags. Each form encodes the same graph information but with different verbosity and structure.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Citation networks (subgraph samples) used in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Serialize edges using the chosen format and include node labels (either embedded in the format or as a separate dictionary) in the prompt text.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node classification (same experimental task)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported comparatively lower performance than Adjacency List in the paper's evaluation (Figure 16 shows adjacency list leads). Exact per-format numeric breakdown not tabulated in the main text, but adjacency list is identified as best.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Edgelist, Edgetext, and GraphML/GML are explicitly compared; adjacency list performed better in accuracy/token tradeoff. GML/GraphML embed labels but were not top-performing.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Some formats (GraphML/GML) can embed node labels directly and are standard interchange formats; Edgetext is human readable.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Edgetext can be verbose; GraphML/GML/XML may add verbosity/complexity that increases token usage; these formats underperformed adjacency list for the LLM prompt setting.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Formats that are too verbose or poorly structured increased token fraction and denial rates; GraphML/GML's extra verbosity did not translate into better model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which Modality should I use - Text, Motif, or Image? : Understanding Graphs with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8868.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8868.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Node-Label Mapping Dict</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Node-to-Label Dictionary Mapping</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A required component across text and motif encodings: a compact dictionary mapping each node ID to its known label (or '?' for the target node); included in the prompt to provide supervision/local label context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Node-to-label dictionary</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>A textual dictionary of the form {nodeID: label, ...} where labeled nodes have explicit labels and the node to be predicted is marked with '?' or a special token.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Used with citation-network subgraphs (ego graphs) across modalities</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Compute mapping from graph data, format as a dictionary string, and prepend or include inside prompt delimiters (triple backticks in examples).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node classification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Implicit component of all LLM experiments; contributes to reported modality metrics (see Text/Motif/Image entries).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Not an alternative but common input across encodings; experiments show combining this mapping with motif lists or images can correct misclassifications compared to text-only prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Compact, necessary for LLM to know labels of neighbors; reduces ambiguity about label-to-color or label-to-node correspondence.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>If omitted (e.g., image-only encoding without explicit legend), VLMs may misinterpret colors and deny prediction; must be combined carefully with edge/motif info.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Image-modality failures highlighted that when only colors implicitly indicated labels (without explicit mapping), GPT-4V sometimes returned -1 because the label-color association was not made explicit.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which Modality should I use - Text, Motif, or Image? : Understanding Graphs with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8868.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8868.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Motif Encoder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Motif-based Text Encoder (Counts + Attached Motifs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Encodes graph structural motifs (triangles, stars, cliques) around the query node as textual features (counts and explicit member lists) and supplies node-to-label mapping; designed to capture both local and some global context with less verbosity than full adjacency lists.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Motif encoding (counts and attached motif member lists)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Compute motif statistics in the local neighborhood (number of triangles, number of stars, cliques), and optionally list motifs attached to the queried node (which nodes form triangles or stars with it). Include these motif counts/lists plus node-label mapping in the prompt text.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Citation networks (local ego graphs); intended for graphs where local structural patterns (motifs) matter</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Detect motifs in the subgraph (triads, star motifs, cliques), produce counts and/or explicit lists of motif members attached to the target node, and include this motif information textually in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node classification (node label prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Motif modality LLM accuracies: CORA 0.73 ± 0.06 (D=0.06 ± 0.01), CITESEER 0.59 ± 0.01 (D=0.32 ± 0.02), PUBMED 0.77 ± 0.006 (D=0.13 ± 0.04). Figure 17 shows that including 'triangle and star attached to ? node' improved mean accuracy and lowered mismatch/denial.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Motif modality lagged behind text in average accuracy but outperformed other modalities on the hardest graphs (motif-rich/low-homophily); image modality generally outperformed motif on medium-difficulty tasks, while motif excelled on the hardest tasks (balance of local/global).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Captures salient local structure that can be more informative than raw adjacency lists for complex/hard graphs; motif-attachment info (which specific motifs the node participates in) improved performance more than mere motif counts.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Computationally expensive to detect motifs at scale (authors restricted to 3-hop ego subgraphs); raw motif counts alone are less informative than motif-attachment lists; motif-only encoding may miss global context.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>High denial/mismatch in fragmented graphs (e.g., CITESEER) when motif information insufficient; scalability limits due to motif detection compute costs—authors constrained sampling to manage cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which Modality should I use - Text, Motif, or Image? : Understanding Graphs with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8868.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8868.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Motif-Attached</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Motif-attachment Listing (Triangles and Stars Attached to Target Node)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A refinement of the motif encoding that explicitly lists the specific motif instances attached to the queried node (e.g., '[u,v,w]' triangles including the target), which empirically improved accuracy over mere motif counts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Attached-motif listing</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Instead of only counts, provide explicit lists of motif node-IDs that include the target node (e.g., 'Triangle motifs attached to ? node: [n1,n2,?], [n3,n4,?]'), enabling the LLM to see exact local structural participants.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Local ego-graph subgraphs in citation datasets</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Detect motifs in neighborhood, filter motifs that include the target node, serialize these motif member lists into the prompt along with node-label mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node classification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Figure 17 indicates that the 'triangle and star attached to ?' variant had higher accuracy and lower mismatch/denial rates compared to other motif encodings (quantitative averages reported in motif modality metrics above).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Outperformed simple motif-count encodings; increased LLM performance on hard graphs compared to count-only motif encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Provides richer local structural context that helps the LLM infer node labels better than counts alone.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>More verbose than count-only motif encodings and requires computing which motifs attach to the target (increased compute).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Still limited by motif-detection scalability; if attached motifs are sparse or uninformative (e.g., highly heterophilous neighborhoods), benefit is reduced.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which Modality should I use - Text, Motif, or Image? : Understanding Graphs with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8868.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8868.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Template (Text)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Prompt Template with Encoded Graph Text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A concrete prompt design used in experiments: provide task description, adjacency/motif info in triple backticks, request output in strict format, and instruct to return -1 if uncertain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Zero-shot textual prompt with delimitation and failure token</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Prompt begins with Task description (Node Label Prediction), then encloses adjacency list / node-label mapping / motif info in triple backticks, specifies required output format 'Label of Node = <predicted label>' and instructs returning -1 if cannot determine.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Used for node classification on ego-graph subgraphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Serialize chosen representation (adjacency list / motif / node-label dict) into prompt body; include strict response formatting and uncertainty handling instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node classification (zero-shot LLM prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Metrics reported for modalities above used this prompt design (Accuracy, Mismatch, Denial, Token fraction). The explicit '-1' instruction enabled computation of Denial rate D.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Authors used zero-shot prompting intentionally (no few-shot/examples) to isolate modality effects; qualitative results showed few-shot examples (image/text combined) could reduce denial errors in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple, reproducible prompt format; explicit uncertainty output enables measuring Denial rate.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Zero-shot design may underutilize LLM reasoning capacity compared to few-shot / chain-of-thought methods; may contribute to denials that few-shot examples would avoid.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Examples where LLM returned -1 because color-to-label mapping was implicit in images (see image modality failure cases) or where zero-shot lacked exemplars for complex heterophilous patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which Modality should I use - Text, Motif, or Image? : Understanding Graphs with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8868.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8868.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ego Graph Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ego Graph Sampling (Local Neighborhood Subgraph)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph sampling strategy that extracts a node-centered subgraph (ego graph) up to a specified hop radius (authors used up to 3 hops) to limit prompt size and provide local context for the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Ego-graph based subgraph extraction</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For a query node, extract the induced subgraph consisting of nodes within N hops (e.g., 3) and their edges; serialize this subgraph using the chosen text encoding (adjacency list, motif list, etc.) for the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Used on real-world citation networks to produce prompt-sized subgraphs for LLM inputs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Perform breadth-first expansion from query node to N hops; collect nodes/edges and produce node-label mapping and edge/motif encodings for the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node classification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Sampling impacted metrics: ego sampling produced smaller token fractions for fragmented graphs (CITESEER) and was preferable for local-community graphs (CORA); exact numeric impact shown in dataset-specific figures (Fig.9, Fig.15).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared with Forest Fire sampling: Ego graphs better revealed local community structure and often improved accuracy for clustered datasets (CORA); Forest Fire provided broader global view useful for large networks (PUBMED) but sometimes increased token usage.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Keeps prompt sizes manageable, preserves local structural info relevant to node-label prediction (homophily-based inference), aligns with node-classification needs.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>May miss global context beyond the sampled hops; choice of hop count trades off between informativeness and verbosity.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>For very large graphs or graphs requiring global context (medium-difficulty tasks), ego-only samples can omit crucial information and reduce performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which Modality should I use - Text, Motif, or Image? : Understanding Graphs with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8868.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8868.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Forest Fire Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Forest Fire Graph Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stochastic sampling method that grows a sample by probabilistic edge 'burning' to capture a broader and sparser portion of the graph; used to create prompt-sized subgraphs with more global coverage than ego graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Forest-fire stochastic sampling</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Randomly select a seed node and expand the sampled region by probabilistically 'burning' outgoing edges with some probability, producing variable-sized subgraphs emphasizing global structure.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Applied to citation networks (PUBMED, CITESEER, CORA) in evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Start from random seed(s), iteratively include neighbors with burn-probabilities until desired sample size; serialize resulting subgraph for prompt encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node classification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Forest fire sampling provided a broader view useful for PUBMED (large, connected), but led to higher misclassification or denial in some datasets (CITESEER) when used with image modality; reported impacts are shown in dataset/sampling plots (Fig.9, Fig.15).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to ego sampling: Forest fire gives broader coverage and is suitable for large graphs but can produce sparser/more fragmentary samples that reduce local label signal in some datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Better for capturing global network structure in large graphs where ego graphs become too large; can reduce redundancy from purely-local views.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Produces variable sample sizes and sometimes sparser local signal; may increase misclassification for tasks needing local homophily context.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>CITESEER (fragmented graph) struggled with forest-fire sampling when using text/image encodings, producing higher denial rates and lower mean accuracy in some configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Which Modality should I use - Text, Motif, or Image? : Understanding Graphs with Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Talk like a graph: Encoding graphs for large language models <em>(Rating: 2)</em></li>
                <li>Gpt4graph: Can large language models understand graph structured data? <em>(Rating: 2)</em></li>
                <li>Can language models solve graph problems in natural language? <em>(Rating: 2)</em></li>
                <li>Exploring the potential of large language models (llms) in learning on graphs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8868",
    "paper_id": "paper-a7f801222c4d053c1ac49fb91ce3a4f750e8343b",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "Text Encoder",
            "name_full": "Text Modality Graph Encoder (Adjacency/Edge Serializations + Node-Label Mapping)",
            "brief_description": "Encodes graph structure and node labels as textual serializations (node-to-label dictionary plus an edge encoding) and prompts an LLM (GPT-4) to perform node-level prediction (zero-shot). Used with different edge serialization formats to balance informativeness and verbosity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Text serialization / adjacency-list based linearization",
            "representation_description": "A node-to-label dictionary (e.g., {nodeID: label}) combined with an explicit serialized edge representation (adjacency list, edgelist, or plain-English edge descriptions) placed inside the prompt. The prompt encloses the mapping and edges in delimiting markers and asks the LLM to predict the label of a marked node.",
            "graph_type": "Citation networks (real-world graphs: CORA, CITESEER, PUBMED); generally node-classification graphs / undirected graphs used in experiments",
            "conversion_method": "Construct node-to-label mapping as a dictionary and serialize edges using one of several formats (Adjacency List: node: [neighbors]; Edgelist: list of (u,v) tuples; Edgetext: 'Node A is connected to Node B'; or GraphML/GML XML-like formats) and place this text in the prompt alongside the task instruction.",
            "downstream_task": "Node classification (Node label prediction for an unlabeled node in a subgraph/ego-graph)",
            "performance_metrics": "Text modality (LLM GPT-4) reported mean test accuracies: CORA 0.81 ± 0.04 (denial rate D=0.07 ± 0.03), CITESEER 0.75 ± 0.05 (D=0.07 ± 0.01), PUBMED 0.83 ± 0.01 (D=0.08 ± 0.01). Metrics used: Accuracy (A), Mismatch rate (M), Denial rate (D), Token limit fraction (T).",
            "comparison_to_others": "Text modality achieved the highest accuracy in several settings and is comparable to GNN baselines, but uses more tokens and has higher denial and token-limit fractions than image modality. Within text encodings, the Adjacency List was identified as the best-performing edge representation (see separate entry).",
            "advantages": "Provides detailed local context (explicit neighbor lists and label counts); can reach or exceed GNN baseline accuracy in experiments; straightforward to construct and human-readable.",
            "disadvantages": "Verbose for larger subgraphs and real-world graphs, often approaches/exceeds LLM token/context limits; higher token-limit fraction and higher denial rate in some datasets; sensitive to choice of edge representation and sampling strategy.",
            "failure_cases": "Fails or returns -1 when prompts become too verbose or confusing due to large subgraph sizes (high token usage); certain graph structures (high heterophily, fragmented components like CITESEER with inappropriate sampling) produce high denial or misclassification; requires careful sampling (ego vs forest-fire) and edge-format selection to avoid poor results.",
            "uuid": "e8868.0",
            "source_info": {
                "paper_title": "Which Modality should I use - Text, Motif, or Image? : Understanding Graphs with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Adjacency List",
            "name_full": "Adjacency List Text Encoding",
            "brief_description": "A specific text serialization used as the edge-encoding in the text modality where each node maps to an explicit list of its neighbors (node: [neighbors]); reported as the best edge representation among alternatives in this work.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Adjacency list linearization",
            "representation_description": "Edges serialized as lines of 'nodeID: [neighborID1, neighborID2, ...]' alongside a node-to-label mapping dictionary; gives a granular view of local neighborhoods without excessive verbosity compared to some alternatives.",
            "graph_type": "Citation networks (CORA, CITESEER, PUBMED) and subgraph samples (ego graphs up to 3 hops)",
            "conversion_method": "Compute subgraph (typically ego graph up to 3 hops), produce node-to-label mapping dictionary, then emit an adjacency list string and include it in the prompt.",
            "downstream_task": "Node classification (same as Text Encoder)",
            "performance_metrics": "Adjacency list reported as highest-performing edge representation in Figure 16: produces the highest Accuracy and lower Mismatch, Denial and Token limit fraction compared to Edgelist, Edgetext, GML, GraphML in their experiments (quantitative per-modality metrics aggregated in paper figures/tables).",
            "comparison_to_others": "Outperformed Edgelist, Edgetext, GML and GraphML encodings in experiments (Fig.16). It achieved the best balance of informativeness (accuracy) and verbosity (token fraction).",
            "advantages": "Granular neighborhood information with relatively compact textual form; best empirical tradeoff between accuracy and token usage among tested text encodings.",
            "disadvantages": "Still can be verbose for large neighborhoods (large ego graphs / many hops); subject to token-limit concerns for big subgraphs.",
            "failure_cases": "Large ego-graphs in PUBMED produced higher token-limit fractions; when sampling strategy yields very large neighborhoods, adjacency-list prompts can become unwieldy and lead to denials or mispredictions.",
            "uuid": "e8868.1",
            "source_info": {
                "paper_title": "Which Modality should I use - Text, Motif, or Image? : Understanding Graphs with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Edgelist / Edgetext / GML / GraphML",
            "name_full": "Alternative Text Edge Encodings (Edgelist, Edgetext, GML, GraphML)",
            "brief_description": "Several text-based edge serialization formats evaluated as alternatives to adjacency lists, including explicit lists of edge tuples (edgelist), plain-English edge descriptions (edgetext), and structured graph markup (GML/GraphML XML).",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Edgelist, Edgetext, GML/GraphML textual encodings",
            "representation_description": "Edgelist: a sequence of (u,v) pairs; Edgetext: human-readable sentences like 'Node A is connected to Node B'; GML/GraphML: structured markup embedding node ids and labels with edge source/target tags. Each form encodes the same graph information but with different verbosity and structure.",
            "graph_type": "Citation networks (subgraph samples) used in experiments",
            "conversion_method": "Serialize edges using the chosen format and include node labels (either embedded in the format or as a separate dictionary) in the prompt text.",
            "downstream_task": "Node classification (same experimental task)",
            "performance_metrics": "Reported comparatively lower performance than Adjacency List in the paper's evaluation (Figure 16 shows adjacency list leads). Exact per-format numeric breakdown not tabulated in the main text, but adjacency list is identified as best.",
            "comparison_to_others": "Edgelist, Edgetext, and GraphML/GML are explicitly compared; adjacency list performed better in accuracy/token tradeoff. GML/GraphML embed labels but were not top-performing.",
            "advantages": "Some formats (GraphML/GML) can embed node labels directly and are standard interchange formats; Edgetext is human readable.",
            "disadvantages": "Edgetext can be verbose; GraphML/GML/XML may add verbosity/complexity that increases token usage; these formats underperformed adjacency list for the LLM prompt setting.",
            "failure_cases": "Formats that are too verbose or poorly structured increased token fraction and denial rates; GraphML/GML's extra verbosity did not translate into better model performance.",
            "uuid": "e8868.2",
            "source_info": {
                "paper_title": "Which Modality should I use - Text, Motif, or Image? : Understanding Graphs with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Node-Label Mapping Dict",
            "name_full": "Node-to-Label Dictionary Mapping",
            "brief_description": "A required component across text and motif encodings: a compact dictionary mapping each node ID to its known label (or '?' for the target node); included in the prompt to provide supervision/local label context.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Node-to-label dictionary",
            "representation_description": "A textual dictionary of the form {nodeID: label, ...} where labeled nodes have explicit labels and the node to be predicted is marked with '?' or a special token.",
            "graph_type": "Used with citation-network subgraphs (ego graphs) across modalities",
            "conversion_method": "Compute mapping from graph data, format as a dictionary string, and prepend or include inside prompt delimiters (triple backticks in examples).",
            "downstream_task": "Node classification",
            "performance_metrics": "Implicit component of all LLM experiments; contributes to reported modality metrics (see Text/Motif/Image entries).",
            "comparison_to_others": "Not an alternative but common input across encodings; experiments show combining this mapping with motif lists or images can correct misclassifications compared to text-only prompts.",
            "advantages": "Compact, necessary for LLM to know labels of neighbors; reduces ambiguity about label-to-color or label-to-node correspondence.",
            "disadvantages": "If omitted (e.g., image-only encoding without explicit legend), VLMs may misinterpret colors and deny prediction; must be combined carefully with edge/motif info.",
            "failure_cases": "Image-modality failures highlighted that when only colors implicitly indicated labels (without explicit mapping), GPT-4V sometimes returned -1 because the label-color association was not made explicit.",
            "uuid": "e8868.3",
            "source_info": {
                "paper_title": "Which Modality should I use - Text, Motif, or Image? : Understanding Graphs with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Motif Encoder",
            "name_full": "Motif-based Text Encoder (Counts + Attached Motifs)",
            "brief_description": "Encodes graph structural motifs (triangles, stars, cliques) around the query node as textual features (counts and explicit member lists) and supplies node-to-label mapping; designed to capture both local and some global context with less verbosity than full adjacency lists.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Motif encoding (counts and attached motif member lists)",
            "representation_description": "Compute motif statistics in the local neighborhood (number of triangles, number of stars, cliques), and optionally list motifs attached to the queried node (which nodes form triangles or stars with it). Include these motif counts/lists plus node-label mapping in the prompt text.",
            "graph_type": "Citation networks (local ego graphs); intended for graphs where local structural patterns (motifs) matter",
            "conversion_method": "Detect motifs in the subgraph (triads, star motifs, cliques), produce counts and/or explicit lists of motif members attached to the target node, and include this motif information textually in the prompt.",
            "downstream_task": "Node classification (node label prediction)",
            "performance_metrics": "Motif modality LLM accuracies: CORA 0.73 ± 0.06 (D=0.06 ± 0.01), CITESEER 0.59 ± 0.01 (D=0.32 ± 0.02), PUBMED 0.77 ± 0.006 (D=0.13 ± 0.04). Figure 17 shows that including 'triangle and star attached to ? node' improved mean accuracy and lowered mismatch/denial.",
            "comparison_to_others": "Motif modality lagged behind text in average accuracy but outperformed other modalities on the hardest graphs (motif-rich/low-homophily); image modality generally outperformed motif on medium-difficulty tasks, while motif excelled on the hardest tasks (balance of local/global).",
            "advantages": "Captures salient local structure that can be more informative than raw adjacency lists for complex/hard graphs; motif-attachment info (which specific motifs the node participates in) improved performance more than mere motif counts.",
            "disadvantages": "Computationally expensive to detect motifs at scale (authors restricted to 3-hop ego subgraphs); raw motif counts alone are less informative than motif-attachment lists; motif-only encoding may miss global context.",
            "failure_cases": "High denial/mismatch in fragmented graphs (e.g., CITESEER) when motif information insufficient; scalability limits due to motif detection compute costs—authors constrained sampling to manage cost.",
            "uuid": "e8868.4",
            "source_info": {
                "paper_title": "Which Modality should I use - Text, Motif, or Image? : Understanding Graphs with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Motif-Attached",
            "name_full": "Motif-attachment Listing (Triangles and Stars Attached to Target Node)",
            "brief_description": "A refinement of the motif encoding that explicitly lists the specific motif instances attached to the queried node (e.g., '[u,v,w]' triangles including the target), which empirically improved accuracy over mere motif counts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Attached-motif listing",
            "representation_description": "Instead of only counts, provide explicit lists of motif node-IDs that include the target node (e.g., 'Triangle motifs attached to ? node: [n1,n2,?], [n3,n4,?]'), enabling the LLM to see exact local structural participants.",
            "graph_type": "Local ego-graph subgraphs in citation datasets",
            "conversion_method": "Detect motifs in neighborhood, filter motifs that include the target node, serialize these motif member lists into the prompt along with node-label mapping.",
            "downstream_task": "Node classification",
            "performance_metrics": "Figure 17 indicates that the 'triangle and star attached to ?' variant had higher accuracy and lower mismatch/denial rates compared to other motif encodings (quantitative averages reported in motif modality metrics above).",
            "comparison_to_others": "Outperformed simple motif-count encodings; increased LLM performance on hard graphs compared to count-only motif encodings.",
            "advantages": "Provides richer local structural context that helps the LLM infer node labels better than counts alone.",
            "disadvantages": "More verbose than count-only motif encodings and requires computing which motifs attach to the target (increased compute).",
            "failure_cases": "Still limited by motif-detection scalability; if attached motifs are sparse or uninformative (e.g., highly heterophilous neighborhoods), benefit is reduced.",
            "uuid": "e8868.5",
            "source_info": {
                "paper_title": "Which Modality should I use - Text, Motif, or Image? : Understanding Graphs with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Prompt Template (Text)",
            "name_full": "Zero-shot Prompt Template with Encoded Graph Text",
            "brief_description": "A concrete prompt design used in experiments: provide task description, adjacency/motif info in triple backticks, request output in strict format, and instruct to return -1 if uncertain.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Zero-shot textual prompt with delimitation and failure token",
            "representation_description": "Prompt begins with Task description (Node Label Prediction), then encloses adjacency list / node-label mapping / motif info in triple backticks, specifies required output format 'Label of Node = &lt;predicted label&gt;' and instructs returning -1 if cannot determine.",
            "graph_type": "Used for node classification on ego-graph subgraphs",
            "conversion_method": "Serialize chosen representation (adjacency list / motif / node-label dict) into prompt body; include strict response formatting and uncertainty handling instructions.",
            "downstream_task": "Node classification (zero-shot LLM prediction)",
            "performance_metrics": "Metrics reported for modalities above used this prompt design (Accuracy, Mismatch, Denial, Token fraction). The explicit '-1' instruction enabled computation of Denial rate D.",
            "comparison_to_others": "Authors used zero-shot prompting intentionally (no few-shot/examples) to isolate modality effects; qualitative results showed few-shot examples (image/text combined) could reduce denial errors in some cases.",
            "advantages": "Simple, reproducible prompt format; explicit uncertainty output enables measuring Denial rate.",
            "disadvantages": "Zero-shot design may underutilize LLM reasoning capacity compared to few-shot / chain-of-thought methods; may contribute to denials that few-shot examples would avoid.",
            "failure_cases": "Examples where LLM returned -1 because color-to-label mapping was implicit in images (see image modality failure cases) or where zero-shot lacked exemplars for complex heterophilous patterns.",
            "uuid": "e8868.6",
            "source_info": {
                "paper_title": "Which Modality should I use - Text, Motif, or Image? : Understanding Graphs with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Ego Graph Sampling",
            "name_full": "Ego Graph Sampling (Local Neighborhood Subgraph)",
            "brief_description": "A graph sampling strategy that extracts a node-centered subgraph (ego graph) up to a specified hop radius (authors used up to 3 hops) to limit prompt size and provide local context for the LLM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Ego-graph based subgraph extraction",
            "representation_description": "For a query node, extract the induced subgraph consisting of nodes within N hops (e.g., 3) and their edges; serialize this subgraph using the chosen text encoding (adjacency list, motif list, etc.) for the prompt.",
            "graph_type": "Used on real-world citation networks to produce prompt-sized subgraphs for LLM inputs",
            "conversion_method": "Perform breadth-first expansion from query node to N hops; collect nodes/edges and produce node-label mapping and edge/motif encodings for the prompt.",
            "downstream_task": "Node classification",
            "performance_metrics": "Sampling impacted metrics: ego sampling produced smaller token fractions for fragmented graphs (CITESEER) and was preferable for local-community graphs (CORA); exact numeric impact shown in dataset-specific figures (Fig.9, Fig.15).",
            "comparison_to_others": "Compared with Forest Fire sampling: Ego graphs better revealed local community structure and often improved accuracy for clustered datasets (CORA); Forest Fire provided broader global view useful for large networks (PUBMED) but sometimes increased token usage.",
            "advantages": "Keeps prompt sizes manageable, preserves local structural info relevant to node-label prediction (homophily-based inference), aligns with node-classification needs.",
            "disadvantages": "May miss global context beyond the sampled hops; choice of hop count trades off between informativeness and verbosity.",
            "failure_cases": "For very large graphs or graphs requiring global context (medium-difficulty tasks), ego-only samples can omit crucial information and reduce performance.",
            "uuid": "e8868.7",
            "source_info": {
                "paper_title": "Which Modality should I use - Text, Motif, or Image? : Understanding Graphs with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Forest Fire Sampling",
            "name_full": "Forest Fire Graph Sampling",
            "brief_description": "A stochastic sampling method that grows a sample by probabilistic edge 'burning' to capture a broader and sparser portion of the graph; used to create prompt-sized subgraphs with more global coverage than ego graphs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Forest-fire stochastic sampling",
            "representation_description": "Randomly select a seed node and expand the sampled region by probabilistically 'burning' outgoing edges with some probability, producing variable-sized subgraphs emphasizing global structure.",
            "graph_type": "Applied to citation networks (PUBMED, CITESEER, CORA) in evaluation",
            "conversion_method": "Start from random seed(s), iteratively include neighbors with burn-probabilities until desired sample size; serialize resulting subgraph for prompt encoding.",
            "downstream_task": "Node classification",
            "performance_metrics": "Forest fire sampling provided a broader view useful for PUBMED (large, connected), but led to higher misclassification or denial in some datasets (CITESEER) when used with image modality; reported impacts are shown in dataset/sampling plots (Fig.9, Fig.15).",
            "comparison_to_others": "Compared to ego sampling: Forest fire gives broader coverage and is suitable for large graphs but can produce sparser/more fragmentary samples that reduce local label signal in some datasets.",
            "advantages": "Better for capturing global network structure in large graphs where ego graphs become too large; can reduce redundancy from purely-local views.",
            "disadvantages": "Produces variable sample sizes and sometimes sparser local signal; may increase misclassification for tasks needing local homophily context.",
            "failure_cases": "CITESEER (fragmented graph) struggled with forest-fire sampling when using text/image encodings, producing higher denial rates and lower mean accuracy in some configurations.",
            "uuid": "e8868.8",
            "source_info": {
                "paper_title": "Which Modality should I use - Text, Motif, or Image? : Understanding Graphs with Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Talk like a graph: Encoding graphs for large language models",
            "rating": 2,
            "sanitized_title": "talk_like_a_graph_encoding_graphs_for_large_language_models"
        },
        {
            "paper_title": "Gpt4graph: Can large language models understand graph structured data?",
            "rating": 2,
            "sanitized_title": "gpt4graph_can_large_language_models_understand_graph_structured_data"
        },
        {
            "paper_title": "Can language models solve graph problems in natural language?",
            "rating": 2,
            "sanitized_title": "can_language_models_solve_graph_problems_in_natural_language"
        },
        {
            "paper_title": "Exploring the potential of large language models (llms) in learning on graphs",
            "rating": 1,
            "sanitized_title": "exploring_the_potential_of_large_language_models_llms_in_learning_on_graphs"
        }
    ],
    "cost": 0.01265235,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Which Modality should I use - Text, Motif, or Image? : Understanding Graphs with Large Language Models</h1>
<p>Debarati Das Ishaan Gupta Jaideep Srivastava Dongyeop Kang
Department of Computer Science, University of Minnesota
{das00015, gupta737, srivasta, dongyeop}@umn.edu</p>
<h6>Abstract</h6>
<p>Our research integrates graph data with Large Language Models (LLMs), which, despite their advancements in various fields using large text corpora, face limitations in encoding entire graphs due to context size constraints. This paper introduces a new approach to encoding a graph with diverse modalities, such as text, image, and motif, coupled with prompts to approximate a graph's global connectivity, thereby enhancing LLMs' efficiency in processing complex graph structures. The study also presents GRAPHTMI, a novel benchmark for evaluating LLMs in graph structure analysis, focusing on homophily, motif presence, and graph difficulty. Key findings indicate that the image modality, especially with vision-language models like GPT-4V, is superior to text in balancing token limits and preserving essential information and outperforms prior graph neural net (GNN) encoders. Furthermore, the research assesses how various factors affect the performance of each encoding modality and outlines the existing challenges and potential future developments for LLMs in graph understanding and reasoning tasks. All data will be publicly available upon acceptance.</p>
<h2>1 Introduction</h2>
<p>Large Language Models (LLMs) are increasingly utilized in areas with inherent graph structures like social network analysis (Mislove et al., 2007), drug discovery (Vishveshwara et al., 2002), and recommendation systems (Melville and Sindhwani, 2010), but they face limitations due to their reliance on unstructured text and challenges in incorporating new data post-training (Zhang et al., 2023; Lewis et al., 2020; Pan et al., 2023). Graph-structured data can address these issues, providing a nuanced and flexible representation of real-world relationships.</p>
<p>While there has been progress in interpreting multi-modal information (Yin et al., 2023),</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Input modality encoding for graphs impacts node classification, with text modality offering detailed information from a local point of view but violating the input context limitations for LLMs due to verbosity. Motif modality provides local and global context, while image modality gives a comprehensive global view, efficiently processed by GPT-4V, which integrates capabilities from both vision and text.</p>
<p>integrating graph understanding into LLMs remains a developing area. Current challenges include LLMs' difficulty directly processing complex graph-structured data, necessitating innovative input encoding and prompt design (Fatemi et al., 2023; Chen et al., 2023) for various graph tasks. Current research typically employs limited setups with small real-world graphs (Guo et al., 2023) or synthetic ones (Wang et al., 2023), exposing a gap in effectively incorporating large real-world graphs into LLMs, owing to their context window constraints. This suggests that text-only encoding may not be the optimal approach for complex, large graph structures.</p>
<p>This paper investigates the impact of different modalities for encoding global and local graph structures, focusing on node classification tasks,</p>
<table>
<thead>
<tr>
<th>Properties</th>
<th>CORA</th>
<th>Citeseer</th>
<th>Pubmed</th>
</tr>
</thead>
<tbody>
<tr>
<td>Classes</td>
<td>7</td>
<td>6</td>
<td>3</td>
</tr>
<tr>
<td>Nodes</td>
<td>2,708</td>
<td>3,327</td>
<td>19,717</td>
</tr>
<tr>
<td>Edges</td>
<td>5,278</td>
<td>4,552</td>
<td>44,324</td>
</tr>
<tr>
<td>Density</td>
<td>0.0014</td>
<td>0.0008</td>
<td>0.0002</td>
</tr>
<tr>
<td>Avg deg</td>
<td>3.89</td>
<td>2.74</td>
<td>4.49</td>
</tr>
<tr>
<td>Clust coeff</td>
<td>0.24</td>
<td>0.14</td>
<td>0.06</td>
</tr>
<tr>
<td>Diameter</td>
<td>$\infty$</td>
<td>$\infty$</td>
<td>18</td>
</tr>
<tr>
<td>Components</td>
<td>78</td>
<td>438</td>
<td>1</td>
</tr>
<tr>
<td>2-hop nodes</td>
<td>36</td>
<td>15</td>
<td>60</td>
</tr>
</tbody>
</table>
<p>and compares three modalities: Text, Motif, and Image (See Figure 1). Text modality offers detailed local insights but becomes verbose for large graphs (Bubeck et al., 2023), often exceeding the input limits of models like GPT-4. The Motif modality is suggested to address this, capturing essential patterns in a node's vicinity for a balanced localglobal perspective. Additionally, Image modality is proposed, utilizing fewer tokens to convey a more global view of the node's neighborhood, a method enhanced by the vision capabilities of the newly released GPT-4V (OpenAI, 2023a). Finding the optimal prompt input format is a notably complex challenge, with text modality encoding requiring extensive exploration compared to the simpler, more human-readable image modality. In our evaluations, we balance informativeness and prompt conciseness across all modalities using a combination of metrics.</p>
<p>Our main contributions are as follows:</p>
<ul>
<li>We conduct breadth-first analysis of various modalities, such as text, image, and motif, in graph-structure prompting, utilizing large language and vision-language models for node classification tasks.</li>
<li>We also perform a depth-first analysis of how different factors influence the performance of each encoding modality.</li>
<li>We introduce GRAPHTMI, a novel graph benchmark featuring a hierarchy of graphs, associated prompts, and encoding modalities designed to further the community's understanding of graph structure effects using LLMs.
Some key findings: 1) When balancing the constraint of token limits while preserving crucial information, the image modality is more effective than the text modality for graph-related tasks. 2) The choice of encoding modality for graph task classification depends on the task's difficulty, assessed by homophily and motif counts, with image modality being optimal for medium-difficulty tasks and motif modality for harder ones. 3) Factors like edge encoding function, graph structure, and graph sampling techniques impact the performance of node classification using text modality. 4) Motif attachment information has a more significant impact on node classification than motif count information. 5) Image representation correlated with human readability positively impacts node classification performance.</li>
</ul>
<p>Our research shows that while LLMs are pro-</p>
<p>Table 1: Comparison of network properties of popular citation network datasets CORA, Citeseer and Pubmed.
gressing in graph data processing, they still fall short compared to Graph Neural Networks in managing real-world graphs, emphasizing both the current limitations and the future potential of LLMs with image understanding in graph interpretation and reasoning tasks.</p>
<h2>2 Setups</h2>
<h3>2.1 Seed Datasets</h3>
<p>We experiment with three citation network datasets, which are popular node classification benchmarks, CORA (McCallum et al., 2000) with seven categories : [0-Rule Learning, 1-Neural Networks, 2-Case-Based, 3-Genetic Algorithms, 4-Theory, 5Reinforcement Learning, and 6-Probabilistic Methods], CITESEER (Giles et al., 1998) with six categories of areas in Computer Science: [0-Agents, 1-ML, 2-IR, 3-DB, 4-HCI, 5-AI] and PUBMED (Sen et al., 2008) that consists of scientific journals collected from the PubMed database with the following three categories: [0-Diabetes Mellitus, Experimental, 1-Diabetes Mellitus Type 1, 2-Diabetes Mellitus Type 2]. This paper focuses solely on the structural information of graphs for node classification. Hence, our experiments exclusively utilize node and label IDs.</p>
<h3>2.2 Evaluation Metrics</h3>
<p>This paper assesses the performance of node classification using four metrics chosen to balance the tradeoff between the encoding's informativeness and verbosity. The metrics used are Accuracy rate (which should increase $\uparrow$ ), Denial rate (which should decrease $\downarrow$ ), Mismatch rate (which should decrease $\downarrow$ ), indicating the prompt's informativeness, and Token limit fraction (which should de-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Node Classification on a Graph using different input modality encodings like Text, Motif, and Image.
crease $\downarrow$ ), reflecting the prompt's verbosity.
Accuracy Rate $A$ : This metric indicates the LLM's performance on the task of node classification.</p>
<p>$$
A=\frac{\text { No. of correct predictions }}{\text { Total no. of samples }}
$$</p>
<p>Mismatch Rate $M$ : This metric indicates the degree of misclassification by LLM (when the ground truth value is not the same as the predicted value).</p>
<p>$$
M=\frac{\text { No. of incorrect predictions }}{\text { Total no. of samples }}
$$</p>
<p>Denial Rate $D$ : When we craft our prompt, we instruct the LLM to return -1 if it cannot predict the label of the ? node (node to be classified). The denial rate metric describes the rate of failure of the LLM (when the predicted value is -1 ).</p>
<p>$$
\begin{gathered}
D=\frac{\text { No. of predictions }=-1}{\text { Total no. of samples }} \
1-A=M+D
\end{gathered}
$$</p>
<p>Token Limit Fraction $T$ : This metric evaluates how effectively a Large Language Model's encoding modality uses its input context window, specifically focusing on the constraints imposed by the fixed-size attention window in transformer-based models like GPT-4 and GPT-4V. These constraints,
dictated by the model's neural network architecture, limit the number of tokens that can be processed simultaneously, impacting both computational cost and performance.</p>
<p>$$
T=\frac{\text { Number of usage tokens }}{\text { Token limit constraint for the model }}
$$</p>
<h3>2.3 Graph Encoder Baselines</h3>
<p>We compare our LLM models, which use different encoding modalities, to traditional graph learning models GCN (Kipf and Welling, 2016), GRAPHSage(Hamilton et al., 2017) and GAT (Veličković et al., 2017) as a baseline for node classification tasks. We provide the training details for the GNN models in Appendix B.</p>
<h2>3 Proposed Encoders with Different Modalities</h2>
<p>Graph encoding is crucial for converting graphstructured data into a sequence format that language models can process. As shown in Figure 2, the experimental setup involves using a modality encoder to input the graph structure and a graph query, such as predicting a node's label in node classification tasks. The graph structure is encoded according to the chosen modality (text and motif using GPT-4 and image using GPT-4V) and then</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Image representation changes were applied sequentially on a graph, and we observed a distinct increase from (a) to (f) in human readability and understanding of the graph structure.</p>
<p>passed as a prompt to the LLMs to generate the required label.</p>
<h3>3.1 Text Encoder</h3>
<p>In encoding graphs as text, nodes are mapped to labels using a dictionary format, and different edge-encoding representations (Guo et al., 2023; Fatemi et al., 2023) are experimented with (Table 6), providing local context through edge connections and node labels to GPT-4 (OpenAI, 2023a). However, larger graphs can lead to verbose text encodings, which may exceed LLM input limits. We evaluate the impact of graph structure on classification (Yasir et al., 2023; Palowitch et al., 2022) by analyzing real-world citation datasets like PUBMED, CITESEER, and CORA, each with distinct network properties (definitions for these are provided in Table 8 and distinguished through Table 1). PUBMED is the largest and most connected but has the lowest clustering coefficient, indicating less local clustering. In contrast, CITESEER is highly fragmented with many disconnected components, while CORA, the smallest network, exhibits the highest density and clustering coefficient, suggesting strong local connectivity. Additionally, the research examines graph sampling techniques like ego graph (Stolz and Schlereth, 2021) and forest fire sampling (Leskovec and Faloutsos, 2006), crucial due to LLMs' limited context window and complex real-world graphs (Wei and Hu, 2022). These methods vary in their effectiveness, with Forest Fire sampling providing a broad network view, suitable for large networks like PUBMED, and Ego graph sampling excelling in revealing local community structures in more clustered and locally dense networks like CORA and CITESEER.</p>
<h3>3.2 Motif Encoder</h3>
<p>Network motifs, recurring patterns in social and biological networks (Milo et al., 2002; Carring-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Classifying graph task difficulty based on the criteria of Homophily and Number of Motifs yields a dataset of EASY, MEDIUM, and HARD graph problems and their associated modality encodings and classifications. This benchmark is called the GRAPHTMI dataset.</p>
<p>ton et al., 2005; Holland and Leinhardt, 1974), are pivotal in understanding local structures and behaviors. In LLMs, motif modality encoding leverages these motifs to provide local and global context, aiding in classifying unlabeled nodes (Yang et al., 2018). This process entails mapping nodes to labels using a dictionary format and identifying key motifs around the unlabeled node, which are inputted into GPT-4 as graph-motif information (detailed in Table 9). Differentiating between the count and specific members of motifs like stars, triangles, and cliques in a graph, our approach posits that a node's connection to influential motifs, such as being central in a star for network influence or part of a triangle or clique for close community ties, can significantly affect its classification by revealing key aspects of the network structure.</p>
<h3>3.3 Image Encoder</h3>
<p>Adopting the idea that "a picture is worth a thousand words", the image modality in graph analysis uses visual representations to outperform text in depicting structures, networks, labels, and spatial relationships using fewer tokens. Vision-language models like GPT-4V (OpenAI, 2023b) interpret these graph images, offering a global context of the graph's structure. GPT-4V, a multimodal model, merges visual interpretation with language processing, underscoring the importance of image representation in enhancing node classification. Our experiments involved using graph rendering methods to generate images with color-coded nodes, with a focus on improving human readability through various image modifications (Figure 3). These changes were evaluated for their impact on node classifica- tion, highlighting the critical role of visual representation in this modality.</p>
<h2>4 GraphTMI Benchmark Creation</h2>
<p>Our study reveals that the ease of node classification in graphs varies across different modalities, depending on the graph's "difficulty," determined by motif count and homophily. Homophily (McPherson et al., 2001), based on network theory, suggests that nodes are more likely to connect with similar nodes; thus, graphs with higher homophily (more nodes sharing the same label) are simpler to classify than those with more heterophily (diverse labels). This is illustrated through CORA dataset examples in Figure 4. Graphs are categorized as "easy," "medium," or "hard" based on the diversity of labels. Additionally, graphs with more network motifs are considered more complex and challenging for classification (Tu et al., 2018). The "task difficulty" is defined across eight categories (2<sup>3</sup> = 8), with the final difficulty level determined by the higher of the two criteria, homophily or motif count. This led to the creation of GRAPHTMI, a new benchmark dataset that includes various graph structures along with their respective modalities (text, motif, and image), prompts, and LLM classifications, thereby providing deeper insights into how different graphs affect LLM prompting. Specific statistics are given in Appendix A.</p>
<h2>5 Results</h2>
<h3>5.1 Results Across All Modalities</h3>
<p>Comparing Node Classification Accuracies between Graph baselines and LLM models: Table</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Cora</th>
<th style="text-align: center;">Citeseer</th>
<th style="text-align: center;">Pubmed</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GNN</td>
<td style="text-align: center;">GCN</td>
<td style="text-align: center;">$0.7584 \pm 0.121$</td>
<td style="text-align: center;">$0.6102 \pm 0.087$</td>
<td style="text-align: center;">$0.7546 \pm 0.076$</td>
</tr>
<tr>
<td style="text-align: center;">Baselines</td>
<td style="text-align: center;">GAT</td>
<td style="text-align: center;">$0.7989 \pm 0.092$</td>
<td style="text-align: center;">$0.6583 \pm 0.074$</td>
<td style="text-align: center;">$0.7490 \pm 0.060$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GraphSage</td>
<td style="text-align: center;">$0.7719 \pm 0.124$</td>
<td style="text-align: center;">$0.6017 \pm 0.103$</td>
<td style="text-align: center;">$0.7193 \pm 0.076$</td>
</tr>
<tr>
<td style="text-align: center;">LLMs +</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">$\mathbf{0 . 8 1} \pm 0.04[0.07 \pm 0.03]$</td>
<td style="text-align: center;">$\mathbf{0 . 7 5} \pm 0.05[0.07 \pm 0.01]$</td>
<td style="text-align: center;">$\mathbf{0 . 8 3} \pm 0.01[0.08 \pm 0.01]^{\prime}$</td>
</tr>
<tr>
<td style="text-align: center;">Encoding</td>
<td style="text-align: center;">Motif</td>
<td style="text-align: center;">$0.73 \pm 0.06[0.06 \pm 0.01]$</td>
<td style="text-align: center;">$0.59 \pm 0.01[0.32 \pm 0.02]$</td>
<td style="text-align: center;">$0.77 \pm 0.006[0.13 \pm 0.04]$</td>
</tr>
<tr>
<td style="text-align: center;">Modality</td>
<td style="text-align: center;">Image</td>
<td style="text-align: center;">$0.77 \pm 0.05[0.04 \pm 0.02]^{\prime}$</td>
<td style="text-align: center;">$0.71 \pm 0.09[0.06 \pm 0.0]^{\prime}$</td>
<td style="text-align: center;">$0.79 \pm 0.03[0.19 \pm 0.01]$</td>
</tr>
</tbody>
</table>
<p>Table 2: We report test accuracy rates of node classification across different datasets and denial rates $D$ in [brackets] for LLM models. ${ }^{\text {* }}$ indicates the lowest denial rate for each modality. The highest accuracy rate for the dataset is in bold, while the second highest is underlined. The text modality in LLMS is comparable to GNN baselines with image modality not far behind.</p>
<p>2 compares node classification accuracies of traditional GNN methods and LLM baselines across datasets, assessing if LLMs match conventional techniques. Limited by GPT-V's rate limit, the study used 50 ego graphs, with more extensive results in Appendix A. The text modality of LLMs performs comparably to graph baselines in all datasets, with the image modality close behind, indicating LLMs' potential in graph analysis. In larger datasets like Pubmed, the image modality showed a higher denial rate, possibly due to overcrowding in larger subgraphs, leading to more frequent classification denials by the LLM.</p>
<p>Comparing Node Classification Performance across Encoding Modalities: Figure 5 compares node classification across encoding modalities, focusing on accuracy, mismatch, denial rates, and token limit fraction. The text modality shows high accuracy but struggles with a high denial rate and token limit fraction, likely due to verbose inputs that confuse the LLM. In contrast, the image modality offers similar accuracy but with lower denial rates and token limit fractions, indicating the image modality's effectiveness in providing a concise, global context that the LLM processes more efficiently. This can be further emphasized through our qualitative analysis in Table 3.</p>
<p>Qualitative analysis of denial of classification in the Image Modality Figure 7 shows instances from multiple datasets where GPT-4V, using image modality, did not assign labels (returned -1 ) to graph nodes, explaining the reasons for denial. Key observations include: a) the LLM lacked explicit context on label assignments to nodes, as the encoding only implicitly indicated labels through node colors, with red reserved for unlabeled nodes. b) For one image, the absence of a clear link be-
tween node colors and labels, exacerbated by high heterophily, caused confusion. c) Another case highlighted the need for few-shot learning, suggesting that showing the LLM similar graph examples could help it learn to identify unlabeled (red) nodes more accurately.
Insights from GraphTMI In our evaluation of node classification accuracy using the GraphTMI benchmark across various modalities, we found in Figure 6 that "easy" tasks (characterized by high homophily and simpler structures) showed comparable accuracy across text, image, and motif modalities. However, for "medium" or "hard" tasks, marked by heterophyllous nature or complex structures, the image modality outperformed others, followed by the motif modality, underscoring the importance of global information in LLM processing. Notably, "hard" graphs achieved the highest accuracy with the motif modality, indicating the value of balancing local and global information. This suggests a growing effectiveness of image and motif modalities in enhancing graph reasoning tasks like node classification.</p>
<h3>5.2 Modality Specific Results</h3>
<p>Text modality results: Figure 16 shows that using the Adjacency List as the mode of edge representation with node label mapping is the most informative encoding function, which balances the trade-off between high accuracy and low token limit fraction. Figure 9 shows how metrics vary across datasets with different graph structures and sampling strategies. For CORA, a small, dense, and clustered graph, both sampling methods yield high accuracy, with forest fire (ff) sampling resulting in a lower denial rate. CITESEER, with its local clustering nature, struggles with ff sampling, showing the</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: We observe that while the text and image modalities have similar accuracy rates, the motif modality exhibits the highest mismatch rate, and the image modality stands out with the lowest denial rate and token limit fraction, as depicted along the mean metrics (y-axis) against each modality type (x-axis)
highest denial rate and the lowest mean accuracy, indicating difficulty in accurate predictions. In contrast, large and highly connected PubMed generates larger samples through ego graph sampling, leading to higher token limit fractions. Citeser's fragmented, disconnected nature results in smaller ego graph samples and lower token limit fractions. Thus, we can see graph structure and sampling strategy significantly impact performance metrics. Motif modality results: Figure 17 shows GPT-4's improved performance by adding the "triangle and star attached to ? node" motif in the motif modality encoder (detailed in Appendix Table 9). This enhancement in mean accuracy and other metrics is attributed to the effective combination of local and global context provided to the LLM through nodelabel mapping and the associations within triads or star motifs.
Image modality results: Figure 3 shows different tweaks to image representation, and Figure 8 demonstrates that optimal node classification correlates with high accuracy and low denial and mismatch rates. Interestingly, as human image readability increases, metric performance also improves, highlighting the easier use of images over text for LLM prompts.</p>
<h2>6 Related Work</h2>
<p>LLMs with Graphs: Graph Neural Networks (GNNs) are renowned for their effectiveness in node classification and link prediction (Dwivedi et al., 2020), with applications in diverse fields like social networks, computer vision, and biology (Hou et al., 2022). GNNs struggle with processing
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Modality encoder trends ( $\mathrm{T}=$ Text, $\mathrm{M}=$ Motif, I= Image) with graph task difficulty based on homophily and no. of motifs, highlight the significance of integrating local and global information in LLM processing.
non-numeric data like text and images, necessitating preprocessing such as feature engineering (Wang et al., 2021). In contrast, recent studies have explored using Large Language Models (LLMs) for graph reasoning, demonstrating their potential in complex tasks (Huang et al., 2022). This includes using LLMs for feature enhancement (Chen et al., 2023), node classification (Chen et al., 2023), and training neural networks in graph-based tasks (He et al., 2023), with benchmarks like NLGraph (Wang et al., 2023) assessing LLMs in traditional graph challenges. These studies typically employ LLMs as sub-components within graph learning frameworks. Our research examines LLMs' ability to process graph modalities directly, aiming to understand LLMs' intrinsic graph-handling capabilities, thus presenting a novel direction in the field. Prompt Design for Graphs: Prompting strategies for querying large language models (LLMs) aim to optimize the prompt text for enhanced task performance. Few-shot in-context learning (Brown et al., 2020) provides examples with desired outputs for the model to learn and generalize. Chain-of-thought (CoT) prompting (Wei et al., 2022) offers step-by-step problem-solving examples, leading the model to develop reasoning paths, while its zero-shot variant (Kojima et al., 2022) initiates reasoning with a starter phrase. Bag prompting (Wang et al., 2023) focuses on graph tasks, recommending graph construction before the task. Format explanations and role prompting (Guo et al., 2023) are proposed for better task clarity and strategic input organization to leverage LLMs' learning capabilities. Self-prompting involves the LLM refining</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" />
(a) (Ground $=0$ ) Without additional context or rules for how labels are assigned, it is not possible to accurately predict the label of the red node.
<img alt="img-7.jpeg" src="img-7.jpeg" />
(b) (Ground $=4$ ) The label cannot be determined with certainty due to the lack of a discernible pattern or rule that associates a node's color or its connections with its label.
<img alt="img-8.jpeg" src="img-8.jpeg" />
(c) (Ground $=2$ ) The label cannot be determined with certainty due to the lack of a clear pattern in the graph and no previous examples of red nodes to infer from.</p>
<p>Figure 7: Examples of graphs where VLM (GPT-4V) returned -1 or denied to predict a label and the reason for denial. The ground truth for this graph is given in brackets. This highlights the need to clarify labeling strategies and few shot learning.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 8: Our comparison of image representations (x-axis) with mean metrics (y-axis) shows that human readability of images correlates with classification performance, considering Accuracy Rate (A $\uparrow$ ), Mismatch Rate (M $\downarrow$ ), and Denial Rate (D $\downarrow$ ), with desired trends indicated in brackets.
prompts via context summarization, tackling issues with complex or insufficient graph data. Our study employs zero-shot prompting, providing only a task description to the LLM, to concentrate on the impact of modalities without the influence of varied prompt designs.</p>
<h2>7 Conclusion and Future Work</h2>
<p>This study explores the application of LLMs in graph-structured data, evaluating their strengths and weaknesses in node classification using various input modalities like motif and image for effective
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 9: Each dataset and sampling type (x-axis) is mapped against mean metrics (y-axis), with bar textures distinguishing between ego graph (ego) and forest fire (ff) sampling; the metrics include accuracy rate ( $\uparrow$ ), mismatch rate $(\downarrow)$, denial rate $(\downarrow)$, and token limit fraction $(\downarrow)$, indicating the desired trends for each. Graph structures of different datasets and sampling strategies influence node classification performance.
data representation. Introducing the GraphTMI benchmark highlights the image modality's efficiency in token limit management. Although LLMs have progressed in graph data processing, they still don't match the performance of GNNs in practical settings. The research advocates for future work combining different modalities to improve node classification, combining LLM-based methods with GNNs, applying these techniques to complex, text-dense graphs, and delving into link prediction and community detection to broaden applications and insights across multiple domains.</p>
<h2>Limitations</h2>
<p>Although innovative in applying LLMs to graphstructured data, this research faces key limitations. The computational demands of detecting network motifs, essential for understanding complex network dynamics, pose a significant challenge. This process requires extensive computational power and advanced algorithms, limiting scalability and efficiency. We subvert these challenges by restricting our subgraph sample size to 3 hops of an ego graph. Additionally, the study is constrained by the current rate limitations of GPT-V, which affects its ability to process and analyze data at scale efficiently. This limitation restricts the depth and scope of the analysis, although future advancements in GPT-V may mitigate this issue. A further limitation lies in the study's simplistic approach to estimating homophily, relying merely on label count and neglecting the importance of hop distance. This overlooks critical network structure and node similarity aspects, leading to a potentially oversimplified analysis. Incorporating hop distance could provide a more accurate representation of network homophily. These limitations underscore the need for further advancements in computational techniques, model capabilities, and more nuanced theoretical methods in network analysis.</p>
<h2>References</h2>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.</p>
<p>Peter J Carrington, John Scott, and Stanley Wasserman. 2005. Models and methods in social network analysis, volume 28. Cambridge university press.</p>
<p>Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, et al. 2023. Exploring the potential of large language models (llms) in learning on graphs. arXiv preprint arXiv:2307.03393.</p>
<p>Vijay Prakash Dwivedi, Chaitanya K Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier</p>
<p>Bresson. 2020. Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982.</p>
<p>John Ellson, Emden Gansner, Lefteris Koutsofios, Stephen C North, and Gordon Woodhull. 2002. Graphviz-open source graph drawing tools. In Graph Drawing: 9th International Symposium, GD 2001 Vienna, Austria, September 23-26, 2001 Revised Papers 9, pages 483-484. Springer.</p>
<p>Bahare Fatemi, Jonathan Halcrow, and Bryan Perozzi. 2023. Talk like a graph: Encoding graphs for large language models. arXiv preprint arXiv:2310.04560.</p>
<p>C Lee Giles, Kurt D Bollacker, and Steve Lawrence. 1998. Citeseer: An automatic citation indexing system. In Proceedings of the third ACM conference on Digital libraries, pages 89-98.</p>
<p>Jiayan Guo, Lun Du, and Hengyu Liu. 2023. Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. arXiv preprint arXiv:2305.15066.</p>
<p>Aric Hagberg, Pieter Swart, and Daniel S Chult. 2008. Exploring network structure, dynamics, and function using networkx. Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM (United States).</p>
<p>Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. Advances in neural information processing systems, 30.</p>
<p>Xiaoxin He, Xavier Bresson, Thomas Laurent, and Bryan Hooi. 2023. Explanations as features: Llmbased features for text-attributed graphs. arXiv preprint arXiv:2305.19523.</p>
<p>Paul W Holland and Samuel Leinhardt. 1974. The statistical analysis of local structure in social networks.</p>
<p>Yifan Hou, Jian Zhang, James Cheng, Kaili Ma, Richard TB Ma, Hongzhi Chen, and Ming-Chang Yang. 2022. Measuring and improving the use of graph information in graph neural networks. arXiv preprint arXiv:2206.13170.</p>
<p>Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pages 9118-9147. PMLR.</p>
<p>Thomas N Kipf and Max Welling. 2016. Semisupervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213.</p>
<p>Jure Leskovec and Christos Faloutsos. 2006. Sampling from large graphs. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 631-636.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459-9474.</p>
<p>Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. 2000. Automating the construction of internet portals with machine learning. Information Retrieval, 3:127-163.</p>
<p>Miller McPherson, Lynn Smith-Lovin, and James M Cook. 2001. Birds of a feather: Homophily in social networks. Annual review of sociology, 27(1):415444.</p>
<p>Prem Melville and Vikas Sindhwani. 2010. Recommender systems. Encyclopedia of machine learning, 1:829-838.</p>
<p>Ron Milo, Shai Shen-Orr, Shalev Itzkovitz, Nadav Kashtan, Dmitri Chklovskii, and Uri Alon. 2002. Network motifs: simple building blocks of complex networks. Science, 298(5594):824-827.</p>
<p>Alan Mislove, Massimiliano Marcon, Krishna P Gummadi, Peter Druschel, and Bobby Bhattacharjee. 2007. Measurement and analysis of online social networks. In Proceedings of the 7th ACM SIGCOMM conference on Internet measurement, pages 29-42.</p>
<p>OpenAI. 2023a. Gpt-4 system card. https://openai. com/research/gpt-4v-system-card.</p>
<p>OpenAI. 2023b. Gpt-4v(ision) system card. https://cdn.openai.com/papers/GPTV_ System_Card.pdf.</p>
<p>John Palowitch, Anton Tsitsulin, Brandon Mayer, and Bryan Perozzi. 2022. Graphworld: Fake graphs bring real insights for gnns. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 3691-3701.</p>
<p>Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. 2023. Unifying large language models and knowledge graphs: A roadmap. arXiv preprint arXiv:2306.08302.</p>
<p>Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. 2008. Collective classification in network data. AI magazine, 29(3):93-93.</p>
<p>Simon Stolz and Christian Schlereth. 2021. Predicting tie strength with ego network structures. Journal of Interactive Marketing, 54(1):40-52.</p>
<p>Kun Tu, Jian Li, Don Towsley, Dave Braines, and Liam D Turner. 2018. Network classification in temporal networks using motifs. arXiv preprint arXiv:1807.03733.</p>
<p>Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint arXiv:1710.10903.</p>
<p>Saraswathi Vishveshwara, KV Brinda, and Natarajan Kannan. 2002. Protein structure: insights from graph theory. Journal of Theoretical and Computational Chemistry, 1(01):187-211.</p>
<p>Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov. 2023. Can language models solve graph problems in natural language? arXiv preprint arXiv:2305.10037.</p>
<p>Yangkun Wang, Jiarui Jin, Weinan Zhang, Yong Yu, Zheng Zhang, and David Wipf. 2021. Bag of tricks for node classification with graph neural networks. arXiv preprint arXiv:2103.13355.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837.</p>
<p>Qiang Wei and Guangmin Hu. 2022. Evaluating graph neural networks under graph sampling scenarios. PeerJ Computer Science, 8:e901.</p>
<p>Carl Yang, Mengxiong Liu, Vincent W Zheng, and Jiawei Han. 2018. Node, motif and subgraph: Leveraging network functional blocks through structural convolution. in 2018 ieee. In ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), pages 47-52.</p>
<p>Mustafa Yasir, John Palowitch, Anton Tsitsulin, Long Tran-Thanh, and Bryan Perozzi. 2023. Examining the effects of degree distribution and homophily in graph learning models. arXiv preprint arXiv:2307.08881.</p>
<p>Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. 2023. A survey on multimodal large language models. arXiv preprint arXiv:2306.13549.</p>
<p>Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. 2023. Siren's song in the ai ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219.</p>
<h2>A LLM Experiments</h2>
<h2>A. 1 Qualitative Analysis on Combining Modalities</h2>
<p>We perform a qualitative analysis of the response returned by LLMs by utilizing the text, image,</p>
<table>
<thead>
<tr>
<th>Image</th>
<th>Ground Truth</th>
<th>Text_Response</th>
<th>Text + Image_Reponse</th>
<th>Image_Reponse</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Label of Node $=$ 3</td>
<td>The label of a node can often be predicted based on the labels of its neighboring nodes. In this case, the node in question is '1677', and its neighbors are '88', '415', '1309', and '191', '1908'. There is no clear majority label among these neighbors. Therefore, it is not possible to predict the label of node '1677' based on this information. Label of Node $=-1$</td>
<td>From the adjacency list, we can determine that the neighbors of node "1677". Given that node "1677" is connected to two nodes with label 0 , two nodes with label 5 , and one node with label 4 , we can attempt to predict its label based on the labels of its neighbors. Since nodes "1309" and "191" both have the label 5 and are each directly connected to "1677," it's reasonable to refer to the homophily in networks concept where nodes are likely to be of the same type (label) as their neighbors. Based on this information, it seems plausible that the red node "1677" would also have the label that appears most frequently among its neighboring nodes, which is label 5. Label of Node $=3$</td>
<td>Label of Node $=$ 0</td>
</tr>
<tr>
<td></td>
<td>Label of Node $=$ 3</td>
<td>The node 1329 is connected to nodes 733, 994, and 78. The labels of these nodes are 0,3 , and 4 respectively. Since there is no majority label among these, the label of node 1329 cannot be determined. Label of Node $=-1$</td>
<td>Label of Node $=3$</td>
<td>Label of Node $=$ 3</td>
</tr>
<tr>
<td></td>
<td>Label of Node $=$ 6</td>
<td>The node 702 is connected to nodes 777, 779, 2069, 2070, 2101, 822, 993, 1368, and 1637. The labels of these nodes are $0,6,0,0,5,5,6$, 5 , and 0 respectively. The most common label among these is 0 . Therefore, we can predict that the label of node 702 is also 0 . Label of Node $=$ 0</td>
<td>Label of Node $=6$</td>
<td>Label of Node $=$ 6</td>
</tr>
<tr>
<td></td>
<td>Label of Node $=$ 3</td>
<td>The node 141 is connected to nodes 1002, 2034, and 740. The labels of these nodes are 3, 4, and 4 respectively. Since the majority of the connected nodes have the label 4 , we can predict that the label of node 141 is also 4 . Label of Node $=4$</td>
<td>Label of Node $=3$</td>
<td>Label of Node $=$ 3</td>
</tr>
</tbody>
</table>
<p>Table 3: The table shows the GPT-4 and GPT4-V responses with the text modality, text+image modality, and image modality. We observe that on combining modalities, the label originally classified as -1 is correctly classified (first two rows), or the original misclassification is rectified (last two rows).
and text combined with image encoding modalities. The intuition here is that the local context provided by the text modality might not be enough for some predictions and could be supplemented through the global context provided by the image modality. Table 3 illustrates that misclassifications and denials by the LLM using text modality could be rectified by using the image modality. For the first two rows, -1 classifications or LLM denials are changed to the correct classification on incorporating the global context of the image modality. We can see in the response that the notion of "homophily" is clearer to the VLM in the image modality. For the last two rows, we see that the graph is originally misclassified, but then this is
corrected by incorporating the image modality. We make similar observations on combining text and motif modalities, and this could be because another factor important to node classification is the presence of motifs, which is highlighted through the motif modality.</p>
<h2>A. 2 Comparing encoding modalities for different datasets and sampling</h2>
<p>Our study evaluates various encoding modalities - text, motif, and image - with ego graphs from CORA detailed in the main manuscript. Figure 15 extends this analysis to the other datasets and sampling techniques. The findings corroborate our assertion that graph structure and the chosen sam-</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 10: Citeseer with ego graph sampling</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 11: Citeseer with forest fire sampling</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 12: Cora with forest fire sampling</p>
<p><img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 13: Pubmed with ego graph sampling</p>
<p><img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 14: Pubmed with forest fire sampling</p>
<p>Figure 15: Modality comparison (text, motif, and image) with the graph structure and sampling type shows the clear dependency of graph structure and sampling on node classification performance.</p>
<p>pling method significantly influence node classification outcomes. Particularly, samples derived from the forest fire method, which emphasize the global configuration while being sparser and less connected than ego graphs, exhibit increased misclassification rates when using the image modality due to limited information for accurate inference and greater instances of non-committal predictions with the motif modality due to the absence of a discernible overarching structure.</p>
<h3>A.3 Graph TMI Benchmark</h3>
<p>We decide on graph "difficulty" based on the dual criteria of 1) count of motifs and 2) homophily in the graph. We apply a naive heuristic to decide homophily, i.e., the count of the distinct labels in the graph. If the count of distinct labels &lt; 3, the graph is considered <em>easy</em>. If the count is ≥ 3 and &lt; 5, it is considered <em>medium</em>, and if the count is ≥ 5, it is considered <em>hard</em>. To decide the motif criteria, we count the total number of motifs (focusing on just triads, star motifs, and cliques) in a graph. For example, if this count of motifs ≤ 10 for the CORA dataset, the graph is considered <em>easy</em>. If the count is &gt; 10 and ≤ 20, it is regarded as <em>medium</em>; if the count is &gt; 20, it is considered <em>hard</em>. Some graphs can have both the homophily and motif criteria applicable to them; for instance, Figure 4 (b) can be classified as <em>medium</em> based on homophily, <em>hard</em> based on the count of motifs. This leads us to combine the homophily and the count of motif criteria to define the "task difficulty". Thus, we can have 2<sup>3</sup> = 8 categories of difficulty, and the final difficulty label is decided by choosing the</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">homophily</th>
<th style="text-align: center;">motif</th>
<th style="text-align: center;">count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">easy</td>
<td style="text-align: center;">easy</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">easy</td>
<td style="text-align: center;">hard</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">easy</td>
<td style="text-align: center;">medium</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">hard</td>
<td style="text-align: center;">easy</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">hard</td>
<td style="text-align: center;">hard</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">hard</td>
<td style="text-align: center;">medium</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">medium</td>
<td style="text-align: center;">easy</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">medium</td>
<td style="text-align: center;">hard</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">medium</td>
<td style="text-align: center;">medium</td>
<td style="text-align: center;">12</td>
</tr>
</tbody>
</table>
<p>Table 4: Statistics about the number of graphs classified as easy, medium, or hard through the homophily and the number of motifs criteria. All possible combinations are covered in our benchmark $\left(3^{2}=9\right)$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">difficulty</th>
<th style="text-align: center;">count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">easy</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">hard</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">medium</td>
<td style="text-align: center;">23</td>
</tr>
</tbody>
</table>
<p>Table 5: Statistics about the number of problems finally classified as easy, medium, or hard based on task difficulty, a function of homophily, and number of motifs.</p>
<h3>A. 4 Modality Specific Experiment Details</h3>
<p>Token limits for each modality Due to their architecture, transformer-based models like GPT-3 and GPT-4 have a fixed-size attention window. This determines how many tokens the model can "remember" or pay attention to at once. This limit also manages the computational cost of running the model and the model's performance. The token limit constraint for GPT-4 is 8192 tokens, while for GPT-4V(vision), the limit is claimed to be 128000 tokens, but currently, only the preview version has been released, and the actual limit is 10000 tokens. Rate limits for each LLM The rate limit for GPT-4 is 10 K RPM (requests per minute), and for GPT-4V, the rate limit is 100 RPD (requests per day).
Modality Experiment Parameters For all modality types, we sample 50 graphs for all datasets, the number of hops considered $=3$, no of runs $=2$, and perform ego graph and forest fire sampling. We report the mean and standard deviation directly or
through error bars in the visualization for all metrics. In the paper, we report the results from ego graph sampling because node classification typically needs a localized view around specific nodes, best provided by ego graph sampling.</p>
<h2>A.4.1 Text Modality</h2>
<p>Encoding graphs as text can be separated into two key parts: First, the mapping of nodes to their corresponding labels in the graph, and second, the encoding of edges between the nodes. We encode the node-to-label mapping as a dictionary of type {node ID: node label}. Finding a concise yet informative representation of the graph structure and edge representation is essential. Example of a prompt using text modality :</p>
<p>Task: Node Label Prediction (Predict the label of the node marked with a ?) given the adjacency list information as a dictionary of type "node: neighborhood" and nodelabel mapping in the text enclosed in triple backticks. The response should be in the format "Label of Node = <predicted label>". If the predicted label cannot be determined, return "Label of Node = -1".
AdjList: [1: [2,3], 2: [3,4], 3: [1,2]]
Node-Label Mapping: {1: A, 2: B, 3: ?} ${ }^{\cdots}$
Impact of Edge encoding function: Motivated by recent works (Fatemi et al., 2023; Guo et al., 2023) describing the importance of selecting the appropriate text encoding for a graph, we experiment with different edge representations (Appendix Table 6) on real-world datasets and evaluate the metrics for node classification and the results of this are illustrated in Figure 16. "Adjacency list" is the best-performing edge representation for the text modality.
Impact of Graph Structure: We selected diverse real-world citation datasets with unique network characteristics, as shown in Table 1. These network properties are defined in Table 8. The average number of nodes and edges in a 2-hop subgraph is also reported for CORA, Citeseer, and Pubmed datasets.
Impact of Sampling Strategy: Graph sampling techniques are essential for applying LLMs in graph reasoning, particularly due to the limited context window of LLMs and the intricacy of realworld graphs (Wei and Hu, 2022). Ego graph sampling centers on a specific node and its direct connections, forming a subgraph that mirrors these immediate relationships. In contrast, Forest Fire sampling randomly selects a node and expands from there, producing varying subgraphs in size</p>
<table>
<thead>
<tr>
<th>Edge Representation</th>
<th>Text Encoding</th>
<th>Description of Edge Representation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Edgelist</td>
<td>Node to Label Mapping : Node 69025: Label 34l Node 17585: Label 10l... Edge list: [(69025, 96211), (69025, 17585), (17585, 104598), (17585, 18844), (17585, 96211), (96211, 34515)]</td>
<td>An Edgelist is a graph data structure that represents a graph by listing the edge connections between two nodes. (A, B) indicates an connection between nodes A and B.</td>
</tr>
<tr>
<td>Edgetext</td>
<td>Node to Label Mapping : Node 85328: Label 16t Node 158122: Label ?l... Edge connections (source node - target node): Node 85328 is connected to Node 158122. Node 158122 is connected to Node 167226.</td>
<td>An Edgetext explicitly lists the connections between two nodes; for example, Node A is connected to Node B or Node A - Node B</td>
</tr>
<tr>
<td>Adjacency List</td>
<td>Node to Label Mapping : Node 2339: Label 3l Node 2340: Label ?l... Adjacency list: 1558: [2339, 2340], 2339: [1558, 2340], 2340: [2339, 1558]</td>
<td>An adjacency list represents a graph as an array of linked lists. The index of the array represents a vertex, and each element in its linked list represents the other vertices that form an edge with the vertex. For example, A: [B, C] shows that A is connected to B and C. This gives an idea of node-neighborhood</td>
</tr>
<tr>
<td>GML</td>
<td>GraphML: graph [ node [ id 2339 label 3 ] node [ id 2340 label ? ] node [ id 1558 label 3 ] edge [ source 2339 target 1558 ] edge [ source 2339 target 2340 ] ]</td>
<td>A GraphML format consists of an unordered sequence of node and edge elements enclosed within []. Each node element has a distinct id and label attribute contained within []. Each edge element has source and target attributes contained within [] that identify the endpoints of an edge by having the same value as the node id attributes of those endpoints. The node label information is embedded within the structure, meaning no node-label mapping is notneeded.</td>
</tr>
<tr>
<td>GraphML</td>
<td>GraphML: <graphml xmlns=http://graphml.graphdrawing.org/xmlns xmlns:xsi=http://www.w3.org/2001/XMLSchema-instance xsi:schemaLocation=http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd> <graph edgedefault=undirected> <node id=2339 label=3 /> <node id=2340 label=? /> <node id=1558 label=3 /> <edge source=2339 target=1558 /> <edge source=2339 target=2340 /> </graph> </graphml></td>
<td>A GraphML file consists of an XML file containing a graph element, within which is an unordered sequence of node and edge elements. Each node element should have a distinct id attribute as well as its label, and each edge element has source and target attributes that identify the endpoints of an edge by having the same value as the id attributes of those endpoints. The node label information is embedded within the structure meaning no node-label mapping is needed.</td>
</tr>
</tbody>
</table>
<p>Table 6: Summary of edge representation passed as a part of the text modality encoder with their associated examples and explanations. We find that the Adjacency list representation provides a granular yet not too verbose view of the graph being passed to the LLM.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">CORA</th>
<th style="text-align: left;">Citeseer</th>
<th style="text-align: left;">Pub.med</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Avg edges 2-hop</td>
<td style="text-align: left;">$62.70 \pm 94.77$</td>
<td style="text-align: left;">$26.35 \pm 61.70$</td>
<td style="text-align: left;">$129.36 \pm 287.61$</td>
</tr>
<tr>
<td style="text-align: left;">Avg nodes 2-hop</td>
<td style="text-align: left;">$36.78 \pm 48.12$</td>
<td style="text-align: left;">$15.11 \pm 24.73$</td>
<td style="text-align: left;">$60.05 \pm 85.12$</td>
</tr>
</tbody>
</table>
<p>Table 7: Subgraph Sampling Statistics about average number of nodes and edges in a 2-hop subgraph from each dataset.
and structure, influenced by factors like 'burning' probabilities. However, both methods have limitations and can potentially distort the overall structure of complex and extensive networks.</p>
<h2>A.4.2 Motif Modality</h2>
<p>Encoding graphs as motifs can be separated into two key parts: First, the encoding of nodes to their corresponding labels in the graph, and second, the
motifs present around the ? (unlabeled) node. We encode the node-to-label mapping as a dictionary of type ${$ node ID: node label $}$. We calculate motifs in the neighborhood of the ? nodes and pass this information to GPT-4 (OpenAI, 2023a) as the graph-motif information. Connections of an unlabeled node to significant nodes or groups (like stars or cliques) are more indicative of its label than just the count of graph motifs, with central nodes in star motifs or members of cliques heavily influenced by their neighbors' labels. We experiment with different network motifs as input to the modality encoder. Table 9 describes the different types of motifs considered, a description of the motif, and an example of the encoding generated as input to GPT-4. An example prompt generated after applying the motif</p>
<p>Table 8: Graph Properties and Their Descriptions</p>
<table>
<thead>
<tr>
<th>Name of Property</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Density</td>
<td>Measures how connected the graph is. It's the ratio of actual edges to possible edges.</td>
</tr>
<tr>
<td>Degree Distribution</td>
<td>The distribution of node degrees. The histogram might follow a specific pattern (e.g.,</td>
</tr>
<tr>
<td></td>
<td>power-law distribution, Gaussian distribution).</td>
</tr>
<tr>
<td>Average Degree</td>
<td>The average degree of nodes in the graph.</td>
</tr>
<tr>
<td>Connected Components</td>
<td>A subgraph in which a path connects any two nodes.</td>
</tr>
<tr>
<td>Clustering Coefficient</td>
<td>Measures the degree to which nodes tend to cluster together.</td>
</tr>
<tr>
<td>Graph Diameter</td>
<td>The longest shortest path between any two nodes. It provides insight into the graph's</td>
</tr>
<tr>
<td></td>
<td>overall size.</td>
</tr>
<tr>
<td>2hop nodes</td>
<td>Average number of nodes present in the subgraph at 2 hop distance from any node</td>
</tr>
</tbody>
</table>
<p><img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Figure 16: We compare the edge representation type (x-axis) with the value of the mean metrics (y-axis). The desired trend is given in brackets for each metric. The highest performing edge representation is the "adjacency list" representation with the highest accuracy (A ↑) and low mismatch rate (M ↓)), denial rate (D ↓), and token limit fraction (T ↓).</p>
<p><img alt="img-17.jpeg" src="img-17.jpeg" /></p>
<p>Figure 17: We compare the motif information (x-axis) to the mean metrics (y-axis). Desired trends are denoted in brackets. Metrics considered are Accuracy Rate (A ↑), Mismatch Rate (M ↓), and Denial Rate (D ↓). The highest performing motif information change "triangle and star attached to ?" has higher accuracy and lower mismatch and denial rate.</p>
<p>encoding modality looks like:</p>
<p><strong>Task</strong>: Node Label Prediction (Predict the label of the node marked with a ?) given the node-label mapping and graph motif information in the text enclosed in triple backticks. The response should be in the format "Label of Node = <predicted label>". If the predicted label cannot be determined, return "Label of Node = -1".</p>
<p><strong>Node-Label Mapping</strong>: {1: A, 2: A, 3: ?}</p>
<p><strong>Graph-motif information</strong>: No of triangles: 11 Triangles attached to ? Node : [1,2,3]1 ' ' '</p>
<h3>A.4.3 Image Modality</h3>
<p>We use GPT-4V (OpenAI, 2023b) to process graph images to give LLMs a global perspective of graph structural information. An example prompt generated after applying the image encoding modality is shown below.</p>
<p><strong>Task</strong>: Node Label Prediction (Predict the label of the red node marked with a ?, given the graph structure information in the image). The response should be in the format "Label of Node = <predicted label>." If the predicted label cannot be determined, return "Label of Node = -1."</p>
<p><img alt="img-18.jpeg" src="img-18.jpeg" /></p>
<table>
<thead>
<tr>
<th>Details</th>
<th>GCN</th>
<th>GAT</th>
<th>GraphSAGE</th>
</tr>
</thead>
<tbody>
<tr>
<td>Epochs</td>
<td>100</td>
<td>100</td>
<td>100</td>
</tr>
<tr>
<td>Learning Rate</td>
<td>0.005</td>
<td>0.005</td>
<td>0.005</td>
</tr>
<tr>
<td>Weight Decay</td>
<td>5e-4</td>
<td>5e-4</td>
<td>5e-4</td>
</tr>
</tbody>
</table>
<p>Table 11: List of GNN training hyperparameters</p>
<h3>B GNN Experiments</h3>
<p>For training our GNN models, we used the best-found hyperparameters reported in Table 11. The</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Type of Motif</th>
<th style="text-align: left;">Motif Encoding</th>
<th style="text-align: left;">Description of Motif</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Node-Label Mapping</td>
<td style="text-align: left;">Node to Label Mapping : Node 1889: Label 4</td>
<td style="text-align: left;">... Node 1893: <br> Label ?\%.</td>
</tr>
<tr>
<td style="text-align: left;">No. of Star Motifs</td>
<td style="text-align: left;">Node to Label Mapping : Node 1889: Label 4</td>
<td style="text-align: left;">... Node 1893: <br> Label ?|... Graph motif information: Number of star motifs: 0|</td>
</tr>
<tr>
<td style="text-align: left;">No. of Triangle Motifs</td>
<td style="text-align: left;">Node to Label Mapping : Node 1889: Label 4</td>
<td style="text-align: left;">... Node 1893: <br> Label ?|... Graph motif information: Number of triangle motifs: <br> 6|</td>
</tr>
<tr>
<td style="text-align: left;">No. of Triangle Motifs <br> Attached</td>
<td style="text-align: left;">Node to Label Mapping : Node 1889: Label 4</td>
<td style="text-align: left;">... Node 1893: <br> Label ?|... Graph motif information: Triangle motifs attached to ? <br> node: [1893,2034,1531], [1893,1531,429]]</td>
</tr>
<tr>
<td style="text-align: left;">No. of Star Motifs At- <br> tached</td>
<td style="text-align: left;">Node to Label Mapping : Node 1889: Label 4</td>
<td style="text-align: left;">... Node 1893: <br> Label ?|... Graph motif information: Star motifs connected to ? <br> node: []</td>
</tr>
<tr>
<td style="text-align: left;">No. of Star and Triangle <br> Motifs</td>
<td style="text-align: left;">Node to Label Mapping : Node 1889: Label 4</td>
<td style="text-align: left;">... Node 1893: <br> Label ?|... Graph motif information: Number of star motifs: 0| <br> Number of triangle motifs: 6|</td>
</tr>
<tr>
<td style="text-align: left;">Star and Triangle Motifs <br> attached</td>
<td style="text-align: left;">Node to Label Mapping : Node 1889: Label 4</td>
<td style="text-align: left;">... Node 1893: <br> Label ?|... Graph motif information: Triangle motifs attached to ? <br> node: [1893,2034,1531], [1893,1531,429]] Star motifs connected <br> to ? node: []</td>
</tr>
<tr>
<td style="text-align: left;">No of cliques ? Node is <br> part of</td>
<td style="text-align: left;">Node to Label Mapping : Node 1889: Label 4</td>
<td style="text-align: left;">... Node 1893: <br> Label ?|... Graph motif information: Number of cliques in graph: <br> 0| ? Node is a part of these cliques: []</td>
</tr>
<tr>
<td style="text-align: left;">No of cliques ? Node is <br> attached to</td>
<td style="text-align: left;">Node to Label Mapping : Node 1889: Label 4</td>
<td style="text-align: left;">... Node 1893: <br> Label ?|...Graph motif information: ? Node is attached to these <br> cliques: []</td>
</tr>
</tbody>
</table>
<p>Table 9: Summary of motif information passed as a part of the motif modality encoder with their associated examples and explanations. The Aggregate of all changes setup combines all of the above motif information to give the LLM a local and global view of the graph being passed.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Cora</th>
<th style="text-align: center;">Citeseer</th>
<th style="text-align: center;">Pubmed</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GNN <br> Baselines</td>
<td style="text-align: left;">GCN</td>
<td style="text-align: center;">$0.7820 \pm 0.133$</td>
<td style="text-align: center;">$0.6540 \pm 0.083$</td>
<td style="text-align: center;">$0.7480 \pm 0.077$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">GAT</td>
<td style="text-align: center;">$\mathbf{0 . 8 2 0 0} \pm 0.084$</td>
<td style="text-align: center;">$0.6680 \pm 0.069$</td>
<td style="text-align: center;">$0.7510 \pm 0.050$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">GraphSage</td>
<td style="text-align: center;">$0.7570 \pm 0.137$</td>
<td style="text-align: center;">$0.6300 \pm 0.098$</td>
<td style="text-align: center;">$0.7430 \pm 0.078$</td>
</tr>
<tr>
<td style="text-align: left;">LLMs +</td>
<td style="text-align: left;">Text</td>
<td style="text-align: center;">$0.81 \pm 0.04[0.07 \pm 0.03]$</td>
<td style="text-align: center;">$\mathbf{0 . 7 5} \pm 0.05[0.07 \pm 0.01]$</td>
<td style="text-align: center;">$\mathbf{0 . 8 3} \pm 0.01[0.08 \pm 0.01]$</td>
</tr>
<tr>
<td style="text-align: left;">Encoding</td>
<td style="text-align: left;">Motif</td>
<td style="text-align: center;">$0.73 \pm 0.06[0.06 \pm 0.01]$</td>
<td style="text-align: center;">$0.59 \pm 0.01[0.32 \pm 0.02]$</td>
<td style="text-align: center;">$0.77 \pm 0.006[0.13 \pm 0.04]$</td>
</tr>
<tr>
<td style="text-align: left;">Modality</td>
<td style="text-align: left;">Image</td>
<td style="text-align: center;">$0.77 \pm 0.05[0.04 \pm 0.02]$</td>
<td style="text-align: center;">$0.71 \pm 0.09[0.06 \pm 0.0]$</td>
<td style="text-align: center;">$0.79 \pm 0.03[0.19 \pm 0.01]$</td>
</tr>
</tbody>
</table>
<p>Table 10: Test accuracy rates of node classification across different datasets using the entire 1000 test data and denial rates $D$ in [brackets] for LLM models. For LLMs, we chose a test sample of 50 graphs. * indicates the lowest denial rate for each modality. The highest accuracy rate for the dataset is in bold, while the second highest is underlined.
training and test set details are given in Table 12. In the main paper, we report the GNN test accuracy on 50 samples due to API constraints by GPT-4, which do not allow us to conduct the apple-to-apple comparison with 1000 test samples. However, Table 10 reports the test accuracy for GNN reported on 1000 samples.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Cora</th>
<th style="text-align: center;">Citeseer</th>
<th style="text-align: center;">Pubmed</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Training Set</td>
<td style="text-align: center;">140</td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">60</td>
</tr>
<tr>
<td style="text-align: left;">Testing Set</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1000</td>
</tr>
<tr>
<td style="text-align: left;">GCN Params</td>
<td style="text-align: center;">23063</td>
<td style="text-align: center;">59366</td>
<td style="text-align: center;">8067</td>
</tr>
<tr>
<td style="text-align: left;">GAT Params</td>
<td style="text-align: center;">92373</td>
<td style="text-align: center;">237586</td>
<td style="text-align: center;">32393</td>
</tr>
<tr>
<td style="text-align: left;">GraphSage Params</td>
<td style="text-align: center;">46103</td>
<td style="text-align: center;">118710</td>
<td style="text-align: center;">16115</td>
</tr>
</tbody>
</table>
<p>Table 12: GNN Train-Test split and Parameters</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1414 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1414</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1414</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-ece57d6dcb7ee3e88d2ee94328badb5b41098744</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ece57d6dcb7ee3e88d2ee94328badb5b41098744" target="_blank">Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> Vista is presented, a generalizable driving world model with high fidelity and versatile controllability, and for the first time, the capacity of Vista itself is utilized to establish a generalizable reward for real-world action evaluation without accessing the ground truth actions.</p>
                <p><strong>Paper Abstract:</strong> World models can foresee the outcomes of different actions, which is of paramount importance for autonomous driving. Nevertheless, existing driving world models still have limitations in generalization to unseen environments, prediction fidelity of critical details, and action controllability for flexible application. In this paper, we present Vista, a generalizable driving world model with high fidelity and versatile controllability. Based on a systematic diagnosis of existing methods, we introduce several key ingredients to address these limitations. To accurately predict real-world dynamics at high resolution, we propose two novel losses to promote the learning of moving instances and structural information. We also devise an effective latent replacement approach to inject historical frames as priors for coherent long-horizon rollouts. For action controllability, we incorporate a versatile set of controls from high-level intentions (command, goal point) to low-level maneuvers (trajectory, angle, and speed) through an efficient learning strategy. After large-scale training, the capabilities of Vista can seamlessly generalize to different scenarios. Extensive experiments on multiple datasets show that Vista outperforms the most advanced general-purpose video generator in over 70% of comparisons and surpasses the best-performing driving world model by 55% in FID and 27% in FVD. Moreover, for the first time, we utilize the capacity of Vista itself to establish a generalizable reward for real-world action evaluation without accessing the ground truth actions.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1414.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1414.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vista</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A latent-video-diffusion-based driving world model (initialized from Stable Video Diffusion) that predicts high-resolution, high-frame-rate front-view driving futures, incorporates dynamic priors via latent replacement, two auxiliary losses to improve dynamics and structure, and a unified multi-modal action-conditioning interface with LoRA adapters for efficient learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vista</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent video diffusion model built on Stable Video Diffusion (SVD) with a UNet denoiser; inputs are latent video tensors where up to three condition frames are injected by latent replacement (to provide position, velocity, acceleration priors). Training objective = diffusion loss + dynamics-enhancement loss (re-weighted adjacent-frame motion residuals) + structure-preservation loss (high-frequency Fourier-domain supervision). Actions (angle/speed, trajectory, command, goal point) are encoded via Fourier embeddings and injected through cross-attention layers; LoRA adapters added to attention layers during action-control learning to limit parameter updates.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (diffusion-based visual world model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>autonomous driving (front-view visual prediction, action evaluation/control)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Generative perceptual & temporal metrics: FID (visual fidelity), FVD (video-fidelity/temporal coherence); also trajectory-difference (L2 over inferred/estimated trajectory via an inverse dynamics model) and human evaluation on visual quality and motion rationality.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>On nuScenes validation: FID = 6.9, FVD = 89.4. Human evaluators preferred Vista over strong baselines in >70% of pairwise comparisons. Compared to prior driving world models, Vista lowers FID by ~55% and FVD by ~27% relative to the best prior in the paper's comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Primarily a black-box neural generative model; limited interpretability beyond controllability inputs and uncertainty outputs. Interpretability is operationalized via controllable conditioning (multi-modal actions) and uncertainty-based reward (conditional variance). No claim of explicit human-interpretable latent factors.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Indirect/operational methods: (1) controlled counterfactual generation via explicit action inputs to observe causal effects; (2) inverse-dynamics-model (IDM) to read out trajectories from generated video for quantitative assessment; (3) conditional-variance as an uncertainty proxy (used as reward). No explicit latent-space disentanglement or symbolic extraction reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Large: model framework uses ~2.5B parameters (including 1.6B UNet parameters). Training pipeline is two-phase (low-res majority of iterations at 320×576 for 3.5× throughput, then finetune at 576×1024). Sampling/generation uses iterative diffusion sampling (default full-generation ~50 denoising steps referenced); reward estimation uses multi-round denoising (example: 10 steps × 5 rounds = 50 steps) and authors note reward estimation cost per situation is no greater than full generation in alternative detector-based pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Two-stage training (low-res then high-res) yields ~3.5× higher training throughput during the main action-control learning stage; LoRA adapters enable parameter-efficient adaptation of attention layers without full UNet finetuning. Authors note reward estimation can be done with fewer denoising steps and is comparable in compute to detector-based reward pipelines, but overall the model is computationally expensive compared to lower-resolution/low-rate alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>High: FID=6.9 and FVD=89.4 on nuScenes validation outperforming listed prior driving world models (see comparisons). Action-conditioning reduces average trajectory-difference (L2, 2s) substantially: e.g., on nuScenes with 3 dynamic priors average trajectory difference for action-free = 1.820, for 'angle & speed' = 0.832, for 'trajectory' = 0.835 (Table 3). Reward function correlates negatively with L2 trajectory perturbation when evaluated zero-shot on Waymo.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High-fidelity visual prediction plus multi-modal controllability enables (1) improved open-loop evaluation of candidate actions via conditional generation and conditional-variance reward, (2) use as a forward model for model-predictive control / action ranking, and (3) conversion of generated video to executable actions via an inverse-dynamics model. Authors demonstrate the model's predictions better match downstream trajectory estimates and human judgments, indicating fidelity translates to practical utility for action evaluation and counterfactual reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Key trade-offs discussed: (1) fidelity vs compute — high spatiotemporal resolution yields significantly higher compute and latency; (2) perceptual quality vs motion intensity — need to balance smoothing and realistic motion, motivating the dynamics and structure losses; (3) adaptability vs stability — finetuning UNet risks degrading pretrained prediction, solved via LoRA; (4) action-generalization vs training cost — they train action modes independently per sample (action-independence constraint) to avoid combinatorial training costs.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Latent-diffusion backbone (SVD) for high-quality video; latent replacement of condition frames (up to 3) to inject position/velocity/acceleration priors; dynamics-enhancement loss that reweights supervision on dynamic regions computed from adjacent-frame residuals; structure-preservation loss via high-pass Fourier supervision; Fourier embeddings for heterogeneous action sequences; cross-attention conditioning; LoRA adapters for parameter-efficient action-control learning; two-stage training (low-res bulk + high-res finetune); collaborative dataset training (OpenDV-YouTube for generalization + nuScenes for action annotations); action-independence during training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Quantitative comparisons: Table 2 shows Vista (FID 6.9, FVD 89.4) outperforms DriveGAN (FID 73.4, FVD 502.3), DriveDreamer (52.6 / 452.0), WoVoGen (27.6 / 417.7), Drive-WM (15.8 / 122.7), and GenAD (15.4 / 184.0). Table 1 shows Vista operates at higher data scale (1740h), higher frame rate (10 Hz) and higher resolution (576×1024) and supports more action modes (angle&speed, trajectory, command, goal point) compared to many prior world models. Computationally, Vista is larger and more costly than many of these lower-resolution, lower-frame-rate baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Recommendations and insights in paper: (1) condition on at least three consecutive frames to capture position, velocity, and acceleration priors for coherent rollouts; (2) augment diffusion loss with dynamics-aware reweighting and high-frequency structure supervision for high-resolution driving prediction; (3) use Fourier-embedded action conditioning via cross-attention and LoRA adapters for efficient controllability learning; (4) two-phase training (low-res then short high-res finetune) to maximize iterations while maintaining high-res performance; (5) enforce action-independence during training to avoid combinatorial explosion and to make action modes learnable individually; (6) use conditional-variance (multi-sample denoising) as a generalizable reward signal.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1414.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1414.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Drive-WM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Drive-WM (Driving World Model from prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously proposed driving world model referenced as a baseline; used detector-based reward in prior work and models driving futures at relatively low frame rate and resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Drive-WM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A visual world model for autonomous driving (prior work) that was evaluated at lower temporal/spatial resolutions; in prior literature Drive-WM used perception detectors to form reward signals for actions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>visual world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>autonomous driving</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reported FID and FVD in comparison table (FID, FVD). Also used detector outputs for reward evaluation in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Table 2 (as reported in this paper): FID = 15.8, FVD = 122.7 on nuScenes validation (comparison baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed in this paper beyond the use of detector-based reward (which relies on interpretable detector outputs), so internal model interpretability is not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>N/A (no internal interpretability methods described in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not specified in this paper (only reported resolution/frame-rate/data regime: 5h data, 2 Hz, 192×384 per Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Not directly compared for compute; the paper contrasts Vista's reward estimator (conditional variance) as an alternative to Drive-WM's detector-based reward and argues comparable or lower compute for reward estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>As above: FID 15.8, FVD 122.7 on nuScenes val (worse than Vista in paper comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>In Drive-WM, reward estimation relied on external detectors trained on a specific dataset; the present paper notes that such detector-based rewards may not generalize across domains—motivation for Vista's uncertainty-based reward.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Detector-based reward can give interpretable, structured signals but may reduce generalization to novel domains; computational pipeline requires full video generation before perception-based reward, per authors' discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Not detailed in this paper beyond being a baseline world model and using detector-based reward in its original formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Drive-WM's FID/FVD are intermediate among prior methods (worse than Vista). The paper positions Vista's reward as more generalizable than Drive-WM's detector-based reward because Vista's uncertainty is model-native and does not require external detectors.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1414.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1414.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GenAD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GenAD (Generative End-to-End Autonomous Driving)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior generative driving model used as a comparison baseline that supports trajectory and command control modes at lower temporal/spatial resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GenAD</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior driving generative model that conditions on action modalities such as trajectory and goal/command; operates at lower frame rate and spatial resolution (per Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>visual generative world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>autonomous driving</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>FID and FVD used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Table 2 reports GenAD: FID = 15.4, FVD = 184.0 on nuScenes validation.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not described in this paper. The paper notes GenAD's trajectory-conditioned extension was finetuned on nuScenes and its generalization was not validated.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not mentioned.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not specified here; Table 1 shows GenAD trained on 2000h at 2 Hz and resolution 256×448 (indicates larger data scale but lower spatiotemporal fidelity than Vista).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Compared to Vista, GenAD runs at much lower frame rate (2 Hz) and lower resolution; Vista reports better FID/FVD with lower model complexity per the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>As above (FID 15.4 / FVD 184.0), worse than Vista on the reported metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Authors argue GenAD's action control generalization is limited since GenAD finetuned on nuScenes; Vista aims for zero-shot action controllability across datasets via collaborative training.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>GenAD opts for larger-scale trajectory conditioning at lower spatiotemporal resolution, trading visual fidelity for other design choices.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Trajectory and command conditioning (finetuned per-dataset in original work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Vista achieves substantially better FID/FVD and higher spatiotemporal resolution while maintaining multi-modal controls that generalize zero-shot, per authors' comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1414.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1414.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GAIA-1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GAIA-1: A Generative World Model for Autonomous Driving</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent large-scale driving world model baseline reported to operate at high frame-rate and moderate resolution with angle&speed controls.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GAIA-1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A generative driving world model (prior work) that reportedly trained on large-scale data and supports angle & speed control; Table 1 lists a very large data scale and high frame rate.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>visual world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>autonomous driving</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not quantified in this paper beyond being listed in Table 1; FID/FVD not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Table 1 indicates GAIA-1 trained on ~4700h and operates at 25 Hz, resolution 288×512, and supports angle & speed control. No direct FID/FVD numbers reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not mentioned.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Large data scale (4700h) and high frame rate imply high training compute, but no explicit compute numbers are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Vista emphasizes higher resolution (576×1024) though at lower frame rate (10 Hz) and smaller training data (1740h) compared to GAIA-1's 4700h at 25 Hz; direct compute comparison not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>GAIA-1 is a high-data, high-frame-rate baseline; the present paper positions Vista as providing higher spatial resolution and multi-modal action control with strong generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Higher frame-rate training (25 Hz) and larger dataset likely implies higher compute; Vista trades frame rate for much higher spatial resolution and explicit controllability design.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Not detailed in this paper beyond table entry.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Listed as part of Table 1; Vista differs by prioritizing spatial resolution and multi-modal action support.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1414.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1414.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DriveGAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DriveGAN</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior controllable generative driving simulator model used as a baseline; reported at moderate resolution and frame rate.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DriveGAN</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A generative driving model (GAN-based in original work) used to synthesize driving scenes with some controllability (angle & speed listed in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>visual generative model (GAN)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>autonomous driving</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>FID and FVD reported in paper's comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Table 2 reports DriveGAN: FID = 73.4, FVD = 502.3 on nuScenes validation (baseline values reported in this paper). Table 1 indicates 160h data, 8 Hz, 256×256 resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not mentioned.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not detailed here beyond dataset scale and resolution listed in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Vista substantially outperforms DriveGAN on FID/FVD; DriveGAN lower resolution and higher FID/FVD.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>As above (FID 73.4 / FVD 502.3).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>DriveGAN provides controllable visual simulation but with lower fidelity compared to modern diffusion-based approaches per reported metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Earlier GAN-based approaches may be faster to sample but generated lower-fidelity or less temporally-coherent outputs compared to diffusion-based models in the comparisons shown.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>GAN-based video synthesis with angle&speed conditioning (per table entry).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperformed by Vista and other modern diffusion-based models in FID/FVD per Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1414.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1414.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DriveDreamer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DriveDreamer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior driving world model (Dreamer-style/latent dynamics approach) used as a baseline with low training data and low-to-moderate fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DriveDreamer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A prior world-model approach for driving that follows Dreamer-style latent dynamics; reported at low data scale and moderate frame-rate/resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent dynamics world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>autonomous driving</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>FID and FVD used in comparative table.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Table 2 reports DriveDreamer: FID = 52.6, FVD = 452.0 on nuScenes validation. Table 1 lists data scale 5h, 12 Hz, resolution 128×192.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not mentioned.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not detailed here (small dataset size listed suggests lower training compute but no specifics).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Vista outperforms DriveDreamer by large margins in FID/FVD per Table 2 and operates at higher resolution and sustained generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>As above (FID 52.6 / FVD 452.0).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Prior Dreamer-style latent dynamics approaches demonstrate sample-efficiency in control tasks but, per the paper, may lack high-resolution visual fidelity needed for detailed driving scene prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Latency and low-resolution dynamics modeling vs. high-fidelity image/video prediction trade-offs; paper argues diffusion-based latent video models (Vista) better preserve visual fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Dreamer-style latent prediction (per original work), angle&speed control listed in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Vista yields much lower FID/FVD and higher spatial resolution in this paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1414.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1414.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WoVoGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WoVoGen (World Volume-Aware Diffusion for Controllable Multi-Camera Driving Scene Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior diffusion-based driving video generator that supports angle&speed conditioning; included as a baseline with moderate fidelity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>WoVoGen</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Diffusion-based multi-camera driving scene generator (prior work) which models world-volume awareness; listed as a baseline in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>diffusion-based visual generator / world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>autonomous driving / multi-camera scene generation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>FID and FVD (as reported in paper's comparison table).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Table 2 reports WoVoGen: FID = 27.6, FVD = 417.7 on nuScenes validation. Table 1 shows 5h data, 2 Hz, 256×448 resolution and angle&speed control.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not mentioned.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not specified here beyond listed training data and resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Vista outperforms WoVoGen in both FID and FVD and operates at higher frame rate/resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>As above (FID 27.6 / FVD 417.7).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>WoVoGen provides controllable generation but with less temporal fidelity and higher FID than Vista in reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Not detailed beyond comparative metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Angle & speed conditioning; diffusion-based architecture (per original work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Vista shows improved fidelity and higher spatiotemporal resolution in the paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1414.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1414.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ADriver-I</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ADriver-I</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior general world model reference that reportedly supports high-level command/goal-point control and is listed as a baseline in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ADriver-I</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior driving world model referenced in Table 1; supports goal/command-level action control per table summary.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>visual world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>autonomous driving</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not provided in this paper (only table-level meta information).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Table 1 lists ADriver-I training data 300h, 2 Hz, resolution 256×512 and command/goal point control support; no FID/FVD reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not mentioned.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not specified beyond dataset and resolution listed.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Not directly compared.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Listed as an example of a model that supports high-level goal/command inputs; paper positions Vista as combining multiple action granularities in one model.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Supports command/goal-point control (per table entry).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Vista supports the same high-level modes plus lower-level maneuvers and operates at higher spatial resolution per the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1414.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1414.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DriveSim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DriveSim</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier driving simulator/world-model baseline referenced in Table 1 with low resolution and low frame rate.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DriveSim</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior learned driving simulator/world-model (baseline) with modest dataset size and low spatiotemporal fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>visual simulation/world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>autonomous driving</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not specified beyond being listed as a baseline (no FID/FVD in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Table 1 lists DriveSim: 7h data, 5 Hz, 80×160 resolution; no FID/FVD reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not mentioned.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not provided; small resolution and data size suggest lower compute but no specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Not compared quantitatively here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Serves as an earlier example of learned driving simulators with lower fidelity; Vista aims to improve resolution, frame rate, and controllability.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Historical baseline with low-res predictions per table entry.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Vista outperforms older low-resolution approaches in fidelity and control versatility.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1414.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1414.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SVD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stable Video Diffusion (SVD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained latent video diffusion model used to initialize Vista; SVD provides high aesthetic video generation but lacks strict predictive alignment and action conditioning for driving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Stable Video Diffusion (SVD)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent diffusion model for image-to-video generation (continuous-timestep diffusion) that processes sequences of latent codes and uses a UNet denoiser; SVD serves as the initialization for Vista's denoiser and timestep embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent video diffusion model (generative video model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>general video generation (image-to-video)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Perceptual video quality metrics used in SVD literature (not enumerated here); in this paper SVD is described qualitatively as high aesthetic quality but unsuitable for strict predictive rollouts because its first frame is inconsistent with the condition image and it lacks action-conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported numerically here for driving tasks; authors note SVD's first frame mismatch makes it impractical for autoregressive rollouts in prediction settings.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box generative model; no interpretability methods described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned in the context of this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>SVD architecture used has comparable scale to Vista's backbone; Vista notes SVD can be applied without degrading generation quality when using latent replacement. Full SVD compute details not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>SVD provides a strong pretrained backbone enabling Vista to focus on predictive modifications rather than training from scratch; SVD itself lacks predictive alignment which Vista fixes via training modifications.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not applicable as a driving world model; SVD fails in autoregressive long-horizon rollouts for driving prediction per the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>SVD is useful as a starting point to obtain high-quality video synthesis priors; Vista adapts SVD into a predictive world model by discarding noise augmentation at training and using latent replacement for conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>SVD offers high aesthetic quality but not temporal-consistent prediction for driving; Vista modifies SVD to add predictive alignment and action control at the cost of additional architecture and losses.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Initialization from SVD; separation of timestep embeddings for condition vs prediction frames; removal of noise augmentation during predictive training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>SVD is a broad image-to-video generator; Vista specializes and extends it into a predictive, controllable driving world model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>DriveGAN: Towards a Controllable High-Quality Neural Simulation <em>(Rating: 2)</em></li>
                <li>DriveDreamer: Towards Real-World-Driven World Models for Autonomous Driving <em>(Rating: 2)</em></li>
                <li>Drive-WM <em>(Rating: 2)</em></li>
                <li>WoVoGen: World Volume-Aware Diffusion for Controllable Multi-Camera Driving Scene Generation <em>(Rating: 2)</em></li>
                <li>GenAD: Generative End-to-End Autonomous Driving <em>(Rating: 2)</em></li>
                <li>GAIA-1: A Generative World Model for Autonomous Driving <em>(Rating: 2)</em></li>
                <li>Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets <em>(Rating: 2)</em></li>
                <li>Generalized Predictive Model for Autonomous Driving <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1414",
    "paper_id": "paper-ece57d6dcb7ee3e88d2ee94328badb5b41098744",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "Vista",
            "name_full": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
            "brief_description": "A latent-video-diffusion-based driving world model (initialized from Stable Video Diffusion) that predicts high-resolution, high-frame-rate front-view driving futures, incorporates dynamic priors via latent replacement, two auxiliary losses to improve dynamics and structure, and a unified multi-modal action-conditioning interface with LoRA adapters for efficient learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Vista",
            "model_description": "Latent video diffusion model built on Stable Video Diffusion (SVD) with a UNet denoiser; inputs are latent video tensors where up to three condition frames are injected by latent replacement (to provide position, velocity, acceleration priors). Training objective = diffusion loss + dynamics-enhancement loss (re-weighted adjacent-frame motion residuals) + structure-preservation loss (high-frequency Fourier-domain supervision). Actions (angle/speed, trajectory, command, goal point) are encoded via Fourier embeddings and injected through cross-attention layers; LoRA adapters added to attention layers during action-control learning to limit parameter updates.",
            "model_type": "latent world model (diffusion-based visual world model)",
            "task_domain": "autonomous driving (front-view visual prediction, action evaluation/control)",
            "fidelity_metric": "Generative perceptual & temporal metrics: FID (visual fidelity), FVD (video-fidelity/temporal coherence); also trajectory-difference (L2 over inferred/estimated trajectory via an inverse dynamics model) and human evaluation on visual quality and motion rationality.",
            "fidelity_performance": "On nuScenes validation: FID = 6.9, FVD = 89.4. Human evaluators preferred Vista over strong baselines in &gt;70% of pairwise comparisons. Compared to prior driving world models, Vista lowers FID by ~55% and FVD by ~27% relative to the best prior in the paper's comparison.",
            "interpretability_assessment": "Primarily a black-box neural generative model; limited interpretability beyond controllability inputs and uncertainty outputs. Interpretability is operationalized via controllable conditioning (multi-modal actions) and uncertainty-based reward (conditional variance). No claim of explicit human-interpretable latent factors.",
            "interpretability_method": "Indirect/operational methods: (1) controlled counterfactual generation via explicit action inputs to observe causal effects; (2) inverse-dynamics-model (IDM) to read out trajectories from generated video for quantitative assessment; (3) conditional-variance as an uncertainty proxy (used as reward). No explicit latent-space disentanglement or symbolic extraction reported.",
            "computational_cost": "Large: model framework uses ~2.5B parameters (including 1.6B UNet parameters). Training pipeline is two-phase (low-res majority of iterations at 320×576 for 3.5× throughput, then finetune at 576×1024). Sampling/generation uses iterative diffusion sampling (default full-generation ~50 denoising steps referenced); reward estimation uses multi-round denoising (example: 10 steps × 5 rounds = 50 steps) and authors note reward estimation cost per situation is no greater than full generation in alternative detector-based pipelines.",
            "efficiency_comparison": "Two-stage training (low-res then high-res) yields ~3.5× higher training throughput during the main action-control learning stage; LoRA adapters enable parameter-efficient adaptation of attention layers without full UNet finetuning. Authors note reward estimation can be done with fewer denoising steps and is comparable in compute to detector-based reward pipelines, but overall the model is computationally expensive compared to lower-resolution/low-rate alternatives.",
            "task_performance": "High: FID=6.9 and FVD=89.4 on nuScenes validation outperforming listed prior driving world models (see comparisons). Action-conditioning reduces average trajectory-difference (L2, 2s) substantially: e.g., on nuScenes with 3 dynamic priors average trajectory difference for action-free = 1.820, for 'angle & speed' = 0.832, for 'trajectory' = 0.835 (Table 3). Reward function correlates negatively with L2 trajectory perturbation when evaluated zero-shot on Waymo.",
            "task_utility_analysis": "High-fidelity visual prediction plus multi-modal controllability enables (1) improved open-loop evaluation of candidate actions via conditional generation and conditional-variance reward, (2) use as a forward model for model-predictive control / action ranking, and (3) conversion of generated video to executable actions via an inverse-dynamics model. Authors demonstrate the model's predictions better match downstream trajectory estimates and human judgments, indicating fidelity translates to practical utility for action evaluation and counterfactual reasoning.",
            "tradeoffs_observed": "Key trade-offs discussed: (1) fidelity vs compute — high spatiotemporal resolution yields significantly higher compute and latency; (2) perceptual quality vs motion intensity — need to balance smoothing and realistic motion, motivating the dynamics and structure losses; (3) adaptability vs stability — finetuning UNet risks degrading pretrained prediction, solved via LoRA; (4) action-generalization vs training cost — they train action modes independently per sample (action-independence constraint) to avoid combinatorial training costs.",
            "design_choices": "Latent-diffusion backbone (SVD) for high-quality video; latent replacement of condition frames (up to 3) to inject position/velocity/acceleration priors; dynamics-enhancement loss that reweights supervision on dynamic regions computed from adjacent-frame residuals; structure-preservation loss via high-pass Fourier supervision; Fourier embeddings for heterogeneous action sequences; cross-attention conditioning; LoRA adapters for parameter-efficient action-control learning; two-stage training (low-res bulk + high-res finetune); collaborative dataset training (OpenDV-YouTube for generalization + nuScenes for action annotations); action-independence during training.",
            "comparison_to_alternatives": "Quantitative comparisons: Table 2 shows Vista (FID 6.9, FVD 89.4) outperforms DriveGAN (FID 73.4, FVD 502.3), DriveDreamer (52.6 / 452.0), WoVoGen (27.6 / 417.7), Drive-WM (15.8 / 122.7), and GenAD (15.4 / 184.0). Table 1 shows Vista operates at higher data scale (1740h), higher frame rate (10 Hz) and higher resolution (576×1024) and supports more action modes (angle&speed, trajectory, command, goal point) compared to many prior world models. Computationally, Vista is larger and more costly than many of these lower-resolution, lower-frame-rate baselines.",
            "optimal_configuration": "Recommendations and insights in paper: (1) condition on at least three consecutive frames to capture position, velocity, and acceleration priors for coherent rollouts; (2) augment diffusion loss with dynamics-aware reweighting and high-frequency structure supervision for high-resolution driving prediction; (3) use Fourier-embedded action conditioning via cross-attention and LoRA adapters for efficient controllability learning; (4) two-phase training (low-res then short high-res finetune) to maximize iterations while maintaining high-res performance; (5) enforce action-independence during training to avoid combinatorial explosion and to make action modes learnable individually; (6) use conditional-variance (multi-sample denoising) as a generalizable reward signal.",
            "uuid": "e1414.0",
            "source_info": {
                "paper_title": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Drive-WM",
            "name_full": "Drive-WM (Driving World Model from prior work)",
            "brief_description": "A previously proposed driving world model referenced as a baseline; used detector-based reward in prior work and models driving futures at relatively low frame rate and resolution.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Drive-WM",
            "model_description": "A visual world model for autonomous driving (prior work) that was evaluated at lower temporal/spatial resolutions; in prior literature Drive-WM used perception detectors to form reward signals for actions.",
            "model_type": "visual world model",
            "task_domain": "autonomous driving",
            "fidelity_metric": "Reported FID and FVD in comparison table (FID, FVD). Also used detector outputs for reward evaluation in prior work.",
            "fidelity_performance": "Table 2 (as reported in this paper): FID = 15.8, FVD = 122.7 on nuScenes validation (comparison baseline).",
            "interpretability_assessment": "Not discussed in this paper beyond the use of detector-based reward (which relies on interpretable detector outputs), so internal model interpretability is not described here.",
            "interpretability_method": "N/A (no internal interpretability methods described in this paper).",
            "computational_cost": "Not specified in this paper (only reported resolution/frame-rate/data regime: 5h data, 2 Hz, 192×384 per Table 1).",
            "efficiency_comparison": "Not directly compared for compute; the paper contrasts Vista's reward estimator (conditional variance) as an alternative to Drive-WM's detector-based reward and argues comparable or lower compute for reward estimation.",
            "task_performance": "As above: FID 15.8, FVD 122.7 on nuScenes val (worse than Vista in paper comparisons).",
            "task_utility_analysis": "In Drive-WM, reward estimation relied on external detectors trained on a specific dataset; the present paper notes that such detector-based rewards may not generalize across domains—motivation for Vista's uncertainty-based reward.",
            "tradeoffs_observed": "Detector-based reward can give interpretable, structured signals but may reduce generalization to novel domains; computational pipeline requires full video generation before perception-based reward, per authors' discussion.",
            "design_choices": "Not detailed in this paper beyond being a baseline world model and using detector-based reward in its original formulation.",
            "comparison_to_alternatives": "Drive-WM's FID/FVD are intermediate among prior methods (worse than Vista). The paper positions Vista's reward as more generalizable than Drive-WM's detector-based reward because Vista's uncertainty is model-native and does not require external detectors.",
            "optimal_configuration": "Not specified in this paper.",
            "uuid": "e1414.1",
            "source_info": {
                "paper_title": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GenAD",
            "name_full": "GenAD (Generative End-to-End Autonomous Driving)",
            "brief_description": "A prior generative driving model used as a comparison baseline that supports trajectory and command control modes at lower temporal/spatial resolution.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GenAD",
            "model_description": "Prior driving generative model that conditions on action modalities such as trajectory and goal/command; operates at lower frame rate and spatial resolution (per Table 1).",
            "model_type": "visual generative world model",
            "task_domain": "autonomous driving",
            "fidelity_metric": "FID and FVD used for comparison.",
            "fidelity_performance": "Table 2 reports GenAD: FID = 15.4, FVD = 184.0 on nuScenes validation.",
            "interpretability_assessment": "Not described in this paper. The paper notes GenAD's trajectory-conditioned extension was finetuned on nuScenes and its generalization was not validated.",
            "interpretability_method": "Not mentioned.",
            "computational_cost": "Not specified here; Table 1 shows GenAD trained on 2000h at 2 Hz and resolution 256×448 (indicates larger data scale but lower spatiotemporal fidelity than Vista).",
            "efficiency_comparison": "Compared to Vista, GenAD runs at much lower frame rate (2 Hz) and lower resolution; Vista reports better FID/FVD with lower model complexity per the authors.",
            "task_performance": "As above (FID 15.4 / FVD 184.0), worse than Vista on the reported metrics.",
            "task_utility_analysis": "Authors argue GenAD's action control generalization is limited since GenAD finetuned on nuScenes; Vista aims for zero-shot action controllability across datasets via collaborative training.",
            "tradeoffs_observed": "GenAD opts for larger-scale trajectory conditioning at lower spatiotemporal resolution, trading visual fidelity for other design choices.",
            "design_choices": "Trajectory and command conditioning (finetuned per-dataset in original work).",
            "comparison_to_alternatives": "Vista achieves substantially better FID/FVD and higher spatiotemporal resolution while maintaining multi-modal controls that generalize zero-shot, per authors' comparisons.",
            "optimal_configuration": "Not specified in this paper.",
            "uuid": "e1414.2",
            "source_info": {
                "paper_title": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GAIA-1",
            "name_full": "GAIA-1: A Generative World Model for Autonomous Driving",
            "brief_description": "A recent large-scale driving world model baseline reported to operate at high frame-rate and moderate resolution with angle&speed controls.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GAIA-1",
            "model_description": "A generative driving world model (prior work) that reportedly trained on large-scale data and supports angle & speed control; Table 1 lists a very large data scale and high frame rate.",
            "model_type": "visual world model",
            "task_domain": "autonomous driving",
            "fidelity_metric": "Not quantified in this paper beyond being listed in Table 1; FID/FVD not provided here.",
            "fidelity_performance": "Table 1 indicates GAIA-1 trained on ~4700h and operates at 25 Hz, resolution 288×512, and supports angle & speed control. No direct FID/FVD numbers reported in this paper.",
            "interpretability_assessment": "Not described here.",
            "interpretability_method": "Not mentioned.",
            "computational_cost": "Large data scale (4700h) and high frame rate imply high training compute, but no explicit compute numbers are provided in this paper.",
            "efficiency_comparison": "Vista emphasizes higher resolution (576×1024) though at lower frame rate (10 Hz) and smaller training data (1740h) compared to GAIA-1's 4700h at 25 Hz; direct compute comparison not provided.",
            "task_performance": "Not reported in this paper.",
            "task_utility_analysis": "GAIA-1 is a high-data, high-frame-rate baseline; the present paper positions Vista as providing higher spatial resolution and multi-modal action control with strong generalization.",
            "tradeoffs_observed": "Higher frame-rate training (25 Hz) and larger dataset likely implies higher compute; Vista trades frame rate for much higher spatial resolution and explicit controllability design.",
            "design_choices": "Not detailed in this paper beyond table entry.",
            "comparison_to_alternatives": "Listed as part of Table 1; Vista differs by prioritizing spatial resolution and multi-modal action support.",
            "optimal_configuration": "Not specified here.",
            "uuid": "e1414.3",
            "source_info": {
                "paper_title": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "DriveGAN",
            "name_full": "DriveGAN",
            "brief_description": "A prior controllable generative driving simulator model used as a baseline; reported at moderate resolution and frame rate.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "DriveGAN",
            "model_description": "A generative driving model (GAN-based in original work) used to synthesize driving scenes with some controllability (angle & speed listed in Table 1).",
            "model_type": "visual generative model (GAN)",
            "task_domain": "autonomous driving",
            "fidelity_metric": "FID and FVD reported in paper's comparison.",
            "fidelity_performance": "Table 2 reports DriveGAN: FID = 73.4, FVD = 502.3 on nuScenes validation (baseline values reported in this paper). Table 1 indicates 160h data, 8 Hz, 256×256 resolution.",
            "interpretability_assessment": "Not discussed.",
            "interpretability_method": "Not mentioned.",
            "computational_cost": "Not detailed here beyond dataset scale and resolution listed in Table 1.",
            "efficiency_comparison": "Vista substantially outperforms DriveGAN on FID/FVD; DriveGAN lower resolution and higher FID/FVD.",
            "task_performance": "As above (FID 73.4 / FVD 502.3).",
            "task_utility_analysis": "DriveGAN provides controllable visual simulation but with lower fidelity compared to modern diffusion-based approaches per reported metrics.",
            "tradeoffs_observed": "Earlier GAN-based approaches may be faster to sample but generated lower-fidelity or less temporally-coherent outputs compared to diffusion-based models in the comparisons shown.",
            "design_choices": "GAN-based video synthesis with angle&speed conditioning (per table entry).",
            "comparison_to_alternatives": "Outperformed by Vista and other modern diffusion-based models in FID/FVD per Table 2.",
            "optimal_configuration": "Not discussed.",
            "uuid": "e1414.4",
            "source_info": {
                "paper_title": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "DriveDreamer",
            "name_full": "DriveDreamer",
            "brief_description": "A prior driving world model (Dreamer-style/latent dynamics approach) used as a baseline with low training data and low-to-moderate fidelity.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "DriveDreamer",
            "model_description": "A prior world-model approach for driving that follows Dreamer-style latent dynamics; reported at low data scale and moderate frame-rate/resolution.",
            "model_type": "latent dynamics world model",
            "task_domain": "autonomous driving",
            "fidelity_metric": "FID and FVD used in comparative table.",
            "fidelity_performance": "Table 2 reports DriveDreamer: FID = 52.6, FVD = 452.0 on nuScenes validation. Table 1 lists data scale 5h, 12 Hz, resolution 128×192.",
            "interpretability_assessment": "Not discussed in this paper.",
            "interpretability_method": "Not mentioned.",
            "computational_cost": "Not detailed here (small dataset size listed suggests lower training compute but no specifics).",
            "efficiency_comparison": "Vista outperforms DriveDreamer by large margins in FID/FVD per Table 2 and operates at higher resolution and sustained generalization.",
            "task_performance": "As above (FID 52.6 / FVD 452.0).",
            "task_utility_analysis": "Prior Dreamer-style latent dynamics approaches demonstrate sample-efficiency in control tasks but, per the paper, may lack high-resolution visual fidelity needed for detailed driving scene prediction.",
            "tradeoffs_observed": "Latency and low-resolution dynamics modeling vs. high-fidelity image/video prediction trade-offs; paper argues diffusion-based latent video models (Vista) better preserve visual fidelity.",
            "design_choices": "Dreamer-style latent prediction (per original work), angle&speed control listed in Table 1.",
            "comparison_to_alternatives": "Vista yields much lower FID/FVD and higher spatial resolution in this paper's comparisons.",
            "optimal_configuration": "Not provided here.",
            "uuid": "e1414.5",
            "source_info": {
                "paper_title": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "WoVoGen",
            "name_full": "WoVoGen (World Volume-Aware Diffusion for Controllable Multi-Camera Driving Scene Generation)",
            "brief_description": "A prior diffusion-based driving video generator that supports angle&speed conditioning; included as a baseline with moderate fidelity metrics.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "WoVoGen",
            "model_description": "Diffusion-based multi-camera driving scene generator (prior work) which models world-volume awareness; listed as a baseline in comparisons.",
            "model_type": "diffusion-based visual generator / world model",
            "task_domain": "autonomous driving / multi-camera scene generation",
            "fidelity_metric": "FID and FVD (as reported in paper's comparison table).",
            "fidelity_performance": "Table 2 reports WoVoGen: FID = 27.6, FVD = 417.7 on nuScenes validation. Table 1 shows 5h data, 2 Hz, 256×448 resolution and angle&speed control.",
            "interpretability_assessment": "Not discussed in this paper.",
            "interpretability_method": "Not mentioned.",
            "computational_cost": "Not specified here beyond listed training data and resolution.",
            "efficiency_comparison": "Vista outperforms WoVoGen in both FID and FVD and operates at higher frame rate/resolution.",
            "task_performance": "As above (FID 27.6 / FVD 417.7).",
            "task_utility_analysis": "WoVoGen provides controllable generation but with less temporal fidelity and higher FID than Vista in reported comparisons.",
            "tradeoffs_observed": "Not detailed beyond comparative metrics.",
            "design_choices": "Angle & speed conditioning; diffusion-based architecture (per original work).",
            "comparison_to_alternatives": "Vista shows improved fidelity and higher spatiotemporal resolution in the paper's comparisons.",
            "optimal_configuration": "Not specified here.",
            "uuid": "e1414.6",
            "source_info": {
                "paper_title": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ADriver-I",
            "name_full": "ADriver-I",
            "brief_description": "A prior general world model reference that reportedly supports high-level command/goal-point control and is listed as a baseline in Table 1.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "ADriver-I",
            "model_description": "Prior driving world model referenced in Table 1; supports goal/command-level action control per table summary.",
            "model_type": "visual world model",
            "task_domain": "autonomous driving",
            "fidelity_metric": "Not provided in this paper (only table-level meta information).",
            "fidelity_performance": "Table 1 lists ADriver-I training data 300h, 2 Hz, resolution 256×512 and command/goal point control support; no FID/FVD reported here.",
            "interpretability_assessment": "Not discussed in this paper.",
            "interpretability_method": "Not mentioned.",
            "computational_cost": "Not specified beyond dataset and resolution listed.",
            "efficiency_comparison": "Not directly compared.",
            "task_performance": "Not reported here.",
            "task_utility_analysis": "Listed as an example of a model that supports high-level goal/command inputs; paper positions Vista as combining multiple action granularities in one model.",
            "tradeoffs_observed": "Not detailed in this paper.",
            "design_choices": "Supports command/goal-point control (per table entry).",
            "comparison_to_alternatives": "Vista supports the same high-level modes plus lower-level maneuvers and operates at higher spatial resolution per the authors.",
            "optimal_configuration": "Not provided.",
            "uuid": "e1414.7",
            "source_info": {
                "paper_title": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "DriveSim",
            "name_full": "DriveSim",
            "brief_description": "An earlier driving simulator/world-model baseline referenced in Table 1 with low resolution and low frame rate.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "DriveSim",
            "model_description": "Prior learned driving simulator/world-model (baseline) with modest dataset size and low spatiotemporal fidelity.",
            "model_type": "visual simulation/world model",
            "task_domain": "autonomous driving",
            "fidelity_metric": "Not specified beyond being listed as a baseline (no FID/FVD in this paper).",
            "fidelity_performance": "Table 1 lists DriveSim: 7h data, 5 Hz, 80×160 resolution; no FID/FVD reported in this paper.",
            "interpretability_assessment": "Not discussed.",
            "interpretability_method": "Not mentioned.",
            "computational_cost": "Not provided; small resolution and data size suggest lower compute but no specifics.",
            "efficiency_comparison": "Not compared quantitatively here.",
            "task_performance": "Not reported in this paper.",
            "task_utility_analysis": "Serves as an earlier example of learned driving simulators with lower fidelity; Vista aims to improve resolution, frame rate, and controllability.",
            "tradeoffs_observed": "Not detailed.",
            "design_choices": "Historical baseline with low-res predictions per table entry.",
            "comparison_to_alternatives": "Vista outperforms older low-resolution approaches in fidelity and control versatility.",
            "optimal_configuration": "Not provided.",
            "uuid": "e1414.8",
            "source_info": {
                "paper_title": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "SVD",
            "name_full": "Stable Video Diffusion (SVD)",
            "brief_description": "A pretrained latent video diffusion model used to initialize Vista; SVD provides high aesthetic video generation but lacks strict predictive alignment and action conditioning for driving.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Stable Video Diffusion (SVD)",
            "model_description": "Latent diffusion model for image-to-video generation (continuous-timestep diffusion) that processes sequences of latent codes and uses a UNet denoiser; SVD serves as the initialization for Vista's denoiser and timestep embeddings.",
            "model_type": "latent video diffusion model (generative video model)",
            "task_domain": "general video generation (image-to-video)",
            "fidelity_metric": "Perceptual video quality metrics used in SVD literature (not enumerated here); in this paper SVD is described qualitatively as high aesthetic quality but unsuitable for strict predictive rollouts because its first frame is inconsistent with the condition image and it lacks action-conditioning.",
            "fidelity_performance": "Not reported numerically here for driving tasks; authors note SVD's first frame mismatch makes it impractical for autoregressive rollouts in prediction settings.",
            "interpretability_assessment": "Black-box generative model; no interpretability methods described in this paper.",
            "interpretability_method": "None mentioned in the context of this paper.",
            "computational_cost": "SVD architecture used has comparable scale to Vista's backbone; Vista notes SVD can be applied without degrading generation quality when using latent replacement. Full SVD compute details not specified here.",
            "efficiency_comparison": "SVD provides a strong pretrained backbone enabling Vista to focus on predictive modifications rather than training from scratch; SVD itself lacks predictive alignment which Vista fixes via training modifications.",
            "task_performance": "Not applicable as a driving world model; SVD fails in autoregressive long-horizon rollouts for driving prediction per the authors.",
            "task_utility_analysis": "SVD is useful as a starting point to obtain high-quality video synthesis priors; Vista adapts SVD into a predictive world model by discarding noise augmentation at training and using latent replacement for conditioning.",
            "tradeoffs_observed": "SVD offers high aesthetic quality but not temporal-consistent prediction for driving; Vista modifies SVD to add predictive alignment and action control at the cost of additional architecture and losses.",
            "design_choices": "Initialization from SVD; separation of timestep embeddings for condition vs prediction frames; removal of noise augmentation during predictive training.",
            "comparison_to_alternatives": "SVD is a broad image-to-video generator; Vista specializes and extends it into a predictive, controllable driving world model.",
            "uuid": "e1414.9",
            "source_info": {
                "paper_title": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "DriveGAN: Towards a Controllable High-Quality Neural Simulation",
            "rating": 2
        },
        {
            "paper_title": "DriveDreamer: Towards Real-World-Driven World Models for Autonomous Driving",
            "rating": 2
        },
        {
            "paper_title": "Drive-WM",
            "rating": 2
        },
        {
            "paper_title": "WoVoGen: World Volume-Aware Diffusion for Controllable Multi-Camera Driving Scene Generation",
            "rating": 2
        },
        {
            "paper_title": "GenAD: Generative End-to-End Autonomous Driving",
            "rating": 2
        },
        {
            "paper_title": "GAIA-1: A Generative World Model for Autonomous Driving",
            "rating": 2
        },
        {
            "paper_title": "Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets",
            "rating": 2
        },
        {
            "paper_title": "Generalized Predictive Model for Autonomous Driving",
            "rating": 1
        }
    ],
    "cost": 0.02270175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability</h1>
<p>Shenyuan Gao ${ }^{1,2}$ Jiazhi Yang ${ }^{2}$ Li Chen ${ }^{2,5}$ Kashyap Chitta ${ }^{3,4}$ Yihang Qiu ${ }^{2}$<br>Andreas Geiger ${ }^{3,4 \dagger}$ Jun Zhang ${ }^{1 \dagger}$ Hongyang Li ${ }^{2,5 \dagger}$<br>${ }^{1}$ Hong Kong University of Science and Technology ${ }^{2}$ OpenDriveLab at Shanghai AI Lab<br>${ }^{3}$ University of Tübingen ${ }^{4}$ Tübingen AI Center ${ }^{5}$ University of Hong Kong<br>Code and model: github.com/OpenDriveLab/Vista<br>Demo page: opendrivelab.com/Vista</p>
<h4>Abstract</h4>
<p>World models can foresee the outcomes of different actions, which is of paramount importance for autonomous driving. Nevertheless, existing driving world models still have limitations in generalization to unseen environments, prediction fidelity of critical details, and action controllability for flexible application. In this paper, we present Vista, a generalizable driving world model with high fidelity and versatile controllability. Based on a systematic diagnosis of existing methods, we introduce several key ingredients to address these limitations. To accurately predict real-world dynamics at high resolution, we propose two novel losses to promote the learning of moving instances and structural information. We also devise an effective latent replacement approach to inject historical frames as priors for coherent long-horizon rollouts. For action controllability, we incorporate a versatile set of controls from high-level intentions (command, goal point) to low-level maneuvers (trajectory, angle, and speed) through an efficient learning strategy. After large-scale training, the capabilities of Vista can seamlessly generalize to different scenarios. Extensive experiments on multiple datasets show that Vista outperforms the most advanced general-purpose video generator in over $70 \%$ of comparisons and surpasses the best-performing driving world model by $55 \%$ in FID and $27 \%$ in FVD. Moreover, for the first time, we utilize the capacity of Vista itself to establish a generalizable reward for real-world action evaluation without accessing the ground truth actions.</p>
<h2>1 Introduction</h2>
<p>Driven by scalable learning techniques, autonomous driving has made encouraging strides over the past few years [18, 58, 135]. However, intricate and out-of-distribution situations are still intractable for state-of-the-art techniques [83]. One promising solution lies in world models [57, 76], which infer the possible future states of the world from historical observations and alternative actions, in turn assessing the feasibility of such actions. They hold the potential to reason with uncertainty and avoid catastrophic errors [54, 76, 127], thereby promoting generalization and safety in autonomous driving.
Although a primary prospect of world models is to enable the generalization ability to novel environments, existing driving world models are still constrained by data scale [90, 125, 127, 143, 147] and geographical coverage [54, 61]. As summarized in Table 1 and Fig. 1, they are also often confined to low frame rates and resolutions, resulting in a loss of critical details. Furthermore, most models only support a single control modality such as the steering angle and speed. This is insufficient to express various action formats ranging from high-level intentions to low-level maneuvers, and incompatible with the outputs of prevalent planning algorithms [12, 14, 21, 56, 58, 64]. In addition, generalizing action controllability to unseen datasets is understudied. These limitations impede the applicability of existing works, making it imperative to develop a world model that overcomes these limitations.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Real-world driving world models. Trained on large-scale high-quality driving data, Vista performs at high spatiotemporal resolution and supports versatile action controllability. Private data.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Model Setups</th>
<th></th>
<th></th>
<th>Action Control Modes</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Data Scale</td>
<td>Frame Rate</td>
<td>Resolution</td>
<td>Angle&amp;Speed</td>
<td>Trajectory</td>
<td>Command Goal Point</td>
</tr>
<tr>
<td>DriveSim [102]</td>
<td>7h</td>
<td>5 Hz</td>
<td>80$\times$160</td>
<td>✓</td>
<td></td>
<td></td>
</tr>
<tr>
<td>DriveGAN [68]</td>
<td>160h</td>
<td>8 Hz</td>
<td>256$\times$256</td>
<td>✓</td>
<td></td>
<td></td>
</tr>
<tr>
<td>DriveDreamer [125]</td>
<td>5h</td>
<td>12 Hz</td>
<td>128$\times$192</td>
<td>✓</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Drive-WM [127]</td>
<td>5h</td>
<td>2 Hz</td>
<td>192$\times$384</td>
<td></td>
<td>✓</td>
<td></td>
</tr>
<tr>
<td>WoVoGen [90]</td>
<td>5h</td>
<td>2 Hz</td>
<td>256$\times$448</td>
<td>✓</td>
<td></td>
<td></td>
</tr>
<tr>
<td>ADriver-I [61]</td>
<td>300h</td>
<td>2 Hz</td>
<td>256$\times$512</td>
<td></td>
<td></td>
<td>✓</td>
</tr>
<tr>
<td>GenAD [136]</td>
<td>2000h</td>
<td>2 Hz</td>
<td>256$\times$448</td>
<td></td>
<td>✓</td>
<td>✓</td>
</tr>
<tr>
<td>GAIA-1 [54]</td>
<td>4700h</td>
<td>25 Hz</td>
<td>288$\times$512</td>
<td>✓</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Vista (Ours)</td>
<td>1740h</td>
<td>10 Hz</td>
<td>576$\times$1024</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
</tr>
</tbody>
</table>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Resolution comparison. Vista predicts at a higher resolution than previous literature.</p>
<p>To this end, we introduce <em>Vista</em>, a driving world model that is proficient in cross-domain generalization, high-fidelity prediction, and multi-modal action controllability. Specifically, we develop the predictive model on a large corpus of worldwide driving videos [136] to foster its generalization ability. To enable coherent future extrapolation, we condition Vista on three essential dynamic priors (Sec. 3.1). Instead of solely relying on the standard diffusion loss [5], we introduce two explicit loss functions to enhance dynamics and preserve structural details (Sec. 3.1), promoting Vista's ability to simulate realistic futures at high resolution. For flexible controllability, we incorporate a versatile set of action formats, including both high-level intentions such as commands and goal points, as well as low-level maneuvers like trajectories, steering angles, and speeds. These action conditions are injected via a unified interface, which is learned through an efficient training strategy (Sec. 3.2). Consequently, as Fig. 2 shows, Vista acquires the ability to anticipate realistic futures at 10 Hz and 576×1024 pixels, and obtains versatile action controllability across various levels of granularity. We also demonstrate the potential of Vista as a generalizable reward function to evaluate the reliability of different actions.</p>
<p>Our contributions are three-fold: (1) We present <em>Vista</em>, a generalizable driving world model that can predict realistic futures at high spatiotemporal resolution. Its prediction fidelity is greatly improved by two novel losses that capture dynamics and preserve structures, along with exhaustive dynamic priors to sustain consistency in long-horizon rollouts. (2) Propelled by an efficient learning strategy, we integrate versatile action controllability into Vista through a unified conditioning interface. The action controllability of Vista can also generalize to different domains in a zero-shot manner. (3) We conduct comprehensive experiments across multiple datasets to verify the effectiveness of Vista. It outperforms the most competitive general-purpose video generator and sets a new state-of-the-art on nuScenes. Our empirical evidence shows that Vista can be used as a reward function to assess actions.</p>
<h2>2 Preliminary</h2>
<p>We initialize Vista with the pretrained Stable Video Diffusion (SVD) [5], a latent diffusion model for image-to-video generation. For sampling flexibility, SVD adopts a continuous-timestep formula [66, 111]. It converts data samples $\boldsymbol{x}$ to noise $\boldsymbol{n}$ through a diffusion process $p(\boldsymbol{n}|\boldsymbol{x}) \sim \mathcal{N}\left(\boldsymbol{x}, \sigma^{2} \mathbf{I}\right)$, and generates new samples by progressively denoising the latent towards $\sigma_{0}=0$ from Gaussian noise. The training of SVD can be simplified to minimizing $\mathbb{E}<em _sigma="\sigma">{\boldsymbol{x}, \sigma, \boldsymbol{n}}\left[\lambda</em>$ and generates a video with $K=25$ frames. The generation process is guided by a condition image, whose latent is concatenated channel-wise to the inputs, serving as a reference for content generation.}\left|D_{\theta}(\boldsymbol{n} ; \sigma)-\boldsymbol{x}\right|^{2}\right]$, where $D_{\theta}$ is a parameterized UNet denoiser and $\lambda_{\sigma}$ is a re-weighting function omitted hereinafter for brevity. Based on this framework, SVD processes a sequence of noisy latent $\boldsymbol{n}=\left{n_{1}, n_{2}, \ldots, n_{K}\right} \in \mathbb{R}^{K \times C \times H \times W</p>
<p>Despite the high aesthetic quality, SVD lacks several key properties to function as a driving world model. As shown in Sec. 4, the first frame predicted by SVD is not identical to the condition image, making it impractical for autoregressive rollout due to content inconsistency. In addition, SVD struggles with the intricate dynamics of driving scenarios, entailing implausible motions. Moreover,</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Capabilities of Vista. Starting from arbitrary environments, Vista can anticipate realistic and continuous futures at high spatiotemporal resolution (A-B). It can be controlled by multi-modal actions (C), and serve as a generalizable reward function to evaluate real-world driving actions (D).</p>
<p>SVD cannot be controlled by any action format. In contrast, we aim to build a generalizable driving world model that predicts high-fidelity futures with realistic dynamics. It ought to be continuously extendable to long horizons and flexibly controllable by multi-modal actions as illustrated in Fig. 2.</p>
<h1>3 Learning a Generalizable Driving World Model</h1>
<p>As depicted in Fig. 3, Vista adopts a two-phase training pipeline. First, we build a dedicated predictive model, which involves a latent replacement approach to enable coherent future prediction and two novel losses to enhance fidelity (Sec. 3.1). To ensure the generalization to unseen scenarios, we utilize the largest public driving dataset [136] for training. In the second phase, we incorporate multi-modal actions to learn action controllability with an efficient and collaborative training strategy (Sec. 3.2). Using the ability of Vista, we further introduce a generalizable approach to evaluate actions (Sec. 3.3).</p>
<h3>3.1 Phase One: Learning High-Fidelity Future Prediction</h3>
<p>Basic Setup. Since world models are initiated to predict futures from the current state, the starting of their prediction should be firmly aligned with the condition image. Therefore, we tailor SVD into a dedicated predictive model by imposing the first frame as the condition image and discarding the noise augmentation [5, 49] during training. With this prediction ability, Vista can perform long-term rollouts by iteratively predicting short-term clips and resetting the condition image with the last clip.
Dynamic Prior Injection. Nevertheless, using the aforementioned setup for training often results in irrational dynamics with respect to historical frames, especially in long-term rollouts. We conjecture that this mainly arises from the ambiguity caused by insufficient priors about the tendency of future motions, which is also a common limitation of existing driving world models [54, 68, 125, 127, 136].
Estimating coherent futures requires at least three essential priors that inherently govern the future motion of instances in the scene: position, velocity, and acceleration. Since velocity and acceleration are the first- and second-order derivative of position respectively, these priors can be entirely derived by using three consecutive frames for conditioning. Concretely, we build a frame-wise mask $\boldsymbol{m} \in$ ${0,1}^{K}$ with a length of $K$ to indicate the presence of condition frames. The mask is set sequentially following the time order, with at most three elements being assigned as 1 to denote three condition</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: [Left]: Vista pipeline. In addition to the initial frame, Vista can absorb more priors about future dynamics via latent replacement. Its prediction can be controlled by different actions and be extended to long horizons through autoregressive rollouts. [Right]: Training procedure. Vista takes two training phases, where the second phase freezing the pretrained weights to learn action controls.</p>
<p>frames. Instead of concatenating additional channels to the inputs, we inject new condition frames by replacing the corresponding noisy latent <em>n<sub>i</sub></em> with the clean latent <em>z<sub>i</sub></em> encoded by the image encoder. Formally, the input latent is constructed as <em>ṅ</em> = <em>m · z</em> + (1 − <em>m</em>) · <em>n</em> (see Fig. 3 [Left]). To discern the clean latent, we duplicate a new timestep embedding from the pretrained weights and allocate it to the condition frames according to <em>m</em>. The timestep embeddings for condition frames and prediction frames are trained separately. Compared to channel-wise concatenation, we find that replacing the latent is more effective and flexible in absorbing varying numbers of condition frames. In addition, we observe that the replaced latent, when applied to SVD directly, does not degrade its generation quality. Thus, the original performance will not be disturbed when the training is launched. Since there is no need to predict the observed condition frames, we exclude them from the loss as follows:</p>
<p>$$\mathcal{L}<em _mathbf_z="\mathbf{z">{\text{diffusion}} = \mathbb{E}</em>$$}, \sigma, \mathbf{n}} \left[ \sum_{i=1}^{K} (1 - m_i) \odot |D_{\theta}(\hat{n}_i; \sigma) - z_i|^2 \right], \tag{1</p>
<p>where <em>D<sub>θ</sub></em> is the UNet denoiser that shares the same architecture with SVD. With the replaced latent holding sufficient priors, Vista can fully capture the status of the surrounding instances and predict more coherent and plausible long-term futures through iterative rollouts. In practice, we leverage the last three frames of a predicted clip as dynamic priors for the next prediction step during rollouts.</p>
<p>Dynamics Enhancement Loss. Unlike general videos that cover rather small spaces, driving videos capture much larger scenes [136]. In most driving videos, distant and monotonous regions dominate the view, with the moving foreground instances only occupying a relatively small area [17]. However, the latter often exhibit higher stochasticity, complicating their prediction. Since Eq. (1) supervises all outputs uniformly, it cannot effectively discriminate the nuances of different regions as Fig. 4(b) shows. As a result, the model cannot efficiently learn to predict realistic dynamics in crucial regions.</p>
<p>As the discrepancy between two adjacent frames provides considerable motion patterns [123, 132], we introduce an additional supervision to encourage the learning of dynamics for crucial regions. To be specific, we first introduce a dynamics-aware weight <em>w</em> = {<em>w</em><sub>2</sub>, <em>w</em><sub>3</sub>, ..., <em>w</em><sub><em>k</em></sub>} ∈ ℝ<sup><em>K</em>−1×<em>C</em>×<em>H</em>×<em>W</em></sup> that highlights the regions where the prediction has inconsistent motion compared to the ground truth:</p>
<p>$$w_i = |(D_{\theta}(\hat{n}<em _theta="\theta">i; \sigma) - D</em>}(\hat{n<em i-1="i-1">{i-1}; \sigma)) - (z_i - z</em>$$})|^2. \tag{2</p>
<p>For numerical stability, we normalize <em>w</em> within each video clip. As shown in Fig. 4(c), the weight amplifies the presence of large motion disparities, highlighting dynamic regions while excluding monotonous backgrounds. Given the causality of future prediction, <em>i.e</em>. subsequent frames ought to follow previous ones, we define a new loss by penalizing the latter frame of each adjacent frame pair:</p>
<p>$$\mathcal{L}<em _mathbf_z="\mathbf{z">{\text{dynamics}} = \mathbb{E}</em>$$}, \sigma, \mathbf{n}} \left[ \sum_{i=2}^{K} \text{sg}(w_i) \odot (1 - m_i) \odot |D_{\theta}(\hat{n}_i; \sigma) - z_i|^2 \right], \tag{3</p>
<p>where sg(·) stops the gradient. By adaptively re-weighting the standard diffusion loss, <em>L<sub>dynamics</sub></em> can boost the learning efficiency of dynamic regions, <em>e.g</em>., the moving vehicles and sidewalks in Fig. 4(d).</p>
<p>Structure Preservation Loss. The trade-off between perceptual quality and motion intensity has been widely acknowledged in video generation [3, 32, 73, 144], and our case is no exception. When</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Illustration on loss design. Different from the standard diffusion loss (b) that is distributed uniformly, our dynamics enhancement loss (d) enables an adaptive concentration on critical regions (c) (e.g., moving vehicles and roadsides) for dynamics modeling. Moreover, by explicitly supervising high-frequency features (e), the learning of structural details (e.g., edges and lanes) can be enhanced.</p>
<p>it comes to high-resolution prediction for dynamic driving scenarios, we discover that the predicted structural details degrade severely with over-smoothed or broken objects, e.g., the outlines of vehicles unravel quickly as they move (see Fig. 12). To alleviate this problem, it is important to place more emphasis on structural details. Based on the fact that structural details, such as edges and textures, mainly reside in high-frequency components, we identify them in the frequency domain as follows:</p>
<p>$$z_i' = \mathcal{F}(z_i) = \text{IFFT}(\mathcal{H} \odot \text{FFT}(z_i)),\tag{4}$$</p>
<p>where FFT and IFFT are the 2D discrete Fourier transform and inverse discrete Fourier transform respectively, and $\mathcal{H}$ is an ideal 2D high-pass filter that truncates low-frequency components under a certain threshold. The Fourier transforms are applied on each channel of $z_i$ independently. As illustrated in Fig. 4(e), features associated with structural information can be effectively emphasized by Eq. (4). The corresponding features from the predicted latent $D_{\theta}(\hat{n}_i; \sigma)$ can also be extracted similarly. With the extracted high-frequency features, we devise a new structure preservation loss as:</p>
<p>$$\mathcal{L}<em _hat_mathbf_z="\hat{\mathbf{z">{\text{structure}} = \mathbb{E}</em>$$}}, \sigma, \hat{\mathbf{n}}} \left[ \sum_{i=1}^{K} (1 - m_i) \odot \left| \mathcal{F}(D_{\theta}(\hat{n}_i; \sigma)) - \mathcal{F}(z_i) \right|^2 \right]. \tag{5</p>
<p>This loss function minimizes the disparity of high-frequency features between prediction and ground truth, so that more structural information can be retained. Our final training objective is a weighted sum of Eq. (1), Eq. (3) and Eq. (5), where $\lambda_1$ and $\lambda_2$ are trade-off weights to balance the optimization:</p>
<p>$$\mathcal{L}<em _text_diffusion="\text{diffusion">{\text{final}} = \mathcal{L}</em>}} + \lambda_1 \mathcal{L<em _text_structure="\text{structure">{\text{dynamics}} + \lambda_2 \mathcal{L}</em>$$}}.\tag{6</p>
<h3>3.2 Phase Two: Learning Versatile Action Controllability</h3>
<p>Unified Conditioning of Versatile Actions. To maximize usage flexibility, a driving world model should be able to leverage multiple action formats with different characteristics. For instance, one may use the world model to evaluate high-level policies [127], or to execute low-level maneuvers [102]. However, existing approaches only support limited action controls [54, 61, 90, 125, 127], inhibiting their flexibility and applicability. Therefore, we incorporate a versatile set of action modes for Vista: (1) Angle and Speed stand for the utmost fine-grained action controls. We normalize angles to [−1, 1] and represent speeds in km/h. The signals from different timestamps are concatenated sequentially. (2) Trajectory is a series of 2D displacements in ego coordinates. It is widely used as the output of planning algorithms [12, 21, 58, 62, 63]. We represent the trajectory in meters and flatten it into a sequence. (3) Command is the most high-level intention. Without loss of generality, we define four commands, i.e. go forward, turn right, turn left, and stop, which are implemented as categorical indices. (4) Goal Point is a 2D coordinate projected from the short-term ego destination onto the initial frame, serving as an interactive interface [74]. The coordinate is normalized by the image size.</p>
<p>Note that these actions are heterogeneous and cannot be used interchangeably. After transforming all these actions into numerical sequences, we encode them as a unified concatenation of Fourier embeddings [114, 116] (see Fig. 3). These embeddings can be jointly ingested by learning additional projections to expand the input dimension of the cross-attention layers in the UNet [5]. The new</p>
<p>projections are initialized as zeros to enable gradual learning from the pretrained state. We empirically discover that incorporating action conditions through cross-attention layers yields faster convergence and stronger controllability compared to other approaches such as additive embeddings [128, 136].
Efficient Learning. We learn action controllability after the first training phase. Since the number of total iterations is crucial for diffusion training [5, 22, 32, 99], we separate action control learning into two stages. In the first stage, we train our model at a low resolution ( $320 \times 576$ ), which achieves $3.5 \times$ higher training throughput compared to the original resolution ( $576 \times 1024$ ). This stage constitutes the majority of training iterations. Then, we finetune the model at the desired resolution ( $576 \times 1024$ ) for a short duration, so that the learned controllability can effectively cater to high-resolution prediction.</p>
<p>However, tuning the UNet [5] at a lower resolution directly may undermine the high-fidelity prediction ability. Conversely, freezing all UNet weights and training the new projections alone would precipitate a quality decline (see Appendix D), suggesting the necessity to make the UNet adaptable. To solve this, we freeze the pretrained UNet and introduce parameter-efficient LoRA adapters [55] to each attention layer. After training, the low-rank matrices can be seamlessly integrated with the frozen weights, without introducing extra inference latency. Thus, the pretrained weights remain intact when training at the low resolution, avoiding deterioration of the pretrained high-fidelity prediction ability.
Since the parameters of the camera and vehicle are unavailable for open-world scenarios, it seems impossible to obtain multiple equivalent action conditions simultaneously at inference time. Additionally, it will entail prohibitively expensive training to encompass all possible combinations of action conditions. Hence, unlike common practices that activate all conditions during training, we enforce the independence of different action formats by enabling only one of them for each training sample. The remaining action conditions will be filled with zeros as unconditional inputs. As demonstrated in Appendix D, this simple constraint prevents the squandering of training cost on action combinations and maximizes the learning efficiency of each individual action mode within the same training steps.
Collaborative Training. Note that the aforementioned action conditions are not available in OpenDVYouTube [136]. On the other hand, nuScenes [10] has adequate annotations to derive these conditions. To maintain generalization and learn controllability in tandem, we introduce a collaborative training strategy by utilizing the samples from both datasets, with the action conditions for OpenDV-YouTube set to zero. The action control learning phase adopts the same loss as Eq. (6). By learning from two complementary datasets, Vista gains versatile controllability that are generalizable to novel datasets.</p>
<h1>3.3 Generalizable Reward Function</h1>
<p>One application of world models is to evaluate actions by engaging a reward module [40, 42, 43, 76]. Drive-WM [127] establishes a reward using external detectors [82, 84]. However, these detectors are developed on a particular dataset [10], which may become a bottleneck for reward estimation in arbitrary scenarios. On the other hand, Vista has ingested millions of human driving logs, exhibiting strong generalization across scenes. Based on the observation that out-of-distribution conditions will lead to increased diversity in generation [28, 60], we utilize the prediction uncertainty from Vista itself as the source of our reward. Different from Drive-WM, our reward function seamlessly inherits the generalization of Vista without resorting to external models. Specifically, we estimate uncertainty via conditional variance. For reliable approximation, we denoise from randomly sampled noise with the same condition frame $\boldsymbol{c}$ and action $\boldsymbol{a}$ for $M$ rounds. Our reward function $R(\boldsymbol{c}, \boldsymbol{a})$ is then defined as the exponential of averaged negative conditional variance:</p>
<p>$$
\begin{gathered}
\mu^{\prime}=\frac{1}{M} \sum_{m} D_{\theta}^{(m)}(\hat{\boldsymbol{n}} ; \boldsymbol{c}, \boldsymbol{a}) \
R(\boldsymbol{c}, \boldsymbol{a})=\exp \left[\operatorname{avg}\left(-\frac{1}{M-1} \sum_{m}\left(D_{\theta}^{(m)}(\hat{\boldsymbol{n}} ; \boldsymbol{c}, \boldsymbol{a})-\mu^{\prime}\right)^{2}\right)\right]
\end{gathered}
$$</p>
<p>where avg(·) averages all latent values within the video clip. Based on this formulation, unfavorable actions with larger uncertainties will lead to lower rewards. In contrast to commonly used evaluation protocols (e.g., the L2 error), our reward function can evaluate actions without referring to the ground truth actions. Note that we do not normalize the estimated rewards for the simplicity of definition, but it is straightforward to amplify the relative contrast by rescaling the estimated rewards with a factor.</p>
<p>Table 2: Comparison of prediction fidelity on nuScenes validation set. Vista achieves encouraging results that outperform the state-of-the-art driving world models with a significant performance gain.</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>DriveGAN</th>
<th>DriveDreamer</th>
<th>WoVoGen</th>
<th>Drive-WM</th>
<th>GenAD</th>
<th>Vista</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>[102]</td>
<td>[125]</td>
<td>[90]</td>
<td>[127]</td>
<td>[136]</td>
<td>(Ours)</td>
</tr>
<tr>
<td>FID $\downarrow$</td>
<td>73.4</td>
<td>52.6</td>
<td>27.6</td>
<td>15.8</td>
<td>15.4</td>
<td>6.9</td>
</tr>
<tr>
<td>FVD $\downarrow$</td>
<td>502.3</td>
<td>452.0</td>
<td>417.7</td>
<td>122.7</td>
<td>184.0</td>
<td>89.4</td>
</tr>
</tbody>
</table>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Driving futures predicted by different models using the same condition frame. We contrast Vista to publicly available video generation models using their default configurations. Whilst previous models produce misaligned and corrupted results, Vista does not suffer from these caveats.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: [Top]: Long-horizon prediction. Vista can forecast 15 seconds high-resolution futures without much degradation, encompassing long driving distances. The length of the blue lines indicate the duration of the longest prediction showcased by previous works. [Bottom]: Long-term extension results of SVD. SVD fails to generate consistent high-fidelity videos autoregressively as Vista does.</p>
<h2>4 Experiments</h2>
<p>In this section, we first demonstrate Vista's strengths in generalization and fidelity in Sec. 4.1. We then show the impact of action controls in Sec. 4.2. We also substantiate the efficacy of the proposed reward function in Sec. 4.3. Finally, we conduct ablation studies on our key designs in Sec. 4.4. For more implementation details and experimental results, please refer to Appendix C and Appendix D.</p>
<h3>4.1 Comparisons of Generalization and Fidelity</h3>
<p>Automatic Evaluation. Since none of the driving world models are publicly accessible, we compare these methods with their quantitative results on nuScenes. We filter 5369 valid samples from the validation set to conduct FID [47] and FVD [115] evaluation. For FID evaluation, we crop and resize the predicted frames to the resolution of 256×448. For FVD evaluation, we use all 25 frames in each video clip and downsample them to 224×224 following LVDM [46]. Table 2 reports the results of all methods. In both metrics, Vista surpasses previous driving world models with a considerable margin.</p>
<p>Human Evaluation. To analyze the generalization of Vista across different datasets, we compare it against three prominent general-purpose video generators trained on web-scale data [5, 133, 144] (see Fig. 5). It is known that automatic metrics like FVD [115] cannot conclusively reveal perceptual</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Human evaluation results. The value denotes the percentage of the times that one model is preferred over the other. Vista outperforms existing works in both metrics.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Efficacy of action controls. Applying action controls will produce more similar predictions to the real data.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Versatile action controllability. Vista can predict the corresponding outcomes in response to multi-modal action conditions across diverse scenarios. More results can be found in Appendix E.
quality [3, 6, 32, 130, 136], let alone real-world dynamics. Therefore, we opt for human evaluation for more faithful analysis. Following recent advances [3, 5, 6, 15, 16, 32, 122, 126], we adopt the Two-Alternative Forced Choice protocol. Specifically, participants are presented with a side-by-side video pair and asked to choose the video they deem better on two orthogonal aspects: visual quality and motion rationality. To avoid potential bias, we crop each video to a fixed aspect ratio, downsample them to the same resolution, and trim the excess frames when Vista generates longer videos than others. We only feed one condition frame to align with other models. To ensure the variety of scenes, we uniformly assemble 60 scenes from four representative datasets: OpenDV-YouTube-val [136], nuScenes [10], Waymo [112], and CODA [79]. These datasets collectively exemplify the intricacy and diversity of real-world driving, e.g., OpenDV-YouTube-val includes geofenced districts, Waymo offers a unique domain compared to our training data, and CODA contains extremely challenging corner cases. We collect a total of 2640 answers from 33 participants. As presented in Fig. 7, Vista outperforms all baselines on both aspects, demonstrating its profound comprehension of the driving dynamics. Further, unlike other models that are only applicable for short-term generation, Vista can accommodate more dynamic priors and produce coherent long-horizon rollouts as shown in Fig. 6.</p>
<h1>4.2 Results of Action Controllability</h1>
<p>Quantitative Results. To evaluate the impact of action controls, we divide the validation set of both nuScenes and the unseen Waymo dataset into four subsets according to our command categories. We then generate predictions using different modalities of the ground truth actions. The FVD score is measured on each subset and then averaged. A lower FVD score reflects a closer distribution to the ground truth videos, indicating that the predictions exhibit more resemblance to each specific type of behavior. Fig. 8 shows that our action controls can emulate the corresponding movements effectively.</p>
<p>We also introduce a new metric named Trajectory Difference to assess control consistency. Following GenAD [136], we train an inverse dynamics model (IDM) that estimates the corresponding trajectory from a video clip. An illustration of IDM is shown in Fig. 13. We then send Vista's prediction to the IDM and calculate the L2 difference between the ground truth trajectory and the estimated trajectory.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: [Left]: Average reward on Waymo with different L2 errors. [Right]: Case study. The relative contrast of our reward can properly assess the actions that the L2 error fails to judge.</p>
<p>Figure 11: Effect of dynamic priors. Injecting more dynamic priors yields more consistent future motions with the ground truth, such as the motions of the white vehicle and the billboard on the left.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 12: [Left]: Effect of dynamics enhancement loss. The model supervised by the dynamics enhancement loss generates more realistic dynamics. In the first example, instead of remaining static, the front car moves forward normally. In the second example, when the ego-vehicle steers right, the trees shift towards the left naturally adhering to the real-world geometric rules. [Right]: Effect of structure preservation loss. The proposed loss yields a clearer outline of the objects as they move.</p>
<p>The differences are measured over 2 seconds. The lower the trajectory difference, the stronger the control consistency Vista exhibits. We conduct the experiments on nuScenes and Waymo. For each dataset, we collect a subset that contains 537 samples. As reported in Table 3, Vista can be effectively controlled by different modalities of actions, resulting in more consistent motions to the ground truth.
Qualitative Results. Fig. 9 exhibits the versatile action controllability of our model. Vista can be effectively controlled by multi-modal actions, even in unseen scenarios beyond the training domain. In Appendix E, we also showcase the counterfactual reasoning ability of Vista using abnormal actions.</p>
<h1>4.3 Results of Reward Modeling</h1>
<p>To validate the efficacy of our reward function, we jitter the ground truth trajectories into a series of inferior trajectories. Specifically, we compute the standard deviation of each waypoint from the nuScenes training set as prior distributions. These priors are jointly rescaled to sample perturbations with different L2 errors. The perturbations are then added as offsets to the ground truth trajectories. To ensure the plausibility of sampled trajectories, we adopt an explicit correlating strategy [35, 95] to regularize offset sampling and recursively sample new trajectories until their offsets are consistent in tendencies. To demonstrate the generality of our reward function, we conduct reward estimation on Waymo [112], which is unseen in training. This is done by uniformly sampling from each command category on Waymo validation set, resulting in 1500 cases in total. We compare the average reward of the trajectories with varying L2 errors in Fig. 10. Our reward decreases when the deviation from the ground truth increases, underscoring the potential of our approach to serve as a viable reward function. It also holds the promise to remedy the irrationality in current evaluation protocols for planning $[18,83,141]$, such as the L2 error shown in Fig. 10. More in-depth analysis of rewards, including sensitivity to hyperparameters and reward of other actions, are provided in Appendix D.</p>
<p>Table 3: Impacts of different action conditions and dynamic priors. By applying action conditions and dynamic priors, Vista can predict motion that is more consistent compared to the ground truth.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Condition</th>
<th style="text-align: center;">Average Trajectory Difference $\downarrow$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">with 1 prior</td>
<td style="text-align: center;">with 2 priors</td>
<td style="text-align: center;">with 3 priors</td>
</tr>
<tr>
<td style="text-align: center;">nuScenes</td>
<td style="text-align: center;">GT video</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.379</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">action-free</td>
<td style="text-align: center;">3.785</td>
<td style="text-align: center;">2.597</td>
<td style="text-align: center;">1.820</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+ goal point</td>
<td style="text-align: center;">2.869</td>
<td style="text-align: center;">2.192</td>
<td style="text-align: center;">1.585</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+ command</td>
<td style="text-align: center;">3.129</td>
<td style="text-align: center;">2.403</td>
<td style="text-align: center;">1.593</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+ angle \&amp; speed</td>
<td style="text-align: center;">1.562</td>
<td style="text-align: center;">1.123</td>
<td style="text-align: center;">0.832</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+ trajectory</td>
<td style="text-align: center;">1.559</td>
<td style="text-align: center;">1.148</td>
<td style="text-align: center;">0.835</td>
</tr>
<tr>
<td style="text-align: center;">Waymo</td>
<td style="text-align: center;">GT video</td>
<td style="text-align: center;">0.893</td>
<td style="text-align: center;">0.893</td>
<td style="text-align: center;">0.893</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">action-free</td>
<td style="text-align: center;">3.646</td>
<td style="text-align: center;">2.901</td>
<td style="text-align: center;">2.052</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+ command</td>
<td style="text-align: center;">3.160</td>
<td style="text-align: center;">2.561</td>
<td style="text-align: center;">1.902</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+ trajectory</td>
<td style="text-align: center;">1.187</td>
<td style="text-align: center;">1.147</td>
<td style="text-align: center;">1.140</td>
</tr>
</tbody>
</table>
<h1>4.4 Ablation Study</h1>
<p>Dynamic Priors. Fig. 11 visualizes the outcomes of using different orders of dynamic priors. The order of priors corresponds to the number of condition frames. It shows that dynamic priors play a pivotal role in long-horizon rollouts, where the coherence with respect to historical frames is essential.</p>
<p>To further demonstrate the efficacy of dynamic priors, we conduct a quantitative evaluation in Table 3. Specifically, we use the IDM in Sec. 4.2 to infer the trajectories of the predicted videos with different orders of dynamic priors. The diminishing differences in trajectory suggest that introducing more priors can effectively improve the consistency between prediction and ground truth.
Auxiliary Supervisions. To verify the effectiveness of the two losses proposed in Sec. 3.1, we devise two additional variants by individually ablating each loss from a variant that incorporates both losses. We qualitatively compare their effects through Fig. 12, which confirms that the dynamics enhancement loss can promote the learning of real-world dynamics, and the structure preservation loss can reinforce the prediction of structural details.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 13: An illustration of the IDM experiments in Table 3.</p>
<h2>5 Conclusion</h2>
<p>In this paper, we introduce Vista, a generalizable driving world model with enhanced fidelity and controllability. Based on our systematic investigations, Vista is able to predict realistic and continuous futures at high spatiotemporal resolution. It also possesses versatile action controllability that is generalizable to unseen scenarios. Moreover, it can be formulated as a reward function to evaluate actions. We hope Vista will usher in broader interest in developing generalizable autonomy systems.
Limitations and future work. As an early endeavor, Vista still exhibits some limitations with respect to computation efficiency, quality maintenance, and training scale. Our future work will look into applying our method to scalable architectures [54, 97]. More discussions are included in Appendix A.</p>
<h2>Acknowledgments</h2>
<p>This work is supported by National Key R\&amp;D Program of China (2022ZD0160104), National Natural Science Foundation of China (62206172), and Shanghai Committee of Science and Technology (23YF1462000). This work is also partially supported by the BMBF (Tübingen AI Center, FKZ: 01IS18039A), the DFG (SFB 1233, TP 17, project number: 276693517), and the EXC (number 2064/1 - project number: 390727645). We thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Kashyap Chitta. We also appreciate Zetong Yang, Chonghao Sima, Linyan Huang, and the rest members from OpenDriveLab for valuable feedback. We express our sincere gratitude to all anonymous participants for helping with the human evaluation.</p>
<h1>References</h1>
<p>[1] Anurag Ajay, Seungwook Han, Yilun Du, Shuang Li, Abhi Gupta, Tommi Jaakkola, Josh Tenenbaum, Leslie Kaelbling, Akash Srivastava, and Pulkit Agrawal. Compositional Foundation Models for Hierarchical Planning. In NeurIPS, 2023. 19
[2] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video Pretraining (VPT): Learning to Act by Watching Unlabeled Online Videos. In NeurIPS, 2022. 19
[3] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, Yuanzhen Li, Michael Rubinstein, Tomer Michaeli, Oliver Wang, Deqing Sun, Tali Dekel, and Inbar Mosseri. Lumiere: A Space-Time Diffusion Model for Video Generation. arXiv preprint arXiv:2401.12945, 2024. 4, 8
[4] Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Walke, Chelsea Finn, Aviral Kumar, and Sergey Levine. Zero-Shot Robotic Manipulation with Pretrained Image-Editing Diffusion Models. In $I C L R, 2024.19$
[5] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets. arXiv preprint arXiv:2311.15127, 2023. 2, 3, 5, 6, 7, 8, 21, 22, 23, 24, 26
[6] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align Your Latents: High-Resolution Video Synthesis with Latent Diffusion Models. In CVPR, 2023. 8, 20, 21
[7] Daniel Bogdoll, Yitian Yang, and J Marius Zöllner. MUVO: A Multimodal Generative World Model for Autonomous Driving with Geometric Representations. arXiv preprint arXiv:2311.11762, 2023. 21
[8] Tim Brooks, Aleksander Holynski, and Alexei A Efros. InstructPix2Pix: Learning to Follow Image Editing Instructions. In CVPR, 2023. 20
[9] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, Yusuf Aytar, Sarah Bechtle, Feryal Behbahani, Stephanie Chan, Nicolas Heess, Lucy Gonzalez, Simon Osindero, Sherjil Ozair, Scott Reed, Jingwei Zhang, Konrad Zolna, Jeff Clune, Nando de Freitas, Satinder Singh, and Tim Rocktäschel. Genie: Generative Interactive Environments. arXiv preprint arXiv:2402.15391, 2024. 19, 20, 21
[10] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yuxin Pan, Giancarlo Baldan, and Oscar Beijbom. nuScenes: A Multimodal Dataset for Autonomous Driving. In CVPR, 2020. 6, 8, 20, 21, 23, 26
[11] Holger Caesar, Juraj Kabzan, Kok Seang Tan, Whye Kit Fong, Eric Wolff, Alex Lang, Luke Fletcher, Oscar Beijbom, and Sammy Omari. nuPlan: A Closed-Loop ML-based Planning Benchmark for Autonomous Vehicles. In CVPR Workshops, 2021. 20
[12] Sergio Casas, Abbas Sadat, and Raquel Urtasun. MP3: A Unified Model to Map, Perceive, Predict and Plan. In CVPR, 2021. 1, 5, 21
[13] Jun Cen, Chenfei Wu, Xiao Liu, Shengming Yin, Yixuan Pei, Jinglong Yang, Qifeng Chen, Nan Duan, and Jianguo Zhang. Using Left and Right Brains Together: Towards Vision and Language Planning. arXiv preprint arXiv:2402.10534, 2024. 19
[14] Dian Chen, Brady Zhou, Vladlen Koltun, and Philipp Krähenbühl. Learning by Cheating. In CoRL, 2019. 1,21
[15] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, and Ying Shan. VideoCrafter1: Open Diffusion Models for High-Quality Video Generation. arXiv preprint arXiv:2310.19512, 2023. 8, 21
[16] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models. In CVPR, 2024. 8
[17] Kai Chen, Enze Xie, Zhe Chen, Yibo Wang, Lanqing Hong, Zhenguo Li, and Dit-Yan Yeung. GeoDiffusion: Text-Prompted Geometric Control for Object Detection Data Generation. In ICLR, 2024. 4
[18] Li Chen, Penghao Wu, Kashyap Chitta, Bernhard Jaeger, Andreas Geiger, and Hongyang Li. End-to-End Autonomous Driving: Challenges and Frontiers. arXiv preprint arXiv:2306.16927, 2023. 1, 9, 19
[19] Shoufa Chen, Mengmeng Xu, Jiawei Ren, Yuren Cong, Sen He, Yanping Xie, Animesh Sinha, Ping Luo, Tao Xiang, and Juan-Manuel Perez-Rua. GenTron: Delving Deep into Diffusion Transformers for Image and Video Generation. In CVPR, 2024. 20</p>
<p>[20] Kashyap Chitta, Daniel Dauner, and Andreas Geiger. SLEDGE: Synthesizing Simulation Environments for Driving Agents with Generative Models. arXiv preprint arXiv:2403.17933, 2024. 21
[21] Kashyap Chitta, Aditya Prakash, Bernhard Jaeger, Zehao Yu, Katrin Renz, and Andreas Geiger. TransFuser: Imitation with Transformer-based Sensor Fusion for Autonomous Driving. IEEE TPAMI, 2023. 1, 5,21
[22] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, Matthew Yu, Abhishek Kadian, Filip Radenovic, Dhruv Mahajan, Kunpeng Li, Yue Zhao, Vladan Petrovic, Mitesh Kumar Singh, Simran Motwani, Yi Wen, Yiwen Song, Roshan Sumbaly, Vignesh Ramanathan, Zijian He, Peter Vajda, and Devi Parikh. Emu: Enhancing Image Generation Models using Photogenic Needles in a Haystack. arXiv preprint arXiv:2309.15807, 2023. 6
[23] Daniel Dauner, Marcel Hallgarten, Tianyu Li, Xinshuo Weng, Zhiyu Huang, Zetong Yang, Hongyang Li, Igor Gilitschenski, Boris Ivanovic, Marco Pavone, et al. NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and Benchmarking. arXiv preprint arXiv:2406.15349, 2024. 20
[24] Prafulla Dhariwal and Alexander Nichol. Diffusion Models Beat GANs on Image Synthesis. In NeurIPS, 2021. 21
[25] Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning Universal Policies via Text-Guided Video Generation. In NeurIPS, 2023. 19
[26] Yilun Du, Sherry Yang, Pete Florence, Fei Xia, Ayzaan Wahid, brian ichter, Pierre Sermanet, Tianhe Yu, Pieter Abbeel, Joshua B. Tenenbaum, Leslie Pack Kaelbling, Andy Zeng, and Jonathan Tompson. Video Language Planning. In ICLR, 2024. 19, 21
[27] Frederik Ebert, Chelsea Finn, Sudeep Dasari, Annie Xie, Alex Lee, and Sergey Levine. Visual Foresight: Model-based Deep Reinforcement Learning for Vision-based Robotic Control. arXiv preprint arXiv:1812.00568, 2018. 19, 21
[28] Alejandro Escontrela, Ademi Adeniji, Wilson Yan, Ajay Jain, Xue Bin Peng, Ken Goldberg, Youngwoon Lee, Danijar Hafner, and Pieter Abbeel. Video Prediction Models as Rewards for Reinforcement Learning. In NeurIPS, 2023. 6, 19
[29] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming Transformers for High-Resolution Image Synthesis. In CVPR, 2021. 21
[30] Chelsea Finn and Sergey Levine. Deep Visual Foresight for Planning Robot Motion. In ICRA, 2017. 19, 21
[31] Zeyu Gao, Yao Mu, Ruoyan Shen, Chen Chen, Yangang Ren, Jianyu Chen, Shengbo Eben Li, Ping Luo, and Yanfeng Lu. Enhance Sample Efficiency and Robustness of End-to-End Urban Autonomous Driving via Semantic Masked World Model. In NeurIPS Workshops, 2022. 21
[32] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning. arXiv preprint arXiv:2311.10709, 2023. 4, 6, 8, 20, 21
[33] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets. In NeurIPS, 2014. 21
[34] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning. In ICLR, 2024. 21
[35] Agrim Gupta, Stephen Tian, Yunzhi Zhang, Jiajun Wu, Roberto Martín-Martín, and Li Fei-Fei. MaskViT: Masked Visual Pre-Training for Video Prediction. In ICLR, 2023. 9, 19, 21, 23
[36] Tarun Gupta, Wenbo Gong, Chao Ma, Nick Pawlowski, Agrin Hilmkil, Meyer Scetbon, Ade Famoti, Ashley Juan Llorens, Jianfeng Gao, Stefan Bauer, Bernhard Kragic, Danica an Schölkopf, and Cheng Zhang. The Essential Role of Causality in Foundation World Models for Embodied AI. arXiv preprint arXiv:2402.06665, 2024. 25
[37] Wes Gurnee and Max Tegmark. Language Models Represent Space and Time. In ICLR, 2024. 21
[38] Nicholas Guttenberg and CrossLabs. Diffusion with Offset Noise, 2023. 22
[39] David Ha and Jürgen Schmidhuber. Recurrent World Models Facilitate Policy Evolution. In NeurIPS, 2018. 21
[40] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to Control: Learning Behaviors by Latent Imagination. arXiv preprint arXiv:1912.01603, 2019. 6, 21
[41] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning Latent Dynamics for Planning from Pixels. In ICML, 2019. 21
[42] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering Atari with Discrete World Models. In ICLR, 2021. 6, 21</p>
<p>[43] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering Diverse Domains through World Models. arXiv preprint arXiv:2301.04104, 2023. 6, 21
[44] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with Language Model is Planning with World Model. In EMNLP, 2023. 21
[45] Haoran He, Chenjia Bai, Ling Pan, Weinan Zhang, Bin Zhao, and Xuelong Li. Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning. arXiv preprint arXiv:2402.14407, 2024. 21
[46] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent Video Diffusion Models for High-Fidelity Long Video Generation. arXiv preprint arXiv:2211.13221, 2022. 7, 21
[47] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. In NeurIPS, 2017. 7
[48] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. In NeurIPS, 2020. 21
[49] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded Diffusion Models for High Fidelity Image Generation. JMLR, 2022. 3, 20, 22
[50] Jonathan Ho and Tim Salimans. Classifier-Free Diffusion Guidance. arXiv preprint arXiv:2207.12598, 2022. 22
[51] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video Diffusion Models. arXiv preprint arXiv:2204.03458, 2022. 21
[52] Anthony Hu, Gianluca Corrado, Nicolas Griffiths, Zachary Murez, Corina Gurau, Hudson Yeo, Alex Kendall, Roberto Cipolla, and Jamie Shotton. Model-based Imitation Learning for Urban Driving. In NeurIPS, 2022. 21
[53] Anthony Hu, Zak Murez, Nikhil Mohan, Sofía Dudas, Jeffrey Hawke, Vijay Badrinarayanan, Roberto Cipolla, and Alex Kendall. FIERY: Future Instance Prediction in Bird's-Eye View from Surround Monocular Cameras. In ICCV, 2021. 19, 21
[54] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. GAIA-1: A Generative World Model for Autonomous Driving. arXiv preprint arXiv:2309.17080, 2023. 1, 2, 3, 5, 10, 20, 21
[55] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. In ICLR, 2022. 6, 22
[56] Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, Junchi Yan, and Dacheng Tao. ST-P3: End-to-end Vision-based Autonomous Driving via Spatial-Temporal Feature Learning. In ECCV, 2022. 1, 21, 22
[57] Yafei Hu, Quanting Xie, Vidhi Jain, Jonathan Francis, Jay Patrikar, Nikhil Keetha, Seungchan Kim, Yaqi Xie, Tianyi Zhang, Shibo Zhao, Yu Quan Chong, Chen Wang, Katia Sycara, Matthew Johnson-Roberson, Dhruv Batra, Xiaolong Wang, Sebastian Scherer, Zsolt Kira, Fei Xia, and Yonatan Bisk. Toward GeneralPurpose Robots via Foundation Models: A Survey and Meta-Analysis. arXiv preprint arXiv:2312.08782, 2023. 1, 20
[58] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu Qiao, and Hongyang Li. PlanningOriented Autonomous Driving. In CVPR, 2023. 1, 5, 21, 22
[59] Zhiting Hu and Tianmin Shu. Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning. arXiv preprint arXiv:2312.05230, 2023. 21
[60] Tao Huang, Guangqi Jiang, Yanjie Ze, and Huazhe Xu. Diffusion Reward: Learning Rewards via Conditional Video Diffusion. arXiv preprint arXiv:2312.14134, 2023. 6, 19
[61] Fan Jia, Weixin Mao, Yingfei Liu, Yucheng Zhao, Yuqing Wen, Chi Zhang, Xiangyu Zhang, and Tiancai Wang. ADriver-I: A General World Model for Autonomous Driving. arXiv preprint arXiv:2311.13549, 2023. 1, 2, 5, 21
[62] Xiaosong Jia, Yulu Gao, Li Chen, Junchi Yan, Patrick Langechuan Liu, and Hongyang Li. DriveAdapter: Breaking the Coupling Barrier of Perception and Planning in End-to-End Autonomous Driving. In ICCV, 2023. 5
[63] Xiaosong Jia, Penghao Wu, Li Chen, Jiangwei Xie, Conghui He, Junchi Yan, and Hongyang Li. Think Twice Before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving. In CVPR, 2023. 5
[64] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang. VAD: Vectorized Scene Representation for Efficient Autonomous Driving. In ICCV, 2023. 1, 21, 22</p>
<p>[65] Michael I Jordan and David E Rumelhart. Forward Models: Supervised Learning with a Distal Teacher. Cognitive Science, 1992. 19
[66] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the Design Space of Diffusion-based Generative Models. In NeurIPS, 2022. 2, 22
[67] Tarasha Khurana, Peiyun Hu, David Held, and Deva Ramanan. Point Cloud Forecasting as a Proxy for 4D Occupancy Forecasting. In CVPR, 2023. 21
[68] Seung Wook Kim, Jonah Philion, Antonio Torralba, and Sanja Fidler. DriveGAN: Towards a Controllable High-Quality Neural Simulation. In CVPR, 2021. 2, 3, 21
[69] Seung Wook Kim, Yuhao Zhou, Jonah Philion, Antonio Torralba, and Sanja Fidler. Learning to Simulate Dynamic Environments with GameGAN. In CVPR, 2020. 21
[70] Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. arXiv preprint arXiv:1312.6114, 2013. 21
[71] Po-Chen Ko, Jiayuan Mao, Yilun Du, Shao-Hua Sun, and Joshua B Tenenbaum. Learning to Act from Actionless Videos through Dense Correspondences. In ICLR, 2024. 19
[72] Jing Yu Koh, Honglak Lee, Yinfei Yang, Jason Baldridge, and Peter Anderson. Pathdreamer: A World Model for Indoor Navigation. In ICCV, 2021. 21
[73] Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, Yong Cheng, Ming-Chang Chiu, Josh Dillon, Irfan Essa, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, David Ross, Grant Schindler, Mikhail Sirotenko, Kihyuk Sohn, Krishna Somandepalli, Huisheng Wang, Jimmy Yan, Ming-Hsuan Yang, Xuan Yang, Bryan Seybold, and Lu Jiang. VideoPoet: A Large Language Model for Zero-Shot Video Generation. arXiv preprint arXiv:2312.14125, 2023. 4
[74] Hanyang Kong, Dongze Lian, Michael Bi Mi, and Xinchao Wang. DreamDrone. arXiv preprint arXiv:2312.08746, 2023. 5, 21
[75] Lei Lai, Zhongkai Shangguan, Jimuyang Zhang, and Eshed Ohn-Bar. XVO: Generalized Visual Odometry via Cross-Modal Self-Training. In ICCV, 2023. 19
[76] Yann LeCun. A Path towards Autonomous Machine Intelligence. Open Review, 62, 2022. 1, 6, 19, 20, 21
[77] Hongyang Li, Yang Li, Huijie Wang, Jia Zeng, Huilin Xu, Pinlong Cai, Li Chen, Junchi Yan, Feng Xu, Lu Xiong, Jingdong Wang, Futang Zhu, Chunjing Xu, Tiancai Wang, Fei Xia, Beipeng Mu, Zhihui Peng, Dahua Lin, and Yu Qiao. Open-sourced Data Ecosystem in Autonomous Driving: the Present and Future. arXiv preprint arXiv:2312.03408, 2023. 21
[78] Hongyang Li, Chonghao Sima, Jifeng Dai, Wenhai Wang, Lewei Lu, Huijie Wang, Jia Zeng, Zhiqi Li, Jiazhi Yang, Hanming Deng, Hao Tian, Enze Xie, Jiangwei Xie, Li Chen, Tianyu Li, Yang Li, Yulu Gao, Xiaosong Jia, Si Liu, Jianping Shi, Dahua Lin, and Yu Qiao. Delving Into the Devils of Bird’s-Eye-View Perception: A Review, Evaluation and Recipe. IEEE TPAMI, 2023. 21
[79] Kaican Li, Kai Chen, Haoyu Wang, Lanqing Hong, Chaoqiang Ye, Jianhua Han, Yukuai Chen, Wei Zhang, Chunjing Xu, Dit-Yan Yeung, Xiaodan Liang, Zhenguo Li, and Hang Xu. CODA: A Real-World Road Corner Case Dataset for Object Detection in Autonomous Driving. In ECCV, 2022. 8, 26
[80] Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task. In ICLR, 2023. 21
[81] Qifeng Li, Xiaosong Jia, Shaobo Wang, and Junchi Yan. Think2Drive: Efficient Reinforcement Learning by Thinking in Latent World Model for Quasi-Realistic Autonomous Driving (in CARLA-v2). arXiv preprint arXiv:2402.16720, 2024. 21
[82] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. BEVFormer: Learning Bird’s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers. In ECCV, 2022. 6, 19
[83] Zhiqi Li, Zhiding Yu, Shiyi Lan, Jiahan Li, Jan Kautz, Tong Lu, and Jose M Alvarez. Is Ego Status All You Need for Open-Loop End-to-End Autonomous Driving? In CVPR, 2024. 1, 9, 19, 21
[84] Bencheng Liao, Shaoyu Chen, Xinggang Wang, Tianheng Cheng, Qian Zhang, Wenyu Liu, and Chang Huang. MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction. In ICLR, 2023. 6, 19
[85] Jessy Lin, Yuqing Du, Olivia Watkins, Danijar Hafner, Pieter Abbeel, Dan Klein, and Anca Dragan. Learning to Model the World with Language. arXiv preprint arXiv:2308.01399, 2023. 21
[86] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World Model on Million-Length Video and Language With RingAttention. arXiv preprint arXiv:2402.08268, 2024. 21</p>
<p>[87] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. arXiv preprint arXiv:1711.05101, 2017. 22
[88] William Lotter, Gabriel Kreiman, and David Cox. Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning. In $I C L R, 2017.21$
[89] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps. In NeurIPS, 2022. 20
[90] Jiachen Lu, Ze Huang, Jiahui Zhang, Zeyu Yang, and Li Zhang. WoVoGen: World Volume-Aware Diffusion for Controllable Multi-Camera Driving Scene Generation. arXiv preprint arXiv:2312.02934, 2023. 1, 2, 5, 7, 21
[91] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference. arXiv preprint arXiv:2310.04378, 2023. 20
[92] Russell Mendonca, Shikhar Bahl, and Deepak Pathak. Structured World Models from Human Videos. In RSS, 2023. 21
[93] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On Distillation of Guided Diffusion Models. In CVPR, 2023. 20
[94] Vincent Micheli, Eloi Alonso, and François Fleuret. Transformers are Sample-Efficient World Models. In $I C L R, 2023.21$
[95] Anusha Nagabandi, Kurt Konolige, Sergey Levine, and Vikash Kumar. Deep Dynamics Models for Learning Dexterous Manipulation. In CoRL, 2020. 9, 23
[96] Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and Satinder Singh. Action-Conditional Video Prediction using Deep Networks in Atari Games. In NeurIPS, 2015. 21
[97] William Peebles and Saining Xie. Scalable Diffusion Models with Transformers. In ICCV, 2023. 10, 20
[98] AJ Piergiovanni, Alan Wu, and Michael S Ryoo. Learning Real-World Robot Policies by Dreaming. In IROS, 2019. 21
[99] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis. In $I C L R, 2024.6,20$
[100] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-Resolution Image Synthesis with Latent Diffusion Models. In CVPR, 2022. 21
[101] Tim Salimans and Jonathan Ho. Progressive Distillation for Fast Sampling of Diffusion Models. In $I C L R$, 2023. 20
[102] Eder Santana and George Hotz. Learning a Driving Simulator. arXiv preprint arXiv:1608.01230, 2016. $2,5,7,21$
[103] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation. arXiv preprint arXiv:2403.12015, 2024. 20
[104] Ingmar Schubert, Jingwei Zhang, Jake Bruce, Sarah Bechtle, Emilio Parisotto, Martin Riedmiller, Jost Tobias Springenberg, Arunkumar Byravan, Leonard Hasenclever, and Nicolas Heess. A Generalist Dynamics Model for Control. arXiv preprint arXiv:2305.10912, 2023. 21
[105] Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip Bachman. Data-Efficient Reinforcement Learning with Self-Predictive Representations. In $I C L R, 2021.21$
[106] Dhruv Shah, Ajay Sridhar, Nitish Dashora, Kyle Stachowicz, Kevin Black, Noriaki Hirose, and Sergey Levine. ViNT: A Foundation Model for Visual Navigation. In CoRL, 2023. 19
[107] Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Ping Luo, Andreas Geiger, and Hongyang Li. DriveLM: Driving with Graph Visual Question Answering. arXiv preprint arXiv:2312.14150, 2023. 20, 21
[108] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-A-Video: Text-to-Video Generation without Text-Video Data. In $I C L R, 2023.21$
[109] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising Diffusion Implicit Models. In ICLR, 2021. 22
[110] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency Models. In ICML, 2023. 20
[111] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based Generative Modeling through Stochastic Differential Equations. In ICLR, 2021. 2</p>
<p>[112] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott M. Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang, Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. Scalability in Perception for Autonomous Driving: Waymo Open Dataset. In CVPR, 2020. 8, 9, 20, 25, 26
[113] Richard S Sutton. The Quest for a Common Model of the Intelligent Decision Maker. arXiv preprint arXiv:2202.13252, 2022. 20, 21
[114] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains. In NeurIPS, 2020. 5, 21
[115] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards Accurate Generative Models of Video: A New Metric \&amp; Challenges. arXiv preprint arXiv:1812.01717, 2018. 7
[116] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is All You Need. In NeurIPS, 2017. 5, 21
[117] Vikram Voleti, Alexia Jolicoeur-Martineau, and Chris Pal. MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation. In NeurIPS, 2022. 21
[118] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. SV3D: Novel Multi-View Synthesis and 3D Generation from a Single Image using Latent Video Diffusion. arXiv preprint arXiv:2403.12008, 2024. 22
[119] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating Videos with Scene Dynamics. In NeurIPS, 2016. 21
[120] Hanqing Wang, Wei Liang, Luc Van Gool, and Wenguan Wang. DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation. In ICCV, 2023. 21
[121] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. ModelScope Text-to-Video Technical Report. arXiv preprint arXiv:2308.06571, 2023. 21
[122] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, and Jiaying Liu. VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation. arXiv preprint arXiv:2305.10874, 2023. 8
[123] Xiang Wang, Shiwei Zhang, Hangjie Yuan, Zhiwu Qing, Biao Gong, Yingya Zhang, Yujun Shen, Changxin Gao, and Nong Sang. A Recipe for Scaling up Text-to-Video Generation with Text-Free Videos. In CVPR, 2024. 4
[124] Xiang Wang, Shiwei Zhang, Han Zhang, Yu Liu, Yingya Zhang, Changxin Gao, and Nong Sang. VideoLCM: Video Latent Consistency Model. arXiv preprint arXiv:2312.09109, 2023. 20
[125] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, and Jiwen Lu. DriveDreamer: Towards Real-World-Driven World Models for Autonomous Driving. arXiv preprint arXiv:2309.09777, 2023. 1, 2, 3, 5, 7,21
[126] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, Yuwei Guo, Tianxing Wu, Chenyang Si, Yuming Jiang, Cunjian Chen, Chen Change Loy, Bo Dai, Dahua Lin, Yu Qiao, and Ziwei Liu. LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models. arXiv preprint arXiv:2309.15103, 2023. 8, 20
[127] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving. In CVPR, 2024. 1, 2, 3, 5, 6, 7, 19, 20, 21
[128] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. MotionCtrl: A Unified and Flexible Motion Controller for Video Generation. arXiv preprint arXiv:2312.03641, 2023. 6, 20, 21
[129] Benjamin Wilson, William Qi, Tanmay Agarwal, John Lambert, Jagjeet Singh, Siddhesh Khandelwal, Bowen Pan, Ratnesh Kumar, Andrew Hartnett, Jhony Kaesemodel Pontes, et al. Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting. In NeurIPS Datasets and Benchmarks, 2023. 20
[130] Jay Zhangjie Wu, Guian Fang, Haoning Wu, Xintao Wang, Yixiao Ge, Xiaodong Cun, David Junhao Zhang, Jia-Wei Liu, Yuchao Gu, Rui Zhao, Weisi Lin, Wynne Hsu, Ying Shan, and Mike Zheng Shou. Towards A Better Metric for Text-to-Video Generation. arXiv preprint arXiv:2401.07781, 2024. 8
[131] Jialong Wu, Haoyu Ma, Chaoyi Deng, and Mingsheng Long. Pre-Training Contextualized World Models with In-the-Wild Videos for Reinforcement Learning. In NeurIPS, 2023. 21</p>
<p>[132] Penghao Wu, Li Chen, Hongyang Li, Xiaosong Jia, Junchi Yan, and Yu Qiao. Policy Pre-training for Autonomous Driving via Self-supervised Geometric Modeling. In ICLR, 2023. 4
[133] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying Shan. DynamiCrafter: Animating Open-Domain Images with Video Diffusion Priors. arXiv preprint arXiv:2310.12190, 2023. 7, 22
[134] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. VideoGPT: Video Generation using VQ-VAE and Transformers. arXiv preprint arXiv:2104.10157, 2021. 21
[135] Xu Yan, Haiming Zhang, Yingjie Cai, Jingming Guo, Weichao Qiu, Bin Gao, Kaiqiang Zhou, Yue Zhao, Huan Jin, Jiantao Gao, Zhen Li, Lihui Jiang, Wei Zhang, Hongbo Zhang, Dengxin Dai, and Bingbing Liu. Forging Vision Foundation Models for Autonomous Driving: Challenges, Methodologies, and Opportunities. arXiv preprint arXiv:2401.08045, 2024. 1
[136] Jiazhi Yang, Shenyuan Gao, Yihang Qiu, Li Chen, Tianyu Li, Bo Dai, Kashyap Chitta, Penghao Wu, Jia Zeng, Ping Luo, Jun Zhang, Andreas Geiger, Yu Qiao, and Hongyang Li. Generalized Predictive Model for Autonomous Driving. In CVPR, 2024. 2, 3, 4, 6, 7, 8, 20, 21, 23, 25, 26
[137] Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel. Learning Interactive Real-World Simulators. In ICLR, 2024. 19, 21
[138] Sherry Yang, Jacob Walker, Jack Parker-Holder, Yilun Du, Jake Bruce, Andre Barreto, Pieter Abbeel, and Dale Schuurmans. Video as the New Language for Real-World Decision Making. arXiv preprint arXiv:2402.17139, 2024. 20
[139] Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. Direct-a-Video: Customized Video Generation with User-Directed Camera Movement and Object Motion. arXiv preprint arXiv:2402.03162, 2024. 21
[140] Zetong Yang, Li Chen, Yanan Sun, and Hongyang Li. Visual Point Cloud Forecasting Enables Scalable Autonomous Driving. In CVPR, 2024. 21
[141] Jiang-Tian Zhai, Ze Feng, Jinhao Du, Yongqiang Mao, Jiang-Jiang Liu, Zichang Tan, Yifu Zhang, Xiaoqing Ye, and Jingdong Wang. Rethinking the Open-Loop Evaluation of End-to-End Autonomous Driving in nuScenes. arXiv preprint arXiv:2305.10430, 2023. 9, 19
[142] Alex Zhang, Khanh Nguyen, Jens Tuyls, Albert Lin, and Karthik Narasimhan. Language-Guided World Models: A Model-based Approach to AI Control. arXiv preprint arXiv:2402.01695, 2024. 21
[143] Lunjun Zhang, Yuwen Xiong, Ze Yang, Sergio Casas, Rui Hu, and Raquel Urtasun. Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion. In ICLR, 2024. 1, 21
[144] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models. arXiv preprint arXiv:2311.04145, 2023. 4, 7, 20, 21, 22
[145] Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Xinze Chen, Guan Huang, Xiaoyi Bao, and Xingang Wang. DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation. arXiv preprint arXiv:2403.06845, 2024. 20, 21
[146] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models. In NeurIPS, 2023. 20
[147] Wenzhao Zheng, Weiliang Chen, Yuanhui Huang, Borui Zhang, Yueqi Duan, and Jiwen Lu. OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving. arXiv preprint arXiv:2311.16038, 2023. 1, 21
[148] Wenzhao Zheng, Ruiqi Song, Xianda Guo, and Long Chen. GenAD: Generative End-to-End Autonomous Driving. arXiv preprint arXiv:2402.11502, 2024. 19
[149] Yunsong Zhou, Linyan Huang, Qingwen Bu, Jia Zeng, Tianyu Li, Hang Qiu, Hongzi Zhu, Minyi Guo, Yu Qiao, and Hongyang Li. Embodied Understanding of Driving Scenarios. arXiv preprint arXiv:2403.04593, 2024. 20</p>
<h1>Appendix</h1>
<p>A Discussions ..... 19
B Related Work ..... 20
B. 1 World Models ..... 20
B. 2 Video Generation ..... 21
C Implementation Details ..... 21
C. 1 Model ..... 21
C. 2 Dataset ..... 21
C. 3 Training ..... 22
C. 4 Sampling ..... 22
C. 5 Human Evaluation ..... 22
C. 6 Reward Estimation ..... 23
C. 7 Ablation Studies ..... 23
D Additional Experiments ..... 23
D. 1 Parameter Sensitivity of Reward Estimation ..... 23
D. 2 Reward Estimation for Commands ..... 23
D. 3 Action Independence Constraint ..... 23
D. 4 Triangular Guidance Scheme ..... 24
D. 5 LoRA Adaptation ..... 24
D. 6 Action Control Consistency ..... 24
D. 7 Human Evaluation with GenAD ..... 25
E Additional Visualizations ..... 25
E. 1 Generalization Ability ..... 25
E. 2 Long-Horizon Prediction ..... 25
E. 3 Action Controllability ..... 25
E. 4 Counterfactual Reasoning Ability ..... 25
E. 5 Human Evaluation Cases ..... 26
F Licence of Assets ..... 26</p>
<h1>A Discussions</h1>
<p>To help a thorough understanding of this work, we discuss intuitive questions that might be raised.</p>
<h2>Q1. Why is at least position, velocity, and acceleration required to predict coherent futures?</h2>
<p>Position ensures the predicted future begins continuously with the current state. Velocity manifests how objects are moving, e.g., whether they are turning left or turning right. Acceleration represents how velocity changes over time, e.g., whether the surroundings are moving faster or moving slower. Without utilizing acceleration as a cue, a car overtaking the ego-vehicle may suddenly be passed by in the next autoregressive prediction step. These three priors provide essential cues to allow consistent future extension with respect to historical observation.</p>
<h2>Q2. How is the form of the proposed reward function defined?</h2>
<p>Unlike VIPER [28] and Diffusion Reward [60] that both make discrete predictions, our model predicts continuous latent. Therefore, our reward is estimated according to conditional variance rather than log-likelihood or entropy. In addition, measuring uncertainty with log-likelihood requires comparing the prediction to the ground truth. As we deploy the reward in any scenario, the approach of VIPER is infeasible for our objective. Note that our reward calculation is meticulously designed to satisfy the Kolmogorov axioms, i.e. it is non-negative and the measure of the entire sample space is $[0,1]$.</p>
<h2>Q3. Reward estimation efficiency compared to the detector-based method [127].</h2>
<p>Though our reward estimation involves multi-round denoising, it will not spend more compute than the detector-based reward function defined in Drive-WM [127]. To be specific, Drive-WM obtains the rewards from the perception results. Given that the detectors [82, 84] take image sequences as inputs, Drive-WM has to accomplish all steps of the denoising process before perception. Differently, our reward function estimates the reward with the uncertainty that originates from the world model itself without relying on other perception models. Therefore, the estimation of uncertainty does not require completing the generation process. It can be realized by only denoising each sample for a few steps. In fact, as specified in Appendix C, the total computation required for reward estimation per situation ( 10 steps, 5 rounds) is no greater than that of generating the entire video ( 50 steps for our model) as Drive-WM does. Note that the computational cost for our reward estimation can be flexibly reduced to further improve its efficiency. As shown in Fig. 14, using 5 denoising steps ( $50 \%$ of the default computation) also yields satisfactory estimations of the reward.</p>
<h2>Q4. Usage of the proposed reward function.</h2>
<p>(1) As mentioned in Sec. 4.3, the proposed reward function can potentially serve as an alternative metric of driving actions that mitigate the concerns in existing open-loop evaluation [18, 83, 141]. (2) As demonstrated in Fig. 10, better actions generally yield higher rewards with our reward function. Taking advantage of this property, there is great promise for our reward function to be used as a critic module [76], which enables model-predictive control by executing the optimal action that maximizes the estimated reward $[27,30,35]$. This procedure can be performed in conjunction with distribution-based planners [53, 148] that can make action proposals to reduce the searching space.</p>
<h2>Q5. Any other potential applications of Vista?</h2>
<p>(1) As a generalizable predictive model, Vista could be utilized as a forward dynamics model [13, 26] to simulate short-term dynamics and assist long-horizon planning tasks like visual navigation [106]. (2) It is also intriguing to utilize Vista as an implicit driving policy, which is spontaneously acquired through future prediction [1, 25]. After synthesizing the video plan, we can convert the resultant image trajectory to executable actions by a non-causal inverse dynamics model [4, 65, 71], which can be efficiently learned from much fewer data compared with the imitation learning pipeline [2, 9]. In autonomous driving, the inverse dynamics model could be implemented with visual odometry [75]. (3) In collaboration with the reward function, it is also worth investigating if Vista could facilitate model-based reinforcement learning by boosting the sampling efficiency in real-world scenarios [137].</p>
<p>Q6. Differences with GenAD [136].
These two works have fundamental differences in control versatility and prediction fidelity. First of all, Vista is a generalizable world model that can be controlled by multi-modal action conditions. Although GenAD has also trained a trajectory-conditioned extension, its weights are fully finetuned on nuScenes and the generalization of its action control has never been validated. In contrast, Vista integrates versatile action controllability that can generalize to new scenarios in a zero-shot manner. Unlike GenAD that requires labeling OpenDV-YouTube with commands and texts, our collaborative training strategy skillfully averts this labor that may incur accumulated noises and conflicts [136]. In addition, Vista $(10 \mathrm{~Hz}, 576 \times 1024)$ operates at much higher frame rate and resolution, considerably beyond the capability of GenAD ( $2 \mathrm{~Hz}, 256 \times 448$ ) in both temporal and spatial axes. Different from GenAD, we also put forth several dedicated designs for high-fidelity prediction. We find that Vista, with a lower model complexity, achieves much better FID and FVD scores than GenAD (see Table 2).</p>
<h1>Q7. Limitations, failure cases, and possible solutions.</h1>
<p>As one of the pioneering efforts, Vista still has a few limitations that call for future works. (1) Since Vista predicts futures at an exceptional spatiotemporal resolution, it is inevitable to be computationally expensive, particularly in downstream applications. Potential solutions may include faster sampling techniques [89, 146] and training-based distillations [91, 93, 101, 103, 110, 124]. (2) It is possible that the prediction may undergo an apparent degradation in long-horizon rollouts or drastic view shifts. Extra refinements on the prediction results [6, 49, 54, 99, 126, 144] could be helpful. Speculatively, applying our recipe to more scalable architecture [54, 97] is also promising to address this limitation. (3) Similar to other controllable video generation methods [128], the chance of failure still persists in our action controls, especially for ambiguous intentions such as commands and goal points as Fig. 8 reveals. Incorporating more datasets with action annotations [11, 112, 129] for collaborative training could be beneficial. Using compositional classifier-free guidance [8, 19, 32] to amplify the individual impact of action conditions may also help (at a cost of increased inference compute). (4) Although our training data is based on the largest public driving dataset [136], it is nowhere near the entirety of Internet driving data, thus leaving huge untapped potential to further expand the capabilities of Vista.</p>
<h2>Q8. Why not expand the Vista framework to surround-view generation?</h2>
<p>It is true that supporting surround-view generation would further help driving. Existing works [127, 145] have explored the surround-view settings on nuScenes [10]. However, in this paper, we focus on the front-view setting for three main reasons: (1) The front view setting allows leveraging diverse data sources [54, 136]. Conversely, the distinctions in multi-view videos from various datasets, such as different numbers of cameras, hinder unified modeling and data scaling. (2) Models that focus on the front view can be seamlessly applied to different datasets without adaptation [107], broadening their applicability across datasets. (3) Though incomplete, the front view often contains most of the information vital for driving. As demonstrated in NAVSIM [23], using the front view alone results in only a $1.1 \%$ performance downtick in collision rate compared to using five surround-view cameras.</p>
<h2>Q9. Broader impact.</h2>
<p>Despite the encouraging improvements, our work is by no means perfect when it comes to real-world applications that involve dealing with highly complicated situations. As Vista is based on the diffusion framework, which introduces stochastic outcomes and non-negligible latencies, deploying it into autonomous vehicles directly could pose safety risks. While it is not a silver bullet yet, we expect that Vista will inspire the community to further exploit the capabilities and applications of driving world models. As a prototype for generalizable driving world models, we hope that Vista can stimulate the investigations in developing generalizable systems for autonomous driving and machine intelligence.</p>
<h2>B Related Work</h2>
<h2>B. 1 World Models</h2>
<p>Intelligent agents should be able to make efficacious decisions even under unseen situations [9, 57, 76, 113, 138, 149]. This requires fundamental knowledge of the world that generalizes to rare cases. As an</p>
<p>internal manifestation of such knowledge, a world model predicts plausible futures of the world given potential actions [9, 40, 69, 76, 96, 113, 137]. In principle, it not only predicts how the environment will unfold over time, but also deduces the underlying physical dynamics and agentic behaviors. Such properties can be useful for representation learning [35, 45, 88, 105, 131], model-based reinforcement learning [39, 40, 42, 43, 94, 96, 98], as well as model-predictive control [27, 30, 35, 41, 92, 104, 142]. Recent methods [37, 44, 80, 85, 86] also induce language-based world models from large language models, but are restricted in textual space and struggle with grounding on physics [26, 59].
Although world models have been extensively applied and made significant revolutions in simulated games [40, 42, 43] and indoor embodiment [72, 92, 120], such investigations for autonomous driving still lag behind [127, 143]. Different from other tasks, world modeling for autonomous driving poses unique challenges, which primarily arise from the large field of views with highly dynamic motions. Some practices imagine the world in the bird's eye view (BEV) space [20, 31, 52, 53, 78, 81]. Recent practices model the world state as raw sensor observations such as point clouds [7, 67, 140, 143, 147] and images [54, 61, 68, 90, 102, 125, 127, 131, 145]. The latter category, namely visual world models, hold more promise for scaling up due to sensor flexibility and data accessibility. Nevertheless, existing methods are restricted to a particular dataset [61, 77, 90, 125, 127, 143, 145, 147] or simulator [7, 131], compromising their generalization ability to novel domains. Meanwhile, these efforts lack systematic designs for the driving domain and only model the world at relatively low frame rates and resolutions, which discards the fine-grained details and impairs their ability to express real-world behaviors. Moreover, most of them are restricted to a specific control modality [54, 61, 90, 125], which hinders the accommodation to prevailing planning algorithms [12, 14, 21, 56, 58, 64] and extension to more applications like decision-making [127] or user interaction [74]. Besides, existing methods seldom explore zero-shot action controllability across different datasets. The inferior generalization, fidelity and controllability collectively preclude existing driving world models from broadly facilitating the development of autonomous driving.</p>
<h1>B. 2 Video Generation</h1>
<p>Video generation is an effective way to model the world and has undergone remarkable advancements over the years. Pioneering works [119, 134] have studied various kinds of generative models [29, 33, 70]. Swayed by the success of diffusion models [24, 48, 100], a surge of diffusion-based video generation methods have emerged [6, 34, 46, 51, 108, 117, 121]. Recent works [5, 15, 32, 144] shift their focus towards image-to-video generation for its finer content description and better scalability in training data. However, most of them are not strict predictive models that generate videos starting from the condition image. Moreover, existing methods struggle with the intricate dynamics in driving scenarios from the ego perspective [136], which limits their feasibility as driving world models.
While the majority of existing methods produce videos without explicit controllability, two recent works [128, 139] introduce camera motion control to video generation. However, camera motion is conceptually distinct from vehicle actions and both of these works are text-to-video methods without any prediction ability. Contrarily, the model we developed is a predictive world model that produces realistic dynamics and allows versatile action controls for autonomous driving.</p>
<h2>C Implementation Details</h2>
<h2>C. 1 Model</h2>
<p>We adopt the framework of SVD [5] as the architecture of Vista, which consists of 2.5B parameters in total, including 1.6B UNet parameters. For action conditioning, we encode the value of each action sequence into Fourier embeddings [114, 116] with 128 channels.</p>
<h2>C. 2 Dataset</h2>
<p>We utilize a rigorously filtered set of OpenDV-YouTube [136] for training, and incorporate nuScenes training set [10] during the action control learning phase. Concretely, we manually eliminate 15 hours of irrelevant content from OpenDV-YouTube, yielding approximately 1735 hours of unlabeled driving videos. Since nuScenes is heavily biased [83, 107, 127], we balance its samples based on command categories to foster the learning of rare actions. The video clips are sampled with 25 frames at 10 Hz . Although nuScenes [10] is logged at 12 Hz , we find no negative impact of treating them</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{\dagger}$ Primary contact to Shenyuan at sygao@connect.ust.hk ${ }^{\dagger}$ Equal advising.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
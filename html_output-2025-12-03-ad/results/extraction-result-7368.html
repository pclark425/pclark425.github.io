<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7368 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7368</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7368</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-138.html">extraction-schema-138</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <p><strong>Paper ID:</strong> paper-267750253</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.11451v2.pdf" target="_blank">SciAgent: Tool-augmented Language Models for Scientific Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Scientific reasoning poses an excessive challenge for even the most advanced Large Language Models (LLMs). To make this task more practical and solvable for LLMs, we introduce a new task setting named tool-augmented scientific reasoning. This setting supplements LLMs with scalable toolsets, and shifts the focus from pursuing an omniscient problem solver to a proficient tool-user. To facilitate the research of such setting, we construct a tool-augmented training corpus named MathFunc which encompasses over 30,000 samples and roughly 6,000 tools. Building on MathFunc, we develop SciAgent to retrieve, understand and, if necessary, use tools for scientific problem solving. Additionally, we craft a benchmark, SciToolBench, spanning five scientific domains to evaluate LLMs’ abilities with tool assistance. Extensive experiments on SciToolBench confirm the effectiveness of SciAgent. Notably, SciAgent-Llama3-8B surpasses other LLMs with the comparable size by more than 8.0% in absolute accuracy. Furthermore, SciAgent-DeepMath-7B shows much superior performance than ChatGPT.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7368.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7368.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCIAGENT-DEEPMATH-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SCIAGENT (fine-tuned on DeepMath / MATHFUNC) 7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tool-augmented LLM agent built by fine-tuning an open-source 7B model (DeepMath) on the MATHFUNC corpus to perform planning, retrieve domain functions, generate Python code calling retrieved functions, and execute code to answer scientific questions across multiple domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SCIAGENT-DEEPMATH</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>fine-tuned on domain/tool-use data (MATHFUNC); tool-augmented agent with retrieval and code-execution</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics, Physics, Chemistry, Finance, EECS (evaluated on SCITOOLBENCH)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based simulation of domain problem solving: generate a high-level plan, retrieve appropriate Python functions from a domain-specific toolset, produce interleaved natural-language rationale and Python code that calls retrieved functions, execute code to compute numeric/analytic answers to textbook-level scientific questions.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Agent is fine-tuned with explicit instruction templates for Planning and Action from MATHFUNC; evaluation uses a unified Program-of-Thought (PoT)-based prompt with 2 demonstrations; retrieval-augmented prompting (same dense retriever for all models) supplies top-3 retrieved functions; models instructed to call functions only if they lower problem difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (percentage of correctly answered questions on benchmark datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>SCITOOLBENCH (all domains) = 46.3% (paper reports SCIAGENT-DEEPMATH-7B overall accuracy = 46.3%). CREATION benchmark reported = 60.4% (per table).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Best open-source 7B baselines reported in paper: Deepseek-Math (7B) ≈ 27.7% overall; MAmmoTH-Coder (7B) = 19.7%; ChatGPT (with toolset) = 35.4% (paper compares explicitly). SCIAGENT-DEEPMATH-7B outperforms ChatGPT(with tools) in this evaluation (46.3% vs 35.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['presence/quality of planning module (improves retrieval recall and final accuracy)', 'cross-retrieval strategy during corpus construction (avoids ad-hoc function overfitting)', 'availability of function-augmented training samples (improves tool-use learning)', 'presence of function-free (math) solutions in training (critical for math skill transfer)', 'retriever quality and hit@3 of retrieved functions (positive correlation with task accuracy)', 'model architecture/fine-tuning (math-specialized fine-tuning benefits)', 'domain alignment (physics and chemistry require additional in-domain knowledge)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Same dense retriever (DPR-style fine-tuned Roberta backbone) used for all LLMs; retriever returns top-3 functions per query; evaluation prompt = unified PoT-based prompt with 2 demonstrations; Python-Executor used to run generated code; dataset-generation used nucleus sampling (p=0.6) with 5 repeats for some samples; training used ZeRO Stage 3 on 8 V100 GPUs; planning+action modules fine-tuned using MATHFUNC instruction pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Lower performance in physics and chemistry compared to math/finance/EECS; observed performance ceiling (~40% accuracy) even when retrieved functions are 100% correct (indicating remaining reasoning challenges); sensitive to retriever quality (performance drops when retriever replaced with weaker variants); removal of planning, cross-retrieval, function-augmented or function-free samples causes substantial accuracy declines; concerns about potential information leakage from function construction addressed but remain a limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciAgent: Tool-augmented Language Models for Scientific Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7368.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7368.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCIAGENT-MISTRAL-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SCIAGENT (fine-tuned Mistral) 7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tool-augmented agent produced by fine-tuning the Mistral 7B model on MATHFUNC to perform planning, retrieval of Python functions, code-generation that calls functions, and execution to solve scientific problems across multiple domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SCIAGENT-MISTRAL</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>fine-tuned on MATHFUNC; tool-augmented agent (retrieval + code execution)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics, Physics, Chemistry, Finance, EECS (SCITOOLBENCH)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Answer domain-specific textbook-style questions by producing plans, retrieving domain functions, emitting Python code that optionally calls retrieved functions, and executing code to produce numeric/analytic answers.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Fine-tuned planning/action modules with instruction templates; evaluated with unified PoT-based prompt and top-3 retrieval per question; models constrained to use functions only when they help.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (percentage correct) on SCITOOLBENCH and CREATION</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Paper reports SCIAGENT-MISTRAL-7B overall (SCITOOLBENCH all-domains) ≈ 34.1% and CREATION ≈ 54.0% (numbers taken from the paper tables/summary).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Paper states SCIAGENT-MISTRAL-7B surpasses best comparable open-source 7B models by >13% absolute; for example, compared to MAmmoTH-Coder (7B) = 19.7% overall, SCIAGENT-MISTRAL-7B is ≈14% absolute higher.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['planning module presence (improves retrieval recall and performance)', 'cross-retrieval strategy during training corpus creation', 'retriever hit@3 and retriever fine-tuning benefits (domain-finetuned retriever outperforms SimCSE/Contriever)', 'quality and generality of toolset functions', 'domain mismatch (physics/chemistry require more domain data)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Top-3 retrieved functions supplied; unified PoT prompt with 2 demonstrations; same dense retriever for all LLMs (fine-tuned DPR on MATHFUNC); training hyperparameters for Mistral fine-tuning as reported (learning rate 2e-6, cosine scheduler, ZeRO stage3 on 8 V100s).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Performance still behind proprietary strongest models (GPT-4) when both have same tool access; lower accuracy in physics and chemistry; sensitive to retriever quality and to removal of planning or cross-retrieval steps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciAgent: Tool-augmented Language Models for Scientific Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7368.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7368.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCIAGENT-CODER-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SCIAGENT (fine-tuned CodeLlama / CodeLlama-based) 7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A CodeLlama-based 7B tool-augmented agent fine-tuned on MATHFUNC that plans, retrieves functions, writes code calling functions, and executes code to answer scientific questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SCIAGENT-CODER</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>fine-tuned on MATHFUNC; tool-augmented agent with retrieval and execution</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics, Physics, Chemistry, Finance, EECS (SCITOOLBENCH)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Solve textbook-style scientific problems by retrieving domain-specific Python functions, generating interleaved rationale and Python code that calls functions, and executing code to compute answers.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Fine-tuned planning/action templates; evaluated with unified PoT prompt and top-3 retrieved functions; instruction encourages calling functions only if helpful.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (percent correct)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Paper reports SCIAGENT-CODER (7B) SCITOOLBENCH all-domains accuracy ≈ 32.2% and CREATION ≈ 53.0% (per reported table).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Compared to MAmmoTH-Coder (7B) = 19.7% overall, SCIAGENT-CODER (7B) obtains +12.5% absolute improvement (paper reports this specific comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['planning module importance', 'value of function-augmented training samples', 'benefit from function-free math samples for math capability transfer', 'retriever quality and hit@3 correlation with accuracy']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Same dense retriever for retrieval; top-3 functions provided; PoT-based prompts; trained with provided hyperparameters (CodeLlama lr 2e-5, batch size 128, ZeRO Stage3, 8 V100 GPUs).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Performance drops substantially if planning or retriever are removed; still far from GPT-4 when both have the same tools; domain gaps for physics and chemistry.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciAgent: Tool-augmented Language Models for Scientific Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7368.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7368.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary conversational LLM used in the paper as a reference baseline; evaluated with the same retrieval mechanism and toolset in the SCITOOLBENCH experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary (not specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>proprietary instruction-tuned chat model (evaluated with retrieval/toolset in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics, Physics, Chemistry, Finance, EECS (SCITOOLBENCH)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Answer domain questions by generating text and (optionally) code with access to retrieved Python functions (paper provided same retriever and top-3 functions to ChatGPT during evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Unified PoT-based prompt with 2 demonstrations; top-3 retrieved functions provided; models instructed to use functions only if necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (percentage correct on SCITOOLBENCH)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Paper reports ChatGPT (with toolset) overall accuracy on SCITOOLBENCH = 35.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Serves as a proprietary baseline in paper; GPT-4 (without toolset) reported ~49.5% in paper (for reference).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['benefit from provided toolset (paper shows multiple models improve when supplemented with toolsets)', 'sensitivity to domain (better on math/finance/EECS, worse on physics/chemistry)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Same retriever used to obtain top-3 functions provided to ChatGPT; unified PoT prompts during evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Performance lower than the best SCIAGENT variant on SCITOOLBENCH (per paper); still shows domain-dependent weaknesses, especially in physics and chemistry per reported breakdowns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciAgent: Tool-augmented Language Models for Scientific Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7368.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7368.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary large multimodal language model used as a high-performing reference in the paper's comparisons; cited for strong performance on scientific reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>proprietary (not specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>proprietary instruction-tuned model (reference baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scientific reasoning across math and other STEM domains (used for reference on TheoremQA, SciBench and in SCITOOLBENCH comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Reference capability for solving theorem/math/scientific questions (paper compares SCIAGENT performance to GPT-4’s reported results on benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Referenced prior use of chain-of-thought and PoT-style prompting in literature; in this paper GPT-4 was used in corpus/tool generation pipeline (GPT-4 generated plans and candidate functions during MATHFUNC creation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (percentage correct on various benchmarks referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Paper cites GPT-4 performance on some prior benchmarks (e.g., TheoremQA ~50% and SciBench ~35% in cited literature) and reports GPT-4 (without toolset) ≈ 49.5% on SCITOOLBENCH (paper's table reference/comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Used as upper reference; SCIAGENT-DEEPMATH-7B comparable to GPT-4 without toolset in one comparison (46.3% vs 49.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['proprietary pretraining and instruction-tuning yields higher baseline reasoning skill', 'tool-augmentation benefits smaller open-source models but gap to strongest proprietary models remains when equal tools provided']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>GPT-4 was used in data construction: generating plans, candidate functions and function-augmented solutions in MATHFUNC (with iterative rectification loops).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Even GPT-4 achieves limited accuracy on some scientific reasoning benchmarks cited (e.g., ~50% on TheoremQA), indicating intrinsic difficulty of tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciAgent: Tool-augmented Language Models for Scientific Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7368.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7368.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAmmoTH-Coder-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MAmmoTH-Coder (math-specialized fine-tuned model) 7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A math-specialized 7B model variant (MAmmoTH-Coder) fine-tuned on math-related instruction data; used in the paper as a competitive open-source baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MAmmoTH-Coder</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>open-source model fine-tuned on math-specific instruction data (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Primarily Mathematics; evaluated across SCITOOLBENCH domains as baseline</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Solve math and scientific questions; in this paper evaluated with same retriever and toolset when applicable to compare tool-augmented agents.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Originally fine-tuned on math instructions; for evaluation the paper appended a short tool-augmented description to its original prompt to allow using retrieved tools.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (percent correct on SCITOOLBENCH and CREATION)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Paper reports MAmmoTH-Coder (7B) overall accuracy on SCITOOLBENCH = 19.7% (table), and higher on some math subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Used as a primary open-source baseline; SCIAGENT variants report double-digit absolute improvements over this baseline (e.g., SCIAGENT-CODER +12.5% over MAmmoTH-Coder 7B).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['benefit from math-specialized fine-tuning for math tasks', 'still substantially improved when supplemented with toolsets']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Evaluated with same retriever and top-3 retrieved functions; prompt augmented to allow tool-use.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>While strong on math relative to some baselines, overall performance on multi-domain scientific reasoning remains limited without additional tool-augmentation or domain data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciAgent: Tool-augmented Language Models for Scientific Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>CREATION Challenge <em>(Rating: 2)</em></li>
                <li>TheoremQA: A theorem-driven question answering dataset <em>(Rating: 2)</em></li>
                <li>SciBench: Evaluating college-level scientific problem-solving abilities of large language models <em>(Rating: 2)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 2)</em></li>
                <li>CRAFT: Customizing LLMs by creating and retrieving from specialized toolsets <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7368",
    "paper_id": "paper-267750253",
    "extraction_schema_id": "extraction-schema-138",
    "extracted_data": [
        {
            "name_short": "SCIAGENT-DEEPMATH-7B",
            "name_full": "SCIAGENT (fine-tuned on DeepMath / MATHFUNC) 7B",
            "brief_description": "A tool-augmented LLM agent built by fine-tuning an open-source 7B model (DeepMath) on the MATHFUNC corpus to perform planning, retrieve domain functions, generate Python code calling retrieved functions, and execute code to answer scientific questions across multiple domains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SCIAGENT-DEEPMATH",
            "model_size": "7B",
            "model_type": "fine-tuned on domain/tool-use data (MATHFUNC); tool-augmented agent with retrieval and code-execution",
            "scientific_domain": "Mathematics, Physics, Chemistry, Finance, EECS (evaluated on SCITOOLBENCH)",
            "simulation_task_description": "Text-based simulation of domain problem solving: generate a high-level plan, retrieve appropriate Python functions from a domain-specific toolset, produce interleaved natural-language rationale and Python code that calls retrieved functions, execute code to compute numeric/analytic answers to textbook-level scientific questions.",
            "prompting_strategy": "Agent is fine-tuned with explicit instruction templates for Planning and Action from MATHFUNC; evaluation uses a unified Program-of-Thought (PoT)-based prompt with 2 demonstrations; retrieval-augmented prompting (same dense retriever for all models) supplies top-3 retrieved functions; models instructed to call functions only if they lower problem difficulty.",
            "evaluation_metric": "Accuracy (percentage of correctly answered questions on benchmark datasets)",
            "reported_accuracy": "SCITOOLBENCH (all domains) = 46.3% (paper reports SCIAGENT-DEEPMATH-7B overall accuracy = 46.3%). CREATION benchmark reported = 60.4% (per table).",
            "baseline_accuracy": "Best open-source 7B baselines reported in paper: Deepseek-Math (7B) ≈ 27.7% overall; MAmmoTH-Coder (7B) = 19.7%; ChatGPT (with toolset) = 35.4% (paper compares explicitly). SCIAGENT-DEEPMATH-7B outperforms ChatGPT(with tools) in this evaluation (46.3% vs 35.4%).",
            "factors_reported": [
                "presence/quality of planning module (improves retrieval recall and final accuracy)",
                "cross-retrieval strategy during corpus construction (avoids ad-hoc function overfitting)",
                "availability of function-augmented training samples (improves tool-use learning)",
                "presence of function-free (math) solutions in training (critical for math skill transfer)",
                "retriever quality and hit@3 of retrieved functions (positive correlation with task accuracy)",
                "model architecture/fine-tuning (math-specialized fine-tuning benefits)",
                "domain alignment (physics and chemistry require additional in-domain knowledge)"
            ],
            "experimental_conditions": "Same dense retriever (DPR-style fine-tuned Roberta backbone) used for all LLMs; retriever returns top-3 functions per query; evaluation prompt = unified PoT-based prompt with 2 demonstrations; Python-Executor used to run generated code; dataset-generation used nucleus sampling (p=0.6) with 5 repeats for some samples; training used ZeRO Stage 3 on 8 V100 GPUs; planning+action modules fine-tuned using MATHFUNC instruction pairs.",
            "limitations_or_failure_modes": "Lower performance in physics and chemistry compared to math/finance/EECS; observed performance ceiling (~40% accuracy) even when retrieved functions are 100% correct (indicating remaining reasoning challenges); sensitive to retriever quality (performance drops when retriever replaced with weaker variants); removal of planning, cross-retrieval, function-augmented or function-free samples causes substantial accuracy declines; concerns about potential information leakage from function construction addressed but remain a limitation.",
            "uuid": "e7368.0",
            "source_info": {
                "paper_title": "SciAgent: Tool-augmented Language Models for Scientific Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "SCIAGENT-MISTRAL-7B",
            "name_full": "SCIAGENT (fine-tuned Mistral) 7B",
            "brief_description": "A tool-augmented agent produced by fine-tuning the Mistral 7B model on MATHFUNC to perform planning, retrieval of Python functions, code-generation that calls functions, and execution to solve scientific problems across multiple domains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SCIAGENT-MISTRAL",
            "model_size": "7B",
            "model_type": "fine-tuned on MATHFUNC; tool-augmented agent (retrieval + code execution)",
            "scientific_domain": "Mathematics, Physics, Chemistry, Finance, EECS (SCITOOLBENCH)",
            "simulation_task_description": "Answer domain-specific textbook-style questions by producing plans, retrieving domain functions, emitting Python code that optionally calls retrieved functions, and executing code to produce numeric/analytic answers.",
            "prompting_strategy": "Fine-tuned planning/action modules with instruction templates; evaluated with unified PoT-based prompt and top-3 retrieval per question; models constrained to use functions only when they help.",
            "evaluation_metric": "Accuracy (percentage correct) on SCITOOLBENCH and CREATION",
            "reported_accuracy": "Paper reports SCIAGENT-MISTRAL-7B overall (SCITOOLBENCH all-domains) ≈ 34.1% and CREATION ≈ 54.0% (numbers taken from the paper tables/summary).",
            "baseline_accuracy": "Paper states SCIAGENT-MISTRAL-7B surpasses best comparable open-source 7B models by &gt;13% absolute; for example, compared to MAmmoTH-Coder (7B) = 19.7% overall, SCIAGENT-MISTRAL-7B is ≈14% absolute higher.",
            "factors_reported": [
                "planning module presence (improves retrieval recall and performance)",
                "cross-retrieval strategy during training corpus creation",
                "retriever hit@3 and retriever fine-tuning benefits (domain-finetuned retriever outperforms SimCSE/Contriever)",
                "quality and generality of toolset functions",
                "domain mismatch (physics/chemistry require more domain data)"
            ],
            "experimental_conditions": "Top-3 retrieved functions supplied; unified PoT prompt with 2 demonstrations; same dense retriever for all LLMs (fine-tuned DPR on MATHFUNC); training hyperparameters for Mistral fine-tuning as reported (learning rate 2e-6, cosine scheduler, ZeRO stage3 on 8 V100s).",
            "limitations_or_failure_modes": "Performance still behind proprietary strongest models (GPT-4) when both have same tool access; lower accuracy in physics and chemistry; sensitive to retriever quality and to removal of planning or cross-retrieval steps.",
            "uuid": "e7368.1",
            "source_info": {
                "paper_title": "SciAgent: Tool-augmented Language Models for Scientific Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "SCIAGENT-CODER-7B",
            "name_full": "SCIAGENT (fine-tuned CodeLlama / CodeLlama-based) 7B",
            "brief_description": "A CodeLlama-based 7B tool-augmented agent fine-tuned on MATHFUNC that plans, retrieves functions, writes code calling functions, and executes code to answer scientific questions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SCIAGENT-CODER",
            "model_size": "7B",
            "model_type": "fine-tuned on MATHFUNC; tool-augmented agent with retrieval and execution",
            "scientific_domain": "Mathematics, Physics, Chemistry, Finance, EECS (SCITOOLBENCH)",
            "simulation_task_description": "Solve textbook-style scientific problems by retrieving domain-specific Python functions, generating interleaved rationale and Python code that calls functions, and executing code to compute answers.",
            "prompting_strategy": "Fine-tuned planning/action templates; evaluated with unified PoT prompt and top-3 retrieved functions; instruction encourages calling functions only if helpful.",
            "evaluation_metric": "Accuracy (percent correct)",
            "reported_accuracy": "Paper reports SCIAGENT-CODER (7B) SCITOOLBENCH all-domains accuracy ≈ 32.2% and CREATION ≈ 53.0% (per reported table).",
            "baseline_accuracy": "Compared to MAmmoTH-Coder (7B) = 19.7% overall, SCIAGENT-CODER (7B) obtains +12.5% absolute improvement (paper reports this specific comparison).",
            "factors_reported": [
                "planning module importance",
                "value of function-augmented training samples",
                "benefit from function-free math samples for math capability transfer",
                "retriever quality and hit@3 correlation with accuracy"
            ],
            "experimental_conditions": "Same dense retriever for retrieval; top-3 functions provided; PoT-based prompts; trained with provided hyperparameters (CodeLlama lr 2e-5, batch size 128, ZeRO Stage3, 8 V100 GPUs).",
            "limitations_or_failure_modes": "Performance drops substantially if planning or retriever are removed; still far from GPT-4 when both have the same tools; domain gaps for physics and chemistry.",
            "uuid": "e7368.2",
            "source_info": {
                "paper_title": "SciAgent: Tool-augmented Language Models for Scientific Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (OpenAI)",
            "brief_description": "A proprietary conversational LLM used in the paper as a reference baseline; evaluated with the same retrieval mechanism and toolset in the SCITOOLBENCH experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_size": "proprietary (not specified in paper)",
            "model_type": "proprietary instruction-tuned chat model (evaluated with retrieval/toolset in experiments)",
            "scientific_domain": "Mathematics, Physics, Chemistry, Finance, EECS (SCITOOLBENCH)",
            "simulation_task_description": "Answer domain questions by generating text and (optionally) code with access to retrieved Python functions (paper provided same retriever and top-3 functions to ChatGPT during evaluation).",
            "prompting_strategy": "Unified PoT-based prompt with 2 demonstrations; top-3 retrieved functions provided; models instructed to use functions only if necessary.",
            "evaluation_metric": "Accuracy (percentage correct on SCITOOLBENCH)",
            "reported_accuracy": "Paper reports ChatGPT (with toolset) overall accuracy on SCITOOLBENCH = 35.4%.",
            "baseline_accuracy": "Serves as a proprietary baseline in paper; GPT-4 (without toolset) reported ~49.5% in paper (for reference).",
            "factors_reported": [
                "benefit from provided toolset (paper shows multiple models improve when supplemented with toolsets)",
                "sensitivity to domain (better on math/finance/EECS, worse on physics/chemistry)"
            ],
            "experimental_conditions": "Same retriever used to obtain top-3 functions provided to ChatGPT; unified PoT prompts during evaluation.",
            "limitations_or_failure_modes": "Performance lower than the best SCIAGENT variant on SCITOOLBENCH (per paper); still shows domain-dependent weaknesses, especially in physics and chemistry per reported breakdowns.",
            "uuid": "e7368.3",
            "source_info": {
                "paper_title": "SciAgent: Tool-augmented Language Models for Scientific Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "A proprietary large multimodal language model used as a high-performing reference in the paper's comparisons; cited for strong performance on scientific reasoning benchmarks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_size": "proprietary (not specified in paper)",
            "model_type": "proprietary instruction-tuned model (reference baseline)",
            "scientific_domain": "General scientific reasoning across math and other STEM domains (used for reference on TheoremQA, SciBench and in SCITOOLBENCH comparisons)",
            "simulation_task_description": "Reference capability for solving theorem/math/scientific questions (paper compares SCIAGENT performance to GPT-4’s reported results on benchmarks).",
            "prompting_strategy": "Referenced prior use of chain-of-thought and PoT-style prompting in literature; in this paper GPT-4 was used in corpus/tool generation pipeline (GPT-4 generated plans and candidate functions during MATHFUNC creation).",
            "evaluation_metric": "Accuracy (percentage correct on various benchmarks referenced)",
            "reported_accuracy": "Paper cites GPT-4 performance on some prior benchmarks (e.g., TheoremQA ~50% and SciBench ~35% in cited literature) and reports GPT-4 (without toolset) ≈ 49.5% on SCITOOLBENCH (paper's table reference/comparison).",
            "baseline_accuracy": "Used as upper reference; SCIAGENT-DEEPMATH-7B comparable to GPT-4 without toolset in one comparison (46.3% vs 49.5%).",
            "factors_reported": [
                "proprietary pretraining and instruction-tuning yields higher baseline reasoning skill",
                "tool-augmentation benefits smaller open-source models but gap to strongest proprietary models remains when equal tools provided"
            ],
            "experimental_conditions": "GPT-4 was used in data construction: generating plans, candidate functions and function-augmented solutions in MATHFUNC (with iterative rectification loops).",
            "limitations_or_failure_modes": "Even GPT-4 achieves limited accuracy on some scientific reasoning benchmarks cited (e.g., ~50% on TheoremQA), indicating intrinsic difficulty of tasks.",
            "uuid": "e7368.4",
            "source_info": {
                "paper_title": "SciAgent: Tool-augmented Language Models for Scientific Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "MAmmoTH-Coder-7B",
            "name_full": "MAmmoTH-Coder (math-specialized fine-tuned model) 7B",
            "brief_description": "A math-specialized 7B model variant (MAmmoTH-Coder) fine-tuned on math-related instruction data; used in the paper as a competitive open-source baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MAmmoTH-Coder",
            "model_size": "7B",
            "model_type": "open-source model fine-tuned on math-specific instruction data (baseline)",
            "scientific_domain": "Primarily Mathematics; evaluated across SCITOOLBENCH domains as baseline",
            "simulation_task_description": "Solve math and scientific questions; in this paper evaluated with same retriever and toolset when applicable to compare tool-augmented agents.",
            "prompting_strategy": "Originally fine-tuned on math instructions; for evaluation the paper appended a short tool-augmented description to its original prompt to allow using retrieved tools.",
            "evaluation_metric": "Accuracy (percent correct on SCITOOLBENCH and CREATION)",
            "reported_accuracy": "Paper reports MAmmoTH-Coder (7B) overall accuracy on SCITOOLBENCH = 19.7% (table), and higher on some math subsets.",
            "baseline_accuracy": "Used as a primary open-source baseline; SCIAGENT variants report double-digit absolute improvements over this baseline (e.g., SCIAGENT-CODER +12.5% over MAmmoTH-Coder 7B).",
            "factors_reported": [
                "benefit from math-specialized fine-tuning for math tasks",
                "still substantially improved when supplemented with toolsets"
            ],
            "experimental_conditions": "Evaluated with same retriever and top-3 retrieved functions; prompt augmented to allow tool-use.",
            "limitations_or_failure_modes": "While strong on math relative to some baselines, overall performance on multi-domain scientific reasoning remains limited without additional tool-augmentation or domain data.",
            "uuid": "e7368.5",
            "source_info": {
                "paper_title": "SciAgent: Tool-augmented Language Models for Scientific Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "CREATION Challenge",
            "rating": 2,
            "sanitized_title": "creation_challenge"
        },
        {
            "paper_title": "TheoremQA: A theorem-driven question answering dataset",
            "rating": 2,
            "sanitized_title": "theoremqa_a_theoremdriven_question_answering_dataset"
        },
        {
            "paper_title": "SciBench: Evaluating college-level scientific problem-solving abilities of large language models",
            "rating": 2,
            "sanitized_title": "scibench_evaluating_collegelevel_scientific_problemsolving_abilities_of_large_language_models"
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 2,
            "sanitized_title": "program_of_thoughts_prompting_disentangling_computation_from_reasoning_for_numerical_reasoning_tasks"
        },
        {
            "paper_title": "CRAFT: Customizing LLMs by creating and retrieving from specialized toolsets",
            "rating": 1,
            "sanitized_title": "craft_customizing_llms_by_creating_and_retrieving_from_specialized_toolsets"
        }
    ],
    "cost": 0.020596749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SCIAGENT: Tool-augmented Language Models for Scientific Reasoning
21 Feb 2024</p>
<p>Yubo Ma 
Nanyang Technological University</p>
<p>Zhibin Gou 
Tsinghua University</p>
<p>Junheng Hao 
Microsoft</p>
<p>Ruochen Xu 
Microsoft</p>
<p>Shuohang Wang 
Microsoft</p>
<p>Liangming Pan 
University of California
Santa Barbara</p>
<p>Yujiu Yang 
Tsinghua University</p>
<p>Yixin Cao 
Singapore Management University</p>
<p>Aixin Sun 
Nanyang Technological University</p>
<p>Hany Awadalla 
Microsoft</p>
<p>Weizhu Chen 
Microsoft</p>
<p>Zhihong Shao 
Yeyun Gong 
Yelong Shen 
Yujiu Yang 
Minlie Huang 
Nan Duan 
Weizhu Chen 
Tora 
Daya 2024 Guo 
Qihao Zhu 
Dejian Yang 
Zhenda Xie 
Kai Dong 
Wentao Zhang 
Guanting Chen 
Xiao Bi 
Ying Nian Wu 
Y K Li 
Fuli Luo 
Yingfei Xiong 
Wen- Feng Liang 
Deepseek-Coder 
Shibo Hao 
Tianyang Liu 
Zhen Wang 
Zhiting Hu 
Toolkengpt 
Dan Hendrycks 
Collin Burns 
Steven Basart 
Andy Zou 
Mantas Mazeika 
Gautier Izacard 
Mathilde Caron 
Lucas Hosseini 
Se- Bastian Riedel 
Piotr Bojanowski 
Armand Joulin 
Albert Q Jiang 
Alexandre Sablayrolles 
Arthur Men- Sch 
Chris Bamford 
Singh Devendra 
Diego Chaplot 
Florian De Las Casas 
Gianna Bressand 
Guil- Laume Lengyel 
Lucile Lample 
LélioRenard Saulnier 
Marie-Anne Lavaud 
Pierre Lachaux 
Teven Stock 
Thibaut Le Scao 
Thomas Lavril 
Timothée Wang 
William Lacroix 
Sayed El 
Mistral 
Jin Qiao 
Yifan Yang 
Qingyu Chen 
Zhiyong Lu 
Genegpt 
Vladimir Karpukhin 
Barlas Oguz 
Sewon Min 
Patrick Lewis 
Ledell Wu 
Sergey Edunov 
Danqi Chen 
Yuliang Liu 
Xiangru Tang 
Zefan Cai 
Junjie Lu 
Yichi Zhang 
Yanjun Shao 
Zexuan Deng 
Helan Hu 
Zengxian Yang 
Kaikai An 
Ruijun Huang 
Shuzheng Si 
Sheng Chen 
Haozhe Zhao 
Zhengliang Li 
Liang Chen 
Yiming Zong 
Yan Wang 
Tianyu Liu 
Zhi- Wei Jiang 
Baobao Chang 
Yujia Qin 
Wangchunshu Zhou 
Yilun Zhao 
Arman Cohan 
Pan Lu 
Baolin Peng 
Hao Cheng 
Michel Galley 
Kai- Wei Chang 
Pengcheng He 
Yujia Xie 
Yu Hu 
Qiuyuan Huang 
Lars Liden 
Zhou Yu 
Weizhu Chen 
Jianfeng 2023 Gao 
Check 
Cheng Qian 
Chi Han 
Yi Ren Fung 
Zhiyuan Liu 
Heng Ji 2023 Creator 
Shengding Hu 
Yankai Lin 
Weize Chen 
Ning Ding 
Ganqu Cui 
Zheni Zeng 
Yufei Huang 
Chaojun Xiao 
Yusheng Su 
Huadong Wang 
Runchu Tian 
Kunlun Zhu 
Shihao Liang 
Xingyu Shen 
Bokai Xu 
Zhen Zhang 
Yining Ye 
Bowen Li 
Ziwei Tang 
Jing Yi 
Yuzhang Zhu 
Zhenning Dai 
Lan Yan 
Xin Cong 
Yaxi Lu 
Weilin Zhao 
Yuxiang Huang 
Junxi Yan 
Xu Han 
Xian Sun 
Dahai Li 
Jason Phang 
Cheng Yang 
Tongshuang Wu 
Heng Ji 
Bill Qian 
Sihan Zhao 
Lauren Hong 
Ruobing Xie 
Jie Zhou 
Mark Gerstein 
Baptiste Rozière 
Jonas Gehring 
Fabian Gloeckle 
Sten Sootla 
XiaoqingItai Gat 
Ellen Tan 
Yossi Adi 
Jingyu Liu 
Tal Remez 
Jérémy Rapin 
Artyom Kozhevnikov 
Ivan Evtimov 
Joanna Bitton 
Manish Bhatt 
Cristian Canton Ferrer 
Aaron Grattafiori 
Wen- Han Xiong 
Alexandre Défossez 
Jade Copet 
Faisal Azhar 
Hugo Touvron 
Louis Martin 
Nicolas Usunier 
Thomas Scialom 
Gabriel 2023 Synnaeve 
Code 
Peiyi Wang 
Runxin Xu 
Junxiao Song 
Mingchuan Zhang 
Deepseekmath </p>
<p>Microsoft</p>
<p>SCIAGENT: Tool-augmented Language Models for Scientific Reasoning
21 Feb 20240F4356261A624CBA57E7C7DBCAC02E0BarXiv:2402.11451v2[cs.CL]
Scientific reasoning poses an excessive challenge for even the most advanced Large Language Models (LLMs).To make this task more practical and solvable for LLMs, we introduce a new task setting named tool-augmented scientific reasoning.This setting supplements LLMs with scalable toolsets, and shifts the focus from pursuing an omniscient problem solver to a proficient tool-user.To facilitate the research of such setting, we construct a tool-augmented training corpus named MATH-FUNC which encompasses over 30,000 samples and roughly 6,000 tools.Building on MATH-FUNC, we develop SCIAGENT to retrieve, understand and, if necessary, use tools for scientific problem solving.Additionally, we craft a benchmark, SCITOOLBENCH, spanning five scientific domains to evaluate LLMs' abilities with tool assistance.Extensive experiments on SCITOOLBENCH confirm the effectiveness of SCIAGENT.Notably, SCIAGENT-MISTRAL-7B surpasses other LLMs with the same size by more than 13% in absolute accuracy.Furthermore, SCIAGENT-DEEPMATH-7B shows much superior performance than ChatGPT.</p>
<p>Introduction</p>
<p>Scientific reasoning (Ouyang et al., 2023;Zhao et al., 2023) aims to comprehend and make decisions regarding problems among STEM (Science, Technology, Engineering and Mathematics) domains.It is a fundamental aspect of intelligence, a demanding capability of Large Language Models (LLMs), and a notoriously challenging task.For instance, even GPT-4 (OpenAI, 2023) achieves only 50% and 35% accuracy on TheoremQA (Chen et al., 2023b) and SciBench (Wang et al., 2023b), respectively.Regarding open-source LLMs such as LLaMA-2 (Touvron et al., 2023) and CodeL- LLMs are fine-tuned on math-related, tool-augmented samples (color in red).When adapting LLMs to a specific domain, a pluggable and domain-specific toolset is attached.No additional fine-tuning is further required.lama (Rozière et al., 2023), their performances are only about 10% accuracy or even less.</p>
<p>The challenge in scientific reasoning arises from the need for both mathematical (math) and domainspecific reasoning abilities.To address the physical problem in Figure 3, for example, it is necessary to both understand Malus' law (domain knowledge) for analyzing the intensity of polarized light, and possess quantitative ability for calculating the light intensity ratios.A natural approach involves collecting annotations and fine-tuning LLMs to enhance their math and domain-specific reasoning abilities, as depicted in Figure 1 (left).However, annotating scientific reasoning problems is extremely expensive.What is worse, adapting LLMs to a new domain demands a fresh round of annotation and fine-tuning, rendering this approach impractical.</p>
<p>In this paper, we draw inspirations from tool learning (Qin et al., 2023a) to enhance LLMs' scientific reasoning capabilities.Instead of solving scientific problem from scratch, humans have summarized and wrapped various points as generalized and well-documented functions in scientific computing softwares, such as Matlab, WolframAlpha, SymPy, etc.These functions1 , which could be equivalently viewed as external tools, greatly facilitate math-adept users to solve difficult scientific problems.In analogy with humans, we do not pursue an omniscient solver across various scientific domains.Instead, we assume the access to domainspecific toolsets and purse a unified, generalized LLM-based tool-user as shown in the Figure 1 (right).This approach tackles domain-specific reasoning challenges by enabling LLMs learn to use a reusable and scalable toolkit.It alleviates the reasoning challenges of LLMs by concentrating solely on enhancing their tool-use abilities.These abilities are not only easier to acquire but also applicable across a variety of scientific fields.By attaching domain-specific toolsets, our tool-users can be readily adapted to different fields without the need for additional in-domain fine-tuning.</p>
<p>This work focuses on developing and benchmarking the ability of LLMs in scientific reasoning with the help of tools.We envision a scenario where LLMs have access to a domain-specific toolset, comprising various specialized functions.Upon this scenario, we propose a complete framework of dataset construction, model training and evaluation.Given a scientific question, LLMs are supposed to retrieve functions from the toolset and optionally incorporate functions into the formulated solution.We employ an automatic pipeline featuring GPT-4 to compile a large-scale, mathrelated, tool-augmented training corpus named as MATHFUNC.This corpus is designed to enable LLMs to learn both essential math skills and how to retrieve, understand and use functions properly.As a result, MATHFUNC contains 31,375 samples and equipped with a toolset encompassing 5,981 generalized and well-documented functions.We detail this training corpus in Section 3.</p>
<p>We fine-tune open-source LLMs on MATHFUNC to develop tool-augmented agents named SCIA-GENT detailed in Section 4. As shown in Figure 3, SCIAGENT firstly generate a high-level planning in response to a given question.The agents then use this plan, along with the question, to retrieve functions from the given toolset.Leveraging these retrieved functions, the agents further complete the low-level action integrating natural language and Python code.Finally the agents execute the code to complete the problem at hand.</p>
<p>To benchmark the tool-use abilities in scientific reasoning, we develop a new benchmark named SCITOOLBENCH as described in Section 5. Building upon TheoremQA (Chen et al., 2023b) and SciBench (Wang et al., 2023b), it has 856 questions covering five domains: Mathematics, Physical, Chemistry, EECS, and Finance.It also contains five domain-specific toolsets comprising a total of 2,446 functions.We evaluate SCIAGENT on SCITOOLBENCH and another benchmark derived from CREATOR-challenge (Qian et al., 2023).Experimental results demonstrate that our agents present remarkable scientific reasoning capabilities.Notably, SCIAGENT-MISTRAL-7B surpasses the best comparable open-source LLMs by an absolute 13.4% accuracy, and SCIAGENT-DEEPMATH-7B outperforms ChatGPT by a large margin.We also conduct an extensive analysis of the benefits and limitations of SCIAGENT series, providing valuable insights for future research.</p>
<p>Preliminary</p>
<p>Related Work.Current methods (Chen et al., 2023b;Xu et al., 2023b;Ouyang et al., 2023), especially those based on open-source LLMs, perform far from satisfactory on scientific reasoning benchmarks (Chen et al., 2023b;Wang et al., 2023b).We attribute it to the scarcity of annotated samples across diverse scientific domains.As a comparison, LLMs present much more remarkable performance on math problems (Yue et al., 2023b;Gou et al., 2023b;Azerbayev et al., 2023) due to the abundant training corpora and/or annotations.Different from concurrent work (Zhang et al., 2024) which collects physics and chemistry annotations, we do not pursue a problem-solver on some specific scientific domains.Instead, we consider to develop a generalized tool-user being proficient on solving diverse scientific problems with the aid of tools.Following previous work on math domain (Qian et al., 2023;Cai et al., 2023;Yuan et al., 2023a), the tools here refer to Python functions.Please see more detailed literature review in Appendix A. Task Formulation.Given a scientific domain D (e.g., physics), tool-augmented scientific reasoning task assumes access to (1) a question q ∈ D and (2) a toolset F D .F D encompasses large amounts of well-documented, domain-specific functions {f 1 , ..., f m }.Our objective is to develop an agent M which selectively use functions in F D to enhance the answering for the question q. 3 Training Corpus: MATHFUNC  2).We build this corpus based on MATH (Hendrycks et al., 2021b) training set because we expect to teach LLMs both math skills and tool-use abilities.Sample Format.Each sample is a quintuple (q, G q , F q , S q , a q ).Here q is a question, G q is the planning, F q is the function set filtered from the toolset (F q ⊂ F , |F q | ≪ |F |), S q is the solution and a q is the answer.S q interleaves rationales E q 3 and programs P q which optionally call functions in F q to facilitate the problem solving.</p>
<p>We employ an automatic pipeline to construct MATHFUNC.We illustrate the pipeline in Figure 2 and detail the process in the following subsections.</p>
<p>Planning and Toolset Construction</p>
<p>This module is depicted in the top-left side of Figure 2. Given a question q and its ground-truth solution (written in pure natural language) in MATH training set, we ask GPT-4 to generate (1) a highlevel planning G q to analyze this question, (2) one or more well-documented functions Fq and (3) a solution Sq calling the functions above.The prompt used is shown in Appendix F.1.In the prompt, we emphasize that the functions should be as composable and generalized as possible.Specifically, we do not hope that each question generates only one ad-hoc function (which could only be used by this question).Instead, we expect GPT-4 to generate functions that follow the points in the planning G q and can be reused by other questions.Following previous work (Qian et al., 2023;Pan et al., 2023), we provide the error feedback to GPT-4 if the solutions fail to execute, and ask GPT-4 to rectify the errors in Fq or Sq .We repeat this procedure until successful execution or reaching maximum loop limitation.The prompt used for rectification is shown in Appendix F.2.</p>
<p>We collect G q ( 1 ⃝ in Figure 2, the same below) and add Fq to the toolset ( 2 ⃝) for question q if the rectified solution Sq leads to the correct answer ãq .Regarding the toolset, it is iterated on all questions and finally accumulated as below:
F = q∈D Fq • I(ã q is correct)</p>
<p>Function-augmented Solutions</p>
<p>To collect function-augmented solution S q and F q , a natural idea is to directly use the Sq and Fq generated above.However, we find that Sq tends to be contrived and specifically tailored to fit the requirements of function-calling.Moreover, some functions in Fq tend to be ad-hoc 4 .For examples, the function f(x, y) in Figure 2 merely parameterizes the hyperbola for a specific question.Therefore we disentangle the construction of toolset and function-augmented solutions.Given the developed toolset, we design a cross-retrieval strategy def intensity_ratio(dB1, dB2): def refraction_angle(incident_angle, refractive_index): def malus_law_intensity(I0, theta):</p>
<p>""" Calculates the intensity of polarized light after passing through a polarizer using Malus' Law.</p>
<p>Parameters:</p>
<p>-I0 (float): The intensity of polarized light before the polarizer.</p>
<p>-theta (float): The angle between the polarized light and the polarizer in degrees.</p>
<p>Returns:</p>
<p>-float: The intensity of polarized light after passing through the polarizer.</p>
<p>"""</p>
<p>Question: Unpolarized light passes through a polarizer at angle 40 degree to the first, and then another at angle 15 degree to the second.What percentage of the original intensity was the light coming out of the second polarizer?to retrieve more generalized functions F q and generate more qualified solutions S q .Specifically, we remove Fq from F temporarily and then retrieve new functions F q ⊆ (F \ Fq ) for question q.This strategy eliminates the likelihood of calling ad-hoc functions from Fq in S q .See examples of retrieved functions, all of which are derived from other questions, in the right side of Figure 2.</p>
<p>Retriever.The cross-retrieval strategy necessities a retriever because it is impractical to enumerate thousands of functions in F \ Fq .We train a dense retriever R ( 3 ⃝ in Figure 2).We concatenate the question q and the generated planning G q as the query, and view the generated functions Fq as the keys.See details about R in Appendix B.1.Solution Generation.Upon the toolset F and the retriever R, we retrieve three functions as F q :
F q = R([q, G q ]; F \ Fq )
Then we employ GPT-4 to write solutions which optionally call functions in F q to generate the solution S q ( 4 ⃝).The prompt used is illustrated in Appendix F.3.We explicitly point out in the prompt that f ∈ F q should be called if and only if when they do lower the difficulty of problem solving.It mitigates the over-exploitation of function calling in S q and increases the robustness of models finetuned on these samples.Specifically, we firstly use GPT-4 with greedy decoding to generate solutions.For those failing to yield correct answers, we further apply nucleus sampling (Holtzman et al., 2020) with 5 repeat times and 0.6 temperature.We filter wrong solutions and collect remaining 6,229 samples as our function-augmented solutions.</p>
<p>In parallel, we use GPT-4 to generate functionfree solutions.Though not indispensable, we expect them to further enhance the math reasoning, and accordingly the scientific reasoning, abilities of LLMs.We collect a total of 24,946 functionfree solutions nucleus sampling with 5 repeat times and 0.6 temperature.These samples share similar format as ToRA-corpus (Gou et al., 2023b), and do not retrieve/use any functions, i.e., F q = ∅.</p>
<p>Model: SCIAGENT</p>
<p>We develop SCIAGENT for tool-augmented scientific reasoning task.It could make plan, retrieve functions, and leverage retrieved functions to facilitate the reasoning.We describe its inference procedure and training approach as below.</p>
<p>Overview</p>
<p>As shown in Figure 3, SCIAGENT comprises four successive modules.Planning.This module provides a high-level profile for each question: G q = M planning (q).Such planning instructs a more targeted retrieval process.Retrieval.Given the question and generated planning G q , the retriever M retrieval is introduced to retrieve related functions from the domain-specific toolset:
F q = M retrieval ([q, G q ]; F D ) ⊆ F D .
Action.This module aims to generate low-level solutions.Specifically, the agent produces S q = M action (q; F q ).The solution S q is interleaved with natural language rationale E q and program snippet P q .The program P q call retrieved functions with proper arguments if necessary.Execution.This module is simply a Python Executor to run the program P q for the final answer: a q = Python-Executor(P q ).</p>
<p>Training</p>
<p>Language models are used in three out of four modules in SCIAGENT: planning, retrieval and action.</p>
<p>Rearding retrieval, we directly use the retriever R fine-tuned in Section 3.2 as M retrieval .For planning and action modules, they share the same LLMs: M = M planning = M action .We fine-tune M with different instructions to make it act as planning and action modules, respectively.We construct instructions from d = (q, G q , F q , S q , a q ) in MATHFUNC.
D planning = {(I plan (q), G q )|d ∈ D} D action = {(I action (q, F q ), S q )|d ∈ D}
Here I plan and I action are instruction templates for planning and action modules.We show these instructions in Appendix B.2, and mix up them as the training set D = (D planning D action ).Then we apply imitation learning on D to fine-tune M.
L M = (X,Y )∈D −logP(Y |X)
Implementation We detail the training process of (1) the retriever M retrieval and (2) the planner and actor M in Appendix B.1 and B.2, respectively.</p>
<p>Benchmark: SCITOOLBENCH</p>
<p>There currently exists no benchmark assessing the scientific reasoning capabilities of LLMs when aided by tools.To address this gap, we develop a benchmark called SCITOOLBENCH.Our benchmark covers five domains: Mathematics (math)5 , Physics, Chemistry, Finance, Electrical Engineering and Computer Science (EECS).Each domain is composed of a set of questions and a domainspecific toolset.The toolset consists of abundant generalized, high-quality and well-documented functions.We expect LLMs to retrieve, understand and, if necessary, use functions in it for reasoning.</p>
<p>Dataset Overview.</p>
<p>The statistics of SCITOOLBENCH are presented in Table 1.It comprises a total of 856 questions and 2,446 functions spanning across 5 scientific domains.Notably, SCITOOLBENCH differs from previous tool-based benchmarks, such as Creation Challenge (Qian et al., 2023), in several aspects:</p>
<p>(1) Our benchmark encompasses a diverse range of scientific domains.(2) The tools provided are both composable and generalized across different  questions.As indicated in Table 1, each question requires an average of 1.51 functions for resolution.And as shown in Figure 4, over 500 functions are designed to be applicable to two or more questions, such as integrate_function in math domain, coulombs_law in physical domain, and calculate_pressure_van_der_waals in chemistry domain.It signifies that the functions in our toolset are not ad-hoc solutions tailored for specific questions.Instead, the effective utilization of the toolset demands significant reasoning abilities of tool-augmented LLMs.Thus we claim this benchmark challenging and practical.</p>
<p>Questions</p>
<p>Function Generation</p>
<p>……</p>
<p>Function Refinement</p>
<p>……</p>
<p>Function Verification</p>
<p>Correctness Generalization</p>
<p>Function Generation</p>
<p>……</p>
<p>Toolset Construction</p>
<p>Positive functions Negative functions</p>
<p>Question Filtering</p>
<p>Toolset</p>
<p>TheoremQA SciBench</p>
<p>Original Dataset</p>
<p>Retained</p>
<p>Dataset Annotation</p>
<p>We design a pipeline shown in Figure 5 to annotate the benchmark.It employs both GPT-4 and human annotators to combine their merits.We introduce it  5, the same below) in our benchmark.Toolset Construction: We construct domainspecific toolsets via two cascade modules: positive and negative function construction.We define positive functions ( 2 ⃝) as functions directly deriving from questions.The candidate positive functions ( 2 ⃝) are firstly generated from GPT-4.Then human annotators carefully check them and rewrite and/or remove the unqualified ones.We further automatically construct negative functions ( 3 ⃝) based on positive functions to reduce the shortcuts in our benchmark.We finally combine both positive and negative functions as the toolset in our benchmark.</p>
<p>Experiments</p>
<p>Setup</p>
<p>We conduct experiments on SCITOOLBENCH to evaluate the tool-augmented scientific reasoning abilities of LLMs.We also employ CREATION Challenge (Qian et al., 2023) as the second benchmark.It comprises a total of 2,047 samples, with each sample consisting of a question and a groundtruth function.We aggregate all functions to assemble the toolset (thus including 2,047 functions).We report accuracy as the metric in all experiments.7) MAmmoTH-Coder (Yue et al., 2023b) and ( 8) ToRA-Coder (Gou et al., 2023b).We also list the performance of ChatGPT and GPT-4 for reference.We provide all LLMs the same retriever in Section 3.2 to retrieve functions from toolset (if attached).Please see more details in Appendix C.</p>
<p>Baselines</p>
<p>Main Results</p>
<p>We fine-tune CodeLlama, Mistral and Deepseek-Math for yielding SCIAGENT-CODER, SCIAGENT- ).However, it still falls significantly behind GPT-4 when both are provided with the same tools.Such gap highlights the challenges of tool-augmented scientific reasoning (and our benchmark).( 5) Both our agents and other baselines show relatively higher proficiency in the domains of math, finance, and EECS, but lower performance in physics and chemistry.We speculate that the first three domains align more closely with the training data's source distribution.Additional in-domain knowledge is demanding to further improve the performance in physics and chemistry domains.</p>
<p>Ablation Study</p>
<p>We investigate the effectiveness of components in our training data and agent modules.The specific variants we considered are as follows.</p>
<p>(1) We remove the planning module in the agent.</p>
<p>(2) We additionally drop the cross-retrieval strategy introduced in Section 3.2.In its place, we construct We illustrate the performance of our agents and their ablated variants in Table 3.We observe that (1) Planning module significantly improves scientific reasoning abilities.As detailed and targeted queries for the retriever, the generated plannings increase the relatedness of retrieved functions.For instance, the function's Recall@3 increases from 48.3% to 53.2% in physics domain, and from 37.3% to 39.8% in chemistry domain.(2) The use of the cross-retrieval strategy is essential.Otherwise, the function-augmented solutions directly from Fq and Sq degrade the performance because they are too artificial and ad-hoc to teach LLMs using functions properly.(3) The absence of function-augmented solutions results in a performance drop (row 1 v.s.row 4 in Table 3) of 5.9% and 5.3% in absolute accuracy for 7B and 13B LLMs, respectively.It underscores the critical role of function-augmented solutions to enhance LLMs' tool-use abilities, and the necessity of our MATHFUNC corpus.(4) The removal of function-free solutions (row 4 v.s.row 5) leads to an absolutely 14.4% accuracy decrease.Specifically focusing on non-math samples, there is a notable performance drop of about 12% as well.This clearly demonstrates the fundamental importance of math skills in diverse scientific reasoning tasks, and highlights how our math-related samples enhance LLMs' capabilities in this area.(5) Performance significantly declines when the retriever is removed.It illustrates that the retrieval module is crucial for accessing the appropriate functions from large-scale toolsets.</p>
<p>Analysis</p>
<p>Robustness of Toolsets.We acknowledge the construction and maintenance of toolsets is sometime challenging.Therefore, we stress the importance of our agents' robustness.If a sub-par toolset were provided, an robust agent should at the very least perform comparably, if not better, than other competitive LLMs without tool-use.To evaluate the robustness of SCIAGENT-CODER, we simulate two sub-par settings.(1) weak-related: for each question, we restrict the agents from retrieving functions that are directly derived from it.This setting greatly decreases the likelihood of retrieving a proper function from the toolset.( 2) unrelated: we completely remove the domain-specific toolset in SCITOOLBENCH.As a substitution, we provide the unrelated toolset constructed in MATHFUNC.We compare our agents with two competitive LLMs, i.e., ToRA-Coder and MAmmoTH-Coder, in above two settings.As shown in Table 4, (1) SCIAGENT series with unrelated toolsets present comparable performance with the two LLMs.In other words, our tool-augmented agents are unlikely to degrade the performance even under the extreme scenarios.(2) Our agents with weakrelated toolsets significantly outperform the two LLMs, which further validates the robustness.The Effect of Retriever Quality.We explore the effect of retriever quality on the ending performance.We substitute our fine-tuned retriever in SCIAGENT series by two competitive variants: SimCSE (Gao et al., 2021) and Contriever (Izacard et al., 2021).As shown in Figure 6 (top), our retriever surpasses the other two.It shows that finetuning on the math domain benefits the retrieval of tools in the generalized scientific domains.</p>
<p>We further dive deep into the relationship between the hit ratio of tools and the agents' performance.To this end, we manually control the hit@3 Bottom: Relationship between the performance and the hit@3 of retrieved functions (artificially controlled).</p>
<p>ratio by artificially adding/removing the positive functions to/from the retrieved list.Results in Figure 6 (bottom) show a clearly positive correlation between the hit ratio and the task accuracy.It illustrates that the retrieved functions facilitate the reasoning of scientific problems.However, we still observe a limit (40% accuracy) when the hit ratios reaching 100%, showing the challenge of scientific reasoning even when aided by tools.We hope the future work to bridge this performance gap.How the Retrieved Functions Benefit.To assess how the retrieved functions aid in the reasoning process of LLMs, we divided the samples into two subsets based on whether our agents use the retrieved functions to solve the problems.We evaluate the performance of these two subsets respectively, comparing with MAmmoTH-Coder series (without tool-use).The results in Figure 7 reveal a two-fold benefit: (1) For samples where functions are explicitly called to solve the questions, our agents demonstrate a substantial 25% improvement in absolute accuracy over LLMs that do not have access to functions.(2) Even for samples that do not directly use functions in their written program, we still observe a slight improvement.It suggests that our agents are capable of learning from retrieved functions as a reference, and then imitate these functions to write their own programs.For instance, example in Figure 12 shows the agents learn how to use scipy.integrateby observing the retrieved function aver-age_value_of_function(...).</p>
<p>Conclusion</p>
<p>This</p>
<p>Limitations</p>
<p>The primary limitation of our work comes from the way we compile the toolsets in SciToolBench.These tools are constructed directly based on the benchmark's questions, raising concerns about potential information leakage.To address this, we invest significant human effort in our annotation process as detailed in Appendix D.2.We manually review and, if necessary, revise all derived functions to ensure their generalizability and quality.As shown in Figure 6 (bottom), our agents achieve only about 40% accuracy when we provide each question the exact function from which it derives (i.e., 100% hit ratio).It not only highlights the inherent challenge of scientific reasoning tasks, but also suggests that our benchmark suffers minimal impact from the potential information leakage.We partly attribute this limitation to the absence of a training corpus among scientific (excluding math) domains.The scarcity of annotated solutions for scientific reasoning problems makes it unfeasible to set aside a portion of questions in our benchmark for tool creation.In future work, we plan to collect diverse and high-quality scientific annotations which enable us to develop a more practical and robust tool-augmented benchmark.</p>
<p>Ethics Statement</p>
<p>We ensure that SCITOOLBENCH was constructed in compliance with the terms of use of all source materials and with full respect for the intellectual property and privacy rights of the original authors of the texts.We also provide details on the characteristics and annotation steps of SCITOOLBENCH in Section 5 and Appendix D. We believe our created datasets do not cause any potential risks.</p>
<p>A Detailed Related Work</p>
<p>A.1 Scientific Reasoning Scientific reasoning can be roughly categorized into two branches: (1) mathematical reasoning and (2) reasoning across other scientific domains.</p>
<p>Mathematical Reasoning.Mathematical (math) reasoning has attracted much more attentions recently.Thanks to abundant training datasets and corpus, there are intensive studies for more powerful math-oriented LLMs by prompt engineering (Qian et al., 2023;Zhang et al., 2023b;Zhou et al., 2023), instruction-tuning (Yuan et al., 2023b;Yue et al., 2023b;Gou et al., 2023b;Yu et al., 2023;Wang et al., 2023a) and even pre-training (Luo et al., 2023;Azerbayev et al., 2023;Chern et al., 2023).Regarding instruction-tuning, we notice that recent studies have automatically constructed high-quality instructions from GPT-4, i.e.Reasoning across Other Domains.There have been intensive works on scientific LLMs (Bran et al., 2023;Jin et al., 2023;Fang et al., 2023) and benchmarks (Hendrycks et al., 2021a;Huang et al., 2023;Zhang et al., 2023a;Yue et al., 2023a;Sun et al., 2023).However, they primarily target on problems involving less complicated reasoning like knowledge retrieval or simple tool utilization.</p>
<p>Regarding complicated scientific reasoning problems (Chen et al., 2023b;Wang et al., 2023b), questions are scattered among diverse topics and each topic additionally requires domain-specific knowledge.So annotating questions and their solutions domain by domain is much more laborconsuming.Most current benchmarks (Chen et al., 2023b;Wang et al., 2023b;Zhao et al., 2023) merely include hundreds of questions (in all; less for each single domain) from textbooks and provide no training samples.A concurrent work (Zhang et al., 2024) develop a large-scale scientific training corpus, but only focuses three common domains: math, physical and chemistry.Accordingly, the progress of reasoning tasks in these domains is slower than that in math domain: the most competitive approach only achieves 50% and 35% on TheoremQA and SciBench, respectively, not to mention methods built on open-source LLMs.Instead of developing an omniscient and proficient LLMs on reasoning tasks across various scientific domains, we believe it is more practical to teach LLMs the ability to use domain-specific tools to facilitate their reasoning abilities in some domain when external functions (toolset) are attached.</p>
<p>A.2 Tool Learning</p>
<p>LLMs, both proprietary ones and open-source ones, demonstrate promising capabilities leveraging external tools to solve problems beyond their limits (Qin et al., 2023a).Combined with specific tools, these tool-augmented LLMs achieve great success on various tasks such as machine learning (Wu et al., 2023;Shen et al., 2023;Patil et al., 2023;Yang et al., 2023;Liu et al., 2023), question answering (Peng et al., 2023;Gou et al., 2023a), daily assistance (Xu et al., 2023a;Qin et al., 2023b;Song et al., 2023;Gao et al., 2023), etc.</p>
<p>Previous work usually pre-defines several tools, e.g., equation solver or calculator, to facilitate math reasoning tasks (Gou et al., 2023a;Lu et al., 2023;Hao et al., 2023;Chen et al., 2023c;Wang et al., 2023c;Xu et al., 2023b;Yin et al., 2023).Cai et al. (2023) generalize the concept of tools to Program functions.Following this concept, CRE-ATOR (Qian et al., 2023) scale up the function number towards thousand level.However, these ad-hoc, argument-free functions are more like solution wrapper rather than well-generalized tools.CRAFT (Yuan et al., 2023a) targetedly design an automatic pipeline to extract generalized functions for tool-use.Though leading to improvement, these functions are still not generalized enough and serve more as reference rather than as tools for direct calling.Ouyang et al. 2023 ask LLM to generate chemistry formulae as knowledge reference to assist the following reasoning and achieve enhanced performance on chemistry questions in SciBench.Similar as our attached toolset, Zhao et al. ( 2023) maintain a knowledge bank in which saves more than 900 financial definitions/equations/models as the format of functions for retrieval and use.To our best knowledge, our work is the first which (1) finetunes open-source, tool-augmented LLM agents for scientific reasoning tasks and (2) provides a benchmark covering multiple scientific domains to evaluate LLMs' tool-use abilities.</p>
<p>B Training Details B.1 Retriever</p>
<p>To fine-tune a retriever, we construct the training samples from MATHFUNC.We concatenate the question and its planning as the query, and view the generated functions as the keys.We finally collect a total of 8603 query-key pairs for training, and split 10% training samples as validation set.
query = [q; G q ] key = f ∈ Fq
We follow DPR (Karpukhin et al., 2020) to train a dense retriever R. We use ROBERTA-BASE (Liu et al., 2019) as the backbone.We set the training step as 500, the batch size as 128 and the learning rate as 2e-5.We also set the temperature coefficient of the InfoNCE loss (van den Oord et al., 2019) as 0.07.We run this experiment on a single NVIDIA Quadro RTX8000 GPU.The whole training process lasts for about 20 minutes.</p>
<p>B.2 Planning and Action</p>
<p>We fine-tune CodeLlamA (Rozière et al., 2023), Mistral (Jiang et al., 2023) and DeepMath (Shao et al., 2024) on MATHFUNC to develop the planning and action modules in our tool-augmented agents SCIAGENT series, yielding SCIAGENT-CODER, SCIAGENT-MISTRAL and SCIAGENT-DEEPMATH respectively.We set the global batch size as 128.We use the learning rate as 2e-5 for CodeLlama, 2e-6 for Mistral, and 5e-6 for Deep-Math.We use a cosine scheduler with a 3% warmup period for 2 epochs.We train all models with ZeRO Stage3 (Rajbhandari et al., 2021) on 8 V100 GPUs.The whole training process lasts for about 3 hours for 7B LLMs and 7.5 hours for 13B LLMs.</p>
<p>The planning and action modules share the same model but act differently with different input instructions.We detail the format of planning and action instructions as below: Planning.Given a question q, we construct a planning sample as (I plan (q), G q ), where I plan (q) is the input instruction, G q is the output, and I plan (.) is the template for planning module.We provide an example of planning instruction as below: Listing 1: An example of the planning sample.We separate the input instruction and output answer by the dashed line.</p>
<p>Read the following question and provide a high-level, step-by-step plan for this problem.Question: Two complementary angles are in a ratio of $3:2$.What is the measure, in degrees, of the smaller angle?</p>
<p>Answer:</p>
<p>----------------------------------------To solve this problem, we need to perform the following steps: -Set up an equation using the given ratio and the fact that the angles are complementary.-Solve the equation for the variable representing the ratio.-Calculate the measure of the smaller angle using the found variable value.</p>
<p>Action.Given a question q, we construct an action sample as (I action (q, F q ), S q ), where I action (q, F q ) is the input instruction, S q is the output answer, and I action (.) is the template for action module.F q are retrieved functions from the toolset in MATH-FUNC.We adopt the retriever in Appendix B.1 to retrieve three functions in the toolset.We provide an example of action instruction as below: Listing 2: An example of the action sample.We separate the input instruction and output answer by the dashed line.We only show one retrieved function in this sample for visualization convenience.</p>
<p>Read the following mathematical question and answer it.Please note that you could (optionally, not required) call the following functions when writing the code: def angle_ratio_to_degrees(ratio):</p>
<p>""" Computes the angle measures in degrees given their ratio.</p>
<p>Parameters:</p>
<p>-ratio (tuple): The ratio of the angles.</p>
<p>Returns:</p>
<p>-tuple: The angle measures in degrees.</p>
<p>Formula:</p>
<p>-angle_1 = ratio[0] * x -angle_2 = ratio[1] * x -angle_3 = ratio[2] * x -where x is the common variable such that angle_1 + angle_2 + angle_3 = 180 degrees """ sum_ratios = sum(ratio) x = 180 / sum_ratios return tuple(angle * x for angle in ratio)</p>
<p>[...Two more retrieved functions...] Question: Two complementary angles are in a ratio of $3:2$.What is the measure, in degrees, of the smaller angle?</p>
<p>Answer:</p>
<p>----------------------------------------# Define the ratio of the angles.ratio = (3, 2) # Since the angles are complementary, they add up to 90 degrees.# We can find the value of each part of the ratio by dividing 90 by the sum of the parts of the ratio.value_of_each_part = 90 / sum(ratio) # The measure of the smaller angle is the value of each part times the smaller part of the ratio.smaller_angle = value_of_each_part * min (ratio) # Print the result.print(smaller_angle)</p>
<p>C Evaluation Details</p>
<p>We evaluate (1) the tool-use abilities of LLMs in scientific reasoning task, and (2) their performance when supplemented without any toolsets for comparison.We detail these two settings as below: With Toolsets.We use the unified PoT-based prompt (Chen et al., 2023a) for all pretrainingbased models and our SCIAGENT series.The unified prompt consists of a short task description and two demonstrations.We show the prompt in Appendix F.4.For each question, we provide three retrieved functions and instruct LLMs to use them if (and only if) necessary.Note that we use the same retriever, i.e., fine-tuned from MATHFUNC, for all LLMs.For MAmmoTH-Coder and ToRA-Coder which are fine-tuned on specific (tool-agnostic) instructions, we try to enable them to use retrieved tools while keeping the formats of their original instructions as much as possible.Specifically, we append a short tool-augmented description at the end of their original prompts: Without Toolsets.Similar as above, we use the unified PoT-based prompt (Chen et al., 2023a) shown in Appendix F.5 for all pretraining-based models and our SCIAGENT series.And we follow the orig-inal instructions used for MAmmoTH-Coder and ToRA-Coder to evaluate their performance.</p>
<p>D Details of SCITOOLBENCH Annotation</p>
<p>We provide a more thorough description about SC-ITOOLBENCH construction in this section.This semi-automatic annotation pipeline involves both GPT-4 and humans to balance the quality and cost.Specifically, we enlist two authors to serve as human annotators.Both of them are graduate students with proficiency in English.Additionally, they hold Bachelor of Science and/or Engineering degrees and have completed undergraduate-level courses in the five scientific domains corresponding to our benchmark.We detail the three subsequent submodules in our annotation pipeline, i.e., question filtering, positive function construction and negative function construction, as below.</p>
<p>D.1 Question Filtering</p>
<p>We curate the questions from TheoremQA (Chen et al., 2023b) and SciBench (Wang et al., 2023b), both of which are available under the MIT License.Among 1495 questions in these original two datasets, we remove three kinds of questions.</p>
<p>Image-required: There are 37 questions from The-oremQA which include images and necessitate visual understanding abilities.We remove these samples because our benchmark is text-oriented.</p>
<p>Reasoning-agnostic: There are some multi-choice questions from TheoremQA which merely requires the memorization of knowledge points but involves little reasoning process.For example:</p>
<p>Question: The open mapping theorem can be proved by We manually check each samples and remove 68 such kind of samples.Over-difficult: Too hard questions confuse all models and weaken the discrimination of our benchmark.To balance the difficulty and discrimination, we employ 4 advanced proprietary models 6 to generate related functions and functionaugmented program solutions.We generate 6 so-lutions for each model (one generated by greedy decoding and the other five by nucleus sampling with 0.6 temperature) and 24 solutions in all.We view questions that are answered incorrectly by all 24 solutions as over-difficult questions.We remove all over-difficult questions, and retain 73.5% questions in TheoremQA and 47.8% in SciBench.</p>
<p>By removing three kinds of samples mentioned above, there are a total of 865 questions in our SCITOOLBENCH benchmark.</p>
<p>D.2 Positive Function Construction Function Generation</p>
<p>In practice, we merge this sub-module to the process of over-difficult question identification.We randomly sample one set of functions which yield correct solutions for each question.As a result, we collect a total of 1216 candidates for the next verification sub-module.We additionally save other functions leading to correct solutions and use them as reference in the refinement sub-module.</p>
<p>Function Verification</p>
<p>We verify the generated functions from both correctness and generalizations.We detail them separately as below.1. Correctness: Since all candidate functions lead to correct solutions, we speculate that almost all of them are correct.We randomly sample 100 functions (20 per domain) and manually check their correctness.The results shown in Table 5 validate our speculation.Therefore, we assume all candidate functions are correct and retain them.2. Generalization: We encounter the similar problem as the function construction in MATHFUNC, i.e., some of the auto-generated functions are not generalized enough.If ad-hoc functions were in the provided toolsets of our benchmark, they would cause a significant overestimation of LLMs' tooluse abilities.To mitigate it as much as possible, we manually check all candidate functions to ensure their generalization.Specifically, we design a binary classification task and assign each function a label in {Retained, Refined}.We label a function as refined if it had one of the problems listed below: (1) a pure solution wrapper.</p>
<p>(2) merely defining a non-generalized expression (likely only occur in this question).</p>
<p>(3) the argument names or document describing the special scenario of corresponding question and not being generalized/abstractive enough.(4) including adhoc constants or code snippets.The annotators firstly co-annotate 100 functions.We calculate Cohen's kappa value of their annotation results as 0.85, illustrating an ideal agreement.Therefore, the annotators separately annotate the remaining functions.It takes about 6 hours per annotator to classify about 650 functions.We show some Refined function cases in Figure 10, and the annotation interface in Figure 8.As a result, we collect 1012 Retained and 206 Refined functions.We keep all Retained as the component of positive functions.We also feed the Refined functions to next refinement sub-module to modify them as much as possible.</p>
<p>Function Refinement</p>
<p>This sub-module aims to rewrite 206 Refined functions to make them qualified.To this end, we associate each function with (1) the question from which it is derived, (2) the function-augmented solutions, and (3) the alternative functions from the generation sub-module (if have).Then we provide them to the annotators.The annotators are asked to rewrite the functions to improve their generalization as much as possible.If one function were successfully rewritten, we also require the annotator to write a solution involving the new function to the related question.The solution must yield correct answer to ensure the correctness of the rewritten function.We show some rewritten cases in Figure 10, and the screenshot of the annotation interface in Figure 9.</p>
<p>It takes approximately 12 hours per annotator to check each Refined function and, if applicable, rewrite it.As a consequence, we successfully rewrite 91 Refined functions and drop the remaining ones.We combine these 91 rewritten functions and the 1012 Retained functions to construct 1103 positive functions.</p>
<p>D.3 Negative Function Construction</p>
<p>The positive functions constructed above have satisfied the minimum requirements of the toolset in our benchmark.However, we find that such kind of benchmark contains shortcuts for LLM to retrieve and use functions.Take a physical question about frequency-angular conversion as example, the previous modules construct a positive function named angular_from_frequency(...) to solve this question.Without any other similar functions, the LLMs could readily select and use the only function by superficial shortcuts.These shortcuts significantly weaken the function-understanding and -use abilities evaluation of our benchmark.To mitigate this problem, we design an additional module to eliminate the shortcuts by constructing some (hard) negative functions for each positive function, like frequency_from_angular(...) and frequency_from_energy(...) in the above example.Among three similar functions, LLMs are forced to understand their usages and choose proper ones to use.In summary, we add negative functions into the toolset to simulate a more challenging scenario and better evaluate LLMs' tooluse abilities.</p>
<p>Listing 3: Prompt for constructing negative functions Given a function about the {subfield} field, could you please write two more functions which satisfy: -The functions should be in the same field with the provided function, while the knowledge point is not compulsorily the same.-The functions should be similar, but not identical with the provided function.</p>
<p>-The new written functions should be wrapped as the below format:
New function 1: <code>python [new_written_function_1] ``Ǹ ew function 2:</code>python [new_written_function_2] ``S
pecifically, we employ GPT-4 for each positive function to generate two similar but not identical functions as the negative functions.The prompt used is shown as below.We do not validate the correctness of negative functions for simplicity, as they are not intended to be used for any question.We filter the duplicated functions and retain the other 1343 functions in all.By merging the 1103 positive functions and 1343 negative functions, we finally collect a total of 2446 functions in our toolset.def birge_vieta(p, tol=1e-3, max_iter=100):</p>
<p>""" Finds a real root of the polynomial x^3 -11x^2 + 32x -22 using the Birge-Vieta method.</p>
<p>Parameters:</p>
<p>-p (float): The initial guess for the root.</p>
<p>-tol (float, optional): The desired tolerance for the root.Default is 1e-3.</p>
<p>-max_iter (int, optional): The maximum number of iterations.Default is 100.</p>
<p>Returns:</p>
<p>-float: The real root of the polynomial found using the Birge-Vieta method.</p>
<p>""" for _ in range(max_iter): p_new = p -polynomial(p) / polynomial_derivative(p) if abs(p_new -p) &lt; tol: return p_new p = p_new raise ValueError("Birge-Vieta method did not converge within the maximum number of iterations.")def birge_vieta_iteration(polynomial, p, tol=1e-3, max_iter=100):</p>
<p>""" Finds a real root of a polynomial using the Birge-Vieta method.</p>
<p>Parameters:</p>
<p>-polynomial (sympy expression): The polynomial for which the root is to be found.</p>
<p>-p (float): The initial guess for the root.</p>
<p>-tol (float): The desired tolerance for the root.</p>
<p>-max_iter (int): The maximum number of iterations allowed.</p>
<p>Returns:</p>
<p>-float: The real root of the polynomial, if found within the maximum number of iterations.</p>
<p>Raises a ValueError if the root is not found within the maximum number of iterations.</p>
<p>"</p>
<p>Function before rewriting Function after rewriting</p>
<p>Rewrite the specific polynomial (and its derivative) to an argument of the function def calculate_emptying_time(height, radius, side_length, g=9.81): """ Calculates the time it takes for a cylindrical tank to go from full to empty.</p>
<p>Returns:</p>
<p>-int: 1 if the CDF is log-concave, 0 otherwise.</p>
<p>Note:</p>
<p>-The second derivative of the natural logarithm of the CDF of the standard Gaussian distribution is always non-positive.</p>
<p>Therefore, the function is log-concave, and we can return 1 without performing any calculations.</p>
<p>""" return 1 def is_log_concave(f, x): """ Determines if a given function <code>f</code> with respect to variable <code>x</code> is logconcave.</p>
<p>Parameters:</p>
<p>-f (sympy expression): The function for which the log-concavity is to be checked.</p>
<p>-x (sympy symbol): The variable with respect to which log-concavity is to be checked.We also briefly describe the modifications for each example (color in red).</p>
<p>Figure 1 :
1
Figure 1: Two paradigms for scientific reasoning.Different colors represent different scientific domains.Left: Collecting annotations and fine-tuning LLMs domain by domain.Right: Our proposed tool-augmented setting.LLMs are fine-tuned on math-related, tool-augmented samples (color in red).When adapting LLMs to a specific domain, a pluggable and domain-specific toolset is attached.No additional fine-tuning is further required.</p>
<p>Figure 2 :
2
Figure2: Automatic pipeline for MATHFUNC construction.Please view it starting from the bottom left corner and proceed clockwise.We disentangle the constructions of toolset (dashed lines) and function-augmented samples (solid lines) for more generalized annotations.We do not visualize the function-free samples for simplicity.</p>
<p>Toolset ( 1 )Figure 3 :
13
Figure 3: The model architecture of SCIAGENT.Given a domain-specific toolset , our agent answers the question through four consecutive modules.(1) Planning : provides a high-level plan for this problem.(2) Retrieval : retrieves related functions from attached toolset.(3) Action : generates a low-level solution interleaving rationale and program.The program uses the retrieved functions if necessary.(4) Execution : calls Python executor to run the program and outputs the final answer.Not included in this figure for simplicity.</p>
<p>Figure 4 :
4
Figure 4: Left: Histogram of FPQ (function per question).Higher values indicate greater composability.Right: Histogram of function occurrence.Higher values indicate more generalization and wider application.</p>
<p>Figure 5 :
5
Figure 5: Semi-automatic annotation pipeline for SCI-TOOLBENCH.: GPT-4.: Human annotator.</p>
<p>We compare SCIAGENT series with eight opensource LLMs: (1) LLaMA-2 (Touvron et al., 2023), (2) CodeLlama (Rozière et al., 2023), (3) Mistral (Jiang et al., 2023), (4) Llemma (Azerbayev et al., 2023), (5) Deepseek-Coder (Guo et al., 2024), (6) Deepseek-Math (Shao et al., 2024), (</p>
<p>Figure 6 :
6
Figure 6: Top: Performance of SCIAGENT-CODER on SCITOOLBENCH with different retriever variants.Bottom: Relationship between the performance and the hit@3 of retrieved functions (artificially controlled).</p>
<p>Figure 7 :
7
Figure 7: The performance of SCIAGENT-CODER (w.toolset) and MAmmoTH-Coder (wo.toolset) on samples which (1) use and (2) not use retrieved functions.</p>
<p>, finetuning open-source LLMs by Program-of-thought (PoT; Chen et al. 2023a) prompting.It enables open-source LLMs to present remarkable performance, even comparable with GPT-4.</p>
<p>(a) Baire category theorem.(b) Cauchy integral theorem.(c) Random graph theorem.(d) None of the above.</p>
<p>Figure 8 :
8
Figure 8: The screenshot of our annotation interface to evaluate functions' generalization.</p>
<p>Figure 9 :
9
Figure9: The screenshot of our annotation interface to rewrite functions.We provide no alternative functions in this example for convenience of visualization.</p>
<p>float): The height of the cylindrical tank.-radius (float): The radius of the cylindrical tank.-side_length (float): The length of the side of the square hole in the bottom of the tank.-g (float): The acceleration due to gravity.Returns: -float: The time it takes for the tank to empty.""" from math import pi, sqrt # Calculate the area of the tank and the hole tank_area = pi * radius<strong>2 hole_area = side_length</strong>2 # Use Torricelli's law to calculate the time time = (2 * height * tank_area) / (sqrt(2<em>g</em>height) * hole_area) return time def calculate_drain_time(volume, area, gravity=9.81):"""Calculates the time it takes for a cylindrical object to drain using Torricelli's Law.Parameters:-volume (float): The volume of the cylindrical object.-area(float): The area of the hole through which the object is draining.-gravity(float): The acceleration due to gravity.Returns:-float: The time it takes for the object to drain.""" from math import sqrt return volume / (area * sqrt(2*gravity)) cumulative distribution function (CDF) of the standard Gaussian distribution is log-concave.</p>
<p>Figure10: Three examples of Refined functions (before rewriting) and their rewritten functions (after rewriting).We also briefly describe the modifications for each example (color in red).</p>
<p>Table 1 :
1
The statistics of our benchmark.#Func: Number of functions.#Pos./#Neg.: The number of positive/negative functions in the toolset.FPQ (function per question): The number of derived positive functions from each question.</p>
<h1>Question # Func # Pos. / # Neg. Avg. FPQMath4341072511 / 5611.47Physics156534243 / 2911.63Chemistry118366155 / 2111.34Finance6625397 / 1561.62EECS8222197 / 1241.68All85624461103 / 13431.51</h1>
<p>Table 2 :
2
Main results on two benchmarks.We highlight our SCIAGENT series in blue .The best results (among all open-source LLMs, the same below) are in bold face and the second best are underlined.
ModelSize Toolset CREATIONSCITOOLBENCH Math Physics Chemistry Finance EECSAllChatGPT-✗ ✓54.6 59.833.4 32.019.2 31.418.6 33.953.0 53.025.6 48.829.6 35.4GPT-4-✗ ✓60.0 69.852.8 63.142.9 63.547.5 63.665.2 80.335.4 80.549.5 66.2LLaMA27B✓12.64.310.98.413.611.08.3CodeLlama7B✗17.76.50.65.14.97.65.1CodeLlama7B✓26.19.28.310.224.225.611.9Llemma7B✗26.410.44.58.510.67.38.8Llemma7B✓34.316.421.214.436.422.019.1Mistral7B✗30.111.34.57.616.76.19.5Mistral7B✓27.613.113.514.434.819.515.6Deepseek-Coder7B✗36.820.38.35.922.712.215.5Deepseek-Coder7B✓31.321.015.410.230.336.620.7Deepseek-Math7B✗44.726.519.217.827.320.723.5Deepseek-Math7B✓41.324.224.425.443.942.727.7ToRA-Coder7B✗29.726.34.56.89.124.418.1ToRA-Coder7B✓21.421.74.55.113.615.915.1MAmmoTH-Coder7B✓21.614.818.511.025.840.019.7SCIAGENT-CODER7B✓53.030.028.324.639.357.332.2SCIAGENT-MISTRAL7B✓54.031.328.822.951.561.034.1SCIAGENT-DEEPMATH7B✓60.441.254.544.957.551.246.3LLaMA213B✓23.312.211.56.822.714.612.4CodeLlama13B✗23.09.93.21.79.16.17.1CodeLlama13B✓38.912.714.77.633.334.116.0ToRA-Coder13B✗30.928.63.84.216.730.520.0ToRA-Coder13B✓28.032.02.611.924.235.423.6MAmmoTH-Coder13B✓34.721.418.611.025.839.021.5SCIAGENT-CODER13B✓54.435.032.128.842.451.235.7briefly as below and leave details in Appendix D.Question Filtering: We curate questions from The-oremQA (Chen et al., 2023b) and SciBench (Wanget al., 2023b) to collect 856 questions ( 1 ⃝ in Fig-ure</p>
<p>Table 3 :
3
Ablation study on SCITOOLBENCH.We report the accuracy of samples across (1) all domains, (2) four domains excluding the math domain (wo.math).
Planning Function-augmented Function-free RetrieverAccuracy (7B)Accuracy (13B)solutionssolutionsAllwo. mathAllwo. mathSciAgent-Coder✓✓(cross-retrieval)✓✓32.234.635.736.5Intermediate variants 1-3✗ ✗ ✗✓(cross-retrieval) ✓(direct-use) ✗✓ ✓ ✓✓ ✓ ✓30.3 17.8 26.333.9 17.3 26.132.8 26.6 30.434.4 31.0 31.7CodeLlama✗✗✗✓11.914.716.019.4wo. retriever✗✗✗✗5.13.87.14.3MISTRAL and SCIAGENT-DEEPMATH. We showtheir results in Table 2 and observe: (1) Almost allLLMs present improved performance, i.e., 5.3%absolute and 61.6% relative accuracy increase onaverage, when supplemented with toolsets. Itvalidates the promise of the tool-augmented set-ting for scientific reasoning. (2) The models fine-tuned on math-related datasets from CodeLlama,i.e., ToRA-and MAmmoTH-Coder, perform bet-ter than CodeLlama itself by 5.5% abosolute ac-curacy. It presents the importance of essentialmath skills among diverse scientific domains. (3)Our agents consistently outperform other open-source LLMs by a large margin. Notably, SCIA-GENT-CODER surpasses the most competitive base-line, MAmmoTH-Coder, by absolute accuracy of12.5% and 14.2% on the 7B and 13B versions. (4)Our strongest agent, SCIAGENT-DEEPMATH-7B,substantially outperforms ChatGPT with toolset(46.3% v.s. 35.4%) and shows comparable resultsto GPT-4 without toolset (46.3% v.s. 49.5%</p>
<p>(Chen et al., 2023a)olutions directly from Fq and Sq .(3)Wefurtherremoveallfunction-augmentedsolutions from our training data, and only keep the solutions without function callings (functionfree solutions).(4)Wedonotfine-tuneagents but merely use CodeLlama as M action for inference.(5)Wedropthe retriever to disable the LLMs' tool-use abilities.Equivalently, it degrades to the baseline of CodeLlama + PoT(Chen et al., 2023a)prompting.</p>
<p>Table 4 :
4
Accuracy on SCIAGENT with sub-par toolsets.WR: weak-related toolsets.UR: unrelated toolsets.NA: No toolset.The subscripts indicate the difference from the best LLMs (wo.toolsets) each column.
ModelToolsetAccuracy (7B) All wo.mathAccuracy (13B) All wo. mathSciAgentWR18.8+0.7 18.0+8.3 24.6+4.619.9+7.6-CoderUR14.7−3.7 10.7+1.0 20.3+0.314.7+2.4MAmmo-CNA12.79.016.412.3ToRA-CNA18.19.720.011.1</p>
<p>Table 5 :
5
The correctness of 100 randomly sampled functions across five domains.
Correct Partially Correct Wrong AllMath182020Physics191020Chemistry200020Finance190120EECS173020All9361100</p>
<p>No variables found in the polynomial.")if len(variables) &gt; 1: raise ValueError("The polynomial contains more than one variable.")variable = variables[0] # Compute the derivative of the polynomial derivative = diff(polynomial, variable) # Convert the polynomial and its derivative to functions f = lambdify(variable, polynomial, 'numpy') f_prime = lambdify(variable, derivative, 'numpy') # Iterate using the Birge-Vieta method for _ in range(max_iter): p_new = p -f(p) / f_prime(p) if np.abs(p_new -p) &lt; tol: return p_new p = p_new raise ValueError("Maximum number of iterations reached without convergence.")
""from sympy import lambdify, diffimport numpy as np# Extract the variable from the polynomialvariables = list(polynomial.free_symbols)if not variables:raise ValueError("
In this work, tools refer to Python functions. We use tools and functions interchangeably unless otherwise specified.
We remove the domain-specific subscript D for expression simplicity. The same below.
  3  Here Eq is written in natural language but formatted as the annotation lines in the program.
Despite we instruct GPT-4 to avoid generating ad-hoc functions, there are still some ad-hoc functions in Fq
Our benchmark contains college-level questions on calculus, differential equations, group theory, etc, which are different from the questions in our training corpus MATHFUNC.
 gpt-4, gpt4-32k, gpt-3.5-turbo, gpt-3.5turbo-16k <br />
Breaking the gpu memory wall for extreme scale deep learning.In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC '21, New York, NY, USA.Association for Computing Machinery.E ExamplesProblem &amp; PlaningRead the following mathematical question and answer it.Please note that you could (optionally, not required) call the following functions when writing the code: Question:Suppose a stock has the following information.It is listed on the London stock exchange and operates throughout Europe.The yield on a UK 10 year treasury is 2.8%.The stock in question will earn 8.6% as per historical data.The Beta for the stock is 1.4, i.e., it is 140% volatile to the changes in the general stock market.What is the expected rate of return?(1) Calculate the expected return based on the beta and the yield on a UK 10 year treasury.(2) Add the stock's historical return to the expected return.Retrieved Functionsdef expected_return(rf, beta, rm):""" Computes the expected return using the Capital Asset Pricing Model (CAPM) formula.Parameters:-rf (float): The risk-free rate.-beta (float): The beta of the portfolio.-rm (float): The return on the market.Returns:-float: The expected return.""" return rf + beta * (rm -rf) def expected_stock_return(true_prob, u, d):""" Calculates the expected return of the stock in a one-period binomial model.Parameters:-true_prob (float): The true probability of the stock price going up.-u (float): One plus the rate of capital gain on the stock if the price goes up.-d (float): One plus the rate of capital loss on the stock if the price goes down.Returns:-float: The expected return of the stock.""" return true_prob * u + (1 -true_prob) * d ...Solution# Given values.rf = 0.028 # The yield on a UK 10 year treasury beta = 1.4 # The stock is 140% volatile to the changes in the general stock market rm = 0.086 # The stock in question will earn 8.6% as per historical data # Calculate the expected return .expected_return = expected_return(rf, beta, rm) # Print the result.print(expected_return)Problem &amp; PlaningRead the following mathematical question and answer it.Please note that you could (optionally, not required) call the following functions when writing the code:Question:The linear density in a rod $8 \mathrm{~m}$ long is $12 / \sqrt{x+1} \mathrm{~kg} / \mathrm{m}$ , where $x$ is measured in meters from one end of the rod.Find the average density of the rod.(1) Integrate the linear density function with respect to x from 0 to 8.(2) Divide the result by the length of the rod.Retrieved FunctionsSolution# The linear density is given by 12 / sqrt(x+1) kg/m.# The average density is the total mass of the rod divided by its length.# The total mass of the rod is the integral of the linear density times the length of the rod from 0 to 8. from sympy import symbols, integrate x = symbols('x') length = 8 # Define the linear density function.linear_density = 12 / (x + 1)<strong>0.5 # Calculate the total mass of the rod.total_mass = integrate(linear_density, (x, 0, length)) # Calculate the average density by dividing the total mass by the length.average_density = total_mass / length print(average_density) Figure12: An example of implicitly using functions as a reference.F PromptsF.1 Tool/Function CollectionListing 4: Prompt used for planning and function collectionGiven the question and the reference solution, do the following things:-Think about what math knowledge points are required to solve this problem step by step.-write some python one or more functions to abstract the solution.Please note that the functions should be well-documented as much as possible and not too specific (for example, do not write the values in this problem within the functions.Pass them as the function arguments).We hope your written functions could be re-used in anywhere else.-Instantiate these functions to solve the problem.The last line of your program should be a 'print' command to print the final answerHere are some examples you may refer to:Question: There are integers $b,c$ for which both roots of the polynomial $x^2-x-1$ are also roots of the polynomial $x^5-bx-c$.Determine the product $bc$.Answer: Let $r$ be a root of $x^2-x-1$.Then, rearranging, we have\n$$r^2 = r+1.$$Multiplying both sides by $r$ and substituting gives\n\begin{align<em>}\nr^3 &amp;= r^2+r \\\n&amp;= (r+1)+r \\\n&amp;= 2r+1.\n\end{align</em>}Repeating this process twice more, we have\n\begin{align<em>}\nr^4 &amp;= r(2r+1) \\\n&amp;= 2r^2+r \\\n&amp;= 2(r+1)+ r \\\n&amp;= 3r+2\n\end{align</em>}and\n\begin{align<em>}\nr^5 &amp;= r(3r+2) \\\n&amp;= 3r ^2+2r \\\n&amp;= 3(r+1)+2r \\\n&amp;= 5r+3.\n\end{align</em>}Thus, each root of $x^2-x -1$ is also a root of $x^5-5x-3$, which gives $bc = 5\cdot 3 = \boxed{15}$.Think: To solve this question, we can follow the steps below: (1) Find the roots of the polynomial $x^2-x-1$.(2) Substitute them into the the polynomial $x^5-bx-c$ and obtain two equations.(3) Solve the equations.Functions: <code>function 1 def find_roots_of_polynomial(polynomial, variable):""" Finds the roots of a given polynomial using the sympy library.Parameters:-polynomial (sympy expression): The polynomial whose roots are to be found.-variable (sympy symbol): The variable of the polynomial.Returns:-list: The roots of the polynomial.""" from sympy import solve roots = solve(polynomial, variable) return roots</code>f unction 2 def substitute_roots_into_polynomial(roots, polynomial, variable):""" Substitutes the given roots into the polynomial and returns the resulting expressions.Parameters:-roots (list): The list of roots to be substituted into the polynomial.-polynomial (sympy expression): The polynomial into which the roots are to be substituted.-variable (sympy symbol): The variable of the polynomial.Returns:-list: The resulting expressions after substituting the roots into the polynomial.""" return[polynomial.subs(variable, root)for root in roots] <code></code>function 3 def solve_equations(equations, variables):""" Solves a system of equations for the specified variables using the sympy library.Parameters:-equations (list of sympy expressions or a single sympy expression):The equations to be solved.If solving a single equation, this can be a single expression.-variables (list of sympy symbols or a single sympy symbol):The variables for which the solution is to be found.If solving for a single variable, this can be a single symbol.Returns:-list of dictionaries: Each dictionary represents a solution, with keys being the variables and values being their corresponding values.If there's only one solution, the list will contain a single dictionary.""" from sympy import solve solution = solve(equations, variables, dict=True) return solution <code>S olution: ```python # Import required functions and classes from sympy from sympy import symbols, Eq Therefore, $DG = \\frac23(DP) = 12$ and $EG = \\frac23(EQ) =16$, so applying the Pythagorean Theorem to $\\triangle EGD$ gives us $DE = \\sqrt{EG^2 + GD^2} = \\ boxed{20}$.Think: Given two perpendicular medians in a triangle, we need to perform the following steps: (1) Identify the relationship between the segments of medians and the centroid.(2) Use the ratios provided to determine the lengths of the individual segments from the centroid to the vertices.(3) Use the Pythagorean theorem to determine the length of the side connecting the two vertices from which the medians originate.Functions: ```function 1 def median_segments_length(median_length, ratio):""" Computes the lengths of the segments of a median split by the centroid.Parameters:-median_length (float): Total length of the median.-ratio (tuple): Ratio in which the centroid splits the median.Default is (2,1) for standard triangles.Returns:-tuple: Lengths of the two segments.Formula:[sol[d]for sol in solution]\n return d_values\nd_values = find_d_values()\nprint(d_values)\n```\n\nError Message:\nFile &lt;string&gt;:15\n\ nFile &lt;string&gt;:9, in find_d_values()\n\nNameError: name 'Eq' is not defined" }, { "role": "assistant", "content": "Analysis: The error is occurring because the Eq function from the sympy module is not recognized in the find_d_values function.This is happening because the ```from sympy import symbols, Eq, solve```line is only inside the ```solve_quadratic```function, so Eq and other imported functions and classes are only available within the scope of that function.To fix this, you should claim import statement within the ```find_d_values```function as well.\n\nNewProgram:\n```function 1\ndef solve_quadratic(a, b, c):\n from sympy import symbols, Eq, solve\n x = symbols('x')\n quadratic = Eq(a * x**2 + b * x + c, 0)\n solutions = solve(quadratic, x)\n return solutions\n```\n\n ```python\ndef find_d_values():\n from sympy import symbols, Eq, solve\n a, b, c, d = symbols('a b c d')\n eq1 = Eq(6 * a * d, a + d)\n eq2 = Eq((6 * d -1) * (1 -a), a + d)\n eq3 = Eq(a, 4 * d -1)\n solution = solve([eq1, eq2, eq3], (a, b, c, d), dict=True)\n d_values = [sol[d]for sol in solution]\n return d_values\nd_values = find_d_values()\nprint(d_values)\n```" }, { "role": "user", "content": "finished" }, { "role": "user", "content": "Original Program:\n```function 1\ndef matrix_square(matrix):\n from sympy import Matrix\n if not isinstance(matrix, Matrix):\n matrix = Matrix( matrix)\n return matrix * matrix\n```\n\n```function 2\ndef solve_equations( equations, variables): \n from sympy import solve \n solution = solve( equations, variables, dict=True) \n return solution \n```\n\n```function 3\ ndef count_solutions(solutions):\n return len(solutions)\n```\n\n```python\ nfrom sympy import symbols, Eq \na, d = symbols('a d') for i in range(2) for j in range(2)] \nsolution = solve_equations(equations, [a, d ]) \nans = count_solutions(solution)\nprint(ans)\n```\n\nError Message:\nFile &lt;string&gt;:17\n\nFile &lt;string&gt;:17, in &lt;listcomp&gt;(.0)\n\nTypeError:'Add' object is not subscriptable" }, { "role": "assistant", "content": "Analysis: The error message is occurring because when you are creating the list of equations with the list comprehension, squared_matrix[i ][j] is not working as expected.The squared_matrix is a SymPy Matrix object, and you should use the .rowand .colmethods.\n\nNewProgram:\n```function 1\ ndef matrix_square(matrix):\n from sympy import Matrix\n if not isinstance( matrix, Matrix):\n matrix = Matrix(matrix)\n return matrix * matrix\n```\n\n ```function 2\ndef solve_equations(equations, variables): \n from sympy import solve \n solution = solve(equations, variables, dict=True) \n return solution \n```\n\n```function 3\ndef count_solutions(solutions):\n return len( solutions)\n```\n\n```python\nfrom sympy import symbols, Eq \na, d = symbols(' a d') \nmatrix =[[a, 3],[-8, d]] \nsquared_matrix = matrix_square(matrix) for i in range(2) for j in range(2)] \nsolution = solve_equations(equations,[a, d]) \nans = count_solutions(solution)\nprint( ans)\n```" }, { "role": "user", "content": "finished" }, { "role": "user", "content": "Original Program:\n```function 1\ndef sum_to_product_cos(alpha, beta) :\n return (cmath.cos(alpha-beta) + cmath.cos(alpha+ beta)) / 2\n```\n\n</code>f unction 2\ndef simplify_expression(expression): \n from sympy import simplify \n return simplify(expression) \n<code>\n\n</code>python\nfrom sympy import symbols, cos\n\nx = symbols(' "role": "user", "content": "finished" }, { "role": "user", "content": "Original Program:\n<code>function 1\ndef tan_double_angle(tan_theta):\n return 2 * tan_theta / (1 -tan_theta**2)\n</code>\n\n<code>function 2\ndef tan_triple_angle(tan_theta):\n return (3 * tan_theta -tan_theta**3) / (1 -3 * tan_theta**2)\n</code>\n\n<code>function 3\ndef solve_polynomial(polynomial, x):\n from sympy import solve\n return solve(polynomial, x)\n</code>\n\n<code>python\nfrom sympy import symbols\ntan_theta = symbols('tan_theta')\ntan_2_theta = tan_double_angle(tan_theta)\ntan_3_theta = tan_triple_angle(tan_theta)\ nequation = tan_theta + tan_2_theta + tan_3_theta\nsimplified_equation = simplify_expression(equation)\ntan_theta_solutions = solve_polynomial( simplified_equation, tan_theta)\n\n# Find the solution in the given range (0 &lt; tan_theta &lt; 1)\ntan_theta_value = [solution for solution in tan_theta_solutions if 0 &lt; solution &lt; 1][0]\nprint(tan_theta_value)\n</code>\n\ nError Message:\nFile <string>:13\n\nNameError: name 'simplify_expression' is not defined" }, { "role": "assistant", "content": "Analysis: The error message is occurring because there is no function named simplify_expression defined in your script.To fix this, you can define such a function that uses SymPy's simplify function to simplify expressions.\n\nNewProgram:\n<code>function 1\ndef simplify_expression( expression): \n from sympy import simplify \n return simplify(expression)\n</code>\n\n<code>function 2\ndef tan_double_angle(tan_theta):\n return 2 * tan_theta / (1 -tan_theta**2)\n</code>\n\n<code>function 3\ndef tan_triple_angle(tan_theta):\n return (3 * tan_theta -tan_theta**3) / (1 -3 * tan_theta**2)\n</code>\n\n<code>f unction 4\ndef solve_polynomial(polynomial, x):\n from sympy import solve\n return solve(polynomial, x)\n```\n\n```python\nfrom sympy import symbols\ ntan_theta = symbols('tan_theta')\ntan_2_theta = tan_double_angle(tan_theta)\ ntan_3_theta = tan_triple_angle(tan_theta)\nequation = tan_theta + tan_2_theta + tan_3_theta\nsimplified_equation = simplify_expression(equation )\ntan_theta_solutions = solve_polynomial(simplified_equation, tan_theta)\n\n # Find the solution in the given range (0 &lt; tan_theta &lt; 1)\ntan_theta_value = [solution for solution in tan_theta_solutions if 0 &lt; solution &lt; 1][0]\nprint (tan_theta_value)\n```" }, { "role": "user", "content": "finished" }F.3 Function-augmented SolutionsListing 6: Prompt used for the generation of function-augmented solutions (cross-retrieval strategy)You will encounter a mathematical problem and are required to write a piece of Python code to solve this problem.Now we have a suite of wrapped functions.Take note:-The newly provided wrapped functions have NOT been verified.They may be irrelevant or potentially flawed.-It's essential that the solution doesn't overly depend on wrapped functions.You're welcome to utilize one or more functions from the new set in your solution but only after you've determined: (1) Their accuracy.(2) Their inclusion significantly streamlines the problem-solving approach.Additionally take note that(1) The last line of your written code shall be a 'print' command to print the final answer.(2) The wrapped functions should not be duplicated within your code.Instead, call them directly if needed.(3) Should you need to create custom functions, do so without adding documentation comments for the sake of brevity.(4) Write simple but clear annotations interleaving your code solution.""" Retrieved functions: [List of called function names from the new set] ```python [Your Written Python Code.]</code>" "" For example: ---Question: What is the 100th digit to the right of the decimal point in the decimal representation of $\frac{13}{90}$?New provided functions: <code>New Function 0 def decimal_representation(numerator, denominator, max_digits=1000): """ Computes the decimal representation of a fraction.Parameters:-numerator (int): The numerator of the fraction.-denominator (int): The denominator of the fraction.-max_digits (int): The maximum number of decimal digits to compute.Returns:-str: The decimal representation of the fraction as a string.""" result = "" remainder = numerator % denominator for _ in range(max_digits): remainder *= 10 result += str(remainder // denominator) remainder %= denominator if remainder == 0: break return result</code>N ew Function 1 def decimal_to_scientific(decimal_number): from sympy import log, floor exponent = -floor(log(decimal_number, 10)) coefficient = decimal_number * 10</strong>(-exponent) return coefficient, exponent <code>N ew Function 2 def repeating_decimal_representation(numerator, denominator):""" Computes the repeating decimal representation of a fraction.Parameters:-numerator (int): The numerator of the fraction.-denominator (int): The denominator of the fraction.Returns:-str: The repeating decimal representation of the fraction as a string.""" # Initialize the result string and a dictionary to store remainders.result = "" remainders = {} # Perform long division to find the decimal representation.while numerator != 0: # If the remainder has been seen before, we found the repeating block.if numerator in remainders: start = remainders[numerator] return result[:start] + "(" + result[start:] + ")" # Otherwise, store the remainder and continue the division.remainders[numerator] = len(result) numerator *= 10 result += str(numerator // denominator) numerator %= denominator return result</code> <code>New Function 3 def nth_digit_of_decimal_representation(numerator, denominator, n):""" Computes the nth digit after the decimal point of the decimal representation of a fraction.Parameters:-numerator (int): The numerator of the fraction.-denominator (int): The denominator of the fraction.-n (int): The position of the digit after the decimal point.Returns:-int: The nth digit after the decimal point of the decimal representation of the fraction.""" # Get the repeating decimal representation of the fraction.decimal_representation = repeating_decimal_representation(numerator, denominator) # Remove the parentheses from the repeating block.decimal_representation = decimal_representation.replace("(","").replace(")", "") # Calculate the nth digit using the repeating block.return int(decimal_representation[(n -1) % len(decimal_representation)]) ``R Returns:-float: The value of x that satisfies the equation.Formula:-x = (c^2 + b) / a """ return (c**2 + b) / a</code>N ew Function 1 def find_integer_square_less_than_double():""" Finds the only integer whose square is less than its double.Returns:-int: The integer that satisfies the condition.Method:-Iterate through integers starting from 1, and check if the square of the integer is less than its double.-If the condition is satisfied, return the integer.-If the condition is not satisfied for any integer up to a certain limit, return None.""" limit = 100 for x in range(1, limit): if x<strong>2 &lt; 2*x: return x return None ```N ew Function 2 def solve_equation():""" Solves the equation (x-2)^(25-x^2) = 1 for integer solutions.""" Computes the nth digit after the decimal point of the decimal representation of a fraction.Parameters:-numerator (int): The numerator of the fraction.-denominator (int): The denominator of the fraction.-n (int): The position of the digit after the decimal point.Returns:-int: The nth digit after the decimal point of the decimal representation of the fraction.""" # Get the repeating decimal representation of the fraction.decimal_representation = repeating_decimal_representation(numerator, denominator) # Remove the parentheses from the repeating block.decimal_representation = decimal_representation.replace("(","").replace(")", "") # Calculate the nth digit using the repeating block.return int(decimal_representation[(n -1) % len(decimal_representation)]) def decimal_representation(numerator, denominator, max_digits=1000):""" Computes the decimal representation of a fraction.Parameters:-numerator (int): The numerator of the fraction.-denominator (int): The denominator of the fraction.-max_digits (int): The maximum number of decimal digits to compute.Returns:-str: The decimal representation of the fraction as a string.---------------------------------------------------Question:The square root of $x$ is greater than 3 and less than 4. How many integer values of $x$ satisfy this condition?Functions: def count_integers_in_range(lower_bound, upper_bound, exclude_zero=True): """ Counts the number of integers within a given range.Parameters:-lower_bound (int): The lower bound of the range.-upper_bound (int): The upper bound of the range.-exclude_zero (bool): Whether to exclude 0 from the count.Default is True.Returns:-int: The number of integers within the range.""" count = upper_bound -lower_bound + 1 if exclude_zero and lower_bound &lt;= 0 and upper_bound &gt;= 0: count -= 1 return count def find_integer_square_less_than_double(): """ Finds the only integer whose square is less than its double.Returns:-int: The integer that satisfies the condition.Method:-Iterate through integers starting from 1, and check if the square of the integer is less than its double.-If the condition is satisfied, return the integer.-If the condition is not satisfied for any integer up to a certain limit, return None.""" limit = 100 for x in range(1Returns:-float: The value of x that satisfies the equation.Formula:-x = (c^2 + b) / a """ return (c</strong>2 + b) / a Solution: # We need to find the integer values of x for which sqrt(x) &gt; 3 and sqrt(x) &lt; 4. To this end, we can count the number of integers in the range (9, 16) directly.result = 0 for x in range(10, 16): if 9 &lt; x &lt; 16: result += 1 print(result) print(digit_100th)F.5 Evaluation without Toolsets----------------------------------------------------Question:The square root of $x$ is greater than 3 and less than 4. How many integer values of $x$ satisfy this condition?Solution: # We need to find the integer values of x for which sqrt(x) &gt; 3 and sqrt(x) &lt; 4. To this end, we can count the number of integers in the range (9, 16) directly.result = 0 for x in range(10, 16): if 9 &lt; x &lt; 16: result += 1 print(result)
Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Mcaleer, Albert Q Jiang, Jia Deng, Stella Biderman, Sean Welleck, Llemma: An open language model for mathematics. 2023</p>
<p>Sam Andres M Bran, Oliver Cox, Carlo Schilter, Andrew D Baldassari, Philippe White, Schwaller, Chemcrow: Augmenting large-language models with chemistry tools. 2023</p>
<p>Large language models as tool makers. Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, Denny Zhou, 2023</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, Transactions on Machine Learning Research. 2023a</p>
<p>TheoremQA: A theorem-driven question answering dataset. Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, Tony Xia, 10.18653/v1/2023.emnlp-main.489Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023b</p>
<p>ChatCoT: Tool-augmented chain-of-thought reasoning on chatbased large language models. Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Xin Zhao, Ji-Rong Wen, 10.18653/v1/2023.findings-emnlp.985Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023c</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, Advances in Neural Information Processing Systems. 2023</p>
<p>Yifan Song, Weimin Xiong, Dawei Zhu, Wenhao Wu, Han Qian, Mingbo Song, Hailiang Huang, Cheng Li, Ke Wang, Rong Yao, Ye Tian, Sujian Li, Restgpt: Connecting large language models with real-world restful apis. 2023</p>
<p>Scieval: A multi-level large language model evaluation benchmark for scientific research. Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, Kai Yu, 2023</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, ; Jian, Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Pushkar Mishra, Igor Molybog. Andrew Nie, Jeremy Poulton, Rashi Reizenstein, Kalyan Rungta, Alan Saladi, Ruan Schelten, Eric Michael Silva, Ranjan Smith, Xiaoqing Subramanian, Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,; Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurelien Rodriguezand Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models</p>
<p>Representation learning with contrastive predictive coding. Aaron Van Den Oord, Yazhe Li, Oriol Vinyals, 2019</p>
<p>Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning. Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, Hongsheng Li, 2023a</p>
<p>Scibench: Evaluating college-level scientific problem-solving abilities of large language models. Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R Loomba, Shichang Zhang, Yizhou Sun, Wei Wang, 2023b</p>
<p>Mint: Evaluating llms in multi-turn interaction with tools and language feedback. Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, Heng Ji, 2023c</p>
<p>Visual chatgpt: Talking, drawing and editing with visual foundation models. Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan, 2023</p>
<p>On the tool manipulation capability of open-source large language models. Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, Jian Zhang, 2023a</p>
<p>Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, Zhoujun Cheng, Siheng Zhao, Lingpeng Kong, Bailin Wang, Caiming Xiong, Tao Yu, Lemur: Harmonizing natural language and code for language agents. 2023b</p>
<p>Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, Ying Shan, Gpt4tools: Teaching large language model to use tools via self-instruction. 2023</p>
<p>Lumos: Learning agents with unified data, modular design, and open-source llms. Faeze Da Yin, Abhilasha Brahman, Khyathi Ravichander, Kai-Wei Chandu, Yejin Chang, Bill Choi, Lin Yuchen, 2023</p>
<p>Metamath: Bootstrap your own mathematical questions for large language models. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, 2023ArXiv preprint, abs/2309.12284</p>
<p>Craft: Customizing llms by creating and retrieving from specialized toolsets. Lifan Yuan, Yangyi Chen, Xingyao Wang, Yi R Fung, Hao Peng, Heng Ji, 2023a</p>
<p>Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, Jingren Zhou, Scaling relationship on learning mathematical reasoning with large language models. 2023b</p>
<p>Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, 2023a</p>
<p>Mammoth: Building math generalist models through hybrid instruction tuning. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, 2023b</p>
<p>Sciglm: Training scientific language models with self-reflective instruction annotation and tuning. Dan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao Du, Kaiyu Yang, Zihan Wang, Yisong Yue, Yuxiao Dong, Jie Tang, 2024</p>
<p>M3exam: A multilingual, multimodal, multilevel benchmark for examining large language models. Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Ken Yew, Lidong Chia, Bing, 2023a</p>
<p>Cumulative reasoning with large language models. Yifan Zhang, Jingqin Yang, Yang Yuan, Andrew Chi-Chih Yao, 2023b</p>
<p>Knowledgemath: Knowledge-intensive math word problem solving in finance domains. Yilun Zhao, Hongjun Liu, Yitao Long, Rui Zhang, Chen Zhao, Arman Cohan, 2023</p>
<p>Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, Hongsheng Li, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
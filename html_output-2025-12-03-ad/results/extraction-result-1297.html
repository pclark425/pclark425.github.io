<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1297 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1297</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1297</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-52f3960fa991a3b9a0a750bd2740f3dade369011</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/52f3960fa991a3b9a0a750bd2740f3dade369011" target="_blank">Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization</a></p>
                <p><strong>Paper Venue:</strong> Journal of Artificial Intelligence Research</p>
                <p><strong>Paper TL;DR:</strong> It is proved that if a problem satisfies adaptive submodularity, a simple adaptive greedy algorithm is guaranteed to be competitive with the optimal policy, providing performance guarantees for both stochastic maximization and coverage.</p>
                <p><strong>Paper Abstract:</strong> Solving stochastic optimization problems under partial observability, where one needs to adaptively make decisions with uncertain outcomes, is a fundamental but notoriously difficult challenge. In this paper, we introduce the concept of adaptive submodularity, generalizing submodular set functions to adaptive policies. We prove that if a problem satisfies this property, a simple adaptive greedy algorithm is guaranteed to be competitive with the optimal policy. In addition to providing performance guarantees for both stochastic maximization and coverage, adaptive submodularity can be exploited to drastically speed up the greedy algorithm by using lazy evaluations. We illustrate the usefulness of the concept by giving several examples of adaptive submodular objectives arising in diverse applications including sensor placement, viral marketing and active learning. Proving adaptive submodularity for these problems allows us to recover existing results in these applications as special cases, improve approximation guarantees and handle natural generalizations.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1297.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1297.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AdaptiveGreedy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive Greedy Policy / Adaptive Greedy Algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential decision policy that at each step selects the item maximizing the conditional expected marginal benefit Δ(e | ψ) given observations so far; provably near-optimal for problems satisfying adaptive submodularity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Adaptive Greedy Policy</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A policy (decision-tree style) that iteratively: (1) conditions on the current partial realization ψ to form posterior p(φ | ψ); (2) computes or approximates Δ(e | ψ) = E[f(dom(ψ) ∪ {e}, Φ) - f(dom(ψ), Φ) | Φ ~ ψ] for all candidate items e (or their benefit/cost ratio); (3) selects arg max_e Δ(e | ψ) (or arg max Δ(e|ψ)/c(e)); (4) observes the outcome Φ(e) and updates ψ. Supports randomized/approximate selection and cost-weighted variants.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>information gain / greedy expected marginal benefit maximization (active adaptive experimental design)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts by conditioning on observed outcomes (partial realization ψ) and recomputing expected marginal benefits Δ(e | ψ) using the posterior p(φ | ψ); chooses next experiment/item that maximizes expected immediate gain (myopic greedy rule). Can be implemented approximately (α-approximate greedy) or with lazy upper bounds to avoid recomputing all Δ values.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>General adaptive stochastic optimization environments (e.g., Stochastic Submodular Maximization / Coverage / Active Learning / Viral Marketing)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable (only selected items' states observed), stochastic realizations Φ drawn from known prior p(φ) (often assumed factorized/independent per item in examples), discrete finite item state space O per item, large combinatorial realization space (|O|^{|E|}), budgeted sequential decision process (budget k or coverage quota Q).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Action space size = |E| (choose one of E items each step); episode length ≤ budget k or until coverage Q achieved; realization space size exponential in |E| (|O|^{|E|}); computational cost dominated by computing/estimating Δ(e|ψ) for up to |E| items per step (accelerations reduce this).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Theoretical guarantees: adaptive monotone submodular objectives -> greedy achieves (1 - 1/e) approximation for maximizing expected value (Theorem 5); for min-sum objective, greedy is a 4·α approximation (Theorem 15); for adaptive min-cost cover: squared-logarithmic approximation bounds (Theorem 13) and worst-case ln-type bound (Theorem 14). Empirical: accelerated (lazy) implementation produced speedups of ~4x to ~40x on two sensor selection problems (Section 10).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Non-adaptive baselines (commit-to-set) produce submodular guarantees in expectation but can be strictly worse than adaptive policies; adaptivity gaps cited (e.g., Θ(log n) and larger in related works) but no single unified numerical baseline reported in-paper. Specific non-adaptive greedy achieves (1 - 1/e) for the nonadaptive expected objective when f is pointwise submodular.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>No single empirical sample-efficiency curve provided; theoretical sample-use: Δ(e|ψ) can be estimated via Monte Carlo sampling from p(φ | ψ) with N samples giving O(1/√N) error bounds; small absolute-error estimation of Δ yields additive guarantees (see discussion in §4.2 and Theorem 5).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Purely myopic greedy — trades off exploration vs exploitation implicitly by maximizing expected immediate information/reward (Δ). Exploration arises when Δ is larger for uncertain items; exploitation when Δ favors known high-payoff items. Approximate greedy or cost-weighted selection incorporates exploitation via benefit/cost ratio.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared analytically to optimal adaptive policy (approximation bounds); compared conceptually to non-adaptive greedy/optimal sets; accelerated variant compared empirically to naive greedy (full recomputation) showing runtime speedups of 4–40×; comparisons to other algorithms appear via references (Kempe et al. 2003, Goemans & Vondrák 2006, Asadpour et al. 2008, Liu et al. 2008).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>If objective is adaptive monotone and adaptive submodular relative to the prior p(φ): (a) adaptive greedy matches classical greedy approximation guarantees (Theorem 5: (1-1/e) factor for k-budget max); (b) data-dependent upper bounds on optimum are computable from Δ values (Lemma 6); (c) accelerated lazy-eval implementation exploits monotone decreasing Δ(e|ψ) to drastically reduce computations; (d) approximate greedy (α-approx) preserves multiplicative approximation factors; (e) for coverage/min-cost objectives, greedy achieves squared-logarithmic (average-case) or ln-type (worst-case) approximations under stronger conditions (Theorems 13,14).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires adaptive submodularity w.r.t. the prior — many problems violate this (synergistic interactions, actions that alter underlying realization ⇒ general POMDPs). If f is not adaptive submodular, problems can be extremely hard to approximate (Section 12). Strong adaptive submodularity is sometimes needed for tighter coverage bounds; some earlier claimed bounds were corrected (squared-log vs logarithmic) and improvement remains open. Empirical performance beyond reported sensor examples not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization', 'publication_date_yy_mm': '2010-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1297.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1297.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LazyAdaptiveGreedy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Accelerated Adaptive Greedy with Lazy Evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An accelerated implementation of the adaptive greedy policy that maintains upper bounds on item marginal benefits and only recomputes Δ(e|ψ) as needed, exploiting monotone nonincreasing Δ under adaptive submodularity to reduce computations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Accelerated Adaptive Greedy (lazy evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same decision rule as Adaptive Greedy, but uses a priority queue of items keyed by cached upper bounds on Δ(e|ψ). On each selection, it pops the current top, recomputes Δ for that item under current ψ, re-inserts if still not provably maximum, and repeats until an item is confirmed maximal. This leverages adaptive submodularity (Δ nonincreasing with more observations) to avoid recomputing many Δ values each round.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>greedy expected marginal benefit maximization with lazy evaluation (computational adaptive design acceleration)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts both in experiment choice (standard greedy using Δ) and in computational effort: reuses previous Δ upper bounds across iterations because Δ(e|ψ) forms a nonincreasing sequence as ψ grows, thereby prioritizing recomputation for promising items only.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same class as AdaptiveGreedy (sensor selection experiments reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable stochastic environments with discrete item states and independent priors in examples; computationally expensive Δ evaluations (e.g., requiring Monte Carlo or complex model inference).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same decision complexity as AdaptiveGreedy but aims to reduce per-step computation from O(|E|) Δ evaluations to much fewer on average; empirical speedups observed depend on problem structure.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Empirical runtime speedups reported: roughly 4× to 40× reduction in computational cost vs naive adaptive greedy on two sensor selection problems (Section 4.4 / §10). Theoretical solution quality is identical to standard adaptive greedy (no loss in approximation guarantees) because lazy evaluations only avoid unnecessary recomputation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baseline naive adaptive greedy computes Δ(e|ψ) for all e each round; identical solution quality but higher computation. No numerical reward/cost tradeoff beyond runtime speedups reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>By reducing number of Δ evaluations, it reduces required model samples/evaluations in practice by the observed empirical factor (4–40×) on tested sensor tasks; no general sample-complexity bounds beyond savings due to fewer Δ computations.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Retains greedy myopic selection; lazy mechanism only affects computation, not exploration policy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared empirically to naive adaptive greedy (full recomputation) on two sensor selection problems; theoretical comparisons are to standard greedy guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Adaptive submodularity implies Δ(e|ψ) nonincreasing in ψ, enabling safe lazy evaluation: the accelerated algorithm returns the same selection sequence as adaptive greedy but with far fewer Δ computations in practice; observed speedups 4–40× on sensor selection instances.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Savings depend on problem structure and how rapidly Δ upper bounds become tight; in worst case (no decrease structure exploited) lazy evaluation may still require many recomputations. No effect on approximation guarantees but requires adaptive submodularity to justify correctness of pruning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization', 'publication_date_yy_mm': '2010-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1297.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1297.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StochasticSubmodMax</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stochastic Submodular Maximization (Adaptive Sensor Placement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptive experimental design instance where sensors with stochastic states are placed sequentially to maximize expected submodular utility; treated by modeling f(A, φ) = ĥf({(e, φ(e)): e∈A}) and applying adaptive greedy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Adaptive Greedy applied to Stochastic Submodular Maximization</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Adaptive Greedy policy (as above) specialized to sensor-placement setting: computes Δ(e|ψ) by marginal expected increase in ĥf when adding sensor e given posterior over sensor states; often assumes independent per-item priors so posterior factorizes for unobserved items.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>adaptive active selection maximizing expected marginal utility (active sensing / information-directed sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Selects next sensor location by maximizing expected improvement in the submodular utility given current observations about which previously placed sensors are functioning; uses independence assumptions to compute coupled expectations and update posterior.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Stochastic Sensor Placement / Stochastic Submodular Maximization</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable (sensor health unknown until deployed), stochastic independent sensor states (common example), discrete states per sensor (e.g., working/failed or multi-state), objective derived from ĥf over (location,state) pairs, budget k placements.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Ground set size |E| (possible locations); episode length = k placements; realization space size = ∏_e |O|_e (exponential); Δ computations may require Monte Carlo sampling over φ|ψ.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Theoretical: when ĥf is monotone submodular and item states independent, f is adaptive monotone and adaptive submodular, so adaptive greedy achieves (1 - 1/e) approximation to optimal adaptive policy for k-budget (Theorem 16).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Non-adaptive solutions (choose k locations upfront) can be suboptimal relative to adaptive policies; Asadpour et al. (2008) studied special binary-failure case and showed a (1 - 1/e) guarantee for adaptive greedy in that setting as well. No single empirical baseline numeric provided in-paper beyond theoretical comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Δ(e|ψ) estimation requires sampling from posterior; authors note small absolute-error Monte Carlo estimates yield additive guarantees, enabling practical approximation with polynomial sampling per decision (discussion in §5.1).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Greedy rule trades exploration (select uncertain sensors that might inform future choices) and exploitation (select sensors expected to yield highest immediate utility) via Δ(e|ψ).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Analytical comparison to optimal adaptive policy (approximation ratio); related prior work: Asadpour et al. (2008) for binary failures; non-adaptive greedy and offline optimal set selection as baselines in theory.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Modeling sensor deployment as f(A, φ) = ĥf({(e, φ(e))}) with independent per-item priors yields adaptive monotonicity and adaptive submodularity; thus adaptive greedy inherits (1 - 1/e) approximation guarantee for expected utility under budget constraints (Theorem 16).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires independence or factorization assumptions in proof; if sensor states exhibit complex dependencies or the objective is not adaptive submodular then guarantees do not hold. Empirical evaluation limited to runtime of lazy eval; no wide empirical reward comparisons presented.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization', 'publication_date_yy_mm': '2010-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1297.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1297.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StochasticSetCover</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stochastic Submodular Coverage / Stochastic Set Coverage</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adaptive coverage problems where items yield random subsets or contributions on selection; goal is to adaptively select items to reach quota Q with minimum expected cost—an adaptive experimental design for covering unknown elements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Adaptive Greedy applied to Stochastic Submodular Coverage / Stochastic Set Coverage</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Adaptive Greedy policy that at each step selects the item maximizing conditional expected marginal coverage Δ(e|ψ) (or Δ(e|ψ)/c(e) for costs), continuing until quota Q is provably achieved for all realizations consistent with observations (coverage definition).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>adaptive coverage-driven active selection (greedy expected marginal coverage maximization)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Uses observed sampled sets (or (item,state) pairs) to update ψ and posterior; selects next item maximizing expected additional coverage conditioned on ψ; termination requires 'proof' that all consistent realizations meet coverage Q (self-certifying condition often used).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Stochastic Submodular Coverage; special case: Stochastic Set Coverage (items sample random subsets of U)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable; items sample subsets (φ(e) ⊆ U) drawn independently in common examples; objective f(A,φ) = min(Q, ĥf({(e, φ(e))})); assumed that Q achievable for all φ (f(E, φ) = Q).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Ground set of elements U size n; item set E size m; realization space exponential; goal is expected cost c_avg(π) minimal where cost counts selected items; Δ computations potentially expensive (Monte Carlo).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Theoretical: for independent item-state priors and ĥf monotone submodular, adaptive greedy yields (ln(Q/η)+1)^2 · α factor bound on expected cost in general and (ln(Q/η)+1)^2 for self-certifying instances (Theorem 17 & 13). For stochastic set coverage (cover n elements) yields (ln(n)+1)^2 approximation (Corollary 18).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Prior work shows adaptivity gaps for set coverage variants (e.g., Goemans & Vondrák show adaptivity gaps Θ(log n) in some variants); non-adaptive algorithms may achieve worse approximation factors. Paper does not provide explicit empirical baseline reward numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Depends on sampling cost to estimate Δ; no empirical sample counts reported; theoretical dependence via logarithmic factors in Q, δ, η (minimum prior probability and discretization parameter) appears in approximation bound.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Greedy focuses on coverage per-step (exploitation) but will pick items that are expected to reduce remaining uncovered mass (exploration) when uncertainty about elements remains; termination requires certifying coverage across all consistent realizations resulting in potential extra exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Theoretical comparisons to optimal adaptive policy (approximation bounds); references to Goemans & Vondrák (2006), Liu et al. (2008) which study related stochastic set coverage variants and adaptivity gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Adaptive submodularity and self-certifying structure imply provable approximation guarantees for adaptive greedy in stochastic coverage problems: squared-logarithmic expected-cost bounds in general and improved bounds for self-certifying instances; special-case stochastic set coverage admits (ln(n)+1)^2 approximation (Corollary 18).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Proofs require strong conditions (adaptive submodularity, sometimes strong adaptive monotonicity / pointwise submodularity); constants include δ (min prior probability) and η (discretization) causing possibly loose bounds; historical correction: earlier claimed purely logarithmic bounds were corrected to squared-log, leaving open whether log bounds hold; instances that are not self-certifying may have worse guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization', 'publication_date_yy_mm': '2010-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1297.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1297.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AdaptiveViral</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive Viral Marketing (Independent Cascade, Full-Adoption Feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adaptive analog of influence maximization where seeds are selected sequentially, observations reveal which edges are live (full-adoption feedback), and the greedy adaptive policy maximizes expected spread under submodular reward functions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Adaptive Greedy for Adaptive Viral Marketing</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Adaptive Greedy policy applied to independent cascade diffusion: items = nodes (people); selecting a node reveals statuses of edges reachable from that node (Full-Adoption Feedback); Δ(e|ψ) equals expected marginal increase in chosen (possibly submodular) reward ĥf of influenced nodes given current observations.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>adaptive influence maximization via expected marginal benefit (active seeding based on posterior over live/dead edges)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>After selecting a seed and observing the propagation and edge statuses reachable from it, the policy updates posterior beliefs about which edges are live and selects next seed maximizing expected marginal reward under updated beliefs; uses independence of edge random variables in proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Adaptive Viral Marketing under Independent Cascade model with Full-Adoption Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable network diffusion: edge variables X_uv ∈ {0,1} (live/dead) drawn independently with known means p_uv; observations upon activation reveal statuses for edges reachable from the activated node (Full-Adoption Feedback); stochastic, discrete, combinatorial diffusion dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Graph with |V| nodes and |A| edges; action space = selecting any unselected node; diffusion process can reach many nodes per activation (cascades); realization space size 2^{|A|}; episode length = budget k or until coverage objective met.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Theoretical: with ĥf monotone submodular over set of influenced nodes and independent edge priors, the adaptive greedy policy achieves (1 - 1/e) approximation to optimal adaptive policy for budgeted maximization (extends Kempe et al. 2003 to adaptive setting). For min-cost cover variant, squared-log type approximation results also apply (paper claims adaptive analogs and provides Theorems in §8).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Non-adaptive seeding (choose all seeds upfront) has classical (1 - 1/e) guarantee for deterministic fixed diffusion model; adaptive selection can improve performance in some instances (adaptivity gap analysis in Section 11); exact empirical baselines not provided in-paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Computing Δ(e|ψ) may require sampling over edge realizations consistent with ψ; no empirical sample counts are provided; theoretical results assume ability to compute or approximate expectations under posterior.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Greedy myopic selection balances immediate expected spread (exploitation) and information that could improve future seed choices (exploration) since selecting a node reveals many edge statuses (informative), which the greedy Δ naturally accounts for.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Theoretical comparison to optimal adaptive policy and to non-adaptive influence maximization results of Kempe et al. (2003); paper extends those non-adaptive results to adaptive setting. No empirical comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Adaptive submodularity holds for independent-cascade with full-adoption feedback and monotone submodular ĥf, enabling the greedy policy to attain (1 - 1/e) approximation for expected spread under budget (adaptive analog of Kempe et al.). Also extends to more general monotone submodular reward functions and to coverage/min-cost variants with provable approximation factors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Proofs rely on independence of edge variables and the chosen feedback model (full-adoption); if feedback is weaker or edges correlate, adaptive submodularity may fail and guarantees do not apply. No empirical validation presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization', 'publication_date_yy_mm': '2010-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1297.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1297.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AdaptiveActiveLearn</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive Active Learning / Generalized Binary Search (Adaptive Querying)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adaptive querying strategies (including generalized binary search variants) that select examples/tests to maximally shrink version space probability mass, shown to satisfy adaptive submodularity and therefore near-optimality guarantees for query-efficient diagnosis/active learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Adaptive Greedy for Active Learning / Generalized Binary Search</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Adaptive Greedy policy applied to active learning and diagnostic testing: items = examples/tests; each item reveals label/state; objective f is reduction in version space probability mass (or related utility), Δ(e|ψ) quantifies expected shrinkage, policy picks item maximizing Δ(e|ψ) until version space sufficiently small or hypothesis identified.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>active learning via expected version-space mass reduction (information gain-like greedy selection)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Selects the next example/test whose expected reduction in posterior version-space mass (or posterior entropy proxy) is maximal given current observed labels; updates posterior over hypotheses after each label and repeats.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Pool-based Active Learning / Diagnostic Testing under stochastic/noisy labels</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable unknown hypothesis drawn from prior p(φ); labels revealed only for queried examples/tests; version space evolves with each observation; possibly persistent noise handled by using surrogate objectives that remain adaptive submodular.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Hypothesis space size can be large; action space is the pool of unlabeled examples (|E|); episode length until identification or label budget exhausted; computing exact Δ may be expensive (requires posterior over hypotheses).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Theoretical: reduction-in-version-space-mass objective is adaptive submodular, hence adaptive greedy (generalized binary search and approximate variants) is near-optimal with formal bounds (new analysis provided in §9). Specific numeric empirical metrics not reported in this paper excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Non-adaptive querying (random or preselected sets) is typically less sample-efficient; prior results (Kosaraju et al., Dasgupta) show gaps where adaptive querying can provide exponential advantages in some settings; exact baselines not enumerated here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Adaptive greedy minimizes expected number of queries to identify hypothesis under adaptive submodularity; theoretical bounds on expected query count derived in §9 (not numerically instantiated in this excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Greedy selection targets maximal expected information (exploration) per query; exploitation is implicit as queries that reduce version space quickly also lead to quicker confident labeling/decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Relates to and extends generalized binary search, Kosaraju et al. (1999), Dasgupta (2004); compared theoretically against optimal policies (approximation guarantees) and to worst-case interactive submodular set cover (Guillory & Bilmes).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Reduction-in-version-space objectives are adaptive submodular; therefore adaptive greedy querying policies (and approximate variants) have provable near-optimal guarantees for active learning and diagnostic testing tasks (Section 9). The framework also handles natural generalizations including noisy/persistent label noise via suitable surrogate objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Adaptive submodularity may not hold for some active learning objectives or in presence of complex noise models without careful surrogate choices; computational cost of exact Δ may be high requiring sampling/approximations; no broad empirical validation provided in this paper excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization', 'publication_date_yy_mm': '2010-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Asadpour et al. (2008) <em>(Rating: 2)</em></li>
                <li>Kempe, Kleinberg, and Tardos (2003) <em>(Rating: 2)</em></li>
                <li>Goemans and Vondrák (2006) <em>(Rating: 2)</em></li>
                <li>Liu et al. (2008) <em>(Rating: 2)</em></li>
                <li>Streeter and Golovin (2008) <em>(Rating: 2)</em></li>
                <li>Leskovec et al. (2007) <em>(Rating: 2)</em></li>
                <li>Guillory and Bilmes (2010) <em>(Rating: 1)</em></li>
                <li>Guillory and Bilmes (2011) <em>(Rating: 1)</em></li>
                <li>Kosaraju et al. (1999) <em>(Rating: 1)</em></li>
                <li>Dasgupta (2004) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1297",
    "paper_id": "paper-52f3960fa991a3b9a0a750bd2740f3dade369011",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "AdaptiveGreedy",
            "name_full": "Adaptive Greedy Policy / Adaptive Greedy Algorithm",
            "brief_description": "A sequential decision policy that at each step selects the item maximizing the conditional expected marginal benefit Δ(e | ψ) given observations so far; provably near-optimal for problems satisfying adaptive submodularity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Adaptive Greedy Policy",
            "agent_description": "A policy (decision-tree style) that iteratively: (1) conditions on the current partial realization ψ to form posterior p(φ | ψ); (2) computes or approximates Δ(e | ψ) = E[f(dom(ψ) ∪ {e}, Φ) - f(dom(ψ), Φ) | Φ ~ ψ] for all candidate items e (or their benefit/cost ratio); (3) selects arg max_e Δ(e | ψ) (or arg max Δ(e|ψ)/c(e)); (4) observes the outcome Φ(e) and updates ψ. Supports randomized/approximate selection and cost-weighted variants.",
            "adaptive_design_method": "information gain / greedy expected marginal benefit maximization (active adaptive experimental design)",
            "adaptation_strategy_description": "Adapts by conditioning on observed outcomes (partial realization ψ) and recomputing expected marginal benefits Δ(e | ψ) using the posterior p(φ | ψ); chooses next experiment/item that maximizes expected immediate gain (myopic greedy rule). Can be implemented approximately (α-approximate greedy) or with lazy upper bounds to avoid recomputing all Δ values.",
            "environment_name": "General adaptive stochastic optimization environments (e.g., Stochastic Submodular Maximization / Coverage / Active Learning / Viral Marketing)",
            "environment_characteristics": "Partially observable (only selected items' states observed), stochastic realizations Φ drawn from known prior p(φ) (often assumed factorized/independent per item in examples), discrete finite item state space O per item, large combinatorial realization space (|O|^{|E|}), budgeted sequential decision process (budget k or coverage quota Q).",
            "environment_complexity": "Action space size = |E| (choose one of E items each step); episode length ≤ budget k or until coverage Q achieved; realization space size exponential in |E| (|O|^{|E|}); computational cost dominated by computing/estimating Δ(e|ψ) for up to |E| items per step (accelerations reduce this).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Theoretical guarantees: adaptive monotone submodular objectives -&gt; greedy achieves (1 - 1/e) approximation for maximizing expected value (Theorem 5); for min-sum objective, greedy is a 4·α approximation (Theorem 15); for adaptive min-cost cover: squared-logarithmic approximation bounds (Theorem 13) and worst-case ln-type bound (Theorem 14). Empirical: accelerated (lazy) implementation produced speedups of ~4x to ~40x on two sensor selection problems (Section 10).",
            "performance_without_adaptation": "Non-adaptive baselines (commit-to-set) produce submodular guarantees in expectation but can be strictly worse than adaptive policies; adaptivity gaps cited (e.g., Θ(log n) and larger in related works) but no single unified numerical baseline reported in-paper. Specific non-adaptive greedy achieves (1 - 1/e) for the nonadaptive expected objective when f is pointwise submodular.",
            "sample_efficiency": "No single empirical sample-efficiency curve provided; theoretical sample-use: Δ(e|ψ) can be estimated via Monte Carlo sampling from p(φ | ψ) with N samples giving O(1/√N) error bounds; small absolute-error estimation of Δ yields additive guarantees (see discussion in §4.2 and Theorem 5).",
            "exploration_exploitation_tradeoff": "Purely myopic greedy — trades off exploration vs exploitation implicitly by maximizing expected immediate information/reward (Δ). Exploration arises when Δ is larger for uncertain items; exploitation when Δ favors known high-payoff items. Approximate greedy or cost-weighted selection incorporates exploitation via benefit/cost ratio.",
            "comparison_methods": "Compared analytically to optimal adaptive policy (approximation bounds); compared conceptually to non-adaptive greedy/optimal sets; accelerated variant compared empirically to naive greedy (full recomputation) showing runtime speedups of 4–40×; comparisons to other algorithms appear via references (Kempe et al. 2003, Goemans & Vondrák 2006, Asadpour et al. 2008, Liu et al. 2008).",
            "key_results": "If objective is adaptive monotone and adaptive submodular relative to the prior p(φ): (a) adaptive greedy matches classical greedy approximation guarantees (Theorem 5: (1-1/e) factor for k-budget max); (b) data-dependent upper bounds on optimum are computable from Δ values (Lemma 6); (c) accelerated lazy-eval implementation exploits monotone decreasing Δ(e|ψ) to drastically reduce computations; (d) approximate greedy (α-approx) preserves multiplicative approximation factors; (e) for coverage/min-cost objectives, greedy achieves squared-logarithmic (average-case) or ln-type (worst-case) approximations under stronger conditions (Theorems 13,14).",
            "limitations_or_failures": "Requires adaptive submodularity w.r.t. the prior — many problems violate this (synergistic interactions, actions that alter underlying realization ⇒ general POMDPs). If f is not adaptive submodular, problems can be extremely hard to approximate (Section 12). Strong adaptive submodularity is sometimes needed for tighter coverage bounds; some earlier claimed bounds were corrected (squared-log vs logarithmic) and improvement remains open. Empirical performance beyond reported sensor examples not provided.",
            "uuid": "e1297.0",
            "source_info": {
                "paper_title": "Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization",
                "publication_date_yy_mm": "2010-03"
            }
        },
        {
            "name_short": "LazyAdaptiveGreedy",
            "name_full": "Accelerated Adaptive Greedy with Lazy Evaluations",
            "brief_description": "An accelerated implementation of the adaptive greedy policy that maintains upper bounds on item marginal benefits and only recomputes Δ(e|ψ) as needed, exploiting monotone nonincreasing Δ under adaptive submodularity to reduce computations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Accelerated Adaptive Greedy (lazy evaluations)",
            "agent_description": "Same decision rule as Adaptive Greedy, but uses a priority queue of items keyed by cached upper bounds on Δ(e|ψ). On each selection, it pops the current top, recomputes Δ for that item under current ψ, re-inserts if still not provably maximum, and repeats until an item is confirmed maximal. This leverages adaptive submodularity (Δ nonincreasing with more observations) to avoid recomputing many Δ values each round.",
            "adaptive_design_method": "greedy expected marginal benefit maximization with lazy evaluation (computational adaptive design acceleration)",
            "adaptation_strategy_description": "Adapts both in experiment choice (standard greedy using Δ) and in computational effort: reuses previous Δ upper bounds across iterations because Δ(e|ψ) forms a nonincreasing sequence as ψ grows, thereby prioritizing recomputation for promising items only.",
            "environment_name": "Same class as AdaptiveGreedy (sensor selection experiments reported in paper)",
            "environment_characteristics": "Partially observable stochastic environments with discrete item states and independent priors in examples; computationally expensive Δ evaluations (e.g., requiring Monte Carlo or complex model inference).",
            "environment_complexity": "Same decision complexity as AdaptiveGreedy but aims to reduce per-step computation from O(|E|) Δ evaluations to much fewer on average; empirical speedups observed depend on problem structure.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Empirical runtime speedups reported: roughly 4× to 40× reduction in computational cost vs naive adaptive greedy on two sensor selection problems (Section 4.4 / §10). Theoretical solution quality is identical to standard adaptive greedy (no loss in approximation guarantees) because lazy evaluations only avoid unnecessary recomputation.",
            "performance_without_adaptation": "Baseline naive adaptive greedy computes Δ(e|ψ) for all e each round; identical solution quality but higher computation. No numerical reward/cost tradeoff beyond runtime speedups reported.",
            "sample_efficiency": "By reducing number of Δ evaluations, it reduces required model samples/evaluations in practice by the observed empirical factor (4–40×) on tested sensor tasks; no general sample-complexity bounds beyond savings due to fewer Δ computations.",
            "exploration_exploitation_tradeoff": "Retains greedy myopic selection; lazy mechanism only affects computation, not exploration policy.",
            "comparison_methods": "Compared empirically to naive adaptive greedy (full recomputation) on two sensor selection problems; theoretical comparisons are to standard greedy guarantees.",
            "key_results": "Adaptive submodularity implies Δ(e|ψ) nonincreasing in ψ, enabling safe lazy evaluation: the accelerated algorithm returns the same selection sequence as adaptive greedy but with far fewer Δ computations in practice; observed speedups 4–40× on sensor selection instances.",
            "limitations_or_failures": "Savings depend on problem structure and how rapidly Δ upper bounds become tight; in worst case (no decrease structure exploited) lazy evaluation may still require many recomputations. No effect on approximation guarantees but requires adaptive submodularity to justify correctness of pruning.",
            "uuid": "e1297.1",
            "source_info": {
                "paper_title": "Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization",
                "publication_date_yy_mm": "2010-03"
            }
        },
        {
            "name_short": "StochasticSubmodMax",
            "name_full": "Stochastic Submodular Maximization (Adaptive Sensor Placement)",
            "brief_description": "An adaptive experimental design instance where sensors with stochastic states are placed sequentially to maximize expected submodular utility; treated by modeling f(A, φ) = ĥf({(e, φ(e)): e∈A}) and applying adaptive greedy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Adaptive Greedy applied to Stochastic Submodular Maximization",
            "agent_description": "Adaptive Greedy policy (as above) specialized to sensor-placement setting: computes Δ(e|ψ) by marginal expected increase in ĥf when adding sensor e given posterior over sensor states; often assumes independent per-item priors so posterior factorizes for unobserved items.",
            "adaptive_design_method": "adaptive active selection maximizing expected marginal utility (active sensing / information-directed sampling)",
            "adaptation_strategy_description": "Selects next sensor location by maximizing expected improvement in the submodular utility given current observations about which previously placed sensors are functioning; uses independence assumptions to compute coupled expectations and update posterior.",
            "environment_name": "Stochastic Sensor Placement / Stochastic Submodular Maximization",
            "environment_characteristics": "Partially observable (sensor health unknown until deployed), stochastic independent sensor states (common example), discrete states per sensor (e.g., working/failed or multi-state), objective derived from ĥf over (location,state) pairs, budget k placements.",
            "environment_complexity": "Ground set size |E| (possible locations); episode length = k placements; realization space size = ∏_e |O|_e (exponential); Δ computations may require Monte Carlo sampling over φ|ψ.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Theoretical: when ĥf is monotone submodular and item states independent, f is adaptive monotone and adaptive submodular, so adaptive greedy achieves (1 - 1/e) approximation to optimal adaptive policy for k-budget (Theorem 16).",
            "performance_without_adaptation": "Non-adaptive solutions (choose k locations upfront) can be suboptimal relative to adaptive policies; Asadpour et al. (2008) studied special binary-failure case and showed a (1 - 1/e) guarantee for adaptive greedy in that setting as well. No single empirical baseline numeric provided in-paper beyond theoretical comparisons.",
            "sample_efficiency": "Δ(e|ψ) estimation requires sampling from posterior; authors note small absolute-error Monte Carlo estimates yield additive guarantees, enabling practical approximation with polynomial sampling per decision (discussion in §5.1).",
            "exploration_exploitation_tradeoff": "Greedy rule trades exploration (select uncertain sensors that might inform future choices) and exploitation (select sensors expected to yield highest immediate utility) via Δ(e|ψ).",
            "comparison_methods": "Analytical comparison to optimal adaptive policy (approximation ratio); related prior work: Asadpour et al. (2008) for binary failures; non-adaptive greedy and offline optimal set selection as baselines in theory.",
            "key_results": "Modeling sensor deployment as f(A, φ) = ĥf({(e, φ(e))}) with independent per-item priors yields adaptive monotonicity and adaptive submodularity; thus adaptive greedy inherits (1 - 1/e) approximation guarantee for expected utility under budget constraints (Theorem 16).",
            "limitations_or_failures": "Requires independence or factorization assumptions in proof; if sensor states exhibit complex dependencies or the objective is not adaptive submodular then guarantees do not hold. Empirical evaluation limited to runtime of lazy eval; no wide empirical reward comparisons presented.",
            "uuid": "e1297.2",
            "source_info": {
                "paper_title": "Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization",
                "publication_date_yy_mm": "2010-03"
            }
        },
        {
            "name_short": "StochasticSetCover",
            "name_full": "Stochastic Submodular Coverage / Stochastic Set Coverage",
            "brief_description": "Adaptive coverage problems where items yield random subsets or contributions on selection; goal is to adaptively select items to reach quota Q with minimum expected cost—an adaptive experimental design for covering unknown elements.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Adaptive Greedy applied to Stochastic Submodular Coverage / Stochastic Set Coverage",
            "agent_description": "Adaptive Greedy policy that at each step selects the item maximizing conditional expected marginal coverage Δ(e|ψ) (or Δ(e|ψ)/c(e) for costs), continuing until quota Q is provably achieved for all realizations consistent with observations (coverage definition).",
            "adaptive_design_method": "adaptive coverage-driven active selection (greedy expected marginal coverage maximization)",
            "adaptation_strategy_description": "Uses observed sampled sets (or (item,state) pairs) to update ψ and posterior; selects next item maximizing expected additional coverage conditioned on ψ; termination requires 'proof' that all consistent realizations meet coverage Q (self-certifying condition often used).",
            "environment_name": "Stochastic Submodular Coverage; special case: Stochastic Set Coverage (items sample random subsets of U)",
            "environment_characteristics": "Partially observable; items sample subsets (φ(e) ⊆ U) drawn independently in common examples; objective f(A,φ) = min(Q, ĥf({(e, φ(e))})); assumed that Q achievable for all φ (f(E, φ) = Q).",
            "environment_complexity": "Ground set of elements U size n; item set E size m; realization space exponential; goal is expected cost c_avg(π) minimal where cost counts selected items; Δ computations potentially expensive (Monte Carlo).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Theoretical: for independent item-state priors and ĥf monotone submodular, adaptive greedy yields (ln(Q/η)+1)^2 · α factor bound on expected cost in general and (ln(Q/η)+1)^2 for self-certifying instances (Theorem 17 & 13). For stochastic set coverage (cover n elements) yields (ln(n)+1)^2 approximation (Corollary 18).",
            "performance_without_adaptation": "Prior work shows adaptivity gaps for set coverage variants (e.g., Goemans & Vondrák show adaptivity gaps Θ(log n) in some variants); non-adaptive algorithms may achieve worse approximation factors. Paper does not provide explicit empirical baseline reward numbers.",
            "sample_efficiency": "Depends on sampling cost to estimate Δ; no empirical sample counts reported; theoretical dependence via logarithmic factors in Q, δ, η (minimum prior probability and discretization parameter) appears in approximation bound.",
            "exploration_exploitation_tradeoff": "Greedy focuses on coverage per-step (exploitation) but will pick items that are expected to reduce remaining uncovered mass (exploration) when uncertainty about elements remains; termination requires certifying coverage across all consistent realizations resulting in potential extra exploration.",
            "comparison_methods": "Theoretical comparisons to optimal adaptive policy (approximation bounds); references to Goemans & Vondrák (2006), Liu et al. (2008) which study related stochastic set coverage variants and adaptivity gaps.",
            "key_results": "Adaptive submodularity and self-certifying structure imply provable approximation guarantees for adaptive greedy in stochastic coverage problems: squared-logarithmic expected-cost bounds in general and improved bounds for self-certifying instances; special-case stochastic set coverage admits (ln(n)+1)^2 approximation (Corollary 18).",
            "limitations_or_failures": "Proofs require strong conditions (adaptive submodularity, sometimes strong adaptive monotonicity / pointwise submodularity); constants include δ (min prior probability) and η (discretization) causing possibly loose bounds; historical correction: earlier claimed purely logarithmic bounds were corrected to squared-log, leaving open whether log bounds hold; instances that are not self-certifying may have worse guarantees.",
            "uuid": "e1297.3",
            "source_info": {
                "paper_title": "Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization",
                "publication_date_yy_mm": "2010-03"
            }
        },
        {
            "name_short": "AdaptiveViral",
            "name_full": "Adaptive Viral Marketing (Independent Cascade, Full-Adoption Feedback)",
            "brief_description": "Adaptive analog of influence maximization where seeds are selected sequentially, observations reveal which edges are live (full-adoption feedback), and the greedy adaptive policy maximizes expected spread under submodular reward functions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Adaptive Greedy for Adaptive Viral Marketing",
            "agent_description": "Adaptive Greedy policy applied to independent cascade diffusion: items = nodes (people); selecting a node reveals statuses of edges reachable from that node (Full-Adoption Feedback); Δ(e|ψ) equals expected marginal increase in chosen (possibly submodular) reward ĥf of influenced nodes given current observations.",
            "adaptive_design_method": "adaptive influence maximization via expected marginal benefit (active seeding based on posterior over live/dead edges)",
            "adaptation_strategy_description": "After selecting a seed and observing the propagation and edge statuses reachable from it, the policy updates posterior beliefs about which edges are live and selects next seed maximizing expected marginal reward under updated beliefs; uses independence of edge random variables in proofs.",
            "environment_name": "Adaptive Viral Marketing under Independent Cascade model with Full-Adoption Feedback",
            "environment_characteristics": "Partially observable network diffusion: edge variables X_uv ∈ {0,1} (live/dead) drawn independently with known means p_uv; observations upon activation reveal statuses for edges reachable from the activated node (Full-Adoption Feedback); stochastic, discrete, combinatorial diffusion dynamics.",
            "environment_complexity": "Graph with |V| nodes and |A| edges; action space = selecting any unselected node; diffusion process can reach many nodes per activation (cascades); realization space size 2^{|A|}; episode length = budget k or until coverage objective met.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Theoretical: with ĥf monotone submodular over set of influenced nodes and independent edge priors, the adaptive greedy policy achieves (1 - 1/e) approximation to optimal adaptive policy for budgeted maximization (extends Kempe et al. 2003 to adaptive setting). For min-cost cover variant, squared-log type approximation results also apply (paper claims adaptive analogs and provides Theorems in §8).",
            "performance_without_adaptation": "Non-adaptive seeding (choose all seeds upfront) has classical (1 - 1/e) guarantee for deterministic fixed diffusion model; adaptive selection can improve performance in some instances (adaptivity gap analysis in Section 11); exact empirical baselines not provided in-paper.",
            "sample_efficiency": "Computing Δ(e|ψ) may require sampling over edge realizations consistent with ψ; no empirical sample counts are provided; theoretical results assume ability to compute or approximate expectations under posterior.",
            "exploration_exploitation_tradeoff": "Greedy myopic selection balances immediate expected spread (exploitation) and information that could improve future seed choices (exploration) since selecting a node reveals many edge statuses (informative), which the greedy Δ naturally accounts for.",
            "comparison_methods": "Theoretical comparison to optimal adaptive policy and to non-adaptive influence maximization results of Kempe et al. (2003); paper extends those non-adaptive results to adaptive setting. No empirical comparisons provided.",
            "key_results": "Adaptive submodularity holds for independent-cascade with full-adoption feedback and monotone submodular ĥf, enabling the greedy policy to attain (1 - 1/e) approximation for expected spread under budget (adaptive analog of Kempe et al.). Also extends to more general monotone submodular reward functions and to coverage/min-cost variants with provable approximation factors.",
            "limitations_or_failures": "Proofs rely on independence of edge variables and the chosen feedback model (full-adoption); if feedback is weaker or edges correlate, adaptive submodularity may fail and guarantees do not apply. No empirical validation presented in this paper.",
            "uuid": "e1297.4",
            "source_info": {
                "paper_title": "Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization",
                "publication_date_yy_mm": "2010-03"
            }
        },
        {
            "name_short": "AdaptiveActiveLearn",
            "name_full": "Adaptive Active Learning / Generalized Binary Search (Adaptive Querying)",
            "brief_description": "Adaptive querying strategies (including generalized binary search variants) that select examples/tests to maximally shrink version space probability mass, shown to satisfy adaptive submodularity and therefore near-optimality guarantees for query-efficient diagnosis/active learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Adaptive Greedy for Active Learning / Generalized Binary Search",
            "agent_description": "Adaptive Greedy policy applied to active learning and diagnostic testing: items = examples/tests; each item reveals label/state; objective f is reduction in version space probability mass (or related utility), Δ(e|ψ) quantifies expected shrinkage, policy picks item maximizing Δ(e|ψ) until version space sufficiently small or hypothesis identified.",
            "adaptive_design_method": "active learning via expected version-space mass reduction (information gain-like greedy selection)",
            "adaptation_strategy_description": "Selects the next example/test whose expected reduction in posterior version-space mass (or posterior entropy proxy) is maximal given current observed labels; updates posterior over hypotheses after each label and repeats.",
            "environment_name": "Pool-based Active Learning / Diagnostic Testing under stochastic/noisy labels",
            "environment_characteristics": "Partially observable unknown hypothesis drawn from prior p(φ); labels revealed only for queried examples/tests; version space evolves with each observation; possibly persistent noise handled by using surrogate objectives that remain adaptive submodular.",
            "environment_complexity": "Hypothesis space size can be large; action space is the pool of unlabeled examples (|E|); episode length until identification or label budget exhausted; computing exact Δ may be expensive (requires posterior over hypotheses).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Theoretical: reduction-in-version-space-mass objective is adaptive submodular, hence adaptive greedy (generalized binary search and approximate variants) is near-optimal with formal bounds (new analysis provided in §9). Specific numeric empirical metrics not reported in this paper excerpt.",
            "performance_without_adaptation": "Non-adaptive querying (random or preselected sets) is typically less sample-efficient; prior results (Kosaraju et al., Dasgupta) show gaps where adaptive querying can provide exponential advantages in some settings; exact baselines not enumerated here.",
            "sample_efficiency": "Adaptive greedy minimizes expected number of queries to identify hypothesis under adaptive submodularity; theoretical bounds on expected query count derived in §9 (not numerically instantiated in this excerpt).",
            "exploration_exploitation_tradeoff": "Greedy selection targets maximal expected information (exploration) per query; exploitation is implicit as queries that reduce version space quickly also lead to quicker confident labeling/decisions.",
            "comparison_methods": "Relates to and extends generalized binary search, Kosaraju et al. (1999), Dasgupta (2004); compared theoretically against optimal policies (approximation guarantees) and to worst-case interactive submodular set cover (Guillory & Bilmes).",
            "key_results": "Reduction-in-version-space objectives are adaptive submodular; therefore adaptive greedy querying policies (and approximate variants) have provable near-optimal guarantees for active learning and diagnostic testing tasks (Section 9). The framework also handles natural generalizations including noisy/persistent label noise via suitable surrogate objectives.",
            "limitations_or_failures": "Adaptive submodularity may not hold for some active learning objectives or in presence of complex noise models without careful surrogate choices; computational cost of exact Δ may be high requiring sampling/approximations; no broad empirical validation provided in this paper excerpt.",
            "uuid": "e1297.5",
            "source_info": {
                "paper_title": "Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization",
                "publication_date_yy_mm": "2010-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Asadpour et al. (2008)",
            "rating": 2
        },
        {
            "paper_title": "Kempe, Kleinberg, and Tardos (2003)",
            "rating": 2
        },
        {
            "paper_title": "Goemans and Vondrák (2006)",
            "rating": 2
        },
        {
            "paper_title": "Liu et al. (2008)",
            "rating": 2
        },
        {
            "paper_title": "Streeter and Golovin (2008)",
            "rating": 2
        },
        {
            "paper_title": "Leskovec et al. (2007)",
            "rating": 2
        },
        {
            "paper_title": "Guillory and Bilmes (2010)",
            "rating": 1
        },
        {
            "paper_title": "Guillory and Bilmes (2011)",
            "rating": 1
        },
        {
            "paper_title": "Kosaraju et al. (1999)",
            "rating": 1
        },
        {
            "paper_title": "Dasgupta (2004)",
            "rating": 1
        }
    ],
    "cost": 0.021061499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization</h1>
<p>Daniel Golovin<br>California Institute of Technology<br>Pasadena, CA 91125, USA<br>Andreas Krause<br>KRAUSEA@ETHZ.CH<br>ETH Zurich<br>8092 Zurich, Switzerland</p>
<h4>Abstract</h4>
<p>Many problems in artificial intelligence require adaptively making a sequence of decisions with uncertain outcomes under partial observability. Solving such stochastic optimization problems is a fundamental but notoriously difficult challenge. In this paper, we introduce the concept of adaptive submodularity, generalizing submodular set functions to adaptive policies. We prove that if a problem satisfies this property, a simple adaptive greedy algorithm is guaranteed to be competitive with the optimal policy. In addition to providing performance guarantees for both stochastic maximization and coverage, adaptive submodularity can be exploited to drastically speed up the greedy algorithm by using lazy evaluations. We illustrate the usefulness of the concept by giving several examples of adaptive submodular objectives arising in diverse AI applications including management of sensing resources, viral marketing and active learning. Proving adaptive submodularity for these problems allows us to recover existing results in these applications as special cases, improve approximation guarantees and handle natural generalizations.</p>
<p>Keywords: Adaptive Optimization, Stochastic Optimization, Submodularity, Partial Observability, Active Learning, Optimal Decision Trees</p>
<h2>1. Introduction</h2>
<p>In many problems arising in artificial intelligence one needs to adaptively make a sequence of decisions, taking into account observations about the outcomes of past decisions. Often these outcomes are uncertain, and one may only know a probability distribution over them. Finding optimal policies for decision making in such partially observable stochastic optimization problems is notoriously intractable (see, e.g., Littman et al. (1998)). A fundamental challenge is to identify classes of planning problems for which simple solutions obtain (near-) optimal performance.</p>
<p>In this paper, we introduce the concept of adaptive submodularity, and prove that if a partially observable stochastic optimization problem satisfies this property, a simple adaptive greedy algorithm is guaranteed to obtain near-optimal solutions. In fact, under reasonable complexity-theoretic assumptions, no polynomial time algorithm is able to obtain better solutions in general. Adaptive submodularity generalizes the classical notion of submodularity ${ }^{1}$, which has been successfully used to develop approximation algorithms for a variety of non-adaptive optimization problems. Submodularity, informally, is an intuitive notion of diminishing returns, which states that adding an element to a small set helps more than adding that same element to a larger (super-) set. A celebrated result of the work of Nemhauser et al. (1978) guarantees that for such submodular functions, a simple greedy algorithm, which adds the element that maximally increases the objective value, selects a near-optimal set of $k$ elements. Similarly, it is guaranteed to find a set of near-minimal cost that achieves a desired quota of utility (Wolsey, 1982), using near-minimum average time to do so (Streeter and Golovin,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>2008). Besides guaranteeing theoretical performance bounds, submodularity allows us to speed up algorithms without loss of solution quality by using lazy evaluations (Minoux, 1978), often leading to performance improvements of several orders of magnitude (Leskovec et al., 2007). Submodularity has been shown to be very useful in a variety of problems in artificial intelligence (Krause and Guestrin, 2009b).</p>
<p>The challenge in generalizing submodularity to adaptive planning - where the action taken in each step depends on information obtained in the previous steps - is that feasible solutions are now policies (decision trees or conditional plans) instead of subsets. We propose a natural generalization of the diminishing returns property for adaptive problems, which reduces to the classical characterization of submodular set functions for deterministic distributions. We show how these results of Nemhauser et al. (1978), Wolsey (1982), Streeter and Golovin (2008), and Minoux (1978) generalize to the adaptive setting. Hence, we demonstrate how adaptive submodular optimization problems enjoy theoretical and practical benefits similar to those of classical, nonadaptive submodular problems. We further demonstrate the usefulness and generality of the concept by showing how it captures known results in stochastic optimization and active learning as special cases, admits tighter performance bounds, leads to natural generalizations and allows us to solve new problems for which no performance guarantees were known.</p>
<p>As a first example, consider the problem of deploying (or controlling) a collection of sensors to monitor some spatial phenomenon. Each sensor can cover a region depending on its sensing range. Suppose we would like to find the best subset of $k$ locations to place the sensors. In this application, intuitively, adding a sensor helps more if we have placed few sensors so far and helps less if we have already placed many sensors. We can formalize this diminishing returns property using the notion of submodularity - the total area covered by the sensors is a submodular function defined over all sets of locations. Krause and Guestrin (2007) show that many more realistic utility functions in sensor placement (such as the improvement in prediction accuracy w.r.t. some probabilistic model) are submodular as well. Now consider the following stochastic variant: Instead of deploying a fixed set of sensors, we deploy one sensor at a time. With a certain probability, deployed sensors can fail, and our goal is to maximize the area covered by the functioning sensors. Thus, when deploying the next sensor, we need to take into account which of the sensors we deployed in the past failed. This problem has been studied by Asadpour et al. (2008) for the case where each sensor fails independently at random. In this paper, we show that the coverage objective is adaptive submodular, and use this concept to handle much more general settings (where, e.g., rather than all-or-nothing failures there are different types of sensor failures of varying severity). We also consider a related problem where the goal is to place the minimum number of sensors to achieve the maximum possible sensor coverage (i.e., the coverage obtained by deploying sensors everywhere), or more generally the goal may be to achieve any fixed percentage of the maximum possible sensor coverage. Under the first goal, the problem is equivalent to one studied by Goemans and Vondrák (2006), and generalizes a problem studied by Liu et al. (2008).</p>
<p>As another example, consider a viral marketing problem, where we are given a social network, and we want to influence as many people as possible in the network to buy some product. We do that by giving the product for free to a subset of the people, and hope that they convince their friends to buy the product as well. Formally, we have a graph, and each edge $e$ is labeled by a number $0 \leq p_{e} \leq 1$. We "influence" a subset of nodes in the graph, and for each influenced node, their neighbors get randomly influenced according to the probability annotated on the edge connecting the nodes. This process repeats until no further node gets influenced. Kempe et al. (2003) show that the set function which quantifies the expected number of nodes influenced is submodular. A natural stochastic variant of the problem is where we pick a node, get to see which nodes it influenced, then adaptively pick the next node based on these observations and so on. We show that a large class of such adaptive influence maximization problems satisfies adaptive submodularity.</p>
<p>Our third application is in active learning, where we are given an unlabeled data set, and we would like to adaptively pick a small set of examples whose labels imply all other labels. The same problem arises in automated diagnosis, where we have hypotheses about the state of the system (e.g., what illness a patient has), and would like to perform tests to identify the correct hypothesis. In both domains we want to pick examples / tests to shrink the remaining version space (the set of consistent hypotheses) as quickly as possible. Here, we show that the reduction in version space probability mass is adaptive submodular, and use that observation to prove that the adaptive greedy algorithm is a near-optimal querying policy. Our results for active learning</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>New Results</th>
<th>Location</th>
</tr>
</thead>
<tbody>
<tr>
<td>A.S. Maximization</td>
<td>Tight $(1-1 / e)$-approx. for A.M.S. objectives</td>
<td>$\S 5.1$, page 10</td>
</tr>
<tr>
<td>A.S. Min Cost Coverage</td>
<td>Squared logarithmic approx. for A.M.S. objectives</td>
<td>$\S 5.2$, page 12</td>
</tr>
<tr>
<td>A.S. Min Sum Cover</td>
<td>Tight 4-approx. for A.M.S. objectives</td>
<td>$\S 5.3$, page 15</td>
</tr>
<tr>
<td>Data Dependent Bounds</td>
<td>Generalization to A.M.S. functions</td>
<td>$\S 5.1$, page 10</td>
</tr>
<tr>
<td>Accelerated Greedy</td>
<td>Generalization of lazy evaluations to the adaptive setting</td>
<td>$\S 4$, page 9</td>
</tr>
<tr>
<td>Stochastic Submodular</td>
<td>Generalization of the previous $(1-1 / e)$-approx. to arbitrary</td>
<td>$\S 6$, page 16</td>
</tr>
<tr>
<td>Maximization</td>
<td>per-item set distributions, and to item costs</td>
<td></td>
</tr>
<tr>
<td>Stochastic Set Cover</td>
<td>$(\ln (n)+1)^{2}$-approx. for arbitrary per-item set distributions,</td>
<td>$\S 7$, page 18</td>
</tr>
<tr>
<td></td>
<td>with item costs</td>
<td></td>
</tr>
<tr>
<td>Adaptive Viral Marketing</td>
<td>Adaptive analog of previous $(1-1 / e)$-approx. for non-adaptive</td>
<td>$\S 8$, page 20</td>
</tr>
<tr>
<td></td>
<td>viral marketing, under more general reward functions; squared</td>
<td></td>
</tr>
<tr>
<td></td>
<td>logarithmic approx. for the adaptive min cost cover version</td>
<td></td>
</tr>
<tr>
<td>Active Learning</td>
<td>New analysis for generalized binary search and its approximate</td>
<td>$\S 9$, page 24</td>
</tr>
<tr>
<td></td>
<td>versions with and without item costs</td>
<td></td>
</tr>
<tr>
<td>Hardness in the absence of</td>
<td>$\Omega\left(</td>
<td>E</td>
</tr>
<tr>
<td>Adapt. Submodularity</td>
<td>Min Cost Coverage, and Min-Sum Cover, if $f$ is not adaptive</td>
<td></td>
</tr>
<tr>
<td></td>
<td>submodular.</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 1: Summary of our theoretical results. A.S. is shorthand for "adaptive stochastic", and A.M.S. is shorthand for "adaptive monotone submodular."
and automated diagnosis are also related to recent results of Guillory and Bilmes $(2010,2011)$ who study generalizations of submodular set cover to an interactive setting. In contrast to our approach however, Guillory and Bilmes analyze worst-case costs, and use rather different technical definitions and proof techniques.</p>
<p>We summarize our main contributions below, and provide a more technical summary in Table 1. At a high level, our main contributions are:</p>
<ul>
<li>We consider a particular class of partially observable adaptive stochastic optimization problems, which we prove to be hard to approximate in general.</li>
<li>We introduce the concept of adaptive submodularity, and prove that if a problem instance satisfies this property, a simple adaptive greedy policy performs near-optimally, for both adaptive stochastic maximization and coverage, and also a natural min-sum objective.</li>
<li>We show how adaptive submodularity can be exploited by allowing the use of an accelerated adaptive greedy algorithm using lazy evaluations, and how we can obtain data-dependent bounds on the optimum.</li>
<li>We illustrate adaptive submodularity on several realistic problems, including Stochastic Maximum Coverage, Stochastic Submodular Coverage, Adaptive Viral Marketing, and Active Learning. For these applications, adaptive submodularity allows us to recover known results and prove natural generalizations.</li>
</ul>
<h1>1.1 Organization</h1>
<p>This article is organized as follows. In $\S 2$ (page 4) we set up notation and formally define the relevant adaptive optimization problems for general objective functions. For the reader's convenience, we have also provided a reference table of important symbols on page 58. In $\S 3$ (page 6) we review the classical notion of submodularity and introduce the novel adaptive submodularity property. In $\S 4$ (page 9) we introduce the adaptive greedy policy, as well as an accelerated variant. In $\S 5$ (page 10) we discuss the theoretical guarantees that the adaptive greedy policy enjoys when applied to problems with adaptive submodular objectives. Sections 6 through 9 provide examples on how to apply the adaptive submodular framework to various applications,</p>
<p>namely Stochastic Submodular Maximization (§6, page 16), Stochastic Submodular Coverage (§7, page 18), Adaptive Viral Marketing (§8, page 20), and Active Learning (§9, page 24). In $\S 10$ (page 29) we report empirical results on two sensor selection problems. In $\S 11$ (page 31) we discuss the adaptivity gap of the problems we consider, and in $\S 12$ (page 33) we prove hardness results indicating that problems which are not adaptive submodular can be extremely inapproximable under reasonable complexity assumptions. We review related work in $\S 13$ (page 34) and provide concluding remarks in $\S 14$ (page 36). The Appendix (page 41) gives details of how to incorporate item costs and includes all of the proofs omitted from the main text.</p>
<h1>2. Adaptive Stochastic Optimization</h1>
<p>We start by introducing notation and defining the general class of adaptive optimization problems that we address in this paper. For sake of clarity, we will illustrate our notation using the sensor placement application mentioned in $\S 1$. We give examples of other applications in $\S 6, \S 7, \S 8$, and $\S 9$.</p>
<h3>2.1 Items and Realizations</h3>
<p>Let $E$ be a finite set of items (e.g., sensor locations). Each item $e \in E$ is in a particular (initially unknown) state from a set $O$ of possible states (describing whether a sensor placed at location $e$ would malfunction or not). We represent the item states using a function $\phi: E \rightarrow O$, called a realization (of the states of all items in the ground set). Thus, we say that $\phi(e)$ is the state of $e$ under realization $\phi$. We use $\Phi$ to denote a random realization. We take a Bayesian approach and assume that there is a known prior probability distribution $p(\phi):=\mathbb{P}[\Phi=\phi]$ over realizations (e.g., modeling that sensors fail independently with failure probability), so that we can compute posterior distributions ${ }^{2}$. We will consider problems where we sequentially pick an item $e \in E$, get to see its state $\Phi(e)$, pick the next item, get to see its state, and so on (e.g., place a sensor, see whether it fails, and so on). After each pick, our observations so far can be represented as a partial realization $\psi$, a function from some subset of $E$ (i.e., the set of items that we already picked) to their states (e.g., $\psi$ encodes where we placed sensors and which of them failed). For notational convenience, we sometimes represent $\psi$ as a relation, so that $\psi \subseteq E \times O$ equals ${(e, o): \psi(e)=o}$. We use the notation $\operatorname{dom}(\psi)={e: \exists o .(e, o) \in \psi}$ to refer to the domain of $\psi$ (i.e., the set of items observed in $\psi$ ). A partial realization $\psi$ is consistent with a realization $\phi$ if they are equal everywhere in the domain of $\psi$. In this case we write $\phi \sim \psi$. If $\psi$ and $\psi^{\prime}$ are both consistent with some $\phi$, and $\operatorname{dom}(\psi) \subseteq \operatorname{dom}\left(\psi^{\prime}\right)$, we say $\psi$ is a subrealization of $\psi^{\prime}$. Equivalently, $\psi$ is a subrealization of $\psi^{\prime}$ if and only if, when viewed as relations, $\psi \subseteq \psi^{\prime}$.</p>
<p>Partial realizations are similar to the notion of "belief states" in Partially Observable Markov Decision Problems (POMDPs), as they encode the effect of all actions taken (items selected) and observations made, and determine our posterior belief about the state of the world (i.e., the state of all items $e$ not yet selected, $p(\phi \mid \psi):=\mathbb{P}[\Phi=\phi \mid \Phi \sim \psi])$.</p>
<h3>2.2 Policies</h3>
<p>We encode our adaptive strategy for picking items as a policy $\pi$, which is a function from a set of partial realizations to $E$, specifying which item to pick next under a particular set of observations (e.g., $\pi$ chooses the next sensor location given where we have placed sensors so far, and whether they failed or not). We also allow randomized policies that are functions from a set of partial realizations to distributions on $E$, though our emphasis will primarily be on deterministic policies. If $\psi$ is not in the domain of $\pi$, the policy terminates (stops picking items) upon observation of $\psi$. We use $\operatorname{dom}(\pi)$ to denote the domain of $\pi$. Technically, we require that $\operatorname{dom}(\pi)$ be closed under subrealizations. That is, if $\psi^{\prime} \in \operatorname{dom}(\pi)$ and $\psi$ is a subrealization of $\psi^{\prime}$ then $\psi \in \operatorname{dom}(\pi)$. We use the notation $E(\pi, \phi)$ to refer to the set of items selected by $\pi$ under realization $\phi$. Each deterministic policy $\pi$ can be associated with a decision tree $T^{\pi}$ in a natural way (see Fig. 1 for an</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of a policy $\pi$, its corresponding decision tree representation, and the decision tree representation of $\pi_{[2]}$, the level 2 truncation of $\pi$ (as defined in $\S 5.1$ ).
illustration). Here, we adopt a policy-centric view that admits concise notation, though we find the decision tree view to be valuable conceptually.</p>
<p>Since partial realizations are similar to POMDP belief states, our definition of policies is similar to the notion of policies in POMDPs, which are usually defined as functions from belief states to actions. We will further discuss the relationship between the stochastic optimization problems considered in this paper and POMDPs in Section 13.</p>
<h1>2.3 Adaptive Stochastic Maximization, Coverage, and Min-Sum Coverage</h1>
<p>We wish to maximize, subject to some constraints, a utility function $f: 2^{E} \times O^{E} \rightarrow \mathbb{R}<em _avg="{avg" _text="\text">{\geq 0}$ that depends on which items we pick and which state each item is in (e.g., modeling the total area covered by the working sensors). Based on this notation, the expected utility of a policy $\pi$ is $f</em>$ such that}}(\pi):=\mathbb{E}[f(E(\pi, \Phi), \Phi)]$ where the expectation is taken with respect to $p(\phi)$. The goal of the Adaptive Stochastic Maximization problem is to find a policy $\pi^{*</p>
<p>$$
\pi^{*} \in \underset{\pi}{\arg \max } f_{\text {avg }}(\pi) \text { subject to }|E(\pi, \phi)| \leq k \text { for all } \phi
$$</p>
<p>where $k$ is a budget on how many items can be picked (e.g., we would like to adaptively choose $k$ sensor locations such that the working sensors provide as much information as possible in expectation).</p>
<p>Alternatively, we can specify a quota $Q$ of utility that we would like to obtain, and try to find the cheapest policy achieving that quota (e.g., we would like to achieve a certain amount of information, as cheaply as possible in expectation). Formally, we define the average cost $c_{\text {avg }}(\pi)$ of a policy as the expected number of items it picks, so that $c_{\text {avg }}(\pi):=\mathbb{E}[|E(\pi, \Phi)|]$. Our goal is then to find</p>
<p>$$
\pi^{*} \in \underset{\pi}{\arg \min } c_{\text {avg }}(\pi) \text { such that } f(E(\pi, \phi), \phi) \geq Q \text { for all } \phi
$$</p>
<p>i.e., the policy $\pi^{*}$ that minimizes the expected number of items picked such that under all possible realizations, at least utility $Q$ is achieved. We call Problem 2 the Adaptive Stochastic Minimum Cost Cover problem. We will also consider the problem where we want to minimize the worst-case cost $c_{\text {wc }}(\pi):=\max <em _text="\text" _wc="{wc">{\phi}|E(\pi, \phi)|$. This worst-case cost $c</em>$, the decision tree associated with $\pi$.}}(\pi)$ is the cost incurred under adversarially chosen realizations, or equivalently the depth of the deepest leaf in $T^{\pi</p>
<p>Yet another important variant is to minimize the average time required by a policy to obtain its utility. Formally, let $u(\pi, t)$ be the expected utility obtained by $\pi$ after $t$ steps $^{3}$, let $Q=\mathbb{E}[f(E, \Phi)]$ be the maximum possible expected utility, and define the min-sum cost $c_{\Sigma}(\pi)$ of a policy as $c_{\Sigma}(\pi):=\sum_{t=0}^{\infty}(Q-u(\pi, t))$. We then define the Adaptive Stochastic Min-Sum Cover problem as the search for</p>
<p>$$
\pi^{*} \in \underset{\pi}{\arg \min } c_{\Sigma}(\pi)
$$</p>
<p>Unfortunately, as we will show in $\S 12$, even for linear functions $f$, i.e., those where $f(A, \phi)=\sum_{e \in A} w_{e, \phi}$ is simply the sum of weights (depending on the realization $\phi$ ), Problems (1), (2), and (3) are hard to approximate</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>under reasonable complexity theoretic assumptions. Despite the hardness of the general problems, in the following sections we will identify conditions that are sufficient to allow us to approximately solve them.</p>
<h1>2.4 Incorporating Item Costs</h1>
<p>Instead of quantifying the cost of a set $E(\pi, \phi)$ by the number of elements $|E(\pi, \phi)|$, we can also consider the case where each item $e \in E$ has a cost $c(e)$, and the cost of a set $S \subseteq E$ is $c(S)=\sum_{e \in S} c(e)$. We can then consider variants of Problems (1), (2), and (3) with the $|E(\pi, \phi)|$ replaced by $c(E(\pi, \phi))$. For clarity of presentation, we will focus on the unit cost case, i.e., $c(e)=1$ for all $e$, and explain how our results generalize to the non-uniform case in the Appendix.</p>
<h2>3. Adaptive Submodularity</h2>
<p>We first review the classical notion of submodular set functions, and then introduce the novel notion of adaptive submodularity.</p>
<h3>3.1 Background on Submodularity</h3>
<p>Let us first consider the very special case where $p(\phi)$ is deterministic or, equivalently, $|O|=1$ (e.g., in our sensor placement applications, sensors never fail). In this case, the realization $\phi$ is known to the decision maker in advance, and thus there is no benefit in adaptive selection. Given the realization $\phi$, Problem (1) is equivalent to finding a set $A^{*} \subseteq E$ such that</p>
<p>$$
A^{*} \in \underset{A \subseteq E}{\arg \max } f(A, \phi) \text { such that }|A| \leq k
$$</p>
<p>For most interesting classes of utility functions $f$, this is an NP-hard optimization problem. However, in many practical problems, such as those mentioned in $\S 1, f(A)=f(A, \phi)$ satisfies submodularity. A set function $f: 2^{E} \rightarrow \mathbb{R}$ is called submodular if, whenever $A \subseteq B \subseteq E$ and $e \in E \backslash B$ it holds that</p>
<p>$$
f(A \cup{e})-f(A) \geq f(B \cup{e})-f(B)
$$</p>
<p>i.e., adding $e$ to the smaller set $A$ increases $f$ by at least as much as adding $e$ to the superset $B$. Furthermore, $f$ is called monotone, if, whenever $A \subseteq B$ it holds that $f(A) \leq f(B)$ (e.g., adding a sensor can never reduce the amount of information obtained). A celebrated result by Nemhauser et al. (1978) states that for monotone submodular functions with $f(\emptyset)=0$, a simple greedy algorithm that starts with the empty set, $A_{0}=\emptyset$ and chooses</p>
<p>$$
A_{i+1}=A_{i} \cup\left{\underset{e \in E \backslash A_{i}}{\arg \max } f\left(A_{i} \cup{e}\right)\right}
$$</p>
<p>guarantees that $f\left(A_{k}\right) \geq(1-1 / e) \max <em k="k">{|A| \leq k} f(A)$. Thus, the greedy set $A</em>$; under this assumption no polynomial time algorithm can do strictly better than the greedy algorithm, i.e., achieve a $(1-1 / e+\epsilon)$-approximation for any constant $\epsilon&gt;0$, even for the special case of Maximum $k$-Cover where $f(A)$ is the cardinality of the union of sets indexed by $A$. Similarly, Wolsey (1982) shows that the same greedy algorithm also near-optimally solves the deterministic case of Problem (2), called the Minimum Submodular Cover problem:}$ obtains at least a $(1-1 / e)$ fraction of the optimal value achievable using $k$ elements. Furthermore, Feige (1998) shows that this result is tight if $\mathrm{P} \neq \mathrm{NP</p>
<p>$$
A^{*} \in \underset{A \subseteq E}{\arg \min }|A| \text { such that } f(A) \geq Q
$$</p>
<p>Pick the first set $A_{\ell}$ constructed by the greedy algorithm such that $f\left(A_{\ell}\right) \geq Q$. Then, for integer-valued submodular functions, $\ell$ is at most $\left|A^{*}\right|\left(1+\log \max _{e} f(e)\right)$, i.e., the greedy set is at most a logarithmic factor larger than the smallest set achieving quota $Q$. For the special case of Set Cover, where $f(A)$ is the</p>
<p>cardinality of a union of sets indexed by $A$, this result matches a lower bound by Feige (1998): Unless $\mathrm{NP} \subseteq \operatorname{DTIME}\left(n^{\mathcal{O}(\log \log n)}\right)$, Set Cover is hard to approximate by a factor better than $(1-\varepsilon) \ln Q$, where $Q$ is the number of elements to be covered.</p>
<p>Now let us relax the assumption that $p(\phi)$ is deterministic. In this case, we may still want to find a nonadaptive solution (i.e., a constant policy $\pi_{A}$ that always picks set $A$ independently of $\Phi$ ) maximizing $f_{\text {avg }}\left(\pi_{A}\right)$. If $f$ is pointwise submodular, i.e., $f(A, \phi)$ is submodular in $A$ for any fixed $\phi$, the function $f(A)=f_{\text {avg }}\left(\pi_{A}\right)$ is submodular, since nonnegative linear combinations of submodular functions remain submodular. Thus, the greedy algorithm allows us to find a near-optimal non-adaptive policy. That is, in our sensor placement example, if we are willing to commit to all locations before finding out whether the sensors fail or not, the greedy algorithm can provide a good solution to this non-adaptive problem.</p>
<p>However, in practice, we may be more interested in obtaining a non-constant policy $\pi$, that adaptively chooses items based on previous observations (e.g., takes into account which sensors are working before placing the next sensor). In many settings, selecting items adaptively offers huge advantages, analogous to the advantage of binary search over sequential (linear) search ${ }^{4}$. Thus, the question is whether there is a natural extension of submodularity to policies. In the following, we will develop such a notion - adaptive submodularity.</p>
<h1>3.2 Adaptive Monotonicity and Submodularity</h1>
<p>The key challenge is to find appropriate generalizations of monotonicity and of the diminishing returns condition (5). We begin by again considering the very special case where $p(\phi)$ is deterministic, so that the policies are non-adaptive. In this case a policy $\pi$ simply specifies a sequence of items $\left(e_{1}, e_{2}, \ldots, e_{r}\right)$ which it selects in order. Monotonicity in this context can be characterized as the property that "the marginal benefit of selecting an item is always nonnegative," meaning that for all such sequences $\left(e_{1}, e_{2}, \ldots, e_{r}\right)$, items $e$ and $1 \leq i \leq r$ it holds that $f\left(\left{e_{j}: j \leq i\right} \cup{e}\right)-f\left(\left{e_{j}: j \leq i\right}\right) \geq 0$. Similarly, submodularity can be viewed as the property that "selecting an item later never increases its marginal benefit," meaning that for all sequences $\left(e_{1}, e_{2}, \ldots, e_{r}\right)$, items $e$, and all $i \leq r, f\left(\left{e_{j}: j \leq i\right} \cup{e}\right)-f\left(\left{e_{j}: j \leq i\right}\right) \geq$ $f\left(\left{e_{j}: j \leq r\right} \cup{e}\right)-f\left(\left{e_{j}: j \leq r\right}\right)$.</p>
<p>We take these views of monotonicity and submodularity when defining their adaptive analogues, by using an appropriate generalization of the marginal benefit. When moving to the general adaptive setting, the challenge is that the items' states are now random and only revealed upon selection. A natural approach is thus to condition on observations (i.e., partial realizations of selected items), and take the expectation with respect to the items that we consider selecting. Hence, we define our adaptive monotonicity and submodularity properties in terms of the conditional expected marginal benefit of an item.</p>
<p>Definition 1 (Conditional Expected Marginal Benefit) Given a partial realization $\psi$ and an item e, the conditional expected marginal benefit of e conditioned on having observed $\psi$, denoted $\Delta(e \mid \psi)$, is</p>
<p>$$
\Delta(e \mid \psi):=\mathbb{E}\left[f(\operatorname{dom}(\psi) \cup{e}, \Phi)-f(\operatorname{dom}(\psi), \Phi) \mid \Phi \sim \psi\right]
$$</p>
<p>where the expectation is computed with respect to $p(\phi \mid \psi)=\mathbb{P}[\Phi=\phi \mid \Phi \sim \psi]$. Similarly, the conditional expected marginal benefit of a policy $\pi$ is</p>
<p>$$
\Delta(\pi \mid \psi):=\mathbb{E}\left[f(\operatorname{dom}(\psi) \cup E(\pi, \Phi), \Phi)-f(\operatorname{dom}(\psi), \Phi) \mid \Phi \sim \psi\right]
$$</p>
<p>In our sensor placement example, $\Delta(e \mid \psi)$ quantifies the expected amount of additional area covered by placing a sensor at location $e$, in expectation over the posterior distribution $p_{\Phi(e)}(o):=\mathbb{P}[\Phi(e)=o \mid \Phi \sim \psi]$ of whether the sensor will fail or not, and taking into account the area covered by the placed working</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>sensors as encoded by $\psi$. Note that the benefit we have accrued upon observing $\psi$ (and hence after having selected the items in $\operatorname{dom}(\psi)$ ) is $\mathbb{E}[f(\operatorname{dom}(\psi), \Phi) \mid \Phi \sim \psi]$, which is the benefit term subtracted out in Eq. (8) and Eq. (9). Similarly, the expected total benefit obtained after observing $\psi$ and then selecting $e$ is $\mathbb{E}[f(\operatorname{dom}(\psi) \cup{e}, \Phi) \mid \Phi \sim \psi]$. The corresponding benefit for running $\pi$ after observing $\psi$ is slightly more complex. Under realization $\phi \sim \psi$, the final cumulative benefit will be $f(\operatorname{dom}(\psi) \cup E(\pi, \phi), \phi)$. Taking the expectation with respect to $p(\phi \mid \psi)$ and subtracting out the benefit already obtained by $\operatorname{dom}(\psi)$ then yields the conditional expected marginal benefit of $\pi$.</p>
<p>We are now ready to introduce our generalizations of monotonicity and submodularity to the adaptive setting:</p>
<p>Definition 2 (Adaptive Monotonicity) A function $f: 2^{E} \times O^{E} \rightarrow \mathbb{R}_{\geq 0}$ is adaptive monotone with respect to distribution $p(\phi)$ if the conditional expected marginal benefit of any item is nonnegative, i.e., for all $\psi$ with $\mathbb{P}[\Phi \sim \psi]&gt;0$ and all $e \in E$ we have</p>
<p>$$
\Delta(e \mid \psi) \geq 0
$$</p>
<p>Definition 3 (Adaptive Submodularity) A function $f: 2^{E} \times O^{E} \rightarrow \mathbb{R}_{\geq 0}$ is adaptive submodular with respect to distribution $p(\phi)$ if the conditional expected marginal benefit of any fixed item does not increase as more items are selected and their states are observed. Formally, $f$ is adaptive submodular w.r.t. $p(\phi)$ if for all $\psi$ and $\psi^{\prime}$ such that $\psi$ is a subrealization of $\psi^{\prime}$ (i.e., $\psi \subseteq \psi^{\prime}$ ), and for all $e \in E \backslash \operatorname{dom}\left(\psi^{\prime}\right)$, we have</p>
<p>$$
\Delta(e \mid \psi) \geq \Delta\left(e \mid \psi^{\prime}\right)
$$</p>
<p>From the decision tree perspective, the condition $\Delta(e \mid \psi) \geq \Delta\left(e \mid \psi^{\prime}\right)$ amounts to saying that for any decision tree $T$, if we are at a node $v$ in $T$ which selects an item $e$, and compare the expected marginal benefit of $e$ selected at $v$ with the expected marginal benefit $e$ would have obtained if it were selected at an ancestor of $v$ in $T$, then the latter must be no smaller than the former. Note that when comparing the two expected marginal benefits, there is a difference in both the set of items previously selected (i.e., $\operatorname{dom}(\psi)$ vs. $\left.\operatorname{dom}\left(\psi^{\prime}\right)\right)$ and in the distribution over realizations (i.e., $p(\phi \mid \psi)$ vs. $p\left(\phi \mid \psi^{\prime}\right)$ ). It is also worth emphasizing that adaptive submodularity is defined relative to the distribution $p(\phi)$ over realizations; it is possible that $f$ is adaptive submodular with respect to one distribution, but not with respect to another.</p>
<p>We will give concrete examples of adaptive monotone and adaptive submodular functions that arise in the applications introduced in $\S 1$ in $\S 6, \S 7, \S 8$, and $\S 9$. In the Appendix, we will explain how the notion of adaptive submodularity can be extended to handle non-uniform costs (since, e.g., the cost of placing a sensor at an easily accessible location may be smaller than at a location that is hard to get to).</p>
<h1>3.3 Properties of Adaptive Submodular Functions</h1>
<p>It can be seen that adaptive monotonicity and adaptive submodularity enjoy similar closure properties as monotone submodular functions. In particular, if $w_{1}, \ldots, w_{m} \geq 0$ and $f_{1}, \ldots, f_{m}$ are adaptive monotone submodular w.r.t. distribution $p(\phi)$, then $f(A, \phi)=\sum_{i=1}^{m} w_{i} f_{i}(A, \phi)$ is adaptive monotone submodular w.r.t. $p(\phi)$. Similarly, for a fixed constant $c \geq 0$ and adaptive monotone submodular function $f$, the function $g(E, \phi)=\min (f(E, \phi), c)$ is adaptive monotone submodular. Thus, adaptive monotone submodularity is preserved by nonnegative linear combinations and by truncation. Adaptive monotone submodularity is also preserved by restriction, so that if $f: 2^{E} \times O^{E} \rightarrow \mathbb{R}<em 0="0" _geq="\geq">{\geq 0}$ is adaptive monotone submodular w.r.t. $p(\phi)$, then for any $e \in E$, the function $g: 2^{E \backslash{e}} \times O^{E} \rightarrow \mathbb{R}</em>[\Phi=\phi \mid \Phi \sim \psi]$.}$ defined by $g(A, \phi):=f(A, \phi)$ for all $A \subseteq E \backslash{e}$ and all $\phi$ is also adaptive submodular w.r.t. $p(\phi)$. Finally, if $f: 2^{E} \times O^{E} \rightarrow \mathbb{R}_{\geq 0}$ is adaptive monotone submodular w.r.t. $p(\phi)$ then for each partial realization $\psi$ the conditional function $g(A, \phi):=f(A \cup \operatorname{dom}(\psi), \phi)$ is adaptive monotone submodular w.r.t. $p(\phi \mid \psi):=\mathbb{P</p>
<h3>3.4 What Problem Characteristics Suggest Adaptive Submodularity?</h3>
<p>Adaptive submodularity is a diminishing returns property for policies. Speaking informally, it can be applied in situations where there is an objective function to be optimized does not feature synergies in the benefits</p>
<p>of items conditioned on observations. In some cases, the primary objective might not have this property, but a suitably chosen proxy of it does, as is the case with active learning with persistent noise (Golovin et al., 2010; Bellala and Scott, 2010). We give example applications in $\S 6$ through $\S 9$. It is also worth mentioning where adaptive submodularity is not directly applicable. An extreme example of synergistic effects between items conditioned on observations is the class of "treasure hunting" instances used to prove Theorem 26 on page 33 , where the (binary) state of certain groups of items encode the treasure's location in a complex manner. Another problem feature which adaptive submodularity does not directly address is the possibility that items selection can alter the underlying realization $\phi$, as is the case for the problem of optimizing policies for general POMDPs.</p>
<h1>4. The Adaptive Greedy Policy</h1>
<p>The classical non-adaptive greedy algorithm (6) has a natural generalization to the adaptive setting. The greedy policy $\pi^{\text {greedy }}$ tries, at each iteration, to myopically increase the expected objective value, given its current observations. That is, suppose $f: 2^{E} \times O^{E} \rightarrow \mathbb{R}_{\geq 0}$ is the objective, and $\psi$ is the partial realization indicating the states of items selected so far. Then the greedy policy will select the item $e$ maximizing the expected increase in value, conditioned on the observed states of items it has already selected (i.e., conditioned on $\Phi \sim \psi$ ). That is, it will select $e$ to maximize the conditional expected marginal benefit $\Delta(e \mid \psi)$ as defined in Eq. (8). Pseudocode of the adaptive greedy algorithm is given in Algorithm 1. The only difference to the classic, non-adaptive greedy algorithm studied by Nemhauser et al. (1978), is Line 1, where an observation $\Phi\left(e^{<em>}\right)$ of the selected item $e^{</em>}$ is obtained. Note that the algorithms in this section are presented for Adaptive Stochastic Maximization. For the coverage objectives, we simply keep selecting items as prescribed by $\pi^{\text {greedy }}$ until achieving the quota on objective value (for the min-cost objective) or until we have selected every item (for the min-sum objective).</p>
<h3>4.1 Incorporating Item Costs</h3>
<p>The adaptive greedy algorithm can be naturally modified to handle non-uniform item costs by replacing its selection rule by</p>
<p>$$
e^{*} \in \underset{e}{\arg \max } \frac{\Delta(e \mid \psi)}{c(e)}
$$</p>
<p>In the following, we will focus on the uniform cost case $(c \equiv 1)$, and defer the analysis with costs to the Appendix.</p>
<h3>4.2 Approximate Greedy Selection</h3>
<p>In some applications, finding an item maximizing $\Delta(e \mid \psi)$ may be computationally intractable, and the best we can do is find an $\alpha$-approximation to the best greedy selection. This means we find an $e^{\prime}$ such that</p>
<p>$$
\Delta\left(e^{\prime} \mid \psi\right) \geq \frac{1}{\alpha} \max _{e} \Delta(e \mid \psi)
$$</p>
<p>We call a policy which always selects such an item an $\alpha$-approximate greedy policy.</p>
<h3>4.3 Robustness \&amp; Approximate Greedy Selection</h3>
<p>As we will show, $\alpha$-approximate greedy policies have performance guarantees on several problems. The fact that these performance guarantees of greedy policies are robust to approximate greedy selection suggests a particular robustness guarantee against incorrect priors $p(\phi)$. Specifically, if our incorrect prior $p^{\prime}$ is such that when we evaluate $\Delta(e \mid \psi)$ we err by a multiplicative factor of at most $\alpha$, then when we compute the greedy policy with respect to $p^{\prime}$ we are actually implementing an $\alpha$-approximate greedy policy (with respect to the true prior), and hence obtain the corresponding guarantees. For example, a sufficient condition for erring by at most a multiplicative factor of $\alpha$ is that there exists $c \leq 1$ and $d \geq 1$ with $\alpha=d / c$ such that $c p(\phi) \leq p^{\prime}(\phi) \leq d p(\phi)$ for all $\phi$, where $p$ is the true prior.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Input</span><span class="o">:</span><span class="w"> </span><span class="n">Budget</span><span class="w"> </span><span class="o">\(</span><span class="n">k</span><span class="o">\);</span><span class="w"> </span><span class="n">ground</span><span class="w"> </span><span class="kd">set</span><span class="w"> </span><span class="o">\(</span><span class="n">E</span><span class="o">\);</span><span class="w"> </span><span class="n">distribution</span><span class="w"> </span><span class="o">\(</span><span class="n">p</span><span class="o">(\</span><span class="n">phi</span><span class="o">)\);</span><span class="w"> </span><span class="kd">function</span><span class="w"> </span><span class="o">\(</span><span class="n">f</span><span class="o">\).</span>
<span class="n">Output</span><span class="o">:</span><span class="w"> </span><span class="n">Set</span><span class="w"> </span><span class="o">\(</span><span class="n">A</span><span class="w"> </span><span class="o">\</span><span class="n">subseteq</span><span class="w"> </span><span class="n">E</span><span class="o">\)</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">\(</span><span class="n">k</span><span class="o">\)</span>
<span class="n">begin</span>
<span class="w">    </span><span class="o">\(</span><span class="n">A</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="o">\</span><span class="n">emptyset</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="o">\</span><span class="n">psi</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="o">\</span><span class="n">emptyset</span><span class="w"> </span><span class="o">;\)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="o">\(</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="o">\)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="o">\(</span><span class="n">k</span><span class="o">\)</span><span class="w"> </span><span class="k">do</span>
<span class="w">        </span><span class="n">foreach</span><span class="w"> </span><span class="o">\(</span><span class="n">e</span><span class="w"> </span><span class="o">\</span><span class="k">in</span><span class="w"> </span><span class="n">E</span><span class="w"> </span><span class="o">\</span><span class="n">backslash</span><span class="w"> </span><span class="n">A</span><span class="o">\)</span><span class="w"> </span><span class="k">do</span><span class="w"> </span><span class="n">compute</span><span class="w"> </span><span class="o">\(\</span><span class="n">Delta</span><span class="o">(</span><span class="n">e</span><span class="w"> </span><span class="o">\</span><span class="n">mid</span><span class="w"> </span><span class="o">\</span><span class="n">psi</span><span class="o">)=\</span><span class="n">mathbb</span><span class="o">{</span><span class="n">E</span><span class="o">}[</span><span class="n">f</span><span class="o">(</span><span class="n">A</span><span class="w"> </span><span class="o">\</span><span class="n">cup</span><span class="o">\{</span><span class="n">e</span><span class="o">\},</span><span class="w"> </span><span class="o">\</span><span class="n">Phi</span><span class="o">)-</span><span class="n">f</span><span class="o">(</span><span class="n">A</span><span class="o">,</span><span class="w"> </span><span class="o">\</span><span class="n">Phi</span><span class="o">)</span><span class="w"> </span><span class="o">\</span><span class="n">mid</span><span class="w"> </span><span class="o">\</span><span class="n">Phi</span><span class="w"> </span><span class="o">\</span><span class="n">sim</span><span class="w"> </span><span class="o">\</span><span class="n">psi</span><span class="o">]\);</span>
<span class="w">        </span><span class="n">Select</span><span class="w"> </span><span class="o">\(</span><span class="n">e</span><span class="o">^{*}</span><span class="w"> </span><span class="o">\</span><span class="k">in</span><span class="w"> </span><span class="o">\</span><span class="n">arg</span><span class="w"> </span><span class="o">\</span><span class="n">max</span><span class="w"> </span><span class="n">_</span><span class="o">{</span><span class="n">e</span><span class="o">}</span><span class="w"> </span><span class="o">\</span><span class="n">Delta</span><span class="o">(</span><span class="n">e</span><span class="w"> </span><span class="o">\</span><span class="n">mid</span><span class="w"> </span><span class="o">\</span><span class="n">psi</span><span class="o">)\);</span>
<span class="w">        </span><span class="n">Set</span><span class="w"> </span><span class="o">\(</span><span class="n">A</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">\</span><span class="n">cup</span><span class="o">\</span><span class="n">left</span><span class="o">\{</span><span class="n">e</span><span class="o">^{*}\</span><span class="n">right</span><span class="o">\}\);</span>
<span class="w">        </span><span class="n">Observe</span><span class="w"> </span><span class="o">\(\</span><span class="n">Phi</span><span class="o">\</span><span class="n">left</span><span class="o">(</span><span class="n">e</span><span class="o">^{*}\</span><span class="n">right</span><span class="o">)</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="o">\</span><span class="n">operatorname</span><span class="o">{</span><span class="n">Set</span><span class="o">}</span><span class="w"> </span><span class="o">\</span><span class="n">psi</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="o">\</span><span class="n">psi</span><span class="w"> </span><span class="o">\</span><span class="n">cup</span><span class="o">\</span><span class="n">left</span><span class="o">\{\</span><span class="n">left</span><span class="o">(</span><span class="n">e</span><span class="o">^{*},</span><span class="w"> </span><span class="o">\</span><span class="n">Phi</span><span class="o">\</span><span class="n">left</span><span class="o">(</span><span class="n">e</span><span class="o">^{*}\</span><span class="n">right</span><span class="o">)\</span><span class="n">right</span><span class="o">)\</span><span class="n">right</span><span class="o">\}\);</span>
<span class="n">end</span>
</code></pre></div>

<p>Algorithm 1: The adaptive greedy algorithm, which implements the greedy policy.</p>
<h1>4.4 Lazy Evaluations and the Accelerated Adaptive Greedy Algorithm</h1>
<p>The definition of adaptive submodularity allows us to implement an "accelerated" version of the adaptive greedy algorithm using lazy evaluations of marginal benefits as originally suggested for the non-adaptive case by Minoux (1978). The idea is as follows. Suppose we run $\pi^{\text {greedy }}$ under some fixed realization $\phi$, and select items $e_{1}, e_{2}, \ldots, e_{k}$. Let $\psi_{i}:=\left{\left(e_{j}, \phi\left(e_{j}\right): j \leq i\right)\right}$ be the partial realizations observed during the run of $\pi^{\text {greedy }}$. The adaptive greedy algorithm computes $\Delta\left(e \mid \psi_{i}\right)$ for all $e \in E$ and $0 \leq i&lt;k$, unless $e \in \operatorname{dom}\left(\psi_{i}\right)$. Naively, the algorithm thus needs to compute $\Theta(|E| k)$ marginal benefits (which can be expensive to compute). The key insight is that $i \mapsto \Delta\left(e \mid \psi_{i}\right)$ is nonincreasing for all $e \in E$, because of the adaptive submodularity of the objective. Hence, if when deciding which item to select as $e_{i}$ we know $\Delta\left(e^{\prime} \mid \psi_{j}\right) \leq \Delta\left(e \mid \psi_{i}\right)$ for some items $e^{\prime}$ and $e$ and $j&lt;i$, then we may conclude $\Delta\left(e^{\prime} \mid \psi_{i}\right) \leq \Delta\left(e \mid \psi_{i}\right)$ and hence eliminate the need to compute $\Delta\left(e^{\prime} \mid \psi_{i}\right)$. The accelerated version of the adaptive greedy algorithm exploits this observation in a principled manner, by computing $\Delta(e \mid \psi)$ for items $e$ in decreasing order of the upper bounds known on them, until it finds an item whose value is at least as great as the upper bounds of all other items. Pseudocode of this version of the adaptive greedy algorithm is given in Algorithm 2.</p>
<p>In the non-adaptive setting, the use of lazy evaluations has been shown to significantly reduce running times in practice (Leskovec et al., 2007). We evaluated the naive and accelerated implementations of the adaptive greedy algorithm on two sensor selection problems, and obtained speedup factors that range from roughly 4 to 40 for those problems. See $\S 10$ on page 29 for details.</p>
<h2>5. Guarantees for the Greedy Policy</h2>
<p>In this section we show that if the objective function is adaptive submodular with respect to the probabilistic model of the environment in which we operate, then the greedy policy inherits precisely the performance guarantees of the greedy algorithm for classic (non-adaptive) submodular maximization and submodular coverage problems, such as Maximum $k$-Cover and Minimum Set Cover, as well as min-sum submodular coverage problems, such as Min-Sum Set Cover. In fact, we will show that this holds true more generally: $\alpha$-approximate greedy policies inherit precisely the performance guarantees of $\alpha$-approximate greedy algorithms for these classic problems. These guarantees suggest that adaptive submodularity is an appropriate generalization of submodularity to policies. In this section we focus on the unit cost case (i.e., every item has the same cost). In the Appendix we provide the proofs omitted in this section, and show how our results extend to non-uniform item costs if we greedily maximize the expected benefit/cost ratio.</p>
<h3>5.1 The Maximum Coverage Objective</h3>
<p>In this section we consider the maximum coverage objective, where the goal is to select $k$ items adaptively to maximize their expected value. The task of maximizing expected value subject to more complex constraints,</p>
<p>Input: Budget $k$; ground set $E$; distribution $p(\phi)$; function $f$.
Output: Set $A \subseteq E$ of size $k$
begin
$1 A \leftarrow \emptyset ; \psi \leftarrow \emptyset$; Priority Queue $Q \leftarrow$ EMPTY_QUEUE;
2 foreach $e \in E$ do $Q$. insert $(e,+\infty)$;
3 for $i=1$ to $k$ do
$4 \delta_{\max } \leftarrow-\infty ; e_{\max } \leftarrow$ NULL;
5 while $\delta_{\max }&lt;Q$. $\max$ Priority( ) do
$6 \quad e \leftarrow Q . \operatorname{pop}()$;
$7 \quad \delta \leftarrow \Delta(e \mid \psi)=\mathbb{E}[f(A \cup{e}, \Phi)-f(A, \Phi) \mid \Phi \sim \psi] ;$
$8 \quad Q$. insert $(e, \delta)$;
$9 \quad$ if $\delta_{\max }&lt;\delta$ then
$10 \quad \delta_{\max } \leftarrow \delta ; e_{\max } \leftarrow e ;$
$11 A \leftarrow A \cup\left{e_{\max }\right} ; Q$. remove $\left(e_{\max }\right)$;
12 Observe $\Phi\left(e_{\max }\right)$; Set $\psi \leftarrow \psi \cup\left{\left(e_{\max }, \Phi\left(e_{\max }\right)\right)\right}$;
end
Algorithm 2: The accelerated version of the adaptive greedy algorithm. Here, $Q$. insert $(e, \delta)$ inserts $e$ with priority $\delta, Q$. pop( ) removes and returns the item with greatest priority, $Q$. maxPriority( ) returns the maximum priority of the elements in $Q$, and $Q$. remove $(e)$ deletes $e$ from $Q$.
such as matroid constraints and intersections of matroid constraints, is considered in the work of Golovin and Krause (2011b). Before stating our result, we require the following definition.</p>
<p>Definition 4 (Policy Truncation) For a policy $\pi$, define the level- $k$-truncation $\pi_{[k]}$ of $\pi$ to be the policy obtained by running $\pi$ until it terminates or until it selects $k$ items, and then terminating. Formally, $\operatorname{dom}\left(\pi_{[k]}\right)={\psi \in \operatorname{dom}(\pi):|\psi|&lt;k}$, and $\pi_{[k]}(\psi)=\pi(\psi)$ for all $\psi \in \operatorname{dom}\left(\pi_{[k]}\right)$.</p>
<p>We have the following result, which generalizes the classic result of the work of Nemhauser et al. (1978) that the greedy algorithm achieves a $(1-1 / e)$-approximation to the problem of maximizing monotone submodular functions under a cardinality constraint. By setting $\ell=k$ and $\alpha=1$ in Theorem 5, we see that the greedy policy which selects $k$ items adaptively obtains at least $(1-1 / e)$ of the value of the optimal policy that selects $k$ items adaptively, measured with respect to $f_{\text {avg }}$. For a proof see Theorem 38 in Appendix 15.3, which generalizes Theorem 5 to nonuniform item costs.</p>
<p>Theorem 5 Fix any $\alpha \geq 1$. If $f$ is adaptive monotone and adaptive submodular with respect to the distribution $p(\phi)$, and $\pi$ is an $\alpha$-approximate greedy policy, then for all policies $\pi^{*}$ and positive integers $\ell$ and $k$,</p>
<p>$$
f_{\text {avg }}\left(\pi_{[\ell]}\right)&gt;\left(1-e^{-\ell / \alpha k}\right) f_{\text {avg }}\left(\pi_{[k]}^{*}\right)
$$</p>
<p>In particular, with $\ell=k$ this implies any $\alpha$-approximate greedy policy achieves a $\left(1-e^{-1 / \alpha}\right)$ approximation to the expected reward of the best policy, if both are terminated after running for an equal number of steps.</p>
<p>If the greedy rule can be implemented only with small absolute error rather than small relative error, i.e., $\Delta\left(e^{\prime} \mid \psi\right) \geq \max _{e} \Delta(e \mid \psi)-\varepsilon$, an argument similar to that used to prove Theorem 5 shows that</p>
<p>$$
f_{\text {avg }}\left(\pi_{[\ell]}\right) \geq\left(1-e^{-\ell / k}\right) f_{\text {avg }}\left(\pi_{[k]}^{*}\right)-\ell \varepsilon
$$</p>
<p>This is important, since small absolute error can always be achieved (with high probability) whenever $f$ can be evaluated efficiently, and sampling $p(\phi \mid \psi)$ is efficient. In this case, we can approximate</p>
<p>$$
\Delta(e \mid \psi) \approx \frac{1}{N} \sum_{i=1}^{N}\left[f\left(\operatorname{dom}(\psi) \cup{e}, \phi_{i}\right)-f\left(\operatorname{dom}(\psi), \phi_{i}\right)\right]
$$</p>
<p>where $\phi_{i}$ are sampled i.i.d. from $p(\phi \mid \psi)$.</p>
<h1>5.1.1 Data Dependent Bounds</h1>
<p>For the maximum coverage objective, adaptive submodular functions have another attractive feature: they allow us to obtain data dependent bounds on the optimum, in a manner similar to the bounds for the non-adaptive case (Minoux, 1978). Consider the non-adaptive problem of maximizing a monotone submodular function $f: 2^{A} \rightarrow \mathbb{R}_{\geq 0}$ subject to the constraint $|A| \leq k$. Let $A^{*}$ be an optimal solution, and fix any $A \subseteq E$. Then</p>
<p>$$
f\left(A^{*}\right) \leq f(A)+\max <em B="B" _in="\in" e="e">{B:|B| \leq k} \sum</em>(f(A \cup{e})-f(A))
$$</p>
<p>because setting $B=A^{<em>}$ we have $f\left(A^{</em>}\right) \leq f(A \cup B) \leq f(A)+\sum_{e \in B}(f(A \cup{e})-f(A))$. Note that unlike the original objective, we can easily compute $\max <em B="B" _in="\in" e="e">{B:|B| \leq k} \sum</em>\right)-f(A)$. In practice, such data-dependent bounds can be much tighter than the problem-independent performance guarantees of Nemhauser et al. (1978) for the greedy algorithm (Leskovec et al., 2007). Further note that these bounds hold for any set $A$, not just sets selected by the greedy algorithm.}(f(A \cup{e})-f(A))$ by computing $\delta(e):=f(A \cup{e})-f(A)$ for each $e$, and summing the $k$ largest values. Hence we can quickly compute an upper bound on our distance from the optimal value, $f\left(A^{*</p>
<p>These data dependent bounds have the following analogue for adaptive monotone submodular functions. See Appendix 15.2 for a proof.</p>
<p>Lemma 6 (The Adaptive Data Dependent Bound) Suppose we have made observations $\psi$ after selecting $\operatorname{dom}(\psi)$. Let $\pi^{<em>}$ be any policy such that $\left|E\left(\pi^{</em>}, \phi\right)\right| \leq k$ for all $\phi$. Then for adaptive monotone submodular $f$</p>
<p>$$
\Delta\left(\pi^{*} \mid \psi\right) \leq \max <em A="A" _in="\in" e="e">{A \subseteq E,|A| \leq k} \sum</em> \Delta(e \mid \psi)
$$</p>
<p>Thus, after running any policy $\pi$, we can efficiently compute a bound on the additional benefit that the optimal solution $\pi^{*}$ could obtain beyond the reward of $\pi$. We do that by computing the conditional expected marginal benefits for all elements $e$, and summing the $k$ largest of them. Note that these bounds can be computed on the fly when running the greedy algorithm, in a similar manner as discussed by Leskovec et al. (2007) for the non-adaptive setting.</p>
<h3>5.2 The Min Cost Cover Objective</h3>
<p>Another natural objective is to minimize the number of items selected while ensuring that a sufficient level of value is obtained. This leads to the Adaptive Stochastic Minimum Cost Coverage problem described in $\S 2$, namely $\pi^{*} \in \arg \min <em _avg="{avg" _text="\text">{\pi} c</em>[|E(\pi, \Phi)|]$. If the objective is adaptive monotone submodular, this is an adaptive version of the Minimum Submodular Cover problem (described on line (7) in $\S 3.1$ ). Recall that the greedy algorithm is known to give a $(\ln (Q)+1)$-approximation for Minimum Submodular Cover assuming the coverage function is integer-valued in addition to being monotone submodular (Wolsey, 1982). Adaptive Stochastic Minimum Cost Coverage is also related to the (Noisy) Interactive Submodular Set Cover problem studied by Guillory and Bilmes (2010, 2011), which considers the worst-case setting (i.e., there is no distribution over states; instead states are realized in an adversarial manner). Similar results for active learning have been proved by Kosaraju et al. (1999) and Dasgupta (2004), as we discuss in more detail in $\S 9$.}}(\pi)$ such that $f(E(\pi, \phi), \phi) \geq Q$ for all $\phi$. Recall that $c_{\text {avg }}(\pi)$ is the expected cost of $\pi$, which in the unit cost case equals the expected number of items selected by $\pi$, i.e., $c_{\text {avg }}(\pi):=$ $\mathbb{E</p>
<p>We assume throughout this section that there exists a quality threshold $Q$ such that $f(E, \phi)=Q$ for all $\phi$, and for all $S \subseteq E$ and all $\phi, f(S, \phi) \leq Q$. Note that, as discussed in Section 3, if we replace $f(S, \phi)$ by a new function $g(S, \phi)=\min \left(f(S, \phi), Q^{\prime}\right)$ for some constant $Q^{\prime}, g$ will be adaptive submodular if $f$ is. Thus, if $f(E, \phi)$ varies across realizations, we can instead use the greedy algorithm on the function truncated at some threshold $Q^{\prime} \leq \min _{\phi} f(E, \phi)$ achievable by all realizations.</p>
<p>In contrast to Adaptive Stochastic Maximization, for the coverage problem additional subtleties arise. In particular, it is not enough that a policy $\pi$ achieves value $Q$ for the true realization; in order for $\pi$ to terminate, it also requires a proof of this fact. Formally, we require that $\pi$ covers $f$ :</p>
<p>Definition 7 (Coverage) Let $\psi=\psi(\pi, \phi)$ be the partial realization encoding all states observed during the execution of $\pi$ under realization $\phi$. Given $f: 2^{E} \times O^{E} \rightarrow \mathbb{R}$, we say a policy $\pi$ covers $\phi$ with respect to $f$ if $f\left(\operatorname{dom}(\psi), \phi^{\prime}\right)=f\left(E, \phi^{\prime}\right)$ for all $\phi^{\prime} \sim \psi$. We say that $\pi$ covers $f$ if it covers every realization with respect to $f$.</p>
<p>Coverage is defined in such a way that upon terminating, $\pi$ might not know which realization is the true one, but has guaranteed that it has achieved the maximum reward in every possible case (i.e., for every realization consistent with its observations). We obtain results for both the average and worst-case cost objectives.</p>
<h1>5.2.1 Minimizing the Average Cost</h1>
<p>Before presenting our approximation guarantee for the Adaptive Stochastic Minimum Cost Coverage, we introduce a special class of instances, called self-certifying instances. We make this distinction because the greedy policy has stronger performance guarantees for self-certifying instances, and such instances arise naturally in applications. For example, the Stochastic Submodular Cover and Stochastic Set Cover instances in $\S 7$, the Adaptive Viral Marketing instances in $\S 8$, and the Pool-Based Active Learning instances in $\S 9$ are all self-certifying.</p>
<p>Definition 8 (Self-Certifying Instances) An instance of Adaptive Stochastic Minimum Cost Coverage is self-certifying if whenever a policy achieves the maximum possible value for the true realization it immediately has a proof of this fact. Formally, an instance $(f, p(\phi))$ is self-certifying if for all $\phi, \phi^{\prime}$, and $\psi$ such that $\phi \sim \psi$ and $\phi^{\prime} \sim \psi$, we have $f(\operatorname{dom}(\psi), \phi)=f(E, \phi)$ if and only if $f\left(\operatorname{dom}(\psi), \phi^{\prime}\right)=f\left(E, \phi^{\prime}\right)$.</p>
<p>One class of self-certifying instances which commonly arise are those in which $f(A, \phi)$ depends only on the state of items in $A$, and in which there is a uniform maximum amount of reward that can be obtained across realizations. Formally, we have the following observation.</p>
<p>Proposition 9 Fix an instance $(f, p(\phi))$. If there exists $Q$ such that $f(E, \phi)=Q$ for all $\phi$ and there exists some $g: 2^{E \times O} \rightarrow \mathbb{R}_{\geq 0}$ such that $f(A, \phi)=g({(e, \phi(e)): e \in A})$ for all $A$ and $\phi$, then $(f, p(\phi))$ is self-certifying.</p>
<p>Proof Fix $\phi, \phi^{\prime}$, and $\psi$ such that $\phi \sim \psi$ and $\phi^{\prime} \sim \psi$. Assuming the existence of $g$ and treating $\psi$ as a relation, we have $f(\operatorname{dom}(\psi), \phi)=g(\psi)=f\left(\operatorname{dom}(\psi), \phi^{\prime}\right)$. Hence $f(\operatorname{dom}(\psi), \phi)=Q=f(E, \phi)$ if and only if $f\left(\operatorname{dom}(\psi), \phi^{\prime}\right)=Q=f\left(E, \phi^{\prime}\right)$.</p>
<p>For our results on minimum cost coverage, we also need a stronger monotonicity condition and a stronger submodularity condition:</p>
<p>Definition 10 (Strong Adaptive Monotonicity) A function $f: 2^{E} \times O^{E} \rightarrow \mathbb{R}$ is strongly adaptive monotone with respect to $p(\phi)$ if, informally "selecting more items never hurts" with respect to the expected reward. Formally, for all $\psi$, all $e \notin \operatorname{dom}(\psi)$, and all possible outcomes $o \in O$ such that $\mathbb{P}[\Phi(e)=o \mid \Phi \sim \psi]&gt;0$, we require</p>
<p>$$
\mathbb{E}[f(\operatorname{dom}(\psi), \Phi) \mid \Phi \sim \psi] \leq \mathbb{E}[f(\operatorname{dom}(\psi) \cup{e}, \Phi) \mid \Phi \sim \psi, \Phi(e)=o]
$$</p>
<p>Strong adaptive monotonicity implies adaptive monotonicity, as the latter means that "selecting more items never hurts in expectation," i.e.,</p>
<p>$$
\mathbb{E}[f(\operatorname{dom}(\psi), \Phi) \mid \Phi \sim \psi] \leq \mathbb{E}[f(\operatorname{dom}(\psi) \cup{e}, \Phi) \mid \Phi \sim \psi]
$$</p>
<p>To define strong adaptive submodularity, we first need the following extension of $\Delta(e \mid \psi)$ :
Definition 11 (Conditional Expected Marginal Benefit (Extended version)) Given partial realizations $\psi \subseteq \psi^{\prime}$, let</p>
<p>$$
\Delta\left(e \mid \psi ; \psi^{\prime}\right):=\mathbb{E}\left[f(\operatorname{dom}(\psi) \cup{e}, \Phi)-f(\operatorname{dom}(\psi), \Phi) \mid \Phi \sim \psi^{\prime}\right]
$$</p>
<p>Definition 12 (Strong Adaptive Submodularity) A function $f: 2^{E} \times O^{E} \rightarrow \mathbb{R}_{\geq 0}$ is strongly adaptive submodular with respect to distribution $p(\phi)$ if it is adaptive submodular and moreover the expected marginal benefit of any fixed item does not increase as more items are selected and their states are observed, conditioned on the (item, observation) pairs. Formally, $f$ is adaptive submodular w.r.t. $p(\phi)$ if for all $\psi$ and $\psi^{\prime}$ such that $\psi$ is a subrealization of $\psi^{\prime}$ (i.e., $\psi \subseteq \psi^{\prime}$ ), and for all $e \in E \backslash \operatorname{dom}\left(\psi^{\prime}\right)$, we have</p>
<p>$$
\Delta\left(e \mid \psi ; \psi^{\prime}\right) \geq \Delta\left(e \mid \psi^{\prime}\right)
$$</p>
<p>In other words, conditioning on $\psi^{\prime}$, adding items $\operatorname{dom}\left(\psi^{\prime}\right) \backslash \operatorname{dom}(\psi)$ cannot increase the expected marginal benefit of $e$.</p>
<p>A sufficient condition for strong adaptive submodularity with respect to $p(\phi)$ is that the function be adaptive submodular and pointwise submodular (i.e., $f(A, \phi)$ is submodular in $A$ for any fixed $\phi$ ), as we prove in Appendix 15.4. It is worth noting that pointwise submodularity is not sufficient to establish adaptive submodularity. A simple counterexample is $f(S, \phi)=|{e: e \in S, \phi(e)=1}|$, with $p(\phi)=1 / 2$ if $\forall e \in$ $E, \phi(e)=1$ and $p(\phi)=1 / 2$ if $\forall e \in E, \phi(e)=0$. In that case, $\Delta(e \mid \emptyset)=1 / 2$ yet $\Delta\left(e \mid\left{\left(e^{\prime}, 1\right)\right}\right)=1$ for any $e^{\prime} \neq e$.</p>
<p>We now state our main result for the average case $\operatorname{cost} c_{\text {avg }}(\pi)$ :
Theorem 13 Suppose $f: 2^{E} \times O^{E} \rightarrow \mathbb{R}<em _phi="\phi">{\geq 0}$ is strongly adaptive submodular and strongly adaptive monotone with respect to $p(\phi)$ and there exists $Q$ such that $f(E, \phi)=Q$ for all $\phi$. Let $\eta$ be any value such that $f(S, \phi)&gt;Q-\eta$ implies $f(S, \phi)=Q$ for all $S$ and $\phi$. Let $\delta=\min </em>$ be an optimal policy minimizing the expected number of items selected to guarantee every realization is covered. Let $\pi$ be an $\alpha$-approximate greedy policy with respect to the item costs. Then in general} p(\phi)$ be the minimum probability of any realization. Let $\pi_{\text {avg }}^{*</p>
<p>$$
c_{\text {avg }}(\pi) \leq \alpha c_{\text {avg }}\left(\pi_{\text {avg }}^{*}\right)\left(\ln \left(\frac{Q}{\delta \eta}\right)+1\right)^{2}
$$</p>
<p>and for self-certifying instances</p>
<p>$$
c_{\text {avg }}(\pi) \leq \alpha c_{\text {avg }}\left(\pi_{\text {avg }}^{*}\right)\left(\ln \left(\frac{Q}{\eta}\right)+1\right)^{2}
$$</p>
<p>Note that if range $(f) \subset \mathbb{Z}$, then $\eta=1$ is a valid choice, so for general and self-certifying instances we have $c_{\text {avg }}(\pi) \leq \alpha c_{\text {avg }}\left(\pi_{\text {avg }}^{<em>}\right)\left(\ln (Q / \delta)+1\right)^{2}$ and $c_{\text {avg }}(\pi) \leq \alpha c_{\text {avg }}\left(\pi_{\text {avg }}^{</em>}\right)\left(\ln (Q)+1\right)^{2}$, respectively.
Historical Note: An earlier version of Theorem 13 claimed logarithmic approximation factors rather than the squared-logarithmic factors present here. Unfortunately, the proof was flawed as pointed out by Nan and Saligrama (2017). Determining whether the logarithmic bounds hold remains an interesting open problem. In particular, it remains open whether $c_{\text {avg }}(\pi) \leq \alpha c_{\text {avg }}\left(\pi_{\text {avg }}^{<em>}\right)\left(\ln \left(\frac{Q}{\delta \eta}\right)+1\right)$ for general instances and $c_{\text {avg }}(\pi) \leq \alpha c_{\text {avg }}\left(\pi_{\text {avg }}^{</em>}\right)\left(\ln \left(\frac{Q}{\eta}\right)+1\right)$ for self-certifying instances under the conditions specified by Theorem 13. It also remains open whether the strong adaptive submodularity condition is required.</p>
<h1>5.2.2 Minimizing the Worst-Case Cost</h1>
<p>For the worst-case $\operatorname{cost} c_{\text {wv }}(\pi):=\max <em 0="0" _geq="\geq">{\phi}|E(\pi, \phi)|$, strong adaptive monotonicity and strong submodularity are not required; adaptive monotonicity and adaptive submodularity suffice. We obtain the following result.
Theorem 14 Suppose $f: 2^{E} \times O^{E} \rightarrow \mathbb{R}</em>$ is adaptive monotone and adaptive submodular with respect to $p(\phi)$, and let $\eta$ be any value such that $f(S, \phi)&gt;f(E, \phi)-\eta$ implies $f(S, \phi)=f(E, \phi)$ for all $S$ and $\phi$. Let $\delta=\min <em c="c" w="w">{\phi} p(\phi)$ be the minimum probability of any realization. Let $\pi</em>[f(E, \phi)]$ be the maximum possible expected reward. Then}^{*}$ be the optimal policy minimizing the worst-case number of queries to guarantee every realization is covered. Let $\pi$ be an $\alpha$-approximate greedy policy. Finally, let $Q:=\mathbb{E</p>
<p>$$
c_{w c}(\pi) \leq \alpha c_{w c}\left(\pi_{w c}^{*}\right)\left(\ln \left(\frac{Q}{\delta \eta}\right)+1\right)
$$</p>
<p>The proofs of Theorems 13 and 14 are given in Appendix 15.4.
Thus, even though adaptive submodularity is defined w.r.t. a particular distribution, perhaps surprisingly, the adaptive greedy algorithm is competitive even in the case of adversarially chosen realizations, against a policy optimized to minimize the worst-case cost. Theorem 14 therefore suggests that if we do not have a strong prior, we can obtain the strongest guarantees if we choose a distribution that is "as uniform as possible" (i.e., maximizes $\delta$ ) while still guaranteeing adaptive submodularity.</p>
<h1>5.2.3 DISCUSSION</h1>
<p>Note that the approximation factor for self-certifying instances in Theorem 14 reduces to the $(\ln (Q)+1)$ approximation guarantee for the greedy algorithm for Set Cover instances with $Q$ elements, in the case of a deterministic distribution $p(\phi)$. Moreover, with a deterministic distribution $p(\phi)$ there is no distinction between average-case and worst-case cost. Hence, an immediate corollary of the result of Feige (1998) mentioned in $\S 3$ is that for every constant $\epsilon&gt;0$ there is no polynomial time $(1-\epsilon) \ln (Q / \eta)$ approximation algorithm for self-certifying instances of Adaptive Stochastic Min Cost Cover, under either the $c_{\text {avg }}(\cdot)$ or the $c_{\text {wc }}(\cdot)$ objective, unless NP $\subseteq$ DTIME $\left(n^{\mathcal{O}(\log \log n)}\right)$. It remains open to determine whether or not Adaptive Stochastic Min Cost Cover with the worst-case cost objective admits a $\ln (Q / \eta)+1$ approximation for self-certifying instances via a polynomial time algorithm, and in particular whether the greedy policy has such an approximation guarantee. However, in Lemma 50 we show that Feige's result also implies there is no $(1-\epsilon) \ln (Q / \delta \eta)$ polynomial time approximation algorithm for general (non self-certifying) instances of Adaptive Stochastic Min Cost Cover under either objective, unless NP $\subseteq$ DTIME $\left(n^{\mathcal{O}(\log \log n)}\right)$. In that sense, Theorem 14 is best-possible and Theorem 13 cannot be improved by more than a logarithmic factor and under reasonable complexity-theoretic assumptions.</p>
<h3>5.3 The Min-Sum Cover Objective</h3>
<p>Yet another natural objective is the min-sum objective, in which an unrealized reward of $x$ incurs a cost of $x$ in each time step, and the goal is to minimize the total cost incurred.</p>
<h3>5.3.1 BACKGROUND ON THE NON-ADAPTIVE MIN-SUM COVER PROBLEM</h3>
<p>In the non-adaptive setting, perhaps the simplest form of a coverage problem with this objective is the Min-Sum Set Cover problem (Feige et al., 2004) in which the input is a set system $(U, \mathcal{S})$, the output is a permutation of the sets $\left\langle S_{1}, S_{2}, \ldots, S_{m}\right\rangle$, and the goal is to minimize the sum of element coverage times, where the coverage time of $u$ is the index of the first set that contains it (e.g., it is $j$ if $u \in S_{j}$ and $u \notin S_{i}$ for all $i&lt;j$ ). In this problem and its generalizations the min-sum objective is useful in modeling processing costs in certain applications, for example in ordering diagnostic tests to identify a disease cheaply (Kaplan et al., 2005), in ordering multiple filters to be applied to database records while processing a query (Munagala et al., 2005), or in ordering multiple heuristics to run on boolean satisfiability instances as a means to solve them faster in practice (Streeter and Golovin, 2008). A particularly expressive generalization of min-sum set cover has been studied under the names Min-Sum Submodular Cover (Streeter and Golovin, 2008) and $L_{1}$-Submodular Set Cover (Golovin et al., 2008). The former paper extends the greedy algorithm to a natural online variant of the problem, while the latter studies a parameterized family of $L_{p}$-Submodular Set Cover problems in which the objective is analogous to minimizing the $L_{p}$ norm of the coverage times for Min-Sum Set Cover instances. In the Min-Sum Submodular Cover problem, there is a monotone submodular function $f: 2^{E} \rightarrow \mathbb{R}<em 1="1">{\geq 0}$ defining the reward obtained from a collection of elements ${ }^{5}$. There is an integral cost $c(e)$ for each element, and the output is a sequence of all of the elements $\sigma=\left\langle e</em>$, we define the set of}, e_{2}, \ldots, e_{n}\right\rangle$. For each $t \in \mathbb{R}_{\geq 0</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>elements in the sequence $\sigma$ within a budget of $t$ :</p>
<p>$$
\sigma_{[t]}:=\left{e_{i}: \sum_{j \leq i} c\left(e_{j}\right) \leq t\right}
$$</p>
<p>The cost we wish to minimize is then</p>
<p>$$
c_{\Sigma}(\sigma):=\sum_{t=0}^{\infty}\left(f(E)-f\left(\sigma_{[t]}\right)\right)
$$</p>
<p>Feige et al. (2004) proved that for Min-Sum Set cover, the greedy algorithm achieves a 4-approximation to the minimum cost, and also that this is optimal in the sense that no polynomial time algorithm can achieve a $(4-\epsilon)$-approximation, for any $\epsilon&gt;0$, unless $\mathrm{P}=\mathrm{NP}$. Interestingly, the greedy algorithm also achieves a 4-approximation for the more general Min-Sum Submodular Cover problem as well (Streeter and Golovin, 2008; Golovin et al., 2008).</p>
<h1>5.3.2 The Adaptive Stochastic Min-Sum Cover Problem</h1>
<p>In this article, we extend the result of Streeter and Golovin (2008) and Golovin et al. (2008) to an adaptive version of Min-Sum Submodular Cover. For clarity's sake we will consider the unit-cost case here (i.e., $c(e)=1$ for all $e$ ); we show how to extend adaptive submodularity to handle general costs in the Appendix. In the adaptive version of the problem, $\pi_{[t]}$ plays the role of $\sigma_{[t]}$, and $f_{\text {avg }}$ plays the role of $f$. The goal is to find a policy $\pi$ minimizing</p>
<p>$$
c_{\Sigma}(\pi):=\sum_{t=0}^{\infty}\left(\mathbb{E}[f(E, \Phi)]-f_{\text {avg }}\left(\pi_{[t]}\right)\right)=\sum_{\phi} p(\phi) \sum_{t=0}^{\infty}\left(f(E, \phi)-f\left(E\left(\pi_{[t]}, \phi\right), \phi\right)\right)
$$</p>
<p>We call this problem the Adaptive Stochastic Min-Sum Cover problem. The key difference between this objective and the minimum cost cover objective is that here, the cost at each step is only the fractional extent that we have not covered the true realization, whereas in the minimum cost cover objective we are charged in full in each step until we have completely covered the true realization (according to Definition 7). We prove the following result for the Adaptive Stochastic Min-Sum Cover problem with arbitrary item costs in Appendix 15.5.</p>
<p>Theorem 15 Fix any $\alpha \geq 1$. If $f$ is adaptive monotone and adaptive submodular with respect to the distribution $p(\phi), \pi$ is an $\alpha$-approximate greedy policy with respect to the item costs, and $\pi^{<em>}$ is any policy, then $c_{\Sigma}(\pi) \leq 4 \alpha c_{\Sigma}\left(\pi^{</em>}\right)$.</p>
<h2>6. Application: Stochastic Submodular Maximization</h2>
<p>As our first application, consider the sensor placement problem introduced in $\S 1$. Suppose we would like to monitor a spatial phenomenon such as temperature in a building. We discretize the environment into a set $E$ of locations. We would like to pick a subset $A \subseteq E$ of $k$ locations that is most "informative", where we use a set function $\hat{f}(A)$ to quantify the informativeness of placement $A$. Krause and Guestrin (2007) show that many natural objective functions (such as reduction in predictive uncertainty measured in terms of Shannon entropy with conditionally independent observations) are monotone submodular.</p>
<p>Now consider the problem, where the informativeness of a sensor is unknown before deployment (e.g., when deploying cameras for surveillance, the location of objects and their associated occlusions may not be known in advance, or varying amounts of noise may reduce the sensing range). We can model this extension by assigning a state $\phi(e) \in O$ to each possible location, indicating the extent to which a sensor placed at location $e$ is working. To quantify the value of a set of sensor deployments under a realization $\phi$ indicating to what</p>
<p>extent the various sensors are working, we first define $(e, o)$ for each $e \in E$ and $o \in O$, which represents the placement of a sensor at location $e$ which is in state $o$. We then suppose there is a function $\hat{f}: 2^{E \times O} \rightarrow \mathbb{R}_{\geq 0}$ which quantifies the informativeness of a set of sensor deployments in arbitrary states. (Note $\hat{f}$ is a set function taking a set of (sensor deployment, state) pairs as input.) The utility $f(A, \phi)$ of placing sensors at the locations in $A$ under realization $\phi$ is then</p>
<p>$$
f(A, \phi):=\hat{f}({(e, \phi(e)): e \in A})
$$</p>
<p>We aim to adaptively place $k$ sensors to maximize our expected utility. We assume that sensor failures at each location are independent of each other, i.e., $\mathbb{P}[\Phi=\phi]=\prod_{e \in E} \mathbb{P}[\Phi(e)=\phi(e)]$, where $\mathbb{P}[\phi(e)=o]$ is the probability that a sensor placed at location $e$ will be in state $o$. Asadpour et al. (2008) studied a special case of our problem, in which sensors either fail completely (in which case they contribute no value at all) or work perfectly, under the name Stochastic Submodular Maximization. They proved that the adaptive greedy algorithm obtains a $(1-1 / e)$ approximation to the optimal adaptive policy, provided $\hat{f}$ is monotone submodular. We extend their result to multiple types of failures by showing that $f(A, \phi)$ is adaptive submodular with respect to distribution $p(\phi)$ and then invoking Theorem 5. Fig. 2 illustrates an instance of Stochastic Submodular Maximization where $f(A, \phi)$ is the cardinality of union of sets index by $A$ and parameterized by $\phi$.</p>
<p>Theorem 16 Fix a prior such that $\mathbb{P}[\Phi=\phi]=\prod_{e \in E} \mathbb{P}[\Phi(e)=\phi(e)]$ and an integer $k$, and let the objective function $\hat{f}: 2^{E \times O} \rightarrow \mathbb{R}_{\geq 0}$ be monotone submodular. Let $\pi$ be any $\alpha$-approximate greedy policy attempting to maximize $f$, and let $\pi^{*}$ be any policy. Then for all positive integers $\ell$,</p>
<p>$$
f_{\text {avg }}\left(\pi_{[\ell]}\right) \geq\left(1-e^{-\ell / \alpha k}\right) f_{\text {avg }}\left(\pi_{[k]}^{*}\right)
$$</p>
<p>In particular, if $\pi$ is the greedy policy (i.e., $\alpha=1$ ) and $\ell=k$, then $f_{\text {avg }}\left(\pi_{[k]}\right) \geq\left(1-\frac{1}{e}\right) f_{\text {avg }}\left(\pi_{[k]}^{*}\right)$.
Proof We prove Theorem 16 by first proving $f$ is adaptive monotone and adaptive submodular in this model, and then applying Theorem 5. Adaptive monotonicity is readily proved after observing that $f(\cdot, \phi)$ is monotone for each $\phi$. Moving on to adaptive submodularity, fix any $\psi, \psi^{\prime}$ such that $\psi \subseteq \psi^{\prime}$ and any $e \notin \operatorname{dom}\left(\psi^{\prime}\right)$. We aim to show $\Delta\left(e \mid \psi^{\prime}\right) \leq \Delta(e \mid \psi)$. Intuitively, this is clear, as $\Delta\left(e \mid \psi^{\prime}\right)$ is the expected marginal benefit of adding $e$ to a larger base set than is the case with $\Delta(e \mid \psi)$, namely $\operatorname{dom}\left(\psi^{\prime}\right)$ as compared to $\operatorname{dom}(\psi)$, and the realizations are independent. To prove it rigorously, we define a coupled distribution $\mu$ over pairs of realizations $\phi \sim \psi$ and $\phi^{\prime} \sim \psi^{\prime}$ such that $\phi\left(e^{\prime}\right)=\phi^{\prime}\left(e^{\prime}\right)$ for all $e^{\prime} \notin \operatorname{dom}\left(\psi^{\prime}\right)$. Formally, $\mu\left(\phi, \phi^{\prime}\right)=\prod_{e \in E \backslash \operatorname{dom}(\psi)} \mathbb{P}[\Phi(e)=\phi(e)]$ if $\phi \sim \psi, \phi^{\prime} \sim \psi^{\prime}$, and $\phi\left(e^{\prime}\right)=\phi^{\prime}\left(e^{\prime}\right)$ for all $e^{\prime} \notin \operatorname{dom}\left(\psi^{\prime}\right)$; otherwise $\mu\left(\phi, \phi^{\prime}\right)=0$. (Note that $\mu\left(\phi, \phi^{\prime}\right)&gt;0$ implies $\phi\left(e^{\prime}\right)=\phi^{\prime}\left(e^{\prime}\right)$ for all $e^{\prime} \in \operatorname{dom}(\psi)$ as well, since $\phi \sim \psi, \phi^{\prime} \sim \psi^{\prime}$, and $\psi \subseteq \psi^{\prime}$.) Also note that $p(\phi \mid \psi)=\sum_{\phi^{\prime}} \mu\left(\phi, \phi^{\prime}\right)$ and $p\left(\phi^{\prime} \mid \psi^{\prime}\right)=\sum_{\phi} \mu\left(\phi, \phi^{\prime}\right)$. Calculating $\Delta\left(e \mid \psi^{\prime}\right)$ and $\Delta(e \mid \psi)$ using $\mu$, we see that for any $\left(\phi, \phi^{\prime}\right)$ in the support of $\mu$,</p>
<p>$$
\begin{aligned}
f\left(\operatorname{dom}\left(\psi^{\prime}\right) \cup{e}, \phi^{\prime}\right)-f\left(\operatorname{dom}\left(\psi^{\prime}\right), \phi^{\prime}\right) &amp; =\hat{f}\left(\psi^{\prime} \cup\left{\left(e, \phi^{\prime}(e)\right)\right}\right)-\hat{f}\left(\psi^{\prime}\right)) \
&amp; \leq \hat{f}(\psi \cup{(e, \phi(e))})-\hat{f}(\psi)) \
&amp; =f(\operatorname{dom}(\psi) \cup{e}, \phi)-f(\operatorname{dom}(\psi), \phi)
\end{aligned}
$$</p>
<p>from the submodularity of $\hat{f}$. Hence</p>
<p>$$
\begin{aligned}
\Delta\left(e \mid \psi^{\prime}\right) &amp; =\sum_{\left(\phi, \phi^{\prime}\right)} \mu\left(\phi, \phi^{\prime}\right)\left(f\left(\operatorname{dom}\left(\psi^{\prime}\right) \cup{e}, \phi^{\prime}\right)-f\left(\operatorname{dom}\left(\psi^{\prime}\right), \phi^{\prime}\right)\right) \
&amp; \leq \sum_{\left(\phi, \phi^{\prime}\right)} \mu\left(\phi, \phi^{\prime}\right)\left(f(\operatorname{dom}(\psi) \cup{e}, \phi)-f(\operatorname{dom}(\psi), \phi)\right) \quad=\Delta(e \mid \psi)
\end{aligned}
$$</p>
<p>which completes the proof.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of part of a Stochastic Set Cover instance. Shown are the supports of two distributions over sets, indexed by items $e$ (marked in blue) and $e^{\prime}$ (yellow).</p>
<h1>7. Application: Stochastic Submodular Coverage</h1>
<p>Suppose that instead of wishing to adaptively place $k$ unreliable sensors to maximize the utility of the information obtained, as discussed in $\S 6$, we have a quota on utility and wish to adaptively place the minimum number of unreliable sensors to achieve this quota. This amounts to a minimum-cost coverage version of the Stochastic Submodular Maximization problem introduced in $\S 6$, which we call Stochastic Submodular Coverage.</p>
<p>As in $\S 6$, in the Stochastic Submodular Coverage problem we suppose there is a function $\hat{f}: 2^{E \times O} \rightarrow \mathbb{R}<em E="E" _in="\in" e="e">{\geq 0}$ which quantifies the utility of a set of sensors in arbitrary states. Also, the states of each sensor are independent, so that $\mathbb{P}[\Phi=\phi]=\prod</em>[|E(\pi, \Phi)|]$. We additionally assume that this quota can always be obtained using sufficiently many sensor placements; formally, this amounts to $f(E, \phi)=Q$ for all $\phi$. We obtain the following result, whose proof we defer until the end of this section.} \mathbb{P}[\Phi(e)=\phi(e)]$. The goal is to obtain a quota $Q$ of utility at minimum cost. Thus, we define our objective as $f(A, \phi):=\min {Q, \hat{f}({(e, \phi(e)): e \in A})}$, and want to find a policy $\pi$ covering every realization and minimizing $c_{\text {avg }}(\pi):=\mathbb{E</p>
<p>Theorem 17 Fix a prior with independent sensor states s.t. $\mathbb{P}[\Phi=\phi]=\prod_{e \in E} \mathbb{P}[\Phi(e)=\phi(e)]$, and let $\hat{f}$ : $2^{E \times O} \rightarrow \mathbb{R}<em 0="0" _geq="\geq">{\geq 0}$ be a mon. submodular function. Fix $Q \in \mathbb{R}</em>$ be any policy. Then}$ s.t. $f(A, \phi):=\min (Q, \hat{f}({(e, \phi(e)): e \in A}))$ satisfies $f(E, \phi)=Q$ for all $\phi$. Let $\eta$ be any value such that $f(S, \phi)&gt;Q-\eta$ implies $f(S, \phi)=Q$ for all $S$ and $\phi$. Finally, let $\pi$ be an $\alpha$-approximate greedy policy for maximizing $f$, and let $\pi^{*</p>
<p>$$
c_{\text {avg }}(\pi) \leq \alpha c_{\text {avg }}\left(\pi^{*}\right)\left(\ln \left(\frac{Q}{\eta}\right)+1\right)^{2}
$$</p>
<h3>7.1 A Special Case: The Stochastic Set Coverage Problem</h3>
<p>The Stochastic Submodular Coverage problem is a generalization of the Stochastic Set Coverage problem (Goemans and Vondrák, 2006). In Stochastic Set Coverage the underlying submodular objective $\hat{f}$ is the number of elements covered in some input set system. In other words, there is a ground set $U$ of $n$ elements to be covered, and items $E$ such that each item $e$ is associated with a distribution over subsets of $U$. When an item is selected, a set is sampled from its distribution, as illustrated in Fig. 2. The problem is to adaptively select items until all elements of $U$ are covered by sampled sets, while minimizing the expected number of items selected. Like us, Goemans and Vondrák also assume that the subsets are sampled independently for each item, and every element of $U$ can be covered in every realization, so that $f(E, \phi)=|U|$ for all $\phi$.</p>
<p>Goemans and Vondrák primarily investigated the adaptivity gap (quantifying how much adaptive policies can outperform non-adaptive policies) of Stochastic Set Coverage, for variants in which items can be repeatedly</p>
<p>selected or not, and prove adaptivity gaps of $\Theta(\log n)$ in the former case, and between $\Omega(n)$ and $\mathcal{O}\left(n^{2}\right)$ in the latter. They also provide an $n$-approximation algorithm. More recently, Liu et al. (2008) considered a special case of Stochastic Set Coverage in which each item may be in one of two states. They were motivated by a streaming database problem, in which a collection of queries sharing common filters must all be evaluated on a stream element. They transform the problem to a Stochastic Set Coverage instance in which (filter, query) pairs are to be covered by filter evaluations; which pairs are covered by a filter depends on the (binary) outcome of evaluating it on the stream element. The resulting instances satisfy the assumption that every element of $U$ can be covered in every realization. They study, among other algorithms, the adaptive greedy algorithm specialized to this setting, and show that if the subsets are sampled independently for each item, so that $\mathbb{P}[\Phi=\phi]=\prod_{e} \mathbb{P}[\Phi(e)=\phi(e)]$, then it is an $\mathcal{H}<em e="1">{n}:=\sum</em> \leq \ln (n)+1$ for all $n \geq 1$.) Moreover, Liu et al. report that it empirically outperforms a number of other algorithms in their experiments.}^{n} \frac{1}{e}$ approximation. (Recall $\ln (n) \leq \mathcal{H}_{n</p>
<p>The adaptive submodularity framework allows us to prove approximate results for richer item distributions over subsets of $U$ than considered by Liu et al. (2008) as a corollary of Theorem 17. Specifically, we obtain a $(\ln (n)+1)^{2}$-approximation for the Stochastic Set Coverage problem with arbitrarily many outcomes for each stochastic set, where $n:=|U|$.</p>
<p>We model the Stochastic Set Coverage problem by letting $\phi(e) \subseteq U$ indicate the random set sampled from $e$ 's distribution. Since the sampled sets are independent we have $\mathbb{P}[\Phi=\phi]=\prod_{e} \mathbb{P}[\Phi(e)=\phi(e)]$. For any $A \subseteq E$ let $f(A, \phi):=\left|\cup_{e \in A} \phi(e)\right|$ be the number of elements of $U$ covered by the sets sampled from items in $A$. As in the previous work mentioned above, we assume $f(E, \phi)=n$ for all $\phi$. Therefore we may set $Q=n$. Since the range of $f$ includes only integers, we may set $\eta=1$. Applying Theorem 17 then yields the following result.</p>
<p>Corollary 18 The adaptive greedy algorithm achieves a $(\ln (n)+1)^{2}$-approximation for Stochastic Set Coverage, where $n:=|U|$ is the size of the ground set.</p>
<p>We now provide the proof of Theorem 17.
Proof of Theorem 17: We will ultimately prove Theorem 17 by applying the bound from Theorem 13 for Stochastic Submodular Cover instances.</p>
<p>The proof mostly consists of justifying this application. Without loss of generality we may assume $\hat{f}$ is truncated at $Q$, otherwise we may use $\hat{g}(S)=\min \left{Q, \hat{f}(S)\right}$ in lieu of $\hat{f}$. This removes the need to truncate $f$. Since we established the adaptive submodularity of $f$ in the proof of Theorem 16, and by assumption $f(E, \phi)=Q$ for all $\phi$, to apply Theorem 13 we need only show that $f$ is strongly adaptive monotone and strongly adaptive submodular and that the instances under consideration are self-certifying.</p>
<p>We begin by showing the strong adaptive monotonicity of $f$. Fix a partial realization $\psi$, an item $e \notin \operatorname{dom}(\psi)$ and a state $o$. Let $\psi^{\prime}=\psi \cup{(e, o)}$. Then treating $\psi$ and $\psi^{\prime}$ as subsets of $E \times O$, and using the monotonicity of $\hat{f}$, we obtain</p>
<p>$$
\mathbb{E}[f(\operatorname{dom}(\psi), \Phi) \mid \Phi \sim \psi]=\hat{f}(\psi) \leq \hat{f}\left(\psi^{\prime}\right) \leq \mathbb{E}\left[f\left(\operatorname{dom}\left(\psi^{\prime}\right), \Phi\right) \mid \Phi \sim \psi^{\prime}\right]
$$</p>
<p>which is equivalent to the strong adaptive monotonicity condition.
Next we show the strong adaptive adaptive submodularity of $f$ by showing it is pointwise submodular (having already proven adaptive submodularity for it). This is clearly true, since for all $\phi, S \mapsto$ $\hat{f}({(e, \phi(e)): e \in S})$ is monotone submodular by assumption.</p>
<p>Finally we prove that these instances are self-certifying. Consider any $\psi$ and $\phi, \phi^{\prime}$ consistent with $\psi$. Then</p>
<p>$$
f(\operatorname{dom}(\psi), \phi)=\hat{f}(\psi)=f\left(\operatorname{dom}(\psi), \phi^{\prime}\right)
$$</p>
<p>Since $f(E, \phi)=f\left(E, \phi^{\prime}\right)=Q$ by assumption, it follows that $f(\operatorname{dom}(\psi), \phi)=f(E, \phi)$ iff $f\left(\operatorname{dom}(\psi), \phi^{\prime}\right)=$ $f\left(E, \phi^{\prime}\right)$, so the instance is self-certifying.</p>
<p>We have shown that $f$ and $p(\phi)$ satisfy the assumptions of Theorem 13 on this self-certifying instance. Hence we may apply it to obtain the claimed approximation guarantee.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Illustration of the Adaptive Viral Marketing problem. Left: the underlying social network. Middle: the people influenced and the observations obtained after one person is selected.</p>
<h1>8. Application: Adaptive Viral Marketing</h1>
<p>For our next application, consider the following scenario. Suppose we would like to generate demand for a genuinely novel product. Potential customers do not realize how valuable the new product will be to them, and conventional advertisements are failing to convince them to try it. In this case, we may try to spur demand by offering a special promotional deal to a select few people, and hope that demand builds virally, propagating through the social network as people recommend the product to their friends and associates. Supposing we know something about the structure of the social networks people inhabit, and how ideas, innovation, and new product adoption diffuse through them, this begs the question: to which initial set of people should we offer the promotional deal, in order to spur maximum demand for our product?</p>
<p>This, broadly, is the viral marketing problem. The same problem arises in the context of spreading technological, cultural, and intellectual innovations, broadly construed. In the interest of unified terminology we follow Kempe et al. (2003) and talk of spreading influence through the social network, where we say people are active if they have adopted the idea or innovation in question, and inactive otherwise, and that $a$ influences $b$ if $a$ convinces $b$ to adopt the idea or innovation in question.</p>
<p>There are many ways to model the diffusion dynamics governing the spread of influence in a social network. We consider a basic and well-studied model, the independent cascade model, described in detail below. For this model Kempe et al. (2003) obtain a very interesting result; they show that the eventual spread of the influence $f$ (i.e., the ultimate number of customers that demand the product) is a monotone submodular function of the seed set $S$ of people initially selected. This, in conjunction with the results of Nemhauser et al. (1978) implies that the greedy algorithm obtains at least $\left(1-\frac{1}{e}\right)$ of the value of the best feasible seed set of size at most $k$, i.e., $\arg \max <em S="S">{S:|S| \leq k} f(S)$, where we interpret $k$ as the budget for the promotional campaign. Though Kempe et al. consider only the maximum coverage version of the viral marketing problem, their result in conjunction with that of Wolsey (1982) also implies that the greedy algorithm will obtain a quota $Q$ of value at a cost of at most $\ln (Q)+1$ times the cost of the optimal set $\arg \min </em>{c(S): f(S) \geq Q}$ if $f$ takes on only integral values.</p>
<h3>8.1 Adaptive Viral Marketing</h3>
<p>The viral marketing problem has a very natural adaptive analog. Instead of selecting a fixed set of people in advance, we may select a person to offer the promotion to, make some observations about the resulting spread of demand for our product, and repeat. See Fig. 3 for an illustration. In $\S 8.2$, we use the idea of</p>
<p>adaptive submodularity to obtain results analogous to those of Kempe et al. (2003) in the adaptive setting. Specifically, we show that the greedy policy obtains at least $\left(1-\frac{1}{c}\right)$ of the value of the best policy. Moreover, we extend this result by achieving that guarantee not only for the case where our reward is simply the number of influenced people, but also for any (nonnegative) monotone submodular function of the set of people influenced. In $\S 8.3$ we consider the minimum cost cover objective, and show that the greedy policy obtains a squared logarithmic approximation for it. To our knowledge, no approximation results for this adaptive variant of the viral marketing problem have been known.</p>
<h1>8.1.1 INDEPENDENT CASCADE MODEL</h1>
<p>In this model, the social network is a directed graph $G=(V, A)$ where each vertex in $V$ is a person, and each edge $(u, v) \in A$ has an associated binary random variable $X_{u v}$ indicating if $u$ will influence $v$. That is, $X_{u v}=1$ if $u$ will influence $v$ once it has been influenced, and $X_{u v}=0$ otherwise. The random variables $X_{u v}$ are independent, and have known means $p_{u v}:=\mathbb{E}\left[X_{u v}\right]$. We will call an edge $(u, v)$ with $X_{u v}=1$ a live edge and an edge with $X_{u v}=0$ a dead edge. When a node $u$ is activated, the edges $X_{u v}$ to each neighbor $v$ of $u$ are sampled, and $v$ is activated if $(u, v)$ is live. Influence can then spread from $u$ 's neighbors to their neighbors, and so on, according to the same process. Once active, nodes remain active throughout the process, however Kempe et al. (2003) show that this assumption is without loss of generality, and can be removed.</p>
<h3>8.1.2 The Feedback Model</h3>
<p>In the Adaptive Viral Marketing problem under the independent cascades model, the items correspond to people we can activate by offering them the promotional deal. How we define the states $\phi(u)$ depends on what information we obtain as a result of activating $u$. Given the nature of the diffusion process, activating $u$ can have wide-ranging effects, so the state $\phi(u)$ has more to do with the state of the social network on the whole than with $u$ in particular. Specifically, we model $\phi(u)$ as a function $\phi_{u}: A \rightarrow{0,1, ?}$, where $\phi_{u}((u, v))=0$ means that activating $u$ has revealed that $(u, v)$ is dead, $\phi_{u}((u, v))=1$ means that activating $u$ has revealed that $(u, v)$ is live, and $\phi_{u}((u, v))=$ ? means that activating $u$ has not revealed the status of $(u, v)$ (i.e., the value of $X_{u v}$ ). We require each realization to be consistent and complete. Consistency means that no edge should be declared both live and dead by any two states. That is, for all $u, v \in V$ and $a \in A,\left(\phi_{u}(a), \phi_{v}(a)\right) \notin{(0,1),(1,0)}$. Completeness means that the status of each edge is revealed by some activation. That is, for all $a \in A$ there exists $u \in V$ such that $\phi_{u}(a) \in{0,1}$. A consistent and complete realization thus encodes $X_{u v}$ for each edge $(u, v)$. Let $A(\phi)$ denote the live edges as encoded by $\phi$. There are several candidates for which edge sets we are allowed to observe when activating a node $u$. Here we consider what we call the Full-Adoption Feedback Model: After activating $u$ we get to see the status (live or dead) of all edges exiting $v$, for all nodes $v$ reachable from $u$ via live edges (i.e., reachable from $u$ in $(V, A(\phi))$, where $\phi$ is the true realization. We illustrate the full-adoption feedback model in Fig. 3.</p>
<h3>8.1.3 The Objective Function</h3>
<p>In the simplest case, the reward for influencing a set $U \subseteq V$ of nodes is $\hat{f}(U):=|U|$. Kempe et al. (2003) obtain an $\left(1-\frac{1}{c}\right)$-approximation for the slightly more general case in which each node $u$ has a weight $w_{u}$ indicating its importance, and the reward is $\hat{f}(U):=\sum_{u \in U} w_{u}$. We generalize this result further, to include arbitrary nonnegative monotone submodular reward functions $\hat{f}$. This allows us, for example, to encode a value associated with the diversity of the set of nodes influenced, such as the notion that it is better to achieve $20 \%$ market penetration in five different (equally important) demographic segments than $100 \%$ market penetration in one and $0 \%$ in the others.</p>
<h3>8.2 Guarantees for the Maximum Coverage Objective</h3>
<p>We are now ready to formally state our result for the maximum coverage objective.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ol>
<li>To encode Min-Sum Set Cover instance $(U, \mathcal{S})$, let $E:=\mathcal{S}$ and $f(A):=\left|\cup_{e \in A} e\right|$, where each $e \in E$ is a subset of elements in $U$.</li>
</ol>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
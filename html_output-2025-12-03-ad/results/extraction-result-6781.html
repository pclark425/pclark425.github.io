<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6781 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6781</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6781</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-130.html">extraction-schema-130</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <p><strong>Paper ID:</strong> paper-207f0c123e48b9e980f49e42cbc35711e14fea07</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/207f0c123e48b9e980f49e42cbc35711e14fea07" target="_blank">Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This novel approach significantly improves the multimodal mathematical reasoning capabilities of LLaVA-1.5, achieving a 19-point increase and comparable performance to GPT-4V on MathVista's minitest split, and yielding leading performance on Math-V and MathVerse.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have demonstrated impressive reasoning capabilities, particularly in textual mathematical problem-solving. However, existing open-source image instruction fine-tuning datasets, containing limited question-answer pairs per image, do not fully exploit visual information to enhance the multimodal mathematical reasoning capabilities of Multimodal LLMs (MLLMs). To bridge this gap, we address the lack of high-quality, diverse multimodal mathematical datasets by collecting 40K high-quality images with question-answer pairs from 24 existing datasets and synthesizing 320K new pairs, creating the MathV360K dataset, which enhances both the breadth and depth of multimodal mathematical questions. We introduce Math-LLaVA, a LLaVA-1.5-based model fine-tuned with MathV360K. This novel approach significantly improves the multimodal mathematical reasoning capabilities of LLaVA-1.5, achieving a 19-point increase and comparable performance to GPT-4V on MathVista's minitest split, and yielding leading performance on Math-V and MathVerse. Furthermore, Math-LLaVA demonstrates enhanced generalizability, showing substantial improvements on the MMMU benchmark. Our research highlights the importance of dataset diversity and synthesis in advancing MLLMs' mathematical reasoning abilities. The code and data are available at: \url{https://github.com/HZQ950419/Math-LLaVA}.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6781.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6781.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Math-LLaVA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Math-LLaVA (LLaVA-1.5 fine-tuned on MathV360K)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source multimodal large language model (MLLM) built by fine-tuning LLaVA-1.5 (Vicuna-v1.5 + ViT image encoder) on the MathV360K dataset to improve multimodal mathematical and geometry reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Math-LLaVA (fine-tuned LLaVA-1.5-13B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaVA-1.5 architecture: Vicuna-v1.5 decoder-only language model combined with a pretrained ViT image encoder; projection layer and language model weights fine-tuned on multimodal instruction data (MathV360K).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Geometry Problem Solving (GPS) / multimodal geometry diagram problems</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>spatial reasoning / geometry (diagram-based)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>MathVista (minitest; GPS subset), Math-V, MathVerse; training dataset MathV360K (40K images + 320K synthesized QA pairs) built from many geometry/diagram datasets (Geometry3K, GeoQA+, UniGeo, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Fine-tuned on a large multimodal instruction dataset (MathV360K) created with GPT-4V few-shot generation; evaluated zero-shot on benchmarks; dataset generation used few-shot GPT-4V demonstrations clustered by TF-IDF + K-means.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>No explicit symbolic/schematic reasoning algorithm reported; model learns multimodal spatial/mathematical reasoning via supervised instruction fine-tuning (no annotated chain-of-thought or formal symbolic solver used).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Image-text pairs: images encoded by ViT-Large-Patch16-224, resized to 336x336; visual features projected via a linear projection into the language model's input space; QA pairs are plain text paired with images.</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>During data creation and evaluation (not during Math-LLaVA inference), GPT-4V was used to annotate image clarity/complexity, to synthesize QA pairs (few-shot), and GPT-4 was used to extract model-predicted answers from free-form responses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Answer accuracy (exact-match against ground truth after extracting predicted choice/answer via GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>46.6% overall accuracy on MathVista minitest; 57.7% accuracy on the GPS (geometry) subset of MathVista; 15.7% overall on Math-V (reported in paper tables); also leading/open-source top performance on Math-V and MathVerse per reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Fine-tuning with the MathV360K dataset yields substantial gains on geometry/spatial tasks relative to base LLaVA-1.5; model shows improved robustness to underspecified and rephrased questions in case studies; case examples demonstrate Math-LLaVA solves more high-school/college-level geometry and counting problems than the base model.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Comparisons reported: base LLaVA-1.5-13B reproduced overall ~27.7% on MathVista -> Math-LLaVA 46.6% (+19 pts). Math-LLaVA-DS (fine-tuned only on selected 40K real data) achieved 38.2% overall; adding synthesized augmentations (AskImg, CompQ, RephQ, SimpQ) raised performance to 46.6%; individual augmentation gains reported (AskImg: 42.2%, CompQ: 39.8%, RephQ: 40.9%, SimpQ: 41.1% on MathVista).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Training data and annotations lack intermediate reasoning steps / chain-of-thought rationales (stated limitation); model can still underperform on certain modalities/subsets (e.g., VQA lower relative accuracy); no explicit symbolic/diagrammatic reasoning or provable guarantees; some domain gaps remain despite improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6781.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6781.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA-1.5-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA-1.5 (13B) base model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The open-source multimodal foundation used as the base model (Vicuna + ViT) on which Math-LLaVA was fine-tuned; serves as the baseline for multimodal math/geometry tasks in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA-1.5-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal model combining a pretrained Vision Transformer encoder with the Vicuna-v1.5 language model; used as the base for further instruction fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Geometry Problem Solving (GPS) and other multimodal math tasks</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>spatial reasoning / geometry (diagram-based)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Evaluated on MathVista minitest (GPS subset) and Math-V, MathVerse for baselines</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Base model inference evaluated zero-shot (image+question -> free-form answer); used as starting point for fine-tuning experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>No specialized symbolic pipeline; relies on learned multimodal representations.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Image-text pairs with ViT image encoding and tokenized text input to Vicuna</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reproduced overall ~27.7% on MathVista minitest; GPS subset ~22.7% (per reproduced table entries).</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Base LLaVA-1.5 performs poorly on multimodal mathematical/geometry problems compared to Math-LLaVA and stronger closed-source models; demonstrates room for improvement via targeted data augmentation and fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Used as baseline in fine-tuning studies; Math-LLaVA shows large gains over this base after MathV360K fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Limited multimodal mathematical reasoning out-of-the-box; struggles on numerical computations from diagrams, counting, and geometry tasks per case studies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6781.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6781.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4V (GPT-4 Vision Preview)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's vision-enabled GPT-4 variant used in this work for image annotation, QA synthesis (few-shot), and for extracting predicted answers from model outputs during evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT-4V (OpenAI) ?(system card referenced in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4V (vision-capable GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multimodal transformer (GPT-4 with vision); used as an external tool to annotate images, cluster/generate question formats, and synthesize large numbers of image-question-answer pairs via few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Used to generate/annotate multimodal math/geometry QA (images from GPS/Math tasks, e.g., diagrams, charts, CLEVR-style scenes)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>spatial reasoning / geometry and general multimodal math question generation</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Used to create MathV360K (synthesized QA) and to annotate 10K images' clarity/complexity; also used to extract model answers during evaluation on MathVista</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Few-shot demonstrations constructed by clustering existing question formats (TF-IDF + K-means) and sampling references; used explicit prompt templates for image review and QA generation (figures 2,3,5). Zero-shot generation was tested and found to produce lower-quality, one-sided questions.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>LLM-based reasoning; no external symbolic solver reported; used its multimodal understanding to generate varied QA and to classify images along clarity/comprehension axes.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Processes image + text inputs directly (vision+language), used for annotation and text generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Served as the data-generation and annotation engine: annotated image clarity and complexity for training ViT classifiers; generated ~200K new QA by mining images with few-shot demonstrations; generated complex/rephrased/simplified versions of existing questions (AskImg, CompQ, RephQ, SimpQ). Also used (GPT-4 text) to parse and extract predicted answers from MLLM responses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy when evaluated on MathVista (comparison table); quality of generated QA evaluated indirectly by downstream model improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported in paper comparisons: GPT-4V achieves 49.9% overall accuracy on MathVista minitest and 50.5% on GPS subset (table entries).</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Few-shot prompting with clustered demonstrations produces more diverse and task-appropriate QA than zero-shot generation; GPT-4V enabled large-scale augmentation that substantially improved downstream fine-tuned model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Paper reports that zero-shot QA generation via GPT-4V tended to focus on one-sided scene descriptions lacking reasoning, whereas few-shot demonstrations (constructed from clusters) generated richer, reasoning-oriented questions leading to better downstream performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper notes that even strong multimodal LLMs can produce low-diversity or shallow questions in zero-shot; GPT-4V was used only for data generation/annotation and not as the deployed inference model for the reported open-source system.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6781.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6781.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>G-LLaVA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>G-LLaVA (geometry-focused LLaVA variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A geometry-specialized multimodal model referenced in the paper that was trained on a large curated set of geometric image-caption and QA pairs to improve diagram/math problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>G-llava: Solving geometric problem with multi-modal large language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>G-LLaVA (reported as G-LLaVA-13B in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A LLaVA-derived model variant specifically trained on large-scale geometry-focused image-caption and question-answer data (paper reports ~170K geometric pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B (reported variant in table: G-LLaVA-13B)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Geometry Problem Solving (GPS) / diagram geometry</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>spatial reasoning / geometry (diagram-based)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Trained on ~170K high-quality geometric image-caption and QA pairs (as reported in cited work); evaluated on geometry benchmarks (GPS subset of MathVista referenced for comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Described in cited paper (related work) as specialized fine-tuning on geometry data (not used directly in experiments of current paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>Reported to rely on large-scale supervised fine-tuning on geometry-specific multimodal data rather than added symbolic solvers (per referenced description in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Image-text multimodal representation (ViT + language model) per related-work description; exact encoding details not given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy on GPS/geometry benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Referenced GPS performance ~56.7% (table entry) — Math-LLaVA reports 57.7% and is described as outperforming G-LLaVA-13B on the GPS subset in the paper's comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Cited as a competitive geometry-specialized MLLM; used as a point of comparison to show Math-LLaVA's improvements due to cross-dataset selection and synthetic augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Not evaluated directly in this paper; referenced performance used for cross-model comparison only.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Details of failure modes not discussed in this paper beyond comparative accuracy figures; full methodology and limits are in the cited G-LLaVA work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>G-llava: Solving geometric problem with multi-modal large language model <em>(Rating: 2)</em></li>
                <li>Mathvista: Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models <em>(Rating: 2)</em></li>
                <li>Visual instruction tuning <em>(Rating: 1)</em></li>
                <li>Clevr-math: A dataset for compositional language, visual and mathematical reasoning <em>(Rating: 2)</em></li>
                <li>GPT-4V (OpenAI) <em>(Rating: 1)</em></li>
                <li>Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6781",
    "paper_id": "paper-207f0c123e48b9e980f49e42cbc35711e14fea07",
    "extraction_schema_id": "extraction-schema-130",
    "extracted_data": [
        {
            "name_short": "Math-LLaVA",
            "name_full": "Math-LLaVA (LLaVA-1.5 fine-tuned on MathV360K)",
            "brief_description": "An open-source multimodal large language model (MLLM) built by fine-tuning LLaVA-1.5 (Vicuna-v1.5 + ViT image encoder) on the MathV360K dataset to improve multimodal mathematical and geometry reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Math-LLaVA (fine-tuned LLaVA-1.5-13B)",
            "model_description": "LLaVA-1.5 architecture: Vicuna-v1.5 decoder-only language model combined with a pretrained ViT image encoder; projection layer and language model weights fine-tuned on multimodal instruction data (MathV360K).",
            "model_size": "13B",
            "puzzle_name": "Geometry Problem Solving (GPS) / multimodal geometry diagram problems",
            "puzzle_type": "spatial reasoning / geometry (diagram-based)",
            "dataset_name": "MathVista (minitest; GPS subset), Math-V, MathVerse; training dataset MathV360K (40K images + 320K synthesized QA pairs) built from many geometry/diagram datasets (Geometry3K, GeoQA+, UniGeo, etc.)",
            "prompting_method": "Fine-tuned on a large multimodal instruction dataset (MathV360K) created with GPT-4V few-shot generation; evaluated zero-shot on benchmarks; dataset generation used few-shot GPT-4V demonstrations clustered by TF-IDF + K-means.",
            "reasoning_technique": "No explicit symbolic/schematic reasoning algorithm reported; model learns multimodal spatial/mathematical reasoning via supervised instruction fine-tuning (no annotated chain-of-thought or formal symbolic solver used).",
            "internal_representation": "Image-text pairs: images encoded by ViT-Large-Patch16-224, resized to 336x336; visual features projected via a linear projection into the language model's input space; QA pairs are plain text paired with images.",
            "use_of_external_tool": false,
            "external_tool_description": "During data creation and evaluation (not during Math-LLaVA inference), GPT-4V was used to annotate image clarity/complexity, to synthesize QA pairs (few-shot), and GPT-4 was used to extract model-predicted answers from free-form responses.",
            "evaluation_metric": "Answer accuracy (exact-match against ground truth after extracting predicted choice/answer via GPT-4)",
            "performance": "46.6% overall accuracy on MathVista minitest; 57.7% accuracy on the GPS (geometry) subset of MathVista; 15.7% overall on Math-V (reported in paper tables); also leading/open-source top performance on Math-V and MathVerse per reported comparisons.",
            "analysis_findings": "Fine-tuning with the MathV360K dataset yields substantial gains on geometry/spatial tasks relative to base LLaVA-1.5; model shows improved robustness to underspecified and rephrased questions in case studies; case examples demonstrate Math-LLaVA solves more high-school/college-level geometry and counting problems than the base model.",
            "ablation_comparison": "Comparisons reported: base LLaVA-1.5-13B reproduced overall ~27.7% on MathVista -&gt; Math-LLaVA 46.6% (+19 pts). Math-LLaVA-DS (fine-tuned only on selected 40K real data) achieved 38.2% overall; adding synthesized augmentations (AskImg, CompQ, RephQ, SimpQ) raised performance to 46.6%; individual augmentation gains reported (AskImg: 42.2%, CompQ: 39.8%, RephQ: 40.9%, SimpQ: 41.1% on MathVista).",
            "limitations": "Training data and annotations lack intermediate reasoning steps / chain-of-thought rationales (stated limitation); model can still underperform on certain modalities/subsets (e.g., VQA lower relative accuracy); no explicit symbolic/diagrammatic reasoning or provable guarantees; some domain gaps remain despite improvements.",
            "uuid": "e6781.0",
            "source_info": {
                "paper_title": "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLaVA-1.5-13B",
            "name_full": "LLaVA-1.5 (13B) base model",
            "brief_description": "The open-source multimodal foundation used as the base model (Vicuna + ViT) on which Math-LLaVA was fine-tuned; serves as the baseline for multimodal math/geometry tasks in the paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaVA-1.5-13B",
            "model_description": "Multimodal model combining a pretrained Vision Transformer encoder with the Vicuna-v1.5 language model; used as the base for further instruction fine-tuning.",
            "model_size": "13B",
            "puzzle_name": "Geometry Problem Solving (GPS) and other multimodal math tasks",
            "puzzle_type": "spatial reasoning / geometry (diagram-based)",
            "dataset_name": "Evaluated on MathVista minitest (GPS subset) and Math-V, MathVerse for baselines",
            "prompting_method": "Base model inference evaluated zero-shot (image+question -&gt; free-form answer); used as starting point for fine-tuning experiments.",
            "reasoning_technique": "No specialized symbolic pipeline; relies on learned multimodal representations.",
            "internal_representation": "Image-text pairs with ViT image encoding and tokenized text input to Vicuna",
            "use_of_external_tool": false,
            "external_tool_description": null,
            "evaluation_metric": "Accuracy",
            "performance": "Reproduced overall ~27.7% on MathVista minitest; GPS subset ~22.7% (per reproduced table entries).",
            "analysis_findings": "Base LLaVA-1.5 performs poorly on multimodal mathematical/geometry problems compared to Math-LLaVA and stronger closed-source models; demonstrates room for improvement via targeted data augmentation and fine-tuning.",
            "ablation_comparison": "Used as baseline in fine-tuning studies; Math-LLaVA shows large gains over this base after MathV360K fine-tuning.",
            "limitations": "Limited multimodal mathematical reasoning out-of-the-box; struggles on numerical computations from diagrams, counting, and geometry tasks per case studies.",
            "uuid": "e6781.1",
            "source_info": {
                "paper_title": "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4V",
            "name_full": "GPT-4V (GPT-4 Vision Preview)",
            "brief_description": "OpenAI's vision-enabled GPT-4 variant used in this work for image annotation, QA synthesis (few-shot), and for extracting predicted answers from model outputs during evaluation.",
            "citation_title": "GPT-4V (OpenAI) ?(system card referenced in paper)",
            "mention_or_use": "use",
            "model_name": "GPT-4V (vision-capable GPT-4)",
            "model_description": "A multimodal transformer (GPT-4 with vision); used as an external tool to annotate images, cluster/generate question formats, and synthesize large numbers of image-question-answer pairs via few-shot prompting.",
            "model_size": null,
            "puzzle_name": "Used to generate/annotate multimodal math/geometry QA (images from GPS/Math tasks, e.g., diagrams, charts, CLEVR-style scenes)",
            "puzzle_type": "spatial reasoning / geometry and general multimodal math question generation",
            "dataset_name": "Used to create MathV360K (synthesized QA) and to annotate 10K images' clarity/complexity; also used to extract model answers during evaluation on MathVista",
            "prompting_method": "Few-shot demonstrations constructed by clustering existing question formats (TF-IDF + K-means) and sampling references; used explicit prompt templates for image review and QA generation (figures 2,3,5). Zero-shot generation was tested and found to produce lower-quality, one-sided questions.",
            "reasoning_technique": "LLM-based reasoning; no external symbolic solver reported; used its multimodal understanding to generate varied QA and to classify images along clarity/comprehension axes.",
            "internal_representation": "Processes image + text inputs directly (vision+language), used for annotation and text generation tasks.",
            "use_of_external_tool": true,
            "external_tool_description": "Served as the data-generation and annotation engine: annotated image clarity and complexity for training ViT classifiers; generated ~200K new QA by mining images with few-shot demonstrations; generated complex/rephrased/simplified versions of existing questions (AskImg, CompQ, RephQ, SimpQ). Also used (GPT-4 text) to parse and extract predicted answers from MLLM responses.",
            "evaluation_metric": "Accuracy when evaluated on MathVista (comparison table); quality of generated QA evaluated indirectly by downstream model improvements.",
            "performance": "Reported in paper comparisons: GPT-4V achieves 49.9% overall accuracy on MathVista minitest and 50.5% on GPS subset (table entries).",
            "analysis_findings": "Few-shot prompting with clustered demonstrations produces more diverse and task-appropriate QA than zero-shot generation; GPT-4V enabled large-scale augmentation that substantially improved downstream fine-tuned model performance.",
            "ablation_comparison": "Paper reports that zero-shot QA generation via GPT-4V tended to focus on one-sided scene descriptions lacking reasoning, whereas few-shot demonstrations (constructed from clusters) generated richer, reasoning-oriented questions leading to better downstream performance.",
            "limitations": "Paper notes that even strong multimodal LLMs can produce low-diversity or shallow questions in zero-shot; GPT-4V was used only for data generation/annotation and not as the deployed inference model for the reported open-source system.",
            "uuid": "e6781.2",
            "source_info": {
                "paper_title": "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "G-LLaVA",
            "name_full": "G-LLaVA (geometry-focused LLaVA variant)",
            "brief_description": "A geometry-specialized multimodal model referenced in the paper that was trained on a large curated set of geometric image-caption and QA pairs to improve diagram/math problem solving.",
            "citation_title": "G-llava: Solving geometric problem with multi-modal large language model",
            "mention_or_use": "mention",
            "model_name": "G-LLaVA (reported as G-LLaVA-13B in related work)",
            "model_description": "A LLaVA-derived model variant specifically trained on large-scale geometry-focused image-caption and question-answer data (paper reports ~170K geometric pairs).",
            "model_size": "13B (reported variant in table: G-LLaVA-13B)",
            "puzzle_name": "Geometry Problem Solving (GPS) / diagram geometry",
            "puzzle_type": "spatial reasoning / geometry (diagram-based)",
            "dataset_name": "Trained on ~170K high-quality geometric image-caption and QA pairs (as reported in cited work); evaluated on geometry benchmarks (GPS subset of MathVista referenced for comparison).",
            "prompting_method": "Described in cited paper (related work) as specialized fine-tuning on geometry data (not used directly in experiments of current paper).",
            "reasoning_technique": "Reported to rely on large-scale supervised fine-tuning on geometry-specific multimodal data rather than added symbolic solvers (per referenced description in this paper).",
            "internal_representation": "Image-text multimodal representation (ViT + language model) per related-work description; exact encoding details not given in this paper.",
            "use_of_external_tool": null,
            "external_tool_description": null,
            "evaluation_metric": "Accuracy on GPS/geometry benchmarks",
            "performance": "Referenced GPS performance ~56.7% (table entry) — Math-LLaVA reports 57.7% and is described as outperforming G-LLaVA-13B on the GPS subset in the paper's comparison.",
            "analysis_findings": "Cited as a competitive geometry-specialized MLLM; used as a point of comparison to show Math-LLaVA's improvements due to cross-dataset selection and synthetic augmentation.",
            "ablation_comparison": "Not evaluated directly in this paper; referenced performance used for cross-model comparison only.",
            "limitations": "Details of failure modes not discussed in this paper beyond comparative accuracy figures; full methodology and limits are in the cited G-LLaVA work.",
            "uuid": "e6781.3",
            "source_info": {
                "paper_title": "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "G-llava: Solving geometric problem with multi-modal large language model",
            "rating": 2
        },
        {
            "paper_title": "Mathvista: Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models",
            "rating": 2
        },
        {
            "paper_title": "Visual instruction tuning",
            "rating": 1
        },
        {
            "paper_title": "Clevr-math: A dataset for compositional language, visual and mathematical reasoning",
            "rating": 2
        },
        {
            "paper_title": "GPT-4V (OpenAI)",
            "rating": 1
        },
        {
            "paper_title": "Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?",
            "rating": 2
        }
    ],
    "cost": 0.0171465,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models</h1>
<p>Wenhao Shi ${ }^{1 <em>}$, Zhiqiang $\mathrm{Hu}^{2 </em>}$, Yi Bin ${ }^{3,4 *}$, Junhua Liu ${ }^{2}$, Yang Yang ${ }^{1}$<br>See-Kiong $\mathbf{N g}^{4}$, Lidong Bing, Roy Ka-Wei Lee ${ }^{2}$<br>${ }^{1}$ University of Electronic Science and Technology of China<br>${ }^{2}$ Singapore University of Technology and Design<br>${ }^{3}$ Tongji University ${ }^{4}$ National University of Singapore</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have demonstrated impressive reasoning capabilities, particularly in textual mathematical problemsolving. However, existing open-source image instruction fine-tuning datasets, containing limited question-answer pairs per image, do not fully exploit visual information to enhance the multimodal mathematical reasoning capabilities of Multimodal LLMs (MLLMs). To bridge this gap, we address the lack of high-quality, diverse multimodal mathematical datasets by collecting 40K high-quality images with question-answer pairs from 24 existing datasets and synthesizing 320K new pairs, creating the MathV360K dataset, which enhances both the breadth and depth of multimodal mathematical questions. We introduce MathLLaVA, a LLaVA-1.5-based model fine-tuned with MathV360K. This novel approach significantly improves the multimodal mathematical reasoning capabilities of LLaVA-1.5, achieving a 19-point increase and comparable performance to GPT-4V on MathVista's minitest split, and yielding leading performance on Math-V and MathVerse. Furthermore, Math-LLaVA demonstrates enhanced generalizability, showing substantial improvements on the MMMU benchmark. Our research highlights the importance of dataset diversity and synthesis in advancing MLLMs' mathematical reasoning abilities. The code and data are available at: https: //github.com/HZQ950419/Math-LLaVA.</p>
<h2>1 Introduction</h2>
<p>Motivation. Large language models (LLMs) exhibit impressive reasoning capabilities, drawing significant research interest in mathematical problemsolving in textual form (Zhang et al., 2020; Wei et al., 2022; Wang et al., 2023; Bin et al., 2023; Luo et al., 2023; Yue et al., 2023b; Gou et al., 2023;</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Zhou et al., 2023). However, the task of multimodal mathematical reasoning (Lu et al., 2023) requires models to interpret diverse images and apply advanced reasoning skills. While open-source multimodal large language models (MLLMs) like LLaVA (Liu et al., 2023) and Mini-GPT4 (Zhu et al., 2023) perform well on VQA tasks (Guo et al., 2023), they fall short of proprietary MLLMs (OpenAI; Google) in solving complex mathematical problems involving visual content.</p>
<p>Two common approaches to enhance MLLMs' mathematical reasoning skills are prompt methods and fine-tuning methods. Prompt methods ( Lu et al., 2023; Wang et al., 2024c) leverage MLLMs' latent abilities through carefully designed prompts, while fine-tuning methods (Wang et al., 2024b; Hu et al., 2023a; Zheng et al., 2023) adjust model parameters using reasoning data collected from realworld or synthetic data from advanced LLMs (e.g., GPT-4). However, existing open-source image instruction fine-tuning datasets (Lu et al., 2022b; Li et al., 2023; Lu et al., 2022a), which contain limited question-answer pairs per image, do not fully exploit visual information to enhance MLLMs' multimodal mathematical reasoning capabilities.</p>
<p>Research Objectives. To bridge this gap, we select 40K high-quality images with corresponding question-answer pairs from 24 pre-existing datasets. These images and queries span various subjects, including algebra, arithmetic, geometry, logic, numeric commonsense, science, and visual question answering. The selection criteria were based on image clarity and comprehension complexity. Additionally, we propose a pipeline to synthesize 320K new pairs based on the 40 K images and seed inquiries.</p>
<p>Constructing such a dataset presents significant challenges, including selecting diverse and highquality multimodal question-answer data and enhancing question diversity. Selecting suitable data involves filtering for image clarity and comprehen-</p>
<p>sion complexity, ensuring the dataset covers a wide range of mathematical concepts and question types. Enhancing question diversity requires synthesizing new questions that probe different aspects of the images and involve multiple reasoning steps. To further improve model robustness and comprehension, we focus on enhancing logical consistency (Tascon-Morales et al., 2023) and the ability to understand underspecified language (Pezzelle, 2023).</p>
<p>Contributions. Using the selected 40K data, the fine-tuned LLaVA-1.5 model, named Math-LLaVA-DS, achieved a significant improvement of 10.6\% on MathVista (Lu et al., 2023). To further enhance multimodal mathematical reasoning capabilities, we synthesized an additional 320K question-answer pairs based on the 40K images and seed questions, resulting in the MathV360K dataset. This comprehensive dataset, containing around 40K images and 360K question-answer pairs, significantly expands the coverage of multimodal mathematical reasoning. Fine-tuning LLaVA-1.5 with MathV360K, we developed Math-LLaVA, which outperforms the original LLaVA-1.5 by 19\% on MathVista's minitest split and achieves leading performance on Math-V (Wang et al., 2024a) and MathVerse (Zhang et al., 2024). We also evaluated Math-LLaVA on MMMU (Yue et al., 2023a), demonstrating its improved generalizability.</p>
<h2>2 Related Works</h2>
<h3>2.1 Multimodal Large Language Models</h3>
<p>The advancement of LLMs has spurred significant research interest in vision-language interaction, particularly in integrating visual knowledge into LLMs. The CLIP series (Radford et al., 2021; Li et al., 2022) aligned visual and language modalities using contrastive learning on extensive imagetext pairs. Recent studies increasingly use pretraining alignment and visual instruction tuning on LLMs for complex tasks like visual question answering, artwork analysis, and multimodal reasoning (Li et al., 2024a; Bin et al., 2024). MiniGPT-4 (Zhu et al., 2023) engages in image-text dialogues by aligning visual features with text. Similarly, models like LLaVA (Liu et al., 2023) and InstructBLIP (Dai et al., 2024) use learnable projectors or query embeddings to interact with visual features. These approaches aimed to leverage high-quality pre-training and fine-tuning data to comprehend complex instructions. Models like mPLUG-Owl (Ye et al., 2023), SPHINX (Lin et al., 2023b), and</p>
<p>MiniCPM-V2 (Hu et al., 2024) introduced new grounding data types and modularization training to minimize hallucinations and enhance grounding abilities. Despite these advancements, MLLMs face challenges in solving multimodal mathematical problems using diagrams. Further exploration of the quality and format of image instructions is needed to improve the reasoning capabilities of MLLMs.</p>
<h3>2.2 Multimodal Reasoning</h3>
<p>The rapid development of MLLMs has advanced research on multimodal reasoning (Chen et al., 2024a; You et al., 2023). Augmenting the original question and answer text data in restricted domains to further fine-tune MLLMs is a popular approach. For raw answers, rationales were either generated by humans (Zhang et al., 2023c) or gathered from prominent LLMs (Wang et al., 2024b; Lin et al., 2023a; Chen and Feng, 2023; Li et al., 2024b). Additionally, VPD (Hu et al., 2023b) proposed expanding answers by converting programming code formats to natural language formats. For raw questions, DDCoT (Zheng et al., 2023) used LLMs to decompose the original questions into sub-questions. These methods, however, only utilize LLMs to target text-only data within restricted domains, neglecting to fully exploit the visual information in raw images for further enhancement. To evaluate the multimodal reasoning abilities of MLLMs more comprehensively, MathVista (Lu et al., 2023), Math-V (Wang et al., 2024a) and MathVerse (Zhang et al., 2024), which involve various types of mathematical reasoning and skills, and MMMU (Yue et al., 2023a), which encompasses multidisciplinary tasks, have been proposed. There is still significant room for improvement in open-source MLLMs.</p>
<h2>3 Data Synthesis</h2>
<p>Existing open-source image instruction fine-tuning datasets (Lu et al., 2022b; Li et al., 2023; Lu et al., 2022a), containing limited question-answer pairs per image, do not fully exploit visual information to enhance the multimodal mathematical reasoning capabilities of MLLMs. To address this, we propose MathV360K, a robust dataset synthesized based on the 40K selected images and seed question-answer pairs from multiple sub-domains. As shown in the left side of Figure 1, we first select 40K highquality data points based on the image clarity and</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The overall flowchart of the proposed multimodal question-answer data selection and data augmentation. Our data selection depends on the fine-tuned ViT as image classifier. The data generation process depends on the vision-language model.</p>
<p>comprehension complexity from 24 open-source multimodal question-answering datasets. In the second step, illustrated in the top right of Figure 1, we attempt to fully mine the visual information of the images to generate additional questions. The data generation pipeline includes creating diverse new questions to fully exploit the visual information, more complex questions to further improve the reasoning capabilities, rephrased questions and underspecified questions to improve the robustness of the model. With the data generation pipeline, we collected 360K high-quality and diverse instruction-tuning data for the selected 40K data points to enhance the image understanding and mathematical reasoning capabilities of the LLaVA-1.5 open-source model.</p>
<h3>3.1 Multimodal Reasoning Data Selection</h3>
<h4>3.1.1 Source Data</h4>
<p>We collected 24 visual question answering and multimodal mathematical reasoning datasets, each targeting a specific task type and visual content. We focused on five problem task types requiring high-level reasoning to compile the source dataset: Figure Question Answering (FQA), Geometry Problem Solving (GPS), Math Word Problem (MWP), Textbook Question Answering (TQA), and Visual Question Answering (VQA). Table 6 in Appendix shows more details about the task type and visual context of each source dataset.</p>
<p>Each multimodal training sample consists of three components: an image $I_i$, a text question $Q_i$, and a ground-truth answer $A_i$. From this data format, the model aims to capture visual information and question semantics to reason the final answer.</p>
<h4>3.1.2 Image Filtering and Proportioning</h4>
<p>After acquiring the 24 source datasets, we intentionally selected data from the raw images based on the following criteria: (1) The clarity of the images, as poor-quality images introduce noise and interfere with learning image semantics; (2) The comprehension complexity of the images, which varies from easy to complex. By categorizing images into different levels of complexity and selecting proportionally, we can form a training set with an appropriate difficulty distribution; (3) The quality of the corresponding textual question data, ensuring that the difficulty aligns with the comprehension complexity of the images.</p>
<p>We fine-tuned two Vision Transformer (ViT) <em>Dosovitskiy et al. (2021)</em> models to classify image clarity and image comprehension complexity, respectively. Due to the lack of annotated image data, we first sampled 10K images uniformly and randomly from the source datasets. These images were labeled for clarity and comprehension complexity using GPT-4V <em>OpenAI (2023)</em>, with our designed prompt shown in Figure 2. For image clarity, label 0 indicates a blurred, poor-quality image, and label 1 indicates a clear, good-quality image. Image comprehension complexity is determined by the number of objects, their positional relationships, the need for mathematical calculations, detail level,</p>
<p>texture, and material properties. Images are categorized into scores of $0,1,2$, and 3 , with lower values indicating easier visual context comprehension.</p>
<p>Based on the 10 K annotated images, we trained two ViT models with initialized fully connected layers for classification using cross-entropy loss. We first classified all source training dataset images using the fine-tuned image clarity classifier and filtered out images labeled as 0 . Table 6 shows the number of images before (i.e., Training Images) and after (i.e., Clear Images) filtering.</p>
<p>Next, we used the image comprehension complexity classifier to score the filtered images. Table 6 shows that most images are classified as medium complexity, followed by easy, and finally the most complex. Considering that simple images are easier to learn from, while complex images are harder and require more reference samples, we sampled the first three complexity categories using a progressive scale from simple to complex. Since images with a score of 3 are the least abundant, we collected all of them. We selected 40 K data points based on an overall ratio of complexity 2:3:4:1, ensuring samples from different complexities are uniformly selected from each source dataset. As a result, we obtained 40 K high-quality $(I, Q, A)$ real data points that are diverse in image information and questions are progressive in difficulty.</p>
<h2>Prompt-Image Annotation:</h2>
<p>[ROLE] You are an AI assistant to help me review the image.
[Task1] Your first task is to review the image and classify the clarity and quality of the given image into 0 or 1.0 indicates that the image is not clear and of poor quality. 1 indicates that the image is clear enough and of high quality. Your answer MUST be in the format: "The label is [0 or 1]".
[Task2] Your second task is to assess the complexity of the image. Rate based on the number of objects in the image, their positional relationships, whether mathematical calculations are needed for understanding, detail level, texture and material properties. The score ranges from 0 to 3 , with higher scores indicating greater complexity. A score of 3 represents the highest complexity. Your answer MUST be in the format: "The ranking is [YOUR SCORE]".</p>
<p>Figure 2: The prompt template used in our GPT-4V API for image annotation. Image clarity is considered as binary classification and image comprehension complexity is viewd as multi-classification.</p>
<h3>3.2 Data Augmentation</h3>
<h3>3.2.1 Mining Image for QA Generation</h3>
<p>After selecting 40K multimodal reasoning data, we observed that each image typically corresponds to
limited questions. As shown in the tabular image of Figure 1, the original question often focuses only on localized arithmetic differences. However, additional questions about overall averages, continuous variations, and more can also be asked, indicating that the visual information of an image is not fully exploited with just one question. Therefore, we can further augment the available real data by generating more question-answer pairs for each image.</p>
<p>We use GPT-4V to generate additional questions based on the input image and the original question. If questions are generated in a zero-shot manner, they often focus on one-sided visual scenes, lacking reasoning and mathematical skills. For images from specific tasks, such as geometric figures, more task-specific questions should be asked. Therefore, we adopt few-shot demonstrations for GPT-4V to generate new questions.</p>
<p>For an image belonging to one of the categories (FQA, GPS, MWP, TQA, VQA), we first internally cluster the questions into five classes for each source dataset within that task category. Specifically, features of text questions are obtained using TF-IDF and clustered using K-Means. As shown in Figure 4, we take IconQA as an example. After clustering the questions in the training set, each cluster internally represents a specific questioning format and pattern that can be referenced. Demonstrations are constructed by randomly sampling one question from each cluster of each source dataset belonging to a certain task type.</p>
<p>The prompt for generating new questions for an input image is shown in Figure 3. This method ensures that the newly generated questions are consistent with the distribution of the original reference questions while improving diversity. Using this approach, we generated 200K new question-answer pairs based on the selected 40K data points.</p>
<h3>3.2.2 Augmentation of Original Question</h3>
<p>We designed prompts to augment the original questions, as shown in Figure 5. Using GPT-4V, we generated 40K more complex questions, 40K simplified questions, and 40K rephrased questions. The augmentation focused on the following aspects:
Complexity. More complex reasoning samples can enhance the reasoning capabilities of fine-tuned LLMs (Luo et al., 2023). Our first approach involves creating more complex questions based on the original image and corresponding inquiries.
Logical Consistency. Robust MLLMs should answer consistently about similar content in a given</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: The prompt template used in our GPT-4V API generates additional questions for each input image. Demonstrations are constructed by randomly sampling one question from each cluster of each source dataset belonging to a specific task type.
image [Tascon-Morales et al., 2023]. We employed GPT-4V to ask the same question in different ways without changing the answer.</p>
<p>Underspecification. Robust MLLMs must deal with semantic underspecification, where the linguistic signal conveys only part of the necessary information for successful communication (Pezzelle, 2023). Therefore, we simplified the original questions without affecting their semantic understanding when combined with the image.</p>
<h2>4 Experiments</h2>
<h3>4.1 Model and Training</h3>
<p>We employ the LLaVA-1.5 architecture as our base model, which primarily comprises the Vicuna-v1.5 language model (Team, 2023) and a pretrained Vision Transformer (ViT) as the image encoder. To preserve the foundational model's superior visual perception and descriptive abilities, we fine-tune LLaVA-1.5-13B using the proposed MathV360K instruction-tuning dataset. The diverse question patterns and rich visual content within this dataset enhance the model's multimodal mathematical reasoning capabilities while maintaining its general vision-language understanding skills.</p>
<h3>4.2 Evaluation and Metrics</h3>
<p>We evaluate our model using the minitest subset of MathVista (Lu et al., 2023) in a zero-shot manner. This minitest subset comprises 1,000 samples, including 540 multiple-choice questions and 460 questions that require free-form answers in the form of integers, floats, or lists. MathVista adequately assesses the MLLMs' multimodal mathematical skills, including algebraic reasoning (ALG), arithmetic reasoning (ARI), geometry reasoning (GEO), logical reasoning (LOG), numeric commonsense (NUM), scientific reasoning (SCI), and statistical reasoning (STA). Furthermore, MathVista questions can be categorized into the following subsets: FQA, GPS, MWP, TQA, and VQA. For evaluation, we first employ GPT-4 to extract the predicted choices or answers from responses, then report the answer accuracy, which entails determining whether the final answer matches the ground truth. We also conduct evaluation using the Math-V (Wang et al., 2024a) and MathVerse (Zhang et al., 2024). Math-V is a meticulously curated collection of 3,040 mathematical problems with visual contexts sourced from real math competitions. MathVerse consists of 2,612 multi-subject math problems varying degrees of information content in multi-modality. Additionally, we evaluate our model's enhanced generalizability using the MMMU benchmark (Yue et al., 2023a). The</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: The visualization of the K-Means by T-SNE. We take IconQA as example. The questioning format of each cluster can be used as a reference to generate new questions for similar visual content.</p>
<p>MMMU benchmark, with 900 evaluation samples, encompasses six core disciplines: Art \&amp; Design, Business, Science, Health \&amp; Medicine, Humanities \&amp; Social Science, and Technology \&amp; Engineering, making it suitable for assessing the generalization of MLLMs' reasoning capabilities.</p>
<h3>4.3 Implementation Details</h3>
<p>We utilize GPT-4V (GPT-4 Vision Preview) for the data generation process. To classify image clarity and comprehension complexity, we fine-tune two ViT-Large-Patch16-224 models, each with a learning rate of $2 \mathrm{e}-4$ and a training period of 5 epochs. For the LLaVA-1.5-13B model, the input image resolution is configured to 336 by 336 pixels. Both the projection linear layer and the language model are trainable. During the fine-tuning phase, we set a learning rate of $2 \mathrm{e}-5$, employ a batch size of 16 , and conduct fine-tuning over 2 epochs using A800 GPUs equipped with 80 GB of memory.</p>
<h2>5 Results and Analysis</h2>
<h3>5.1 Main Comparison on MathVista</h3>
<p>We compare Math-LLaVA with other MLLMs on the minitest split of the MathVista benchmark in Ta-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: The prompt templates used in our GPT-4V API to generate more complex, logically consistent and underspecified questions from original question text.
ble 1. As shown in the table, open-source MLLMs such as miniGPT4 (Zhu et al., 2023), instructBLIP (Dai et al., 2024), and LLaVA-1.5-13B have poor performance in multimodal mathematics, with overall accuracy lower than $30 \%$. Compared to the base model, LLaVA-1.5-13B, with poor multimodal mathematical ability, Math-LLaVA achieves 46.6\% overall accuracy with a significant improvement of $19 \%$. More surprisingly, the proposed MathLLaVA model outperforms close-source models Gemini 1.0 Pro (Team et al., 2023) and Claude 3 Haiku (Anthropic, 2024), even achieving comparable performance to GPT-4V (OpenAI), the most powerful close-source MLLMs. Interestingly, Math-LLaVA achieves $57.7 \%$ accuracy on GPS subset, outperforming G-LLaVA-13B (Gao et al., 2023a), which has been trained on 170 K high-quality geometric image-caption and questionanswer pairs. The results on Math-V are shown in Table 2. Math-LLaVA also achieves significant improvement compared to the base model and leading performance than Qwen-VL-Max (Bai et al., 2023) and most open-source MLLMs. The results on MathVerse could be found from Table 7 in Appendix. The superior performance of Math-LLaVA indicates that the data selection and synthesis of high-quality, diverse multimodal question-answer pairs are effective in improving MLLM's multimodal mathematical reasoning capabilities.</p>
<h3>5.2 Generalizability of Math-LLaVA</h3>
<p>The proposed Math-LLaVA model has demonstrated exceptional performance in multimodal</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">MathVista</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ALL</td>
<td style="text-align: center;">FQA</td>
<td style="text-align: center;">GPS</td>
<td style="text-align: center;">MWP</td>
<td style="text-align: center;">TQA</td>
<td style="text-align: center;">VQA</td>
<td style="text-align: center;">ALG</td>
<td style="text-align: center;">ARI</td>
<td style="text-align: center;">GEO</td>
<td style="text-align: center;">LOG</td>
<td style="text-align: center;">NUM</td>
<td style="text-align: center;">SCI</td>
<td style="text-align: center;">STA</td>
</tr>
<tr>
<td style="text-align: center;">Heuristics Baselines</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Random Chance</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">8.3</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">16.3</td>
</tr>
<tr>
<td style="text-align: center;">Frequent Guess (Lu et al., 2023)</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">19.4</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">20.9</td>
</tr>
<tr>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">60.3</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">55.9</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">40.7</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">63.9</td>
</tr>
<tr>
<td style="text-align: center;">Close-Source Multimodal Large Langugae Models (MLLMs)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Gemini 1.0 Nano 2 (Team et al., 2023)</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">41.8</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">33.5</td>
</tr>
<tr>
<td style="text-align: center;">Qwen-VL-Plus (Bai et al., 2023)</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">39.1</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">56.1</td>
</tr>
<tr>
<td style="text-align: center;">Gemini 1.0 Pro (Team et al., 2023)</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">39.1</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">56.8</td>
</tr>
<tr>
<td style="text-align: center;">Claude 3 Haiku (Anthropic, 2024)</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4V (OpenAI)</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">55.8</td>
</tr>
<tr>
<td style="text-align: center;">Open-Source Multimodal Large Langugae Models (MLLMs)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">mPLUG-Owl-7B (Ye et al., 2023)</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">12.7</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">21.4</td>
</tr>
<tr>
<td style="text-align: center;">miniGPT4-7B (Zhu et al., 2023)</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">17.9</td>
</tr>
<tr>
<td style="text-align: center;">LLaVAR-13B (Zhang et al., 2023b)</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">21.9</td>
</tr>
<tr>
<td style="text-align: center;">InstructBLIP-7B (Dai et al., 2024)</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">23.1</td>
</tr>
<tr>
<td style="text-align: center;">LLaVA-13B (Liu et al., 2023)</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">25.1</td>
</tr>
<tr>
<td style="text-align: center;">SPHINX-V1-13B (Lin et al., 2023b)</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">39.9</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">23.6</td>
</tr>
<tr>
<td style="text-align: center;">LLaVA-1.5-13B (Liu et al., 2024)</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">LLaVA-1.5-13B ${ }^{\dagger}$ (Liu et al., 2024)</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">23.6</td>
</tr>
<tr>
<td style="text-align: center;">OmniLMM-12B (OpenBMB, 2024)</td>
<td style="text-align: center;">34.9</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">17.8</td>
<td style="text-align: center;">26.9</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">39.1</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">44.2</td>
</tr>
<tr>
<td style="text-align: center;">SPHINX-V2-13B (Lin et al., 2023b)</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">41.8</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;">17.6</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">51.5</td>
</tr>
<tr>
<td style="text-align: center;">G-LLaVA-13B (Gao et al., 2023a)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Math-LLaVA-DS</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">35.2</td>
</tr>
<tr>
<td style="text-align: center;">Math-LLaVA</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">43.9</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison with baselines on the testmini set of MathVista benchmark. Baseline results are obtained from Lu et al. (2023). ${ }^{\dagger}$ represents our reproduced results of LLaVA-1.5-13B. The best results in both the close-source and open-source MLLMs are in bold. MathVista is divided in two ways: task type or mathematical skill, and we report the accuracy under each subset.
mathematical reasoning tasks. To assess its generalization capability, we conduct evaluation experiments using the MMMU benchmark, which encompasses various disciplines and domains. The results are shown in Table 3. With only the selected data, Math-LLaVA has a performance drop on science subset. However, we can observe that the MathLLaVA model fine-tuned on MathV360K can significantly outperforms the base model, LLaVA-1.513B, as well as several other open-source MLLMs on all six sub-domains. This superior performance underscores its capability to generalize to downstream multimodal understanding and reasoning tasks. Furthermore, the fine-tuning process using our synthetic data does not detract from the model's reasoning abilities in other domains; rather, it enhances its generalizability.</p>
<h3>5.3 Overfitting to Text Modality</h3>
<p>The proposed data synthesis pipeline generates additional question-answer pairs for each image to enhance the mathematical reasoning of MLLMs.</p>
<p>Intuitively, we should investigate whether the proposed model, Math-LLaVA, is overfitting on the generated question-answer pairs. If overfitting occurs, Math-LLaVA might memorize or retrieve image information without requiring any visual input. To examine this, we compare the performance of Math-LLaVA before and after data synthesis, referred to as Math-LLaVA-DS and Math-LLaVA, respectively, on MathVista using text inputs only. As shown in Table 4, Math-LLaVA exhibits similar performance, approximately $32.0 \%$, as MathLLaVA-DS on MathVista when inference is performed without any visual information. Furthermore, fine-tuning Math-LLaVA with only text data also yields similar observations. This indicates that the Math-LLaVA model is not overfitting on the synthesized question-answer pairs.</p>
<p>Interestingly, we also observe that with text-only input, LLaVA-1.5-13B achieves an accuracy of $23.3 \%$ on MathVista. Potential reasons as explored in (Chen et al., 2024b) could be that visual content</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Math-V</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ALL</td>
<td style="text-align: center;">Alg</td>
<td style="text-align: center;">AnaG</td>
<td style="text-align: center;">Ari</td>
<td style="text-align: center;">CG</td>
<td style="text-align: center;">Comb</td>
<td style="text-align: center;">Cnt</td>
<td style="text-align: center;">DG</td>
<td style="text-align: center;">GT</td>
<td style="text-align: center;">Log</td>
<td style="text-align: center;">Angle</td>
<td style="text-align: center;">Area</td>
<td style="text-align: center;">Len</td>
<td style="text-align: center;">SG</td>
<td style="text-align: center;">Sta Topo TG</td>
</tr>
<tr>
<td style="text-align: center;">Heuristics Baselines</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Random Chance</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">11.9</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">8.2</td>
<td style="text-align: center;">8.6</td>
</tr>
<tr>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">98.5</td>
<td style="text-align: center;">91.3</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">93.1</td>
</tr>
<tr>
<td style="text-align: center;">Close-Source Multimodal Large Langugae Models (MLLMs)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Qwen-VL-Plus (Bai et al., 2023)</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">12.7</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">10.5</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">6.4</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">6.9</td>
</tr>
<tr>
<td style="text-align: center;">Qwen-VL-Max (Bai et al., 2023)</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">14.2</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">20.7</td>
</tr>
<tr>
<td style="text-align: center;">Gemini Pro (Team et al., 2023)</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">11.9</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">13.8</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4V (OpenAI)</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">24.1</td>
</tr>
<tr>
<td style="text-align: center;">Open-Source Multimodal Large Langugae Models (MLLMs)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SPHINX-V2-13B (Lin et al., 2023b)</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: center;">12.9</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">9.6</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">11.8</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">8.2</td>
<td style="text-align: center;">8.6</td>
</tr>
<tr>
<td style="text-align: center;">LLaVA-1.5-13B (Liu et al., 2024)</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">13.8</td>
</tr>
<tr>
<td style="text-align: center;">Math-LLaVA</td>
<td style="text-align: center;">15.7</td>
<td style="text-align: center;">9.0</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">15.7</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">10.5</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">17.6</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">24.1</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance Comparison on the Math-V benchmark with the accuracy metric across various mathmatical subjects. Baseline results are obtained from Wang et al. (2024a). The best results in both the close-source and open-source MLLMs are in bold.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">MMMU</th>
<th style="text-align: center;">Art \&amp; Design</th>
<th style="text-align: center;">Business</th>
<th style="text-align: center;">Sci.</th>
<th style="text-align: center;">Health \&amp; Med.</th>
<th style="text-align: center;">Human. \&amp; Social</th>
<th style="text-align: center;">Tech. \&amp; Eng.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Random Chance</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">21.4</td>
</tr>
<tr>
<td style="text-align: center;">Frequent Guess</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">25.8</td>
<td style="text-align: center;">24.8</td>
</tr>
<tr>
<td style="text-align: center;">miniGPT4-7B</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">23.8</td>
</tr>
<tr>
<td style="text-align: center;">mPLUG-Owl-7B</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">31.0</td>
</tr>
<tr>
<td style="text-align: center;">SPHINX-13B</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">26.2</td>
</tr>
<tr>
<td style="text-align: center;">InstructBLIP-7B</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;">27.1</td>
</tr>
<tr>
<td style="text-align: center;">LLaVA-1.5-13B</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">31.4</td>
</tr>
<tr>
<td style="text-align: center;">Math-LLaVA-DS</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">32.4</td>
</tr>
<tr>
<td style="text-align: center;">Math-LLaVA</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">33.3</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison with baselines on the MMMU benchmark.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Training</th>
<th style="text-align: center;">Inference</th>
<th style="text-align: center;">MathVista</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LLaVA-1.5-13B</td>
<td style="text-align: center;">Image-Text</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">23.3</td>
</tr>
<tr>
<td style="text-align: center;">Math-LLaVA-DS</td>
<td style="text-align: center;">Image-Text</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">32.2</td>
</tr>
<tr>
<td style="text-align: center;">Math-LLaVA</td>
<td style="text-align: center;">Image-Text</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">32.4</td>
</tr>
<tr>
<td style="text-align: center;">Math-LLaVA-DS</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">32.1</td>
</tr>
<tr>
<td style="text-align: center;">Math-LLaVA</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">32.5</td>
</tr>
</tbody>
</table>
<p>Table 4: Results of inference using only text of MathVista as input. Fine-tuning LLaVA-1.5 using image-text or text-only data.
is unnecessary for many samples in MathVista and that unintentional data leakage may occur during the pre-training of LLMs and MLLMs.</p>
<h3>5.4 Effectiveness of Synthesis</h3>
<p>To verify the effectiveness of data selection and the proposed data augmentation strategies, we conduct experiments on various components of MathV360K independently. Initially, we fine-tune the LLaVA-1.5 model on 40K randomly sampled data points from the source dataset, without any selection, to demonstrate the efficacy of data filtering and proportioning. Subsequently, we separately combine the selected 40K data points with the generated data using four augmentation methods: mining images for QA generation (AskImg), posing complex questions (CompQ), rephrasing questions for logical consistency (RephQ), and simplifying questions for underspecification (SimpQ). Table 5 presents the accuracy achieved by different combinations of augmentations on MathVista. The results indicate that our data synthesis approach, which incorporates data selection and each augmentation method, yields better performance. Collectively, these strategies result in a significant $11 \%$ improvement over randomly sampling 40 K data points.</p>
<h3>5.5 Enhancements from Augmentation of Each Task Type</h3>
<p>Given that we selected data from five different question-answering task types, our aim is to investi-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Select</th>
<th style="text-align: center;">AskImg</th>
<th style="text-align: center;">CompQ</th>
<th style="text-align: center;">RephQ</th>
<th style="text-align: center;">SimpQ</th>
<th style="text-align: center;">ALL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">35.6</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">38.2</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">42.2</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">39.8</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">40.9</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">41.1</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">46.6</td>
</tr>
</tbody>
</table>
<p>Table 5: Effectiveness of data selection and different data augmentation strategies on MathVista.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Accuracy on MathVista by augmentation for each task type.
gate which types or skills in multimodal mathematical reasoning could be enhanced by augmenting the source data from each individual task category. To this end, we conduct experiments with newly synthesized data for each task type, mixed with selected data. The results on MathVista are presented in Figure 6. We observe that augmentation of various types of source data can further improve the model's performance on the corresponding tasks. The enhancements are particularly pronounced for tasks involving FQA, MWP, and VQA. Interestingly, data augmentation for a single task type also shows improvements in effectiveness for other task types, likely due to the overlap in reasoning skills required across different tasks.</p>
<h2>6 Conclusions</h2>
<p>We addressed the shortage of high-quality and diverse multimodal mathematical training datasets by creating MathV360K, which consists of 40K high-quality multimodal questions and answers from 24 existing datasets, along with 320K newly synthesized question-answer pairs. This comprehensive dataset enhances both the breadth and depth of multimodal mathematical questions. Using MathV360K, we fine-tuned Math-LLaVA, significantly improving its capability in multimodal mathematical reasoning, outperforming LLaVA-
1.5 by 19 points on the minitest split of MathVista, and yielding leading performance on Math-V and MathVerse. Additionally, Math-LLaVA was validated on the MMMU benchmark, demonstrating its generalizability. Our research underscores the importance of dataset diversity and synthesis in enhancing the mathematical reasoning abilities of MLLMs.</p>
<h2>7 Limitations</h2>
<p>The data we selected and synthesized are in the format of images, questions, and answers, lacking intermediate steps that could be further improved. In future work, we will introduce annotated intermediate steps and rationale to construct more comprehensive and high-quality datasets to further enhance the MLLMs's multimodal reasoning capability.</p>
<h2>8 Ethics Statement</h2>
<p>We do not envision that our work will result in any harm as defined in ethics policy. LLaVA-1.5 base model uses LLaMA 2 Community License and ViT-Large-Patch16-224 uses Apache License 2.0. For datasets, GEOS, A-OKVQA and MMMU use Apache License 2.0. Geometry3K, FigureQA and PMC-VQA use MIT License. Super-CLEVR uses BSD License. ChartQA uses GPL 3.0 License. GeoQA+, UniGeo and DocVQA are publicly available for research purposes. The rest of the dataset use permissive Creative Commons Licenses. The intended use of these source datasets and evaluation datasets is to train and test the model's multimodal reasoning capability, which is consistent with our utilization of all these data. Our proposed MathV360K can improve the multimodal mathematical reasoning ability of the open-source LLaVA-1.5 through training. Our data and model are publicly available.</p>
<h2>9 Acknowledgement</h2>
<p>This work is supported by the National Natural Science Foundation of China under grant 62220106008, U20B2063, and 62102070. This research is supported by A*STAR, CISCO Systems (USA) Pte. Ltd and National University of Singapore under its Cisco-NUS Accelerated Digital Economy Corporate Laboratory (Award I21001E0002).</p>
<h2>References</h2>
<p>AI Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card.</p>
<p>Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. 2015. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425-2433.</p>
<p>Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: A frontier large vision-language model with versatile abilities. CoRR, abs/2308.12966.</p>
<p>Yi Bin, Mengqun Han, Wenhao Shi, Lei Wang, Yang Yang, See-Kiong Ng, and Heng Tao Shen. 2023. Non-autoregressive math word problem solver with unified tree structure. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3290-3301.</p>
<p>Yi Bin, Wenhao Shi, Yujuan Ding, Zhiqiang Hu, Zheng Wang, Yang Yang, See-Kiong Ng, and Heng Tao Shen. 2024. Gallerygpt: Analyzing paintings with large multimodal models. In ACM Multimedia 2024.</p>
<p>Jie Cao and Jing Xiao. 2022. An augmented benchmark dataset for geometric question answering through dual parallel text encoding. In Proceedings of the 29th International Conference on Computational Linguistics, pages 1511-1520.</p>
<p>Shuaichen Chang, David Palzer, Jialin Li, Eric FoslerLussier, and Ningchuan Xiao. 2022. Mapqa: A dataset for question answering on choropleth maps. arXiv preprint arXiv:2211.08545.</p>
<p>Feng Chen and Yujian Feng. 2023. Chain-of-thought prompt distillation for multimodal named entity and multimodal relation extraction. arXiv preprint arXiv:2306.14122.</p>
<p>Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu Chen, and Xiaodan Liang. 2022. UniGeo: Unifying geometry logical reasoning via reformulating mathematical expression. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3313-3323.</p>
<p>Jiaxing Chen, Yuxuan Liu, Dehu Li, Xiang An, Ziyong Feng, Yongle Zhao, and Yin Xie. 2024a. Plug-and-play grounding of reasoning in multimodal large language models. arXiv preprint arXiv:2403.19322.</p>
<p>Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. 2024b. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330.</p>
<p>Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,</p>
<p>Boyang Li, Pascale N Fung, and Steven Hoi. 2024. Instructblip: Towards general-purpose visionlanguage models with instruction tuning. Advances in Neural Information Processing Systems, 36.</p>
<p>Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations.</p>
<p>Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. 2023a. G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370.</p>
<p>Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. 2023b. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010.</p>
<p>Google. Gemini. https://gemini.google.com.
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. 2023. Tora: A tool-integrated reasoning agent for mathematical problem solving. CoRR, abs/2309.17452.</p>
<p>Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904-6913.</p>
<p>Yanyang Guo, Fangkai Jiao, Zhiqi Shen, Liqiang Nie, and Mohan S. Kankanhalli. 2023. UNK-VQA: A dataset and A probe into multi-modal large models' abstention ability. CoRR, abs/2310.10942.</p>
<p>Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. 2018. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3608-3617.</p>
<p>Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. 2024. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395.</p>
<p>Yushi Hu, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy Viswanathan, Kenji Hata, Enming Luo, Ranjay Krishna, and Ariel Fuxman. 2023a. Visual program distillation: Distilling tools and programmatic reasoning into vision-language models. arXiv preprint arXiv:2312.03052.</p>
<p>Yushi Hu, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy Viswanathan, Kenji Hata, Enming Luo, Ranjay Krishna, and Ariel Fuxman. 2023b. Visual program distillation: Distilling tools and programmatic reasoning into vision-language models. CoRR, abs/2312.03052.</p>
<p>Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. 2018. Dvqa: Understanding data visualizations via question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5648-5656.</p>
<p>Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Ákos Kádár, Adam Trischler, and Yoshua Bengio. 2017. Figureqa: An annotated figure dataset for visual reasoning. arXiv preprint arXiv:1710.07300.</p>
<p>Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. 2016. A diagram is worth a dozen images. In Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV 14, pages 235251.</p>
<p>Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension. In Proceedings of the IEEE Conference on Computer Vision and Pattern recognition, pages 4999-5007.</p>
<p>Jason J Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. 2018. A dataset of clinically generated visual questions and answers about radiology images. Scientific data, 5(1):1-10.</p>
<p>Haoxuan Li, Zhengmao Yang, Yunshan Ma, Yi Bin, Yang Yang, and Tat-Seng Chua. 2024a. Mm-forecast: A multimodal approach to temporal event forecasting with large language models. In ACM Multimedia 2024.</p>
<p>Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 12888-12900.</p>
<p>Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. 2024b. Multimodal ArXiv: A dataset for improving scientific comprehension of large vision-language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, pages $14369-14387$.</p>
<p>Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, and Alan L Yuille. 2023. Super-clevr: A virtual benchmark to diagnose domain robustness in visual reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages $14963-14973$.</p>
<p>Hongzhan Lin, Ziyang Luo, Jing Ma, and Long Chen. 2023a. Beneath the surface: Unveiling harmful memes with multimodal reasoning distilled from large language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9114-9128.</p>
<p>Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et al. 2023b. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575.</p>
<p>Adam Dahlgren Lindström and Savitha Sam Abraham. 2022. Clevr-math: A dataset for compositional language, visual and mathematical reasoning. arXiv preprint arXiv:2208.05358.</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26296-26306.</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. In Advances in Neural Information Processing Systems.</p>
<p>Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. 2023. Mathvista: Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models. CoRR, abs/2310.02255.</p>
<p>Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. 2021a. Inter-GPS: Interpretable geometry problem solving with formal language and symbolic reasoning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6774-6786.</p>
<p>Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022a. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:2507-2521.</p>
<p>Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. 2022b. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. arXiv preprint arXiv:2209.14610.</p>
<p>Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. 2021b. Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. arXiv preprint arXiv:2110.13214.</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei</p>
<p>Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. CoRR, abs/2308.09583.</p>
<p>Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. 2022. ChartQA: A benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 22632279.</p>
<p>Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. 2022. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1697-1706.</p>
<p>Nitesh Methani, Pritha Ganguly, Mitesh M Khapra, and Pratyush Kumar. 2020. Plotqa: Reasoning over scientific plots. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages $1527-1536$.</p>
<p>OpenAI. Gpt-4v(ision). https://openai.com/ research/gpt-4v-system-card.</p>
<p>OpenBMB. 2024. Large multi-modal models for strong performance and efficient deployment. https:// github.com/OpenBMB/OmniLMM.</p>
<p>Sandro Pezzelle. 2023. Dealing with semantic underspecification in multimodal nlp. arXiv preprint arXiv:2306.05240.</p>
<p>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763.</p>
<p>Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. 2022. A-okvqa: A benchmark for visual question answering using world knowledge. In European Conference on Computer Vision, pages 146-162.</p>
<p>Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Etzioni, and Clint Malcolm. 2015. Solving geometry problems: Combining text and diagram interpretation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages $1466-1476$.</p>
<p>Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. 2019. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8317-8326.</p>
<p>Sergio Tascon-Morales, Pablo Márquez-Neila, and Raphael Sznitman. 2023. Logical implications for visual question answering consistency. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6725-6735.</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805.</p>
<p>The Vicuna Team. 2023. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality. https://lmsys.org/blog/2023-03-30-vicuna.</p>
<p>Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. 2024a. Measuring multimodal mathematical reasoning with math-vision dataset. arXiv preprint arXiv:2402.14804.</p>
<p>Lei Wang, Yi Hu, Jiabang He, Xing Xu, Ning Liu, Hui Liu, and Heng Tao Shen. 2024b. T-sciq: Teaching multimodal chain-of-thought reasoning via large language model signals for science question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 19162-19170.</p>
<p>Lei Wang, Wanyu Xu, Zhiqiang Hu, Yihuai Lan, Shan Dong, Hao Wang, Roy Ka-Wei Lee, and Ee-Peng Lim. 2024c. All in a single image: Large multimodal models are in-image learners. arXiv preprint arXiv:2402.17971.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems.</p>
<p>Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. 2023. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178.</p>
<p>Haoxuan You, Rui Sun, Zhecan Wang, Long Chen, Gengyu Wang, Hammad A. Ayyubi, Kai-Wei Chang, and Shih-Fu Chang. 2023. Idealgpt: Iteratively decomposing vision and language reasoning via large language models. In Findings of the Association for Computational Linguistics: EMNLP, pages 1128911303.</p>
<p>Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023a. MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI. CoRR, abs/2311.16502.</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023b. Mammoth: Building math generalist models through hybrid instruction tuning. CoRR, abs/2309.05653.</p>
<p>Jipeng Zhang, Lei Wang, Roy Ka-Wei Lee, Yi Bin, Yan Wang, Jie Shao, and Ee-Peng Lim. 2020. Graph-to-tree learning for solving math word problems. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3928-3937.</p>
<p>Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. 2024. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? arXiv preprint arXiv:2403.14624.</p>
<p>Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2023a. Pmc-vqa: Visual instruction tuning for medical visual question answering. arXiv preprint arXiv:2305.10415.</p>
<p>Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. 2023b. Llavar: Enhanced visual instruction tuning for text-rich image understanding. arXiv preprint arXiv:2306.17107.</p>
<p>Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. 2023c. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923.</p>
<p>Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, and Sibei Yang. 2023. Ddcot: Duty-distinct chain-of-thought prompting for multimodal reasoning in language models. Advances in Neural Information Processing Systems, 36:5168-5191.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V. Le, and Ed H. Chi. 2023. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations.</p>
<p>Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models. CoRR, abs/2304.10592.</p>
<h2>A Appendix</h2>
<h2>A. 1 Source Data Statistics</h2>
<p>We collected 24 visual question answering and multimodal mathematical reasoning datasets, each targeting a specific task type and visual content. We focused on five problem task types to compile the source dataset: Figure Question Answering (FQA), which involves analyzing charts and plots statistically; Geometry Problem Solving (GPS), which involves solving geometrical problems with diagrams and figures; Math Word Problem (MWP), which involves arithmetic calculations within the context of images; Textbook Question Answering (TQA), where reasoning is based on scientific knowledge and figures; and Visual Question Answering (VQA), which requires reasoning about objects, scenes, or relationships within images. These datasets from different domains can be combined to cover multiple tasks, incorporating diverse visual contexts and mathematical skills. Although TQA and VQA primarily involve questions about scenes and relationships, they also include questions requiring arithmetic or numeric skills. Such data enhances multimodal mathematical reasoning and generalizes to other question answering tasks.</p>
<p>The source data are summarized in Table 6 corresponding to Section 3.1.</p>
<h2>A. 2 Results on MathVerse Benchmark</h2>
<p>The proposed Math-LLaVA model has demonstrated impressive performance on MathVista and Math-V. To assess its multimodal mathematical reasoning capabilities more comprehensively, we conduct evaluation experiments using the MathVerse benchmark (Zhang et al., 2024). The results are shown in Table 7. Math-LLaVA also achieves significant improvement compared to the base model and impressive performance among most opensource MLLMs.</p>
<h2>A. 3 Distribution Proportioning of Image Comprehension Complexity</h2>
<p>We select images from the source data based on an overall complexity ratio of 2:3:4:1. Due to the limited number of the most complex images, all images with complexity level 3 are sampled. We employ a progressive distribution scale from easy to complex, as described in Section 3.1.2. In this section, we examine the impact of varying distribution proportions of the first three image comprehension complexity levels on model performance. We explore settings with different proportions of comprehension complexities 0,1 , and 2 , including uniform distribution, decreasing proportions as complexity increases, and proportions that fluctuate with complexity. As demonstrated in Table 8, both uniform distribution of image complexity and decreasing proportion with increasing difficulty are less effective compared to a progressive propor-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Visual Context</th>
<th style="text-align: center;">Training Images</th>
<th style="text-align: center;">Clear Images</th>
<th style="text-align: center;">Image Complexity</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">DocVQA (2022)</td>
<td style="text-align: center;">FQA</td>
<td style="text-align: center;">Document Image</td>
<td style="text-align: center;">8535</td>
<td style="text-align: center;">8227</td>
<td style="text-align: center;">2086</td>
<td style="text-align: center;">6007</td>
<td style="text-align: center;">125</td>
<td style="text-align: center;">9</td>
</tr>
<tr>
<td style="text-align: center;">FigureQA (2017)</td>
<td style="text-align: center;">FQA</td>
<td style="text-align: center;">Charts and Plots</td>
<td style="text-align: center;">18173</td>
<td style="text-align: center;">18173</td>
<td style="text-align: center;">687</td>
<td style="text-align: center;">16792</td>
<td style="text-align: center;">694</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">DVQA (2018)</td>
<td style="text-align: center;">FQA</td>
<td style="text-align: center;">Bar Chart</td>
<td style="text-align: center;">19092</td>
<td style="text-align: center;">19092</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">18021</td>
<td style="text-align: center;">1045</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;">PlotQA (2020)</td>
<td style="text-align: center;">FQA</td>
<td style="text-align: center;">Bar, Line, Scatter</td>
<td style="text-align: center;">18782</td>
<td style="text-align: center;">18782</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">18759</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">ChartQA (2022)</td>
<td style="text-align: center;">FQA</td>
<td style="text-align: center;">Charts and Plots</td>
<td style="text-align: center;">3699</td>
<td style="text-align: center;">3699</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3649</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">MapQA (2022)</td>
<td style="text-align: center;">FQA</td>
<td style="text-align: center;">Map Chart</td>
<td style="text-align: center;">10020</td>
<td style="text-align: center;">10016</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10015</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">IconQA (2021b)</td>
<td style="text-align: center;">MWP</td>
<td style="text-align: center;">Abstract Scene</td>
<td style="text-align: center;">20000</td>
<td style="text-align: center;">19068</td>
<td style="text-align: center;">10991</td>
<td style="text-align: center;">8055</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">CLEVR-Math (2022)</td>
<td style="text-align: center;">MWP</td>
<td style="text-align: center;">Synthetic Scene</td>
<td style="text-align: center;">17552</td>
<td style="text-align: center;">17551</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">17550</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">TabMWP (2022b)</td>
<td style="text-align: center;">MWP</td>
<td style="text-align: center;">Table</td>
<td style="text-align: center;">20000</td>
<td style="text-align: center;">20000</td>
<td style="text-align: center;">14919</td>
<td style="text-align: center;">5081</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">GEOS (2015)</td>
<td style="text-align: center;">GPS</td>
<td style="text-align: center;">Geometry Diagram</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">Geometry3K (2021a)</td>
<td style="text-align: center;">GPS</td>
<td style="text-align: center;">Geometry Diagram</td>
<td style="text-align: center;">2101</td>
<td style="text-align: center;">2101</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">1508</td>
<td style="text-align: center;">568</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">GeoQA+ (2022)</td>
<td style="text-align: center;">GPS</td>
<td style="text-align: center;">Geometry Diagram</td>
<td style="text-align: center;">6027</td>
<td style="text-align: center;">5956</td>
<td style="text-align: center;">103</td>
<td style="text-align: center;">4399</td>
<td style="text-align: center;">1454</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">UniGeo (2022)</td>
<td style="text-align: center;">GPS</td>
<td style="text-align: center;">Geometry Diagram</td>
<td style="text-align: center;">3499</td>
<td style="text-align: center;">3432</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">2514</td>
<td style="text-align: center;">846</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">TQA (2017)</td>
<td style="text-align: center;">TQA</td>
<td style="text-align: center;">Scientific Figure</td>
<td style="text-align: center;">1499</td>
<td style="text-align: center;">1497</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">949</td>
<td style="text-align: center;">498</td>
<td style="text-align: center;">30</td>
</tr>
<tr>
<td style="text-align: center;">AI2D (2016)</td>
<td style="text-align: center;">TQA</td>
<td style="text-align: center;">Scientific Figure</td>
<td style="text-align: center;">3247</td>
<td style="text-align: center;">3235</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">2321</td>
<td style="text-align: center;">823</td>
<td style="text-align: center;">59</td>
</tr>
<tr>
<td style="text-align: center;">ScienceQA (2022a)</td>
<td style="text-align: center;">TQA</td>
<td style="text-align: center;">Scientific Figure</td>
<td style="text-align: center;">6218</td>
<td style="text-align: center;">6061</td>
<td style="text-align: center;">1533</td>
<td style="text-align: center;">4251</td>
<td style="text-align: center;">273</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">A-OKVQA (2022)</td>
<td style="text-align: center;">VQA</td>
<td style="text-align: center;">Natural Image</td>
<td style="text-align: center;">16540</td>
<td style="text-align: center;">14526</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">11724</td>
<td style="text-align: center;">2743</td>
<td style="text-align: center;">49</td>
</tr>
<tr>
<td style="text-align: center;">VQA2.0 (2017)</td>
<td style="text-align: center;">VQA</td>
<td style="text-align: center;">Natural Image</td>
<td style="text-align: center;">16912</td>
<td style="text-align: center;">14521</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">12783</td>
<td style="text-align: center;">1672</td>
<td style="text-align: center;">21</td>
</tr>
<tr>
<td style="text-align: center;">PMC-VQA (2023a)</td>
<td style="text-align: center;">VQA</td>
<td style="text-align: center;">Medical Image</td>
<td style="text-align: center;">19682</td>
<td style="text-align: center;">9846</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">2989</td>
<td style="text-align: center;">3501</td>
<td style="text-align: center;">3294</td>
</tr>
<tr>
<td style="text-align: center;">VizWiz (2018)</td>
<td style="text-align: center;">VQA</td>
<td style="text-align: center;">Natural Image</td>
<td style="text-align: center;">20,000</td>
<td style="text-align: center;">16400</td>
<td style="text-align: center;">790</td>
<td style="text-align: center;">14800</td>
<td style="text-align: center;">770</td>
<td style="text-align: center;">40</td>
</tr>
<tr>
<td style="text-align: center;">Super-CLEVR (2023)</td>
<td style="text-align: center;">VQA</td>
<td style="text-align: center;">Synthetic Scene</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">1950</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1568</td>
<td style="text-align: center;">381</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">VQA-AS (2015)</td>
<td style="text-align: center;">VQA</td>
<td style="text-align: center;">Abstract Scene</td>
<td style="text-align: center;">14065</td>
<td style="text-align: center;">14065</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">13996</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">VQA-RAD (2018)</td>
<td style="text-align: center;">VQA</td>
<td style="text-align: center;">Medical Image</td>
<td style="text-align: center;">259</td>
<td style="text-align: center;">248</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">62</td>
</tr>
<tr>
<td style="text-align: center;">TextVQA (2019)</td>
<td style="text-align: center;">VQA</td>
<td style="text-align: center;">Natural Image</td>
<td style="text-align: center;">15815</td>
<td style="text-align: center;">11350</td>
<td style="text-align: center;">179</td>
<td style="text-align: center;">9497</td>
<td style="text-align: center;">1598</td>
<td style="text-align: center;">76</td>
</tr>
</tbody>
</table>
<p>Table 6: Summary of the 24 different source traing datasets for collection. The table provides details on their task, visual context, distribution of image clarity and comprehension complexity according to fine-tuned ViT classification model. Among them, only the text data of GeoQA+ are in Chinese, the rest source data are in English.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">MathVerse</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ALL</td>
<td style="text-align: center;">Text Dominant</td>
<td style="text-align: center;">Text Lite</td>
<td style="text-align: center;">Vision Intensive</td>
<td style="text-align: center;">Vision Dominant</td>
<td style="text-align: center;">Vision Only</td>
</tr>
<tr>
<td style="text-align: center;">Heuristics Baselines</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Random Chance</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">12.4</td>
</tr>
<tr>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">66.7</td>
</tr>
<tr>
<td style="text-align: center;">Close-Source Multimodal Large Langugae Models (MLLMs)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Qwen-VL-Plus (Bai et al., 2023)</td>
<td style="text-align: center;">11.8</td>
<td style="text-align: center;">15.7</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">9.0</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;">Gemini Pro (Team et al., 2023)</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">19.4</td>
<td style="text-align: center;">20.3</td>
<td style="text-align: center;">20.5</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4V (OpenAI)</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">40.9</td>
<td style="text-align: center;">34.9</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">29.8</td>
</tr>
<tr>
<td style="text-align: center;">Open-Source Multimodal Large Langugae Models (MLLMs)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">mPLUG-Owl-7B (Ye et al., 2023)</td>
<td style="text-align: center;">4.6</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">4.9</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-Adapter-V2-7B (Gao et al., 2023b)</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">4.2</td>
<td style="text-align: center;">6.1</td>
</tr>
<tr>
<td style="text-align: center;">LLaVA-1.5-13B (Liu et al., 2024)</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">8.8</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">6.9</td>
</tr>
<tr>
<td style="text-align: center;">SPHINX-V2-13B (Lin et al., 2023b)</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">13.9</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">10.4</td>
</tr>
<tr>
<td style="text-align: center;">Math-LLaVA</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">15.2</td>
</tr>
</tbody>
</table>
<p>Table 7: Performance Comparison on the MathVerse benchmark with the accuracy metric. Baseline results are obtained from Zhang et al. (2024). The best results in both the close-source and open-source MLLMs are in bold.</p>
<p>tional distribution aligned with complexity. These findings suggest that MLLMs require fewer simple images and question-answer pairs, but benefit from a larger proportion of complex training data to enhance multimodal mathematical reasoning.</p>
<table>
<thead>
<tr>
<th>Proportion</th>
<th>ALL</th>
<th>FQA</th>
<th>GPS</th>
<th>MWP</th>
<th>TQA</th>
<th>VQA</th>
</tr>
</thead>
<tbody>
<tr>
<td>3:3:3:1</td>
<td>36.0</td>
<td>29.0</td>
<td>44.4</td>
<td>40.9</td>
<td>35.6</td>
<td>34.5</td>
</tr>
<tr>
<td>4:3:2:1</td>
<td>36.4</td>
<td>32.0</td>
<td>39.6</td>
<td>42.5</td>
<td>36.9</td>
<td>35.1</td>
</tr>
<tr>
<td>2:4:3:1</td>
<td>35.1</td>
<td>32.0</td>
<td>40.5</td>
<td>35.5</td>
<td>36.2</td>
<td>34.6</td>
</tr>
<tr>
<td>2:3:4:1</td>
<td>38.2</td>
<td>33.5</td>
<td>47.2</td>
<td>41.4</td>
<td>36.7</td>
<td>34.6</td>
</tr>
</tbody>
</table>
<p>Table 8: Comparison with different distribution proportioning of image comprehension complexity on MathVista.</p>
<h3>A. 4 Cases Study</h3>
<p>We present several examples of solutions generated by Math-LLaVA and LLaVA-1.5 for imagequestion pairs of high school or college-level difficulty in MathVista. As illustrated in Figure 7, the base model (LLaVA-1.5) often performs inadequately on numerical computations involving tables, geometric problems, and counting tasks at the high school level. In contrast, our MathLLaVA model demonstrates superior proficiency in addressing these high school problems, thanks to its training on selected and synthesized data designed to tackle complex issues. Although LLaVA-1.5 faces challenges when dealing with more advanced functions and detailed tables, Math-LLaVA shows promise and capability in solving such intricate problems.</p>
<p>Additionally, we present several examples of newly generated questions, created by thoroughly mining images and questions from the selected dataset. As depicted in Figure 8, existing dataset contains a limited number of image-question pairs. By fully utilizing the visual information from the images, we are able to generate a wider variety of questions from different perspectives, thereby enhancing the diversity of the problem set. The generated questions are created in a few-shot manner, referencing the format of existing question types. Consequently, these questions encompass more than just isolated visual content; they involve reasoning with the images. Moreover, the inclusion of complex questions, logically consistent rephrased questions, and simplified, underspecified questions increases the diversity and robustness of the dataset in terms of both question depth and format, compared to the original set of questions.</p>
<p>Interestingly, Multimodal Language Models (MLLMs) demonstrate biases when handling multimodal mathematical reasoning tasks, particularly with logically consistent rephrased or underspecified questions. As illustrated at the top of Figure 9, LLaVA-1.5 exhibits the ability to answer the original question correctly but tends to falter with simplified, underspecified questions. In contrast, Math-LLaVA proves to be more robust, consistently providing correct answers to underspecified questions. This trend is also observed with logically consistent rephrased questions. Therefore, the use of logically consistent and simplified underspecified questions for data augmentation can enhance the robustness of MLLMs in mathematical reasoning tasks.</p>
<h1>Grade Level: College</h1>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.
Question: Based on the image, what is the most likely equilibrium population count?
Choices: (A) 40 (B) 60 (C) 80 (D) 100
LLaVA-1.5: The answer is (A) $40 \times$ Math-LLaVA: The answer is (C) $80 \checkmark$</p>
<p>Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.
Question: Is the function (f: R to R) surjective?
Choices: (A) Yes (B) No
LLaVA-1.5: The answer is (B) No $X \quad$ Math-LLaVA: The answer is (A) Yes $\checkmark$
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Hint: Please answer the question requiring an integer answer and provide the final value, e.g., $1,2,3$, at the end.
Question: How many methods in the table achieve an A-847 score higher than 20.0 ?
LLaVA-1.5: The answer is $1 \times$
Math-LLaVA: The answer is $3 \checkmark$</p>
<h2>Grade Level: High School</h2>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Hint: Please answer the question requiring an integer answer and provide the final value, e.g., 1, 2, 3, at the end.
Question: An administrator at the Department of Motor Vehicles (DMV) tracked the average wait time from month to month. According to the table, what was the rate of change between August and September? (Unit: minutes per month)
LLaVA-1.5: The answer is $11.5 \times$ Math-LLaVA: The answer is $-3 \checkmark$</p>
<p>Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.
Question: Find x.
Choices: (A) 10 (B) 11 (C) 12 (D) 13
LLaVA-1.5: The answer is (B) $11 \times$ Math-LLaVA: The answer is (C) $12 \checkmark$</p>
<p>Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.</p>
<p>Question: Use a sector paper sheet with a central angle of 120.0 and a radius of 6.0 to roll into a conical bottomless paper cap (as shown in the picture), then the bottom perimeter of the paper cap is ()
Choices: (A) $2 \pi \mathrm{~cm}$ (B) $3 \pi \mathrm{~cm}$ (C) $4 \pi \mathrm{~cm}$ (D) $5 \pi \mathrm{~cm}$
LLaVA-1.5: The answer is (B) $3 \pi \mathrm{~cm} \times$
Math-LLaVA: The answer is (C) $4 \pi \mathrm{~cm} \checkmark$
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.</p>
<p>Question: If you wanted the leaf with the least main veins, which would you choose?
Choices: (A) 3 main veins (B) pinnate (C) reticulate (D) palmate
LLaVA-1.5: The answer is (B) pinnate $X$
Math-LLaVA: The answer is (A) 3 main veins $\checkmark$</p>
<p>Figure 7: Cases on college and high school difficulty problems of MathVista, Math-LLaVA can solve better compared to LLaVA-1.5.</p>
<h1>Cases of Questions Generation</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">purple T-shirt</th>
<th style="text-align: left;">$\$ 8.03$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">denim skirt</td>
<td style="text-align: left;">$\$ 8.67$</td>
</tr>
<tr>
<td style="text-align: left;">purple dress</td>
<td style="text-align: left;">$\$ 7.67$</td>
</tr>
<tr>
<td style="text-align: left;">pair of jeans</td>
<td style="text-align: left;">$\$ 3.81$</td>
</tr>
<tr>
<td style="text-align: left;">gray sweatshirt</td>
<td style="text-align: left;">$\$ 9.74$</td>
</tr>
</tbody>
</table>
<h2>Original Question:</h2>
<p>Clara has $\$ 13.00$. Does she have enough to buy a pair of jeans and a gray sweatshirt?</p>
<h2>New Asked Question:</h2>
<ul>
<li>How much would it cost to buy one of each clothing item listed?</li>
<li>Can Clara afford to buy both a denim skirt and a purple dress with $\$ 13.00$ ?</li>
<li>If Clara buys two pairs of jeans, how much money will she have left?</li>
<li>What is the average cost of the items listed?</li>
</ul>
<h2>More Complex Question:</h2>
<p>Clara has $\$ 13.00$. If she buys a pair of jeans and a gray sweatshirt, how much money will she have left?</p>
<h2>Rephrased Question:</h2>
<p>With $\$ 13.00$, does Clara have sufficient funds to purchase both a pair of jeans and a gray sweatshirt?</p>
<h2>Simplified Question:</h2>
<p>Clara has $\$ 13.00$. Does she have enough to buy some jeans and a sweatshirt?
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<h2>Original Question:</h2>
<p>Tommy is writing a letter one evening. The clock shows the time. What time is it?</p>
<h2>New Asked Question:</h2>
<ul>
<li>How many minutes past 7 o'clock is it?</li>
<li>If Tommy started writing his letter at 7:00, how many minutes has he been writing?</li>
<li>If Tommy needs to finish his letter by 8:00, how many minutes does he have left to write?</li>
<li>If the clock is 10 minutes fast, what is the actual time?</li>
</ul>
<h2>More Complex Question:</h2>
<p>If Tommy started writing the letter at 7:30 P.M. and finished it 2 hours and 45 minutes later, what time did he finish writing the letter?</p>
<h2>Rephrased Question:</h2>
<p>What is the current time shown on the clock while Tommy is writing a letter in the evening?</p>
<h2>Simplified Question:</h2>
<p>The clock shows something one evening. What time is it?</p>
<h2>Original Question:</h2>
<p>What is the difference between domestic and cross border in 2018?</p>
<h2>New Asked Question:</h2>
<ul>
<li>What is the total number of M\&amp;A deals in 2019?</li>
<li>What is the percentage increase in domestic M\&amp;A deals from 2015 to 2016?</li>
<li>What is the average number of cross-border M\&amp;A deals from 2015 to 2017?</li>
<li>In which year did cross-border M\&amp;A deals surpass domestic deals?</li>
<li>What is the difference in the number of domestic M\&amp;A deals between the years 2018 and 2020?</li>
</ul>
<h2>More Complex Question:</h2>
<p>What is the total number of domestic and cross-border deals combined for the years 2018 and 2022?</p>
<h2>Rephrased Question:</h2>
<p>In 2018, what is the contrast between domestic and crossborder?</p>
<h2>Simplified Question:</h2>
<p>What is the difference between these two categories in 2018?
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<h2>Original Question:</h2>
<p>Find $x$. Round to the nearest tenth.</p>
<h2>New Asked Question:</h2>
<ul>
<li>What is the length of side FG in the triangle GFH?</li>
<li>What is the measure of angle HGF in the triangle GFH?</li>
<li>What is the length of side FH in the triangle GFH?</li>
<li>Find the length of GH. Round to the nearest tenth.</li>
<li>What is the perimeter of triangle GFH?</li>
</ul>
<h2>More Complex Question:</h2>
<p>What is the area of triangle GFH?</p>
<h2>Rephrased Question:</h2>
<p>Determine the value of $x$ and round it to the nearest tenth.</p>
<h2>Simplified Question:</h2>
<p>Determine $x$.</p>
<h1>Inference for Underspecified Question</h1>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Original Question: What is the highest amount this class measures?
LLaVA-1.5: $400 \checkmark \quad$ Math-LLaVA: $400 \checkmark$
Underspecified Question: What amount can this measure up to?
LLaVA-1.5: $3000 \times$ Math-LLaVA: $400 \checkmark$</p>
<p>Original Question: Look at the table. Then answer the question. At a price of $\$ 320$, is there a shortage or a surplus? Choices: (A) shortage (B) surplus
LLaVA-1.5: (A) shortage $\checkmark$ Math-LLaVA: (A) shortage $\checkmark$
Underspecified Question: At a price of $\$ 320$, what is the market situation?
LLaVA-1.5: (B) surplus $\times \quad$ Math-LLaVA: (A) shortage $\checkmark$
Original Question: What is the value of the smallest individual element in the whole chart?
LLaVA-1.5: $1 \checkmark \quad$ Math-LLaVA: $1 \checkmark$
Underspecified Question: What is the smallest element value?
LLaVA-1.5: $0 \times \quad$ Math-LLaVA: $1 \checkmark$</p>
<p>Original Question: Which year has the least difference between the used and new cars?
LLaVA-1.5: $2015 \checkmark \quad$ Math-LLaVA: $2015 \checkmark$
Underspecified Question: Which year has the least difference between these two types?
LLaVA-1.5: $2014 \times \quad$ Math-LLaVA: $2015 \checkmark$</p>
<h2>Inference for Rephrased Question</h2>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Original Question: Which region is larger? R1 or R2?
LLaVA-1.5: R2 $\checkmark$ Math-LLaVA: R2 $\checkmark$
Rephrased Question: Which region, R1 or R2, has a greater area?
LLaVA-1.5: R1 $\times$ Math-LLaVA: R2 $\checkmark$</p>
<p>Original Question: Subtract all blue metal things. Subtract all tiny objects. How many objects are left?
LLaVA-1.5: $4 \checkmark \quad$ Math-LLaVA: $4 \checkmark$
Rephrased Question: Remove all blue metallic items. Remove all small things. What is the number of remaining things?
LLaVA-1.5: $6 \times \quad$ Math-LLaVA: $4 \checkmark$
Original Question: As shown in the figure, AB is a long ladder leaning on the wall, the foot of the ladder B is away from the wall 1.6, the point D on the ladder is away from the wall 1.4 , the length of BD is 0.55 , then the length of the ladder is ()
LLaVA-1.5: $4.40 \checkmark \quad$ Math-LLaVA: $4.40 \checkmark$
Rephrased Question: In the given figure, $A B$ represents a ladder leaning against the wall, with the foot $B$ of the ladder located 1.6 units away from the wall. Point D on the ladder is located 1.4 units away from the wall, and the length of BD is 0.55 units. What is the length of the ladder?
LLaVA-1.5: $4.00 \times \quad$ Math-LLaVA: $4.40 \checkmark$</p>
<p>Figure 9: Examples of testing on underspecified and rephrased questions.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal Contribution.
${ }^{\dagger}$ The corresponding author, email: yi.bin@hotmail.com&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
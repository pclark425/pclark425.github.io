<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6393 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6393</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6393</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-271270574</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.13690v1.pdf" target="_blank">DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving</a></p>
                <p><strong>Paper Abstract:</strong> Solving mathematical problems requires advanced reasoning abilities and presents notable challenges for large language models. Previous works usually synthesize data from proprietary models to augment existing datasets, followed by instruction tuning to achieve top-tier results. However, our analysis of these datasets reveals severe biases towards easy queries, with frequent failures to generate any correct response for the most challenging queries. Hypothesizing that difficult queries are crucial to learn complex reasoning, we propose Difficulty-Aware Rejection Tuning (DART), a method that allocates difficult queries more trials during the synthesis phase, enabling more extensive training on difficult samples. Utilizing DART, we have created new datasets for mathematical problem-solving that focus more on difficult queries and are substantially smaller than previous ones. Remarkably, our synthesis process solely relies on a 7B-sized open-weight model, without reliance on the commonly used proprietary GPT-4. We fine-tune various base models on our datasets ranging from 7B to 70B in size, resulting in a series of strong models called DART-MATH. In comprehensive in-domain and out-of-domain evaluation on 6 mathematical benchmarks, DART-MATH outperforms vanilla rejection tuning significantly, being superior or comparable to previous arts, despite using much smaller datasets and no proprietary models. Furthermore, our results position our synthetic datasets as the most effective and cost-efficient publicly available resources for advancing mathematical problem-solving.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6393.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6393.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3 8B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8-billion-parameter general-purpose decoder-only LLM from Meta used as a base model for instruction tuning in this work; fine-tuned with DART-Math datasets and evaluated on multiple math benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in detail in this paper; described as a general pretrained LLM (standard web and curated corpora used by Meta for Llama3).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K (grade-school arithmetic), MATH (competition math); OOD: CollegeMath, DeepMind-Mathematics, OlympiadBench-Math, TheoremQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step word problems (grade-school arithmetic on GSM8K; competition-level math on MATH and OOD benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language math word problems (text), evaluated via natural-language responses</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>range: grade-school arithmetic (GSM8K, easier) to competition / Olympiad-level hard (MATH, OlympiadBench-Math)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Instruction tuning with chain-of-thought (CoT) used during data synthesis; evaluation used greedy decoding (temperature=0). Models trained with DART synthesized CoT-style responses.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (exact-answer correctness; final-answer matching ground truth)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On Llama3-8B, DART-Math raised MATH accuracy from 21.2% (VRT baseline) to 46.6% and GSM8K from 51.0% (VRT) to 82.5% (reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No mechanistic/neuron-level probing reported. Paper analyzes model behavior via dataset-level statistics: pass@k sampling curves (coverage of correct responses), fail-rate as difficulty proxy, scaling curves across dataset sizes, and effect of one-response coverage; these analyses are used to explain where gains come from rather than direct internal neuron/attention analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Vanilla rejection sampling biases toward easy queries causing low/no coverage for hard queries; models fine-tuned without difficulty-aware sampling underperform on hard items. For inference/synthesis, high sampling temperatures (t >= 1.7) produce nonsense outputs. DART improvements are smaller on already strong / math-specialized models.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Significant gains from DART when scaling training-sample count for general models: DART outperforms vanilla rejection tuning across data scales; larger improvements on general 7–8B models versus math-specialized or very large models; steady improvement with more synthetic training data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6393.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6393.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral 7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7-billion-parameter general-purpose LLM used as a base model for instruction tuning and evaluated on the same math benchmarks as other base models in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper beyond being a general pretrained LLM; standard large-scale web and curated corpora typical for foundation models.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH (primary), GSM8K (in-domain), OOD: CollegeMath, DeepMind-Mathematics, OlympiadBench-Math, TheoremQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step mathematical problem solving (competition-level math and grade-school word problems)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems with chain-of-thought style responses</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>range from grade-school (GSM8K) to difficult competition problems (MATH, OlympiadBench)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Instruction tuning using DART synthetic data; chain-of-thought prompting used during synthesis; evaluation via greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (final-answer correctness)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>DART-Math (Prop2Diff) on Mistral-7B outperforms Mistral-7B VRT baseline by several absolute points (paper reports average ~4.6 points over MMIQC baseline and consistent gains vs VRT; exact per-benchmark numbers in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No direct analysis of internal numeric processing (no neuron/attention probing); analyzed via dataset coverage metrics (pass@k, fail rate), scaling curves, and synthesis cost/coverage trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Same dataset-level failure: vanilla rejection tuning under-covers difficult queries. DART requires more raw samples (higher synthesis cost) to cover hard queries; margin of improvement shrinks for models that are already math-specialized or heavily pretrained on math.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Performance improves steadily with more synthetic training samples; DART yields better scaling than vanilla rejection tuning for Mistral-7B across data sizes tested.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6393.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6393.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeekMath-7B-RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeekMath-7B-RL (math-specialized 7B with RL fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B open-weight math-specialized model used in this work as the sole synthesis agent to generate CoT-style math solutions and to evaluate query difficulty (fail rate).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeekMath-7B-RL</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Math-specialized continual pretraining (paper notes DeepSeekMath has extensive math training/continual pretraining), used here both for generation and difficulty evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Used to synthesize responses for MATH and GSM8K training queries; evaluated via pass@k on MATH500 subset</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>synthesis of multi-step math solutions and answer generation for arithmetic and competition math problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language chain-of-thought responses synthesized for text math problems</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>capable across a wide difficulty range; pass@k experiments show coverage on hard queries with enough sampling</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Chain-of-thought prompts used for generation/synthesis; sampling with top-p=0.95 and temperature tuned (t=1.6 used for synthesis); used for fail-rate computation and rejection sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>pass@k (proportion of queries with at least one correct response among k samples), used also final accuracy after fine-tuning downstream</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Pass@k behavior: paper reports ~90% pass@k for MATH500 when sampling over 100 responses per query (DeepSeekMath-7B-RL can synthesize correct responses for most queries with extensive sampling). In synthesis experiments, fine-tuned DART-Math models based on DeepSeekMath-7B achieved competitive test accuracies (small improvements over VRT for this math-specialized base).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Used as a tool to measure fail rate (difficulty proxy) and to study sampling coverage; no deeper mechanistic probing of numeric representation inside the model reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Even strong 7B synthesis model sometimes needs many samples to find correct solutions for hard queries; synthesis cost can be high (paper reports ~150M samples in their runs). High temperature (>1.7) leads to nonsense outputs. Some queries cannot reach designated correct-response quotas within n_max cap.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>High sampling (large k) dramatically increases coverage (pass@k), enabling recovery of correct responses for hard queries; however, fine-tuning gains from additional synthesized samples are smaller for DeepSeekMath-7B compared to general models (less headroom).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6393.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6393.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3 70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 70-billion-parameter general-purpose LLM from Meta evaluated as a base model for instruction tuning with DART-Math datasets; shows smaller relative gains from DART compared to 7–8B general models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not detailed in this paper; standard large-scale pretraining used by Meta for Llama3 family (likely web + curated corpora).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH, GSM8K, and OOD benchmarks (CollegeMath, DeepMind-Mathematics, OlympiadBench-Math, TheoremQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step problem solving from grade-school to competition level</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language math word problems with CoT-style responses used in training synthesis; greedy decoding at evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>range from easy GSM8K to hard competition problems</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Instruction tuning on DART synthetic datasets; CoT used in synthesis; greedy decoding evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (final-answer correctness)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>DART-Math yields modest improvement over VRT for Llama3-70B (paper reports improvement margins narrow to about ~1 percentage point on average vs VRT for very strong models).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No neuron-level or mechanistic analysis; analysis limited to dataset-coverage, scaling behavior, and marginal gains discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Smaller headroom for gains via response augmentation — i.e., query coverage becomes the bottleneck rather than response quality; vanilla rejection-bias still present but has less impact on these large/pretrained models.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>DART still helps but gains are smaller for the largest models evaluated; improvements saturate earlier compared to 7–8B general models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6393.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6393.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary large language model from OpenAI mentioned in the paper as commonly used by prior works to synthesize high-quality math reasoning data; not used in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (proprietary)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified (proprietary)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not described in this paper; referenced only as an external synthesis agent used in prior works' data augmentation pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Mentioned as being used by prior works to synthesize data for benchmarks like GSM8K and MATH (MetaMath, KPMath-Plus, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step math problem synthesis (used for data augmentation in other studies)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language chain-of-thought synthesis in prior studies</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>varying (used widely to generate solutions across difficulty levels in other datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Referenced broadly in context of prior synthetic-data work; chain-of-thought and rejection-sampling pipelines in related literature often used GPT-4 as the generator.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>N/A within this paper (GPT-4 is not evaluated here)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No internal analysis provided in this paper; GPT-4 is only cited as a commonly used external synthesis agent in prior works' pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not directly analyzed here; paper notes many prior synthetic datasets that used GPT-4 suffer bias toward easy queries due to vanilla rejection sampling procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Not applicable within this paper (mentioned as expensive/proprietary synthesis source used by other works).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6393.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6393.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DART / DARS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Difficulty-Aware Rejection Tuning (DART) / Difficulty-Aware Rejection Sampling (DARS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of methods introduced in this paper to bias rejection-sampling synthesis toward harder queries: DARS-Uniform (collect ku correct responses per query) and DARS-Prop2Diff (allocate correct responses proportional to difficulty), used to create DART-Math datasets for instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>method (data synthesis / instruction-tuning pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>method / training-data synthesis strategy (not a neural model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Synthesizes CoT-style responses using an open-weight synthesis model (DeepSeekMath-7B-RL) over existing training queries (MATH and GSM8K) to produce datasets (~590K samples) biased toward harder queries.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Datasets synthesized for training and evaluated on GSM8K, MATH, CollegeMath, DeepMind-Mathematics, OlympiadBench-Math, TheoremQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>improving multi-step math word-problem solving via better synthetic training data</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language CoT responses paired with original queries</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Specifically targets improved coverage of hard queries (MATH Level 5 and other high-difficulty items); defines difficulty via fail rate (proportion of incorrect responses when sampling nd responses)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Uses chain-of-thought prompting during synthesis; sampling with top-p=0.95 and tuned temperature (t=1.6) to increase diversity; enforces ku or kp correct responses per query (with n_max cap and optional one-response coverage)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>downstream accuracy on math benchmarks; dataset synthesis metrics: pass@k coverage, fail rate, ratio r of queries achieving designated correct-response quotas</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>DART-Math datasets used to finetune models produced substantial improvements vs vanilla rejection tuning: e.g., Llama3-8B MATH: 21.2% -> 46.6%; GSM8K: 51.0% -> 82.5%; typical average improvement vs VRT ~3.5–4.5 points on 7–8B models and 5.2–9.5 absolute points on some OOD benchmarks for Prop2Diff.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper introduces fail rate as an automatic difficulty metric and uses pass@k curves to show that open-weight models can synthesize correct responses for most queries given enough samples; analyzes dataset-level biases (easy-query over-representation under vanilla rejection sampling), scaling curves, effect of one-response coverage, and synthesis cost as functions of n_max and achieved ratio r. No neuron-level interpretability or probing of numeric token processing is performed.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>DARS requires many raw samples to reach quotas for very hard queries (high synthesis cost); if ku/kp set poorly, Prop2Diff can undersample easy queries (mitigated by enforcing at least one synthetic response); some queries cannot reach designated quotas within n_max leading to incomplete coverage; high-temperature synthesis can yield nonsense outputs; fail-rate metric may be suboptimal for difficulty estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>DART datasets yield better scaling of downstream performance than vanilla rejection tuning for general 7–8B models; improvements grow with training data size, though marginal gains shrink for math-specialized or very large models where pretraining already covers many math skills (i.e., query coverage becomes the bottleneck).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deepseekmath: Pushing the limits of mathematical reasoning in open language models <em>(Rating: 2)</em></li>
                <li>Measuring mathematical problem solving with the math dataset <em>(Rating: 2)</em></li>
                <li>GSM8K: Grade School Math Dataset (Cobbe et al., 2021a) <em>(Rating: 2)</em></li>
                <li>MetaMath: Bootstrap your own mathematical questions for large language models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>KPMath-Plus: Key-point-driven data synthesis with its enhancement on mathematical reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6393",
    "paper_id": "paper-271270574",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "Llama3-8B",
            "name_full": "Llama 3 8B",
            "brief_description": "An 8-billion-parameter general-purpose decoder-only LLM from Meta used as a base model for instruction tuning in this work; fine-tuned with DART-Math datasets and evaluated on multiple math benchmarks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama3-8B",
            "model_family": "decoder-only transformer",
            "model_size": "8B",
            "training_data_description": "Not specified in detail in this paper; described as a general pretrained LLM (standard web and curated corpora used by Meta for Llama3).",
            "benchmark_name": "GSM8K (grade-school arithmetic), MATH (competition math); OOD: CollegeMath, DeepMind-Mathematics, OlympiadBench-Math, TheoremQA",
            "task_type": "multi-step word problems (grade-school arithmetic on GSM8K; competition-level math on MATH and OOD benchmarks)",
            "problem_format": "natural-language math word problems (text), evaluated via natural-language responses",
            "difficulty_level": "range: grade-school arithmetic (GSM8K, easier) to competition / Olympiad-level hard (MATH, OlympiadBench-Math)",
            "prompting_method": "Instruction tuning with chain-of-thought (CoT) used during data synthesis; evaluation used greedy decoding (temperature=0). Models trained with DART synthesized CoT-style responses.",
            "performance_metric": "accuracy (exact-answer correctness; final-answer matching ground truth)",
            "performance_value": "On Llama3-8B, DART-Math raised MATH accuracy from 21.2% (VRT baseline) to 46.6% and GSM8K from 51.0% (VRT) to 82.5% (reported in paper)",
            "internal_analysis": "No mechanistic/neuron-level probing reported. Paper analyzes model behavior via dataset-level statistics: pass@k sampling curves (coverage of correct responses), fail-rate as difficulty proxy, scaling curves across dataset sizes, and effect of one-response coverage; these analyses are used to explain where gains come from rather than direct internal neuron/attention analysis.",
            "failure_modes": "Vanilla rejection sampling biases toward easy queries causing low/no coverage for hard queries; models fine-tuned without difficulty-aware sampling underperform on hard items. For inference/synthesis, high sampling temperatures (t &gt;= 1.7) produce nonsense outputs. DART improvements are smaller on already strong / math-specialized models.",
            "scaling_trend": "Significant gains from DART when scaling training-sample count for general models: DART outperforms vanilla rejection tuning across data scales; larger improvements on general 7–8B models versus math-specialized or very large models; steady improvement with more synthetic training data.",
            "uuid": "e6393.0",
            "source_info": {
                "paper_title": "DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Mistral-7B",
            "name_full": "Mistral 7B",
            "brief_description": "A 7-billion-parameter general-purpose LLM used as a base model for instruction tuning and evaluated on the same math benchmarks as other base models in the paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral-7B",
            "model_family": "decoder-only transformer",
            "model_size": "7B",
            "training_data_description": "Not specified in this paper beyond being a general pretrained LLM; standard large-scale web and curated corpora typical for foundation models.",
            "benchmark_name": "MATH (primary), GSM8K (in-domain), OOD: CollegeMath, DeepMind-Mathematics, OlympiadBench-Math, TheoremQA",
            "task_type": "multi-step mathematical problem solving (competition-level math and grade-school word problems)",
            "problem_format": "natural-language word problems with chain-of-thought style responses",
            "difficulty_level": "range from grade-school (GSM8K) to difficult competition problems (MATH, OlympiadBench)",
            "prompting_method": "Instruction tuning using DART synthetic data; chain-of-thought prompting used during synthesis; evaluation via greedy decoding.",
            "performance_metric": "accuracy (final-answer correctness)",
            "performance_value": "DART-Math (Prop2Diff) on Mistral-7B outperforms Mistral-7B VRT baseline by several absolute points (paper reports average ~4.6 points over MMIQC baseline and consistent gains vs VRT; exact per-benchmark numbers in paper tables).",
            "internal_analysis": "No direct analysis of internal numeric processing (no neuron/attention probing); analyzed via dataset coverage metrics (pass@k, fail rate), scaling curves, and synthesis cost/coverage trade-offs.",
            "failure_modes": "Same dataset-level failure: vanilla rejection tuning under-covers difficult queries. DART requires more raw samples (higher synthesis cost) to cover hard queries; margin of improvement shrinks for models that are already math-specialized or heavily pretrained on math.",
            "scaling_trend": "Performance improves steadily with more synthetic training samples; DART yields better scaling than vanilla rejection tuning for Mistral-7B across data sizes tested.",
            "uuid": "e6393.1",
            "source_info": {
                "paper_title": "DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "DeepSeekMath-7B-RL",
            "name_full": "DeepSeekMath-7B-RL (math-specialized 7B with RL fine-tuning)",
            "brief_description": "A 7B open-weight math-specialized model used in this work as the sole synthesis agent to generate CoT-style math solutions and to evaluate query difficulty (fail rate).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeekMath-7B-RL",
            "model_family": "decoder-only transformer",
            "model_size": "7B",
            "training_data_description": "Math-specialized continual pretraining (paper notes DeepSeekMath has extensive math training/continual pretraining), used here both for generation and difficulty evaluation.",
            "benchmark_name": "Used to synthesize responses for MATH and GSM8K training queries; evaluated via pass@k on MATH500 subset",
            "task_type": "synthesis of multi-step math solutions and answer generation for arithmetic and competition math problems",
            "problem_format": "natural-language chain-of-thought responses synthesized for text math problems",
            "difficulty_level": "capable across a wide difficulty range; pass@k experiments show coverage on hard queries with enough sampling",
            "prompting_method": "Chain-of-thought prompts used for generation/synthesis; sampling with top-p=0.95 and temperature tuned (t=1.6 used for synthesis); used for fail-rate computation and rejection sampling.",
            "performance_metric": "pass@k (proportion of queries with at least one correct response among k samples), used also final accuracy after fine-tuning downstream",
            "performance_value": "Pass@k behavior: paper reports ~90% pass@k for MATH500 when sampling over 100 responses per query (DeepSeekMath-7B-RL can synthesize correct responses for most queries with extensive sampling). In synthesis experiments, fine-tuned DART-Math models based on DeepSeekMath-7B achieved competitive test accuracies (small improvements over VRT for this math-specialized base).",
            "internal_analysis": "Used as a tool to measure fail rate (difficulty proxy) and to study sampling coverage; no deeper mechanistic probing of numeric representation inside the model reported.",
            "failure_modes": "Even strong 7B synthesis model sometimes needs many samples to find correct solutions for hard queries; synthesis cost can be high (paper reports ~150M samples in their runs). High temperature (&gt;1.7) leads to nonsense outputs. Some queries cannot reach designated correct-response quotas within n_max cap.",
            "scaling_trend": "High sampling (large k) dramatically increases coverage (pass@k), enabling recovery of correct responses for hard queries; however, fine-tuning gains from additional synthesized samples are smaller for DeepSeekMath-7B compared to general models (less headroom).",
            "uuid": "e6393.2",
            "source_info": {
                "paper_title": "DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Llama3-70B",
            "name_full": "Llama 3 70B",
            "brief_description": "A 70-billion-parameter general-purpose LLM from Meta evaluated as a base model for instruction tuning with DART-Math datasets; shows smaller relative gains from DART compared to 7–8B general models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama3-70B",
            "model_family": "decoder-only transformer",
            "model_size": "70B",
            "training_data_description": "Not detailed in this paper; standard large-scale pretraining used by Meta for Llama3 family (likely web + curated corpora).",
            "benchmark_name": "MATH, GSM8K, and OOD benchmarks (CollegeMath, DeepMind-Mathematics, OlympiadBench-Math, TheoremQA)",
            "task_type": "multi-step problem solving from grade-school to competition level",
            "problem_format": "natural-language math word problems with CoT-style responses used in training synthesis; greedy decoding at evaluation",
            "difficulty_level": "range from easy GSM8K to hard competition problems",
            "prompting_method": "Instruction tuning on DART synthetic datasets; CoT used in synthesis; greedy decoding evaluation",
            "performance_metric": "accuracy (final-answer correctness)",
            "performance_value": "DART-Math yields modest improvement over VRT for Llama3-70B (paper reports improvement margins narrow to about ~1 percentage point on average vs VRT for very strong models).",
            "internal_analysis": "No neuron-level or mechanistic analysis; analysis limited to dataset-coverage, scaling behavior, and marginal gains discussion.",
            "failure_modes": "Smaller headroom for gains via response augmentation — i.e., query coverage becomes the bottleneck rather than response quality; vanilla rejection-bias still present but has less impact on these large/pretrained models.",
            "scaling_trend": "DART still helps but gains are smaller for the largest models evaluated; improvements saturate earlier compared to 7–8B general models.",
            "uuid": "e6393.3",
            "source_info": {
                "paper_title": "DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "A proprietary large language model from OpenAI mentioned in the paper as commonly used by prior works to synthesize high-quality math reasoning data; not used in this paper's experiments.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_family": "decoder-only transformer (proprietary)",
            "model_size": "not specified (proprietary)",
            "training_data_description": "Not described in this paper; referenced only as an external synthesis agent used in prior works' data augmentation pipelines.",
            "benchmark_name": "Mentioned as being used by prior works to synthesize data for benchmarks like GSM8K and MATH (MetaMath, KPMath-Plus, etc.)",
            "task_type": "multi-step math problem synthesis (used for data augmentation in other studies)",
            "problem_format": "natural-language chain-of-thought synthesis in prior studies",
            "difficulty_level": "varying (used widely to generate solutions across difficulty levels in other datasets)",
            "prompting_method": "Referenced broadly in context of prior synthetic-data work; chain-of-thought and rejection-sampling pipelines in related literature often used GPT-4 as the generator.",
            "performance_metric": "N/A within this paper (GPT-4 is not evaluated here)",
            "performance_value": null,
            "internal_analysis": "No internal analysis provided in this paper; GPT-4 is only cited as a commonly used external synthesis agent in prior works' pipelines.",
            "failure_modes": "Not directly analyzed here; paper notes many prior synthetic datasets that used GPT-4 suffer bias toward easy queries due to vanilla rejection sampling procedures.",
            "scaling_trend": "Not applicable within this paper (mentioned as expensive/proprietary synthesis source used by other works).",
            "uuid": "e6393.4",
            "source_info": {
                "paper_title": "DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "DART / DARS",
            "name_full": "Difficulty-Aware Rejection Tuning (DART) / Difficulty-Aware Rejection Sampling (DARS)",
            "brief_description": "A family of methods introduced in this paper to bias rejection-sampling synthesis toward harder queries: DARS-Uniform (collect ku correct responses per query) and DARS-Prop2Diff (allocate correct responses proportional to difficulty), used to create DART-Math datasets for instruction tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "method (data synthesis / instruction-tuning pipeline)",
            "model_family": "method / training-data synthesis strategy (not a neural model)",
            "model_size": "N/A",
            "training_data_description": "Synthesizes CoT-style responses using an open-weight synthesis model (DeepSeekMath-7B-RL) over existing training queries (MATH and GSM8K) to produce datasets (~590K samples) biased toward harder queries.",
            "benchmark_name": "Datasets synthesized for training and evaluated on GSM8K, MATH, CollegeMath, DeepMind-Mathematics, OlympiadBench-Math, TheoremQA",
            "task_type": "improving multi-step math word-problem solving via better synthetic training data",
            "problem_format": "natural-language CoT responses paired with original queries",
            "difficulty_level": "Specifically targets improved coverage of hard queries (MATH Level 5 and other high-difficulty items); defines difficulty via fail rate (proportion of incorrect responses when sampling nd responses)",
            "prompting_method": "Uses chain-of-thought prompting during synthesis; sampling with top-p=0.95 and tuned temperature (t=1.6) to increase diversity; enforces ku or kp correct responses per query (with n_max cap and optional one-response coverage)",
            "performance_metric": "downstream accuracy on math benchmarks; dataset synthesis metrics: pass@k coverage, fail rate, ratio r of queries achieving designated correct-response quotas",
            "performance_value": "DART-Math datasets used to finetune models produced substantial improvements vs vanilla rejection tuning: e.g., Llama3-8B MATH: 21.2% -&gt; 46.6%; GSM8K: 51.0% -&gt; 82.5%; typical average improvement vs VRT ~3.5–4.5 points on 7–8B models and 5.2–9.5 absolute points on some OOD benchmarks for Prop2Diff.",
            "internal_analysis": "Paper introduces fail rate as an automatic difficulty metric and uses pass@k curves to show that open-weight models can synthesize correct responses for most queries given enough samples; analyzes dataset-level biases (easy-query over-representation under vanilla rejection sampling), scaling curves, effect of one-response coverage, and synthesis cost as functions of n_max and achieved ratio r. No neuron-level interpretability or probing of numeric token processing is performed.",
            "failure_modes": "DARS requires many raw samples to reach quotas for very hard queries (high synthesis cost); if ku/kp set poorly, Prop2Diff can undersample easy queries (mitigated by enforcing at least one synthetic response); some queries cannot reach designated quotas within n_max leading to incomplete coverage; high-temperature synthesis can yield nonsense outputs; fail-rate metric may be suboptimal for difficulty estimation.",
            "scaling_trend": "DART datasets yield better scaling of downstream performance than vanilla rejection tuning for general 7–8B models; improvements grow with training data size, though marginal gains shrink for math-specialized or very large models where pretraining already covers many math skills (i.e., query coverage becomes the bottleneck).",
            "uuid": "e6393.5",
            "source_info": {
                "paper_title": "DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models",
            "rating": 2,
            "sanitized_title": "deepseekmath_pushing_the_limits_of_mathematical_reasoning_in_open_language_models"
        },
        {
            "paper_title": "Measuring mathematical problem solving with the math dataset",
            "rating": 2,
            "sanitized_title": "measuring_mathematical_problem_solving_with_the_math_dataset"
        },
        {
            "paper_title": "GSM8K: Grade School Math Dataset (Cobbe et al., 2021a)",
            "rating": 2,
            "sanitized_title": "gsm8k_grade_school_math_dataset_cobbe_et_al_2021a"
        },
        {
            "paper_title": "MetaMath: Bootstrap your own mathematical questions for large language models",
            "rating": 2,
            "sanitized_title": "metamath_bootstrap_your_own_mathematical_questions_for_large_language_models"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "KPMath-Plus: Key-point-driven data synthesis with its enhancement on mathematical reasoning",
            "rating": 1,
            "sanitized_title": "kpmathplus_keypointdriven_data_synthesis_with_its_enhancement_on_mathematical_reasoning"
        }
    ],
    "cost": 0.014393749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving
18 Jun 2024</p>
<p>Yuxuan Tong 
7 38.8 38.7 37.4 35.2 34.3 35.6 34.2 30.8 28.9Llama3-8B 39</p>
<p>Xiwen Zhang 
7 38.8 38.7 37.4 35.2 34.3 35.6 34.2 30.8 28.9Llama3-8B 39</p>
<p>Rui Wang 
7 38.8 38.7 37.4 35.2 34.3 35.6 34.2 30.8 28.9Llama3-8B 39</p>
<p>Ruidong Wu 
7 38.8 38.7 37.4 35.2 34.3 35.6 34.2 30.8 28.9Llama3-8B 39</p>
<p>Junxian He junxianh@cse.ust.hk 
7 38.8 38.7 37.4 35.2 34.3 35.6 34.2 30.8 28.9Llama3-8B 39</p>
<p>Tsinghua University 
7 38.8 38.7 37.4 35.2 34.3 35.6 34.2 30.8 28.9Llama3-8B 39</p>
<p>Helixon Research 
7 38.8 38.7 37.4 35.2 34.3 35.6 34.2 30.8 28.9Llama3-8B 39</p>
<p>DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving
18 Jun 202462C9568B9B8F1FE6058C718C4BE90323arXiv:2407.13690v1[cs.CL]
Solving mathematical problems requires advanced reasoning abilities and presents notable challenges for large language models.Previous works usually synthesize data from proprietary models to augment existing datasets, followed by instruction tuning to achieve top-tier results.However, our analysis of these datasets reveals severe biases towards easy queries, with frequent failures to generate any correct response for the most challenging queries.Hypothesizing that difficult queries are crucial to learn complex reasoning, we propose Difficulty-Aware Rejection Tuning (DART), a method that allocates difficult queries more trials during the synthesis phase, enabling more extensive training on difficult samples.Utilizing DART, we have created new datasets for mathematical problem-solving that focus more on difficult queries and are substantially smaller than previous ones.Remarkably, our synthesis process solely relies on a 7B-sized open-weight model, without reliance on the commonly used proprietary GPT-4.We fine-tune various base models on our datasets ranging from 7B to 70B in size, resulting in a series of strong models called DART-Math.In comprehensive in-domain and out-of-domain evaluation on 6 mathematical benchmarks, DART-Math outperforms vanilla rejection tuning significantly, being superior or comparable to previous arts, despite using much smaller datasets and no proprietary models.Furthermore, our results position our synthetic datasets as the most effective and cost-efficient publicly available resources for advancing mathematical problem-solving. 1 * Work done during visit to HKUST. 1 Our datasets, models and code are publicly available at https://github.com/hkust-nlp/dart-math.Preprint.Under review.</p>
<p>Introduction</p>
<p>Recent years have seen remarkable advancements in various tasks through the use of large language models (LLMs) (Brown et al., 2020;Touvron et al., 2023;Chowdhery et al., 2023;Anthropic, 2023;OpenAI et al., 2023).However, these models still struggle with complex reasoning (Hendrycks et al., 2021;Jimenez et al., 2024;He et al., 2024;Lin et al., 2024), a cornerstone of human cognitive essential for tackling intricate tasks.Mathematical reasoning, in particular, represents a significant challenge and stands as one of the most difficult categories of reasoning for state-of-the-art LLMs (Hendrycks et al., 2021;Cobbe et al., 2021b;Zheng et al., 2022).</p>
<p>In this work, we focus on mathematical problem-solving to explore enhancement of the mathematical reasoning abilities of pretrained LLMs.We investigate instruction tuning (Longpre et al., 2023;Wang et al., 2023), which is recognized as the most cost-effective method and achieves the state-of-theart performance on various mathematical benchmarks (Yu et al., 2024;Yue et al., 2024).Current SOTA instruction tuning methods for mathematical problem-solving are typically implemented as</p>
<p>VRT Uniform Prop2Diff</p>
<p>Figure 1: Left: Average accuracy on six mathematical benchmarks.We compare with models fine-tuned on the best, public instruction tuning datasets for mathematical problem-solving: MetaMath (Yu et al., 2024) with 395K examples, MMIQC (Liu et al., 2024a) (Hendrycks et al., 2021).VRT is the baseline biased towards easy queries, while Uniform and Prop2Diff are proposed in this work to balance and bias towards difficult queries respectively.Points are slightly shifted and downsampled for clarity.</p>
<p>augmenting existing training datasets with synthetic data generated from proprietary models like GPT-4 (OpenAI et al., 2023).A prevalent method of data augmentation is to sample multiple responses to given queries from a strong model and filter out the incorrect ones.This method, known as rejection tuning, ensures the high quality of the augmented thought steps and yields competitive performance (Yuan et al., 2023;Yu et al., 2024;Singh et al., 2023).</p>
<p>However, after careful examination of these SOTA synthetic datasets, we find that they suffer from a severe bias towards responses to easy queries and low coverage for hard queries.For example, as shown in Figure 2 (Left and Middle), while the original queries vary in difficulty, the augmented samples in the MetaMathQA dataset (Yu et al., 2024) focus more on easier queries, with zero new responses generated for 51.1% of the most difficult training queries in the MATH training set (Hendrycks et al., 2021).This phenomenon commonly exists in rejection-sampling-based data synthesis which typically samples an equal number of raw responses for each query, disadvantaging difficult queries that are less likely to yield correct responses.We hypothesize that such biases hinder the learning of mathematical problem-solving, since difficult examples are often deemed more crucial during training (Sorscher et al., 2022;Burns et al., 2023;Liu et al., 2024b).</p>
<p>To address this issue, we propose Difficulty-Aware Rejecting Tuning (DART), a method that prioritizes more sampling trials for challenging queries, thereby generating synthetic datasets enriched with more responses for difficult questions compared to previous methods.Specifically, we develop two strategies to achieve this: Uniform which collects the same number of correct responses for all queries, and Prop2Diff which biases the data samples towards the difficult queries, contrasting with vanilla rejection tuning.These different strategies are summarized in Figure 1 (Right), where the difficulty of a query is automatically assessed by sampling multiple responses and calculating the ratio of incorrect answers.Our difficulty-aware synthesis produces two synthetic datasets corresponding to Uniform and Prop2Diff strategies respectively, consisting of ∼590K examples.Notably, while previous works mostly utilize  to synthesize data, we only rely on the DeepSeekMath-7B-RL model (Shao et al., 2024) to produce all the data, thereby eliminating dependence on proprietary models.</p>
<p>In our experiments, we evaluate DART based on Mistral-7B (Jiang et al., 2023), DeepSeekMath-7B (Shao et al., 2024), Llama3-8B, andLlama3-70B (Meta, 2024), creating a series of strong mathematical models that termed DART-Math.Across 6 in-domain and challenging out-of-domain benchmarks, DART-Math significantly outperforms vanilla rejection tuning and the baselines trained on the previously established top public datasets as shown in Figure 1 (Left), this is often achieved with smaller training data size.For example, DART-Math improves Llama3-8B from 21.2% to 46.6% on MATH (Hendrycks et al., 2021), and from 51.0% to 82.5% on GSM8K (Cobbe et al., 2021a); Our results mark the DART-Math datasets as the state-of-the-art public resources of instruction tuning for mathematical problem-solving. 2 Biases in Rejection-Based Data Synthesis</p>
<p>In this section, we first introduce the background for rejection sampling and rejection tuning, and then present our examination on the biases of rejection-based data synthesis.</p>
<p>Background: Rejection Sampling and Rejection Tuning</p>
<p>We begin by formulating the data synthesis setting used for instruction tuning.For instruction tuning, the training dataset consists of (x, y) pairs, where x is the input query and y is the response.The process of data synthesis involves generating new (x, y) pairs to augment the original training dataset, thereby enhancing performance.For each input query x i , it is typical to sample M responses from advanced models such as GPT-4, forming the set {(x i , y</p>
<p>i )} M j=1 .In the context of mathematical problem-solving, a subsequent filtering step is often implemented to eliminate incorrect y (j) i .This elimination is based on whether the final answer in the synthetic response aligns with the ground-truth answer.2This is crucial as mathematical reasoning poses a significant challenge for current LLMs, and the generated y (j) i may often be of poor quality.This method of response sampling is known as rejection sampling, and the subsequent fine-tuning process is referred to as rejection tuning, which is widely employed to enhance the mathematical problem-solving abilities of LLMs (Yuan et al., 2023;Yu et al., 2024;Singh et al., 2023;Xu et al., 2024).In addition to response synthesis, the queries are typically kept constant (Singh et al., 2023;Hosseini et al., 2024;Toshniwal et al., 2024) or altered in a controlled manner (Yu et al., 2024) to ensure that ground-truth answers are readily available, which facilitates the implementation of rejection sampling.While some studies also synthesize queries without utilizing rejection tuning (Li et al., 2024;Tang et al., 2024), our focus in this work is primarily on rejection tuning, a method prevalently used for advancing the mathematical skills of LLMs.</p>
<p>On the Imbalance of Rejection-Based Data Synthesis</p>
<p>Next, we examine a representative synthetic dataset to identify the inherent biases present in rejectionbased data synthesis as implemented in most existing works.Specifically, our analysis focuses on the AnsAug subset of the MetaMathQA-MATH dataset (Yu et al., 2024), which is a synthetic dataset that produces multiple responses for each query in the original training set of the MATH dataset (Hendrycks et al., 2021), through rejection sampling as described in §2.1.MetaMathQA has been recognized as one of the most effective synthetic datasets for mathematical problem-solving.We concentrate on the MATH split because it is a notably challenging benchmark in mathematical reasoning, equipped with human-annotated difficulty levels that aid in our analysis.</p>
<p>Rejection-based data synthesis biases towards easy queries: Across different difficulty levels, Figure 2 (Left) shows the original query distribution of the MATH training dataset as well as the new query distribution after synthesis in the MetaMathQA-Math dataset.While the most difficult queries (Level 5) takes the largest proportion in the original query set, MetaMathQA changes the query distribution implicitly towards easier queries, dropping many hard problems.For instance, the proportion of Level 5 (the most difficult) queries notably decreases by 51.1%, indicating that rejection sampling fails to generate any correct response for those queries.As a result, as depicted in Figure 2 (Middle), the responses to the most difficult queries only account for 10.5% of all the samples.Such a phenomenon generally exists in datasets synthesized through the conventional rejection sampling method outlined in §2.1, primarily because the same number of responses is sampled for each query, yet the likelihood of obtaining correct responses for difficult queries is significantly lower, sometimes even zero.We hypothesize that this bias towards easy queries could substantially undermine the effectiveness of instruction tuning, as hard queries are often considered critical for instruction tuning (Lu et al., 2024;Liu et al., 2024b).We note that this bias towards easy queries is less pronounced on relatively simple datasets such as GSM8K (Cobbe et al., 2021a), where most queries are easier and it is not difficult to sample correct responses for most of the queries.However, the bias remains a significant concern when tackling challenging tasks, which represent a more compelling and complex field of study for LLMs.Building on these findings, we will next introduce our method as a potential remedy to the limitations of vanilla rejection tuning.</p>
<p>3 DART -Difficulty-Aware Rejection Tuning 3.1 Open-Weight Models Are Able to Generate Good Responses Intuitively, we aim to collect a sufficient number of responses for the difficult queries.To assess whether this goal is achievable, given that models might not generate correct responses for challenging queries despite extensive sampling, we explore the capabilities of DeepSeekMath-7B-RL (Shao et al., 2024), a strong model specifically trained for mathematical reasoning.Figure 2 (Right) demonstrates the pass@k accuracy on the queries in MATH500 (Lightman et al., 2024), a subset of MATH test set, indicating the proportion of queries that have at least one correct response when sampling k responses for each query.Notably, even though the synthesis model possesses only 7B parameters, a 90% pass@k accuracy can be achieved when sampling over 100 responses per query.These results are consistent with the findings from recent studies (Toshniwal et al., 2024;Shao et al., 2024;Li et al., 2024), which suggest that strong open-weight models are able to synthesize correct responses for most of the queries.This evidence supports the potential for effectively mitigating the insufficient coverage for difficult queries through strategic response sampling, which we introduce next.</p>
<p>DARS -Difficulty-Aware Rejection Sampling</p>
<p>Motivated by the observation above, we aim to collect more responses for harder queries.Specifically, we introduce two strategies to increase the number of correct responses for difficult queries: (1) Uniform, which involves sampling responses for each query until each query accumulates k u correct responses, and k u is a preset hyperparameter determined by the desired size of the synthetic dataset;</p>
<p>(2) Prop2Diff, where we continue sampling responses until the number of correct responses for each query is proportional to its difficulty score.The most challenging queries will receive k p responses and k p is a hyperparameter.This method introduces a deliberate bias in the opposite direction to vanilla rejection sampling, towards more difficult queries.Prop2Diff is inspired by previous works that demonstrate difficult queries can be more effective to enhance model capabilities (Sorscher et al., 2022;Liu et al., 2024b).Both the Uniform and Prop2Diff strategies prescribe a specific number of correct response for each query, determined by k u or k p .Nevertheless, there are certain queries which we cannot sample out the designated number of correct responses even with extensive sampling efforts.To avoid endless running of the synthesis, we impose a cap on the maximum allowable number of raw samples per query as n max -once this limit is reached for a particular query, we cease further sampling and retain any correct responses that have been gathered.The straightforward implementation of the Prop2Diff strategy risks generating no synthetic responses for easier queries if k p is set small.To mitigate this, we guarantee at least one synthetic response for each query when implementing Prop2Diff.While it might seem sufficient to rely on the original, real training dataset to ensure at least one human-annotated response per query, our findings highlight the importance of Dataset # Samples (K) Synthesis Agent Open-Source</p>
<p>WizardMath (Luo et al., 2023) 96 GPT-4 ✗ MetaMathQA (Yu et al., 2024) 395 GPT-3.5 ✓ MMIQC (Liu et al., 2024a) 2294 GPT-4+GPT-3.5+Human✓ Orca-Math (Mitra et al., 2024) 200 GPT-4 ✓ Xwin-Math-V1.1 (Li et al., 2024) 1440 GPT-4 ✗ KPMath-Plus (Huang et al., 2024) 1576 GPT-4 ✗ MathScaleQA (Tang et al., 2024) 2021 GPT-3.maintaining synthetic response coverage to learn to solve easy problems, as we will quantitatively shown in §4.3, partially because the human-annotated response is less detailed and not as beneficial as synthetic responses, demonstrated previously in Yu et al. (2024).For both Uniform and Prop2Diff strategies, we use the DeepSeekMath-7B-RL model to synthesize responses.We refer to the two sampling strategies as DARS-Uniform and DARS-Prop2Diff respectively.DARS-Prop2Diff requires assessing difficulties of queries, next we introduce an automatic approach to measure difficulties.</p>
<p>Evaluating Difficulty: Previous studies have used proprietary models like ChatGPT to assess the difficulty or complexity of data samples (Lu et al., 2024;Liu et al., 2024b).In this work, we introduce a new metric, fail rate -the proportion of incorrect responses when sampling n d responses for a given query -as a proxy for difficulty.This metric aligns with the intuition that harder queries less frequently yield correct responses.We utilize DeepSeekMath-7B-RL as the sampling model to evaluate difficulty across all experiments in the paper.Varying this sampling model to align with the generative model may further enhance performance, which we leave as future work.Notably, one of the benefits of fail rate is that it allows to reuse the sampled responses during difficulty evaluation as synthetic responses for dataset construction.See implementation details in Appendix A.2.</p>
<p>The DART Datasets</p>
<p>We utilize DARS-Uniform and DARS-Prop2Diff to construct two datasets, DART-Math-Uniform and DART-Math-Hard respectively for instruction tuning.We use the original training queries of the GSM8K (Cobbe et al., 2021a) and MATH datasets to synthesize responses.We maintain fixed queries to better isolate the effects of difficulty-aware rejection tuning, while techniques for query augmentation, as discussed in prior studies (Yu et al., 2024), could be potentially incorporated to further improve the performance.The synthetic datasets are augmented with the original GSM8K and MATH training data to form the final datasets.We set k u in DARS-Uniform as 40 and k p in DARS-Prop2Diff as 192 to form both datasets of around 590K samples.Our data samples only involve natural language reasoning without using external tools such as code execution.Comparison of our datasets with previous datasets is illustrated in Table 1.Our datasets are generally smaller than most previous datasets, and in §4.2 we will empirically demonstrate that the DART datasets are the most cost-effective datasets publicly available.Remarkably, our approach solely utilizes DeepSeekMath-7B-RL to evaluate difficulty of queries and synthesize responses, without relying on ChatGPT that is commonly used in other studies.</p>
<p>Our approach typically requires more sampling trials than vanilla rejection sampling to generate a dataset of comparable size because difficult queries often need more samples to secure the required number of correct responses.Despite this, it is crucial to point out that our overall training cost does not exceed that of vanilla instruction tuning.We emphasize that the data synthesis process is a one-time effort.Once the synthetic dataset is created, it can be utilized for multiple training runs across various base models.Furthermore, this dataset will be publicly available, extending its utility to a wide range of users.From this perspective, the initial higher synthesis cost is effectively amortized over numerous training runs and the broad user base, rendering the synthesis cost virtually imperceptible to individual dataset users.We will discuss the synthesis cost further in §4.3.</p>
<p>Experiments</p>
<p>General Setup</p>
<p>Below we summarize the key setup details, while we include more information in Appendix A.</p>
<p>Data synthesis: We synthesize responses using the original training queries of the MATH and GSM8K datasets.As described in §3.2, we utilize DeepSeekMath-7B-RL to synthesize all the data.We use temperature sampling with adjusted temperature to sample answer-correct responses to difficult queries.We set the maximum number of output tokens as 2048 and adopt top-p sampling with p = 0.95.We use chain-of-thought prompt (Wei et al., 2022) to synthesize.We use the vLLM library (Kwon et al., 2023) to accelerate the generation.In our setting, sampling 35K samples on MATH / GSM8k queries takes about 1 NVIDIA A100 GPU hour.</p>
<p>Training: We perform standard instruction tuning on our synthetic datasets DART-Math-Uniform and DART-Math-Hard, based on several base models including Llama3-8B (Meta, 2024), Mistral-7B (Jiang et al., 2023), and Llama3-70B as representatives of general models, and DeepSeekMath-7B (Shao et al., 2024) as the representative of math-specialized models.For simplicity, we keep most hyperparameters the same across different models and datasets, and tune only several key hyperparameters like learning rate and number of epochs, as detailed in Appendix A.1.</p>
<p>Evaluation: For comprehensive assessment of mathematical reasoning of the models, we adopt 6 benchmarks for both in-domain and out-of-domain (OOD) evaluation.Specifically, we use the GSM8K and MATH test set as the in-domain test.GSM8K consists of grade school arithmetic tasks and are considered much simper than MATH that contains challenging competition mathematical problems.For OOD test, we utilize the following four challenging benchmarks:</p>
<p>• CollegeMath (Tang et al., 2024): This test set contains 2818 college-level mathematical problems extracted from 9 textbooks across 7 domains such as linear algebra and differential equations, testing generalization on complex mathematical reasoning in diverse domains.• DeepMind-Mathematics (Saxton et al., 2019): This test set contains 1000 problems from a diverse range of problem types based on a national school mathematics curriculum (up to age 16), testing basic mathematical reasoning in diverse domains.• OlympiadBench-Math (He et al., 2024): This benchmark contains 675 Olympiad-level mathematical problems from competitions, which is a text-only English subset of Olympiad-Bench, testing generalization on the most complex mathematical reasoning.• TheoremQA (Chen et al., 2023): This benchmark contains 800 problems focused on utilizing mathematical theorems to solve challenging problems in fields such as math, physics and engineering, testing generalization on theoretical reasoning in general STEM.</p>
<p>All results are from natural language reasoning without using external tools, through greedy decoding.</p>
<p>Baselines: We compare DART with the state-of-the-art instruction-tuned mathematical models such as MetaMath (Yu et al., 2024), MMIQC (Liu et al., 2024a), KPMah-Plus (Huang et al., 2024), and Xwin-Math (Li et al., 2024).We copy the results directly from the respective papers except for MetaMath and MMIQC, where we run our own training since their datasets are public.As shown in Table 1, these SOTA datasets all rely on proprietary models for data synthesis.Another ablation baseline to DART is vanilla rejection tuning (VRT), where we synthesize a dataset of the same size of 0.59M examples with DeepSeekMath-7B-RL, using vanilla rejection sampling as described in §2.1.We note that there are other strong models such as Yue et al. (2024); Gou et al. (2024) that are trained to solve mathematical problems utilizing code execution, we exclude them since this study focuses on reasoning without using tools.</p>
<p>Main Results</p>
<p>Comparing with Vanilla Rejection Tuning: The main results are in Table 2. DART-Math based on all four different base models outperforms the VRT baselines on most benchmarks consistently.Focusing on performance with 7-8B general base models, DART-Math-Llama3-8B (Uniform) surpasses the VRT baseline across all 6 benchmarks by an average of 3.5 absolute points, while DART-Math-Llama3-8B (Prop2Diff) achieves an average improvement of 4.5 points.Table 2: Main results on mathematical benchmarks.College, DM, Olympiad, Theorem denote the CollegeMath, DeepMind-Mathematics, OlympiadBench-Math, TheoremQA benchmarks respectively.We annotate the absolute accuracy change compared to the VRT baseline within the same base model.Bold means the best score within the respective base model.ICL, MetaMath, MMIQC, and VRT baselines are from our own runs, while other numbers are copied from the respective papers or reports.For WizardMath and Xwin-Math, we take the public model checkpoints and evaluate ourselves using their official CoT prompt.† : For Xwin-Math, we take the best public models that are based on Llama2 (Touvron et al., 2023), which is not a very fair comparison with others.</p>
<p>by nearly 7 absolute points for both Mistral-7B and Llama3-8B models.For OOD benchmarks, DART-Math (Prop2Diff) shows particularly notable gains on more difficult benchmarks, with improvements ranging from 5.2 to 9.5 absolute points on CollegeMath, DeepMind-Mathematics, and OlympiadBench-Math.This indicates effective generalization of our approach.These improvements over the VRT baselines demonstrate the effectiveness of the proposed difficulty-aware rejection sampling.We note that DART-Math does not greatly boost the relatively simple, in-domain GSM8K benchmark.This is expected, as explained in §2.2, because vanilla rejection tuning expected does not face severe bias issues like those seen in more challenging datasets.Thus, difficulty-aware rejection sampling has a limited impact on easy datasets.Interestingly, on much stronger base models DeepSeekMath-7B and Llama3-70B, the improvement margin of DART-Math over VRT narrows, with about a 1-point gain on average.We hypothesize that this is due to these models' extensive pretraining on mathematical content.This pretraining likely covers most skills that could be learned from the GSM8K and MATH training queries, suggesting that the query set itself, rather than the responses, becomes the bottleneck.Thus augmenting the range of queries could be a more effective strategy for future improvements.</p>
<p>Comparison with previous top-performing methods: DART-Math achieves superior or comparable performance to previous best models.Specifically, when compared with MetaMath, DART-Math wins greatly in all cases.Additionally, DART-Math-DSMath-7B achieves the state-of-theart results for models sized 7-8B on challenging benchmarks such as MATH, OlympiadBench-Math, and TheoremQA.On average, DART-Math-Mistral-7B (Prop2Diff) surpasses Mistral-7B-MMIQC by 4.6 absolute points, despite using only a quarter of its training sample size.Compared with concurrent work KPMath-Plus which relies on GPT-4 and has not released either the data or the model, our approach slightly underperforms on Mistral-7B for GSM8K and MATH.However, DART-Math excels against it on DeepSeekMath-7B by a significant margin, utilizing around one-third of its training data size.The Xwin-Math models perform well on the GSM8K benchmark but fall behind DART-Math (Prop2Diff) on other challenging benchmarks overall, particularly with a more pronounced gap on 70B models -although we note that their models are based on Llama2 which is not very fair to compare with.Importantly, we fully open-source our datasets and models, designating both DART-Math-Uniform and DART-Math-Hard as the best-performing and most cost-effective public instruction tuning datasets available for advancing mathematical problem-solving.</p>
<p>Analysis</p>
<p>Scaling behaviors of different data synthesis methods: We study the scaling behaviors of our data synthesis approach and compare it to vanilla rejection sampling.As described in 2.2, our method is motivated to mitigate the bias towards easy queries that are only pronounced in challenging datasets.Therefore, in the scaling experiment we only synthesize responses for the training queries of the challenging MATH dataset and report the performance on the MATH test set.Figure 3 presents the results across three different base models as we scale the training data size from thousands to nearly 1 million samples.We observe a steady improvement in performance as the training data size increases exponentially.DART consistently outperforms VRT on general base models Mistral-7B and Lllama3-8B, achieving better scaling.On DeepSeekMath-7B, however, the performance differences between various approaches are minimal.Observing the absolute accuracy changes, DeepSeekMath-7B already achieves over 50% accuracy with just thousands of training samples, and scaling up to 1 million samples leads to only a modest 3-point improvement.This is in stark contrast to the over 20-point improvements seen on other models like Mistral-7B and Llama3-8B.As discussed in §4.2, we believe this phenomenon is due to the MATH training queries not being particularly beneficial for DeepSeekMath-7B, which has undergone extensive math-specific continual pretraining.Consequently, for DeepSeekMath-7B, the differences between these approaches are not significant, and the main bottleneck shifts to query coverage rather than the responses themselves.</p>
<p>Effect of one-response coverage: In §3.2, we describe that DARS-Prop2Diff can cause zero synthetic responses for easy queries, especially when the number of training samples is small.Therefore, we ensure that the easy queries have at least one correct response practically.Here we examine the impact of this one-response coverage by comparing the Prop2Diff strategy with and without this coverage constraint, as training data sizes increase.Figure 4 (Left) displays the outcomes on the MATH and GSM8K benchmarks respectively.As anticipated, when the training data size is relatively small, the one-response coverage proves beneficial, particularly on the simpler GSM8K benchmark, improving accuracy by about 8 points.This suggests that effective learning for easy problem-solving can be achieved with just one additional correct response.As we scale up the training data size, the natural increase in coverage for easy queries causes that the difference between the two approaches diminishes.Additionally, we explore the implementation of one-response coverage in vanilla rejection tuning to determine if adding one synthetic response for difficult queries could address its issue of low coverage for such queries.However, this modification does not significantly aid in learning difficult queries, as observed on the challenging MATH benchmark.This indicates that complex problems generally require a greater number of training samples for effective learning.2): Scaling curves studying the effect of one-response coverage."Prop2Diff (−Cover)" denotes DARS-Prop2Diff without enforcing at least one synthetic response for each query, while "VRT (+Cover)" denotes vanilla rejection sampling enforcing at least one synthetic response for each query.</p>
<p>(3) and ( 4): The total number of raw samples needed, and the actual ratio (r) of queries achieving the desiderata of the two DARS synthesis strategy for 585K-sized dataset curation respectively, when we vary the maximum allowable raw samples per query (nmax).</p>
<p>Synthesis cost: DART generally needs more sampling trials to synthesize the same size of dataset compared to vanilla rejection tuning, as discussed in §3.3.It is important to underline that the synthesis cost, although initially higher, is a one-time expense.Once the dataset is synthesized, it can be used by the community and us to train numerous models, effectively amortizing the cost.To provide a quantitative understanding of the synthesis cost, we consider two main factors: n max , the maximum allowable raw samples for each query, and r, the ratio of queries that achieve the designated number of responses.If n max is set too high, sampling may continue indefinitely for particularly difficult or noisy queries, resulting in a high synthesis cost.Conversely, a too small n max may result in many queries not gathering the sufficient number of correct responses, leading to a lower r. Figure 4 (Right) illustrates the total number of raw samples required to synthesize 585K examples and the query achieving ratio r as we increase n max .When n max reaches 2048, over 90% of the queries can collect the designated number of responses under DARS-Uniform, with a corresponding total number of samples around 5 million.To reach 90% achieving ratio for DARS-Prop2Diff, n max needs to be at least 8K, and the total number of raw samples exceeds 15 million.In our experiments, we achieved an over 95% ratio r, sampling approximately 150 million samples in total, which required about 5 days running inference of DeepSeekMath-7B-RL on 32 NVIDIA A100 GPUs.Besides that synthesis is a one-time cost, we would like to emphasize the number of samples is not a fair metric to compare synthesis cost between different works -our synthesis model of 7B size is relatively inexpensive and fast to run, compared to the much more costly and slower GPT-4 used in most previous studies.Moreover, achieving a query ratio as high as 95% may not be necessary to reach good performance.A slightly lower ratio of 85% or 90% might not significantly impact performance but could substantially reduce the synthesis cost.We plan to explore this balance further in future work.</p>
<p>Discussion</p>
<p>In this paper, we focus on instruction tuning for mathematical problem solving, and discuss the impact of distribution and coverage of training queries across different difficulties.We identify the bias towards easy queries in vanilla rejection tuning, and propose difficulty-aware rejection tuning, DART, as a remedy.Based on our approach, we create and open-source the best-performing and the most cost-effective instruction tuning datasets for mathematical reasoning, without relying on proprietary models.Extensive experiments across various base models and benchmarks demonstrate the effectiveness of our approach.</p>
<p>Limitations: We utilize fail rate as the difficulty metric, yet it may be sub-optimal.Other metrics such as direct scoring (Liu et al., 2024b), Elo ratings, or the minimum pretraining compute to train a model that can always answer correctly (Burns et al., 2023) may be further explored.DART-Math is limited by natural language reasoning, while it is shown that generating and executing code helps solve mathematical problems significantly (Zhou et al., 2024;Yue et al., 2024;Gou et al., 2024;Liao et al., 2024;Toshniwal et al., 2024) -we think the bias in vanilla rejection sampling also exists for code generation, and DART could be integrated to potentially improve code generation as well.</p>
<p>Figure 3 :
3
Figure 3: Scaling curves of MATH test performance against number of training samples synthesized from MATH training queries, training is on three base models.</p>
<p>Figure 4 :
4
Figure 4: From Left to Right, (1) and (2): Scaling curves studying the effect of one-response coverage."Prop2Diff (−Cover)" denotes DARS-Prop2Diff without enforcing at least one synthetic response for each query, while "VRT (+Cover)" denotes vanilla rejection sampling enforcing at least one synthetic response for each query.(3) and (4): The total number of raw samples needed, and the actual ratio (r) of queries achieving the desiderata of the two DARS synthesis strategy for 585K-sized dataset curation respectively, when we vary the maximum allowable raw samples per query (nmax).</p>
<p>with 2.3 million examples, as well as vanilla rejection tuning (VRT) with 590K examples.Both DART-Math (Uniform) and DART-Math (Prop2Diff) use 590K training examples.Right: Number of responses for each query descending by difficulty across 3 synthesis strategies.Queries are from the MATH training split</p>
<p>Table 1 :
1
Comparison between our DART-Math datasets and previous mathematical instruction tuning datasets.Most of previous datasets are constructed with ChatGPT, and many of them are not open-source, especially for ones of the best performance.
5+Human✗
Strictly speaking, final answer correctness does not necessarily imply intermediate reasoning correctness. We do not make further distinction across this paper which is not our focus.
A Experimental SetupA.1 Training SetupWe train all the models using the Transformers library(Wolf et al., 2019).Sequence Packing: To efficiently save computation wasted by padding tokens, we employ sequence packing(Krell et al., 2021).We shuffle all samples in each epoch before sequence packing, ensuring that the same semantic sequences are not always in the same computation sequence.Batch Size: The computation sequence token length is set to 4096, considering that most sequences in the training datasets are shorter than this length.The batch size is 64, though there are usually more than 64 samples in one batch because one computation sequence can pack multiple semantic sequences.We disable gradient accumulation(Lin et al., 2018)by default, but when the memory is not sufficient, we increase the number of gradient accumulation steps and keep other settings unchanged.Specifically, we use 2 gradient accumulation steps when training Llama3-8B on 8 NVIDIA A100 GPUs under our setting.Learning Rate: We use the Adam optimizer(Zhang, 2018)with the weight decay as 0. We use a linear warmup with a warmup step ratio of 0.03 and cosine learning rate scheduler.The maximum learning rates are set as follows: Mistral-7B at 1e-5, DeepSeekMath-7B and Llama3-8B at 5e-5, and Llama3-70B at 2e-5.We determine the values by searching through 1e-6,5e-6,1e-5,2e-5,5e-5,1e-4 according to the MATH performance after training on MMIQC for 1 epoch.# Training Epochs:The default number of epochs is 3.For MMIQC, we train for 1 epoch followingLiu et al. (2024a).For Llama3 models, we train for 1 epoch because preliminary experiments indicate that 1 epoch consistently outperforms 3 epochs.Prompt Template: For the prompt template, we use the format followingTaori et al. (2023):Prompt TemplateBelow is an instruction that describes a task.Write a response that appropriately completes the request.\n\n###Instruction:\n{query}\n\n###Response:\n Other Details: For efficiency, We utilize various tools / libraries / techniques including:• the DeepSpeed distributed framework(Rasley et al., 2020)with ZeRO(Rajbhandari et al., 2020)stage 3• gradient checkpointing(Chen et al., 2016)• torch.compile(Ansel et al., 2024)• mixed-precision training(Micikevicius et al., 2018)of BrainFloat16(Kalamkar et al., 2019)and TensorFloat32 (NVIDIA, 2020)Hardware: For 7B or 8B models, we train on 8 NVIDIA A100 GPUs.For 70B models, we train on 32 NVIDIA A100 GPUs.A.2 Synthesis SetupGeneration: We utilize the vLLM libraryKwon et al. (2023), setting the maximum number of output tokens as 2048 and adopt top-p sampling with p = 0.95.For temperature t, we search from 0.3 to 1.8 with a step of 0.1 by using DeepSeekMath-7B-RL to sample answer-correct responses to queries in MATH training set.We observe the speeds to achieve specified correct answer coverage of different temperatures and find that, for DeepSeekMath-7B-RL, higher temperatures achieve faster, but t ≥ 1.0 are quite similar and t ≥ 1.7 cause the output to be nonsense.Besides, we find that higher temperatures produce more diverse responses by visualizing the embedings of response from different temperatures to the same query using t-SNE (Van derMaaten &amp; Hinton, 2008).Finally, we set the temperature as t = 1.6.Grading: To judge whether the answers in raw responses are correct or not as accurately as possible, we implement an elaborate answer extraction and judgement pipeline based on regular expressions and SymPy(Meurer et al., 2017)symbolic calculation, which is able to correctly process most mathematical objects such as matrices (vectors), intervals, symbols besides numbers, as well as some special texts like bool expressions, dates and times.Calculating Fail Rate: For efficiency, we merge DARS-Uniform synthesis and calculating fail rates as mentioned in §3.2.Specifically, we set k u = 192 to synthesize our data pool, and based on all the responses sampled, we calculate fail rate for each query as fail rate = # all correct responses # all raw responses which would produce more accurate fail rate values but is not necessary for general algorithm implementations.A.3 Evaluation SetupGeneration Like §A.2, we use the vLLM library, setting the maximum number of output tokens as 2048 and adopting top-p sampling with p = 0.95.But we use greedy decoding (i.e.set temperature t = 0) for evaluation.Note that there might still be randomness from vLLM implementation despite using greedy decoding, so we run each evaluation in §2 with at least 3 random seeds.When evaluating models trained by us, we use the Alpaca(Taori et al., 2023)
Llama3-70B-ICL. </p>
<p>7B Math-Specialized Base Model DeepSeekMath-7B-ICL. </p>
<p>. -7b-Mmiqc Deepseekmath, </p>
<p>DART-Math-DSMath-7B (Prop2Diff) 0.59M 53.6 ↑0.6 86.8 ↓1.4 40.7 ↓1.2 61.6 ↑1.4 21.7 ↑2.6 32.2 ↑5.0 49.4 ↑1.17-8B General Base Model Llama2-7B-Xwin-Math-V1. </p>
<p>Mistral-7B-WizardMath-V1.1 (RL). </p>
<p>Llama3-8B-ICL. </p>
<p>Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation. Jason References, Edward Ansel, Horace Yang, Natalia He, Animesh Gimelshein, Michael Jain, Bin Voznesensky, Peter Bao, David Bell, Evgeni Berard, Burovski, Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems. the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems20242</p>
<p>Introducing claude. Anthropic, 2023</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, arXiv:2312.093902023arXiv preprint</p>
<p>Training deep nets with sublinear memory cost. Tianqi Chen, Bing Xu, Chiyuan Zhang, Carlos Guestrin, arXiv:1604.061742016arXiv preprint</p>
<p>TheoremQA: A theorem-driven question answering dataset. Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, Tony Xia, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Journal of Machine Learning Research. 242402023</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021aarXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021barXiv preprint</p>
<p>ToRA: A tool-integrated reasoning agent for mathematical problem solving. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, arXiv:2402.140082024arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks. J Vanschoren, S Yeung, the Neural Information Processing Systems Track on Datasets and Benchmarks20211</p>
<p>V-star: Training verifiers for self-taught reasoners. Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, Rishabh Agarwal, arXiv:2402.064572024arXiv preprint</p>
<p>Key-point-driven data synthesis with its enhancement on mathematical reasoning. Yiming Huang, Xiao Liu, Yeyun Gong, Zhibin Gou, Yelong Shen, Nan Duan, Weizhu Chen, arXiv:2403.023332024arXiv preprint</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023</p>
<p>SWE-bench: Can language models resolve real-world github issues?. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, ; Karthik R Narasimhan, The Twelfth International Conference on Learning Representations. Ofir Press2024</p>
<p>A study of bfloat16 for deep learning training. Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Teja Dharma, Nataraj Vooturi, Jianyu Jammalamadaka, Hector Huang, Yuen, arXiv:1905.123222019arXiv preprint</p>
<p>Efficient sequence packing without cross-contamination: Accelerating large language models without impacting performance. Mario Michael Krell, Matej Kosec, Sergio P Perez, Andrew Fitzgibbon, arXiv:2107.020272021arXiv preprint</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the 29th Symposium on Operating Systems Principles. the 29th Symposium on Operating Systems Principles2023</p>
<p>Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng Zhang, Houwen Peng, arXiv:2403.04706Common 7b language models already possess strong math capabilities. 2024arXiv preprint</p>
<p>Minpeng Liao, Wei Luo, Chengxi Li, Jing Wu, Kai Fan, Mario, Math reasoning with code interpreter output -a reproducible pipeline. 2024</p>
<p>Let's verify step by step. Vineet Hunter Lightman, Yuri Kosaraju, Harrison Burda, John Edwards ; Leike, Ilya Schulman, Karl Sutskever, Cobbe, The Twelfth International Conference on Learning Representations. Bowen Baker, Teddy LeeJan. 2024</p>
<p>Deep gradient compression: Reducing the communication bandwidth for distributed training. Yujun Lin, Song Han, Huizi Mao, Yu Wang, Bill Dally, International Conference on Learning Representations. 2018</p>
<p>Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, Yujiu Yang, arXiv:2402.14809Criticbench: Benchmarking llms for critique-correct reasoning. 2024arXiv preprint</p>
<p>Augmenting math word problems via iterative question composing. Haoxiong Liu, Yifan Zhang, Yifan Luo, Andrew Chi-Chih Yao, 2024a</p>
<p>What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning. Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, Junxian He, The Twelfth International Conference on Learning Representations. 2024b</p>
<p>The flan collection: Designing data and methods for effective instruction tuning. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Barret Quoc V Le, Jason Zoph, Adam Wei, Roberts, Proceedings of the 40th International Conference on Machine Learning. Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, Jonathan Scarlett, the 40th International Conference on Machine LearningPMLRJul 2023202</p>
<h1>instag: Instruction tagging for analyzing supervised fine-tuning of large language models. Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, Jingren Zhou, The Twelfth International Conference on Learning Representations. 2024</h1>
<p>Introducing meta llama 3: The most capable openly available llm to date. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Dongmei Zhang, arXiv:2308.09583Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. 2023. 2024arXiv preprint</p>
<p>Sympy: symbolic computing in python. Aaron Meurer, Christopher P Smith, Mateusz Paprocki, Ondřej Čertík, B Sergey, Matthew Kirpichev, Amit Rocklin, Sergiu Kumar, Jason K Ivanov, Sartaj Moore, Singh, PeerJ Computer Science. 3e1032017</p>
<p>Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, Hao Wu, International Conference on Learning Representations. 2018Mixed precision training</p>
<p>Orca-math: Unlocking the potential of slms in grade school math. Arindam Mitra, Hamed Khanpour, Corby Rosset, Ahmed Awadallah, arXiv:2402.148302024arXiv preprint</p>
<p>Tensorfloat-32 in the a100 gpu accelerates ai training, hpc up to 20x. 2020NVIDIA</p>
<p>Josh Openai, Steven Achiam, Sandhini Adler, Lama Agarwal, Ilge Ahmad, Florencia Akkaya, Diogo Leoni Aleman, Janko Almeida, Sam Altenschmidt, Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Zero: Memory optimizations toward training trillion parameter models. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He, SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE2020</p>
<p>Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, Yuxiong He, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2020</p>
<p>Analysing mathematical reasoning abilities of neural models. David Saxton, Edward Grefenstette, Felix Hill, Pushmeet Kohli, International Conference on Learning Representations. 2019</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Li, Daya Wu, Guo, arXiv:2402.03300Deepseekmath: Pushing the limits of mathematical reasoning in open language models. 2024arXiv preprint</p>
<p>Beyond human data: Scaling self-training for problem-solving with language models. Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Peter J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, arXiv:2312.065852023arXiv preprint</p>
<p>Beyond neural scaling laws: beating power law scaling via data pruning. Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, Ari Morcos, Advances in Neural Information Processing Systems. 202235</p>
<p>Mathscale: Scaling instruction tuning for mathematical reasoning. Zhengyang Tang, Xingxing Zhang, Benyou Wan, Furu Wei, arXiv:2403.028842024arXiv preprint</p>
<p>Stanford alpaca: An instruction-following llama model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>Openmathinstruct-1: A 1.8 million math instruction tuning dataset. Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, Igor Gitman, arXiv:2402.101762024arXiv preprint</p>
<p>Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, 2023</p>
<p>Visualizing data using t-sne. Laurens Van Der Maaten, Geoffrey Hinton, Journal of machine learning research. 9112008</p>
<p>Self-instruct: Aligning language models with self-generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, 10.18653/v1/2023.acl-long.754Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Huggingface's transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, arXiv:1910.037712019arXiv preprint</p>
<p>Chatglm-math: Improving math problem-solving in large language models with a self-critique pipeline. Yifan Xu, Xiao Liu, Xinghan Liu, Zhenyu Hou, Yueyan Li, Xiaohan Zhang, Zihan Wang, Aohan Zeng, Zhengxiao Du, Wenyi Zhao, arXiv:2404.028932024arXiv preprint</p>
<p>Metamath: Bootstrap your own mathematical questions for large language models. Longhui Yu, Weisen Jiang, Han Shi, Y U Jincheng, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, Chang Zhou, arXiv:2308.01825Scaling relationship on learning mathematical reasoning with large language models. 2023arXiv preprint</p>
<p>MAmmoTH: Building math generalist models through hybrid instruction tuning. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Improved adam optimizer for deep neural networks. Zijun Zhang, IEEE/ACM 26th international symposium on quality of service (IWQoS). IEEE2018. 2018</p>
<p>minif2f: a cross-system benchmark for formal olympiad-level mathematics. Kunhao Zheng, Jesse Michael Han, Stanislas Polu, International Conference on Learning Representations. 2022</p>
<p>Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification. Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, Hongsheng Li, The Twelfth International Conference on Learning Representations. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5374 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5374</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5374</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-bb98bc96e02396d199fc899287d9b84393c86e79</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/bb98bc96e02396d199fc899287d9b84393c86e79" target="_blank">Graph Transformer for Graph-to-Sequence Learning</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> A new model, known as Graph Transformer, is proposed that uses explicit relation encoding and allows direct communication between two distant nodes and provides a more efficient way for global graph structure modeling.</p>
                <p><strong>Paper Abstract:</strong> The dominant graph-to-sequence transduction models employ graph neural networks for graph representation learning, where the structural information is reflected by the receptive field of neurons. Unlike graph neural networks that restrict the information exchange between immediate neighborhood, we propose a new model, known as Graph Transformer, that uses explicit relation encoding and allows direct communication between two distant nodes. It provides a more efficient way for global graph structure modeling. Experiments on the applications of text generation from Abstract Meaning Representation (AMR) and syntax-based neural machine translation show the superiority of our proposed model. Specifically, our model achieves 27.4 BLEU on LDC2015E86 and 29.7 BLEU on LDC2017T10 for AMR-to-text generation, outperforming the state-of-the-art results by up to 2.2 points. On the syntax-based translation tasks, our model establishes new single-model state-of-the-art BLEU scores, 21.3 for English-to-German and 14.1 for English-to-Czech, improving over the existing best results, including ensembles, by over 1 BLEU.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5374.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5374.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Transformer (relation-enhanced global attention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-to-sequence encoder that replaces local GNN message passing with multi-head self-attention augmented by explicit relation encodings (derived from shortest paths) so any two nodes can communicate directly while preserving graph topology.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Relation-enhanced attention graph encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Does not linearize the graph. Treats the input graph as fully connected for attention, but injects explicit pairwise relation encodings into the attention score: for node pair (i,j) a shortest-path relation sequence sp_{i->j} is encoded by a Bi-GRU into r_{ij}, split into forward and backward relation vectors r_{i->j}, r_{j->i}, and combined additively with node query/key vectors so s_{ij} = (x_i + r_{i->j})W_q^T W_k (x_j + r_{j->i}). Also adds reverse edges, self-loops, a special global node (connected to all nodes), and absolute positional embeddings for nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR (rooted DAG), dependency trees (syntax trees) — generally directed graphs/DAGs used in NLP</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Preserves explicit graph relations via distributed relation encodings (shortest-path sequences); enables direct global node-to-node communication (no hop-limited propagation); keeps edge-label information separate from node embeddings (avoids Levi graph's conflation); introduces a global graph vector; uses absolute positions to expose node-root distances. Properties emphasized: improved expressivity for long-range dependencies, avoidance of edge-node semantic conflation, but higher pairwise representation cost (naïvely O(n^2) relations).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>AMR-to-text generation on LDC2015E86 and LDC2017T10; syntax-based machine translation (source dependency trees) on WMT16 English->German and English->Czech.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BLEU, CHRF++, METEOR. Reported: AMR LDC2015E86 BLEU=27.4, CHRF++=56.4, METEOR=32.9; AMR LDC2017T10 BLEU=29.8, CHRF++=59.4, METEOR=35.1. Syntax-based MT English->German BLEU=21.3, CHRF++=47.9; English->Czech BLEU=14.1, CHRF++=41.1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Outperforms sequence-serialization seq2seq and various GNN encoders in experiments: on LDC2017T10 Graph Transformer BLEU 29.8 vs Guo et al. (GCN) 27.6 and Konstas et al. (seq2seq) 22.0; on LDC2015E86 27.4 vs Guo 25.7 and Pourdamghani (statistical) 26.9. For syntax MT, achieves new single-model state-of-the-art: English->German 21.3 vs Guo et al. 19.0; English->Czech 14.1 vs Guo 12.1. The paper attributes gains to better long-range and reentrancy handling compared to GNNs and to avoiding information loss from linearization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Fully-connected attention risks diluting explicit structure if relations are not encoded (addressed here via relation encodings). Encoding all pairwise shortest-paths is potentially O(n^2) (mitigated in practice by batching distinct shortest paths across a batch). When multiple shortest paths exist, the model samples one during training and averages representations at test time. Performance still degrades on extremely large graphs. Adds complexity from relation encoder (Bi-GRU per distinct shortest path) and extra relation parameters (reverse edges, self-loops, global node).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Transformer for Graph-to-Sequence Learning', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5374.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5374.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Levi graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Levi graph transformation (edge-labels-as-nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph transformation used by several GNN-based graph-to-sequence systems that converts each labeled edge into an unlabeled intermediate node, i.e., turns edges into nodes so that edge labels are represented as separate nodes in the graph.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph-to-sequence learning using gated graph neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Levi graph transformation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Transforms each labeled edge (u -[label]-> v) into two unlabeled edges u -> label_node and label_node -> v, where the original label becomes an explicit node. This lets standard node-centric GNNs encode edge-label information by treating labels as nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR (semantic graphs), other labeled graphs with edge types</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Makes edge labels learnable as embeddings in the same vector space as nodes; increases number of nodes (at least doubling node count), causing heavier decoding attention and copy costs; conflates semantic roles of edges with node semantics since labels share node embedding space; straightforward to plug into existing node-based GNNs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used by GNN-based AMR-to-text and graph-to-sequence models evaluated on AMR benchmarks (LDC2015E86, LDC2017T10) and syntax-based MT datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in prior works cited: e.g., Guo et al. (using Levi graphs + densely connected GCN) report BLEU 25.7 on LDC2015E86 and 27.6 on LDC2017T10 (numbers reproduced in this paper for baseline comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Paper criticizes Levi graphs: Graph Transformer avoids Levi transformation and shows better BLEU/CHRF++/METEOR scores. The main downsides noted are semantic conflation (nodes vs edges sharing space) and increased representation size causing decoder-side complexity; Graph Transformer’s explicit relation encoding is presented as superior.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Doubles or increases node set size (computational/attention overhead); edge labels occupy same semantic space as regular nodes (semantic mismatch); can complicate decoder attention and copy mechanisms; still limited by local message-passing of GNNs (hop-limited propagation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Transformer for Graph-to-Sequence Learning', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5374.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5374.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph linearization / serialization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequence serialization / linearization of graphs for seq2seq models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Convert the input graph into a linear token sequence which is fed into a sequence-to-sequence model; widely used baseline for AMR-to-text where graphs are flattened into sequences with special tokens for edge labels and structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural AMR: Sequence-to-sequence models for parsing and generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph linearization (serialization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Serialize nodes and edges (and their labels) into a single token sequence (various traversal orders possible). The flattened sequence then serves as input to a conventional seq2seq model. Pre-/post-processing often includes entity simplification/anonymization and restoration.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs, other structured semantic graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Simple to implement and compatible with standard seq2seq architectures; can lose or dilute graph structural information (order choice matters); easier to reuse mature seq2seq tooling; may handle smaller graphs well but struggles with preserving long-range/structural dependencies and reentrancies.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>AMR-to-text generation benchmarks LDC2015E86 and LDC2017T10 (as baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Example baseline numbers cited in paper: Konstas et al. (seq2seq linearization) BLEU on LDC2015E86 = 22.0; other linearization-based or seq2seq approaches reported lower BLEU than graph-aware encoders and Graph Transformer in the paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Graph Transformer and GNN-based encoders outperform linearization seq2seq models on AMR-to-text benchmarks; the paper argues linearization incurs obstacle to capturing graph structure and therefore yields worse BLEU/CHRF++/METEOR.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Serialization loses explicit graph topology (ordering is artificial), struggles with reentrancies and long-distance relations; performance suffers compared to graph-aware encoders; dependent on preprocessing heuristics (anonymization etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Transformer for Graph-to-Sequence Learning', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5374.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5374.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Grammar / structured methods</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grammar-based graph-to-text methods (tree/transducer/grammar approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Non-neural or specialized structured methods that generate text from graphs by converting graphs to tree structures, applying tree transducers, synchronous node replacement grammars, or solving graph traversal problems (e.g., TSP formulations).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generation from abstract meaning representation using tree transducers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Grammar/transducer-based graph-to-text</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Uses grammar or transducer frameworks that split/convert graphs into trees or apply synchronous grammars (e.g., hyperedge replacement, node replacement grammar) or cast generation as combinatorial optimization (traveling salesman over concepts), producing linear surface forms from structured rules rather than neural encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs and other semantic graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Often faithful to structure and interpretable due to explicit rules; can leverage linguistic priors; may rely on external resources and specialized algorithms; can be competitive but often require heavy engineering and external corpora or language models.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>AMR-to-text (benchmarks such as LDC2015E86 reported in paper comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Examples cited in paper: Flanigan et al. BLEU=23.0 (LDC2015E86), Pourdamghani et al. BLEU=26.9 (noting these methods used external Gigaword data for LM training).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Some grammar-based methods compete with or beat earlier neural approaches when using large external language models or extra data; Graph Transformer surpasses these methods on reported AMR benchmarks without ensembling or extra silver data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Often require additional external corpora or engineered components (language models, parsers), may not generalize as easily as end-to-end neural methods, and can be complex to develop/maintain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Transformer for Graph-to-Sequence Learning', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5374.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5374.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GNN-based graph encoders</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph neural network encoders (Gated GNNs, Graph Convolutional Networks, attention-over-neighbors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Encoders that compute node representations by iterative local message passing over graph neighbors using GCNs, gated graph neural networks, LSTM-style aggregators, or attention limited to adjacent nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Densely connected graph convolutional networks for graph-to-sequence learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Local message-passing GNN encoders</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Keep graph structure and edge labels (often via Levi transformation) and iterate node updates based on first-order neighborhoods (possibly stacked across layers). Variants include gated graph neural networks, graph convolutional networks, LSTM-style aggregators, and neighbor-attention.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs, dependency syntax trees and other labeled graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Naturally respect graph adjacency and local topology; receptive field grows with depth (L layers allow L-hop communication); may struggle with long-range dependencies and reentrancies if insufficient depth; often implemented with Levi graph to handle edge labels.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>AMR-to-text (LDC2015E86, LDC2017T10) and syntax-aware machine translation (WMT16 datasets) as evaluated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Representative baseline results reproduced in paper: Guo et al. (densely connected GCN with Levi graph) BLEU LDC2015E86=25.7, LDC2017T10=27.6; Beck et al. (gated GNN) reported BLEU 23.3 (LDC2017T10). For syntax-based MT Guo et al. report English-DE BLEU=19.0 and English-CS BLEU=12.1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Graph Transformer outperforms these GNN methods on both AMR-to-text and syntax-based MT. The paper argues GNNs' local propagation is a bottleneck for long-distance dependencies and reentrancies, where Graph Transformer shows a larger advantage.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Require stacking many layers for long-range interactions, which can be inefficient and lead to disrupted information flow across long paths; dependent on Levi graph for labeled edges (which increases node count and conflates edges and nodes unless alternative encodings used).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Transformer for Graph-to-Sequence Learning', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural AMR: Sequence-to-sequence models for parsing and generation <em>(Rating: 2)</em></li>
                <li>Densely connected graph convolutional networks for graph-to-sequence learning <em>(Rating: 2)</em></li>
                <li>Graph-to-sequence learning using gated graph neural networks <em>(Rating: 2)</em></li>
                <li>Generation from abstract meaning representation using tree transducers <em>(Rating: 2)</em></li>
                <li>Text Generation from Knowledge Graphs with Graph Transformers <em>(Rating: 2)</em></li>
                <li>AMR-to-text generation as a traveling salesman problem <em>(Rating: 1)</em></li>
                <li>AMR-to-text generation with synchronous node replacement grammar <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5374",
    "paper_id": "paper-bb98bc96e02396d199fc899287d9b84393c86e79",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "Graph Transformer",
            "name_full": "Graph Transformer (relation-enhanced global attention)",
            "brief_description": "A graph-to-sequence encoder that replaces local GNN message passing with multi-head self-attention augmented by explicit relation encodings (derived from shortest paths) so any two nodes can communicate directly while preserving graph topology.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Relation-enhanced attention graph encoding",
            "representation_description": "Does not linearize the graph. Treats the input graph as fully connected for attention, but injects explicit pairwise relation encodings into the attention score: for node pair (i,j) a shortest-path relation sequence sp_{i-&gt;j} is encoded by a Bi-GRU into r_{ij}, split into forward and backward relation vectors r_{i-&gt;j}, r_{j-&gt;i}, and combined additively with node query/key vectors so s_{ij} = (x_i + r_{i-&gt;j})W_q^T W_k (x_j + r_{j-&gt;i}). Also adds reverse edges, self-loops, a special global node (connected to all nodes), and absolute positional embeddings for nodes.",
            "graph_type": "AMR (rooted DAG), dependency trees (syntax trees) — generally directed graphs/DAGs used in NLP",
            "representation_properties": "Preserves explicit graph relations via distributed relation encodings (shortest-path sequences); enables direct global node-to-node communication (no hop-limited propagation); keeps edge-label information separate from node embeddings (avoids Levi graph's conflation); introduces a global graph vector; uses absolute positions to expose node-root distances. Properties emphasized: improved expressivity for long-range dependencies, avoidance of edge-node semantic conflation, but higher pairwise representation cost (naïvely O(n^2) relations).",
            "evaluation_task": "AMR-to-text generation on LDC2015E86 and LDC2017T10; syntax-based machine translation (source dependency trees) on WMT16 English-&gt;German and English-&gt;Czech.",
            "performance_metrics": "BLEU, CHRF++, METEOR. Reported: AMR LDC2015E86 BLEU=27.4, CHRF++=56.4, METEOR=32.9; AMR LDC2017T10 BLEU=29.8, CHRF++=59.4, METEOR=35.1. Syntax-based MT English-&gt;German BLEU=21.3, CHRF++=47.9; English-&gt;Czech BLEU=14.1, CHRF++=41.1.",
            "comparison_to_other_representations": "Outperforms sequence-serialization seq2seq and various GNN encoders in experiments: on LDC2017T10 Graph Transformer BLEU 29.8 vs Guo et al. (GCN) 27.6 and Konstas et al. (seq2seq) 22.0; on LDC2015E86 27.4 vs Guo 25.7 and Pourdamghani (statistical) 26.9. For syntax MT, achieves new single-model state-of-the-art: English-&gt;German 21.3 vs Guo et al. 19.0; English-&gt;Czech 14.1 vs Guo 12.1. The paper attributes gains to better long-range and reentrancy handling compared to GNNs and to avoiding information loss from linearization.",
            "limitations_or_challenges": "Fully-connected attention risks diluting explicit structure if relations are not encoded (addressed here via relation encodings). Encoding all pairwise shortest-paths is potentially O(n^2) (mitigated in practice by batching distinct shortest paths across a batch). When multiple shortest paths exist, the model samples one during training and averages representations at test time. Performance still degrades on extremely large graphs. Adds complexity from relation encoder (Bi-GRU per distinct shortest path) and extra relation parameters (reverse edges, self-loops, global node).",
            "uuid": "e5374.0",
            "source_info": {
                "paper_title": "Graph Transformer for Graph-to-Sequence Learning",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "Levi graph",
            "name_full": "Levi graph transformation (edge-labels-as-nodes)",
            "brief_description": "A graph transformation used by several GNN-based graph-to-sequence systems that converts each labeled edge into an unlabeled intermediate node, i.e., turns edges into nodes so that edge labels are represented as separate nodes in the graph.",
            "citation_title": "Graph-to-sequence learning using gated graph neural networks",
            "mention_or_use": "mention",
            "representation_name": "Levi graph transformation",
            "representation_description": "Transforms each labeled edge (u -[label]-&gt; v) into two unlabeled edges u -&gt; label_node and label_node -&gt; v, where the original label becomes an explicit node. This lets standard node-centric GNNs encode edge-label information by treating labels as nodes.",
            "graph_type": "AMR (semantic graphs), other labeled graphs with edge types",
            "representation_properties": "Makes edge labels learnable as embeddings in the same vector space as nodes; increases number of nodes (at least doubling node count), causing heavier decoding attention and copy costs; conflates semantic roles of edges with node semantics since labels share node embedding space; straightforward to plug into existing node-based GNNs.",
            "evaluation_task": "Used by GNN-based AMR-to-text and graph-to-sequence models evaluated on AMR benchmarks (LDC2015E86, LDC2017T10) and syntax-based MT datasets.",
            "performance_metrics": "Reported in prior works cited: e.g., Guo et al. (using Levi graphs + densely connected GCN) report BLEU 25.7 on LDC2015E86 and 27.6 on LDC2017T10 (numbers reproduced in this paper for baseline comparison).",
            "comparison_to_other_representations": "Paper criticizes Levi graphs: Graph Transformer avoids Levi transformation and shows better BLEU/CHRF++/METEOR scores. The main downsides noted are semantic conflation (nodes vs edges sharing space) and increased representation size causing decoder-side complexity; Graph Transformer’s explicit relation encoding is presented as superior.",
            "limitations_or_challenges": "Doubles or increases node set size (computational/attention overhead); edge labels occupy same semantic space as regular nodes (semantic mismatch); can complicate decoder attention and copy mechanisms; still limited by local message-passing of GNNs (hop-limited propagation).",
            "uuid": "e5374.1",
            "source_info": {
                "paper_title": "Graph Transformer for Graph-to-Sequence Learning",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "Graph linearization / serialization",
            "name_full": "Sequence serialization / linearization of graphs for seq2seq models",
            "brief_description": "Convert the input graph into a linear token sequence which is fed into a sequence-to-sequence model; widely used baseline for AMR-to-text where graphs are flattened into sequences with special tokens for edge labels and structure.",
            "citation_title": "Neural AMR: Sequence-to-sequence models for parsing and generation",
            "mention_or_use": "mention",
            "representation_name": "Graph linearization (serialization)",
            "representation_description": "Serialize nodes and edges (and their labels) into a single token sequence (various traversal orders possible). The flattened sequence then serves as input to a conventional seq2seq model. Pre-/post-processing often includes entity simplification/anonymization and restoration.",
            "graph_type": "AMR graphs, other structured semantic graphs",
            "representation_properties": "Simple to implement and compatible with standard seq2seq architectures; can lose or dilute graph structural information (order choice matters); easier to reuse mature seq2seq tooling; may handle smaller graphs well but struggles with preserving long-range/structural dependencies and reentrancies.",
            "evaluation_task": "AMR-to-text generation benchmarks LDC2015E86 and LDC2017T10 (as baseline).",
            "performance_metrics": "Example baseline numbers cited in paper: Konstas et al. (seq2seq linearization) BLEU on LDC2015E86 = 22.0; other linearization-based or seq2seq approaches reported lower BLEU than graph-aware encoders and Graph Transformer in the paper's comparisons.",
            "comparison_to_other_representations": "Graph Transformer and GNN-based encoders outperform linearization seq2seq models on AMR-to-text benchmarks; the paper argues linearization incurs obstacle to capturing graph structure and therefore yields worse BLEU/CHRF++/METEOR.",
            "limitations_or_challenges": "Serialization loses explicit graph topology (ordering is artificial), struggles with reentrancies and long-distance relations; performance suffers compared to graph-aware encoders; dependent on preprocessing heuristics (anonymization etc.).",
            "uuid": "e5374.2",
            "source_info": {
                "paper_title": "Graph Transformer for Graph-to-Sequence Learning",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "Grammar / structured methods",
            "name_full": "Grammar-based graph-to-text methods (tree/transducer/grammar approaches)",
            "brief_description": "Non-neural or specialized structured methods that generate text from graphs by converting graphs to tree structures, applying tree transducers, synchronous node replacement grammars, or solving graph traversal problems (e.g., TSP formulations).",
            "citation_title": "Generation from abstract meaning representation using tree transducers",
            "mention_or_use": "mention",
            "representation_name": "Grammar/transducer-based graph-to-text",
            "representation_description": "Uses grammar or transducer frameworks that split/convert graphs into trees or apply synchronous grammars (e.g., hyperedge replacement, node replacement grammar) or cast generation as combinatorial optimization (traveling salesman over concepts), producing linear surface forms from structured rules rather than neural encoders.",
            "graph_type": "AMR graphs and other semantic graphs",
            "representation_properties": "Often faithful to structure and interpretable due to explicit rules; can leverage linguistic priors; may rely on external resources and specialized algorithms; can be competitive but often require heavy engineering and external corpora or language models.",
            "evaluation_task": "AMR-to-text (benchmarks such as LDC2015E86 reported in paper comparisons).",
            "performance_metrics": "Examples cited in paper: Flanigan et al. BLEU=23.0 (LDC2015E86), Pourdamghani et al. BLEU=26.9 (noting these methods used external Gigaword data for LM training).",
            "comparison_to_other_representations": "Some grammar-based methods compete with or beat earlier neural approaches when using large external language models or extra data; Graph Transformer surpasses these methods on reported AMR benchmarks without ensembling or extra silver data.",
            "limitations_or_challenges": "Often require additional external corpora or engineered components (language models, parsers), may not generalize as easily as end-to-end neural methods, and can be complex to develop/maintain.",
            "uuid": "e5374.3",
            "source_info": {
                "paper_title": "Graph Transformer for Graph-to-Sequence Learning",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "GNN-based graph encoders",
            "name_full": "Graph neural network encoders (Gated GNNs, Graph Convolutional Networks, attention-over-neighbors)",
            "brief_description": "Encoders that compute node representations by iterative local message passing over graph neighbors using GCNs, gated graph neural networks, LSTM-style aggregators, or attention limited to adjacent nodes.",
            "citation_title": "Densely connected graph convolutional networks for graph-to-sequence learning",
            "mention_or_use": "mention",
            "representation_name": "Local message-passing GNN encoders",
            "representation_description": "Keep graph structure and edge labels (often via Levi transformation) and iterate node updates based on first-order neighborhoods (possibly stacked across layers). Variants include gated graph neural networks, graph convolutional networks, LSTM-style aggregators, and neighbor-attention.",
            "graph_type": "AMR graphs, dependency syntax trees and other labeled graphs",
            "representation_properties": "Naturally respect graph adjacency and local topology; receptive field grows with depth (L layers allow L-hop communication); may struggle with long-range dependencies and reentrancies if insufficient depth; often implemented with Levi graph to handle edge labels.",
            "evaluation_task": "AMR-to-text (LDC2015E86, LDC2017T10) and syntax-aware machine translation (WMT16 datasets) as evaluated in the paper.",
            "performance_metrics": "Representative baseline results reproduced in paper: Guo et al. (densely connected GCN with Levi graph) BLEU LDC2015E86=25.7, LDC2017T10=27.6; Beck et al. (gated GNN) reported BLEU 23.3 (LDC2017T10). For syntax-based MT Guo et al. report English-DE BLEU=19.0 and English-CS BLEU=12.1.",
            "comparison_to_other_representations": "Graph Transformer outperforms these GNN methods on both AMR-to-text and syntax-based MT. The paper argues GNNs' local propagation is a bottleneck for long-distance dependencies and reentrancies, where Graph Transformer shows a larger advantage.",
            "limitations_or_challenges": "Require stacking many layers for long-range interactions, which can be inefficient and lead to disrupted information flow across long paths; dependent on Levi graph for labeled edges (which increases node count and conflates edges and nodes unless alternative encodings used).",
            "uuid": "e5374.4",
            "source_info": {
                "paper_title": "Graph Transformer for Graph-to-Sequence Learning",
                "publication_date_yy_mm": "2019-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural AMR: Sequence-to-sequence models for parsing and generation",
            "rating": 2
        },
        {
            "paper_title": "Densely connected graph convolutional networks for graph-to-sequence learning",
            "rating": 2
        },
        {
            "paper_title": "Graph-to-sequence learning using gated graph neural networks",
            "rating": 2
        },
        {
            "paper_title": "Generation from abstract meaning representation using tree transducers",
            "rating": 2
        },
        {
            "paper_title": "Text Generation from Knowledge Graphs with Graph Transformers",
            "rating": 2
        },
        {
            "paper_title": "AMR-to-text generation as a traveling salesman problem",
            "rating": 1
        },
        {
            "paper_title": "AMR-to-text generation with synchronous node replacement grammar",
            "rating": 1
        }
    ],
    "cost": 0.013888,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Graph Transformer for Graph-to-Sequence Learning*</h1>
<p>Deng Cai, Wai Lam<br>The Chinese University of Hong Kong<br>thisisjcykcd@gmail.com, wlam@se.cuhk.edu.hk</p>
<h4>Abstract</h4>
<p>The dominant graph-to-sequence transduction models employ graph neural networks for graph representation learning, where the structural information is reflected by the receptive field of neurons. Unlike graph neural networks that restrict the information exchange between immediate neighborhood, we propose a new model, known as Graph Transformer, that uses explicit relation encoding and allows direct communication between two distant nodes. It provides a more efficient way for global graph structure modeling. Experiments on the applications of text generation from Abstract Meaning Representation (AMR) and syntax-based neural machine translation show the superiority of our proposed model. Specifically, our model achieves 27.4 BLEU on LDC2015E86 and 29.7 BLEU on LDC2017T10 for AMR-to-text generation, outperforming the state-of-the-art results by up to 2.2 points. On the syntax-based translation tasks, our model establishes new single-model state-of-the-art BLEU scores, 21.3 for English-to-German and 14.1 for English-to-Czech, improving over the existing best results, including ensembles, by over 1 BLEU.</p>
<h2>Introduction</h2>
<p>Graphical structure plays an important role in natural language processing (NLP), they often serve as the central formalism for representing syntax, semantics, and knowledge. For example, most syntactic representations (e.g., dependency relation) are tree-based while most whole-sentence semantic representation frameworks (e.g., Abstract Meaning Representation (AMR) (Banarescu et al. 2013)) encode sentence meaning as directed acyclic graphs. A range of NLP applications can be framed as the process of graph-to-sequence learning. For instance, text generation may involve realizing a semantic graph into a surface form (Liu et al. 2015) and syntactic machine translation incorporates source-side syntax information for improving translation quality (Bastings et al. 2017). Fig. 1 gives an example of AMR-to-text generation.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An AMR graph (left) for the reference sentence "The boy wants the girl to believe him." and the corresponding Levi graph (right).</p>
<p>While early work uses statistical methods or neural models after the linearization of graphs, graph neural networks (GNNs) have been firmly established as the state-of-theart approaches for this task (Damonte and Cohen 2019; Guo et al. 2019). GNNs typically compute the representation of each node iteratively based on those of its adjacent nodes. This inherently local propagation nature precludes efficient global communication, which becomes critical at larger graph sizes, as the distance between two nodes exceeds the number of stacked layers. For instance, for two nodes staying $L$ hops away, at least $L$ layers will be needed in order to capture their dependencies. Furthermore, even if two distant nodes are reachable, the information may also be disrupted in the long journey (Xu et al. 2018; Guo et al. 2019).</p>
<p>To address the above problems, we propose a new model, known as Graph Transformer, which relies entirely on the multi-head attention mechanism (Vaswani et al. 2017) to draw global dependencies. ${ }^{1}$ Different to GNNs, the Graph Transformer allows modeling of dependencies between any two nodes without regard to their distance in the input graph. One undesirable consequence is that it essentially treats any</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>graph as a fully connected graph, greatly diluting the explicit graph structure. To maintain a graph structure-aware view, our proposed model introduces explicit relation encoding and incorporates it into the pairwise attention score computation as a dynamic parameter.</p>
<p>Our treatment of explicit relation encoding also brings other side advantages compared to GNN-based methods. Previous state-of-the-art GNN-based methods use Levi graph transformation (Beck, Haffari, and Cohn 2018; Guo et al. 2019), where two unlabeled edges are replacing one labeled edge that is present in the original graph. For example, in Fig. 1, the labeled edge want-01 $\xrightarrow{A R G 1}$ believe-01 turns to be two unlabeled edges want-01 $\longrightarrow$ ARG1 and ARG1 $\longrightarrow$ believe-01. Since edge labels are represented as nodes, they end up sharing the same semantic space, which is not ideal as nodes and edges are typically different elements. In addition, the Levi graph transformation at least doubles the number of representation vectors. which will introduce more complexity for the decoder-side attention mechanism (Bahdanau, Cho, and Bengio 2015) and copy mechanism (Gu et al. 2016; See, Liu, and Manning 2017). Through explicit and separate relation encoding, our proposed Graph Transformer inherently avoids these problems.</p>
<p>Experiments show that our model is able to achieve better performance for graph-to-sequence learning tasks for natural language processing. For the AMR-to-text generation task, our model surpasses the current state-of-the-art neural methods trained on LDC2015E86 and LDC2017T10 by 1.6 and 2.2 BLEU points, respectively. For the syntax-based neural machine translation task, our model is also consistently better than others, even including ensemble systems, showing the effectiveness of the model on a large training set. In addition, we give an in-depth study of the source of improvement gain and the internal workings of the proposed model.</p>
<h2>Related Work</h2>
<p>Early research efforts for graph-to-sequence learning use specialized grammar-based methods. Flanigan et al.(2016) split input graphs to trees and uses a tree-to-string transducer. Song et al.(2016) recast generation as a traveling salesman problem. Jones et al.(2012) leverage hyperedge replacement grammar and Song et al.(2017) use a synchronous node replacement grammar. More recent work employs more general approaches, such as phrase-based machine translation model (Pourdamghani, Knight, and Hermjakob 2016) and neural sequence-to-sequence methods (Konstas et al. 2017) after linearizing input graphs. Regarding AMR-totext generation, Cao and Clark(2019) propose an interesting idea that factorizes text generation through syntax. One limitation of sequence-to-sequence models, however, is that they require serialization of input graphs, which inevitably incurs the obstacle of capturing graph structure information.</p>
<p>An emerging trend has been directly encoding the graph with different variants of graph neural networks, which in common stack multiple layers that restrict the update of node representation based on a first-order neighborhood but
use different information passing schemes. Some borrow the ideas from recurrent neural networks (RNNs), e.g, Beck, Haffari, and Cohn(2018) use gated graph neural network ( Li et al. 2016) while Song et al.(2018) introduce LSTM-style information aggregation. Others apply convolutional neural networks (CNNs), e.g., Bastings et al.(2017);Damonte and Cohen(2019);Guo et al.(2019) utilize graph convolutional neural networks (Kipf and Welling 2017). KoncelKedziorski et al.(2019) update vertex information by attention over adjacent neighbors. Furthermore, Guo et al.(2019) allow the information exchange across different levels of layers. Damonte and Cohen(2019) systematically compare different encoders and show the advantages of graph encoder over tree and sequential ones. The contrast between our model and theirs is reminiscent of the contrast between the self-attention network (SAN) and CNN/RNN.</p>
<p>For sequence-to-sequence learning, the SAN-based Transformer model (Vaswani et al. 2017) has been the de facto approach for its empirical successes. However, it is unclear on the adaptation to graphical data and its performance. Our work is partially inspired by the introduction of relative position embedding (Shaw, Uszkoreit, and Vaswani 2018; Dai et al. 2019) in sequential data. However, the extension to graph is nontrivial since we need to model much more complicated relation instead of mere visual distance. To the best of our knowledge, the Graph Transformer is the first graph-to-sequence transduction model relying entirely on self-attention to compute representations.</p>
<h2>Background of Self-Attention Network</h2>
<p>The Transformer introduced by Vaswani et al.(2017) is a sequence-to-sequence neural architecture originally used for neural machine translation. It employs self-attention network (SAN) for implementing both the encoder and the decoder. The encoder consists of multiple identical blocks, of which the core is multi-head attention. The multi-head attention consists of $H$ attention heads, and each of them learns a distinct attention function. Given a source vector $x \in \mathbb{R}^{d_{x}}$ and a set of context vectors $\left{y_{1}, y_{2}, \ldots, y_{m}\right}$ with the same dimension $d_{x}$ or in short $y_{1: m}$, for each attention head, $x$ and $y_{1: m}$ are transformed into distinct query and value representations. The attention score is computed as the dot-product between them.</p>
<p>$$
f\left(x, y_{i}\right)=\left(W_{q} x\right)^{T} W_{k} y_{i}
$$</p>
<p>where $W_{q}, W_{k} \in \mathbb{R}^{d_{z} \times d_{x}}$ are trainable projection matrices. The attention scores are scaled and normalized by a softmax function to compute the final attention output attn.</p>
<p>$$
\begin{aligned}
&amp; a_{i}=\frac{\exp \left(f\left(x, y_{i}\right) / \sqrt{d_{z}}\right)}{\sum_{j=1}^{m} \exp \left(f\left(x, y_{i}\right)\right) / \sqrt{d_{z}})} \
&amp; a t t n=\sum_{i=1}^{m} a_{i} W_{v} y_{i}
\end{aligned}
$$</p>
<p>where $a \in \mathbb{R}^{m}$ is the attention vector (a distribution over all input $y_{1: m}$ ), $W_{v} \in \mathbb{R}^{d_{z} \times d_{x}}$ is a trainable projection matrix. Finally, the outputs of all attention heads are concatenated and projected to the original dimension of $x$, followed by</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An overview of our propose model.
feed-forward layers, residual connection, and layer normalization. ${ }^{2}$ For brevity, we will denote the whole procedure described above as a single function $\operatorname{ATT}\left(x, y_{1: m}\right)$.</p>
<p>For an input sequence $x_{1: n}$, the SAN-based encoder computes the vector representations iteratively by $x_{i}^{L}=$ $\operatorname{ATT}\left(x_{i}^{L}, x_{1: n}^{L-1}\right)$, where $L$ is the total number of blocks and $x_{1: n}^{0}$ are word embeddings. In this way, a representation is allowed to build a direct relationship with another long-distance representation. To feed the sequential order information, the deterministic or learned position embedding (Vaswani et al. 2017) is introduced to expose the position information to the model, i.e., $x_{i}^{0}$ becomes the sum of the corresponding word embedding and the position embedding for $i$.</p>
<p>The aforementioned treatment of SAN on sequential data can be drawn a close resemblance to graph neural networks by regarding the token sequence as an unlabeled fullyconnected graph (each token as a node) and taking the multihead attention mechanism as a specific message-passing scheme. Such view on the relationship between SAN and graph neural networks inspires our work.</p>
<h2>Graph Transformer</h2>
<h2>Overview</h2>
<p>For a graph with $n$ nodes, previous graph neural networks compute the node representation $v_{i}$ as a function of the input node $i$ and all its first-order neighborhoods $N(i)$. The graph structure is implicitly reflected by the receptive field of each node representation. This local communication design, however, could be inefficient for long-distance information exchange. We introduce a new model, known as Graph Transformer, which provides an aggressively different paradigm that enables relation-aware global communication.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>The overall framework is shown in Fig. 2. The most important characteristic of the Graph Transformer is that it has a fully-connected view on arbitrary input graphs. A node is able to directly receive and send information to another node no matter whether they directly connected or not. These operations are achieved by our proposed extension to the original multi-head attention mechanism, the relation-enhanced global attention mechanism described below. Specifically, the relationship between any node pair is depicted as the shortest relation path between them. These pairwise relation paths are fed into a relation encoder for distributed relation encoding. We initialize node vectors as the sum of the node embedding and absolute position embeddings. Multiple blocks of global attention network are stacked to compute the final node representations. At each block, a node vector is updated based on all other node vectors and the corresponding relation encodings. The resulted node vectors at the last block are fed to the sequence decoder for sequence generation.</p>
<h2>Graph Encoder</h2>
<p>Our graph encoder is responsible for transforming an input graph into a set of corresponding node embeddings. To apply global attention on a graph, the central problem is how to maintain the topological structure of the graph while allowing fully-connected communication. To this end, we propose relation-enhanced global attention mechanism, which is an extension of the vanilla multi-head attention. Our idea is to incorporate explicit relation representation between two nodes into their representation learning. Recall that, in the standard multi-head attention, the attention score between the element $x_{i}$ and the element $x_{j}$ is simply the dot-product of their query vector and key vector respectively:</p>
<p>$$
\begin{aligned}
s_{i j} &amp; =f\left(x_{i}, x_{j}\right) \
&amp; =x_{i} W_{q}^{T} W_{k} x_{j}
\end{aligned}
$$</p>
<p>Suppose we have learned a vector representation for the relationship $r_{i j}$, which we will refer as relation encoding, between the node $i$ and the node $j$. Following the idea of relative position embedding (Shaw, Uszkoreit, and Vaswani 2018; Dai et al. 2019), we propose to compute the attention score as follows:</p>
<p>$$
\left[r_{i \rightarrow j} ; r_{j \rightarrow i}\right]=W_{r} r_{i j}
$$</p>
<p>where we split the relation encoding $r_{i j}$ into the forward relation encoding $r_{i \rightarrow j}$ and the backward relation encoding $r_{j \rightarrow i}$. Then we compute the attention score based on both the node representations and their relation representation as shown below:</p>
<p>$$
\begin{aligned}
s_{i j} &amp; =g\left(x_{i}, x_{j}, r_{i j}\right) \
&amp; =\left(x_{i}+r_{i \rightarrow j}\right) W_{q}^{T} W_{k}\left(x_{j}+r_{j \rightarrow i}\right) \
&amp; =\underbrace{x_{i} W_{q}^{T} W_{k} x_{j}}<em i="i">{(a)}+\underbrace{x</em>} W_{q}^{T} W_{k} r_{j \rightarrow i}<em _rightarrow="\rightarrow" i="i" j="j">{(b)} \
&amp; +\underbrace{r</em>} W_{q}^{T} W_{k} x_{j}<em _rightarrow="\rightarrow" i="i" j="j">{(c)}+\underbrace{r</em>
\end{aligned}
$$} W_{q}^{T} W_{k} r_{j \rightarrow i}}_{(d)</p>
<p>Each term in Eq (3) corresponds to some intuitive meaning according to their formalization. The term (a) captures purely content-based addressing, which is the original term in vanilla attention mechanism. The term (b) represents a source-dependent relation bias. The term (c) governs a target-dependent relation bias. The term (d) encodes the universal relation bias. Our formalization provides a principled way to model the element-relation interactions. In comparison, it has broader coverage than Shaw, Uszkoreit, and Vaswani(2018) in terms of additional terms (c) and (d), and than Dai et al.(2019) in terms of the extra term (c) respectively. More importantly, previous methods only model the relative position in the context of sequential data, which merely adopts the immediate embeddings of the relative positions (e.g, $-1,+1$ ). To depict the relation between two nodes in a graph, we utilize a shortest-path based approach as described below.</p>
<p>Relation Encoder Conceptually, the relation encoding gives the model a global guidance about how information should be gathered and distributed, i.e., where to attend. For most graphical structures in NLP, the edge label conveys direct relationship between adjacent nodes (e.g., the semantic role played by concept-to-concept, and the dependency relation between two words). We extend this one-hop relation definition into multi-hop relation reasoning for characterizing the relationship between two arbitrary nodes. For example, in Fig 1, the shortest path from the concept want-01 to girl is " want-01 $\xrightarrow{A R G 1}$ believe-01 $\xrightarrow{A R G 0}$ girl", which conveys that girl is the object of the wanted action. Intuitively, the shortest path between two nodes gives the closest and arguably the most important relationship between them. Therefore, we propose to use the shortest paths (relation sequence) between two nodes to characterize their relationship. ${ }^{3}$ Following the sequential nature of the rela-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tion sequence, we employs recurrent neural networks with Gated Recurrent Unit (GRU) (Cho et al. 2014) for transforming relation sequence into a distributed representation. Formally, we represent the shortest relation path $s p_{i \rightarrow j}=$ $\left[e\left(i, k_{1}\right), e\left(k_{1}, k_{2}\right), \ldots, e\left(k_{n}, j\right)\right]$ between the node $i$ and the node $j$, where $e(\cdot, \cdot)$ indicates the edge label and $k_{1: n}$ are the relay nodes. We employ bi-directional GRUs for sequence encoding:</p>
<p>$$
\begin{aligned}
&amp; \overrightarrow{s_{t}}=\operatorname{GRU}<em t-1="t-1">{f}\left(\overrightarrow{s</em>\right) \
&amp; \overleftarrow{s_{t}}=\operatorname{GRU}}}, s p_{t<em t_1="t+1">{b}\left(\overleftarrow{s</em>\right)
\end{aligned}
$$}}, s p_{t</p>
<p>The last hidden states of the forward GRU network and the backward GRU networks are concatenated to form the final relation encoding $r_{i j}=\left[\overrightarrow{s_{n}} ; \overleftarrow{s_{0}}\right]$.
Bidirectionality Though in theory, our architecture can deal with arbitrary input graphs, the most widely adopted graphs in the real problems are directed acyclic graphs (DAGs). This implies that the node embedding information will be propagated in one pre-specified direction. However, the reverse direction informs the equivalent information flow as well. To facilitate communication in both directions, we add reverse edges to the graph. The reverse edge connects the same two nodes as the original edge but in a different direction and with a reversed label. For example, we will draw a virtual edge believe-01 $\xrightarrow{R A R G 1}$ want-01 according to the original edge want-01 $\xrightarrow{A R G 1}$ believe-01. For convenience, we also introduce self-loop edges for each node. These extra edges have specific labels, hence their own parameters in the network. We also introduce an extra global node into every graph, who has a direct edge to all other nodes with the special label global. The final representation $x_{\text {global }}$ of the global node serves as a whole graph representation.</p>
<p>Absolute Position Besides pairwise relationship, some absolute positional information can also be beneficial. For example, the root of an AMR graph serves as a rudimentary representation of the overall focus, making the minimum distance from the root node partially reflect the importance of the corresponding concept in the whole-sentence semantics. The sequence order of tokens in a dependency tree also provides complementary information to dependency relations. In order for the model to make use of the absolute positions of nodes, we add the positional embeddings to the input embeddings at the bottom of the encoder stacks. For example, want-01 in Fig 1 is the root node of the AMR graph, so its index should be 0 . Notice we denote the index of the global node as 0 as well.</p>
<h2>Sequence Decoder</h2>
<p>Our sequence decoder basically follows the same spirit of the sequential Transformer decoder. The decoder yields the natural language sequence by calculating a sequence of hidden states sequentially. One distinct characteristic is that we use the global graph representation $x_{\text {global }}$ for initializing the hidden states at each time step. The hidden state $h_{t}$ at
during testing.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">#train</th>
<th style="text-align: center;">#dev</th>
<th style="text-align: center;">#test</th>
<th style="text-align: center;">#edge types</th>
<th style="text-align: center;">#node types</th>
<th style="text-align: center;">avg #nodes</th>
<th style="text-align: center;">avg #edges</th>
<th style="text-align: center;">avg diameter</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LDC2015E86</td>
<td style="text-align: center;">16,833</td>
<td style="text-align: center;">1,368</td>
<td style="text-align: center;">1,371</td>
<td style="text-align: center;">113</td>
<td style="text-align: center;">18735</td>
<td style="text-align: center;">17.34</td>
<td style="text-align: center;">17.53</td>
<td style="text-align: center;">6.98</td>
</tr>
<tr>
<td style="text-align: center;">LDC2017T10</td>
<td style="text-align: center;">36,521</td>
<td style="text-align: center;">1,368</td>
<td style="text-align: center;">1,371</td>
<td style="text-align: center;">116</td>
<td style="text-align: center;">24693</td>
<td style="text-align: center;">14.51</td>
<td style="text-align: center;">14.62</td>
<td style="text-align: center;">6.15</td>
</tr>
<tr>
<td style="text-align: center;">English-Czech</td>
<td style="text-align: center;">181,112</td>
<td style="text-align: center;">2,656</td>
<td style="text-align: center;">2,999</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">78017</td>
<td style="text-align: center;">23.18</td>
<td style="text-align: center;">22.18</td>
<td style="text-align: center;">8.36</td>
</tr>
<tr>
<td style="text-align: center;">English-German</td>
<td style="text-align: center;">226,822</td>
<td style="text-align: center;">2,169</td>
<td style="text-align: center;">2,999</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">87219</td>
<td style="text-align: center;">23.29</td>
<td style="text-align: center;">22.29</td>
<td style="text-align: center;">8.42</td>
</tr>
</tbody>
</table>
<p>Table 1: Data statistics of all four datasets. #train/dev/test indicates the number of instances in each set, avg #nodes/edges/diameter represents the averaged value of nodes/edge/diameter size of a graph.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">model component</th>
<th style="text-align: center;">hyper-parameter</th>
<th style="text-align: center;">value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">char-level CNN</td>
<td style="text-align: center;">number of filters</td>
<td style="text-align: center;">256</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">width of filters</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">char embedding size</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">final hidden size</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">Embeddings</td>
<td style="text-align: center;">node embedding size</td>
<td style="text-align: center;">300</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">edge embedding size</td>
<td style="text-align: center;">200</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">token embedding size</td>
<td style="text-align: center;">300</td>
</tr>
<tr>
<td style="text-align: center;">Multi-head attention</td>
<td style="text-align: center;">number of heads</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">hidden state size</td>
<td style="text-align: center;">512</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">feed-forward hidden size</td>
<td style="text-align: center;">1024</td>
</tr>
</tbody>
</table>
<p>Table 2: Hyper-parameters settings.
each time step $t$ is then updated by interleaving multiple rounds of attention over the output of the encoder (node embeddings) and attention over previously-generated tokens (token embeddings). Both are implemented by the multihead attention mechanism. $x_{\text {global }}$ is removed when performing the sequence-to-graph attention.
Copy mechanism To address the data sparsity issue in token prediction, we include a copy mechanism (Gu et al. 2016) in similar spirit to previous works. Concretely, a single-head attention is computed based on the decoder state $h_{t}$ and the node representation $x_{1: u}$, where $a_{t}^{i}$ denotes the attention weight of the node $v_{i}$ in the current time step $t$. Our model can either directly copy the type name of a node or generate from a pre-defined vocabulary $V$. Formally, the prediction probability of a token $y$ is given by:</p>
<p>$$
P\left(y \mid h_{t}\right)=P\left(\operatorname{gen} \mid h_{t}\right) \operatorname{gen}\left(y \mid h_{t}\right)+P\left(\operatorname{copy} \mid h_{t}\right) \sum_{i \in S(y)} a_{t}^{i}
$$</p>
<p>where $S(y)$ is the set of nodes that have the same surface form as $y . P\left(\right.$ gen $\left.\mid h_{t}\right)$ and $P\left(\right.$ copy $\left.\mid h_{t}\right)$ are computed by a single layer neural network with softmax activation, and $\operatorname{gen}\left(y \mid h_{t}\right)=\exp \left(w_{y}^{T} h_{t}\right) / \sum_{y^{\prime} \in V} \exp \left(w_{y}^{\prime T} h_{t}\right)$, where $w_{y}$ (for $y \in V$ ) denotes the model parameters. The copy mechanism facilitates the generation of dates, numbers, and named entities in both AMR-to-text generation and machine translation tasks in experiments.</p>
<h2>Experiments</h2>
<p>We assess the effectiveness of our models on two typical graph-to-sequence learning tasks, namely AMR-to-text generation and syntax-based machine translation (MT). Following previous work, the results are mainly evaluated by BLEU (Papineni et al. 2002) and CHRF++ (Popović 2017).</p>
<p>Specifically, we use case-insensitive scores for AMR and case sensitive BLEU scores for MT.</p>
<h2>AMR-to-text Generation</h2>
<p>Our first application is language generation from AMR, a semantic formalism that represents sentences as rooted DAGs (Banarescu et al. 2013). For this AMR-to-text generation task, we use two benchmarks, namely the LDC2015E86 dataset and the LDC2017T10 dataset. The first block of Table 1 shows the statistics of the two datasets. Similar to Konstas et al.(2017), we apply entity simplification and anonymization in the preprocessing steps and restore them in the postprocessing steps.</p>
<p>The graph encoder uses randomly initialized node embeddings as well as the output from a learnable CNN with character embeddings as input. The sequence decoder uses randomly initialized token embeddings and another charlevel CNN. Model hyperparameters are chosen by a small set of experiments on the development set of LDC2017T10. The detailed settings are listed in Table 2. During testing, we use a beam size of 8 for generating graphs. To mitigate overfitting, we also apply dropout (Srivastava et al. 2014) with the drop rate of 0.2 between different layers. We use a special UNK token to replace the input node tag with a rate of 0.33 . Parameter optimization is performed with the Adam optimizer (Kingma and Ba 2014) with $\beta_{1}=0.9$ and $b e t a_{2}=0.999$. The same learning rate schedule of Vaswani et al.(2017) is adopted in our experiments. ${ }^{4}$ For computation efficiency, we gather all distinct shortest paths in a training/testing batch, and encode them into vector representations by the recurrent relation encoding procedure as described above. ${ }^{5}$</p>
<p>We run comparisons on systems without ensembling nor additional silver data. Specifically, the comparison methods can be grouped into three categories: (1) feature-based statistical methods (Song et al. 2016; Pourdamghani, Knight, and Hermjakob 2016; Song et al. 2017; Flanigan et al. 2016); (2) sequence-to-sequence neural models (Konstas et al. 2017; Cao and Clark 2019), which use linearized graphs as inputs; (3) recent works using different variants of graph neural networks for encoding graph structures directly (Song et al. 2018; Beck, Haffari, and Cohn 2018; Damonte and Cohen 2019; Guo et al. 2019). The results are shown in Table 3. For both datasets, our approach substantially outperforms</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">LDC2015E86</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LDC2017T10</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">CHRF++</td>
<td style="text-align: center;">METEOR</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">CHRF++</td>
<td style="text-align: center;">METEOR</td>
</tr>
<tr>
<td style="text-align: center;">Song et al.(2016) $\dagger$</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Flanigan et al.(2016) $\dagger$</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Pourdamghani, Knight, and Hermjakob(2016) $\dagger$</td>
<td style="text-align: center;">26.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Song et al.(2017) $\dagger$</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Konstas et al.(2017)</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Cao and Clark(2019) $\ddagger$</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Song et al.(2018)</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Beck, Haffari, and Cohn(2018)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Damonte and Cohen(2019)</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">24.1</td>
</tr>
<tr>
<td style="text-align: center;">Guo et al.(2019)</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">$54.5^{*}$</td>
<td style="text-align: center;">$31.5^{*}$</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">57.3</td>
<td style="text-align: center;">$34.0^{*}$</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">35.1</td>
</tr>
</tbody>
</table>
<p>Table 3: Main results on AMR-to-text generation. Numbers with * are from the contact from the authors. - denotes that the result is unknown because it is not provided in the corresponding paper.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">English-German</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">English-Czech</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">CHRF++</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">CHRF++</td>
</tr>
<tr>
<td style="text-align: center;">Bastings et al.(2017)</td>
<td style="text-align: center;">Single</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">9.6</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Beck, Haffari, and Cohn(2018)</td>
<td style="text-align: center;">Single</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">33.3</td>
</tr>
<tr>
<td style="text-align: center;">Guo et al.(2019)</td>
<td style="text-align: center;">Single</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">44.1</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">37.1</td>
</tr>
<tr>
<td style="text-align: center;">Beck, Haffari, and Cohn(2018)</td>
<td style="text-align: center;">Ensemble</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">35.9</td>
</tr>
<tr>
<td style="text-align: center;">Guo et al.(2019)</td>
<td style="text-align: center;">Ensemble</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">37.8</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">Single</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">47.9</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">41.1</td>
</tr>
</tbody>
</table>
<p>Table 4: Main results on syntax-based machine translation.
all previous methods. On the LDC2015E86 dataset, our method achieves a BLEU score of 27.4 , outperforming previous best-performing neural model (Guo et al. 2019) by a large margin of 2.6 BLEU points. Also, our model becomes the first neural model that surpasses the strong non-neural baseline established by Pourdamghani, Knight, and Hermjakob(2016). It is worth noting that those traditional methods marked with $\dagger$ train their language models on the external Gigaword corpus, thus they possess an additional advantage of extra data. On the LDC2017T10 dataset, our model establishes a new record BLEU score of 29.8, improving over the state-of-the-art sequence-to-sequence model (Cao and Clark 2019) by 3 points and the state-of-the-art GNN-based model (Guo et al. 2019) by 2.2 points. The results are even more remarkable since the model of Cao and Clark(2019) (marked with $\ddagger$ ) uses constituency syntax from an external parser. Similar phenomena can be found on the additional metrics of CHRF++ and METEOR (Denkowski and Lavie 2014). Those results suggest that current graph neural networks cannot make full use of the AMR graph structure, and our Graph Transformer provides a promising alternative.</p>
<h2>Syntax-based Machine Translation</h2>
<p>Our second evaluation is syntax-based machine translation, where the input is a source language dependency syntax tree and the output is a plain target language string. We employ the same data and settings from Bastings et al.(2017). Both the English-German and the English-Czech datasets from
the WMT16 translation task. ${ }^{6}$ The English sentences are parsed after tokenization to generate the dependency trees on the source side using SyntaxNet (Alberti et al. 2017). ${ }^{7}$ On the Czech and German sides, texts are tokenized using the Moses tokenizer. ${ }^{8}$ Byte-pair encodings (Sennrich, Haddow, and Birch 2016) with 8,000 merge operations are used to obtain subwords. The second block of Table 1 shows the statistics for both datasets. For model configuration, we just re-use the settings obtained in our AMR-to-text experiments.</p>
<p>Table 4 presents the results with comparison to existing methods. On the English-to-German translation task, our model achieves a BLEU score of 41.0 , outperforming all of the previously published single models by a large margin of 2.3 BLEU score. On the English-to-Czech translation task, our model also outperforms the best previously reported single models by an impressive margin of 2 BLEU points. In fact, our single model already outperforms previous state-of-the-art models that use ensembling. The advantages of our method are also verified by the metric CHRF++.</p>
<p>An important point about these experiments is that we did not tune the architecture: we simply employed the same model in all experiments, only adjusting the batch size for different dataset size. We speculate that even better results would be obtained by tuning the architecture to individual tasks. Nevertheless, we still obtained improved performance over previous works, underlining the generality of</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: CHRF++ scores with respect to (a) the graph size, (b) the graph diameter, and (c) the the number of reentrancies.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The average distance for maximum attention for each head.
our model.</p>
<h2>More Analysis</h2>
<p>The overall scores show a great advantage of the Graph Transformer over existing methods, including the state-of-the-art GNN-based models. However, they do not shed light into how this is achieved. In order to further reveal the source of performance gain, we perform a series of analysis based on different characteristics of graphs. For those analyses, we use sentence-level CHRF++ scores, and take the macro average of them when needed. All experiments are conducted with the test set of LDC2017T10.</p>
<p>Graph Size To assess the model's performance for different sizes of graphs, we group graphs into four classes and show the curves of CHRF++ scores in Figure 3a. The results are presented with the contrast with the state-of-the-art GNN-based model of Guo et al.(2019), denoted as Guo'19. As seen, the performance of both models decreases as the graph size increases. It is expected since a larger graph often contains more complex structure and the interactions between graph elements are more difficult to capture. The gap between ours and Guo'19 becomes larger for relatively larger graphs while for small graphs, both models give similar performance. This result demonstrates that our model has better ability for dealing with complicated graphs. As for extremely large graphs, the performance of both models have a clear drop, yet ours is still slightly better.</p>
<p>Graph Diameter We then study the impact of graph diameter. ${ }^{9}$ Graphs with large diameters have interactions between two nodes that appear distant from each other. We conjecture that it will cause severe difficulties for GNN-based models because they solely rely on local communication. Figure 3 b confirms our hypothesis, as the curve of the GNN-based model shows a clear slope. In contrast, our model has more stable performance, and the gap between the two curves also illustrates the superiority of our model on featuring longdistance dependencies.</p>
<p>Number of Reentrancies We study the ability for handling the reentrancies, where the same node has multiple parent nodes (or the same concept participates in multiple relations for AMR). The recent work (Damonte and Cohen 2019) has been identified reentrancies as one of the most difficult aspects of AMR structure. We bin the number of reentrancies occurred in a graph into four classes and plot Fig. 3c. It can be observed that the gap between the GNN-based model and the Graph transformer becomes noticeably wide when more than one reentrancies start to happen. Since then, our model is consistently better than the GNN-based model, maintaining a margin of over $1 \mathrm{CHRF}++$ score.</p>
<p>How Far Does Attention Look At The Graph Transformer shows a strong capacity for processing complex and large graphs. We attribute the success to the global communication design, as it provides opportunities for direct communication in long distance. A natural and interesting question is how well the model makes use of this property. To answer this question, following Voita et al.(2019), we turn to study the attention distribution of each attention head. Specifically, we record the specific distance of its maximum attention weight is assigned to. Fig. 4 shows the averaged the attention distance after we run on the development set of LDC2017T10. We can observe that nearly half of the attention heads have an average attention distance larger than 2. The number of these distance heads generally increases as layers go deeper. Interestingly, the longest-reaching head (layer1-head5) and the shortest-sighted head (layer1-head2) coexist in the very first layer, while the former has an average distance over 5 .</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Conclusions</h2>
<p>In this paper, we presented the Graph Transformer, the first graph-to-sequence learning based entirely on automatic attention. Different from previous recurrent models that require linearization of input graph and previous graph neural network models that restrict the message passing in the firstorder neighborhood, our model enables global node-to-node communication. With the Graph Transformer, we achieve the new state-of-the-art on two typical graph-to-sequence generation tasks with four benchmark datasets.</p>
<h2>References</h2>
<p>Alberti, C.; Andor, D.; Bogatyy, I.; Collins, M.; Gillick, D.; Kong, L.; Koo, T.; Ma, J.; Omernick, M.; Petrov, S.; et al. 2017. Syntaxnet models for the conll 2017 shared task. arXiv preprint arXiv:1703.04929.
Bahdanau, D.; Cho, K.; and Bengio, Y. 2015. Neural machine translation by jointly learning to align and translate. In $I C L R$.
Banarescu, L.; Bonial, C.; Cai, S.; Georgescu, M.; Griffitt, K.; Hermjakob, U.; Knight, K.; Koehn, P.; Palmer, M.; and Schneider, N. 2013. Abstract meaning representation for sembanking. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, 178-186.
Bastings, J.; Titov, I.; Aziz, W.; Marcheggiani, D.; and Sima'an, K. 2017. Graph convolutional encoders for syntax-aware neural machine translation. In EMNLP, 1957-1967.
Beck, D.; Haffari, G.; and Cohn, T. 2018. Graph-to-sequence learning using gated graph neural networks. In ACL, 273-283.
Cao, K., and Clark, S. 2019. Factorising AMR generation through syntax. In NAACL, 2157-2163.
Cho, K.; Van Merriënboer, B.; Gulcehre, C.; Bahdanau, D.; Bougares, F.; Schwenk, H.; and Bengio, Y. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.
Dai, Z.; Yang, Z.; Yang, Y.; Carbonell, J.; Le, Q.; and Salakhutdinov, R. 2019. Transformer-XL: Attentive language models beyond a fixed-length context. In ACL, 2978-2988.
Damonte, M., and Cohen, S. B. 2019. Structural neural encoders for AMR-to-text generation. In NAACL, 3649-3658.
Denkowski, M., and Lavie, A. 2014. Meteor universal: Language specific translation evaluation for any target language. In Proceedings of the EACL 2014 Workshop on Statistical Machine Translation.
Flanigan, J.; Dyer, C.; Smith, N. A.; and Carbonell, J. 2016. Generation from abstract meaning representation using tree transducers. In NAACL, 731-739.
Gu, J.; Lu, Z.; Li, H.; and Li, V. O. 2016. Incorporating copying mechanism in sequence-to-sequence learning. In $A C L, 1631-1640$.
Guo, Z.; Zhang, Y.; Teng, Z.; and Lu, W. 2019. Densely connected graph convolutional networks for graph-to-sequence learning. Transactions of the Association for Computational Linguistics 7:297-312.
Jones, B.; Andreas, J.; Bauer, D.; Hermann, K. M.; and Knight, K. 2012. Semantics-based machine translation with hyperedge replacement grammars. In COLING, 1359-1376.
Kingma, D. P., and Ba, J. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
Kipf, T. N., and Welling, M. 2017. Semi-supervised classification with graph convolutional networks. In $I C L R$.</p>
<p>Koncel-Kedziorski, R.; Bekal, D.; Luan, Y.; Lapata, M.; and Hajishirzi, H. 2019. Text Generation from Knowledge Graphs with Graph Transformers. In NAACL, 2284-2293.
Konstas, I.; Iyer, S.; Yatskar, M.; Choi, Y.; and Zettlemoyer, L. 2017. Neural AMR: Sequence-to-sequence models for parsing and generation. In $A C L, 146-157$.
Li, Y.; Tarlow, D.; Brockschmidt, M.; and Zemel, R. 2016. Gated graph sequence neural networks. In $I C L R$.
Liu, F.; Flanigan, J.; Thomson, S.; Sadeh, N.; and Smith, N. A. 2015. Toward abstractive summarization using semantic representations. In NAACL, 1077-1086.
Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002. Bleu: a method for automatic evaluation of machine translation. In $A C L$, $311-318$.
Popović, M. 2017. chrf++: words helping character n-grams. In Proceedings of the second conference on machine translation, 612618.</p>
<p>Pourdamghani, N.; Knight, K.; and Hermjakob, U. 2016. Generating english from abstract meaning representations. In INLG, $21-25$.
See, A.; Liu, P. J.; and Manning, C. D. 2017. Get to the point: Summarization with pointer-generator networks. In ACL, 10731083.</p>
<p>Sennrich, R.; Haddow, B.; and Birch, A. 2016. Neural machine translation of rare words with subword units. In ACL, 1715-1725.
Shaw, P.; Uszkoreit, J.; and Vaswani, A. 2018. Self-attention with relative position representations. In NAACL, 464-468.
Song, L.; Zhang, Y.; Peng, X.; Wang, Z.; and Gildea, D. 2016. AMR-to-text generation as a traveling salesman problem. In EMNLP, 2084-2089.
Song, L.; Peng, X.; Zhang, Y.; Wang, Z.; and Gildea, D. 2017. AMR-to-text generation with synchronous node replacement grammar. In $A C L, 7-13$.
Song, L.; Zhang, Y.; Wang, Z.; and Gildea, D. 2018. A graph-tosequence model for AMR-to-text generation. In ACL, 1616-1626.
Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; and Salakhutdinov, R. 2014. Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research 15(1):1929-1958.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is all you need. In NIPS, 5998-6008.
Voita, E.; Talbot, D.; Moiseev, F.; Sennrich, R.; and Titov, I. 2019. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In ACL, 5797-5808.
Xu, K.; Wu, L.; Wang, Z.; Feng, Y.; Witbrock, M.; and Sheinin, V. 2018. Graph2seq: Graph to sequence learning with attention-based neural networks. arXiv preprint arXiv:1804.00823.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{9}$ The diameter of a graph is defined as the length of the longest shortest path between two nodes.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ We note that the name Graph Transformer was used in a recent work (Koncel-Kedziorski et al. 2019). However, it merely focuses on the relations between directly connected nodes as other graph neural networks.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>